Having looked at trunk make-distribution.sh the --with-hive and --with-yarn are now deprecated. Here is the way I have built it: Added to pom.xml: *mvn -Pyarn -Pcdh5 -Phive -Dhadoop.version=2.3.0-cdh5.0.1 -Dyarn.version=2.3.0-cdh5.0.0 -DskipTests clean package* [INFO] ------------------------------------------------------------------------ [INFO] Reactor Summary: [INFO] [INFO] Spark Project Parent POM .......................... SUCCESS [3.165s] [INFO] Spark Project Core ................................ SUCCESS [2:39.504s] [INFO] Spark Project Bagel ............................... SUCCESS [7.596s] [INFO] Spark Project GraphX .............................. SUCCESS [22.027s] [INFO] Spark Project ML Library .......................... SUCCESS [36.284s] [INFO] Spark Project Streaming ........................... SUCCESS [24.309s] [INFO] Spark Project Tools ............................... SUCCESS [3.147s] [INFO] Spark Project Catalyst ............................ SUCCESS [20.148s] [INFO] Spark Project SQL ................................. SUCCESS [18.560s] *[INFO] Spark Project Hive ................................ FAILURE [33.962s]* [ERROR] Failed to execute goal org.apache.maven.plugins:maven-dependency-plugin:2.4:copy-dependencies (copy-dependencies) on project spark-hive_2.10: Execution copy-dependencies of goal org.apache.maven.plugins:maven-dependency-plugin:2.4:copy-dependencies failed: Plugin org.apache.maven.plugins:maven-dependency-plugin:2.4 or one of its dependencies could not be resolved: Could not find artifact Anyone who is presently building with -Phive and has a suggestion for this? Thanks v much Patrick and Sean.  I have the build working now as follows: mvn -Pyarn -Pcdh5 -Phive -DskipTests clean package in Addition, I am in the midst of running some tests and so far so good. The pom.xml changes: Added to main/parent directory pom.xml: Added four dependencies into  *examples/*pom.xml: One each for :  (hbase-common, hbase-client, hbase-protocol, hbase-server). Here is the one for hbase-common: Duplicate the above for: .. .. .. 2014-07-18 3:16 GMT-07:00 Sean Owen  This build invocation works just as you have it, for me. (At least, it Are other developers seeing the following error for the recently added substr() method?  If not, any ideas why the following invocation of tests would be failing for me - i.e. how the given invocation would need to be tweaked? mvn -Pyarn -Pcdh5 test  -pl sql/core -DwildcardSuites=org.apache.spark.sql.SQLQuerySuite (note cdh5 is a custom profile for cdh5.0.0 but should not be affecting these results) Only the test("SPARK-2407 Added Parser of SQL SUBSTR()") fails: all of the other 33 tests pass. SQLQuerySuite: - SPARK-2041 column name equals tablename - SPARK-2407 Added Parser of SQL SUBSTR() *** FAILED *** Exception thrown while executing query: == Logical Plan == java.lang.UnsupportedOperationException == Optimized Logical Plan == java.lang.UnsupportedOperationException == Physical Plan == java.lang.UnsupportedOperationException == Exception == java.lang.UnsupportedOperationException java.lang.UnsupportedOperationException at org.apache.spark.sql.catalyst.analysis.EmptyFunctionRegistry$.lookupFunction(FunctionRegistry.scala:33) at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$5$$anonfun$applyOrElse$3.applyOrElse(Analyzer.scala:131) at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$5$$anonfun$applyOrElse$3.applyOrElse(Analyzer.scala:129) at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:165) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:183) at scala.collection.Iterator$$anon$11.next(Iterator.scala:328) at scala.collection.Iterator$class.foreach(Iterator.scala:727) at scala.collection.AbstractIterator.foreach(Iterator.scala:1157) at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48) at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103) at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47) at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273) at scala.collection.AbstractIterator.to(Iterator.scala:1157) at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265) at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157) at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252) at scala.collection.AbstractIterator.toArray(Iterator.scala:1157) at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:212) at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:168) at org.apache.spark.sql.catalyst.plans.QueryPlan.org $apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionDown$1(QueryPlan.scala:52) at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1$$anonfun$apply$1.apply(QueryPlan.scala:66) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.immutable.List.foreach(List.scala:318) at scala.collection.TraversableLike$class.map(TraversableLike.scala:244) at scala.collection.AbstractTraversable.map(Traversable.scala:105) at OK I did find my error.  The missing step: mvn install I should have republished (mvn install) all of the other modules . The mvn -pl  will rely on the modules locally published and so the latest code that I had git pull'ed was not being used (except  the sql/core module code). The tests are passing after having properly performed the mvn install before  running with the mvn -pl sql/core. 2014-07-24 12:04 GMT-07:00 Stephen Boesch  Are other developers seeing the following error for the recently added I have pulled latest from github this afternoon.   There are many many errors: /assembly/target/scala-2.10: No such file or directory This causes many tests to fail. Here is the command line I am running mvn -Pyarn -Phadoop-2.3 -Phive package test i Reynold, thanks for responding here. Yes I had looked at the building with maven page in the past.  I have not noticed  that the "package" step must happen *before *the test.  I had assumed it were a corequisite -as seen in my command line. So the following sequence appears to work fine (so far so good - well past when the prior attempts failed): mvn -Pyarn -Phadoop-2.3 -DskipTests -Phive clean package mvn -Pyarn -Phadoop-2.3 -Phive test AFA documentation,  yes adding another sentence to that same "Building with Maven" page would likely be helpful to future generations. 2014-07-27 19:10 GMT-07:00 Reynold Xin  To run through all the tests you'd need to create the assembly jar first. OK i'll do it after confirming all the tests run 2014-07-27 19:36 GMT-07:00 Reynold Xin  Would you like to submit a pull request? All doc source code are in the Within its compute.close method, the JdbcRDD class has this interesting logic for closing jdbc connection: try {if (null != conn && ! stmt.isClosed()) conn.close() logInfo("closed connection") } catch {} Notice that the second check is on stmt  having been closed - not on the connection. I would wager this were not a simple oversight and there were some motivation for this logic- curious if anyone would be able to shed some light?   My particular interest is that I have written custom ORM's in jdbc since late 90's  and never did it this way. Thanks Reynold, Ted Yu did mention offline and I put in a jira already. Another small concern: there appears to be no exception handling from the creation of the prepared statement (line 74) through to the executeQuery (line 86).   In case of error/exception it would seem to be leaking connections (/statements).  If that were the case then I would include a small patch for the exception trapping in that section of code as well. BTW I was looking at this code for another reason, not intending to be a bother ;) 2014-08-05 13:03 GMT-07:00 Reynold Xin  I'm pretty sure it is an oversight. Would you like to submit a pull Hi yes that callback takes care of it. thanks! 2014-08-05 13:58 GMT-07:00 Cody Koeninger  The stmt.isClosed just looks like stupidity on my part, no secret The existing callback does take care of it:  within the DAGScheduler  there is a finally block to ensure the callbacks are executed. try {val result = job.func(taskContext, rdd.iterator(split, taskContext)) job.listener.taskSucceeded(0, result) } finally {taskContext.executeOnCompleteCallbacks() } So I have removed that exception handling code from the  PR and updated the JIRA. 2014-08-05 14:01 GMT-07:00 Reynold Xin  Yes it is. I actually commented on it: Although this has been discussed a number of times here, I am still unclear how to add user jars to the spark-shell: a) for importing classes for use directly within the shell interpreter b) for  invoking SparkContext commands with closures referencing user supplied classes contained within jar's. Similarly to other posts, I have gone through: updating bin/spark-env.sh SPARK_CLASSPATH SPARK_SUBMIT_OPTS creating conf/spark-defaults.conf  and adding spark.executor.extraClassPath --driver-class-path etc Hopefully there would be something along the lines of  a single entry added to some claspath somewhere like this SPARK_CLASSPATH/driver-class-path/spark.executor.extraClassPath (or whatever is the correct option..)  = $HBASE_HOME/*:$HBASE_HOME/lib/*:$SPARK_CLASSPATH Any ideas here? thanks Which viewer is capable of seeing all of the content in the spark docs -including the (apparent) extensions? An example page: https://github.com/apache/spark/blob/master/docs/mllib-linear-methods.md Local MD viewers/editors that I have  tried include:   mdcharm,  retext and haroopad: one of these handle the TOC,  the math symbols, or proper formatting of the scala code Even directly opening the md file from  github.com with the browser those same issues appear: no TOC, math, or proper code formatting.   I am tried both FF and chrome (on ubuntu 12.0.4) Any tips from  the creators/maintainers of these pages  Thanks! Sean Owen beat me to (strongly) recommending running zinc server.  Using the -pl option is great too - but be careful to only use it when your work is restricted to the modules in the (comma separated) list you provide to -pl.   Also before using -pl you should do a  mvn compile package install on all modules.  Use the -pl after those steps are done - and then it is very effective. 2014-10-24 13:08 GMT-07:00 Sean Owen  On Fri, Oct 24, 2014 at 8:59 PM, Koert Kuipers  wrote: Zinc absolutely helps - feels like makes builds  more than twice as fast - both on Mac and Linux.   It helps both on fresh and existing builds. 2014-10-24 14:06 GMT-07:00 Patrick Wendell  Does Zinc still help if you are just running a single totally fresh Many of the spark developers use Intellij.   You will in any case probably want a full IDE (either IJ or eclipse) 2014-10-26 8:07 GMT-07:00 ll  i'm new to both scala and spark.  what IDE / dev environment do you find I have run on the command line via maven and it is fine: mvn   -Dscalastyle.failOnViolation=false -DskipTests -Pyarn -Phadoop-2.3 compile package install But with the latest code Intellij builds do not work. Following is one of 26 similar errors: Error:(173, 38) not found: value HiveShim Option(tableParameters.get(HiveShim.getStatsSetupConstTotalSize)) ^ Hi Matei, Until my latest pull from upstream/master it had not been necessary to add the hive profile: is it now?? I am not using sbt gen-idea. The way to open in intellij has been to Open the parent directory. IJ recognizes it as a maven project. There are several steps to do surgery on the yarn-parent / yarn projects , then do a full rebuild.  That was working until one week ago. Intellij/maven is presently broken in  two ways:  this hive shim (which may yet hopefully be a small/simple fix - let us see) and  (2) the "NoClassDefFoundError on ThreadFactoryBuilder" from my prior emails -and which is quite a serious problem . 2014-10-28 19:46 GMT-07:00 Matei Zaharia  Hi Stephen, Thanks Patrick for the heads up. I have not been successful to discover a combination of profiles (i.e. enabling hive or hive-0.12.0 or hive-13.0) that works in Intellij with maven. Anyone who knows how to handle this - a quick note here would be appreciated. 2014-10-28 20:20 GMT-07:00 Patrick Wendell  Hey Stephen, I have selected the same options as Cheng LIang: hadoop-2.4, hive, hive 0.12.0 .  After  a full Rebuild in IJ I  still see the HiveShim errors. I really do not know what is different. I had pulled three hours ago from github upstream master. Just for kicks i am trying PW's combination which uses 0.13.1 now.. But it appears there is something else going on here. Patrick/ Cheng:  did you build on the command line using Maven first?  I do that since in the past that had been required. 2014-10-28 21:57 GMT-07:00 Patrick Wendell  I just started a totally fresh IntelliJ project importing from our Thanks guys - adding the source root for the shim manually was the issue. For some reason the other issue I  was struggling with (NoCLassDefFoundError on ThreadFactoryBuilder) also disappeared. I am able to run tests now inside IJ.  Woot 2014-10-28 22:13 GMT-07:00 Patrick Wendell  Oops - I actually should have added v0.13.0 (i.e. to match whatever I HI Nabeel, In what ways is the IJ version of scala repl enhanced?  thx! 2014-10-30 3:41 GMT-07:00  IntelliJ idea scala plugin comes with an enhanced REPL. It's a pretty HI Nabeel, In what ways is the IJ version of scala repl enhanced?  thx! 2014-10-30 3:41 GMT-07:00  IntelliJ idea scala plugin comes with an enhanced REPL. It's a pretty May we please refrain from using spark mailing list for job inquiries. Thanks. 2014-10-31 13:35 GMT-07:00 Alessandro Baretta  Hello, HI Alessandro, It is important to me and probably others as well to be able to focus on the technical issues and not be distracted that way. thanks stephenb 2014-10-31 13:48 GMT-07:00 Alessandro Baretta  Stephen, Yes I have seen this same error - and for team members as well - repeatedly since June. A Patrick and Cheng mentioned, the next step is to do an sbt clean 2014-11-02 19:37 GMT-08:00 Cheng Lian  I often see this when I first build the whole Spark project with SBT, then HI Michael, That insight is useful.   Some thoughts: * I moved from sbt to maven in June specifically due to Andrew Or's describing mvn as the default build tool.  Developers should keep in mind that jenkins uses mvn so we need to run mvn before submitting PR's - even if sbt were used for day to day dev work *  In addition, as Sean has alluded to, the Intellij seems to comprehend the maven builds a bit more readily than sbt * But for command line and day to day dev purposes:  sbt sounds great to use  Those sound bites you provided about exposing built-in test databases for hive and for displaying available testcases are sweet.  Any easy/convenient way to see "more of " those kinds of facilities available through sbt ? 2014-11-16 13:23 GMT-08:00 Michael Armbrust  I'm going to have to disagree here.  If you are building a release It seems there were some additional settings required to build spark now . This should be a snap for most of you ot there about what I am missing. Here is the command line I have traditionally used: mvn -Pyarn -Phadoop-2.3 -Phive install compile package -DskipTests That command line is however failing with the lastest from HEAD: INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @ spark-network-common_2.10 --- [INFO] Using zinc server for incremental compilation [INFO] compiler plugin: BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null) *[error] Required file not found: scala-compiler-2.10.4.jar* *[error] See zinc -help for information about locating necessary files* [INFO] ------------------------------------------------------------------------ [INFO] Reactor Summary: [INFO] [INFO] Spark Project Parent POM .......................... SUCCESS [4.077s] [INFO] Spark Project Networking .......................... FAILURE [0.445s] OK let's try "zinc -help": 18:38:00/spark2 $*zinc -help* Nailgun server running with 1 cached compiler Version = 0.3.5.1 Zinc compiler cache limit = 5 Resident scalac cache limit = 0 Analysis cache limit = 5 Compiler(Scala 2.10.4) [74ff364f] Setup = {*   scala compiler = /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar* scala library = /Users/steve/.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar scala extra = {/Users/steve/.m2/repository/org/scala-lang/scala-reflect/2.10.4/scala-reflect-2.10.4.jar /shared/zinc-0.3.5.1/lib/scala-reflect.jar } sbt interface = /shared/zinc-0.3.5.1/lib/sbt-interface.jar compiler interface sources = /shared/zinc-0.3.5.1/lib/compiler-interface-sources.jar java home = fork java = false cache directory = /Users/steve/.zinc/0.3.5.1 } Does that compiler jar exist?  Yes! 18:39:34/spark2 $ll /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar -rw-r--r--  1 steve  staff  14445780 Apr  9  2014 /Users/steve/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar Mac as well.  Just found the problem:  I had created an alias to zinc a couple of months back. Apparently that is not happy with the build anymore. No problem now that the issue has been isolated - just need to fix my zinc alias. 2014-12-01 18:55 GMT-08:00 Ted Yu  I tried the same command on MacBook and didn't experience the same error. Anyone maybe can assist on how to run zinc with the latest maven build? I am starting zinc as follows: /shared/zinc-0.3.5.3/dist/target/zinc-0.3.5.3/bin/zinc -scala-home $SCALA_HOME -nailed -start The pertinent env vars are: 19:58:11/lib $echo $SCALA_HOME /shared/scala 19:58:14/lib $which scala /shared/scala/bin/scala 19:58:16/lib $scala -version Scala code runner version 2.10.4 -- Copyright 2002-2013, LAMP/EPFL When I do *not *start zinc then the maven build works .. but v slowly since no incremental compiler available. When zinc is started as shown above then the error occurs on all of the modules except parent: [INFO] Using zinc server for incremental compilation [INFO] compiler plugin: BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null) [error] Required file not found: scala-compiler-2.10.4.jar [error] See zinc -help for information about locating necessary files 2014-12-01 19:02 GMT-08:00 Stephen Boesch  Mac as well.  Just found the problem:  I had created an alias to zinc a The zinc src zip for  0.3.5.3 was  downloaded  and exploded. Then I  ran sbt dist/create .  zinc is being launched from dist/target/zinc-0.3.5.3/bin/zinc 2014-12-01 20:12 GMT-08:00 Ted Yu  I use zinc 0.2.0 and started zinc with the same command shown below. Thanks Sean, I followed suit (brew install zinc) and that is working. 2014-12-01 22:39 GMT-08:00 Sean Owen  I'm having no problems with the build or zinc on my Mac. I use zinc What is the recommended way to do this?  We have some native database client libraries for which we are adding pyspark bindings. The pyspark invokes spark-submit.   Do we add our libraries to the SPARK_SUBMIT_LIBRARY_PATH ? This issue relates back to an error we have been seeing "Py4jError: Trying to call a package" - the suspicion being that the third party libraries may not be available on the jvm side. For building in intellij with sbt my mileage has varied widely: it had built as late as Monday (after the 1.3.0 release)  - and with zero 'special' steps: just "import" as sbt project. However I can not presently repeat the process.  The wiki page has the latest instructions on how to build with maven - but not with sbt.  Is there a resource for that? https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-IntelliJ The error I see is same as from a post in July http://apache-spark-user-list.1001560.n3.nabble.com/Build-spark-with-Intellij-IDEA-13-td9904.html Here is an excerpt: uncaught exception during compilation: java.lang.AssertionError Error:scalac: Error: assertion failed: com.google.protobuf.InvalidProtocolBufferException java.lang.AssertionError: assertion failed: com.google.protobuf.InvalidProtocolBufferException at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1212) The answer in the mailing list to that thread was about using maven .. so that is not useful here. Hi Akhil Those instructions you provided are showing how to manually build an sbt project that may include adding spark dependencies.  Whereas my OP was about how to open the existing spark sbt project .  These two are not similar tasks. 2015-02-04 21:46 GMT-08:00 Akhil Das  Here's the sbt version There are some convenience methods you might consider including: MLUtils.loadLibSVMFile and   MLUtils.loadLabeledPoint 2015-03-26 14:16 GMT-07:00 Ulanov, Alexander  Hi, I am iteratively making changes to the scala side of some new pyspark code and re-testing from the python/pyspark side. Presently my only solution is to rebuild completely sbt assembly after any scala side change - no matter how small. Any better / expedited way for pyspark to see small scala side updates? Compile alone did not show the scala code changes AFAICT. I will reverify. 2015-03-27 10:16 GMT-07:00 Davies Liu  I usually just open a terminal to do `build/sbt ~compile`, coding in Thx much!  This works. My workflow is making changes to files in Intelij and running ipython to execute pyspark. Is there any way for ipython to "see the updated class files without first exiting? 2015-03-27 10:21 GMT-07:00 Davies Liu  put these lines in your ~/.bash_profile Thanks Cheng. Yes, the problem is that the way to set up to run inside Intellij changes v frequently.  It is unfortunately not simply a one-time investment to get IJ debugging working properly: the steps required are a moving target approximately monthly to bi-monthly. Doing remote debugging is probably a good choice to reduce the dev environment volatility/maintenance. 2015-04-04 5:46 GMT-07:00 Cheng Lian  I found in general it's a pain to build/run Spark inside IntelliJ IDEA. I following up from Nicholas, it is [SPARK-12345] Your PR description where 12345 is the jira number. One thing I tend to forget is when/where to include the subproject tag e.g. [MLLIB] 2015-05-13 11:11 GMT-07:00 Nicholas Chammas  That happens automatically when you open a PR with the JIRA key in the PR I am reviving an old thread here. The link for the example code for the java enum based solution is now dead: would someone please post an updated link showing the proper interop? Specifically: it is my understanding that java enum's may not be created within Scala.  So is the proposed solution requiring dropping out into Java to create the enum's? 2015-04-09 17:16 GMT-07:00 Xiangrui Meng  Using Java enums sound good. We can list the values in the JavaDoc and There was a long thread about enum's initiated by Xiangrui several months back in which the final consensus was to use java enum's.  Is that discussion (/decision) applicable here? 2015-09-16 17:43 GMT-07:00 Ulanov, Alexander  Hi Joseph, The dev/change-scala-version.sh [2.11]  script modifies in-place  the pom.xml files across all of the modules.  This is a git-visible change.  So if we wish to make changes to spark source in our own fork's - while developing with scala 2.11 - we would end up conflating those updates with our own. A possible scenario would be to update .gitignore - by adding pom.xml. However I can not get that to work: .gitignore is tricky. Suggestions appreciated. The effects of changing the pom.xml extend beyond cases in which we wish to modify spark itself. In addition when git pull'ing from trunk we need to either stash or roll back the changes before rebase'ing. An effort to look into a better solution (possibly including evaluating Ted Yu's suggested approach) might be considered? 2015-09-20 9:12 GMT-07:00 Ted Yu  Maybe the following can be used for changing Scala version: Yes. The current dev/change-scala-version.sh mutates (/pollutes) the build environment by updating the pom.xml in each of the subprojects. If you were able to come up with a structure that avoids that approach it would be an improvement. 2015-11-05 15:38 GMT-08:00 Jakob Odersky  Hi everyone, Is this a candidate for the version 1.X/2.0 split? 2015-12-09 16:29 GMT-08:00 Michael Armbrust  Yeah, I would like to address any actual gaps in functionality that are 1) you should run zinc incremental compiler 2) if you want breakpoints that should likely be done in local mode 3) adjust the log4j.properties settings and you can start to see the logInfo 2015-12-27 0:20 GMT-08:00 salexln  Hi guys, Which Resource Manager  are you using? 2016-01-20 21:38 GMT-08:00 Renu Yadav  Any suggestions? +1 for java8 only   +1 for 2.11+ only .    At this point scala libraries supporting only 2.10 are typically less active and/or poorly maintained. That trend will only continue when considering the lifespan of spark 2.X. 2016-03-24 11:32 GMT-07:00 Steve Loughran  On 24 Mar 2016, at 15:27, Koert Kuipers  wrote: Yes: will you have cycles to do it? 2016-09-12 9:09 GMT-07:00 Nick Pentreath  Never actually got around to doing this - do folks still think it Along the lines of #1:  the spark packages seemed to have had a good start about two years ago: but now there are not more than a handful in general use - e.g. databricks CSV. When the available packages are browsed the majority are incomplete, empty, unmaintained, or unclear. Any ideas on how to resurrect spark packages in a way that there will be sufficient adoption for it to be meaningful? 2017-01-23 17:03 GMT-08:00 Joseph Bradley  This thread is split off from the "Feedback on MLlib roadmap process re: spark-packages.org  and "Would these really be better in the core project?"   That was not at all the intent of my input: instead to ask "how and where to structure/place deployment quality code that yet were *not* part of the distribution?"   The spark packages has no curation whatsoever : no minimum standards of code quality and deployment structures, let alone qualitative measures of usefulness. While spark packages would never rival CRAN and friends there is not even any mechanism in place to get started.  From the CRAN site: Even at the current growth rate of several packages a day, all submissions are still rigorously quality-controlled using strong testing features available in the R system . Maybe give something that has a subset of these processes a try ? Different folks than are already over-subscribed in MLlib ? 2017-01-24 2:37 GMT-08:00 Sean Owen  My $0.02, which shouldn't be weighted too much. Hi Reynold, This is not necessarily convincing.  Many installations are still on spark 1.X - including at the large company I work at.  When moving to 2.2 - whenever that might happen -  it would be a reasonable expectation to also move off of an old version of scala.  Of the 30% of customers shown I wonder how many are both ( a) on spark 2.X/scala 2.10 *now **and* ( b) would be unable to manage a transition to scala 2.11/2.12 whenever the move to  spark 2.2 were to happen. stephenb 2017-03-06 19:04 GMT-08:00 Reynold Xin  For some reason the previous email didn't show up properly. Trying again.