Actually, with HiveContext, you can join hive tables with registered temporary tables. On Fri, Aug 22, 2014 at 9:07 PM, chutium  wrote: Hello, Is there any way to get the attempt number in a closure? Seems TaskContext.attemptId actually returns the taskId of a task (see this and this ). It looks like a bug. Thanks, Yin Yeah, seems we need to pass the attempt id to executors through TaskDescription. I have created https://issues.apache.org/jira/browse/SPARK-4014. On Mon, Oct 20, 2014 at 1:57 PM, Reynold Xin  wrote: Yes, it is for (2). I was confused because the doc of TaskContext.attemptId (release 1.1) is "the number of attempts to execute this task". Seems the per-task attempt id used to populate "attempt" field in the UI is maintained by TaskSetManager and its value is assigned in resourceOffer. On Mon, Oct 20, 2014 at 4:56 PM, Reynold Xin  wrote: Seems you hit https://issues.apache.org/jira/browse/SPARK-4245. It was fixed in 1.2. Thanks, Yin On Wed, Dec 3, 2014 at 11:50 AM, invkrh  wrote: Hi Alex, Can you attach the output of sql("explain extended ").collect.foreach(println)? Thanks, Yin On Fri, Jan 16, 2015 at 1:54 PM, Alessandro Baretta  Reynold, We probably will extract general purpose functions from JsonRDD and also do the renaming through https://issues.apache.org/jira/browse/SPARK-5260. On Tue, Feb 3, 2015 at 9:15 AM, Daniil Osipov  Thanks Reynold, Hi Quizhuang, Right now, char is not supported in DDL. Can you try varchar or string? Thanks, Yin On Mon, Feb 16, 2015 at 10:39 PM, Qiuzhuang Lian  Hi, Hi Nitay, Can you try using backticks to quote the column name? Like org.apache.spark.sql.hive.HiveMetastoreTypes.toDataType("struct")? Thanks, Yin On Tue, Mar 10, 2015 at 2:43 PM, Michael Armbrust  Thanks for reporting.  This was a result of a change to our DDL parser Hi Michael, Thanks for reporting it. Yes, it is a bug. I have created https://issues.apache.org/jira/browse/SPARK-6437 to track it. Thanks, Yin On Thu, Mar 19, 2015 at 10:51 AM, Michael Allman  I've examined the experimental support for ExternalSorter in Spark SQL, Hi Cesar, Can you try 1.3.1 (https://spark.apache.org/releases/spark-release-1-3-1.html) and see if it still shows the error? Thanks, Yin On Fri, Apr 17, 2015 at 1:58 PM, Reynold Xin  wrote: Spark SQL developers, If you are trying to add new tests based on HiveComparisonTest and want to generate golden answer files with Hive 0.13.1, unfortunately, the setup work is quite different from that for Hive 0.12. We have updated SQL readme to include the new instruction for Hive 0.13.1. You can find it at the the section of "Other dependencies for developers". Please let me know if you still see any issue after setup your environment based on this instruction. Thanks, Yin For Spark SQL internal operations, probably we can just create MapPartitionsRDD directly (like https://github.com/apache/spark/commit/5287eec5a6948c0c6e0baaebf35f512324c0679a ). On Fri, May 29, 2015 at 11:04 AM, Josh Rosen  wrote: Sean, Can you add "-Phive -Phive-thriftserver" and try those Hive tests? Thanks, Yin On Fri, Jun 5, 2015 at 5:19 AM, Sean Owen  wrote: Is it the full stack trace? On Thu, Jun 18, 2015 at 6:39 AM, Sea  wrote: Hi Tom, In Spark 1.4, we have de-coupled the support of Hive's metastore and other parts (parser, Hive udfs, and Hive SerDes). The execution engine of Spark SQL in 1.4 will always use Hive 0.13.1. For the metastore connection part, you can connect to either Hive 0.12 or 0.13.1's metastore. We have removed old shims and profiles of specifying the Hive version (since execution engine is always using Hive 0.13.1 and metastore client part can be configured to use either Hive 0.12 or 0.13.1's metastore). You can take a look at https://spark.apache.org/docs/latest/sql-programming-guide.html#interacting-with-different-versions-of-hive-metastore for connecting to Hive 0.12's metastore. Let me know if you have any question. Thanks, Yin On Wed, Jun 17, 2015 at 4:18 PM, Thomas Dudziak  wrote: +1. I tested those SQL blocker bugs in my laptop and they have been fixed. On Mon, Jun 29, 2015 at 6:51 AM, Sean Owen  wrote: The JSON support in Spark SQL handles a file with one JSON object per line or one JSON array of objects per line. What is the format your file? Does it only contain a single line? On Wed, Aug 26, 2015 at 6:47 AM, gsvic  wrote: -1 Found a problem on reading partitioned table. Right now, we may create a SQL project/filter operator for every partition. When we have thousands of partitions, there will be a huge number of SQLMetrics (accumulators), which causes high memory pressure to the driver and then takes down the cluster (long GC time causes different kinds of timeouts). https://issues.apache.org/jira/browse/SPARK-10339 Will have a fix soon. On Fri, Aug 28, 2015 at 3:18 PM, Jon Bender  Marcelo, What is the Hive version of your metastore server? Looks like you are using a Hive 1.2's metastore client talking to your existing Hive 0.13.1 metastore server? On Thu, Sep 10, 2015 at 10:48 AM, Michael Armbrust  Can you open a JIRA? Yes, Spark 1.5 use Hive 1.2's metastore client by default. You can change it by putting the following settings in your spark conf. spark.sql.hive.metastore.version = 0.13.1 spark.sql.hive.metastore.jars = maven or the path of your hive 0.13 jars and hadoop jars For spark.sql.hive.metastore.jars, basically, it tells spark sql where to find metastore client's classes of Hive 0.13.1. If you set it to maven, we will download needed jars directly (it is an easy way to do testing work). On Thu, Sep 10, 2015 at 7:45 PM, StanZhai  wrote: A scale of 10 means that there are 10 digits at the right of the decimal point. If you also have precision 10, the range of your data will be [0, 1) and casting "10.5" to DecimalType(10, 10) will return null, which is expected. On Mon, Sep 14, 2015 at 1:42 PM, Dirceu Semighini Filho  wrote: Hi Jerry, Looks like it is a Python-specific issue. Can you create a JIRA? Thanks, Yin On Mon, Sep 21, 2015 at 8:56 AM, Jerry Lam  wrote: btw, does 1.4 has the same problem? On Mon, Sep 21, 2015 at 10:01 AM, Yin Huai  wrote: Seems 1.4 has the same issue. On Mon, Sep 21, 2015 at 10:01 AM, Yin Huai  wrote: Looks like the problem is df.rdd does not work very well with limit. In scala, df.limit(1).rdd will also trigger the issue you observed. I will add this in the jira. On Mon, Sep 21, 2015 at 10:44 AM, Jerry Lam  wrote: +1 Tested 1.5.1 SQL blockers. On Sat, Sep 26, 2015 at 1:36 PM, robineast  wrote: Hi Guys, Seems Jenkins is down or very slow? Does anyone else experience it or just me? Thanks, Yin Seems it is back. On Thu, Nov 12, 2015 at 6:21 PM, Yin Huai  wrote: It was generally slow. But, after 5 or 10 minutes, it's all good. On Fri, Nov 13, 2015 at 9:16 AM, shane knapp  wrote: I think they can renew your license. In https://www.jetbrains.com/buy/opensource/?product=idea, you can find "Update Open Source License". On Wed, Dec 2, 2015 at 7:47 AM, Sean Owen  wrote: -1 Tow blocker bugs have been found after this RC. https://issues.apache.org/jira/browse/SPARK-12089 can cause data corruption when an external sorter spills data. https://issues.apache.org/jira/browse/SPARK-12155 can prevent tasks from acquiring memory even when the executor indeed can allocate memory by evicting storage memory. https://issues.apache.org/jira/browse/SPARK-12089 has been fixed. We are still working on https://issues.apache.org/jira/browse/SPARK-12155. On Fri, Dec 4, 2015 at 3:04 PM, Mark Hamstra  0 +1 Critical and blocker issues of SQL have been addressed. On Sat, Dec 12, 2015 at 9:39 AM, Michael Armbrust  I'll kick off the voting with a +1. Ted, Looks like thrift server tests were just hanging. See https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/Spark-Master-Maven-with-YARN/4453/HADOOP_PROFILE=hadoop-2.4,label=spark-test/artifact/sql/hive-thriftserver/target/unit-tests.log. If it is caused by a recent commit, it is also possible that a commit listed in build 4438 or 4439 that caused it since 4438 and 4439 were failed way before the thrift server tests. On Fri, Dec 11, 2015 at 10:27 AM, Ted Yu  wrote: Can you reproduce the problem in your local environment? Our 1.6 hadoop 2.4 maven build looks pretty good. Since our 1.6 is pretty close to master, I am wondering if there is any environment related issue. On Sun, Dec 13, 2015 at 3:38 PM, Ted Yu  wrote: Hi Shane, Seems Spark's lint-r started to fail from https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/Spark-Master-SBT/4260/AMPLAB_JENKINS_BUILD_PROFILE=hadoop1.0,label=spark-test/console. Is it related to the upgrade work of R? Thanks, Yin On Mon, Dec 14, 2015 at 11:55 AM, shane knapp  wrote: oh i see. In your log, I guess you can find a line like "Initializing execution hive, version". The line you showed is actually associated with execution hive, which is a fake metastore that used by spark sql internally. Logs related to the real metastore (the metastore storing table metadata and etc.) starts from the line of "Initializing HiveMetastoreConnection version 1.2.1 using Spark classes."Hope this is helpful. On Wed, Dec 16, 2015 at 12:05 PM, syepes  wrote: Hi Jerry, Looks like https://issues.apache.org/jira/browse/SPARK-11739 is for the issue you described. It has been fixed in 1.6. With this change, when you call SQLContext.getOrCreate(sc2), we will first check if sc has been stopped. If so, we will create a new SQLContext using sc2. Thanks, Yin On Sun, Dec 20, 2015 at 2:59 PM, Jerry Lam  wrote: +1 On Tue, Dec 22, 2015 at 8:10 PM, Denny Lee  wrote: +1 On Mon, Mar 7, 2016 at 12:39 PM, Reynold Xin  wrote: Hello Richard, Looks like the Dataset is Dataset[(Int, Int)]. I guess for the case of Option(u)) })". We are trying to use null to create a "(Int, Int)" and somehow it ended up with a tuple2 having default values. Can you create a jira? We will investigate the issue. Thanks! Yin On Mon, Jun 20, 2016 at 8:21 AM, Richard Marscher  I know recently outer join was changed to preserve actual nulls through -1 because of https://issues.apache.org/jira/browse/SPARK-16121. This jira was resolved after 2.0.0-RC1 was cut. Without the fix, Spark SQL effectively only uses the driver to list files when loading datasets and the driver-side file listing is very slow for datasets having many files and partitions. Since this bug causes a serious performance regression, I am giving -1. On Thu, Jun 23, 2016 at 1:25 AM, Pete Robbins  wrote: Hi Jacek, We will try to create the default database if it does not exist. Hive actually relies on that AlreadyExistsException to determine if a db already exists and ignore the error to implement the logic of "CREATE DATABASE IF NOT EXISTS". So, that message does not mean any bad thing happened. I think we can avoid of having this error log by changing https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala#L84-L91. Basically, we will check if the default db already exists and we only call create database if the default db does not exist. Do you want to try it? Thanks, Yin On Tue, Aug 16, 2016 at 7:33 PM, Jacek Laskowski  wrote: Yea. Please create a jira. Thanks! On Tue, Aug 16, 2016 at 11:06 PM, Jacek Laskowski  wrote: +1 On Sun, Sep 25, 2016 at 11:40 AM, Dongjoon Hyun  wrote: +1 On Thu, Sep 29, 2016 at 4:07 PM, Luciano Resende  +1 (non-binding) +1 On Tue, Nov 1, 2016 at 9:51 PM, Reynold Xin  wrote: Hi all, Apache Spark 2.1.0 is the second release of Spark 2.x line. This release makes significant strides in the production readiness of Structured Streaming, with added support for event time watermarks and Kafka 0.10 support . In addition, this release focuses more on usability, stability, and polish, resolving over 1200 tickets. We'd like to thank our contributors and users for their contributions and early feedback to this release. This release would not have been possible without you. To download Spark 2.1.0, head over to the download page: http://spark.apache.org/downloads.html To view the release notes: https://spark.apache.org/releases/spark-release-2-1-0.html (note: If you see any issues with the release notes, webpage or published artifacts, please contact me directly off-list)