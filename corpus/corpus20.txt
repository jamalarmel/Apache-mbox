I think we should avoid migrating the list too many times, especially the
user list.
Also - are there any rules regarding maintaining a separate, non-Apache
mailing list by 3rd party? Google Groups has been very convenient for
users, both in terms of the UX and the way to quickly and easily search for
archived messages.
--
Reynold Xin, AMPLab, UC Berkeley
http://rxin.org
Copying Chris on this one.
--
Reynold Xin, AMPLab, UC Berkeley
http://rxin.org
They are asking about dedicated matrix libraries.
Neither GraphX nor Giraph are matrix libraries. These are systems that
handle large scale graph processing, which could possibly be modeled as
matrix computations.  Hama looks like a BSP framework, so I am not sure if
it has anything to do with matrix library either.
For very small matrices (3x3, 4x4), the cost of going through jni to do
native matrix operations will likely dominate the computation itself, so
you are probably better off with a simple unrolled for loop in Java.
I haven't looked into this myself, but I heard mahout-math is a decent
library.
--
Reynold Xin, AMPLab, UC Berkeley
http://rxin.org
+1
--
Reynold Xin, AMPLab, UC Berkeley
http://rxin.org
+1
--
Reynold Xin, AMPLab, UC Berkeley
http://rxin.org
FYI
Thanks, Shane. Can you also link to this mailing list discussion from the
JIRA ticket?
--
Reynold Xin, AMPLab, UC Berkeley
http://rxin.org
Is there a way we can track the number of downloads with the apache mirrors?
Hi Martin,
Thanks for updating us. Prashant has also been updating the scala 2.10
branch at https://github.com/mesos/spark/tree/scala-2.10
Did you take a look at his work?
Thanks. I just closed the issue.
It is fairly simple and just runs mini-batch sgd. You can actually just
look at the code.
https://github.com/apache/incubator-spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/classification/SVM.scala
Thanks, Josh. These are very useful for people to understand the APIs and
to write new language bindings.
Just generate the IntelliJ project file using
sbt/sbt gen-idea
And then open the folder in IntelliJ (no need to import anything).
Hi Mark,
I can't comment much on the Spark part right now (because I have to run in
3 mins), but we will make Shark 0.8.1 work with Spark 0.8.1 for sure. Some
of the changes will get cherry picked into branch-0.8 of Shark.
It's not a very elegant solution, but one possibility is for the
CacheManager to check whether it will have enough space. If it is running
out of space, skips buffering the output of the iterator & directly write
the output of the iterator to disk (if storage level allows that).
But it is still tricky to know whether we will run out of space before we
even start running the iterator. One possibility is to use sizing data from
previous partitions to estimate the size of the current partition (i.e.
estimated in memory size = avg of current in-memory size / current input
size).
Do you have any ideas on this one, Kyle?
I chatted with Matt Massie about this, and here are some options:
1. Use dependency injection in google-guice to make Akka use one version of
protobuf, and YARN use the other version.
2. Look into OSGi to accomplish the same goal.
3. Rewrite the messaging part of Spark to use a simple, custom RPC library
instead of Akka. We are really only using a very simple subset of Akka
features, and we can probably implement a simple RPC library tailored for
Spark quickly. We should only do this as the last resort.
4. Talk to Akka guys and hope they can make a maintenance release of Akka
that supports protobuf 2.5.
None of these are ideal, but we'd have to pick one. It would be great if
you have other suggestions.
Adding in a few guys so they can chime in.
I think we are near the end of Scala 2.9.3 development, and will merge the
Scala 2.10 branch into master and make it the future very soon (maybe next
week).  This problem will go away.
Meantime, we are relying on periodically merging the master into the Scala
2.10 branch.
+aaron on this one since he changed the executor runner. (I think it is
probably an oversight but Aaron should confirm.)
That is correct. However, there is no guarantee right now that Akka 2.3
will work correctly for us. We haven't tested it enough yet (or rather, we
haven't tested it at all) E.g. see:
https://github.com/apache/incubator-spark/pull/131
We want to make Spark 0.9.0 based on Scala 2.10, but we have also been
discussing ideas to make a Scala 2.10 version of Spark 0.8.x so it enables
users to move to Scala 2.10 earlier if they want.
Can you provide a link to your pull request?
Take a look at this pull request and see if it fixes your problem:
https://github.com/apache/incubator-spark/pull/201
I changed the semantics of the index from the output partition index back
to the rdd partition index.
Olivier,
Do you want us to create a Spark user meetup event for this hackathon?
Definitely some people will get confused. It's up to you. If we post it, we
can mark it in the title that this is a hackathon.
Thanks for the update Chris.
We do need to graduate soon. People have been asking me does "incubating"
means the project is very immature. :(
One thing we need to do is to import the JIRA tickets from AMPLab's JIRA.
That INFRA ticket hasn't moved much along. Can you help push that?
It took me hours to debug a problem yesterday on the latest master branch
(0.9.0-SNAPSHOT), and I would like to share with the dev list in case
anybody runs into this Akka problem.
A little background for those of you who haven't followed closely the
development of Spark and YARN 2.2: YARN 2.2 uses protobuf 2.5, and Akka
uses an older version of protobuf that is not binary compatible. In order
to have a single build that is compatible for both YARN 2.2 and pre-2.2
YARN/Hadoop, we published a special version of Akka that builds with
protobuf shaded (i.e. using a different package name for the protobuf
stuff).
However, it turned out Scala 2.10 includes a version of Akka jar in its
default classpath (look at the lib folder in Scala 2.10 binary
distribution). If you use the scala command to launch any Spark application
on the current master branch, there is a pretty high chance that you
wouldn't be able to create the SparkContext (stack trace at the end of the
email). The problem is that the Akka packaged with Scala 2.10 takes
precedence in the classloader over the special Akka version Spark includes.
Before we have a good solution for this, the workaround is to use java to
launch the application instead of scala. All you need to do is to include
the right Scala jars (scala-library and scala-compiler) in the classpath.
Note that the scala command is really just a simple script that calls java
with the right classpath.
Stack trace:
java.lang.NoSuchMethodException:
akka.remote.RemoteActorRefProvider.(java.lang.String,
akka.actor.ActorSystem$Settings, akka.event.EventStream,
akka.actor.Scheduler, akka.actor.DynamicAccess)
at java.lang.Class.getConstructor0(Class.java:2763)
at java.lang.Class.getDeclaredConstructor(Class.java:2021)
at
akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$2.apply(DynamicAccess.scala:77)
at scala.util.Try$.apply(Try.scala:161)
at
akka.actor.ReflectiveDynamicAccess.createInstanceFor(DynamicAccess.scala:74)
at
akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply(DynamicAccess.scala:85)
at
akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply(DynamicAccess.scala:85)
at scala.util.Success.flatMap(Try.scala:200)
at
akka.actor.ReflectiveDynamicAccess.createInstanceFor(DynamicAccess.scala:85)
at akka.actor.ActorSystemImpl.(ActorSystem.scala:546)
at akka.actor.ActorSystem$.apply(ActorSystem.scala:111)
at akka.actor.ActorSystem$.apply(ActorSystem.scala:104)
at org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:79)
at org.apache.spark.SparkEnv$.createFromSystemProperties(SparkEnv.scala:120)
at org.apache.spark.SparkContext.(SparkContext.scala:106)
Yup - you are safe if you stick to the official documented method.
A lot of users also use scala for a variety of reasons (e.g. old script)
and that used to work also.
I'm not strongly against Option.fold, but I find the readability getting
worse for the use case you brought up.  For the use case of if/else, I find
Option.fold pretty confusing because it reverses the order of Some vs None.
Also, when code gets long, the lack of an obvious boundary (the only
boundary is "} {") with two closures is pretty confusing.
I usually use sbt. i.e. sbt/sbt test
Again, I usually use sbt ...
sbt/sbt "test-only *TaskResultGetterSuite*"
The application web ui is pretty useful. We have been adding more and more
information to the web ui for easier performance analysis.
Look at Patrick Wendell's two talks at the Spark Summit for more
information: http://spark-summit.org/summit-2013/
I added the option that doesn't require the caller to specify the
mergeCombiner closure a while ago when I wanted to disable mapSideCombine.
In virtually all use cases I know of, it is fine & easy to specify a
mergeCombiner, so I'm all for this given it simplifies the codebase.
It is historic.
I think we are converging towards
worker: the "slave" daemon in the standalone cluster manager
executor: the jvm process that is launched by the worker that executes tasks
Doesn't Apache do redirection from incubation. to the normal website also?
 By the time that happens, we can also update the URL in the script?
Thanks. Why don't you submit a pr and then we can work on it?
We have a Scala style configuration file in Shark:
https://github.com/amplab/shark/blob/master/scalastyle-config.xml
However, the scalastyle project is still pretty primitive and doesn't cover
most of the use cases. It is still great to include it to cover basic
checks such as 100-char wide lines.
Thanks for doing that, DB. Not sure about others, but I'm actually strongly
against blanket automatic code formatters, given that they can be
disruptive. Often humans would intentionally choose to style things in a
certain way for more clear semantics and better readability. Code
formatters don't capture these nuances. It is pretty dangerous to just auto
format everything.
Maybe it'd be ok if we restrict the code formatters to a very limited set
of things, such as indenting function parameters, etc.
Hi Jerry,
Why don't you submit a pull request and then we can discuss there? If
SimRank is not common enough, we might take the matrix multiplication
method in and merge that. At the very least, even if SimRank doesn't get
merged into Spark, we can include a contrib package or a Wiki page that
links to examples of various algorithms community members have implemented.
+1
I also just went over the config options to see how pervasive this is. In
addition to speculation, there is one more "conflict" of this kind:
spark.locality.wait
spark.locality.wait.node
spark.locality.wait.process
spark.locality.wait.rack
spark.speculation
spark.speculation.interval
spark.speculation.multiplier
spark.speculation.quantile
+1
That's a perm gen issue - you need to adjust the perm gem size. In sbt it
should've been set automatically, but I think for Maven, you need to set
the maven opts, which is documented in the build instructions.
+1
It seems to me fixing DAGScheduler to make it not recursive is the better
solution here, given the cost of checkpointing.
I'm not entirely sure, but two candidates are
the visit function in stageDependsOn
submitStage
While I echo Mark's sentiment, versioning has nothing to do with this
problem. It has been the case even in Spark 0.8.0.
Note that mapSideCombine is turned off for groupByKey, so there is no need
to merge any combiners.
It is possible that you have generated the assembly jar using one version
of Hadoop, and then another assembly jar with another version. Those tests
that failed are all using a local cluster that sets up multiple processes,
which would require launching Spark worker processes using the assembly
jar. If that's indeed the problem, removing the extra assembly jars should
fix them.
Do you mind pasting the whole stack trace for the NPE?
Thanks. That does go out of the scope of the Spark release. The EC2 script
starts instances and use some scripts to setup this version. For that to
work, we need to have a release first.
It was a transient thing. There's a script that we are using to
automatically fetch diffs from a PR and apply the diff against the git
repo. Patrick changed the way it works last week, and a regression there
was PRs are no longer closed automatically.
I believe he has fixed it. Patrick will also write an email about the
details of that script soon.
Is it safe if we interrupt the running thread during shutdown?
+1 for 1.0
The point of 1.0 is for us to self-enforce API compatibility in the context
of longer term support. If we continue down the 0.xx road, we will always
have excuse for breaking APIs. That said, a major focus of 0.9 and some of
the work that are happening for 1.0 (e.g. configuration, Java 8 closure
support, security) are for better API compatibility support in 1.x releases.
While not perfect, Spark as is is already more mature than many (ASF)
projects that are versioned 1.x, 2.x, or even 10.x. Software releases are
always a moving target. 1.0 doesn't mean it is "perfect" and "final". The
project will still evolve.
You can do
sbt/sbt assemble-deps
and then just run
sbt/sbt package
each time.
You can even do
sbt/sbt ~package
for automatic incremental compilation.
We can try it on dev, but I personally find the JIRA notifications pretty
spammy ... It will clutter the dev list, and make it harder to search for
useful information here.
I concur wholeheartedly ...
test
I don't think it does.
+1 on both
+1 (binding)
Actually I made a mistake by saying binding.
Just +1 here.
I added you to the dev list on jira for spark.
The perf difference between that and Kryo is pretty small according to
their own benchmark. However, if they can provide better compatibility than
Kryo, we should definitely give it a shot!
Would you like to do some testing?
Hi guys,
Want to bring to the table this issue to see what other members of the
community think and then we can codify it in the Spark coding style guide.
The topic is about declaring return types explicitly in public APIs.
In general I think we should favor explicit type declaration in public
APIs. However, I do think there are 3 cases we can avoid the public API
definition because in these 3 cases the types are self-evident & repetitive.
Case 1. toString
Case 2. A method returning a string or a val defining a string
def name = "abcd" // this is so obvious that it is a string
val name = "edfg" // this too
Case 3. The method or variable is invoking the constructor of a class and
return that immediately. For example:
val a = new SparkContext(...)
implicit def rddToAsyncRDDActions[T: ClassTag](rdd: RDD[T]) = new
AsyncRDDActions(rdd)
Thoughts?
Case 2  should probably be expanded to cover most primitive types.
+1 Christopher's suggestion.
Mridul,
How would that happen? Case 3 requires the method to be invoking the
constructor directly. It was implicit in my email, but the return type
should be the same as the class itself.
Yes, the case you brought up is not a matter of readability or style. If it
returns a different type, it should be declared (otherwise it is just
wrong).
Mridul,
Can you be more specific in the createFoo example?
def myFunc = createFoo
is disallowed in my guideline. It is invoking a function createFoo, not the
constructor of Foo.
FYI I submitted an ASF INFRA ticket on granting the AMPLab Jenkins
permission to use the github commit status API.
If that goes through, we can configure Jenkins to use the commit status API
without leaving comments on the pull requests.
https://issues.apache.org/jira/browse/INFRA-7367
We do include Chill by default. It's a good idea to update the doc to
include chill.
You put your quotes in the wrong place. See
https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools
I'm not sure what you mean by enterprise stash.
But PR is a concept unique to Github. There is no PR model in normal git or
the git ASF maintains.
Hi Deb,
I am not sure how you can create a pull request coming from stash. Maybe I
am not understanding this correctly, but there are only two official Spark
repositories:
1. Apache git
2. Github
Thanks for the suggestion. Just did it.
BlockManager is only responsible for in-memory/on-disk storage. It has
nothing to do with re-computation.
All the recomputation / retry code are done in the DAGScheduler. Note that
when a node crashes, due to lazy evaluation, there is no task that needs to
be re-run. Those tasks are re-run only when their outputs are needed for
another task/job.
Take a look at
https://cwiki.apache.org/confluence/display/SPARK/Spark+Internals
It's mostly stock CentOS installation with some scripts.
Actually we just ran a job with 70TB+ compressed data on 28 worker nodes -
I didn't count the size of the uncompressed data, but I am guessing it is
somewhere between 200TB to 700TB.
I'm not really at liberty to discuss details of the job. It involves some
expensive aggregated statistics, and took 10 hours to complete (mostly
bottlenecked by network & io).
Hi All,
I'm excited to announce a new module in Spark (SPARK-1251). After an
initial review we've merged this as Spark as an alpha component to be
included in Spark 1.0. This new component adds some exciting features,
including:
- schema-aware RDD programming via an experimental DSL
- native Parquet support
- support for executing SQL against RDDs
The pull request itself contains more information:
https://github.com/apache/spark/pull/146
You can also find the documentation for this new component here:
http://people.apache.org/~pwendell/catalyst-docs/sql-programming-guide.html
This contribution was lead by Michael Ambrust with work from several other
contributors who I'd like to highlight here: Yin Huai, Cheng Lian, Andre
Schumacher, Timothy Chen, Henry Cook, and Mark Hamstra.
- Reynold
There is no Java API yet.
Nick and Koert summarized it pretty well. Just to clarify and give some
concrete examples.
If you want to start with a specific vertex, and follow some path, it is
probably easier and faster to use some key values store or even MySQL or a
graph database.
If you want to count the average length of paths between all nodes, or if
you want to compute the pair wise shortest path for all vertices, GraphX
will likely be way faster.
Usually you can just run Spark in local mode on a single machine for most
dev/testing.
If you want to simulate a cluster locally using multiple Spark worker
processes, you can use the undocumented local cluster mode, e.g.
local-cluster[2,1,512]
this launches two worker processes, each with one core and 512m of ram.
Thanks for contributing!
I think often unless the feature is gigantic, you can send a pull request
directly for discussion. One rule of thumb in the Spark code base is that
we typically prefer readability over conciseness, and thus we tend to avoid
using too much Scala magic or operator overloading.
In this specific case, do you know if using - instead of reverse improve
performance? I personally find it slightly awkward to use underscore right
after negation ...
The tail change looks good to me.
For foldLeft, I agree with you that the old way is more readable (although
less idiomatic scala).
I added the config option to use the non-default serializer. However, at
the time, Kryo fails serializing pretty much any closures so that option
was never really used / recommended.
Since then the Scala ecosystem has developed, and some other projects are
starting to use Kryo to serialize more Scala data structures, so I wouldn't
be surprised if there is a way to work around this now. However, I don't
have enough time to look into it at this point. If you do, please do post
your findings. Thanks.
Kryo does generate code for serialization, so the CPU overhead is quite
lower than Java (which I think just uses reflection). As I understand, they
also have a new implementation that uses unsafe intrinsics, which should
lead to even higher performance.
The generated byte[] size was a lot smaller in Kryo, especially for arrays
of objects of the same type. It doesn't need to write the class name for
every object, and it also reduces the size of ints and such using zig zag
encoding.
I don't have numbers around anymore, but when I was benchmarking them, kryo
was substantially better than java. The only reason it is not on by default
is because it doesn't always work.
Good idea. I submitted a pull request for the doc update here:
https://github.com/apache/spark/pull/642
Thanks. Do you mind playing with chill-scala a little bit and see if it
actually works well for closures? One way to try is to hard code the
serializer to use Kryo with chill-scala, and then run through all the unit
tests.
If it works well, we can incorporate that in the next release (probably not
1.0, but after that).
Technically you only need to change the build file, and change part of a
line in SparkEnv so you don't have to break your oath :)
The main reason is that it doesn't always work (e.g. sometimes application
program has special serialization / externalization written already for
Java which don't work in Kryo).
Thanks for the experiments and analysis!
I think Michael already submitted a patch that avoids scanning all columns
for count(*) or count(1).
Thanks for pointing it out. We should update the website to fix the code.
val count = spark.parallelize(1 to NUM_SAMPLES).map { i =>
  val x = Math.random()
  val y = Math.random()
  if (x*x + y*y < 1) 1 else 0
}.reduce(_ + _)
println("Pi is roughly " + 4.0 * count / NUM_SAMPLES)
I didn't see the original message, but only a reply.
This was an optimization that reuses a triplet object in GraphX, and when
you do a collect directly on triplets, the same object is returned.
It has been fixed in Spark 1.0 here:
https://issues.apache.org/jira/browse/SPARK-1188
To work around in older version of Spark, you can add a copy step to it,
e.g.
graph.triplets.map(_.copy()).collect()
Yea unfortunately you need that as well. When 1.0 is released, you wouldn't
need to do that anymore.
BTW - you can also just check out the source code from github to build 1.0.
The current branch-1.0 branch is very already at release candidate status -
so it should be almost identical to the actual 1.0 release.
https://github.com/apache/spark/tree/branch-1.0
Yea unfortunately you need that as well. When 1.0 is released, you wouldn't
need to do that anymore.
BTW - you can also just check out the source code from github to build 1.0.
The current branch-1.0 branch is very already at release candidate status -
so it should be almost identical to the actual 1.0 release.
https://github.com/apache/spark/tree/branch-1.0
reduce always return a single element - maybe you are misunderstanding what
the reduce function in collections does.
You are probably looking for reduceByKey in that case.
"reduce" just reduces everything in the collection into a single element.
You can submit a pull request on the github mirror:
https://github.com/apache/spark
Thanks.
Thanks for sending this in.
The ASF list doesn't support html so the formatting of the code is a little
messed up. For those who want to see the code in clearly formatted text, go
to
http://apache-spark-developers-list.1001551.n3.nabble.com/Kryo-serialization-for-closures-a-workaround-tp6787.html
Would you like to submit a pull request to update it?
Also in the latest version HyperLogLog is serializable. That means we can
get rid of the SerializableHyperLogLog class. (and move to use
HyperLogLogPlus).
2.7 sounds good. I was actually waiting for 2.7 to come out to post a JIRA
(mainly for the serializable HyperLogLogPlus class).
Posted a question on streamlib user group about API compatibility:
https://groups.google.com/forum/#!topic/stream-lib-user/4VDeKcPiTJU
It is actually pretty simple. You will first need to fork Spark on github,
and then push your changes to it, and then follow:
https://help.github.com/articles/using-pull-requests
Take a look at this one: https://issues.apache.org/jira/browse/SPARK-1188
It was an optimization that added user inconvenience. We got rid of that
now in Spark 1.0.
Can you take a look at the latest Spark 1.0 docs and see if they are fixed?
https://github.com/apache/spark/tree/master/docs
Thanks.
I tried but didn't find where I could add you. You probably need Matei to
help out with this.
I think the main concern is this would require scanning the data twice, and
maybe the user should be aware of it ...
If you are interested in openstack/swift integration with Spark, please
drop me a line. We are looking into improving the integration.
Thanks.
Thanks for sending the update. Do you mind posting a link to the bug
reported in the lzf project here as well? Cheers.
Thanks for sending the update. Do you mind posting a link to the bug
reported in the lzf project here as well? Cheers.
I think you guys are / will be leading the effort on that :)
I think you guys are / will be leading the effort on that :)
It is here:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala
It is actually pluggable. You can implement new compression codecs and just
change the config variable to use those.
Hi Michael,
Unfortunately the Apache mailing list filters out attachments. That said,
you can usually just start by looking at the JIRA for Spark and find issues
tagged with the starter tag and work on them. You can submit pull requests
to the github repo or email the dev list for feedbacks on specific issues.
https://github.com/apache/spark
Mridul,
Can you comment a little bit more on this issue? We are running into the
same stack trace but not sure whether it is just different Spark versions
on each cluster (doesn't seem likely) or a bug in Spark.
Thanks.
IntelliJ parser/analyzer/compiler behaves differently from Scala compiler,
and sometimes lead to inconsistent behavior. This is one of the case.
In general while we use IntelliJ, we don't use it to build stuff. I
personally always build in command line with sbt or Maven.
Responded on the jira...
We should make sure we include the following two patches:
https://github.com/apache/spark/pull/1264
https://github.com/apache/spark/pull/1263
This isn't exactly about Spark itself, more about how an application on
YARN/Mesos can communicate with another one.
How about your application launch program just takes in a parameter (or env
variable or command line argument) for the IP address of your client
application, and just send updates? You basically just want to send
messages to report progress. You can do it with a lot of different ways,
such as Akka, custom REST API, Thrift ... I think any of them will do.
Hi Andrew,
The port stuff is great to have, but they are pretty big changes to the
core that are introducing new features and are not exactly fixing important
bugs. For this reason, it probably can't block a release (I'm not even sure
if it should go into a maintenance release where we fix critical bugs for
Spark core).
We should definitely include them for 1.1.0 though (~Aug).
I was actually talking to tgraves today at the summit about this.
Based on my understanding, the sizes we track and send (which is
unfortunately O(M*R) regardless of how we change the implementation --
whether we send via task or send via MapOutputTracker) is only used to
compute maxBytesInFlight so we can throttle the fetching speed to not
result in oom. Perhaps for very large shuffles, we don't need to send the
bytes for each block, and we can send whether they are zero or not (which
can be tracked via a compressed bitmap that can be tiny).
The other thing we do need is the location of blocks. This is actually just
O(n) because we just need to know where the map was run.
Yes it would be great to mention the JIRA ticket number on the pull
request. Thanks!
Yes, that number is likely == 0 in any real workload ...
Note that in my original proposal, I was suggesting we could track whether
block size = 0 using a compressed bitmap. That way we can still avoid
requests for zero-sized blocks.
Thanks for reporting this. I just fixed it.
This blog post probably clarifies a lot of things:
http://databricks.com/blog/2014/07/01/shark-spark-sql-hive-on-spark-and-the-future-of-sql-on-spark.html
Maybe it's time to create an advanced mode in the ui.
You can take a look at
https://github.com/apache/spark/blob/master/dev/run-tests
dev/mima
Also take a look at this:
https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals
Ian,
The LZFOutputStream's large byte buffer is sort of annoying. It is much
smaller if you use the Snappy one. The downside of the Snappy one is
slightly less compression (I've seen 10 - 20% larger sizes).
If we can find a compression scheme implementation that doesn't do very
large buffers, that'd be a good idea too ... let me know if you have any
suggestions.
In the future, we plan to make shuffle write to less number of streams at
the same time.
Hi Spark devs,
I was looking into the memory usage of shuffle and one annoying thing is
the default compression codec (LZF) is that the implementation we use
allocates buffers pretty generously. I did a simple experiment and found
that creating 1000 LZFOutputStream allocated 198976424 bytes (~190MB). If
we have a shuffle task that uses 10k reducers and 32 threads running
currently, the memory used by the lzf stream alone would be ~ 60GB.
In comparison, Snappy only allocates ~ 65MB for every
1k SnappyOutputStream. However, Snappy's compression is slightly lower than
LZF's. In my experience, it leads to 10 - 20% increase in size. Compression
ratio does matter here because we are sending data across the network.
In future releases we will likely change the shuffle implementation to open
less streams. Until that happens, I'm looking for compression codec
implementations that are fast, allocate small buffers, and have decent
compression ratio.
Does anybody on this list have any suggestions? If not, I will submit a
patch for 1.1 that replaces LZF with Snappy for the default compression
codec to lower memory usage.
allocation data here: https://gist.github.com/rxin/ad7217ea60e3fb36c567
Copying Jon here since he worked on the lzf library at Ning.
Jon - any comments on this topic?
FYI dev,
I submitted a PR making Snappy as the default compression codec:
https://github.com/apache/spark/pull/1415
Also submitted a separate PR to add lz4 support:
https://github.com/apache/spark/pull/1416
Hi Spark devs,
Want to give you guys a heads up that I'm working on a small (but major)
change with respect to how task dispatching works. Currently (as of Spark
1.0.1), Spark sends RDD object and closures using Akka along with the task
itself to the executors. This is however inefficient because all tasks in
the same stage use the same RDDs and closures, but we have to send these
closures and RDDs multiple times to the executors. This is especially bad
when some closure references some variable that is very large. The current
design led to users having to explicitly broadcast large variables.
The patch uses broadcast to send RDD objects and the closures to executors,
and use Akka to only send a reference to the broadcast RDD/closure along
with the partition specific information for the task. For those of you who
know more about the internals, Spark already relies on broadcast to send
the Hadoop JobConf every time it uses the Hadoop input, because the JobConf
is large.
The user-facing impact of the change include:
1. Users won't need to decide what to broadcast anymore
2. Task size will get smaller, resulting in faster scheduling and higher
task dispatch throughput.
In addition, the change will simplify some internals of Spark, removing the
need to maintain task caches and the complex logic to broadcast JobConf
(which also led to a deadlock recently).
Pull request attached: https://github.com/apache/spark/pull/1450
Oops - the pull request should be https://github.com/apache/spark/pull/1452
Yup - that is correct.  Thanks for clarifying.
+1
Yes.
Thanks :)
FYI the pull request has been merged and will be part of Spark 1.1.0.
Thanks for the thoughtful email, Neil and Christopher.
If I understand this correctly, it seems like the dynamic variable is just
a variant of the accumulator (a static one since it is a global object).
Accumulators are already implemented using thread-local variables under the
hood. Am I misunderstanding something?
If the purpose is for dropping csv headers, perhaps we don't really need a
common drop and only one that drops the first line in a file? I'd really
try hard to avoid a common drop/dropWhile because they can be expensive to
do.
Note that I think we will be adding this functionality (ignoring headers)
to the CsvRDD functionality in Spark SQL.
 https://github.com/apache/spark/pull/1351
Yes, that could work. But it is not as simple as just a binary flag.
We might want to skip the first row for every file, or the header only for
the first file. The former is not really supported out of the box by the
input format I think?
Actually reflection is probably a better, lighter weight process for this.
An extra project brings more overhead for something simple.
There is one piece of information that'd be useful to know, which is the
source of the input. Even in the presence of an IOException, the input
metrics still specifies the task is reading from Hadoop.
However, I'm slightly confused by this -- I think usually we'd want to
report the number of bytes read, rather than the total input size. For
example, if there is a limit (only read the first 5 records), the actual
number of bytes read is much smaller than the total split size.
Kay, am I mis-interpreting this?
That makes sense, Sandy.
When you add the patch, can you make sure you comment inline on why the
fallback is needed?
To run through all the tests you'd need to create the assembly jar first.
I've seen this asked a few times. Maybe we should make it more obvious.
http://spark.apache.org/docs/latest/building-with-maven.html
Spark Tests in Maven
Tests are run by default via the ScalaTest Maven plugin
.
Some of the require Spark to be packaged first, so always run mvn package
 with -DskipTests the first time. You can then run the tests with mvn
-Dhadoop.version=... test.
The ScalaTest plugin also supports running only a specific test suite as
follows:
mvn -Dhadoop.version=... -DwildcardSuites=org.apache.spark.repl.ReplSuite test
Would you like to submit a pull request? All doc source code are in the
docs folder. Cheers.
You can use publish-local in sbt.
If you want to be more careful, you can give Spark a different version
number and use that version number in your app.
Hi devs,
I don't know if this is going to help, but if you can watch & vote on the
ticket, it might help ASF INFRA prioritize and triage it faster:
https://issues.apache.org/jira/browse/INFRA-8116
Please do. Thanks!
+1 on this.
Thanks for your interest.
I think the main challenge is if we have to call Python functions per
record, it can be pretty expensive to serialize/deserialize across
boundaries of the Python process and JVM process.  I don't know if there is
a good way to solve this problem yet.
I'm pretty sure it is an oversight. Would you like to submit a pull request
to fix that?
Thanks. Those are definitely great problems to fix!
Yes it is. I actually commented on it:
https://github.com/apache/spark/pull/1792/files#r15840899
ScalaTest actually has support for parallelization built-in. We can use
that.
The main challenge is to make sure all the test suites can work in parallel
when running along side each other.
Nick,
Would you like to file a ticket to track this?
I think the first baby step is to log the amount of time each test cases
take. This is supposed to happen already (see the flag), but somehow the
time are not showing. If you have some time to figure that out, that'd be
great.
https://github.com/apache/spark/blob/master/project/SparkBuild.scala#L350
Pasting a better formatted trace:
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at
scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:137)
at
scala.collection.mutable.HashMap$$anonfun$writeObject$1.apply(HashMap.scala:135)
at
scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
at
scala.collection.mutable.HashTable$class.serializeTo(HashTable.scala:124)
at scala.collection.mutable.HashMap.serializeTo(HashMap.scala:39)
at scala.collection.mutable.HashMap.writeObject(HashMap.scala:135)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606) at
 java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1495)
at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
at org.apache.spark.util.Utils$.serialize(Utils.scala:64)
at
org.apache.spark.broadcast.TorrentBroadcast$.blockifyObject(TorrentBroadcast.scala:232)
at
org.apache.spark.broadcast.TorrentBroadcast.sendBroadcast(TorrentBroadcast.scala:85)
at
org.apache.spark.broadcast.TorrentBroadcast.(TorrentBroadcast.scala:66)
at
org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:36)
at
org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:29)
at
 org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)
at org.apache.spark.SparkContext.broadcast(SparkContext.scala:809)
Looks like you didn't actually paste the exception message. Do you mind
doing that?
Yes, I'm pretty sure it doesn't actually use the right serializer in
TorrentBroadcast:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala#L232
And TorrentBroadcast is turned on by default for 1.1 right now. Do you want
to submit a pull request to fix that? This would be a critical fix for 1.1
that's worth doing.
I created a JIRA ticket to track this:
https://issues.apache.org/jira/browse/SPARK-2928
Let me know if you need help with it.
Actually apparently there is a pull request for it. Thanks for reporting!
https://github.com/apache/spark/pull/1836
They only compared their own implementations of couple algorithms on
different platforms rather than comparing the different platforms
themselves (in the case of Spark -- PySpark). I can write two variants of
an algorithm on Spark and make them perform drastically differently.
I have no doubt if you implement a ML algorithm in Python itself without
any native libraries, the performance will be sub-optimal.
What PySpark really provides is:
- Using Spark transformations in Python
- ML algorithms implemented in Scala (leveraging native numerical libraries
for high performance), and callable in Python
The paper claims "Python is now one of the most popular languages for
ML-oriented programming", and that's why they went ahead with Python.
However, as I understand, very few people actually implement algorithms in
Python directly because of the sub-optimal performance. Most people
implement algorithms in other languages (e.g. C / Java), and expose APIs in
Python for ease-of-use. This is what we are trying to do with PySpark as
well.
Actually I believe the same person started both projects.
The Distributed R project from HP was started by Shivaram Venkataraman when
he was there. He since moved to Berkeley AMPLab to pursue a PhD and SparkR
was his latest project.
BTW you can find the original Presto (rebranded as Distributed R) paper
here:
http://eurosys2013.tudos.org/wp-content/uploads/2013/paper/Venkataraman.pdf
I haven't read the code yet, but if it is what I think it is, this is
SUPER, UBER, HUGELY useful.
On a related note, I asked about this on the Scala dev list but never got a
satisfactory answer ....
https://groups.google.com/forum/#!msg/scala-internals/_cZ1pK7q6cU/xyBQA0DdcYwJ
Hi devs,
I posted a design doc proposing an interface for pluggable block transfer
(used in shuffle, broadcast, block replication, etc). This is expected to
be done in 1.2 time frame.
It should make our code base cleaner, and enable us to provide alternative
implementations of block transfers (e.g. via the new Netty module that I'm
working on, or possibly via MapR file system).
https://issues.apache.org/jira/browse/SPARK-3019
Please take a look and comment on the JIRA ticket. Thanks.
I believe docs changes can go in anytime (because we can just publish new
versions of docs).
Critical bug fixes can still go in too.
Hi Rajendran,
I'm assuming you have some concept of schema and you are intending to
integrate with SchemaRDD instead of normal RDDs.
More responses inline below.
Linking to the JIRA tracking APIs to hook into the planner:
https://issues.apache.org/jira/browse/SPARK-3248
Thanks for doing this, Shane.
Welcome, Shane!
Having a SSD help tremendously with assembly time.
Without that, you can do the following in order for Spark to pick up the
compiled classes before assembly at runtime.
export SPARK_PREPEND_CLASSES=true
+1
Tested locally on Mac OS X with local-cluster mode.
that would require github hooks permission and unfortunately asf infra
wouldn't allow that.
Maybe they will change their mind one day, but so far we asked about this
and the answer has been no for security reasons.
Can you be a little bit more specific, maybe give a code snippet?
I don't think so. We should probably add a line to log it.
I didn't know about that ....
It is in the dag scheduler. Look for broadcast.
Thanks for the email, Erik.
The Scala collection library implementation is a complicated beast ...
Xiangrui can comment more, but I believe Joseph and him are actually
working on standardize interface and pipeline feature for 1.2 release.
Hi Egor,
Thanks for the suggestion. It is definitely our intention and practice to
post design docs as soon as they are ready, and short iteration cycles. As
a matter of fact, we encourage design docs for major features posted before
implementation starts, and WIP pull requests before they are fully baked
for large features.
That said, no, not 100% of a committer's time is on a specific ticket.
There are lots of tickets that are open for a long time before somebody
starts actively working on it. So no, it is not true that "all this time
was active development". Xiangrui should post the design doc as soon as it
is ready for feedback.
I'm not familiar with Infiniband, but I can chime in on the Spark part.
There are two kinds of communications in Spark: control plane and data
plane.  Task scheduling / dispatching is control, whereas fetching a block
(e.g. shuffle) is data.
Hi Egor,
I think the design doc for the pipeline feature has been posted.
For the workflow, I believe Oozie actually works fine with Spark if you
want some external workflow system. Do you have any trouble using that?
This is during shutdown right? Looks ok to me since connections are being
closed. We could've handle this more gracefully, but the logs look
harmless.
There might've been some misunderstanding. I was referring to the MLlib
pipeline design doc when I said the design doc was posted, in response to
the first paragraph of your original email.
BTW - a partial solution here: https://github.com/apache/spark/pull/2470
This doesn't address the 0 size block problem yet, but makes my large job
on hundreds of terabytes of data much more reliable.
It seems like you just need to raise the ulimit?
Hi Spark users and developers,
Some of the most active Spark developers (including Matei Zaharia, Michael
Armbrust, Joseph Bradley, TD, Paco Nathan, and me) will be in NYC for
Strata NYC. We are working with the Spark NYC meetup group and Bloomberg to
host a meetup event. This might be the event with the highest committer to
user ratio in the history of user meetups. Look forward to meeting more
users in NYC.
You can sign up for that here:
http://www.meetup.com/Spark-NYC/events/209271842/
Cheers.
Thanks. We might see more failures due to contention on resources. Fingers
acrossed ... At some point it might make sense to run the tests in a VM or
container.
Thanks. I added one.
I also ran into this earlier. It is a bug. Do you want to file a jira?
I think part of the problem is that we don't actually have the attempt id
on the executors. If we do, that's great. If not, we'd need to propagate
that over.
Yes, as I understand it this is for (2).
Imagine a use case in which I want to save some output. In order to make
this atomic, the program uses part_[index]_[attempt].dat, and once it
finishes writing, it renames this to part_[index].dat.
Right now [attempt] is just the TID, which could show up like (assuming
this is not the first stage):
part_0_1000
part_1_1001
part_0_1002 (some retry)
...
This is fairly confusing. The natural thing to expect is
part_0_0
part_1_0
part_0_1
...
Hi all,
We are excited to announce that the benchmark entry has been reviewed by
the Sort Benchmark committee and Spark has officially won the Daytona
GraySort contest in sorting 100TB of data.
Our entry tied with a UCSD research team building high performance systems
and we jointly set a new world record. This is an important milestone for
the project, as it validates the amount of engineering work put into Spark
by the community.
As Matei said, "For an engine to scale from these multi-hour petabyte batch
jobs down to 100-millisecond streaming and interactive queries is quite
uncommon, and it's thanks to all of you folks that we are able to make this
happen."
Updated blog post:
http://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html
Greg,
Thanks a lot for commenting on this, but I feel we are splitting hairs
here. Matei did mention -1, followed by "or give feedback". The original
process outlined by Matei was exactly about review, rather than fighting.
Nobody wants to spend their energy fighting.  Everybody is doing it to
improve the project.
In particular, quoting you in your email
"Be careful here. "Responsibility" is pretty much a taboo word. All of
Apache is a group of volunteers. People can disappear at any point, which
is why you need multiple (as my fellow Director warned, on your private
list). And multiple people can disappear."
Take a look at this page: http://www.apache.org/dev/pmc.html
This Project Management Committee Guide outlines the general
***responsibilities*** of PMC members in managing their projects.
Are you suggesting the wording used by the PMC guideline itself is taboo?
Technically you can already do custom serializer for each shuffle operation
(it is part of the ShuffledRDD). I've seen Matei suggesting on jira issues
(or github) in the past a "storage policy" in which you can specify how
data should be stored. I think that would be a great API to have in the
long run. Designing it won't be trivial though.
This is great. I think the consensus from last time was that we would put
performance stuff into spark-perf, so it is easy to test different Spark
versions.
Do people usually important o.a.spark.rdd._ ?
Also in order to maintain source and binary compatibility, we would need to
keep both right?
That seems like a great idea. Can you submit a pull request?
The current design is not ideal, but the size of dependencies should be
fairly small since we only send the path and timestamp, not the jars
themselves.
Executors can come and go. This is essentially a state replication problem
that you gotta be very careful with consistency.
That's a great idea and it is also a pain point for some users. However, it
is not possible to solve this problem at compile time, because the content
of serialization can only be determined at runtime.
There are some efforts in Scala to help users avoid mistakes like this. One
example project that is more researchy is Spore:
http://docs.scala-lang.org/sips/pending/spores.html
I don't think the code is immediately obvious.
Davies - I think you added the code, and Josh reviewed it. Can you guys
explain and maybe submit a patch to add more documentation on the whole
thing?
Thanks.
This basically stops us from merging patches. I'm wondering if it is
possible for ASF to give some Spark committers write permission to github
repo. In that case, if the sync tool is down, we can manually push
periodically.
Can you elaborate? Not 100% sure if I understand what you mean.
The 1st was referring to different Spark applications connecting to the
standalone cluster manager, and the 2nd one was referring to within a
single Spark application, the jobs can be scheduled using a fair scheduler.
Krishna,
Docs don't block the rc voting because docs can be updated in parallel with
release candidates, until the point a release is made.
Oops my previous response wasn't sent properly to the dev list. Here you go
for archiving.
Yes you can. Scala classes are compiled down to classes in bytecode. Take a
look at this: https://twitter.github.io/scala_school/java.html
Note that questions like this are not exactly what this dev list is meant
for  ...
This would be plausible for specific purposes such as Spark streaming or
Spark SQL, but I don't think it is doable for general Spark driver since it
is just a normal JVM process with arbitrary program state.
+1
Tested on OS X.
I don't think the lineage thing is even turned on in Tachyon - it was
mostly a research prototype, so I don't think it'd make sense for us to use
that.
Actually HY emailed me offline about this and this is supported in the
latest version of Tachyon. It is a hard problem to push this into storage;
need to think about how to handle isolation, resource allocation, etc.
https://github.com/amplab/tachyon/blob/master/core/src/main/java/tachyon/master/Dependency.java
Hi Manoj,
Thanks for the email.
Yes - you should start with the starter task before attempting larger ones.
Last year I signed up as a mentor for GSoC, but no student signed up. I
don't think I'd have time to be a mentor this year, but others might.
I'm in the middle of revamping the SchemaRDD public API and in 1.3, we will
have a public, documented version of the expression library. The Catalyst
expression library will remain hidden.
You can track it with this ticket:
https://issues.apache.org/jira/browse/SPARK-5097
Depending on your use cases. If the use case is to extract small amount of
data out of teradata, then you can use the JdbcRDD and soon a jdbc input
source based on the new Spark SQL external data source API.
Depends on what the other side is doing. You can create your own RDD
implementation by subclassing RDD, or it might work if you use
sc.parallelize(1 to n, n).mapPartitionsWithIndex( /* code to read the data
and return an iterator */ ) where n is the number of partitions.
If it is a small collection of them on the driver, you can just use
sc.parallelize to create an RDD.
It's a bunch of strategies defined here:
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala
In most common use cases (e.g. inner equi join), filters are pushed below
the join or into the join. Doing a cartesian product followed by a filter
is too expensive.
The static fields - Scala can't express JVM static fields unfortunately.
Those will be important once we provide the Java API.
You are running on a local file system right? HDFS orders the file based on
names, but local file system often don't. I think that's why the difference.
We might be able to do a sort and order the partitions when we create a RDD
to make this universal though.
Maybe just to avoid LGTM as a single token when it is not actually
according to Patrick's definition, but anybody can still leave comments
like:
"The direction of the PR looks good to me." or "+1 on the direction"
"The build part looks good to me"
...
We will merge https://issues.apache.org/jira/browse/SPARK-3650  for 1.3.
Thanks for reminding!
Hi Ewan,
Not sure if there is a JIRA ticket (there are too many that I lose track).
I chatted briefly with Aaron on this. The way we can solve it is to create
a new FileSystem implementation that overrides the listStatus method, and
then in Hadoop Conf set the fs.file.impl to that.
Shouldn't be too hard. Would you be interested in working on it?
Definitely go for a pull request!
It will probably eventually make its way into part of the query engine, one
way or another. Note that there are in general a lot of other lower hanging
fruits before you have to do vectorization.
As far as I know, Hive doesn't really have vectorization because the
vectorization in Hive is simply writing everything in small batches, in
order to avoid the virtual function call overhead, and hoping the JVM can
unroll some of the loops. There is no SIMD involved.
Something that is pretty useful, which isn't exactly from vectorization but
comes from similar lines of research, is being able to push predicates down
into the columnar compression encoding. For example, one can turn string
comparisons into integer comparisons. These will probably give much larger
performance improvements in common queries.
I don't know if there is a list, but in general running performance
profiler can identify a lot of things...
You don't need the LocalSparkContext. It is only for Spark's own unit test.
You can just create a SparkContext and use it in your unit tests, e.g.
val sc = new SparkContext("local", "my test app", new SparkConf)
Hi,
We are considering renaming SchemaRDD -> DataFrame in 1.3, and wanted to
get the community's opinion.
The context is that SchemaRDD is becoming a common data format used for
bringing data into Spark from external systems, and used for various
components of Spark, e.g. MLlib's new pipeline API. We also expect more and
more users to be programming directly against SchemaRDD API rather than the
core RDD API. SchemaRDD, through its less commonly used DSL originally
designed for writing test cases, always has the data-frame like API. In
1.3, we are redesigning the API to make the API usable for end users.
There are two motivations for the renaming:
1. DataFrame seems to be a more self-evident name than SchemaRDD.
2. SchemaRDD/DataFrame is actually not going to be an RDD anymore (even
though it would contain some RDD functions like map, flatMap, etc), and
calling it Schema*RDD* while it is not an RDD is highly confusing. Instead.
DataFrame.rdd will return the underlying RDD for all RDD methods.
My understanding is that very few users program directly against the
SchemaRDD API at the moment, because they are not well documented. However,
oo maintain backward compatibility, we can create a type alias DataFrame
that is still named SchemaRDD. This will maintain source compatibility for
Scala. That said, we will have to update all existing materials to use
DataFrame rather than SchemaRDD.
Hi all,
In Spark, we have done reasonable well historically in interface and API
design, especially compared with some other Big Data systems. However, we
have also made mistakes along the way. I want to share a talk I gave about
interface design at Databricks' internal retreat.
https://speakerdeck.com/rxin/interface-design-for-spark-community
Interface design is a vital part of Spark becoming a long-term sustainable,
thriving framework. Good interfaces can be the project's biggest asset,
while bad interfaces can be the worst technical debt. As the project scales
bigger and bigger, the community is expanding and we are getting a wider
range of contributors that have not thought about this as their everyday
development experience outside Spark.
It is part-art part-science and in some sense acquired taste. However, I
think there are common issues that can be spotted easily, and common
principles that can address a lot of the low hanging fruits. Through this
presentation, I hope to bring to everybody's attention the issue of
interface design and encourage everybody to think hard about interface
design in their contributions.
Thanks, Andrew. That's great material.
Dirceu,
That is not possible because one cannot overload return types.
SQLContext.parquetFile (and many other methods) needs to return some type,
and that type cannot be both SchemaRDD and DataFrame.
In 1.3, we will create a type alias for DataFrame called SchemaRDD to not
break source compatibility for Scala.
Koert,
As Mark said, I have already refactored the API so that nothing is catalyst
is exposed (and users won't need them anyway). Data types, Row interfaces
are both outside catalyst package and in org.apache.spark.sql.
+1
Tested on Mac OS X
Alright I have merged the patch ( https://github.com/apache/spark/pull/4173
) since I don't see any strong opinions against it (as a matter of fact
most were for it). We can still change it if somebody lays out a strong
argument.
It's an interesting idea, but there are major challenges with per row
schema.
1. Performance - query optimizer and execution use assumptions about schema
and data to generate optimized query plans. Having to re-reason about
schema for each row can substantially slow down the engine, but due to
optimization and due to the overhead of schema information associated with
each row.
2. Data model: per-row schema is fundamentally a different data model. The
current relational model has gone through 40 years of research and have
very well defined semantics. I don't think there are well defined semantics
of a per-row schema data model. For example, what is the semantics of an
UDF function that is operating on a data cell that has incompatible schema?
Should we also coerce or convert the data type? If yes, will that lead to
conflicting semantics with some other rules? We need to answer questions
like this in order to have a robust data model.
Hopefully problems like this will go away entirely in the next couple of
releases. https://issues.apache.org/jira/browse/SPARK-5293
It shouldn't change the data source api at all because data sources create
RDD[Row], and that gets converted into a DataFrame automatically
(previously to SchemaRDD).
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala
One thing that will break the data source API in 1.3 is the location of
types. Types were previously defined in sql.catalyst.types, and now moved
to sql.types. After 1.3, sql.catalyst is hidden from users, and all public
APIs have first class classes/objects defined in sql directly.
Isn't that just "null" in SQL?
Thanks for doing that, Shane!
Once the data frame API is released for 1.3, you can write your thing in
Python and get the same performance. It can't express everything, but for
basic things like projection, filter, join, aggregate and simple numeric
computation, it should work pretty well.
It is something like this: https://issues.apache.org/jira/browse/SPARK-5097
On the master branch, we have a Pandas like API already.
It's bad naming - JsonRDD is actually not an RDD. It is just a set of util
methods.
The case sensitivity issues seem orthogonal, and would be great to be able
to control that with a flag.
We can use ScalaTest's privateMethodTester also instead of exposing that.
cc dev list
How are you saving the data? There are two relevant 2GB limits:
1. Caching
2. Shuffle
For caching, a partition is turned into a single block.
For shuffle, each map partition is partitioned into R blocks, where R =
number of reduce tasks. It is unlikely a shuffle block > 2G, although it
can still happen.
I think the 2nd problem is easier to fix than the 1st, because we can
handle that in the network transport layer. It'd require us to divide the
transfer of a very large block into multiple smaller blocks.
Haven't sync-ed anything for the last 4 hours. Seems like this little piece
of infrastructure always stops working around our own code freeze time ...
I filed an INFRA ticket: https://issues.apache.org/jira/browse/INFRA-9115
I wish ASF can reconsider requests like this in order to handle downtime
gracefully https://issues.apache.org/jira/browse/INFRA-8738
We should update the style doc to reflect what we have in most places
(which I think is //).
We thought about this today after seeing this email. I actually built a
patch for this (adding filter/column to data source stat estimation), but
ultimately dropped it due to the potential problems the change the cause.
The main problem I see is that column pruning/predicate pushdowns are
advisory, i.e. the data source might or might not apply those filters.
Without significantly complicating the data source API, it is hard for the
optimizer (and future cardinality estimation) to know whether the
filter/column pushdowns are advisory, and whether to incorporate that in
cardinality estimation.
Imagine this scenario: a data source applies a filter and estimates the
filter's selectivity is 0.1, then the data set is reduced to 10% of the
size. Catalyst's own cardinality estimation estimates the filter
selectivity to 0.1 again, and thus the estimated data size is now 1% of the
original data size, lowering than some threshold. Catalyst decides to
broadcast the table. The actual table size is actually 10x the size.
This is the original ticket:
https://issues.apache.org/jira/browse/SPARK-1442
I believe it will happen, one way or another :)
Why don't we just pick // as the default (by encouraging it in the style
guide), since it is mostly used, and then do not disallow /* */? I don't
think it is that big of a deal to have slightly deviations here since it is
dead simple to understand what's going on.
Koert,
Don't get too hang up on the name SQL. This is exactly what you want: a
collection with record-like objects with field names and runtime types.
Almost all of the 40 methods are transformations for structured data, such
as aggregation on a field, or filtering on a field. If all you have is the
old RDD style map/flatMap, then any transformation would lose the schema
information, making the extra schema information useless.
It's a good point. I will update the documentation to say that this is not
meant to be subclassed externally.
Unfortunately this is not to happen for 1.3 (as a snapshot release is
already cut). We need to figure out how we are going to do cardinality
estimation before implementing this. If we need to do this in the future, I
think we can do it in a way that doesn't break existing APIs. Given I think
this won't bring much benefit right now (the only use for it is broadcast
joins), I think it is ok to push this till later.
The issue I asked still stands. What should the optimizer do w.r.t. filters
that are pushed into the data source? Should it ignore those filters, or
apply statistics again?
This also depends on how we want to do statistics. Hive (and a lot of other
database systems) does a scan to figure out statistics, and put all of
those statistics in a catalog. That is a more unified way to solve the
stats problem.
That said, in the world of federated databases, I can see why we might want
to push cardinality estimation to the data sources, since if the use case
is selecting a very small subset of the data from the sources, then it
might be hard for the statistics to be accurate in the catalog built from
data scan.
It seems to me having a version that is 2+ is good for that? Once we move
to 2.0, we can retag those that are not going to be fixed in 2.0 as 2.0.1
or 2.1.0 .
Evan articulated it well.
Can you use the new aggregateNeighbors method? I suspect the null is coming
from "automatic join elimination", which detects bytecode to see if you
need the src or dst vertex data. Occasionally it can fail to detect. In the
new aggregateNeighbors API, the caller needs to explicitly specifying that,
making it more robust.
Then maybe you actually had a null in your vertex attribute?
Spark SQL is not the same as Hive on Spark.
Spark SQL is a query engine that is designed from ground up for Spark
without the historic baggage of Hive. It also does more than SQL now -- it
is meant for structured data processing (e.g. the new DataFrame API) and
SQL. Spark SQL is mostly compatible with Hive, but 100% compatibility is
not a goal (nor desired, since Hive has a lot of weird SQL semantics in the
course of its evolution).
Hive on Spark is meant to replace Hive's MapReduce runtime with Spark's.
For more information, see this blog post:
https://databricks.com/blog/2014/07/01/shark-spark-sql-hive-on-spark-and-the-future-of-sql-on-spark.html
Most likely no. We are using the embedded mode of Jetty, rather than using
servlets.
Even if it is possible, you probably wouldn't want to embed Spark in your
application server ...
Mostly UI.
However, we are also using Jetty as a file server I believe (for
distributing files from the driver to workers).
Michael - it is already transient. This should probably considered a bug in
the scala compiler, but we can easily work around it by removing the use of
destructuring binding.
I submitted a patch
https://github.com/apache/spark/pull/4628
Hi all,
The Hadoop Summit uses community choice voting to decide which talks to
feature. It would be great if the community could help vote for Spark talks
so that Spark has a good showing at this event. You can make three votes on
each track. Below I've listed 3 talks that are important to Spark's
roadmap. Please give 3 votes to each of the following talks.
Committer Track: Lessons from Running Ultra Large Scale Spark Workloads on
Hadoop
https://hadoopsummit.uservoice.com/forums/283260-committer-track/suggestions/7074016
Data Science track: DataFrames: large-scale data science on Hadoop data
with Spark
https://hadoopsummit.uservoice.com/forums/283261-data-science-and-hadoop/suggestions/7074147
Future of Hadoop track: Online Approximate OLAP in SparkSQL
https://hadoopsummit.uservoice.com/forums/283266-the-future-of-apache-hadoop/suggestions/7074424
Thanks!
Thanks for the email and encouragement, Devl. Responses to the 3 requests:
-tonnes of configuration properties and "go faster" type flags. For example
Hadoop and Hbase users will know that there are a whole catalogue of
properties for regions, caches, network properties, block sizes, etc etc.
Please don't end up here for example:
https://hadoop.apache.org/docs/r1.0.4/mapred-default.html, it is painful
having to configure all of this and then create a set of properties for
each environment and then tie this into CI and deployment tools.
As the project grows, it is unavoidable to introduce more config options,
in particular, we often use config options to test new modules that are
still experimental before making them the default (e.g. sort-based shuffle).
The philosophy here is to make it a very high bar to introduce new config
options, and make the default values sensible for most deployments, and
then whenever possible, figure out automatically what is the right setting.
Note that this in general is hard, but we expect for 99% of the users they
only need to know a very small number of options (e.g. setting the
serializer).
-no more daemons and processes to have to monitor and manipulate and
restart and crash.
At the very least you'd need the cluster manager itself to be a daemon
process because we can't defy the law of physics. But I don't think we want
to introduce anything beyond that.
-a project that penalises developers (that will ultimately help promote
Spark to their managers and budget holders) with expensive training,
certification, books and accreditation. Ideally this open source should be
free, free training= more users = more commercial uptake.
I definitely agree with you on making it easier to learn Spark. We are
making a lot of materials freely available, including two freely available
MOOCs on edX:
https://databricks.com/blog/2014/12/02/announcing-two-spark-based-moocs.html
We reached a new milestone today.
https://github.com/apache/spark
10,001 commits now. Congratulations to Xiangrui for making the 10000th
commit!
Hi Ewan,
Sorry it took a while for us to reply. I don't know spark-perf that well,
but I think this would be problematic if it works with only a specific
version of Hadoop. Maybe we can take a different approach -- just have a
bunch of tasks using the HDFS client API to read data, and not relying on
input formats?
Thanks for chiming in, John. I missed your meetup last night - do you have
any writeups or slides about roofline design? In particular, I'm curious
about what optimizations are available for power-law dense * sparse? (I
don't have any background in optimizations)
How did you run the Spark command? Maybe the memory setting didn't actually
apply? How much memory does the web ui say is available?
BTW - I don't think any JVM can actually handle 700G heap ... (maybe Zing).
This is an interesting idea.
Are there well known libraries for doing this? Config is the one place
where it would be great to have something ridiculously simple, so it is
more or less bug free. I'm concerned about the complexity in this patch and
subtle bugs that it might introduce to config options that users will have
no workarounds. Also I believe it is fairly hard for nice error messages to
propagate when using Scala's parser combinator.
It would be great to add a timeout. Do you mind submitting a pull request?
Tom - sorry for the delay. If you try OpenJDK (on a smaller heap), do you
see the same problem? Would be great to isolate whether the problem is
related to J9 or not. In either case we should fix it though.
If scaladoc can show the Java enum types, I do think the best way is then
just Java enum types.
I created a ticket to separate the API refactoring from the implementation.
Would be great to have these as two separate patches to make it easier to
review (similar to the way we are doing RPC refactoring -- first
introducing an internal RPC api, port akka to it, and then add an
alternative implementation).
https://issues.apache.org/jira/browse/SPARK-6479
Can you upload your design doc there so we can discuss the block store api?
Thanks.
In particular:
http://spark.apache.org/docs/latest/sql-programming-guide.html
"Additionally, the implicit conversions now only augment RDDs that are
composed of Products (i.e., case classes or tuples) with a method toDF,
instead of applying automatically."
Igor,
Welcome -- everything is open here:
https://issues.apache.org/jira/browse/SPARK
You should be able to see them even if you are not an ASF member.
Python is tough if you need to change Scala at the same time.
sbt/sbt assembly/assembly
can be slightly faster than just assembly.
I think the worry here is that people often use count() to force execution,
and when coupled with transformations with side-effect, it is no longer
safe to not run it.
However, maybe we can add a new lazy val .size that doesn't require
recomputation.
Reviving this to see if others would like to chime in about this
"expression language" for config options.
Yup - we merged the Java and Scala API so there is now a single set of API
to support both languages.
See more at
http://spark.apache.org/docs/latest/sql-programming-guide.html#unification-of-the-java-and-scala-apis
Hi Spark devs,
I've spent the last few months investigating the feasibility of
re-architecting Spark for mobile platforms, considering the growing
population of Android/iOS users. I'm happy to share with you my findings at
https://issues.apache.org/jira/browse/SPARK-6646
The tl;dr is that we should support running Spark on Android/iOS, and the
best way to do this at the moment is to use Scala.js to compile Spark code
into JavaScript, and then run it in Safari or Chrome (and even node.js
potentially for servers).
If you are on your phones right now and prefer reading a blog post rather
than a PDF file, you can read more about the design doc at
https://databricks.com/blog/2015/04/01/spark-2-rearchitecting-spark-for-mobile.html
This is done in collaboration with TD, Xiangrui, Patrick. Look forward to
your feedback!
You totally can.
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala#L792
There is also an attempt at adding stddev here already:
https://github.com/apache/spark/pull/5228
+1
Tested some DataFrame functions locally on Mac OS X.
Adding Jianping Wang to the thread, since he contributed the SVDPlusPlus
implementaiton.
Jianping,
Can you take a look at this message? Thanks.
+1 too
There is some work to create an off-heap storage API for Spark. I think
with it, it will be easier to support different storage backends.
https://issues.apache.org/jira/browse/SPARK-6479
With that API in place, rest of the integration should probably just live
outside of Spark in Ignite or as a 3rd party package.
Welcome, Dmitriy, to the Spark dev list!
+1
I closed it. Thanks.
It is loaded by Hive's HiveConf, which simply searches for hive-site.xml on
the classpath.
FYI once this pull request goes in, scalastyle will be run against all test
code as well. Previously we didn't run the style checker on test code.
https://github.com/apache/spark/pull/5486
We might see the master build breaking tomorrow if a PR has style
violations in test code and is tested by Jenkins before the above PR is
merged.
Hi all,
Manning (the publisher) is looking for a co-author for the GraphX in Action
book. The book currently has one author (Michael Malak), but they are
looking for a co-author to work closely with Michael and improve the
writings and make it more consumable.
Early access page for the book: http://www.manning.com/malak/
Let me know if you are interested in that. Cheers.
There is a jdbc in the SQLContext scala doc:
https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SQLContext
Note that this is more of a user list question ....
Thanks for the information.
Adding the dev list.
Do you mind trying 1.3.1 to see if the issue is fixed?  I could be wrong
but it migtht be related to https://issues.apache.org/jira/browse/SPARK-6578
It's because you did a repartition -- which rearranges all the data.
Parquet uses all kinds of compression techniques such as dictionary
encoding and run-length encoding, which would result in the size difference
when the data is ordered different.
This is strange. cc the dev list since it might be a bug.
Definitely a bug. I just checked and it looks like we don't actually have a
function that takes a Scala RDD and Seq[String].
cc Davies who added this code a while back.
I replied on JIRA. Let's move the discussion there.
It runs tons of integration tests. I think most developers just let Jenkins
run the full suite of them.
I created a pull request last night for a new InputSource API that is
essentially a stripped down version of the RDD API for providing data into
Spark. Would be great to hear the community's feedback.
Spark currently has two de facto input source API:
1. RDD
2. Hadoop MapReduce InputFormat
Neither of the above is ideal:
1. RDD: It is hard for Java developers to implement RDD, given the implicit
class tags. In addition, the RDD API depends on Scala's runtime library,
which does not preserve binary compatibility across Scala versions. If a
developer chooses Java to implement an input source, it would be great if
that input source can be binary compatible in years to come.
2. Hadoop InputFormat: The Hadoop InputFormat API is overly restrictive.
For example, it forces key-value semantics, and does not support running
arbitrary code on the driver side (an example of why this is useful is
broadcast). In addition, it is somewhat awkward to tell developers that in
order to implement an input source for Spark, they should learn the Hadoop
MapReduce API first.
My patch creates a new InputSource interface, described by:
- an array of InputPartition that specifies the data partitioning
- a RecordReader that specifies how data on each partition can be read
This interface is similar to Hadoop's InputFormat, except that there is no
explicit key/value separation.
JIRA ticket: https://issues.apache.org/jira/browse/SPARK-7025
Pull request: https://github.com/apache/spark/pull/5603
It can reuse. That's a good point and we should document it in the API
contract.
Woh hold on a minute.
Spark has been among the projects that are the most welcoming to new
contributors. And thanks to this, the sheer number of activities in Spark
is much larger than other projects, and our workflow has to accommodate
this fact.
In practice, people just create pull requests on github, which is a newer &
friendlier & better model given the constraints. We even have tools that
automatically tags a ticket with a link to the pull requests.
Your understanding is correct -- there is no partial aggregation currently
for Hive UDAF.
However, there is a PR to fix that:
https://github.com/apache/spark/pull/5542
I like that idea (having a new-issues list instead of directly forwarding
them to dev).
I'd love to see more design discussions consolidated in a single place as
well. That said, there are many practical challenges to overcome. Some of
them are out of our control:
1. For large features, it is fairly common to open a PR for discussion,
close the PR taking some feedback into account, and reopen another one. You
sort of lose the discussions that way.
2. With the way Jenkins is setup currently, Jenkins testing introduces a
lot of noise to GitHub pull requests, making it hard to differentiate
legitimate comments from noise. This is unfortunately due to the fact that
ASF won't allow our Jenkins bot to have API privilege to post messages.
3. The Apache Way is that all development discussions need to happen on ASF
property, i.e. dev lists and JIRA. As a result, technically we are not
allowed to have development discussions on GitHub.
Can you elaborate what you mean by that? (what's already available in
Python?)
Thanks for looking into this, Shane.
This looks like a specific Spray configuration issue (or how Spray reads
config files). Maybe Spray is reading some local config file that doesn't
exist on your executors?
You might need to email the Spray list.
Shane - can we purge all the outstanding builds so we are not running stuff
against stale PRs?
Before we make DataFrame non-alpha, it would be great to decide how we want
to namespace all the functions. There are 3 alternatives:
1. Put all in org.apache.spark.sql.functions. This is how SQL does it,
since SQL doesn't have namespaces. I estimate eventually we will have ~ 200
functions.
2. Have explicit namespaces, which is what master branch currently looks
like:
- org.apache.spark.sql.functions
- org.apache.spark.sql.mathfunctions
- ...
3. Have explicit namespaces, but restructure them slightly so everything is
under functions.
package object functions {
  // all the old functions here -- but deprecated so we keep source
compatibility
  def ...
}
package org.apache.spark.sql.functions
object mathFunc {
  ...
}
object basicFuncs {
  ...
}
To add a little bit more context, some pros/cons I can think of are:
Option 1: Very easy for users to find the function, since they are all in
org.apache.spark.sql.functions. However, there will be quite a large number
of them.
Option 2: I can't tell why we would want this one over Option 3, since it
has all the problems of Option 3, and not as nice of a hierarchy.
Option 3: Opposite of Option 1. Each "package" or static class has a small
number of functions that are relevant to each other, but for some functions
it is unclear where they should go (e.g. should "min" go into basic or
math?)
Scaladoc isn't much of a problem because scaladocs are grouped. Java/Python
is the main problem ...
See
https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.functions$
We definitely still have the name collision problem in SQL.
We should change the trait to abstract class, and then your problem will go
away.
Do you want to submit a pull request?
I filed a ticket: https://issues.apache.org/jira/browse/SPARK-7280
Would you like to give it a shot?
This has been discussed a few times in the past, but now Oracle has ended
support for Java 6 for over a year, I wonder if we should just drop Java 6
support.
There is one outstanding issue Tom has brought to my attention: PySpark on
YARN doesn't work well with Java 7/8, but we have an outstanding pull
request to fix that.
https://issues.apache.org/jira/browse/SPARK-6869
https://issues.apache.org/jira/browse/SPARK-1920
It's really hard to inspect API calls since none of us have the Java
standard library in our brain. The only way we can enforce this is to have
it in Jenkins, and Tom you are currently our mini-Jenkins server :)
Joking aside, looks like we should support Java 6 in 1.4, and in the
release notes include a message saying starting in 1.5 we will drop Java 6
support.
I've personally prototyped completely in-memory shuffle for Spark 3 times.
However, it is unclear how big of a gain it would be to put all of these in
memory, under newer file systems (ext4, xfs). If the shuffle data is small,
they are still in the file system buffer cache anyway. Note that network
throughput is often lower than disk throughput, so it won't be a problem to
read them from disk. And not having to keep all of these stuff in-memory
substantially simplifies memory management.
Part of the reason is that it is really easy to just call toDF on Scala,
and we already have a lot of createDataFrame functions.
(You might find some of the cross-language differences confusing, but I'd
argue most real users just stick to one language, and developers or
trainers are the only ones that need to constantly switch between
languages).
How does the pivotal format decides where to split the files? It seems to
me the challenge is to decide that, and on the top of my head the only way
to do this is to scan from the beginning and parse the json properly, which
makes it not possible with large files (doable for whole input with a lot
of small files though). If there is a better way, we should do it.
After talking with people on this thread and offline, I've decided to go
with option 1, i.e. putting everything in a single "functions" object.
OK I sent an email.
Hi all,
We will drop support for Java 6 starting Spark 1.5, tentative scheduled to
be released in Sep 2015. Spark 1.4, scheduled to be released in June 2015,
will be the last minor release that supports Java 6. That is to say:
Spark 1.4.x (~ Jun 2015): will work with Java 6, 7, 8.
Spark 1.5+ (~ Sep 2015): will NOT work with Java 6, but work with Java 7, 8.
PS: Oracle ended Java 6 updates in Feb 2013.
Sean - Please do.
@tgraves can chime in, but I think this pr aims to fix it:
https://github.com/apache/spark/pull/5580
We should probably get that in for 1.4.
They are usually pretty responsive. We can ping chill to get them to do a
release.
Thanks for doing this. Testing infra is one of the most important parts of
a project, and this will make it easier to identify flaky tests.
In 1.5, we will most likely just rewrite distinct in SQL to either use the
Aggregate operator which will benefit from all the Tungsten optimizations,
or have a Tungsten version of distinct for SQL/DataFrame.
Is this related to s3a update in 2.6?
In 1.4, you can do
row.getInt("colName")
In 1.5, some variant of this will come to allow you to turn a DataFrame
into a typed RDD, where the case class's field names match the column
names. https://github.com/apache/spark/pull/5713
Sorry it's hard to give a definitive answer due to the lack of details (I'm
not sure what exactly is entailed to have this PortableDataStream), but the
answer is probably no if we need to change some existing code and expose a
whole new data type to users.
Looks like it is spending a lot of time doing hash probing. It could be a
number of the following:
1. hash probing itself is inherently expensive compared with rest of your
workload
2. murmur3 doesn't work well with this key distribution
3. quadratic probing (triangular sequence) with a power-of-2 hash table
works really badly for this workload.
One way to test this is to instrument changeValue function to store the
number of probes in total, and then log it. We added this probing
capability to the new Bytes2Bytes hash map we built. We should consider
just having it being reported as some built-in metrics to facilitate
debugging.
https://github.com/apache/spark/blob/b83091ae4589feea78b056827bc3b7659d271e41/unsafe/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java#L214
I added @since version tag for all public dataframe/sql methods/classes in
this patch: https://github.com/apache/spark/pull/6101/files
public functions have @since tag. Thanks.
Was looking at a PR test log just now. Can somebody take a look and remove
the warnings (or just hide them)?
15/05/13 01:49:35 INFO UISeleniumSuite: Trying to start HiveThriftServer2:
port=13125, mode=binary, attempt=0
15/05/13 01:50:28 INFO UISeleniumSuite: HiveThriftServer2 started
successfully
15/05/13 01:50:31 WARN DefaultCssErrorHandler: CSS error: '
http://localhost:29132/static/bootstrap.min.css' [10:11] Error in style
rule. (Invalid token "*". Was expecting one of: , , , "}",
";".)
15/05/13 01:50:31 WARN DefaultCssErrorHandler: CSS warning: '
http://localhost:29132/static/bootstrap.min.css' [10:11] Ignoring the
following declarations in this rule.
15/05/13 01:50:31 WARN DefaultCssErrorHandler: CSS error: '
http://localhost:29132/static/bootstrap.min.css' [15:41] Error in style
rule. (Invalid token "*". Was expecting one of: , , , "}",
";".)
15/05/13 01:50:31 WARN DefaultCssErrorHandler: CSS warning: '
http://localhost:29132/static/bootstrap.min.css' [15:41] Ignoring the
following declarations in this rule.
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS error: '
http://localhost:29132/static/bootstrap.min.css' [26:14] Error in style
rule. (Invalid token "*". Was expecting one of: , , , "}",
";".)
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS warning: '
http://localhost:29132/static/bootstrap.min.css' [26:14] Ignoring the
following declarations in this rule.
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS error: '
http://localhost:29132/static/bootstrap.min.css' [39:24] Error in style
rule. (Invalid token "*". Was expecting one of: , , , "}",
";".)
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS warning: '
http://localhost:29132/static/bootstrap.min.css' [39:24] Ignoring the
following declarations in this rule.
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS error: '
http://localhost:29132/static/bootstrap.min.css' [67:23] Error in style
rule. (Invalid token "*". Was expecting one of: , , , "}",
";".)
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS warning: '
http://localhost:29132/static/bootstrap.min.css' [67:23] Ignoring the
following declarations in this rule.
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS error: '
http://localhost:29132/static/bootstrap.min.css' [69:190] Error in style
rule. (Invalid token "*". Was expecting one of: , , , "}",
";".)
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS warning: '
http://localhost:29132/static/bootstrap.min.css' [69:190] Ignoring the
following declarations in this rule.
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS error: '
http://localhost:29132/static/bootstrap.min.css' [72:31] Error in style
rule. (Invalid token "*". Was expecting one of: , , , "}",
";".)
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS warning: '
http://localhost:29132/static/bootstrap.min.css' [72:31] Ignoring the
following declarations in this rule.
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS error: '
http://localhost:29132/static/bootstrap.min.css' [73:45] Error in style
rule. (Invalid token "*". Was expecting one of: , , , "}",
";".)
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS warning: '
http://localhost:29132/static/bootstrap.min.css' [73:45] Ignoring the
following declarations in this rule.
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS error: '
http://localhost:29132/static/bootstrap.min.css' [74:45] Error in style
rule. (Invalid token "*". Was expecting one of: , , , "}",
";".)
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS warning: '
http://localhost:29132/static/bootstrap.min.css' [74:45] Ignoring the
following declarations in this rule.
15/05/13 01:50:32 WARN DefaultCssErrorHandler: CSS error: '
http://localhost:29132/static/bootstrap.min.css' [75:44] Error in style
rule. (Invalid token "*". Was expecting one of: , , , "}",
";".)
Maybe JIT? The 1st stage -- the scheduler code isn't JITed yet.
Testing html emails ...
Hello
*This is bold*
This is a link 
This is a link
This email should have some attachment
---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
You can just look at this branch, can't you?
https://github.com/apache/spark/tree/branch-1.3
It is just 15 lines of code to copy, isn't it?
That's the nice thing about Spark packages. It is just a package index for
libraries and applications built on top of Spark and not part of the Spark
codebase, so it is not restricted to follow only ASF-compatible licenses.
You definitely don't want to implement kmeans in R, since it would be very
slow. Just providing R wrappers for the MLlib implementation is the way to
go. I believe one of the major items in SparkR next is the MLlib wrappers.
I think it is fairly hard to support recursive data types. What I've seen
in one other proprietary system in the past is to let the user define the
depth of the nested data types, and then just expand the struct/map/list
definition to the maximum level of depth.
Would this solve your problem?
My understanding is that it is good to have this, but a large fraction of
flaky tests are actually also race conditions, etc.
If somebody has some free cycles, we'd greatly appreciate some
investigation & a patch to integrate Google's errorprone with Spark's Maven
build.
https://issues.apache.org/jira/browse/SPARK-7938
I think you are right that there is no way to call Java UDF without
registration right now. Adding another 20 methods to functions would be
scary. Maybe the best way is to have a companion object
for UserDefinedFunction, and define UDF there?
e.g.
object UserDefinedFunction {
  def define(f: org.apache.spark.api.java.function.Function0, returnType:
Class[_]): UserDefinedFunction
  // ... define a few more - maybe up to 5 arguments?
}
Ideally, we should ask for both argument class and return class, so we can
do the proper type conversion (e.g. if the UDF expects a string, but the
input expression is an int, Catalyst can automatically add a cast).
However, we haven't implemented those in UserDefinedFunction yet.
I think you are looking for
http://en.wikipedia.org/wiki/Common_subexpression_elimination in the
optimizer.
One thing to note is that as we do more and more optimization like this,
the optimization time might increase. Do you see a case where this can
bring you substantial performance gains?
We added all the typetags for arguments but haven't got around to use them
yet. I think it'd make sense to have them and do the auto cast, but we can
have rules in analysis to forbid certain casts (e.g. don't auto cast double
to int).
FYI we merged a patch that improves unit test log debugging. In order for
that to work, all test suites have been changed to extend SparkFunSuite
instead of ScalaTest's FunSuite. We also added a rule in the Scala style
checker to fail Jenkins if FunSuite is used.
The patch that introduced SparkFunSuite:
https://github.com/apache/spark/pull/6441
Style check patch: https://github.com/apache/spark/pull/6510
dropping user list, adding dev.
Thanks, Justin, for the poc. This is a good idea to explore, especially for
Spark 2.0.
I don't think so.
There will be in 1.4.
df.write.partitionBy("year", "month", "day").parquet("/path/to/output")
Almost all dataframe stuff are tracked by this umbrella ticket:
https://issues.apache.org/jira/browse/SPARK-6116
For the reader/writer interface, it's here:
https://issues.apache.org/jira/browse/SPARK-7654
https://github.com/apache/spark/pull/6175
What version of Spark is this?
Maybe an incompatible Hive package or Hive metastore?
We improved this in 1.4. Adding 100 columns took 4s on my laptop.
https://issues.apache.org/jira/browse/SPARK-7276
Still not the fastest, but much faster.
scala> Seq((1, 2)).toDF("a", "b")
res6: org.apache.spark.sql.DataFrame = [a: int, b: int]
scala>
scala> val start = System.nanoTime
start: Long = 1433274299441224000
scala> for (i <- 1 to 100) {
     |   df = df.withColumn("n" + i, org.apache.spark.sql.functions.lit(0))
     | }
scala> val end = System.nanoTime
end: Long = 1433274303250091000
scala>
scala> println((end - start) / 1000 / 1000 / 1000)
3
Can you submit a pull request for it? Thanks.
.select itself is the bulk add right?
Hi Tarek,
I took a quick look at the materials you shared. It actually seems to me
it'd be super easy to express a graph as two DataFrames: one for edges
(srcid, dstid, and other edge attributes) and one for vertices (vid, and
other vertex attributes).
Then
intersection is just
edges1.intersect(edges2)
"join" is just
edges1.union(edges2).distinct
I'd think id is the unique identifier by default.
Let me give you the 1st
+1
Enjoy your new shiny mbp.
FYI.
Put them in quotes, e.g.
sbt/sbt "mllib/testOnly *NaiveBayesSuite"
If you read the file one by one and then use parallelize, it is read by a
single thread on a single machine.
How did you exclude it?
I am not sure if it is possible since each task needs to contain the
chunk of data.
---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
We might be able to ask asf infra to help. Can you create a ticket?
You doing something for Haskell??
Take a look at this for Python:
https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals
Most of those threads are not for task execution. They are for RPC,
scheduling, ...
Hi Andrew,
Thanks for the email. This is a known bug with the expression parser. We
will hopefully fix this in 1.5.
There are more keywords with the expression parser, and we have already got
rid of most of them. Count is still there due to the handling of count
distinct, but we plan to get rid of that as well.
Try mapPartitions, which gives you an iterator, and you can produce an
iterator back.
+1
Run
./python/run-tests --help
and you will see. :)
"except" is a keyword in Python unfortunately.
Looks like it is back up now.
FYI there are some problems with ASF's git or ldap infra. As a result, we
cannot merge anything into Spark right now.
An infra ticket has been created:
https://issues.apache.org/jira/browse/INFRA-9932
Please watch/vote on that ticket for progress. Thanks.
I've been adding people to the developer role to get around the jira limit.
BTW Infra has the ability to create multiple groups. Maybe that's a better
solution.
Have contributor1, contributor2, contributor3 ...
+1
I don't get this rule. It is arbitrary, and does not seem like something
that should be enforced at the foundation level. By this reasoning, are we
not allowed to list "source code management" on the project public page as
well?
The download page clearly states the nightly builds are "bleeding-edge".
Note that technically we did not violate any rules, since the ones we
showed were not "nightly builds" by the foundation's definition: "Nightly
Builds are simply built from the Subversion trunk, usually once a day.".
Spark nightly artifacts were built from git, not svn trunk. :)  (joking).
What Sandy meant was there was no out-of-the-box support in Spark for
reading excel files. However, you can still read excel:
If you are using Python, you can use Pandas to load an excel file and then
convert it into a Spark DataFrame.
If you are using the JVM, you can find any excel library for Java/Scala to
read excel files either in the driver, or read them in parallel on workers
if you have lots of excel files.
Note that this question does not really belong in the dev list. It should
be sent to the user list or asked on stackoverflow.
How about just using two fields, one boolean field to mark good/bad, and
another to get the source file?
Yea - I'd just add a bunch of columns. Doesn't seem like that big of a deal.
Hi Bob,
Thanks for the email. You can select Spark as the project when you file a
JIRA ticket at https://issues.apache.org/jira/browse/SPARK
For "select 1 from $table where 0=1" -- if the database's optimizer doesn't
do constant folding and short-circuit execution, could the query end up
scanning all the data?
It's bad that expose a trait - even though we want to mixin stuff. We
should really audit all of these and expose only abstract classes for
anything beyond an extremely simple interface. That itself however would
break binary compatibility.
I took a look at the commit messages in git log -- it looks like the
individual commit messages are not that useful to include, but do make the
commit messages more verbose. They are usually just a bunch of extremely
concise descriptions of "bug fixes", "merges", etc:
    cb3f12d [xxx] add whitespace
    6d874a6 [xxx] support pyspark for yarn-client
    89b01f5 [yyy] Update the unit test to add more cases
    275d252 [yyy] Address the comments
    7cc146d [yyy] Address the comments
    2624723 [yyy] Fix rebase conflict
    45befaa [yyy] Update the unit test
    bbc1c9c [yyy] Fix checkpointing doesn't retain driver port issue
Anybody against removing those from the merge script so the log looks
cleaner? If nobody feels strongly about this, we can just create a JIRA to
remove them, and only keep the author names.
A single commit message consisting of:
1. Pull request title (which includes JIRA number and component, e.g.
[SPARK-1234][MLlib])
2. Pull request description
3. List of authors contributing to the patch
The main thing that changes is 3: we used to also include the individual
commits to the pull request branch that are squashed.
Thanks, Sean.
Is amplab the right owner, given its ending next year? Maybe we should
create spark-ec2, or spark-project instead?
They are already pluggable.
I sent it prematurely.
They are already pluggable, or at least in the process to be more
pluggable. In 1.4, instead of calling the external system's API directly,
we added an API for that.  There is a patch to add support for HDFS
in-memory cache.
Somewhat orthogonal to this, longer term, I am not sure whether it makes
sense to have the current off heap API, because there is no namespacing and
the benefit to end users is actually not very substantial (at least I can
think of much simpler ways to achieve exactly the same gains), and yet it
introduces quite a bit of complexity to the codebase.
Hi all,
FYI, we just merged a patch that fails a build if there is a scala compiler
warning (if it is not deprecation warning).
In the past, many compiler warnings are actually caused by legitimate bugs
that we need to address. However, if we don't fail the build with warnings,
people don't pay attention at all to the warnings (it is also tough to pay
attention since there are a lot of deprecated warnings due to unit tests
testing deprecated APIs and reliance on Hadoop on deprecated APIs).
Note that ideally we should be able to mark deprecation warnings as errors
as well. However, due to the lack of ability to suppress individual warning
messages in the Scala compiler, we cannot do that (since we do need to
access deprecated APIs in Hadoop).
You can give it a shot, but we will have to revert it for a project as soon
as a project uses a deprecated API somewhere.
There is this pull request: https://github.com/apache/spark/pull/5713
We mean to merge it for 1.5. Maybe you can help review it too?
I just pushed a hotfix to disable Pylint.
I think we do support 0 arg UDFs:
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L2165
How are you using UDFs?
Hey All,
Just a friendly reminder that Aug 1st is the feature freeze for Spark
1.5, meaning major outstanding changes will need to land in the this
week.
After May 1st we'll package a release for testing and then go into the
normal triage process where bugs are prioritized and some smaller
features are allowed on a case by case basis (if they are very low risk/
additive/feature flagged/etc).
As always, I'll invite the community to help participate in code
review of patches in the this week, since review bandwidth is the
single biggest determinant of how many features will get in. Please
also keep in mind that most active committers are working overtime
(nights/weekends) during this period and will try their best to help
usher in as many patches as possible, along with their own code.
As a reminder, release window dates are always maintained on the wiki
and are updated after each release according to our 3 month release
cadence:
https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage
Thanks - and happy coding!
- Reynold
We should add UDF0 to it.
For now, can you just create an one-arg UDF and don't use the argument?
BTW for 1.5, there is already a now like function being added, so it should
work out of the box in 1.5.0, to be released end of Aug/early Sep.
Yup - would you be willing to submit a patch to add UDF0?
Should be pretty easy (really just add a new Java class, and then add a new
function to registerUDF)
There are two usage of buckets used in Spark core.
The first usage is in histogram, used to perform sorting. Basically we
build an approximate histogram of the data in order to decide how to
partition the data in sorting. Each bucket is a range in the histogram.
The 2nd is used in shuffle, where we partition the output of each map task
into different "buckets", letting the reduce side fetching the map side
data based on their partition id.
Hi Devs,
Just an announcement that I've cut Spark's branch-1.5 to form the basis of
the 1.5 release. Other than a few stragglers, this represents the end of
active feature development for Spark 1.5. *If committers are merging any
features (outside of alpha modules), please shoot me an email so I can help
coordinate. Any new commits will need to be explicitly merged into
branch-1.5*.
In the next few days, we should come up with testing plans for the release
and create umbrella JIRAs for testing various components and changes. I
plan to cut a preview package for 1.5 towards the end of this week or early
next week.
- R
This is now done with this pull request:
https://github.com/apache/spark/pull/8091
Committers please update the script to get this "feature".
Thanks for finding this. Should we just switch to Java's process library
for now?
That was intentional - what's your use case that require configs not
starting with spark?
Retry sending this again ...
(I tried to send this last night but somehow ASF mailing list rejected my
mail)
In order to facilitate community testing of the 1.5.0 release, I've built a
preview package. This is not a release candidate, so there is no voting
involved. However, it'd be great if community members can start testing
with this preview package.
This preview package contains all the commits to branch-1.5 till commit
cedce9bdb72a00cbcbcc81d57f2a550eaf4416e8.
The source files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-preview-20150812-bin/
The artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1133/
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-preview-20150812-docs/
== How can you help? ==
If you are a Spark user, you can help us test this release by taking a
Spark workload and running on this preview release, then reporting any
regressions.
== Major changes to help you focus your testing ==
As of today, Spark 1.5 contains more than 1000 commits from 220+
contributors. I've curated a list of important changes for 1.5. For the
complete list, please refer to Apache JIRA changelog.
RDD/DataFrame/SQL APIs
- New UDAF interface
- DataFrame hints for broadcast join
- expr function for turning a SQL expression into DataFrame column
- Improved support for NaN values
- StructType now supports ordering
- TimestampType precision is reduced to 1us
- 100 new built-in expressions, including date/time, string, math
- memory and local disk only checkpointing
DataFrame/SQL Backend Execution
- Code generation on by default
- Improved join, aggregation, shuffle, sorting with cache friendly
algorithms and external algorithms
- Improved window function performance
- Better metrics instrumentation and reporting for DF/SQL execution plans
Data Sources, Hive, Hadoop, Mesos and Cluster Management
- Dynamic allocation support in all resource managers (Mesos, YARN,
Standalone)
- Improved Mesos support (framework authentication, roles, dynamic
allocation, constraints)
- Improved YARN support (dynamic allocation with preferred locations)
- Improved Hive support (metastore partition pruning, metastore
connectivity to 0.13 to 1.2, internal Hive upgrade to 1.2)
- Support persisting data in Hive compatible format in metastore
- Support data partitioning for JSON data sources
- Parquet improvements (upgrade to 1.7, predicate pushdown, faster metadata
discovery and schema merging, support reading non-standard legacy Parquet
files generated by other libraries)
- Faster and more robust dynamic partition insert
- DataSourceRegister interface for external data sources to specify short
names
SparkR
- YARN cluster mode in R
- GLMs with R formula, binomial/Gaussian families, and elastic-net
regularization
- Improved error messages
- Aliases to make DataFrame functions more R-like
Streaming
- Backpressure for handling bursty input streams.
- Improved Python support for streaming sources (Kafka offsets, Kinesis,
MQTT, Flume)
- Improved Python streaming machine learning algorithms (K-Means, linear
regression, logistic regression)
- Native reliable Kinesis stream support
- Input metadata like Kafka offsets made visible in the batch details UI
- Better load balancing and scheduling of receivers across cluster
- Include streaming storage in web UI
Machine Learning and Advanced Analytics
- Feature transformers: CountVectorizer, Discrete Cosine transformation,
MinMaxScaler, NGram, PCA, RFormula, StopWordsRemover, and VectorSlicer.
- Estimators under pipeline APIs: naive Bayes, k-means, and isotonic
regression.
- Algorithms: multilayer perceptron classifier, PrefixSpan for sequential
pattern mining, association rule generation, 1-sample Kolmogorov-Smirnov
test.
- Improvements to existing algorithms: LDA, trees/ensembles, GMMs
- More efficient Pregel API implementation for GraphX
- Model summary for linear and logistic regression.
- Python API: distributed matrices, streaming k-means and linear models,
LDA, power iteration clustering, etc.
- Tuning and evaluation: train-validation split and multiclass
classification evaluator.
- Documentation: document the release version of public API methods
Is this through Java properties? For java properties, you can pass them
using spark.executor.extraJavaOptions.
I believe for Hive, there is already a client interface that can be used to
build clients for different Hive metastores. That should also work for your
heavily forked one.
For Hadoop, it is definitely a bigger project to refactor. A good way to
start evaluating this is to list what needs to be changed. Maybe you can
start by telling us what you need to change for every upgrade? Feel free to
email me in private if this is sensitive and you don't want to share in a
public list.
This is already supported with the new partitioned data sources in
DataFrame/SQL right?
Is it possible that you have only upgraded some set of nodes but not the
others?
We have ran some performance benchmarks on this so it definitely runs in
some configuration. Could still be buggy in some other configurations
though.
Five month ago we reached 10000 commits on GitHub. Today we reached 10000
JIRA tickets.
https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20created%3E%3D-1w%20ORDER%20BY%20created%20DESC
Hopefully the extra character we have to type doesn't bring our
productivity much.
I pinged the IBM team to submit a patch that would work on IBM JVM.
Thanks for reporting back, Mark.
I will soon post a release candidate.
How did you run this? I couldn't run your query with 4G of RAM in 1.4, but
in 1.5 it ran.
Also I recommend just dumping the data to parquet on disk to evaluate,
rather than using the in-memory cache, which is super slow and we are
thinking of removing/replacing with something else.
val size = 100000000
val partitions = 10
val repetitions = 5
val data = sc.parallelize(1 to size, partitions).map(x =>
(util.Random.nextInt(size / repetitions),
util.Random.nextDouble)).toDF("key", "value")
data.write.parquet("/scratch/rxin/tmp/alex")
val df = sqlContext.read.parquet("/scratch/rxin/tmp/alex")
val t = System.nanoTime()
val res = df.groupBy("key").agg(sum("value"))
res.count()
println((System.nanoTime() - t) / 1e9)
 I didn't wait long enough earlier. Actually it did finish when I raised
memory to 8g.
In 1.5 with Tungsten (which should be the same as 1.4 with your unsafe
flags), the query took 40s with 4G of mem.
In 1.4, it took 195s with 8G of mem.
This is not a scientific benchmark and I only ran it once.
BTW one other thing -- don't use the count() to do benchmark, since the
optimizer is smart enough to figure out that you don't actually need to run
the sum.
For the purpose of benchmarking, you can use
df.foreach(i => do nothing)
Please vote on releasing the following candidate as Apache Spark version
1.5.0!
The vote is open until Monday, Aug 17, 2015 at 20:00 UTC and passes if a
majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 1.5.0
[ ] -1 Do not release this package because ...
To learn more about Apache Spark, please see http://spark.apache.org/
The tag to be voted on is v1.5.0-rc1:
https://github.com/apache/spark/tree/4c56ad772637615cc1f4f88d619fac6c372c8552
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-rc1-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1137/
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-rc1-docs/
=======================================
== How can I help test this release? ==
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.
================================================
== What justifies a -1 vote for this release? ==
================================================
This vote is happening towards the end of the 1.5 QA period, so -1 votes
should only occur for significant regressions from 1.4. Bugs already
present in 1.4, minor regressions, or bugs related to new features will not
block this release.
===============================================================
== What should happen to JIRA tickets still targeting 1.5.0? ==
===============================================================
1. It is OK for documentation patches to target 1.5.0 and still go into
branch-1.5, since documentations will be packaged separately from the
release.
2. New features for non-alpha-modules should target 1.6+.
3. Non-blocker bug fixes should target 1.5.1 or 1.6.0, or drop the target
version.
==================================================
== Major changes to help you focus your testing ==
==================================================
As of today, Spark 1.5 contains more than 1000 commits from 220+
contributors. I've curated a list of important changes for 1.5. For the
complete list, please refer to Apache JIRA changelog.
RDD/DataFrame/SQL APIs
- New UDAF interface
- DataFrame hints for broadcast join
- expr function for turning a SQL expression into DataFrame column
- Improved support for NaN values
- StructType now supports ordering
- TimestampType precision is reduced to 1us
- 100 new built-in expressions, including date/time, string, math
- memory and local disk only checkpointing
DataFrame/SQL Backend Execution
- Code generation on by default
- Improved join, aggregation, shuffle, sorting with cache friendly
algorithms and external algorithms
- Improved window function performance
- Better metrics instrumentation and reporting for DF/SQL execution plans
Data Sources, Hive, Hadoop, Mesos and Cluster Management
- Dynamic allocation support in all resource managers (Mesos, YARN,
Standalone)
- Improved Mesos support (framework authentication, roles, dynamic
allocation, constraints)
- Improved YARN support (dynamic allocation with preferred locations)
- Improved Hive support (metastore partition pruning, metastore
connectivity to 0.13 to 1.2, internal Hive upgrade to 1.2)
- Support persisting data in Hive compatible format in metastore
- Support data partitioning for JSON data sources
- Parquet improvements (upgrade to 1.7, predicate pushdown, faster metadata
discovery and schema merging, support reading non-standard legacy Parquet
files generated by other libraries)
- Faster and more robust dynamic partition insert
- DataSourceRegister interface for external data sources to specify short
names
SparkR
- YARN cluster mode in R
- GLMs with R formula, binomial/Gaussian families, and elastic-net
regularization
- Improved error messages
- Aliases to make DataFrame functions more R-like
Streaming
- Backpressure for handling bursty input streams.
- Improved Python support for streaming sources (Kafka offsets, Kinesis,
MQTT, Flume)
- Improved Python streaming machine learning algorithms (K-Means, linear
regression, logistic regression)
- Native reliable Kinesis stream support
- Input metadata like Kafka offsets made visible in the batch details UI
- Better load balancing and scheduling of receivers across cluster
- Include streaming storage in web UI
Machine Learning and Advanced Analytics
- Feature transformers: CountVectorizer, Discrete Cosine transformation,
MinMaxScaler, NGram, PCA, RFormula, StopWordsRemover, and VectorSlicer.
- Estimators under pipeline APIs: naive Bayes, k-means, and isotonic
regression.
- Algorithms: multilayer perceptron classifier, PrefixSpan for sequential
pattern mining, association rule generation, 1-sample Kolmogorov-Smirnov
test.
- Improvements to existing algorithms: LDA, trees/ensembles, GMMs
- More efficient Pregel API implementation for GraphX
- Model summary for linear and logistic regression.
- Python API: distributed matrices, streaming k-means and linear models,
LDA, power iteration clustering, etc.
- Tuning and evaluation: train-validation split and multiclass
classification evaluator.
- Documentation: document the release version of public API methods
A mistake in the original email. The vote closes at 20:00 UTC, Aug 24,
rather than Aug 17.
I'm actually somewhat involved with the Google Docs you linked to.
I don't think Oracle will remove Unsafe in JVM 9. As you said, JEP 260
already proposes making Unsafe available. Given the widespread use of
Unsafe for performance and advanced functionalities, I don't think Oracle
can just remove it in one release. If they do, there will be strong
backlash and the act would significantly undermine the credibility of the
JVM as a long-term platform.
Note that for Spark itself, we move pretty fast and can replace all the use
of Unsafe with a newer alternative in one release if absolutely necessary
(the actual coding takes only a day or two).
You've probably hit this bug:
https://issues.apache.org/jira/browse/SPARK-7180
It's fixed in Spark 1.4.1+. Try setting spark.serializer.extraDebugInfo to
false and see if it goes away.
Problem noted. Apparently the release script doesn't automate the
replacement of all version strings yet. I'm going to publish a new RC over
the weekend with the release version properly assigned.
Please continue the testing and report any problems you find. Thanks!
FYI
This has been resolved.
Nope --- I cut that last Friday but had an error. I will remove it and cut
a new one.
Please vote on releasing the following candidate as Apache Spark version
1.5.0. The vote is open until Friday, Aug 29, 2015 at 5:00 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 1.5.0
[ ] -1 Do not release this package because ...
To learn more about Apache Spark, please see http://spark.apache.org/
The tag to be voted on is v1.5.0-rc2:
https://github.com/apache/spark/tree/727771352855dbb780008c449a877f5aaa5fc27a
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-rc2-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release (published as 1.5.0-rc2) can be
found at:
https://repository.apache.org/content/repositories/orgapachespark-1141/
The staging repository for this release (published as 1.5.0) can be found
at:
https://repository.apache.org/content/repositories/orgapachespark-1140/
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-rc2-docs/
=======================================
How can I help test this release?
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.
================================================
What justifies a -1 vote for this release?
================================================
This vote is happening towards the end of the 1.5 QA period, so -1 votes
should only occur for significant regressions from 1.4. Bugs already
present in 1.4, minor regressions, or bugs related to new features will not
block this release.
===============================================================
What should happen to JIRA tickets still targeting 1.5.0?
===============================================================
1. It is OK for documentation patches to target 1.5.0 and still go into
branch-1.5, since documentations will be packaged separately from the
release.
2. New features for non-alpha-modules should target 1.6+.
3. Non-blocker bug fixes should target 1.5.1 or 1.6.0, or drop the target
version.
==================================================
Major changes to help you focus your testing
==================================================
As of today, Spark 1.5 contains more than 1000 commits from 220+
contributors. I've curated a list of important changes for 1.5. For the
complete list, please refer to Apache JIRA changelog.
RDD/DataFrame/SQL APIs
- New UDAF interface
- DataFrame hints for broadcast join
- expr function for turning a SQL expression into DataFrame column
- Improved support for NaN values
- StructType now supports ordering
- TimestampType precision is reduced to 1us
- 100 new built-in expressions, including date/time, string, math
- memory and local disk only checkpointing
DataFrame/SQL Backend Execution
- Code generation on by default
- Improved join, aggregation, shuffle, sorting with cache friendly
algorithms and external algorithms
- Improved window function performance
- Better metrics instrumentation and reporting for DF/SQL execution plans
Data Sources, Hive, Hadoop, Mesos and Cluster Management
- Dynamic allocation support in all resource managers (Mesos, YARN,
Standalone)
- Improved Mesos support (framework authentication, roles, dynamic
allocation, constraints)
- Improved YARN support (dynamic allocation with preferred locations)
- Improved Hive support (metastore partition pruning, metastore
connectivity to 0.13 to 1.2, internal Hive upgrade to 1.2)
- Support persisting data in Hive compatible format in metastore
- Support data partitioning for JSON data sources
- Parquet improvements (upgrade to 1.7, predicate pushdown, faster metadata
discovery and schema merging, support reading non-standard legacy Parquet
files generated by other libraries)
- Faster and more robust dynamic partition insert
- DataSourceRegister interface for external data sources to specify short
names
SparkR
- YARN cluster mode in R
- GLMs with R formula, binomial/Gaussian families, and elastic-net
regularization
- Improved error messages
- Aliases to make DataFrame functions more R-like
Streaming
- Backpressure for handling bursty input streams.
- Improved Python support for streaming sources (Kafka offsets, Kinesis,
MQTT, Flume)
- Improved Python streaming machine learning algorithms (K-Means, linear
regression, logistic regression)
- Native reliable Kinesis stream support
- Input metadata like Kafka offsets made visible in the batch details UI
- Better load balancing and scheduling of receivers across cluster
- Include streaming storage in web UI
Machine Learning and Advanced Analytics
- Feature transformers: CountVectorizer, Discrete Cosine transformation,
MinMaxScaler, NGram, PCA, RFormula, StopWordsRemover, and VectorSlicer.
- Estimators under pipeline APIs: naive Bayes, k-means, and isotonic
regression.
- Algorithms: multilayer perceptron classifier, PrefixSpan for sequential
pattern mining, association rule generation, 1-sample Kolmogorov-Smirnov
test.
- Improvements to existing algorithms: LDA, trees/ensembles, GMMs
- More efficient Pregel API implementation for GraphX
- Model summary for linear and logistic regression.
- Python API: distributed matrices, streaming k-means and linear models,
LDA, power iteration clustering, etc.
- Tuning and evaluation: train-validation split and multiclass
classification evaluator.
- Documentation: document the release version of public API methods
Any reason why you have more than 2G in a single line?
There is a limit of 2G in the Hadoop library we use. Also the JVM doesn't
work when your string is that long.
One small update -- the vote should close Saturday Aug 29. Not Friday Aug
29.
The Scala 2.11 issue should be fixed, but doesn't need to be a blocker,
since Maven builds fine. The sbt build is more aggressive to make sure we
catch warnings.
I'd like this to happen, but it hasn't been super high priority on
anybody's mind.
There are a couple things that could be good to do:
1. At the application level: consolidate task metrics and accumulators.
They have substantial overlap, and from high level should just be
consolidated. Maybe there are some differences in semantics w.r.t. retries
or fault-tolerance, but those can be just modes in the consolidated
interface/implementation.
Once we do that, then users effectively can use the new consolidated
interface to add new metrics.
2. At the process/service monitoring level: expose an internal metrics
interface to make it easier to create new metrics and publish them via a
rest interface. Last time I looked at this (~4 weeks ago), publication of
the current metrics was not as straightforward as I was hoping for. We use
the codahale library only in some places (IIRC just the cluster manager,
but not the actual executors). It'd make sense to create a simple wrapper
for the coda hale library and make it easier to create new metrics.
Marcelo - please submit a patch anyway. If we don't include it in this
release, it will go into 1.5.1.
Supporting non-JVM code without memory copying and serialization is
actually one of the motivations behind Tungsten. We didn't talk much about
it since it is not end-user-facing and it is still too early. There are a
few challenges still:
1. Spark cannot run entirely in off-heap mode (by entirely here I'm
referring to all the data-plane memory, not control-plane such as RPCs
since those don't matter much). There is nothing fundamental. It just takes
a while to make sure all code paths allocate/free memory using the proper
allocators.
2. The memory layout of data is still in flux, since we are only 4 months
into Tungsten. They will change pretty frequently for the foreseeable
future, and as a result, the C++ side of things will have change as well.
BTW if you are interested in this, we could definitely get some help in
terms of prototyping the feasibility, i.e. how we can have a native (e.g.
C++) API for data access shipped with Spark. There are a lot of questions
(e.g. build, portability) that need to be answered.
Please do. Thanks.
Please vote on releasing the following candidate as Apache Spark version
1.5.0. The vote is open until Friday, Sep 4, 2015 at 21:00 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 1.5.0
[ ] -1 Do not release this package because ...
To learn more about Apache Spark, please see http://spark.apache.org/
The tag to be voted on is v1.5.0-rc3:
https://github.com/apache/spark/commit/908e37bcc10132bb2aa7f80ae694a9df6e40f31a
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-rc3-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release (published as 1.5.0-rc3) can be
found at:
https://repository.apache.org/content/repositories/orgapachespark-1143/
The staging repository for this release (published as 1.5.0) can be found
at:
https://repository.apache.org/content/repositories/orgapachespark-1142/
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-rc3-docs/
=======================================
How can I help test this release?
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.
================================================
What justifies a -1 vote for this release?
================================================
This vote is happening towards the end of the 1.5 QA period, so -1 votes
should only occur for significant regressions from 1.4. Bugs already
present in 1.4, minor regressions, or bugs related to new features will not
block this release.
===============================================================
What should happen to JIRA tickets still targeting 1.5.0?
===============================================================
1. It is OK for documentation patches to target 1.5.0 and still go into
branch-1.5, since documentations will be packaged separately from the
release.
2. New features for non-alpha-modules should target 1.6+.
3. Non-blocker bug fixes should target 1.5.1 or 1.6.0, or drop the target
version.
==================================================
Major changes to help you focus your testing
==================================================
As of today, Spark 1.5 contains more than 1000 commits from 220+
contributors. I've curated a list of important changes for 1.5. For the
complete list, please refer to Apache JIRA changelog.
RDD/DataFrame/SQL APIs
- New UDAF interface
- DataFrame hints for broadcast join
- expr function for turning a SQL expression into DataFrame column
- Improved support for NaN values
- StructType now supports ordering
- TimestampType precision is reduced to 1us
- 100 new built-in expressions, including date/time, string, math
- memory and local disk only checkpointing
DataFrame/SQL Backend Execution
- Code generation on by default
- Improved join, aggregation, shuffle, sorting with cache friendly
algorithms and external algorithms
- Improved window function performance
- Better metrics instrumentation and reporting for DF/SQL execution plans
Data Sources, Hive, Hadoop, Mesos and Cluster Management
- Dynamic allocation support in all resource managers (Mesos, YARN,
Standalone)
- Improved Mesos support (framework authentication, roles, dynamic
allocation, constraints)
- Improved YARN support (dynamic allocation with preferred locations)
- Improved Hive support (metastore partition pruning, metastore
connectivity to 0.13 to 1.2, internal Hive upgrade to 1.2)
- Support persisting data in Hive compatible format in metastore
- Support data partitioning for JSON data sources
- Parquet improvements (upgrade to 1.7, predicate pushdown, faster metadata
discovery and schema merging, support reading non-standard legacy Parquet
files generated by other libraries)
- Faster and more robust dynamic partition insert
- DataSourceRegister interface for external data sources to specify short
names
SparkR
- YARN cluster mode in R
- GLMs with R formula, binomial/Gaussian families, and elastic-net
regularization
- Improved error messages
- Aliases to make DataFrame functions more R-like
Streaming
- Backpressure for handling bursty input streams.
- Improved Python support for streaming sources (Kafka offsets, Kinesis,
MQTT, Flume)
- Improved Python streaming machine learning algorithms (K-Means, linear
regression, logistic regression)
- Native reliable Kinesis stream support
- Input metadata like Kafka offsets made visible in the batch details UI
- Better load balancing and scheduling of receivers across cluster
- Include streaming storage in web UI
Machine Learning and Advanced Analytics
- Feature transformers: CountVectorizer, Discrete Cosine transformation,
MinMaxScaler, NGram, PCA, RFormula, StopWordsRemover, and VectorSlicer.
- Estimators under pipeline APIs: naive Bayes, k-means, and isotonic
regression.
- Algorithms: multilayer perceptron classifier, PrefixSpan for sequential
pattern mining, association rule generation, 1-sample Kolmogorov-Smirnov
test.
- Improvements to existing algorithms: LDA, trees/ensembles, GMMs
- More efficient Pregel API implementation for GraphX
- Model summary for linear and logistic regression.
- Python API: distributed matrices, streaming k-means and linear models,
LDA, power iteration clustering, etc.
- Tuning and evaluation: train-validation split and multiclass
classification evaluator.
- Documentation: document the release version of public API methods
See responses inline.
Can you say more about your transformer?
This is a good idea, and indeed we are doing it for R already (the latest
way to run UDFs in R is to pass the entire partition as a local R dataframe
for users to run on). However, what works for R for simple data processing
might not work for your high performance transformer, etc.
I usually write a test case for what I want to test, and then run
sbt/sbt "~module/test:test-only *MyTestSuite"
Hi All,
Spark 1.5.0 is the sixth release on the 1.x line. This release represents
1400+ patches from 230+ contributors and 80+ institutions. To download
Spark 1.5.0 visit the downloads page.
A huge thanks go to all of the individuals and organizations involved in
development and testing of this release.
Visit the release notes [1] to read about the new features, or download [2]
the release today.
For errata in the contributions or release notes, please e-mail me
*directly* (not on-list).
Thanks to everyone who helped work on this release!
[1] http://spark.apache.org/releases/spark-release-1-5-0.html
[2] http://spark.apache.org/downloads.html
Dev/user announcement was made just now.
For Maven, I did publish it this afternoon (so it's been a few hours). If
it is still not there tomorrow morning, I will look into it.
There isn't really any difference I think where you put them. Did you run
into a problem?
It is already there, but the search is not updated. Not sure what's going
on with maven central search.
http://repo1.maven.org/maven2/org/apache/spark/spark-parent_2.10/1.5.0/
Most these files are just package-info.java there for having a good package
index for JavaDoc. If we move them, we will need to create a folder in the
java one for each package that exposes any documentation. And it is very
likely we will forget to update package-info.java when we update
package.scala if the two files are far apart.
Doesn't seem that big of a deal to have them in Scala folder, and the
benefits ain't that big either.
Hi devs,
FYI - we have already accumulated an "interesting" list of issues found
with the 1.5.0 release. I will work on an RC in the next week or two,
depending on how many blocker/critical issues are fixed.
https://issues.apache.org/jira/issues/?filter=12333321
SPARK-9818 you link to actually links to a pull request trying to bring
them back.
Is this on latest master / branch-1.5?
out of the box we reserve only 16% (0.2 * 0.8) of the memory for execution
(e.g. aggregate, join) / shuffle sorting. With a 3GB heap, that's 480MB. So
each task gets 480MB / 32 = 15MB, and each operator reserves at least one
page for execution. If your page size is 4MB, it only takes 3 operators to
use up its memory.
The thing is page size is dynamically determined -- and in your case it
should be smaller than 4MB.
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/shuffle/ShuffleMemoryManager.scala#L174
Maybe there is a place that in the maven tests that we explicitly set the
page size (spark.buffer.pageSize) to 4MB? If yes, we need to find it and
just remove it.
Pete - can you do me a favor?
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/shuffle/ShuffleMemoryManager.scala#L174
Print the parameters that are passed into the getPageSize function, and
check their values.
rxin=# select null and true;
 ?column?
----------
(1 row)
rxin=# select null and false;
 ?column?
----------
 f
(1 row)
null and false should return false.
Yea I think this is where the heuristics is failing -- it uses 8 cores to
approximate the number of active tasks, but the tests somehow is using 32
(maybe because it explicitly sets it to that, or you set it yourself? I'm
not sure which one)
Maybe we can change the heuristics in memory calculation to use
SparkContext.defaultParallelism if it is local mode.
It is exactly the issue here, isn't it?
We are using memory / N, where N should be the maximum number of active
tasks. In the current master, we use the number of cores to approximate the
number of tasks -- but it turned out to be a bad approximation in tests
because it is set to 32 to increase concurrency.
Can you paste the entire stacktrace of the error? In your original email
you only included the last function call.
Maybe I'm missing something here, but I still think the bad heuristics is
the issue.
Some operators pre-reserve memory before running anything in order to avoid
starvation. For example, imagine we have an aggregate followed by a sort.
If the aggregate is very high cardinality, and uses up all the memory and
even starts spilling (falling back to sort-based aggregate), there isn't
memory available at all for the sort operator to use. To work around this,
each operator reserves a page of memory before they process any data.
Page size is computed by Spark using:
the total amount of execution memory / (maximum number of active tasks * 16)
and then rounded to the next power of 2, and cap between 1MB and 64MB.
That is to say, in the worst case, we should be able to reserve at least 8
pages (16 rounded up to the next power of 2).
However, in your case, the max number of active tasks is 32 (set by test
env), while the page size is determined using # cores (8 in your case). So
it is off by a factor of 4. As a result, with this page size, we can only
reserve at least 2 pages. That is to say, if you have more than 3 operators
that need page reservation (e.g. an aggregate followed by a join on the
group by key followed by a shuffle - which seems to be the case of
join31.q), the task can fail to reserve memory before running anything.
There is a 2nd problem (maybe this is the one you were trying to point
out?) that is tasks running at the same time can be competing for memory
with each other.  Spark allows each task to claim up to 2/N share of
memory, where N is the number of active tasks. If a task is launched before
others and hogs a lot of memory quickly, the other tasks that are launched
after it might not be able to get enough memory allocation, and thus will
fail. This is not super ideal, but probably fine because tasks can be
retried, and can succeed in retries.
I'm not sure what we can do here. Nested RDDs are a pain to implement,
support, and explain. The programming model is not well explored.
Maybe a UDAF interface that allows going through the data twice?
Thanks Shane and Jon for the heads up.
SparkEnv for the driver was created in SparkContext. The default
parallelism field is set to the number of slots (max number of active
tasks). Maybe we can just use the default parallelism to compute that in
local mode.
You should reach out to the speakers directly.
This is "expected" in the sense that DataFrame operations can get
re-ordered under the hood by the optimizer. For example, if the optimizer
deems it is cheaper to apply the 2nd filter first, it might re-arrange the
filters. In reality, it doesn't do that. I think this is too confusing and
violates principle of least astonishment, so we should fix it.
I discussed more with Michael offline, and think we can add a rule for the
physical filter operator to replace the general AND/OR/equality/etc with a
special version that treats null as false. This rule needs to be carefully
written because it should only apply to subtrees of AND/OR/equality/etc
(e.g. it shouldn't rewrite children of isnull).
Your understanding is mostly correct. Replies inline.
Maybe you have a hdfs-site.xml lying around somewhere?
Great!
Jon / Shane: Thanks for handling this.
stream.map(record => (keyFunction(record), record))
For future reference, this question should go to the user list, not dev
list.
+dev list
Hi Dirceu,
The answer to whether throwing an exception is better or null is better
depends on your use case. If you are debugging and want to find bugs with
your program, you might prefer throwing an exception. However, if you are
running on a large real-world dataset (i.e. data is dirty) and your query
can take a while (e.g. 30 mins), you then might prefer the system to just
assign null values to the dirty data that could lead to runtime exceptions,
because otherwise you could be spending days just to clean your data.
Postgres throws exceptions here, but I think that's mainly because it is
used for OLTP, and in those cases queries are short-running. Most other
analytic databases I believe just return null. The best we can do is to
provide a config option to indicate behavior for exception handling.
Please vote on releasing the following candidate as Apache Spark version
1.5.1. The vote is open until Sun, Sep 27, 2015 at 10:00 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 1.5.1
[ ] -1 Do not release this package because ...
The release fixes 81 known issues in Spark 1.5.0, listed here:
http://s.apache.org/spark-1.5.1
The tag to be voted on is v1.5.1-rc1:
https://github.com/apache/spark/commit/4df97937dbf68a9868de58408b9be0bf87dbbb94
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.1-rc1-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release (1.5.1) can be found at:
*https://repository.apache.org/content/repositories/orgapachespark-1148/
*
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.1-rc1-docs/
=======================================
How can I help test this release?
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.
================================================
What justifies a -1 vote for this release?
================================================
-1 vote should occur for regressions from Spark 1.5.0. Bugs already present
in 1.5.0 will not block this release.
===============================================================
What should happen to JIRA tickets still targeting 1.5.1?
===============================================================
Please target 1.5.2 or 1.6.0.
Richard,
Thanks for bringing this up and this is a great point. Let's start another
thread for it so we don't hijack the release thread.
I forked a new thread for this. Please discuss NOTICE file related things
there so it doesn't hijack this thread.
I'm going to +1 this myself. Tested on my laptop.
Thanks everybody for voting. I'm going to close the vote now. The vote
passes with 17 +1 votes and 1 -1 vote. I will work on packaging this asap.
+1:
Reynold Xin*
Sean Owen
Hossein Falaki
Xiangrui Meng*
Krishna Sankar
Joseph Bradley
Sean McNamara*
Luciano Resende
Doug Balog
Eugene Zhulenev
Vaquar Khan
Tom Graves*
Michael Armbrust*
Marcelo Vanzin
Robin East
Yin Huai
Suresh Thalamati
0:
-1:
Richard Hillegas [see note]
Note: Richard Hillegas did say in a separate thread the issue he brought up
should not block the release. However, he did not explicitly amend his vote
so I'm including it as a -1 here.
Hi All,
Spark 1.5.1 is a maintenance release containing stability fixes. This
release is based on the branch-1.5 maintenance branch of Spark. We
*strongly recommend* all 1.5.0 users to upgrade to this release.
The full list of bug fixes is here: http://s.apache.org/spark-1.5.1
http://spark.apache.org/releases/spark-release-1-5-1.html
(note: it can take a few hours for everything to be propagated, so you
might get 404 on some download links, but everything should be in maven
central already)
Both work for me. It's possible maven.org is having problems with some
servers.
No, not yet.
You can write the data to local hdfs (or local disk) and just load it from
there.
I meant to say just copy everything to a local hdfs, and then don't use
caching ...
The current implementation of multiple count distinct in a single query is
very inferior in terms of performance and robustness, and it is also hard
to guarantee correctness of the implementation in some of the refactorings
for Tungsten. Supporting a better version of it is possible in the future,
but will take a lot of engineering efforts. Most other Hadoop-based SQL
systems (e.g. Hive, Impala) don't support this feature.
As a result, we are considering removing support for multiple count
distinct in a single query in the next Spark release (1.6). If you use this
feature, please reply to this email. Thanks.
Note that if you don't care about null values, it is relatively easy to
reconstruct a query using joins to support multiple distincts.
To provide more context, if we do remove this feature, the following SQL
query would throw an AnalysisException:
select count(distinct colA), count(distinct colB) from foo;
The following should still work:
select count(distinct colA) from foo;
The following should also work:
select count(distinct colA, colB) from foo;
Adding user list too.
You probably saw that in a presentation given by the drill team. You should
check with them on that.
Hi Spark devs,
It is hard to track everything going on in Spark with so many pull requests
and JIRA tickets. Below are 4 major improvements that will likely be in
Spark 1.6. We have already done prototyping for all of them, and want
feedback on their design.
1. SPARK-9850 Adaptive query execution in Spark
https://issues.apache.org/jira/browse/SPARK-9850
Historically, query planning is done using statistics before the execution
begins. However, the query engine doesn't always have perfect statistics
before execution, especially on fresh data with blackbox UDFs. SPARK-9850
proposes adaptively picking executions plans based on runtime statistics.
2. SPARK-9999 Type-safe API on top of Catalyst/DataFrame
https://issues.apache.org/jira/browse/SPARK-9999
A high level, typed API built on top of Catalyst/DataFrames. This API can
leverage all the work in Project Tungsten to have more robust and efficient
execution (including memory management, code generation, and query
optimization). This API is tentatively named Dataset (i.e. the last D in
RDD).
3. SPARK-10000 Unified memory management (by consolidating cache and
execution memory)
https://issues.apache.org/jira/browse/SPARK-10000
Spark statically divides memory into multiple fractions. The two biggest
ones are cache (aka storage) memory and execution memory. Out of the box,
only 16% of the memory is used for execution. That is to say, if an
application is not using caching, it is wasting majority of the memory
resource with the default configuration. SPARK-10000 proposes a solution to
dynamically allocate memory for these two fractions, and should improve
performance for large workloads without configuration tuning.
4. SPARK-10810 Improved session management in Spark SQL and DataFrames
https://issues.apache.org/jira/browse/SPARK-10810
Session isolation & management is important in SQL query engines. In Spark,
this is slightly more complicated since users can also use DataFrames
interactively beyond SQL. SPARK-10810 implements session management for
both SQL's JDBC/ODBC servers, as well as the DataFrame API.
Most of this work has been merged already in this pull request:
https://github.com/apache/spark/pull/8909
+dev list
Can you reply to this email and provide us with reasons why you disable it?
Thanks.
That could break a lot of applications. In particular, a lot of input data
sources (csv, json) don't have clean schema, and can have duplicate column
names.
For the case of join, maybe a better solution is to ask the left/right
prefix/suffix in the user code, similar to what Pandas does.
Thanks for sharing, Bill.
I just saw this happening:
[info] - map stage submission with multiple shared stages and failures ***
FAILED *** (566 milliseconds)
[info]   java.lang.IndexOutOfBoundsException: 2
[info]   at
scala.collection.mutable.ResizableArray$class.apply(ResizableArray.scala:43)
[info]   at scala.collection.mutable.ArrayBuffer.apply(ArrayBuffer.scala:47)
[info]   at
org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$49.apply$mcV$sp(DAGSchedulerSuite.scala:1667)
[info]   at
org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$49.apply(DAGSchedulerSuite.scala:1634)
[info]   at
org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$49.apply(DAGSchedulerSuite.scala:1634)
[info]   at
org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]   at
org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
Can somebody take a look at it?
Thanks.
How big is your driver heap size? And any reason why you'd need 200k map
and 200k reduce tasks?
Jerry - I think that's been fixed in 1.5.1. Do you still see it?
With Jerry's permission, sending this back to the dev list to close the
loop.
I think we made a mistake and forgot to register the function in the
registry:
https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/FunctionRegistry.scala
Do you mind submitting a pull request to fix this? Should be an one line
change. I filed a ticket to track this:
https://issues.apache.org/jira/browse/SPARK-11233
Is this still Mesos fine grained mode?
Please vote on releasing the following candidate as Apache Spark
version 1.5.2. The vote is open until Wed Oct 28, 2015 at 08:00 UTC and
passes if a majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 1.5.2
[ ] -1 Do not release this package because ...
The release fixes 51 known issues in Spark 1.5.1, listed here:
http://s.apache.org/spark-1.5.2
The tag to be voted on is v1.5.2-rc1:
https://github.com/apache/spark/releases/tag/v1.5.2-rc1
The release files, including signatures, digests, etc. can be found at:
*http://people.apache.org/~pwendell/spark-releases/spark-1.5.2-rc1-bin/
*
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
- as version 1.5.2-rc1:
https://repository.apache.org/content/repositories/orgapachespark-1151
- as version 1.5.2:
https://repository.apache.org/content/repositories/orgapachespark-1150
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-v1.5.2-rc1-docs/
=======================================
How can I help test this release?
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.
================================================
What justifies a -1 vote for this release?
================================================
-1 vote should occur for regressions from Spark 1.5.1. Bugs already present
in 1.5.1 will not block this release.
===============================================================
What should happen to JIRA tickets still targeting 1.5.2?
===============================================================
Please target 1.5.3 or 1.6.0.
Yup looks like I missed that. I will build a new one.
Try
count(distinct columnane)
In SQL distinct is not part of the function name.
What are you trying to accomplish to pickle a Spark DataFrame? If your
dataset is large, it doesn't make much sense to pickle it. If your dataset
is small, maybe it's best to just pickle a Pandas dataframe.
I don't think these are bugs. The SQL standard for average is "avg", not
"mean". Similarly, a distinct count is supposed to be written as
"count(distinct col)", not "countDistinct(col)".
We can, however, make "mean" an alias for "avg" to improve compatibility
between DataFrame and SQL.
No those are just functions for the DataFrame programming API.
Thanks for reporting it, Sjoerd. You might have a different version of
Janino brought in from somewhere else.
This should fix your problem: https://github.com/apache/spark/pull/9372
Can you give it a try?
I don't think there is any special handling w.r.t. Tachyon vs in-heap
caching. As a matter of fact, I think the current offheap caching
implementation is pretty bad, because:
1. There is no namespace sharing in offheap mode
2. Similar to 1, you cannot recover the offheap memory once Spark driver or
executor crashes
3. It requires expensive serialization to go offheap
It would've been simpler to just treat Tachyon as a normal file system, and
use it that way to at least satisfy 1 and 2, and also substantially
simplify the internals.
It is lost unfortunately (although can be recomputed automatically).
It is quite a bit of work. Again, I think going through the file system API
is more ideal in the long run. In the long run, I don't even think the
current offheap API makes much sense, and we should consider just removing
it to simplify things.
Please vote on releasing the following candidate as Apache Spark version
1.5.2. The vote is open until Sat Nov 7, 2015 at 00:00 UTC and passes if a
majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 1.5.2
[ ] -1 Do not release this package because ...
The release fixes 59 known issues in Spark 1.5.1, listed here:
http://s.apache.org/spark-1.5.2
The tag to be voted on is v1.5.2-rc2:
https://github.com/apache/spark/releases/tag/v1.5.2-rc2
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.2-rc2-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
- as version 1.5.2-rc2:
https://repository.apache.org/content/repositories/orgapachespark-1153
- as version 1.5.2:
https://repository.apache.org/content/repositories/orgapachespark-1152
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.2-rc2-docs/
=======================================
How can I help test this release?
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.
================================================
What justifies a -1 vote for this release?
================================================
-1 vote should occur for regressions from Spark 1.5.1. Bugs already present
in 1.5.1 will not block this release.
If you are using Spark with Mesos fine grained mode, can you please respond
to this email explaining why you use it over the coarse grained mode?
Thanks.
Soren,
If I understand how Mesos works correctly, even the fine grained mode keeps
the JVMs around?
It's not supported yet, and not sure if there is a ticket for it. I don't
think there is anything fundamentally hard here either.
Can you use the broadcast hint?
e.g.
df1.join(broadcast(df2))
the broadcast function is in org.apache.spark.sql.functions
If your data came from RDDs (i.e. not a file system based data source), and
you don't want to cache, then no ....
+1 myself too
Hi All,
Spark 1.5.2 is a maintenance release containing stability fixes. This
release is based on the branch-1.5 maintenance branch of Spark. We
*strongly recommend* all 1.5.x users to upgrade to this release.
The full list of bug fixes is here: http://s.apache.org/spark-1.5.2
http://spark.apache.org/releases/spark-release-1-5-2.html
We should consider this for Spark 2.0.
Thanks for the email. Can you explain what the difference is between this
and existing formats such as Parquet/ORC?
It only runs tests that are impacted by the change. E.g. if you only modify
SQL, it won't run the core or streaming tests.
Yes. And those have been happening too.
I actually tried to build a binary for 1.4.2 and wanted to start voting,
but there was an issue with the release script that failed the jenkins job.
Would be great to kick off a 1.4.2 release.
In the interim, you can just build it off branch-1.4 if you want.
It depends on what the next operator is. If the next operator is just an
aggregation, then no, the hash join won't write anything to disk. It will
just stream the data through to the next operator. If the next operator is
shuffle (exchange), then yes.
This (updates) is something we are going to think about in the next release
or two.
It's a completely different path.
No it does not -- although it'd benefit from some of the work to make
shuffle more robust.
What do you mean by starts delay scheduling? Are you saying it is no longer
doing local reads?
If that's the case you can increase the spark.locality.read timeout.
Have you looked into https://github.com/harsha2010/magellan ?
I proposed dropping support for Hadoop 1.x in the Spark 2.0 email, and I
think everybody is for that.
https://issues.apache.org/jira/browse/SPARK-11807
Sean suggested also dropping support for Hadoop 2.2, 2.3, and 2.4. That is
to say, keep only Hadoop 2.6 and greater.
What are the community's thoughts on that?
OK I'm not exactly asking for a vote here :)
I don't think we should look at it from only maintenance point of view --
because in that case the answer is clearly supporting as few versions as
possible (or just rm -rf spark source code and call it a day). It is a
tradeoff between the number of users impacted and the maintenance burden.
So a few questions for those more familiar with Hadoop:
1. Can Hadoop 2.6 client read Hadoop 2.4 / 2.3?
2. If the answer to 1 is yes, are there known, major issues with backward
compatibility?
3. Can Hadoop 2.6+ YARN work on older versions of YARN clusters?
4. (for Hadoop vendors) When did/will support for Hadoop 2.4 and below
stop? To what extent do you care about running Spark on older Hadoop
clusters.
The experimental tag is intended for user facing APIs. It has nothing to do
with internal dependencies.
I think for most jobs the bottleneck isn't in writing shuffle data to disk,
since shuffle data needs to be "shuffled" and sent across the network.
You can always use a ramdisk yourself. Requiring ramdisk by default would
significantly complicate configuration and platform portability.
We need to first implement subtract and intersect in Spark SQL natively first (i.e. add physical operator for them rather than using RDD.subtract/intersect).
Then it should be pretty easy to do that, given it is just about injecting the right exchange operators.
---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
For IntelliJ I think the free version is sufficient for Spark development.
Not aware of any jira ticket, but it does sound like a great idea.
No, since the signature itself limits it.
Thanks for looking at this. Is it worth fixing? Is there a risk (although
small) that the re-import would break other things?
Most of those are done and I don't know how often people search JIRAs by
date across projects.
+1
Tested some dataframe operations on my Mac.
+1
+1
We are just removing Spark's dependency on Akka. It has nothing to do with
whether user applications can use Akka or not. As a matter of fact, by
removing the Akka dependency from Spark, it becomes easier for user
applications to use Akka, because there is no more dependency conflict.
For more information, see https://issues.apache.org/jira/browse/SPARK-5293
If you use hadoopFile (or textFile) and have the same file on the same path
in every node, I suspect it might just work.
+Herman
Is this coming from the newly merged Hive parser?
OK to close the loop - this thread has nothing to do with Spark?
No there is not. I actually manually closed them to cut down the number of
open pull requests. Feel free to reopen individual ones.
Hi Mridul,
Thanks for the message. All you said made sense to me.
It can definitely be frustrating when one of your pull requests got
accidentally closed, and that's why we often ping the original authors to
close them. However, this doesn't always work because the original authors
might be inactive for some old pull requests, and sorting these out of 400+
open pull requests can be pretty tough.
Regardless of whether pull requests should be long-lived or short-lived,
closing a pull request does not wipe any of its content. All the changes
and their associated reviews are kept there, and it is trivial to re-open
(one click of a button).
I also agree with your point that the "date of open" is not the best metric
to look at here. Inactivity for a certain period is a much better metric to
use in the future.
Does anybody here care about us dropping support for Python 2.6 in Spark
2.0?
Python 2.6 is ancient, and is pretty slow in many aspects (e.g. json
parsing) when compared with Python 2.7. Some libraries that Spark depend on
stopped supporting 2.6. We can still convince the library maintainers to
support 2.6, but it will be extra work. I'm curious if anybody still uses
Python 2.6 to run Spark.
Thanks.
Can you file a JIRA ticket? Thanks.
The URL is issues.apache.org/jira/browse/SPARK
How big of a deal this use case is in a heterogeneous endianness
environment? If we do want to fix it, we should do it when right before
Spark shuffles data to minimize performance penalty, i.e. turn big-endian
encoded data into little-indian encoded data before it goes on the wire.
This is a pretty involved change and given other things that might break
across heterogeneous endianness environments, I am not sure if it is high
priority enough to even warrant review bandwidth right now.
If you need it, just copy it over to your own package. That's probably the
safest option.
We've dropped Hadoop 1.x support in Spark 2.0.
There is also a proposal to drop Hadoop 2.2 and 2.3, i.e. the minimal
Hadoop version we support would be Hadoop 2.4. The main advantage is then
we'd be able to focus our Jenkins resources (and the associated maintenance
of Jenkins) to create builds for Hadoop 2.6/2.7. It is my understanding
that all Hadoop vendors have moved away from 2.2/2.3, but there might be
some users that are on these older versions.
What do you think about this idea?
It is not necessary if you are using bucketing available in Spark 2.0. For
partitioning, it is still necessary because we do not assume each partition
is small, and as a result there is no guarantee all the records for a
partition end up in a single Spark task partition.
The original email was asking about data partitioning (Hive style) for
files, not in memory caching.
There are no major obstacles, just a million tiny obstacles that would take
forever to fix.
As of Spark 2.0 (not yet released), Spark does not use Akka any more.
See https://issues.apache.org/jira/browse/SPARK-5293
FYI - I just merged Josh's pull request to switch to Scala 2.11 as the
default build.
https://github.com/apache/spark/pull/10608
Yes they do. We haven't dropped 2.10 support yet. There are too many 2.10
active deployments out there.
Both of these make sense to add. Can you submit a pull request?
Not 100% sure what's going on, but you can try wiping your local ivy2 and
maven cache.
Yea I'm not sure what's going on either. You can just run the unit tests
through "build/sbt sql/test" without running mima.
Matt,
Thanks for the email. Are you just asking whether it should work, or
reporting they don't work?
Internally, the way we track physical data distribution should make the
scenarios described work. If it doesn't, we should make them work.
Can you create a pull request? It is difficult to know what's going on.
I'm not 100% sure I understand your question, but yes, Spark (both the RDD
API and SQL/DataFrame) does partial aggregation.
FYI,
Call for presentations is now open for Spark Summit. The event will take
place on June 6-8 in San Francisco. Submissions are welcome across a
variety of Spark-related topics, including applications, development, data
science, business value, spark ecosystem and research. Please submit by
February 29th to be considered.
Link to submission: https://spark-summit.org/2016/
Looks like a bug. I'm also not sure whether we support Option yet. (If not,
we should definitely support that in 2.0.)
Can you file a JIRA ticket?
Github introduced a new feature today that allows projects to define
templates for pull requests. I pushed a very simple template to the
repository:
https://github.com/apache/spark/blob/master/.github/PULL_REQUEST_TEMPLATE
Over time I think we can see how this works and perhaps add a small
checklist to the pull request template so contributors are reminded every
time they submit a pull request the important things to do in a pull
request (e.g. having proper tests).
## What changes were proposed in this pull request?
(Please fill in changes proposed in this fix)
## How was the this patch tested?
(Please explain how this patch was tested. E.g. unit tests, integration
tests, manual tests)
(If this patch involves UI changes, please attach a screenshot; otherwise,
remove this)
I think Matei was referring to the Kafka direct streaming source added in
2015.
You are correct and we should document that.
Any suggestions on where we should document this? In DoubleType and
FloatType?
Thanks for the email.
Don't make it that complicated. We just want to simplify the common cases
(e.g. csv/parquet), and don't need this to work for everything out there.
+ Joey
We think this is worth doing. Are you interested in submitting a pull
request?
InterpretedOrdering I think is internal, so it's not useful to add it there
for public docs. We should definitely add a small section to the guide.
Thanks for keeping an eye on this!
We usually publish to a staging maven repo hosted by the ASF (not maven
central).
Yes, we don't want to clutter maven central.
The staging repo is included in the release candidate voting thread.
See the following for an example:
http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-6-0-RC1-td15424.html
That's the just transform function in DataFrame
  /**
   * Concise syntax for chaining custom transformations.
   * {{{
   *   def featurize(ds: DataFrame) = ...
   *
   *   df
   *     .transform(featurize)
   *     .transform(...)
   * }}}
   * @since 1.6.0
   */
  def transform[U](t: DataFrame => DataFrame): DataFrame = t(this)
Note that while this is great for chaining, having *only* this leads to
pretty bad user experience, especially in interactive analysis when it is
not obvious what operations are available.
Just want to send a reminder in case people don't know about it. If you are
working on (or with, using) Spark, consider submitting your work to Spark
Summit, coming up in June in San Francisco.
https://spark-summit.org/2016/call-for-presentations/
Cheers.
When we first introduced Dataset in 1.6 as an experimental API, we wanted
to merge Dataset/DataFrame but couldn't because we didn't want to break the
pre-existing DataFrame API (e.g. map function should return Dataset, rather
than RDD). In Spark 2.0, one of the main API changes is to merge DataFrame
and Dataset.
Conceptually, DataFrame is just a Dataset[Row]. In practice, there are two
ways to implement this:
Option 1. Make DataFrame a type alias for Dataset[Row]
Option 2. DataFrame as a concrete class that extends Dataset[Row]
I'm wondering what you think about this. The pros and cons I can think of
are:
Option 1. Make DataFrame a type alias for Dataset[Row]
+ Cleaner conceptually, especially in Scala. It will be very clear what
libraries or applications need to do, and we won't see type mismatches
(e.g. a function expects DataFrame, but user is passing in Dataset[Row]
+ A lot less code
- Breaks source compatibility for the DataFrame API in Java, and binary
compatibility for Scala/Java
Option 2. DataFrame as a concrete class that extends Dataset[Row]
The pros/cons are basically the inverse of Option 1.
+ In most cases, can maintain source compatibility for the DataFrame API in
Java, and binary compatibility for Scala/Java
- A lot more code (1000+ loc)
- Less cleaner, and can be confusing when users pass in a Dataset[Row] into
a function that expects a DataFrame
The concerns are mostly with Scala/Java. For Python, it is very easy to
maintain source compatibility for both (there is no concept of binary
compatibility), and for R, we are only supporting the DataFrame operations
anyway because that's more familiar interface for R users outside of Spark.
It might make sense, but this option seems to carry all the cons of Option
2, and yet doesn't provide compatibility for Java?
Yes - and that's why source compatibility is broken.
Note that it is not just a "convenience" thing. Conceptually DataFrame is a
Dataset[Row], and for some developers it is more natural to think about
"DataFrame" rather than "Dataset[Row]".
If we were in C++, DataFrame would've been a type alias for Dataset[Row]
too, and some methods would return DataFrame (e.g. sql method).
Dropping Kafka list since this is about a slightly different topic.
Every time we expose the API of a 3rd party application as a public Spark
API has caused some problems down the road. This goes from Hadoop, Tachyon,
Kafka, to Guava. Most of these are used for input/output.
The good thing is that in Spark 2.0 we are removing most of those
exposures, and in the new DataFrame/Dataset API we are providing an unified
input/output API for end-users so the internals of the 3rd party
dependencies are no longer exposed directly to users. Unfortunately, some
Spark APIs still depend on Hadoop.
It is important to keep this in mind as we develop Spark. We should avoid
to the best degree possible exposing other projects' APIs for the long term
stability of Spark APIs.
Thanks for the email. This sounds great in theory, but might run into two
major problems:
1. Need to support 4+ programming languages (SQL, Python, Java, Scala)
2. API stability (both backward and forward)
Can you file a JIRA ticket?
But sometimes you might have skew and almost all the result data are in one
or a few tasks though.
That seems reasonable, but it seems pretty unfair to the HPC setup in which
the master is reading all the data. Basically you can make HPC perform
infinitely worse by just adding more modes to Spark.
How big of a deal is this though? If I am reading your email correctly,
either way this job will fail. You simply want it to fail earlier in the
executor side, rather than collecting it and fail on the driver side?
Is the suggestion just to use a different config (and maybe fallback to
appid) in order to publish metrics? Seems reasonable.
SQL is very common and even some business analysts learn them. Scala and
Python are great, but the easiest language to use is often the languages a
user already knows. And for a lot of users, that is SQL.
What do you mean by consistent? Throughout the life cycle of an app, the
executors can come and go and as a result really has no consistency. Do you
just need it for a specific job?
+1 (binding)
Thanks - I've fixed it and it will go out next time we update. For future
reference, you can email directly support@databricks.com for this.
Again - thanks for reporting this.
The doc fix was merged in 1.6.1, so it will get updated automatically once
we push the 1.6.1 docs.
Hi Hamel,
Sorry for the slow reply. Do you mind writing down the thoughts in a
document, with API sketches? I think all the devils are in the details of
the API for this one.
If we can design an API that is type-safe, supports all languages, and also
can be stable, then it sounds like a great idea.
Hi all,
As most of you know, we are doing some API changes in Spark 2.0 to prepare
Spark for the future, and a lot of focus there are on DataFrames and
Datasets. We wrote a high level API doc and uploaded it to JIRA last week,
but I don't think a lot of people monitor JIRA as closely. Here's a link:
https://issues.apache.org/jira/browse/SPARK-13485
Please check it out and comment on it if you have any feedback. Cheers.
You just want to be able to replicate hot cached blocks right?
Isn't this just specified by the user?
Yea we are going to tighten a lot of class' visibility. A lot of APIs were
made experimental, developer, or public for no good reason in the past.
Many of them (not Logging in this case) are tied to the internal
implementation of Spark at a specific time, and no longer make sense given
the project's evolution.
SparkConf is not a singleton.
However, SparkContext in almost all cases are. So you can use
SparkContext.getOrCreate().getConf
Maybe just add a watch dog thread and closed the connection upon some
timeout?
Any objections? Please articulate your use case. SparkEnv is a weird one
because it was documented as "private" but not marked as so in class
visibility.
 * NOTE: This is not intended for external use. This is exposed for Shark
and may be made private
 *       in a future release.
I do see Hive using it to get the config variable. That can probably be
propagated through other means.
There is no way to really know that, because users might run queries at any
given point.
BTW why can't your threads be just daemon threads?
We probably should have the alias. Is this still a problem on master
branch?
Thanks for initiating this discussion. I merged the pull request because it
was unblocking another major piece of work for Spark 2.0: not requiring
assembly jars, which is arguably a lot more important than sources that are
less frequently used. I take full responsibility for that.
I think it's inaccurate to call them "backend" because it makes these
things sound a lot more serious, when in reality they are a bunch of
connectors to less frequently used streaming data sources (e.g. mqtt,
flume). But that's not that important here.
Another important factor is that over time, with the development of
structure streaming, we'd provide a new API for streaming sources that
unifies the way to connect arbitrary sources, and as a result all of these
sources need to be rewritten anyway. This is similar to the RDD ->
DataFrame transition for data sources, although it was initially painful,
but in the long run provides much better experience for end-users because
they only need to learn a single API for all sources, and it becomes
trivial to transition from one source to another, without actually
impacting business logic.
So the truth is that in the long run, the existing connectors will be
replaced by new ones, and they have been causing minor issues here and
there in the code base. Now issues like these are never black and white. By
moving them out, we'd require users to at least change the maven coordinate
in their build file (although things can still be made binary and source
compatible). So I made the call and asked the contributor to keep Kafka and
Kinesis in, because those are the most widely used (and could be more
contentious), and move everything else out.
I have personally done enough data sources or 3rd party packages for Spark
on github that I can setup a github repo with CI and maven publishing in
just under an hour. I do not expect a lot of changes to these packages
because the APIs have been fairly stable. So the thing I was optimizing for
was to minimize the time we need to spent on these packages given the
(expected) low activity and the shift to focus on structured streaming, and
also minimize the chance to break user apps to provide the best user
experience.
Github repo seems the simplest choice to me. I also made another decision
to provide separate repos (and thus issue trackers) on github for these
packages. The reason is that these connectors have very disjoint
communities. For example, the community that care about mqtt is likely very
different from the community that care about akka. It is much easier to
track all of these.
Logistics wise -- things are still in flux. I think it'd make a lot of
sense to give existing Spark committers (or at least the ones that have
contributed to streaming) write access to the github repos. IMHO, it is not
in any of the major Spark contributing organizations' strategic interest to
"own" these projects, especially considering most of the activities will
switch to structured streaming.
If one really feels strongly that we should go through all the overhead to
setup an ASF subproject for these modules that won't work with the new
structured streaming, and want to spearhead to setup separate repos
(preferably one subproject per connector), CI, separate JIRA, governance,
READMEs, voting, we can discuss that. Until then, I'd keep the github
option open because IMHO it is what works the best for end users (including
discoverability, issue tracking, release publishing, ...).
Hi Wes,
Thanks for the email. It is difficult to generalize without seeing a lot
more cases, but the boolean issue is simply a query analysis rule.
I can see us having a config option that changes analysis to match more
Python/R like, which changes the behavior of implicit type coercion and
allows boolean to integral automatically.
Hi Wes,
I agree it is difficult to do this design case by case, but what I was
pointing out was "it is difficult to generalize without seeing a lot more
cases".
I do think we need to see a lot of these cases and then make a call. My
intuition is that we can just have config options that control behavior,
similar to what a lot of relational databases do. The good thing is that
Spark data frames are very abstracted away from the underlying execution so
a lot of the behaviors can be controlled just by analysis rules.
Can you see if this is the patch that caused the issue?
https://github.com/apache/spark/pull/11737
About a year ago we decided to drop Java 6 support in Spark 1.5. I am
wondering if we should also just drop Java 7 support in Spark 2.0 (i.e.
Spark 2.0 would require Java 8 to run).
Oracle ended public updates for JDK 7 in one year ago (Apr 2015), and
removed public downloads for JDK 7 in July 2015. In the past I've actually
been against dropping Java 8, but today I ran into an issue with the new
Dataset API not working well with Java 8 lambdas, and that changed my
opinion on this.
I've been thinking more about this issue today and also talked with a lot
people offline to gather feedback, and I actually think the pros outweighs
the cons, for the following reasons (in some rough order of importance):
1. It is complicated to test how well Spark APIs work for Java lambdas if
we support Java 7. Jenkins machines need to have both Java 7 and Java 8
installed and we must run through a set of test suites in 7, and then the
lambda tests in Java 8. This complicates build environments/scripts, and
makes them less robust. Without good testing infrastructure, I have no
confidence in building good APIs for Java 8.
2. Dataset/DataFrame performance will be between 1x to 10x slower in Java
7. The primary APIs we want users to use in Spark 2.x are
Dataset/DataFrame, and this impacts pretty much everything from machine
learning to structured streaming. We have made great progress in their
performance through extensive use of code generation. (In many dimensions
Spark 2.0 with DataFrames/Datasets looks more like a compiler than a
MapReduce or query engine.) These optimizations don't work well in Java 7
due to broken code cache flushing. This problem has been fixed by Oracle in
Java 8. In addition, Java 8 comes with better support for Unsafe and SIMD.
3. Scala 2.12 will come out soon, and we will want to add support for that.
Scala 2.12 only works on Java 8. If we do support Java 7, we'd have a
fairly complicated compatibility matrix and testing infrastructure.
4. There are libraries that I've looked into in the past that support only
Java 8. This is more common in high performance libraries such as Aeron (a
messaging library). Having to support Java 7 means we are not able to use
these. It is not that big of a deal right now, but will become increasingly
more difficult as we optimize performance.
The downside of not supporting Java 7 is also obvious. Some organizations
are stuck with Java 7, and they wouldn't be able to use Spark 2.0 without
upgrading Java.
One other benefit that I didn't mention is that we'd be able to use Java
8's Optional class to replace our built-in Optional.
I actually talked quite a bit today with an engineer on the scala compiler
team tonight and the scala 2.10 + java 8 combo should be ok. The latest
Scala 2.10 release should have all the important fixes that are needed for
Java 8.
Yes
Actually it's *way* harder to upgrade Scala from 2.10 to 2.11, than
upgrading the JVM runtime from 7 to 8, because Scala 2.10 and 2.11 are not
binary compatible, whereas JVM 7 and 8 are binary compatible except certain
esoteric cases.
If you want to go down that route, you should also ask somebody who has had
experience managing a large organization's applications and try to update
Scala version.
No - it is too painful to develop a jdbc/odbc driver.
They work.
Since my original email, I've talked to a lot more users and looked at what
various environments support. It is true that a lot of enterprises, and
even some technology companies, are still using Java 7. One thing is that
up until this date, users still can't install openjdk 8 on Ubuntu by
default. I see that as an indication that it is too early to drop Java 7.
Looking at the timeline, JDK release a major new version roughly every 3
years. We dropped Java 6 support one year ago, so from a timeline point of
view we would be very aggressive here if we were to drop Java 7 support in
Spark 2.0.
Note that not dropping Java 7 support now doesn't mean we have to support
Java 7 throughout Spark 2.x. We dropped Java 6 support in Spark 1.5, even
though Spark 1.0 started with Java 6.
In terms of testing, Josh has actually improved our test infra so now we
would run the Java 8 tests: https://github.com/apache/spark/pull/12073
Nezih,
Have you had a chance to figure out why this is happening?
BTW do you still see this when dynamic allocation is off?
pyspark and R
What do you mean? The Jenkins build for Spark uses 2.11 and also builds the
thrift server.
Hi Sean,
See http://www.oracle.com/technetwork/java/eol-135779.html
Java 7 hasn't EOLed yet. If you look at support you can get from Oracle,
it's actually goes to 2019. And you can even get more support after that.
Spark has always maintained great backward compatibility with other
systems, way beyond what vendors typically support. For example, we
supported Hadoop 1.x all the way until Spark 1.6 (basically the last
release), while all the vendors have dropped support for them already.
Putting my Databricks hat on we actually only support Java 8, but I think
it would be great to still support Java 7 in the upstream release for some
larger deployments. I like the idea of deprecating or at least strongly
encouraging people to update.
The driver has the data and wouldn't need to rerun.
I think the main things are API things that we need to get right.
- Implement essential DDLs https://issues.apache.org/jira/browse/SPARK-14118
 this blocks the next one
- Merge HiveContext and SQLContext and create SparkSession
https://issues.apache.org/jira/browse/SPARK-13485
- Separate out local linear algebra as a standalone module without Spark
dependency https://issues.apache.org/jira/browse/SPARK-13944
- Run Spark without assembly jars (mostly done?)
Probably realistic to have it in ~ 2 weeks.
Responses inline
We prefer the latter. I don't think there are performance differences
though.
It depends on how big the change is -- massive style updates can make
backports harder.
I think this was added a long time ago by me in order to make certain
things work for Shark (good old times ...). You are probably right that by
now some apps depend on the fact that this is inheritable, and changing
that could break them in weird ways.
Do you mind documenting this, and also add a test case?
Can you post the generated code?
df.queryExecution.debug.codeGen()
(Or something similar to that)
Hi Hyukjin,
Thanks for asking.
For 1 the change is almost always better.
For 2 it depends on the context. In general if the type is not obvious, it
helps readability to explicitly declare them.
For 3 again it depends on context.
So while it is a good idea to change 1 to reflect a more consistent code
base (and maybe we should codify it), it is almost always a bad idea to
change 2 and 3 just for the sake of changing them.
Your understanding is correct. If the driver is stuck in GC, then during
that period it cannot schedule any tasks.
I haven't looked closely at this, but I think your proposal makes sense.
The problem with this is that we might introduce event time based trigger
in the future, and then it would be more confusing...
Nope. It is unclear whether they would be useful enough or not. But when
designing APIs we always need to anticipate future changes.
We have hit a new high in open pull requests: 469 today. While we can
certainly get more review bandwidth, many of these are old and still open
for other reasons. Some are stale because the original authors have become
busy and inactive, and some others are stale because the committers are not
sure whether the patch would be useful, but have not rejected the patch
explicitly. We can cut down the signal to noise ratio by closing pull
requests that have been inactive for greater than 30 days, with a nice
message. I just checked and this would close ~ half of the pull requests.
For example:
"Thank you for creating this pull request. Since this pull request has been
inactive for 30 days, we are automatically closing it. Closing the pull
request does not remove it from history and will retain all the diff and
review comments. If you have the bandwidth and would like to continue
pushing this forward, please reopen it. Thanks again!"
Josh's pull request  on rpc
exception handling got me to think ...
In my experience, there have been a few things related exceptions that
created a lot of trouble for us in production debugging:
1. Some exception is thrown, but is caught by some try/catch that does not
do any logging nor rethrow.
2. Some exception is thrown, but is caught by some try/catch that does not
do any logging, but do rethrow. But the original exception is now masked.
2. Multiple exceptions are logged at different places close to each other,
but we don't know whether they are caused by the same problem or not.
To mitigate some of the above, here's an idea ...
(1) Create a common root class for all the exceptions (e.g. call it
SparkException) used in Spark. We should make sure every time we catch an
exception from a 3rd party library, we rethrow them as SparkException (a
lot of places already do that). In SparkException's constructor, log the
exception and the stacktrace.
(2) SparkException has a monotonically increasing ID, and this ID appears
in the exception error message (say at the end).
I think (1) will eliminate most of the cases that an exception gets
swallowed. The main downside I can think of is we might log an exception
multiple times. However, I'd argue exceptions should be rare, and it is not
that big of a deal to log them twice or three times. The unique ID (2) can
help us correlate exceptions if they appear multiple times.
Thoughts?
Cody,
Thanks for commenting. "inactive" here means no code push nor comments. So
any "ping" would actually keep the pr in the open queue. Getting
auto-closed also by no means indicate the pull request can't be reopened.
Part of it is how difficult it is to automate this. We can build a perfect
engine with a lot of rules that understand everything. But the more
complicated rules we need, the more unlikely for any of these to happen. So
I'd rather do this and create a nice enough message to tell contributors
sometimes mistake happen but the cost to reopen is approximately zero (i.e.
click a button on the pull request).
The cost of "reopen" is close to zero, because it is just clicking a
button. I think you were referring to the cost of closing the pull request,
and you are assuming people look at the pull requests that have been
inactive for a long time. That seems equally likely (or unlikely) as
committers looking at the recently closed pull requests.
In either case, most pull requests are scanned through by us when they are
first open, and if they are important enough, usually they get merged
quickly or a target version is set in JIRA. We can definitely improve that
by making it more explicit.
That's not the only one. For example, the hash shuffle manager has been off
by default since Spark 1.2, and we'd like to remove it in 2.0:
https://github.com/apache/spark/pull/12423
How difficult it is to just change the package name to say v2?
Yea I re-read the email again. It'd work in this case.
The bigger problem is that it is much easier to maintain backward
compatibility rather than dictating forward compatibility. For example, as
Marcin said, if we come up with a slightly different shuffle layout to
improve shuffle performance, we wouldn't be able to do that if we want to
allow Spark 1.6 shuffle service to read something generated by Spark 2.1.
Got it. So Mark is pushing for "best-effort" support.
IIUC, the reason for that PR is that they found the string comparison to
increase the size in large shuffles. Maybe we should add the ability to
support the short name to Spark 1.6.2?
Yea in general I feel examples that bring in a large amount of dependencies
should be outside Spark.
Ted - what's the "bq" thing? Are you using some 3rd party (e.g. Atlassian)
syntax? They are not being rendered in email.
I talked to Lianhui offline and he said it is not that big of a deal to
revert the patch.
I pushed a commit to close all but the last one.
Usually no - but sortByKey does because it needs the range boundary to be
built in order to have the RDD. It is a long standing problem that's
unfortunately very difficult to solve without breaking the RDD API.
In DataFrame/Dataset we don't have this issue though.
Hi devs,
Three weeks ago I mentioned on the dev list creating branch-2.0
(effectively "feature freeze") in 2 - 3 weeks. I've just created Spark's
branch-2.0 to form the basis of the 2.0 release. We have closed ~ 1700
issues. That's huge progress, and we should celebrate that.
Compared with past releases when we cut the release branch, we have way
fewer open issues. In the past we usually have 200 - 400 open issues when
we cut the release branch. As of today we have less than 100 open issues
for 2.0.0, and among these 14 critical and 2 blocker (Jersey dependency
upgrade and some remaining issues in separating out local linear algebra
library).
What does this mean for committers?
0. For patches that should go into Spark 2.0.0, make sure you also merge
them into not just master, but also branch-2.0.
1. In the next couple of days, sheppard some of the more important,
straggler pull requests in.
2. Switch the focus from new feature development to bug fixes, stability
improvements, finalizing API tweaks, and documentation.
3. Experimental features (e.g. R, structured streaming) can continue to be
developed, provided that the changes don't impact the non-experimental
features.
4. We should become increasingly conservative as time goes on, even for
experimental features.
5. Please un-target or re-target issues if they don't make sense for 2.0.
We should burn # issues down to ~ 0 by the time we have a release candidate.
7. If possible, reach out to users and start testing branch-2.0 to find
bugs. The more testing we can do on real workloads before the release, the
less bugs we will find in the actual Spark 2.0 release.
Definitely looks like a bug.
Ted - are you looking at this?
Thanks, Shane!
Probably not. Want to submit a pull request?
Adding Kay
We currently have three levels of interface annotation:
- unannotated: stable public API
- DeveloperApi: A lower-level, unstable API intended for developers.
- Experimental: An experimental user-facing API.
After using this annotation for ~ 2 years, I would like to propose the
following changes:
1. Require explicitly annotation for public APIs. This reduces the chance
of us accidentally exposing private APIs.
2. Separate interface annotation into two components: one that describes
intended audience, and the other that describes stability, similar to what
Hadoop does. This allows us to define "low level" APIs that are stable,
e.g. the data source API (I'd argue this is the API that should be more
stable than end-user-facing APIs).
InterfaceAudience: Public, Developer
InterfaceStability: Stable, Experimental
What do you think?
That's true. I think I want to differentiate end-user vs developer. Public
isn't the best word. Maybe EndUser?
Hm LimitedPrivate is not the intention. Those APIs (e.g. data source) are
by no means private. They are just lower level APIs whose intended audience
is library developers, not end users.
It might be best to fix this with fallback first, and then figure out how
we can do it more intelligently.
It seems like the problem here is that we are not using unique names
for mapelements_isNull?
Hi,
In the past the Apache Spark community have created preview packages (not
official releases) and used those as opportunities to ask community members
to test the upcoming versions of Apache Spark. Several people in the Apache
community have suggested we conduct votes for these preview packages and
turn them into formal releases by the Apache foundation's standard. Preview
releases are not meant to be functional, i.e. they can and highly likely
will contain critical bugs or documentation errors, but we will be able to
post them to the project's website to get wider feedback. They should
satisfy the legal requirements of Apache's release policy (
http://www.apache.org/dev/release.html) such as having proper licenses.
Please vote on releasing the following candidate as Apache Spark version
2.0.0-preview. The vote is open until Friday, May 20, 2015 at 11:00 PM PDT
and passes if a majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 2.0.0-preview
[ ] -1 Do not release this package because ...
To learn more about Apache Spark, please see http://spark.apache.org/
The tag to be voted on is 2.0.0-preview
(8f5a04b6299e3a47aca13cbb40e72344c0114860)
The release files, including signatures, digests, etc. can be found at:
http://home.apache.org/~pwendell/spark-releases/spark-2.0.0-preview-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The documentation corresponding to this release can be found at:
http://home.apache.org/~pwendell/spark-releases/spark-2.0.0-preview-docs/
The list of resolved issues are:
https://issues.apache.org/jira/browse/SPARK-15351?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%202.0.0
If you are a Spark user, you can help us test this release by taking an
existing Apache Spark workload and running on this candidate, then
reporting any regressions.
Michael,
You can comment on the JIRA ticket and tag some of the more active
contributors to Mesos/SparkR. That said, committers are focusing on bug
fixes and stability fixes at the moment for 2.0, and it's unlikely at this
point for new features to get in. It can of course target the next release.
Users would be able to run this already with the 3 lines of code you
supplied right? In general there are a lot of methods already on
SparkContext and we lean towards the more conservative side in introducing
new API variants.
Note that this is something we are doing automatically in Spark SQL for
file sources (Dataset/DataFrame).
It is different isn't it. Whole text files returns one element per file,
whereas combined inout format is similar to coalescing partitions to bin
pack into a certain size.
I filed https://issues.apache.org/jira/browse/SPARK-15441
Andres - this is great feedback. Let me think about it a little bit more
and reply later.
It's probably due to GC.
Kubernetes itself already has facilities for http proxy, doesn't it?
Thanks, Koert. This is great. Please keep them coming.
In the past the Spark community have created preview packages (not official
releases) and used those as opportunities to ask community members to test
the upcoming versions of Apache Spark. Several people in the Apache
community have suggested we conduct votes for these preview packages and
turn them into formal releases by the Apache foundation's standard. This is
a result of that.
Note that this preview release should contain almost all the new features
that will be in Apache Spark 2.0.0. However, it is not meant to be
functional, i.e. the preview release contain critical bugs and
documentation errors. To download, please see the bottom of this web page:
http://spark.apache.org/downloads.html
For the list of known issues, please see
https://issues.apache.org/jira/browse/SPARK-15520?jql=project%20%3D%20SPARK%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)%20AND%20%22Target%20Version%2Fs%22%20%3D%202.0.0
Note 1: The current download link goes directly to dist.apache.org. Once
all the files are propagated to all mirrors, I will update the link to link
to the mirror selector instead.
Note 2: This is the first time we are publishing official, voted preview
releases. Would love to hear feedback.
Yup I have published it to maven. Will post the link in a bit.
One thing is that for developers, it might be better to use the nightly
snapshot because that one probably has fewer bugs than the preview one.
The maven artifacts can be found at
https://repository.apache.org/content/repositories/orgapachespark-1182/
But really for people on this list, it might be better to go straight to
the nightly snapshots.
https://repository.apache.org/content/groups/snapshots/org/apache/spark/
Created JIRA ticket: https://issues.apache.org/jira/browse/SPARK-15533
@Koert - Please keep API feedback coming. One thing - in the future, can
you send api feedbacks to the dev@ list instead of user@?
I think the risk is everybody starts following this, then this will be
unmanageable, given the size of the number of organizations involved.
The two main labels that we actually use are starter + releasenotes.
It's probably a good idea to have the vertica dialect too, since it doesn't
seem like it'd be too difficult to maintain. It is not going to be as
performant as the native Vertica data source, but is going to be much
lighter weight.
Here's a ticket: https://issues.apache.org/jira/browse/SPARK-15598
They should get printed if you turn on debug level logging.
I think your understanding is correct. There will be external libraries
that allow you to use the twitter streaming dstream API even in 2.0 though.
They are here ain't they?
https://repository.apache.org/content/repositories/orgapachespark-1182/
Did you mean publishing them to maven central? My understanding is that
publishing to maven central isn't a required step of doing theses. This
might be a good opportunity to discuss that. My thought is that it is since
Maven central is immutable, and the purposes of the preview releases are to
get people to test it early on in preparation for the actual release, it
might be better to not publish preview releases to maven central. Users
testing with preview releases can just use the temporary repository above.
To play devil's advocate, previews are technically not RCs. They are
actually voted releases.
Hi Sean,
(writing this email with my Apache hat on only and not Databricks hat)
The preview release is available here:
http://spark.apache.org/downloads.html (there is an entire section
dedicated to it and also there is a news link to it on the right).
Again, I think this is a good opportunity to define what a release should
contain. Based on
http://www.apache.org/dev/release.html#where-do-releases-go
"In addition to the distribution directory, project that use Maven or a
related build tool sometimes place their releases on repository.apache.org
beside some convenience binaries. The distribution directory is required,
while the repository system is an optional convenience."
So I'm reading it as that maven publication is not necessary. My
understanding is that the general community (beyond who follows the dev
list) should understand that preview is not a stable release, and we as the
PMC should set expectations accordingly. Developers that can test the
preview releases tend to be more savvy and are comfortable on the bleeding
edge. It is actually fairly easy for them to add a maven repo. Now reading
the page I realized no where on the page did we mention the temporary maven
repo. I will fix that.
I think it'd be pretty bad if preview releases in anyway become "default
version", because they are unstable and contain a lot of blocker bugs.
So my concrete proposal is:
1. Separate (officially voted) releases into normal and preview.
2. On the download page, have two sections. One listing the normal
releases, and the other listing preview releases.
3. Everywhere we mention preview releases, include the proper disclaimer
e.g. "This preview is not a stable release in terms of either API or
functionality, but it is meant to give the community early access to try
the code that will become Spark 2.0."
4. Publish normal releases to maven central, and preview releases only to
the staging maven repo. But of course we should include the temporary maven
repo for preview releases on the download page.
One thing we can do is to do monthly milestone releases, similar to other
projects (e.g. Scala).
So we can have Apache Spark 2.1.0-M1, Apache Spark 2.1.0-M2.
Congratulations, Yanbo!
The bahir one was a good argument actually. I just clicked the button to
push it into Maven central.
Thanks for fixing it!
Thanks for the email. How do you deal with in-memory state that reference
the classes? This can happen in both streaming and caching in RDD and
temporary view creation in SQL.
Take a look at the implementation of typed sum/avg:
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/expressions/scalalang/typed.scala
You can implement a typed max/min.
Due to type erasure they have no difference, although watch out for Scala
tuple serialization.
Yes you can :)
Did you try this on master?
Thanks for the email. Things like this (and bugs) are exactly the reason
the preview releases exist. It seems like enough people have run into
problem with this one that maybe we should just bring it back for backward
compatibility.
You just need to run normal packaging and all the scripts are now setup to
run without the assembly jars.
It's been a while and we have accumulated quite a few bug fixes in
branch-1.6. I'm thinking about cutting 1.6.2 rc this week. Any patches
somebody want to get in last minute?
On a related note, I'm thinking about cutting 2.0.0 rc this week too. I
looked at the 60 unresolved tickets and almost all of them look like they
can be retargeted are are just some doc updates. I'm going to be more
aggressive and pushing individual people about resolving those, in case
this drags on forever.
Are you running Spark on YARN, Mesos, Standalone? For all of them you can
make the Hive dependency just part of your application, and then you can
manage this pretty easily.
You should be fine in 1.6 onward. Count distinct doesn't require data to
fit in memory there.
Please vote on releasing the following candidate as Apache Spark version
1.6.2!
The vote is open until Sunday, June 19, 2016 at 22:00 PDT and passes if a
majority of at least 3+1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 1.6.2
[ ] -1 Do not release this package because ...
The tag to be voted on is v1.6.2-rc1
(4168d9c94a9564f6b3e62f5d669acde13a7c7cf6)
The release files, including signatures, digests, etc. can be found at:
https://home.apache.org/~pwendell/spark-releases/spark-1.6.2-rc1-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1184
The documentation corresponding to this release can be found at:
https://home.apache.org/~pwendell/spark-releases/spark-1.6.2-rc1-docs/
=======================================
== How can I help test this release? ==
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 1.6.1.
================================================
== What justifies a -1 vote for this release? ==
================================================
This is a maintenance release in the 1.6.x series.  Bugs already present in
1.6.1, missing features, or bugs related to new features will not
necessarily block this release.
Cody has graciously worked on a new connector for dstream for Kafka 0.10.
Can people that use Kafka test this connector out? The patch is at
https://github.com/apache/spark/pull/11863
Although we have stopped merging new features into branch-2.0, this
connector is very decoupled from rest of Spark and we might be able to put
this into 2.0.1 (or 2.0.0 if everything works out).
Thanks.
Please go for it!
Looks like that's resolved now.
I will wait till Sunday to cut rc2 to give people more time to find issues
with rc1.
Thanks for the kind words, Krishna! Please keep the feedback coming.
While we wait on the resolution of the test issue, I've created rc2.
This vote is now canceled in favor of rc2.
Please vote on releasing the following candidate as Apache Spark version
1.6.2. The vote is open until Wednesday, June 22, 2016 at 22:00 PDT and
passes if a majority of at least 3+1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 1.6.2
[ ] -1 Do not release this package because ...
The tag to be voted on is v1.6.2-rc2
(54b1121f351f056d6b67d2bb4efe0d553c0f7482)
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.2-rc2-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1186/
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.2-rc2-docs/
=======================================
== How can I help test this release? ==
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 1.6.1.
================================================
== What justifies a -1 vote for this release? ==
================================================
This is a maintenance release in the 1.6.x series.  Bugs already present in
1.6.1, missing features, or bugs related to new features will not
necessarily block this release.
Please vote on releasing the following candidate as Apache Spark version
2.0.0. The vote is open until Friday, June 24, 2016 at 19:00 PDT and passes
if a majority of at least 3+1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 2.0.0
[ ] -1 Do not release this package because ...
The tag to be voted on is v2.0.0-rc1
(0c66ca41afade6db73c9aeddd5aed6e5dcea90df).
This release candidate resolves ~2400 issues:
https://s.apache.org/spark-2.0.0-rc1-jira
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc1-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1187/
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc1-docs/
=======================================
== How can I help test this release? ==
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 1.x.
================================================
== What justifies a -1 vote for this release? ==
================================================
Critical bugs impacting major functionalities.
Bugs already present in 1.x, missing features, or bugs related to new
features will not necessarily block this release. Note that historically
Spark documentation has been published on the website separately from the
main release so we do not need to block the release due to documentation
errors either.
SPARK-12818 is about building a bloom filter on existing data. It has
nothing to do with the ORC bloom filter, which can be used to do predicate
pushdown.
+1 myself
Alex - if you have access to a windows box, can you fix the issue? I'm not
sure how many Spark contributors have windows boxes.
Let me look into this...
Yup this is bad. Can you create a JIRA ticket too?
We are happy to announce the availability of Spark 1.6.2! This maintenance
release includes fixes across several areas of Spark. You can find the list
of changes here: https://s.apache.org/spark-1.6.2
And download the release here: http://spark.apache.org/downloads.html
Filed infra ticket: https://issues.apache.org/jira/browse/INFRA-12185
If people want this to happen, please go comment on the INFRA ticket:
https://issues.apache.org/jira/browse/INFRA-12185
Otherwise it will probably be dropped.
Which version are you using here? If the underlying files change,
technically we should go through optimization again.
Perhaps the real "fix" is to figure out why is logical plan creation so
slow for 700 columns.
drop user@spark and keep only dev@
This is something great to figure out, if you have time. Two things that
would be great to try:
1. See how this works on Spark 2.0.
2. If it is slow, try the following:
org.apache.spark.sql.catalyst.rules.RuleExecutor.resetTime()
// run your query
org.apache.spark.sql.catalyst.rules.RuleExecutor.dumpTimeSpent()
And report back where the time are spent if possible. Thanks!
Multiple instances of test runs are usually running in parallel, so they
would need to bind to different ports.
There isn't one pre-made, but the default works out OK. The main thing
you'd need to update are spacing changes for function argument indentation
and import ordering.
Because in that case you cannot merge anything meant for 2.1 until 2.0 is
released.
Thanks, Koert, for the great email. They are all great points.
We should probably create an umbrella JIRA for easier tracking.
This seems like a Scala compiler bug.
Please consider this vote canceled and I will work on another RC soon.
Jacek,
This is definitely not necessary, but I wouldn't waste cycles "fixing"
things like this when they have virtually zero impact. Perhaps next time we
update this code we can "fix" it.
Also can you comment on the pull request directly?
You can file it here: https://issues.scala-lang.org/secure/Dashboard.jspa
Perhaps "bug" is not the right word, but "limitation". println accepts a
single argument of type Any and returns Unit, and it appears that Scala
fails to infer the correct overloaded method in this case.
  def println() = Console.println()
  def println(x: Any) = Console.println(x)
Please vote on releasing the following candidate as Apache Spark version
2.0.0. The vote is open until Friday, July 8, 2016 at 23:00 PDT and passes
if a majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 2.0.0
[ ] -1 Do not release this package because ...
The tag to be voted on is v2.0.0-rc2
(4a55b2326c8cf50f772907a8b73fd5e7b3d1aa06).
This release candidate resolves ~2500 issues:
https://s.apache.org/spark-2.0.0-jira
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc2-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1189/
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc2-docs/
=================================
How can I help test this release?
=================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 1.x.
==========================================
What justifies a -1 vote for this release?
==========================================
Critical bugs impacting major functionalities.
Bugs already present in 1.x, missing features, or bugs related to new
features will not necessarily block this release. Note that historically
Spark documentation has been published on the website separately from the
main release so we do not need to block the release due to documentation
errors either.
See https://issues.apache.org/jira/browse/SPARK-16390
I think last time I tried I had some trouble releasing it because the
release scripts no longer work with branch-1.4. You can build from the
branch yourself, but it might be better to upgrade to the later versions.
Yes definitely.
When using native data sources (e.g. Parquet, ORC, JSON, ...), partitions
are automatically merged so they would add up to a specific size,
configurable by spark.sql.files.maxPartitionBytes.
spark.sql.files.openCostInBytes is used to specify the cost of each "file".
That is, an empty file will be considered to have at
least spark.sql.files.openCostInBytes bytes.
You can look into its source code:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala
I just bumped master branch version to 2.1.0-SNAPSHOT
https://github.com/apache/spark/commit/ffcb6e055a28f36208ed058a42df09c154555332
We used to have a problem with binary compatibility check not having the
2.0.0 base version in Maven (because 2.0.0 hasn't been released yet) but I
figured out a way yesterday to work around it.
It's related to this apparently:
https://issues.apache.org/jira/servicedesk/customer/portal/1/INFRA-12055
Also Java serialization isn't great for cross platform compatibility.
Platform as a general word, eg language platforms, OS, different JVM
versions, different JVM vendors, different Spark versions...
It's related to
https://issues.apache.org/jira/servicedesk/agent/INFRA/issue/INFRA-12055
Please vote on releasing the following candidate as Apache Spark version
2.0.0. The vote is open until Sunday, July 17, 2016 at 12:00 PDT and passes
if a majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 2.0.0
[ ] -1 Do not release this package because ...
The tag to be voted on is v2.0.0-rc4
(e5f8c1117e0c48499f54d62b556bc693435afae0).
This release candidate resolves ~2500 issues:
https://s.apache.org/spark-2.0.0-jira
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc4-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
*https://repository.apache.org/content/repositories/orgapachespark-1192/
*
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc4-docs/
=================================
How can I help test this release?
=================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 1.x.
==========================================
What justifies a -1 vote for this release?
==========================================
Critical bugs impacting major functionalities.
Bugs already present in 1.x, missing features, or bugs related to new
features will not necessarily block this release. Note that historically
Spark documentation has been published on the website separately from the
main release so we do not need to block the release due to documentation
errors either.
Note: There was a mistake made during "rc3" preparation, and as a result
there is no "rc3", but only "rc4".
This vote is cancelled in favor of rc4.
So the mesos issue (SPARK-16522) Michael is investigating. Unless it is
actually failing Mesos, I wouldn't call it a blocker.
The file path thing Alex brought up (SPARK-15899) only impacts test cases
on Windows. I think it's important to fix, but definitely not a blocker
either.
For the doc changes (SPARK-16553) I will republish the docs, but I won't
build another rc unless this vote does not go through, since docs are
published separately from release artifacts, as outlined in the original
email.
Updated documentation at
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc4-docs-updated/
I just retargeted SPARK-16011 to 2.1.
Good idea.
https://github.com/apache/spark/pull/14252
Please vote on releasing the following candidate as Apache Spark version
2.0.0. The vote is open until Friday, July 22, 2016 at 20:00 PDT and passes
if a majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 2.0.0
[ ] -1 Do not release this package because ...
The tag to be voted on is v2.0.0-rc5
(13650fc58e1fcf2cf2a26ba11c819185ae1acc1f).
This release candidate resolves ~2500 issues:
https://s.apache.org/spark-2.0.0-jira
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc5-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1195/
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc5-docs/
=================================
How can I help test this release?
=================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 1.x.
==========================================
What justifies a -1 vote for this release?
==========================================
Critical bugs impacting major functionalities.
Bugs already present in 1.x, missing features, or bugs related to new
features will not necessarily block this release. Note that historically
Spark documentation has been published on the website separately from the
main release so we do not need to block the release due to documentation
errors either.
This vote is cancelled in favor of rc5.
+1
The vote has passed with the following +1 votes and no -1 votes. I will
work on packaging the new release next week.
+1
Reynold Xin*
Sean Owen*
Shivaram Venkataraman*
Jonathan Kelly
Joseph E. Gonzalez*
Krishna Sankar
Dongjoon Hyun
Ricardo Almeida
Joseph Bradley*
Matei Zaharia*
Luciano Resende
Holden Karau
Michael Armbrust*
Felix Cheung
Suresh Thalamati
Kousuke Saruta
Xiao Li
* binding votes
The presentation at Spark Summit SF was probably referring to Structured
Streaming. The existing Spark Streaming (dstream) in Spark 2.0 has the same
production stability level as Spark 1.6. There is also Kafka 0.10 support
in dstream.
DataFrame can do cartesian joins.
Hi all,
Apache Spark 2.0.0 is the first release of Spark 2.x line. It includes
2500+ patches from 300+ contributors.
To download Spark 2.0, head over to the download page:
http://spark.apache.org/downloads.html
To view the release notes:
http://spark.apache.org/releases/spark-release-2-0-0.html
(note: it can take a few hours for everything to be propagated, so you
might get 404 on some download links.  If you see any issues with the
release notes or webpage *please contact me directly, off-list*)
Nice! Thanks!
We don't.
Please send a pull request to update the doc. Thanks.
I'd use the new SQLQueryTestSuite. Test cases defined in sql files.
We should work on a 2.0.1 release soon, since we have found couple critical
bugs in 2.0.0. Are there any critical bugs outstanding that we should
address in 2.0.1?
Does this problem still exist on today's master/branch-2.0?
SPARK-16550 was merged. It might be fixed already.
Looks like I'm general people like it. Next step is for somebody to take
the lead and implement it.
Tom do you have cycles to do this?
This is great!
A lot of people have been pinging me on github and email directly and
expect instant reply. Just FYI I'm on vacation for two weeks with limited
internet access.
The UDF is a black box so Spark can't know what it is dealing with. There
are simple cases in which we can analyze the UDFs byte code and infer what
it is doing, but it is pretty difficult to do in general.
There is a package called scala.
Yea but the earlier email was asking they were introduced in the first
place.
This is definitely useful, but in reality it might be very difficult to do.
They should be compatible.
What else do you expect to get? A non-zero hash value?
It can technically be any constant.
They are valid, especially in partition pruning.
2.0.1 is definitely coming soon.  Was going to tag a rc yesterday but ran
into some issue. I will try to do it early next week for rc.
I'm working on packaging 2.0.1 rc but encountered a problem: R doc fails to
build. Can somebody take a look at the issue ASAP?
** knitting documentation of write.parquet
** knitting documentation of write.text
** knitting documentation of year
~/workspace/spark-release-docs/spark/R
~/workspace/spark-release-docs/spark/R
processing file: sparkr-vignettes.Rmd
  |
  |                                                                 |   0%
  |
  |.                                                                |   1%
   inline R code fragments
  |
  |.                                                                |   2%
label: unnamed-chunk-1 (with options)
List of 1
 $ message: logi FALSE
Loading required package: methods
Attaching package: 'SparkR'
The following objects are masked from 'package:stats':
    cov, filter, lag, na.omit, predict, sd, var, window
The following objects are masked from 'package:base':
    as.data.frame, colnames, colnames<-, drop, intersect, rank,
    rbind, sample, subset, summary, transform, union
  |
  |..                                                               |   3%
  ordinary text without R code
  |
  |..                                                               |   4%
label: unnamed-chunk-2 (with options)
List of 1
 $ message: logi FALSE
Spark package found in SPARK_HOME:
/home/jenkins/workspace/spark-release-docs/spark
Error: Could not find or load main class org.apache.spark.launcher.Main
Quitting from lines 30-31 (sparkr-vignettes.Rmd)
Error in sparkR.sparkContext(master, appName, sparkHome, sparkConfigMap,  :
  JVM is not ready after 10 seconds
Calls: render ... eval -> eval -> sparkR.session -> sparkR.sparkContext
Execution halted
jekyll 2.5.3 | Error:  R doc generation failed
Deleting credential directory
/home/jenkins/workspace/spark-release-docs/spark-utils/new-release-scripts/jenkins/jenkins-credentials-IXCkuX6w
Build step 'Execute shell' marked build as failure
[WS-CLEANUP] Deleting project workspace...[WS-CLEANUP] done
Finished: FAILURE
Did you try the proposed fix? Would be good to know whether it fixes the
issue.
Please vote on releasing the following candidate as Apache Spark version
2.0.1. The vote is open until Sunday, Sep 25, 2016 at 23:59 PDT and passes
if a majority of at least 3+1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 2.0.1
[ ] -1 Do not release this package because ...
The tag to be voted on is v2.0.1-rc2
(04141ad49806a48afccc236b699827997142bd57)
This release candidate resolves 284 issues:
https://s.apache.org/spark-2.0.1-jira
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc2-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1199
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc2-docs/
Q: How can I help test this release?
A: If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 2.0.0.
Q: What justifies a -1 vote for this release?
A: This is a maintenance release in the 2.0.x series.  Bugs already present
in 2.0.0, missing features, or bugs related to new features will not
necessarily block this release.
Q: What happened to 2.0.1 RC1?
A: There was an issue with RC1 R documentation during release candidate
preparation. As a result, rc1 was canceled before a vote was called.
Hi all,
The R API documentation version error was reported in a separate thread.
I've built a release candidate (RC3) and will send out a new vote email in
a bit.
Please vote on releasing the following candidate as Apache Spark version
2.0.1. The vote is open until Tue, Sep 27, 2016 at 15:30 PDT and passes if
a majority of at least 3+1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 2.0.1
[ ] -1 Do not release this package because ...
The tag to be voted on is v2.0.1-rc3
(9d28cc10357a8afcfb2fa2e6eecb5c2cc2730d17)
This release candidate resolves 290 issues:
https://s.apache.org/spark-2.0.1-jira
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc3-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1201/
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc3-docs/
Q: How can I help test this release?
A: If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 2.0.0.
Q: What justifies a -1 vote for this release?
A: This is a maintenance release in the 2.0.x series.  Bugs already present
in 2.0.0, missing features, or bugs related to new features will not
necessarily block this release.
Q: What fix version should I use for patches merging into branch-2.0 from
now on?
A: Please mark the fix version as 2.0.2, rather than 2.0.1. If a new RC
(i.e. RC4) is cut, I will change the fix version of those patches to 2.0.1.
Thanks for reporting. I sent out an email for rc3 fixing the issue.
We have also automated the version number update for documentation pages so
this won't happen again in the future.
Seems fair & easy to support. Can somebody open a JIRA ticket and patch?
Yes - same thing with children in UnaryExpression, BinaryExpression.
Although I have to say the utility isn't that big here.
We are 2 months past releasing Spark 2.0.0, an important milestone for the
project. Spark 2.0.0 deviated (took 6 month from the regular release
cadence we had for the 1.x line, and we never explicitly discussed what the
release cadence should look like for 2.x. Thus this email.
During Spark 1.x, roughly every three months we make a new 1.x feature
release (e.g. 1.5.0 comes out three months after 1.4.0). Development
happened primarily in the first two months, and then a release branch was
cut at the end of month 2, and the last month was reserved for QA and
release preparation.
During 2.0.0 development, I really enjoyed the longer release cycle because
there was a lot of major changes happening and the longer time was critical
for thinking through architectural changes as well as API design. While I
don't expect the same degree of drastic changes in a 2.x feature release, I
do think it'd make sense to increase the length of release cycle so we can
make better designs.
My strawman proposal is to maintain a regular release cadence, as we did in
Spark 1.x, and increase the cycle from 3 months to 4 months. This
effectively gives us ~50% more time to develop (in reality it'd be slightly
less than 50% since longer dev time also means longer QA time). As for
maintenance releases, I think those should still be cut on-demand, similar
to Spark 1.x, but more aggressively.
To put this into perspective, 4-month cycle means we will release Spark
2.1.0 at the end of Nov or early Dec (and branch cut / code freeze at the
end of Oct).
I am curious what others think.
Actually I'm going to have to -1 the release myself. Sorry for crashing the
party, but I saw two super critical issues discovered in the last 2 days:
https://issues.apache.org/jira/browse/SPARK-17666  -- this would eventually
hang Spark when running against S3 (and many other storage systems)
https://issues.apache.org/jira/browse/SPARK-17673  -- this is a correctness
issue across all non-file data sources.
If we go ahead and release 2.0.1 based on this RC, we would need to cut
2.0.2 immediately.
So technically the vote has passed, but IMHO it does not make sense to
release this and then immediately release 2.0.2. I will work on a new RC
once SPARK-17666 and SPARK-17673 are fixed.
Please shout if you disagree.
Please vote on releasing the following candidate as Apache Spark version
2.0.1. The vote is open until Sat, Oct 1, 2016 at 20:00 PDT and passes if a
majority of at least 3+1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 2.0.1
[ ] -1 Do not release this package because ...
The tag to be voted on is v2.0.1-rc4
(933d2c1ea4e5f5c4ec8d375b5ccaa4577ba4be38)
This release candidate resolves 301 issues:
https://s.apache.org/spark-2.0.1-jira
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc4-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1203/
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc4-docs/
Q: How can I help test this release?
A: If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 2.0.0.
Q: What justifies a -1 vote for this release?
A: This is a maintenance release in the 2.0.x series.  Bugs already present
in 2.0.0, missing features, or bugs related to new features will not
necessarily block this release.
Q: What fix version should I use for patches merging into branch-2.0 from
now on?
A: Please mark the fix version as 2.0.2, rather than 2.0.1. If a new RC
(i.e. RC5) is cut, I will change the fix version of those patches to 2.0.1.
I will kick it off with my own +1.
Hi all,
Xiao Li, aka gatorsmile, has recently been elected as an Apache Spark
committer. Xiao has been a super active contributor to Spark SQL. Congrats
and welcome, Xiao!
- Reynold
We are happy to announce the availability of Spark 2.0.1!
Apache Spark 2.0.1 is a maintenance release containing 300 stability and
bug fixes. This release is based on the branch-2.0 maintenance branch of
Spark. We strongly recommend all 2.0.0 users to upgrade to this stable
release.
To download Apache Spark 2.0.1, visit http://spark.apache.org/downloads.html
We would like to acknowledge all community members for contributing patches
to this release.
They have been published yesterday, but can take a while to propagate.
There is now. Thanks for the email.
I think this is fairly important to do so I went ahead and created a PR for
the first mini step: https://github.com/apache/spark/pull/15374
Boris,
Thanks for the email, but this is not a list for soliciting job
applications. Please do not post any recruiting messages -- otherwise we
will ban your account.
I called Cody last night and talked about some of the topics in his email.
It became clear to me Cody genuinely cares about the project.
Some of the frustrations come from the success of the project itself
becoming very "hot", and it is difficult to get clarity from people who
don't dedicate all their time to Spark. In fact, it is in some ways similar
to scaling an engineering team in a successful startup: old processes that
worked well might not work so well when it gets to a certain size, cultures
can get diluted, building culture vs building process, etc.
I also really like to have a more visible process for larger changes,
especially major user facing API changes. Historically we upload design
docs for major changes, but it is not always consistent and difficult to
quality of the docs, due to the volunteering nature of the organization.
Some of the more concrete ideas we discussed focus on building a culture to
improve clarity:
- Process: Large changes should have design docs posted on JIRA. One thing
Cody and I didn't discuss but an idea that just came to me is we should
create a design doc template for the project and ask everybody to follow.
The design doc template should also explicitly list goals and non-goals, to
make design doc more consistent.
- Process: Email dev@ to solicit feedback. We have some this with some
changes, but again very inconsistent. Just posting something on JIRA isn't
sufficient, because there are simply too many JIRAs and the signal get lost
in the noise. While this is generally impossible to enforce because we
can't force all volunteers to conform to a process (or they might not even
be aware of this),  those who are more familiar with the project can help
by emailing the dev@ when they see something that hasn't been.
- Culture: The design doc author(s) should be open to feedback. A design
doc should serve as the base for discussion and is by no means the final
design. Of course, this does not mean the author has to accept every
feedback. They should also be comfortable accepting / rejecting ideas on
technical grounds.
- Process / Culture: For major ongoing projects, it can be useful to have
some monthly Google hangouts that are open to the world. I am actually not
sure how well this will work, because of the volunteering nature and we
need to adjust for timezones for people across the globe, but it seems
worth trying.
- Culture: Contributors (including committers) should be more direct in
setting expectations, including whether they are working on a specific
issue, whether they will be working on a specific issue, and whether an
issue or pr or jira should be rejected. Most people I know in this
community are nice and don't enjoy telling other people no, but it is often
more annoying to a contributor to not know anything than getting a no.
I like the lightweight proposal to add a SIP label.
During Spark 2.0 development, Tom (Graves) and I suggested using wiki to
track the list of major changes, but that never really materialized due to
the overhead. Adding a SIP label on major JIRAs and then link to them
prominently on the Spark website makes a lot of sense.
You can use the Dataset API -- it should solve this issue for case classes
that are not very complex.
Does Kafka 0.10 work on a Kafka 0.8/0.9 cluster?
Alright looks like there are quite a bit of support. We should wait to hear
from more people too.
To push this forward, Cody and I will be working together in the next
couple of weeks to come up with a concrete, detailed proposal on what this
entails, and then we can discuss this the specific proposal as well.
I think so (at least I think it is socially acceptable). Of course, use
good judgement here :)
Actually let's move the discussion to the JIRA ticket, given there is a
ticket.
Github already links to CONTRIBUTING.md. -- of course, a lot of people
ignore that. One thing we can do is to add an explicit link to the wiki
contributing page in the template (but note that even that introduces some
overhead for every pull request).
Aside from that, I am not sure if the other suggestions in the JIRA ticket
are necessary. For example, the issue with creating a pull request from one
branch to another is a problem, but it happens perhaps less than once a
week and is trivially closeable. Adding an explicit warning there will fix
some cases, but won't entirely eliminate the problem (because I'm sure a
lot of people still don't read the template), and will introduce another
overhead for everybody who submits the proper way.
You should probably check with DataStax who build the Cassandra connector
for Spark.
Hi all,
I tried to use the window function DataFrame API this weekend and found it
awkward to use, especially with respect to specifying frame boundaries. I
wrote down some options here and am curious your thoughts. If you have
suggestions on the API beyond what's already listed in the JIRA ticket, do
bring them up too.
Please comment on the JIRA ticket directly:
https://issues.apache.org/jira/browse/SPARK-17845
I've attached the content of the JIRA ticket here to save you a click:
ANSI SQL uses the following to specify the frame boundaries for window
functions:
ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWING
ROWS BETWEEN UNBOUNDED PRECEDING AND 3 PRECEDING
ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW
ROWS BETWEEN CURRENT ROW AND UNBOUNDED PRECEDING
ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING
In Spark's DataFrame API, we use integer values to indicate relative
position:
   - 0 means "CURRENT ROW"
   - -1 means "1 PRECEDING"
   - Long.MinValue means "UNBOUNDED PRECEDING"
   - Long.MaxValue to indicate "UNBOUNDED FOLLOWING"
// ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWINGWindow.rowsBetween(-3, +3)
// ROWS BETWEEN UNBOUNDED PRECEDING AND 3
PRECEDINGWindow.rowsBetween(Long.MinValue, -3)
// ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT
ROWWindow.rowsBetween(Long.MinValue, 0)
// ROWS BETWEEN CURRENT ROW AND UNBOUNDED
PRECEDINGWindow.rowsBetween(0, Long.MaxValue)
// ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED
FOLLOWINGWindow.rowsBetween(Long.MinValue, Long.MaxValue)
I think using numeric values to indicate relative positions is actually a
good idea, but the reliance on Long.MinValue and Long.MaxValue to indicate
unbounded ends is pretty confusing:
1. The API is not self-evident. There is no way for a new user to figure
out how to indicate an unbounded frame by looking at just the API. The user
has to read the doc to figure this out.
2. It is weird Long.MinValue or Long.MaxValue has some special meaning.
3. Different languages have different min/max values, e.g. in Python we use
-sys.maxsize and +sys.maxsize.
To make this API less confusing, we have a few options:
Option 1. Add the following (additional) methods:
// ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWINGWindow.rowsBetween(-3, +3)
// this one exists already// ROWS BETWEEN UNBOUNDED PRECEDING AND 3
PRECEDINGWindow.rowsBetweenUnboundedPrecedingAnd(-3)
// ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT
ROWWindow.rowsBetweenUnboundedPrecedingAndCurrentRow()
// ROWS BETWEEN CURRENT ROW AND UNBOUNDED
PRECEDINGWindow.rowsBetweenCurrentRowAndUnboundedFollowing()
// ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED
FOLLOWINGWindow.rowsBetweenUnboundedPrecedingAndUnboundedFollowing()
This is obviously very verbose, but is very similar to how these functions
are done in SQL, and is perhaps the most obvious to end users, especially
if they come from SQL background.
Option 2. Decouple the specification for frame begin and frame end into two
functions. Assume the boundary is unlimited unless specified.
// ROWS BETWEEN 3 PRECEDING AND 3 FOLLOWINGWindow.rowsFrom(-3).rowsTo(3)
// ROWS BETWEEN UNBOUNDED PRECEDING AND 3 PRECEDINGWindow.rowsTo(-3)
// ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT
ROWWindow.rowsToCurrent() or Window.rowsTo(0)
// ROWS BETWEEN CURRENT ROW AND UNBOUNDED
PRECEDINGWindow.rowsFromCurrent() or Window.rowsFrom(0)
// ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING// no need to specify
If we go with option 2, we should throw exceptions if users specify
multiple from's or to's. A variant of option 2 is to require explicitly
specification of begin/end even in the case of unbounded boundary, e.g.:
Window.rowsFromBeginning().rowsTo(-3)
or
Window.rowsFromUnboundedPreceding().rowsTo(-3)
How are they using it? Calling some main function directly?
I noticed today that our data types APIs (org.apache.spark.sql.types) are
actually DeveloperApis, which means they can be changed from one feature
release to another. In reality these APIs have been there since the
original introduction of the DataFrame API in Spark 1.3, and has not seen
any breaking changes since then. It makes more sense to mark them stable.
There are also a number of DataFrame related classes that have been
Experimental or DeveloperApi for eternity. I will be marking these stable
in the upcoming feature release (2.1).
Please shout if you disagree.
It actually does -- but do it through a really weird way.
UnaryNodeExec actually defines:
trait UnaryExecNode extends SparkPlan {
  def child: SparkPlan
  override final def children: Seq[SparkPlan] = child :: Nil
  override def outputPartitioning: Partitioning = child.outputPartitioning
}
I think this is very risky because preserving output partitioning should
not be a property of UnaryNodeExec (e.g. exchange). It would be better
(safer) to move the output partitioning definition into each of the
operator and remove it from UnaryExecNode.
Would you be interested in submitting the patch?
I took a look at all the public APIs we expose in o.a.spark.sql tonight,
and realized we still have a large number of APIs that are marked
experimental. Most of these haven't really changed, except in 2.0 we merged
DataFrame and Dataset. I think it's long overdue to mark them stable.
I'm tracking this via ticket:
https://issues.apache.org/jira/browse/SPARK-17900
*The list I've come up with to graduate are*:
Dataset/DataFrame
- functions, since 1.3
- ColumnName, since 1.3
- DataFrameNaFunctions, since 1.3.1
- DataFrameStatFunctions, since 1.4
- UserDefinedFunction, since 1.3
- UserDefinedAggregateFunction, since 1.5
- Window and WindowSpec, since 1.4
Data sources:
- DataSourceRegister, since 1.5
- RelationProvider, since 1.3
- SchemaRelationProvider, since 1.3
- CreatableRelationProvider, since 1.3
- BaseRelation, since 1.3
- TableScan, since 1.3
- PrunedScan, since 1.3
- PrunedFilteredScan, since 1.3
- InsertableRelation, since 1.3
*The list I think we should definitely keep experimental are*:
- CatalystScan in data source (tied to internal logical plans so it is not
stable by definition)
- all classes related to Structured streaming (introduced new in 2.0 and
will likely change)
*The ones that I'm not sure whether we should graduate are:*
Typed operations for Datasets, including:
- all typed methods on Dataset class
- KeyValueGroupedDataset
- o.a.s.sql.expressions.javalang.typed
- o.a.s.sql.expressions.scalalang.typed
- methods that return typed Dataset in SparkSession
Most of these were introduced in 1.6 and had gone through drastic changes
in 2.0. I think we should try very hard not to break them any more, but we
might still run into issues in the future that require changing these.
Let me know what you think.
It's been a while and we have fixed a few bugs in branch-1.6. I plan to cut
rc1 for 1.6.3 next week (just in time for Spark Summit Europe). Let me know
if there are specific issues that should be addressed before that. Thanks.
I took a look at the pull request for memory management and I actually
agree with the existing assessment that the patch is too big and risky to
port into an existing maintenance branch. Things that are backported are
low-risk patches that won't break existing applications on 1.6.x. This
patch is large, invasive, directly hooks into the very internals of Spark.
The chance of it breaking an existing working 1.6.x application is not low.
It is very difficult to give a general answer. We would need to discuss
each case. In general things that are trivially doable using existing APIs,
it is not a good idea to provide them, unless for compatibility with other
frameworks (e.g. Pandas).
Since 2.0.1, there have been a number of correctness fixes as well as some
nice improvements to the experimental structured streaming (notably basic
Kafka support). I'm thinking about cutting 2.0.2 later this week, before
Spark Summit Europe. Let me know if there are specific things (bug fixes)
you really want to merge into branch-2.0.
Cheers.
Please vote on releasing the following candidate as Apache Spark version
1.6.3. The vote is open until Thursday, Oct 20, 2016 at 18:00 PDT and
passes if a majority of at least 3+1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 1.6.3
[ ] -1 Do not release this package because ...
The tag to be voted on is v1.6.3-rc1
(7375bb0c825408ea010dcef31c0759cf94ffe5c2)
This release candidate addresses 50 JIRA tickets:
https://s.apache.org/spark-1.6.3-jira
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.3-rc1-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1205/
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.3-rc1-docs/
=======================================
== How can I help test this release?
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 1.6.2.
================================================
== What justifies a -1 vote for this release?
================================================
This is a maintenance release in the 1.6.x series.  Bugs already present in
1.6.2, missing features, or bugs related to new features will not
necessarily block this release.
For the contributing guide I think it makes more sense to put it in
apache/spark github, since that's where contributors start. I'd also link
to it from the website ...
FYI - Xiangrui submitted an amazing pull request to fix a long standing
issue with a lot of the nondeterministic expressions (rand, randn,
monotonically_increasing_id): https://github.com/apache/spark/pull/15567
Prior to this PR, we were using TaskContext.partitionId as the partition
index in initializing expressions. However, that is actually not a good
index to use in most cases, because it is the physical task's partition id
and does not always reflect the partition index at the time the RDD is
created (or in the Spark SQL physical plan). This makes a big difference
once there is a union or coalesce operation.
The "index" given by mapPartitionsWithIndex, on the other hand, does not
have this problem because it actually reflects the logical partition index
at the time the RDD is created.
When is it safe to use TaskContext.partitionId? It is safe at the very end
of a query plan (the root node), because there partitionId is guaranteed
based on the current implementation to be the same as the physical task
partition id.
For example, prior to Xiangrui's PR, the following query would return 2
rows, whereas the correct behavior should be 1 entry:
spark.range(1).selectExpr("rand(1)").union(spark.range(1)
.selectExpr("rand(1)")).distinct.show()
The reason it'd return 2 rows is because rand was using
TaskContext.partitionId as the per-partition seed, and as a result the two
sides of the union are using different seeds.
I'm starting to think we should deprecate the API and ban the use of it
within the project to be safe ...
Seems like a good new API to add?
This shouldn't be required anymore since Spark 2.0.
We can do the following concrete proposal:
1. Plan to remove support for Java 7 / Scala 2.10 in Spark 2.2.0 (Mar/Apr
2017).
2. In Spark 2.1.0 release, aggressively and explicitly announce the
deprecation of Java 7 / Scala 2.10 support.
(a) It should appear in release notes, documentations that mention how to
build Spark
(b) and a warning should be shown every time SparkContext is started using
Scala 2.10 or Java 7.
Greetings from Spark Summit Europe at Brussels.
Please vote on releasing the following candidate as Apache Spark version
2.0.2. The vote is open until Sun, Oct 30, 2016 at 00:30 PDT and passes if
a majority of at least 3+1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 2.0.2
[ ] -1 Do not release this package because ...
The tag to be voted on is v2.0.2-rc1
(1c2908eeb8890fdc91413a3f5bad2bb3d114db6c)
This release candidate resolves 75 issues:
https://s.apache.org/spark-2.0.2-jira
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.2-rc1-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1208/
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.2-rc1-docs/
Q: How can I help test this release?
A: If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 2.0.1.
Q: What justifies a -1 vote for this release?
A: This is a maintenance release in the 2.0.x series. Bugs already present
in 2.0.1, missing features, or bugs related to new features will not
necessarily block this release.
Q: What fix version should I use for patches merging into branch-2.0 from
now on?
A: Please mark the fix version as 2.0.3, rather than 2.0.2. If a new RC
(i.e. RC2) is cut, I will change the fix version of those patches to 2.0.2.
I created a JIRA ticket to track this:
https://issues.apache.org/jira/browse/SPARK-18138
Welcome!
This is the best guide to get started:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
Maybe just streaming or SS in GitHub?
Ryan want to submit a pull request?
I know there are a lot of people with experience on developing database
internals on this list. Please take a look at this proposal for a new,
simpler way to handle view canonicalization in Spark SQL:
https://issues.apache.org/jira/browse/SPARK-18209
It sounds much simpler than what we currently do in 2.0/2.1, but I'm not
sure if there are obvious holes that I missed.
Please vote on releasing the following candidate as Apache Spark version
2.0.2. The vote is open until Fri, Nov 4, 2016 at 22:00 PDT and passes if a
majority of at least 3+1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 2.0.2
[ ] -1 Do not release this package because ...
The tag to be voted on is v2.0.2-rc2
(a6abe1ee22141931614bf27a4f371c46d8379e33)
This release candidate resolves 84 issues:
https://s.apache.org/spark-2.0.2-jira
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.2-rc2-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1210/
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.2-rc2-docs/
Q: How can I help test this release?
A: If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 2.0.1.
Q: What justifies a -1 vote for this release?
A: This is a maintenance release in the 2.0.x series. Bugs already present
in 2.0.1, missing features, or bugs related to new features will not
necessarily block this release.
Q: What fix version should I use for patches merging into branch-2.0 from
now on?
A: Please mark the fix version as 2.0.3, rather than 2.0.2. If a new RC
(i.e. RC3) is cut, I will change the fix version of those patches to 2.0.2.
Vinayak,
Thanks for the email. This is really not the thread meant for reporting
existing regressions. It's best just commenting on the jira ticket and even
better submit a fix for it.
Hi all,
Following the release schedule as outlined in the wiki, I just created
branch-2.1 to form the basis of the 2.1 release. As of today we have less
than 50 open issues for 2.1.0. The next couple of weeks we as a community
should focus on testing and bug fixes and burn down the number of
outstanding tickets to 0. My general feeling looking at the last 3 months
of development is that this release as a whole focuses more than the past
on stability, bug fixes and internal refactoring (for cleaning up some of
the debts accumulated during 2.0 development).
What does this mean for committers?
1. For patches that should go into Spark 2.1.0, make sure you also merge
them into not just master, but also branch-2.1.
2. Switch the focus from new feature development to bug fixes and
documentation. For "new features" that already have high quality,
outstanding pull requests, shepard them in in the next couple of days.
3. Please un-target or re-target issues if they don't make sense for 2.1.
If a ticket is not "bug fix" and still has no high quality patch, it should
probably be retargeted to 2.2.
4. If possible, reach out to users and start testing branch-2.1 to find
bugs. The more testing we can do on real workloads before the release, the
less bugs we will find in the actual Spark 2.1 release.
Looks like there is an issue with Maven (likely just the test itself
though). We should look into it.
This vote is cancelled and I'm sending out a new vote for rc2 now.
Please vote on releasing the following candidate as Apache Spark version
1.6.3. The vote is open until Sat, Nov 5, 2016 at 18:00 PDT and passes if a
majority of at least 3+1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 1.6.3
[ ] -1 Do not release this package because ...
The tag to be voted on is v1.6.3-rc2
(1e860747458d74a4ccbd081103a0542a2367b14b)
This release candidate addresses 52 JIRA tickets:
https://s.apache.org/spark-1.6.3-jira
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.3-rc2-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1212/
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.3-rc2-docs/
=======================================
== How can I help test this release?
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 1.6.2.
================================================
== What justifies a -1 vote for this release?
================================================
This is a maintenance release in the 1.6.x series.  Bugs already present in
1.6.2, missing features, or bugs related to new features will not
necessarily block this release.
I will cut a new one once https://github.com/apache/spark/pull/15774 gets
in.
We are happy to announce the availability of Spark 1.6.3! This maintenance
release includes fixes across several areas of Spark and encourage users on
the 1.6.x line to upgrade to 1.6.3.
Head to the project's download page to download the new version:
http://spark.apache.org/downloads.html
Please vote on releasing the following candidate as Apache Spark version
2.0.2. The vote is open until Thu, Nov 10, 2016 at 22:00 PDT and passes if
a majority of at least 3+1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 2.0.2
[ ] -1 Do not release this package because ...
The tag to be voted on is v2.0.2-rc3
(584354eaac02531c9584188b143367ba694b0c34)
This release candidate resolves 84 issues:
https://s.apache.org/spark-2.0.2-jira
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.2-rc3-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1214/
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.2-rc3-docs/
Q: How can I help test this release?
A: If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 2.0.1.
Q: What justifies a -1 vote for this release?
A: This is a maintenance release in the 2.0.x series. Bugs already present
in 2.0.1, missing features, or bugs related to new features will not
necessarily block this release.
Q: What fix version should I use for patches merging into branch-2.0 from
now on?
A: Please mark the fix version as 2.0.3, rather than 2.0.2. If a new RC
(i.e. RC4) is cut, I will change the fix version of those patches to 2.0.2.
private[sql] has no impact in Java, and these functions are literally one
line of code. It's overkill to think about code duplication for functions
that simple.
Some places in Spark do use it:
mllib/src/main/scala/org/apache/spark/ml/classification/OneVsRest.scala:
 val models = Range(0, numClasses).par.map { index =>
sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/ScalaReflectionSuite.scala:
           (0 until 10).par.foreach { _ =>
sql/core/src/test/scala/org/apache/spark/sql/execution/SQLExecutionSuite.scala:
     (1 to 100).par.foreach { _ =>
sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala:
   (1 to 100).par.map { i =>
streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala:
 inputStreams.par.foreach(_.start())
streaming/src/main/scala/org/apache/spark/streaming/DStreamGraph.scala:
 inputStreams.par.foreach(_.stop())
Most of the usage are in tests, not the actual execution path. Parallel
collection is fairly complicated and difficult to manage (implicit thread
pools). It is good for more the basic thread management, but Spark itself
has much more sophisticated parallelization built-in.
I want to bring this discussion to the dev list to gather broader feedback,
as there have been some discussions that happened over multiple JIRA
tickets (SPARK-16026 ,
etc) and GitHub pull requests about what statistics to collect and how to
use them.
There are some basic statistics on columns that are obvious to use and we
don't need to debate these: estimated size (in bytes), row count, min, max,
number of nulls, number of distinct values, average column length, max
column length.
In addition, we want to be able to estimate selectivity for equality and
range predicates better, especially taking into account skewed values and
outliers.
Before I dive into the different options, let me first explain count-min
sketch: Count-min sketch is a common sketch algorithm that tracks frequency
counts. It has the following nice properties:
- sublinear space
- can be generated in one-pass in a streaming fashion
- can be incrementally maintained (i.e. for appending new data)
- it's already implemented in Spark
- more accurate for frequent values, and less accurate for less-frequent
values, i.e. it tracks skewed values well.
- easy to compute inner product, i.e. trivial to compute the count-min
sketch of a join given two count-min sketches of the join tables
Proposal 1 is is to use a combination of count-min sketch and equi-height
histograms. In this case, count-min sketch will be used for selectivity
estimation on equality predicates, and histogram will be used on range
predicates.
Proposal 2 is to just use count-min sketch on equality predicates, and then
simple selected_range / (max - min) will be used for range predicates. This
will be less accurate than using histogram, but simpler because we don't
need to collect histograms.
Proposal 3 is a variant of proposal 2, and takes into account that skewed
values can impact selectivity heavily. In 3, we track the list of heavy
hitters (HH, most frequent items) along with count-min sketch on the
column. Then:
- use count-min sketch on equality predicates
- for range predicates, estimatedFreq =  sum(freq(HHInRange)) + range /
(max - min)
Proposal 4 is to not use any sketch, and use histogram for high cardinality
columns, and exact (value, frequency) pairs for low cardinality columns
(e.g. num distinct value <= 255).
Proposal 5 is a variant of proposal 4, and adapts it to track exact (value,
frequency) pairs for the most frequent values only, so we can still have
that for high cardinality columns. This is actually very similar to
count-min sketch, but might use less space, although requiring two passes
to compute the initial value, and more difficult to compute the inner
product for joins.
One additional note: in terms of size, the size of a count-min sketch with
eps = 0.1% and confidence 0.87, uncompressed, is 48k bytes.
To look up what that means, see
http://spark.apache.org/docs/latest/api/java/org/apache/spark/util/sketch/CountMinSketch.html
Historically tpcds and tpch. There is certainly a chance of overfitting one
or two benchmarks. Note that those will probably be impacted more by the
way we set the parameters for CBO rather than using x or y for summary
statistics.
We are happy to announce the availability of Spark 2.0.2!
Apache Spark 2.0.2 is a maintenance release containing 90 bug fixes along
with Kafka 0.10 support and runtime metrics for Structured Streaming. This
release is based on the branch-2.0 maintenance branch of Spark. We strongly
recommend all 2.0.x users to upgrade to this stable release.
To download Apache Spark 2.0.12 visit http://spark.apache.org/downloads.html
We would like to acknowledge all community members for contributing patches
to this release.
It's on there on the page (both the release notes and the download version
dropdown).
The one line text is outdated. I'm just going to delete that text as a
matter of fact so we don't run into this issue in the future.
Good catch. Updated!
Can you submit a pull request to add that to the documentation?
Not right now.
I've noticed that a lot of github pull request notifications no longer come
to my inbox. In the past I'd get an email for every reply to a pull request
that I subscribed to (i.e. commented on). Lately I noticed for a lot of
them I didn't get any emails, but if I opened the pull requests directly on
github, I'd see the new replies. I've looked at spam folder and none of the
missing notifications are there. So it's either github not sending the
notifications, or the emails are lost in transit.
The way it manifests is that I often comment on a pull request, and then I
don't know whether the contributor (author) has updated it or not. From the
contributor's point of view, it looks like I've been ignoring the pull
request.
I think this started happening when github switched over to the new code
review mode (
https://github.com/blog/2256-a-whole-new-github-universe-announcing-new-tools-forums-and-features
)
Did anybody else notice this issue?
Adding a new data type is an enormous undertaking and very invasive. I
don't think it is worth it in this case given there are clear, simple
workarounds.
Thanks for the headsup, Shane.
FYI Github mirroring from Apache's official git repo to GitHub is broken
since Sat Nov 19, and as a result GitHub is now stale. Merged pull requests
won't show up in GitHub until ASF infra fixes the issue.
Can you use the approximate count distinct?
I did send an email out with those information on Nov 1st. It is not meant
to be in new feature development mode anymore.
FWIW, I will cut an RC today to remind people of that. The RC will fail,
but it can serve as a good reminder.
See https://issues.apache.org/jira/browse/SPARK-18557
It's already there isn't it? The in-memory columnar cache format.
bcc dev@ and add user@
This is more a user@ list question rather than a dev@ list question. You
can do something like this:
object MySimpleApp {
  def loadResources(): Unit = // define some idempotent way to load
resources, e.g. with a flag or lazy val
  def main() = {
    ...
    sc.parallelize(1 to 10).mapPartitions { iter =>
      MySimpleApp.loadResources()
      // do whatever you want with the iterator
    }
  }
}
I think this highly depends on what issues are found, e.g. critical bugs
that impact wide use cases, or security bugs.
Please vote on releasing the following candidate as Apache Spark version
2.1.0. The vote is open until Thursday, December 1, 2016 at 18:00 UTC and
passes if a majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 2.1.0
[ ] -1 Do not release this package because ...
To learn more about Apache Spark, please see http://spark.apache.org/
The tag to be voted on is v2.1.0-rc1
(80aabc0bd33dc5661a90133156247e7a8c1bf7f5)
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc1-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1216/
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc1-docs/
=======================================
How can I help test this release?
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.
===============================================================
What should happen to JIRA tickets still targeting 2.1.0?
===============================================================
Committers should look at those and triage. Extremely important bug fixes,
documentation, and API tweaks that impact compatibility should be worked on
immediately. Everything else please retarget to 2.1.1 or 2.2.0.
This one:
https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%202.1.0
Bcc dev@ and add user@
The dev list is not meant for users to ask questions on how to use Spark.
For that you should use StackOverflow or the user@ list.
scala> sql("select 1 & 2").show()
+-------+
|(1 & 2)|
+-------+
|      0|
+-------+
scala> sql("select 1 & 3").show()
+-------+
|(1 & 3)|
+-------+
|      1|
+-------+
Can you give a repro? Anything less than -(1 << 63) is considered negative
infinity (i.e. unbounded preceding).
Ah ok for some reason when I did the pull request sys.maxsize was much
larger than 2^63. Do you want to submit a patch to fix this?
Yes I'd define unboundedPreceding to -sys.maxsize, but also any value less
than min(-sys.maxsize, _JAVA_MIN_LONG) are considered unboundedPreceding
too. We need to be careful with long overflow when transferring data over
to Java.
ThriftHttpCLIService.java code is actually in Spark. That pull request is
basically no-op. Overall we are moving away from Hive dependency by
implementing almost everything in Spark, so the need to change that repo is
getting less and less.
Echoing Nick. I don't see any strong reason to drop Python 2 support.
We typically drop support for X when it is rarely used and support for X is
long past EOL. Python 2 is still very popular, and depending on the
statistics it might be more popular than Python 3.
I would like to re-iterate that committers please be very conservative now
in merging patches into branch-2.1.
Spark is a very sophisticated (compiler, optimizer) project and sometimes
one-line changes can have huge consequences and introduce regressions. If
it is just a tiny optimization, don't merge it into branch-2.1.
Honestly it is pretty difficult. Given the difficulty, would it still make
sense to do that change? (the one that sets the same number of
workers/parallelism across different languages in testing)
Thanks.
Please vote on releasing the following candidate as Apache Spark version
2.1.0. The vote is open until Sun, December 11, 2016 at 1:00 PT and passes
if a majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 2.1.0
[ ] -1 Do not release this package because ...
To learn more about Apache Spark, please see http://spark.apache.org/
The tag to be voted on is v2.1.0-rc2
(080717497365b83bc202ab16812ced93eb1ea7bd)
List of JIRA tickets resolved are:
https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%202.1.0
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc2-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1217
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc2-docs/
(Note that the docs and staging repo are still being uploaded and will be
available soon)
=======================================
How can I help test this release?
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.
===============================================================
What should happen to JIRA tickets still targeting 2.1.0?
===============================================================
Committers should look at those and triage. Extremely important bug fixes,
documentation, and API tweaks that impact compatibility should be worked on
immediately. Everything else please retarget to 2.1.1 or 2.2.0.
This vote is closed in favor of rc2.
I uploaded a new one:
https://repository.apache.org/content/repositories/orgapachespark-1219/
I'm going to -1 this myself: https://issues.apache.org/jira/browse/
SPARK-18856 
You can just write some files out directly (and idempotently) in your
map/mapPartitions functions. It is just a function that you can run
arbitrary code after all.
Committers please use 2.1.1 as the fix version for patches merged into the
branch. I will post a voting email once the packaging is done.
Please vote on releasing the following candidate as Apache Spark version
2.1.0. The vote is open until Sun, December 18, 2016 at 21:30 PT and passes
if a majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 2.1.0
[ ] -1 Do not release this package because ...
To learn more about Apache Spark, please see http://spark.apache.org/
The tag to be voted on is v2.1.0-rc5
(cd0a08361e2526519e7c131c42116bf56fa62c76)
List of JIRA tickets resolved are:
https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%202.1.0
The release files, including signatures, digests, etc. can be found at:
http://home.apache.org/~pwendell/spark-releases/spark-2.1.0-rc5-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1223/
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc5-docs/
*FAQ*
*How can I help test this release?*
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.
*What should happen to JIRA tickets still targeting 2.1.0?*
Committers should look at those and triage. Extremely important bug fixes,
documentation, and API tweaks that impact compatibility should be worked on
immediately. Everything else please retarget to 2.1.1 or 2.2.0.
*What happened to RC3/RC5?*
They had issues withe release packaging and as a result were skipped.
I'm going to start this with a +1!
In Spark 2.1, set spark.sql.files.ignoreCorruptFiles to true.
I get the same result every time on Spark 2.1:
Using Python version 2.7.12 (default, Jul  2 2016 17:43:17)
SparkSession available as 'spark'.
range(100000)]),
... ['t'])
[Row(t=4)]
[Row(t=4)]
[Row(t=4)]
Your understanding is correct - it is indeed slower due to extra
serialization. In some cases we can get rid of the serialization if the
value is already deserialized.
Thanks for the heads up, Ryan!
It's unfortunately difficult to debug -- that's one downside of codegen.
You can dump all the code via "explain codegen" though. That's typically
enough for me to debug.
It would be good to break them down a bit more, provided that we don't
increase for example total runtime due to extra setup.
Yes absolutely.
That is internal, but the amount of code is not a lot. Can you just copy
the relevant classes over to your project?
Are you using G1 GC? G1 sometimes uses a lot more memory than the size
allocated.
I think this is something we are going to change to completely decouple the
Hive support and catalog.
To be clear there are two separate "hive" we are talking about here. One is
the catalog, and the other is the Hive serde and UDF support. We want to
get to a point that the choice of catalog does not impact the functionality
in Spark other than where the catalog is stored.
Hi all,
Burak and Holden have recently been elected as Apache Spark committers.
Burak has been very active in a large number of areas in Spark, including
linear algebra, stats/maths functions in DataFrames, Python/R APIs for
DataFrames, dstream, and most recently Structured Streaming.
Holden has been a long time Spark contributor and evangelist. She has
written a few books on Spark, as well as frequent contributions to the
Python API to improve its usability and performance.
Please join me in welcoming the two!
I don't know if this would help but I think we can also officially stop
supporting Java 7 ...
Bumping this.
Given we see the occassional build breaks with Java 8, we should reconsider
this and do it for 2.2 or 2.3. By the time 2.2 is released, it will almost
be an year since this thread started.
BTW I created a JIRA ticket for tracking:
https://issues.apache.org/jira/browse/SPARK-19493
We of course shouldn't do anything until we achieve consensus.
tl;dr:
The critical internal APIs proposed to remain accessible in JDK 9 are:
sun.misc.{Signal,SignalHandler}
sun.misc.Unsafe (The functionality of many of the methods in this class is
now available via variable handles (JEP 193).)
sun.reflect.Reflection::getCallerClass(int) (The functionality of this
method may be provided in a standard form via JEP 259.)
sun.reflect.ReflectionFactory.newConstructorForSerialization
With complex types it doesn't work as well, but for primitive types the
biggest benefit of whole stage codegen is that we don't even need to put
the intermediate data into rows or columns anymore. They are just variables
(stored in CPU registers).
You can put them in spark's own conf/spark-defaults.conf file
Hi all,
Takuya-san has recently been elected an Apache Spark committer. He's been
active in the SQL area and writes very small, surgical patches that are
high quality. Please join me in congratulating Takuya-san!
With any dependency update (or refactoring of existing code), I always ask
this question: what's the benefit? In this case it looks like the benefit
is to reduce efforts in backports. Do you know how often we needed to do
those?
There is an existing pull request to update it:
https://github.com/apache/spark/pull/16856
But it is a little bit tricky.
What exactly is the issue? I've been working on Spark dev for a long time
and very rarely do I actually run into an issue that only manifest on
Jenkins but not locally. I don't have some magic local setup either.
We should definitely cut down test flakiness.
Josh's tool should give enough signal there already. I don't think we need
some manual process to document them. If you want to work on those that'd
be great. I bet you will get a lot of love because all developers hate
flaky tests.
Most of the previous notifications were caught as spam. We should really
disable this.
Thanks for sending an email. I was going to +1 but then I figured I should
be data driven. I took a look at the distribution of Scala versions across
all the clusters Databricks runs (which is a very high number across a
variety of tech startups, SMBs, large enterprises, and this is the chart:
[image: Inline image 1]
Given 30% are still on Scala 2.10, I'd say we should officially deprecate
Scala 2.10 in Spark 2.2 and remove the support in a future release (e.g.
2.3). Note that in the past we only deprecated Java 7 / Python 2.6 in 2.0,
and didn't do anything with Scala 2.10.
For some reason the previous email didn't show up properly. Trying again.
Hit sent too soon.
Actually my chart included only clusters on Spark 2.x, ie I excluded 1.x. I
also did one with Spark 1.x and I saw no substantial difference in
distribution for Scala versions. On the question of how many "would be
unable to" upgrade to Scala 2.12, I have no way to find out unless I go
talk to every one of them which is too expensive. My experience with Scala
upgrade, having done a few of them for Spark and for other projects, is
that it is very difficult and frustrating experience.
On Databricks this is actually not an issue at all because our customers
can manage multiple clusters with different versions of Spark easily
(select an old version of Spark with Scala 2.10 in one click).
As engineers, we all love to delete old code and simplify the build (5000
line gone!). In a previous email I said we never deprecated it. After
looking at it more, I realized this we did deprecate it partially: We
updated the docs and added a warning in SparkContext, but didn't announce
it in the release notes (mostly my fault).As a result, even I thought Scala
2.10 wasn't deprecated when I saw no mention of it in the release notes.
(Given we had partially deprecated Scala 2.10 support in Spark 2.1, I feel
less strongly about keeping it.)
Now look at the cost of keeping Scala 2.10: The part that defines Scala
2.10/2.11 support rarely changes, at least until we want to add support for
Scala 2.12 (and we are not adding 2.12 support in Spark 2.2). The actually
cost, which annoys some of us, is just the occasional build breaks (mostly
due to the use of Option.contains). It looks like this happened roughly
once a mont,h and each time it took just a few mins to resolve.
So the cost seems very low. Perhaps we should just deprecate it more
formally in 2.2 given the whole system is set to have it working, and kill
it next release.
Actually my chart included only clusters on Spark 2.x, ie I excluded 1.x.
I'm fine without a vote. (are we voting on wether we need a vote?)
We can just start using spip label and link to it.
