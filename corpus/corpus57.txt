Hi, I was wondering if anyone has thought about putting cached data in an RDD into off-heap memory, eg. w/ direct byte buffers.  For really long-lived RDDs that use a lot of memory, this seems like a huge improvement, since all the memory is now totally ignored during GC. (and reading data from direct byte buffers is potentially faster as well, buts thats just a nice bonus). The easiest thing to do is to store memory-serialized RDDs in direct byte buffers, but I guess we could also store the serialized RDD on disk and use a memory mapped file.  Serializing into off-heap buffers is a really simple patch, I just changed a few lines (I haven't done any real tests w/ it yet, though).  But I dont' really have a ton of experience w/ off-heap memory, so I thought I would ask what others think of the idea, if it makes sense or if there are any gotchas I should be aware of, etc. thanks, Imran Hi, a while back, ExecutorRunner was changed so the command line args included the appId. https://github.com/mesos/spark/pull/467 Those changes seem to be gone from the latest code.  Was that intentional, or just an oversight?  I'll add it back in if it was removed accidentally, but wanted to check in case there is some reason it shouldn't be there. thanks, Imran Hi, I've noticed a bug where the master can double-register workers.  I've got a patch for it, but my patch has some conflicts w/ an open PR (the whiltelist+spreadout one), so instead I modified on top of that, and created a PR to our fork: https://github.com/quantifind/incubator-spark/pull/2 (does anybody know of a better way to deal w/ pull requests on top of open pull requests?) Since its a bug fix, I'd like to get discussion going on it even though it can't be merged until the other PR is merged.  Also, if you think there is any hesitation on the other PR, I can instead apply the fix to master, and then just update the other PR later. thanks imran awesome, thanks. I've been wanting this even for all my scala projects for a while On Thu, Dec 5, 2013 at 7:00 PM, Aaron Davidson  wrote: I don't really agree with this logic.  I think we haven't broken API so far because we just keep adding stuff on to it, and we haven't bothered to clean the api up, specifically to *avoid* breaking things.  Here's a handful of api breaking things that we might want to consider: * should we look at all the various configuration properties, and maybe some of them should get renamed for consistency / clarity? * do all of the functions on RDD need to be in core?  or do some of them that are simple additions built on top of the primitives really belong in a "utils" package or something?  Eg., maybe we should get rid of all the variants of the mapPartitions / mapWith / etc.  just have map, and mapPartitionsWithIndex  (too many choices in the api can also be confusing to the user) * are the right things getting tracked in SparkListener?  Do we need to add or remove anything? This is probably not the right list of questions, that's just an idea of the kind of thing we should be thinking about. Its also fine with me if 1.0 is next, I just think that we ought to be asking these kinds of questions up and down the entire api before we release 1.0.  And given that we haven't even started that discussion, it seems possible that there could be new features we'd like to release in 0.10 before that discussion is finished. On Thu, Feb 6, 2014 at 12:56 PM, Matei Zaharia  I think it's important to do 1.0 next. The project has been around for 4 I'm finding the scala compiler crashes when I compile the spark-sql project in sbt.  This happens in both the 1.1 branch and master (full error below).  The other projects build fine in sbt, and everything builds fine in maven.  is there some sbt option I'm forgetting?  Any one else experiencing this? Also, are there up-to-date instructions on how to do common dev tasks in both sbt & maven?  I have only found these instructions on building with maven: http://spark.apache.org/docs/latest/building-with-maven.html and some general info here: https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark but I think this doesn't walk through a lot of the steps of a typical dev cycle, eg, continuous compilation, running one test, running one main class, etc.  (especially since it seems like people still favor sbt for dev.)  If it doesn't already exist somewhere, I could try to put together a brief doc for how to do the basics.  (I'm returning to spark dev after a little hiatus myself, and I'm hitting some stumbling blocks that are probably common knowledge to everyone still dealing with it all the time.) thanks, Imran ------------------------------ full crash info from sbt: [info] Set current project to spark-sql (in build file:/Users/imran/spark/spark/) [info] Compiling 62 Scala sources to /Users/imran/spark/spark/sql/catalyst/target/scala-2.10/classes... [info] Compiling 45 Scala sources and 39 Java sources to /Users/imran/spark/spark/sql/core/target/scala-2.10/classes... [error] [error]      while compiling: /Users/imran/spark/spark/sql/core/src/main/scala/org/apache/spark/sql/types/util/DataTypeConversions.scala [error]         during phase: jvm [error]      library version: version 2.10.4 [error]     compiler version: version 2.10.4 [error]   reconstructed args: -classpath /Users/imran/spark/spark/sql/core/target/scala-2.10/classes:/Users/imran/spark/spark/core/target/scala-2.10/classes:/Users/imran/spark/spark/sql/catalyst/target/scala-2.10/classes:/Users/imran/spark/spark/lib_managed/jars/hadoop-client-1.0.4.jar:/Users/imran/spark/spark/lib_managed/jars/hadoop-core-1.0.4.jar:/Users/imran/spark/spark/lib_managed/jars/xmlenc-0.52.jar:/Users/imran/spark/spark/lib_managed/jars/commons-math-2.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-configuration-1.6.jar:/Users/imran/spark/spark/lib_managed/jars/commons-collections-3.2.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-lang-2.4.jar:/Users/imran/spark/spark/lib_managed/jars/commons-logging-1.1.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-digester-1.8.jar:/Users/imran/spark/spark/lib_managed/jars/commons-beanutils-1.7.0.jar:/Users/imran/spark/spark/lib_managed/jars/commons-beanutils-core-1.8.0.jar:/Users/imran/spark/spark/lib_managed/jars/commons-net-2.2.jar:/Users/imran/spark/spark/lib_managed/jars/commons-el-1.0.jar:/Users/imran/spark/spark/lib_managed/jars/hsqldb-1.8.0.10.jar:/Users/imran/spark/spark/lib_managed/jars/oro-2.0.8.jar:/Users/imran/spark/spark/lib_managed/jars/jets3t-0.7.1.jar:/Users/imran/spark/spark/lib_managed/jars/commons-httpclient-3.1.jar:/Users/imran/spark/spark/lib_managed/bundles/curator-recipes-2.4.0.jar:/Users/imran/spark/spark/lib_managed/bundles/curator-framework-2.4.0.jar:/Users/imran/spark/spark/lib_managed/bundles/curator-client-2.4.0.jar:/Users/imran/spark/spark/lib_managed/jars/zookeeper-3.4.5.jar:/Users/imran/spark/spark/lib_managed/jars/slf4j-log4j12-1.7.5.jar:/Users/imran/spark/spark/lib_managed/bundles/log4j-1.2.17.jar:/Users/imran/spark/spark/lib_managed/jars/jline-0.9.94.jar:/Users/imran/spark/spark/lib_managed/bundles/guava-14.0.1.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-plus-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.transaction-1.1.1.v201105210645.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-webapp-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-xml-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-util-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-servlet-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-security-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-server-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.servlet-3.0.0.v201112011016.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-continuation-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-http-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-io-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/jars/jetty-jndi-8.1.14.v20131031.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.mail.glassfish-1.4.1.v201005082020.jar:/Users/imran/spark/spark/lib_managed/orbits/javax.activation-1.1.0.v201105071233.jar:/Users/imran/spark/spark/lib_managed/jars/commons-lang3-3.3.2.jar:/Users/imran/spark/spark/lib_managed/jars/jsr305-1.3.9.jar:/Users/imran/spark/spark/lib_managed/jars/slf4j-api-1.7.5.jar:/Users/imran/spark/spark/lib_managed/jars/jul-to-slf4j-1.7.5.jar:/Users/imran/spark/spark/lib_managed/jars/jcl-over-slf4j-1.7.5.jar:/Users/imran/spark/spark/lib_managed/bundles/compress-lzf-1.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/snappy-java-1.0.5.3.jar:/Users/imran/spark/spark/lib_managed/jars/lz4-1.2.0.jar:/Users/imran/spark/spark/lib_managed/jars/chill_2.10-0.3.6.jar:/Users/imran/spark/spark/lib_managed/jars/chill-java-0.3.6.jar:/Users/imran/spark/spark/lib_managed/bundles/kryo-2.21.jar:/Users/imran/spark/spark/lib_managed/jars/reflectasm-1.07-shaded.jar:/Users/imran/spark/spark/lib_managed/jars/minlog-1.2.jar:/Users/imran/spark/spark/lib_managed/jars/objenesis-1.2.jar:/Users/imran/spark/spark/lib_managed/bundles/akka-remote_2.10-2.2.3-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/jars/akka-actor_2.10-2.2.3-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/bundles/config-1.0.2.jar:/Users/imran/spark/spark/lib_managed/bundles/netty-3.6.6.Final.jar:/Users/imran/spark/spark/lib_managed/jars/protobuf-java-2.4.1-shaded.jar:/Users/imran/spark/spark/lib_managed/jars/uncommons-maths-1.2.2a.jar:/Users/imran/spark/spark/lib_managed/bundles/akka-slf4j_2.10-2.2.3-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/jars/json4s-jackson_2.10-3.2.10.jar:/Users/imran/spark/spark/lib_managed/jars/json4s-core_2.10-3.2.10.jar:/Users/imran/spark/spark/lib_managed/jars/json4s-ast_2.10-3.2.10.jar:/Users/imran/spark/spark/lib_managed/jars/paranamer-2.6.jar:/Users/imran/spark/spark/lib_managed/jars/scalap-2.10.0.jar:/Users/imran/spark/spark/lib_managed/bundles/jackson-databind-2.3.1.jar:/Users/imran/spark/spark/lib_managed/bundles/jackson-annotations-2.3.0.jar:/Users/imran/spark/spark/lib_managed/bundles/jackson-core-2.3.1.jar:/Users/imran/spark/spark/lib_managed/jars/colt-1.2.0.jar:/Users/imran/spark/spark/lib_managed/jars/concurrent-1.3.4.jar:/Users/imran/spark/spark/lib_managed/jars/mesos-0.18.1-shaded-protobuf.jar:/Users/imran/spark/spark/lib_managed/jars/netty-all-4.0.23.Final.jar:/Users/imran/spark/spark/lib_managed/jars/stream-2.7.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-core-3.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-jvm-3.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-json-3.0.0.jar:/Users/imran/spark/spark/lib_managed/bundles/metrics-graphite-3.0.0.jar:/Users/imran/spark/spark/lib_managed/jars/tachyon-client-0.5.0.jar:/Users/imran/spark/spark/lib_managed/jars/tachyon-0.5.0.jar:/Users/imran/spark/spark/lib_managed/jars/commons-io-2.4.jar:/Users/imran/spark/spark/lib_managed/jars/pyrolite-2.0.1.jar:/Users/imran/spark/spark/lib_managed/jars/py4j-0.8.2.1.jar:/Users/imran/.sbt/boot/scala-2.10.4/lib/scala-compiler.jar:/Users/imran/.sbt/boot/scala-2.10.4/lib/scala-reflect.jar:/Users/imran/spark/spark/lib_managed/jars/quasiquotes_2.10-2.0.1.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-column-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-common-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-encoding-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-generator-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/commons-codec-1.5.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-hadoop-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-format-2.0.0.jar:/Users/imran/spark/spark/lib_managed/jars/parquet-jackson-1.4.3.jar:/Users/imran/spark/spark/lib_managed/jars/jackson-mapper-asl-1.9.11 .jar:/Users/imran/spark/spark/lib_managed/jars/jackson-core-asl-1.9.11.jar -deprecation -feature -P:genjavadoc:out=/Users/imran/spark/spark/sql/core/target/java -Xplugin:/Users/imran/spark/spark/lib_managed/jars/genjavadoc-plugin_2.10.4-0.7.jar -bootclasspath /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/classes:/Users/imran/.sbt/boot/scala-2.10.4/lib/scala-library.jar -unchecked -language:postfixOps [error] [error]   last tree to typer: Literal(Constant(org.apache.spark.sql.catalyst.types.PrimitiveType)) [error]               symbol: null [error]    symbol definition: null [error]                  tpe: Class(classOf[org.apache.spark.sql.catalyst.types.PrimitiveType]) [error]        symbol owners: package util [error] [error] == Enclosing template or block == [error] tree.tpe=org.apache.spark.sql.types.util.anonfun$asScalaDataType$1 [error]   "scala.runtime.AbstractFunction1", "scala.Serializable" // parents [error]   ValDef([error]     private [error]     "_"[error] [error] [error]   ) [error]   // 3 statements [error]   DefDef( // final def apply(javaStructField: org.apache.spark.sql.api.java.StructField): org.apache.spark.sql.catalyst.types.StructField [error]      final [error]     "apply"[error]     [] [error]     // 1 parameter list [error]     ValDef( // javaStructField: org.apache.spark.sql.api.java.StructField [error] [error]       "javaStructField"[error]        // tree.tpe=org.apache.spark.sql.api.java.StructField [error] [error]     ) [error]      // tree.tpe=org.apache.spark.sql.catalyst.types.StructField [error]     Apply( // def asScalaStructField(javaStructField: org.apache.spark.sql.api.java.StructField): org.apache.spark.sql.catalyst.types.StructField in object DataTypeConversions, tree.tpe=org.apache.spark.sql.catalyst.types.StructField [error]       DataTypeConversions.this."asScalaStructField" // def asScalaStructField(javaStructField: org.apache.spark.sql.api.java.StructField): org.apache.spark.sql.catalyst.types.StructField in object DataTypeConversions, tree.tpe=(javaStructField: org.apache.spark.sql.api.java.StructField)org.apache.spark.sql.catalyst.types.StructField [error]       "javaStructField" // javaStructField: org.apache.spark.sql.api.java.StructField, tree.tpe=org.apache.spark.sql.api.java.StructField [error]     ) [error]   ) [error]   DefDef( // final def apply(v1: Object): Object [error]      final [error]     "apply"[error]     [] [error]     // 1 parameter list [error]     ValDef( // v1: Object [error] [error]       "v1"[error]        // tree.tpe=Object [error] [error]     ) [error]      // tree.tpe=Object [error]     Apply( // final def apply(javaStructField: org.apache.spark.sql.api.java.StructField): org.apache.spark.sql.catalyst.types.StructField, tree.tpe=org.apache.spark.sql.catalyst.types.StructField [error]       DataTypeConversions$$anonfun$asScalaDataType$1.this."apply"// final def apply(javaStructField: org.apache.spark.sql.api.java.StructField): org.apache.spark.sql.catalyst.types.StructField, tree.tpe=(javaStructField: org.apache.spark.sql.api.java.StructField)org.apache.spark.sql.catalyst.types.StructField class Object, tree.tpe=org.apache.spark.sql.api.java.StructField in class Object, tree.tpe=()org.apache.spark.sql.api.java.StructField [error]            // tree.tpe=org.apache.spark.sql.api.java.StructField [error]         ) [error]         Nil [error]       ) [error]     ) [error]   ) [error]   DefDef( // def (): org.apache.spark.sql.types.util.anonfun$asScalaDataType$1 [error] [error]     ""[error]     [] [error]     List(Nil) [error]      // tree.tpe=org.apache.spark.sql.types.util.anonfun$asScalaDataType$1 [error]     Block( // tree.tpe=Unit [error]       Apply( // def (): scala.runtime.AbstractFunction1 in class AbstractFunction1, tree.tpe=scala.runtime.AbstractFunction1 [error] DataTypeConversions$$anonfun$asScalaDataType$1.super."" // def (): scala.runtime.AbstractFunction1 in class AbstractFunction1, tree.tpe=()scala.runtime.AbstractFunction1 [error]         Nil [error]       ) [error]       () [error]     ) [error]   ) [error] ) [error] [error] == Expanded type of tree == [error] [error] ConstantType([error]   value = Constant(org.apache.spark.sql.catalyst.types.PrimitiveType) [error] ) [error] [error] uncaught exception during compilation: java.lang.AssertionError [trace] Stack trace suppressed: run last sql/compile:compile for the full output. [error] (sql/compile:compile) java.lang.AssertionError: assertion failed: List(object package$DebugNode, object package$DebugNode) [error] Total time: 23 s, completed Nov 2, 2014 1:00:37 PM thanks everyone, that worked.  I had been just cleaning the "sql" project, which wasn't enough, but a full clean of everything and its happy now. just in case this helps anybody else come up with steps to reproduce, for me the error was always in DataTypeConversions.scala, and I think it *might* have started after I did a maven build as well. ask() is a method on every Actor.  It comes from the akka library, which spark uses for a lot of the communication between various components. There is some documentation on ask() here (go to the section on "Send messages"): http://doc.akka.io/docs/akka/2.2.3/scala/actors.html though if you are totally new to it, you might want to work through a simple akka tutorial first, before diving into the docs. On Fri, Nov 7, 2014 at 4:11 AM, rapelly kartheek   Hi, Hi Xuelin, this type of question is probably better asked on the spark-user mailing list, user@spark.apache.org Do you mean the very first set of tasks take 300 - 500 ms to deserialize? That is most likely because of the time taken to ship the jars from the driver to the executors.  You should only pay this cost once per spark context (assuming you are not adding more jars later on).  You could try simply running the same task again, from the same spark context, and see whether it still takes that much time to deserialize the tasks. If you really want to eliminate that initial time to send the jars, you could ensure that the jars are already on the executors, so they don't need to get sent at all by spark.   (Of course, this makes it harder to deploy new code; you'd still need to update those jars *somehow* when you do.) hope this helps, Imran On Sat, Nov 22, 2014 at 6:52 AM, Xuelin Cao  In our experimental cluster (1 driver, 5 workers), we tried the simplest I agree we should separate out the integration tests so it's easy for dev to just run the other fast tests locally.  I opened a jira for it https://issues.apache.org/jira/plugins/servlet/mobile#issue/SPARK-4746 On Nov 30, 2014 3:08 PM, "Matei Zaharia"  wrote: btw, I updated the wiki to include instructions on making the macro-paradise jar a compiler plugin for Intellij: https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-IntelliJ On Sat, Jan 17, 2015 at 12:16 PM, Chunnan Yao  wrote: Thanks for the explanations, makes sense.  For the record looks like this was worked on a while back (and maybe the work is even close to a solution?) https://issues.apache.org/jira/browse/SPARK-1476 and perhaps an independent solution was worked on here? https://issues.apache.org/jira/browse/SPARK-1391 On Tue, Feb 3, 2015 at 5:20 PM, Reynold Xin  wrote: Hi Mridul, do you think you'll keep working on this, or should this get picked up by others?  Looks like there was a lot of work put into LargeByteBuffer, seems promising. thanks, Imran On Tue, Feb 3, 2015 at 7:32 PM, Mridul Muralidharan  That is fairly out of date (we used to run some of our jobs on it ... But Hi all, We've been considering changing criteria for being a committer (http://s.apache.org/VFw), but I don't think there are any conclusions yet. I had proposed eliminating (or at least weakening) this requirement: I realize it might be hard to nail down what the criteria are very precisely; alternatively we could just start nominating individuals that are good candidates, and see what sticks.  I can think of a few more community members that might make the cut -- some that I think are almost definitely in, others that it kinda depends where we draw the line. thank, Imran This would be pretty tricky to do -- the issue is that right now sparkContext.runJob has you pass in a function from a partition to *one* that idea is baked pretty deep into a lot of the internals, DAGScheduler, Task, Executors, etc. Maybe another possibility worth considering: should we make it easy to go from N partitions to 2N partitions (or any other multiple obviously) without requiring a shuffle?  for that matter, you should also be able to go from 2N to N without a shuffle as well.  That change is also somewhat involved, though. Both are in theory possible, but I imagine they'd need really compelling use cases. An alternative would be to write your RDD to some other data store (eg, hdfs) which has better support for reading data in a streaming fashion, though you would probably be unhappy with the overhead. On Wed, Feb 18, 2015 at 9:09 AM, Andrew Ash  wrote: Has anyone else noticed very strange build behavior in the network-* projects? maven seems to the doing the right, but sbt is very inconsistent. Sometimes when it builds network-shuffle it doesn't know about any of the code in network-common.  Sometimes it will completely skip the java unit tests.  And then some time later, it'll suddenly decide it knows about some more of the java unit tests.  Its not from a simple change, like touching a test file, or a file the test depends on -- nor a restart of sbt.  I am pretty confused. maven had issues when I tried to add scala code to network-common, it would compile the scala code but not make it available to java.  I'm working around that by just coding in java anyhow.  I'd really like to be able to run my tests in sbt, though, it makes the development iterations much faster. thanks, Imran well, perhaps I just need to learn to use maven better, but currently I find sbt much more convenient for continuously running my tests.  I do use zinc, but I'm looking for continuous testing.  This makes me think I need sbt for that: http://stackoverflow.com/questions/11347633/is-there-a-java-continuous-testing-plugin-for-maven 1) I really like that in sbt I can run "~test-only com.foo.bar.SomeTestSuite" (or whatever other pattern) and just leave that running as I code, without having to go and explicitly trigger "mvn test"and wait for the result. 2) I find sbt's handling of sub-projects much simpler (when it works).  I'm trying to make changes to network/common & network/shuffle, which means I have to keep cd'ing into network/common, run mvn install, then go back to network/shuffle and run some other mvn command over there.  I don't want to run mvn at the root project level, b/c I don't want to wait for it to compile all the other projects when I just want to run tests in network/common.  Even with incremental compiling, in my day-to-day coding I want to entirely skip compiling sql, graphx, mllib etc. -- I have to switch branches often enough that i end up triggering a full rebuild of those projects even when I haven't touched them. On Fri, Feb 27, 2015 at 1:14 PM, Ted Yu  wrote: I have a very strong dislike for #1 (scala enumerations).   I'm ok with #4 (with Xiangrui's final suggestion, especially making it sealed & available in Java), but I really think #2, java enums, are the best option. Java enums actually have some very real advantages over the other approaches -- you get values(), valueOf(), EnumSet, and EnumMap.  There has been endless debate in the Scala community about the problems with the approaches in Scala.  Very smart, level-headed Scala gurus have complained about their short-comings (Rex Kerr's name is coming to mind, though I'm not positive about that); there have been numerous well-thought out proposals to give Scala a better enum.  But the powers-that-be in Scala always reject them.  IIRC the explanation for rejecting is basically that (a) enums aren't important enough for introducing some new special feature, scala's got bigger things to work on and (b) if you really need a good enum, just use java's enum. I doubt it really matters that much for Spark internals, which is why I think #4 is fine.  But I figured I'd give my spiel, because every developer loves language wars :) Imran On Thu, Mar 5, 2015 at 1:35 AM, Xiangrui Meng  wrote: Can you expand on the serde issues w/ java enum's at all?  I haven't heard of any problems specific to enums.  The java object serialization rules seem very clear and it doesn't seem like different jvms should have a choice on what they do: http://docs.oracle.com/javase/6/docs/platform/serialization/spec/serial-arch.html#6469 (in a nutshell, serialization must use enum.name()) of course there are plenty of ways the user could screw this up(eg. rename the enums, or change their meaning, or remove them).  But then again, all of java serialization has issues w/ serialization the user has to be aware of.  Eg., if we go with case objects, than java serialization blows up if you add another helper method, even if that helper method is completely compatible. Some prior debate in the scala community: https://groups.google.com/d/msg/scala-internals/8RWkccSRBxQ/AN5F_ZbdKIsJ SO post on which version to use in scala: http://stackoverflow.com/questions/1321745/how-to-model-type-safe-enum-types SO post about the macro-craziness people try to add to scala to make them almost as good as a simple java enum: (NB: the accepted answer doesn't actually work in all cases ...) http://stackoverflow.com/questions/20089920/custom-scala-enum-most-elegant-version-searched Another proposal to add better enums built into scala ... but seems to be dormant: https://groups.google.com/forum/#!topic/scala-sips/Bf82LxK02Kk On Thu, Mar 5, 2015 at 10:49 PM, Mridul Muralidharan    I have a strong dislike for java enum's due to the fact that they I've just switched some of my code over to the new format, and I just want to make sure everyone realizes what we are getting into.  I went from 10 lines as java enums https://github.com/squito/spark/blob/fef66058612ebf225e58dd5f5fea6bae1afd5b31/core/src/main/java/org/apache/spark/status/api/StageStatus.java#L20 to 30 lines with the new format: https://github.com/squito/spark/blob/SPARK-3454_w_jersey/core/src/main/scala/org/apache/spark/status/api/v1/api.scala#L250 its not just that its verbose.  each name has to be repeated 4 times, with potential typos in some locations that won't be caught by the compiler. Also, you have to manually maintain the "values" as you update the set of enums, the compiler won't do it for you. The only downside I've heard for java enums is enum.hashcode().  OTOH, the downsides for this version are: maintainability / verbosity, no values(), more cumbersome to use from java, no enum map / enumset. I did put together a little util to at least get back the equivalent of enum.valueOf() with this format https://github.com/squito/spark/blob/SPARK-3454_w_jersey/core/src/main/scala/org/apache/spark/util/SparkEnum.scala I'm not trying to prevent us from moving forward on this, its fine if this is still what everyone wants, but I feel pretty strongly java enums make more sense. thanks, Imran On Tue, Mar 17, 2015 at 2:07 PM, Xiangrui Meng  wrote: well, perhaps I overstated things a little, I wouldn't call it the "official" solution, just a recommendation in the never-ending debate (and the recommendation from folks with their hands on scala itself). Even if we do get this fixed in scaladoc eventually -- as its not in the current versions, where does that leave this proposal?  personally I'd *still* prefer java enums, even if it doesn't get into scaladoc.  btw, even with sealed traits, the scaladoc still isn't great -- you don't see the values from the class, you only see them listed from the companion object. (though, that is somewhat standard for scaladoc, so maybe I'm reaching a little) On Mon, Mar 23, 2015 at 4:11 PM, Patrick Wendell  wrote: I think this would be a great addition, I totally agree that you need to be able to set these at a finer context than just the SparkContext. Just to play devil's advocate, though -- the alternative is for you just subclass HadoopRDD yourself, or make a totally new RDD, and then you could expose whatever you need.  Why is this solution better?  IMO the criteria are: (a) common operations (b) error-prone / difficult to implement (c) non-obvious, but important for performance I think this case fits (a) & (c), so I think its still worthwhile.  But its also worth asking whether or not its too difficult for a user to extend HadoopRDD right now.  There have been several cases in the past week where we've suggested that a user should read from hdfs themselves (eg., to read multiple files together in one partition) -- with*out* reusing the code in HadoopRDD, though they would lose things like the metric tracking & preferred locations you get from HadoopRDD.  Does HadoopRDD need to some refactoring to make that easier to do?  Or do we just need a good example? Imran (sorry for hijacking your thread, Koert) On Mon, Mar 23, 2015 at 3:52 PM, Koert Kuipers  wrote: Hi Nick, I don't remember the exact details of these scenarios, but I think the user wanted a lot more control over how the files got grouped into partitions, to group the files together by some arbitrary function.  I didn't think that was possible w/ CombineFileInputFormat, but maybe there is a way? thanks On Tue, Mar 24, 2015 at 1:50 PM, Nick Pentreath  Imran, on your point to read multiple files together in a partition, is it IMO, spark's config is kind of a mess right now.  I completely agree with Reynold that Spark's handling of config ought to be super-simple, its not the kind of thing we want to put much effort in spark itself.  It sounds so trivial that everyone wants to redo it, but then all these additional features start to get thrown in, it starts to get complicated.  This is one of many reasons our config handling is inadequate.  It would be better if we could outsource it to other libraries, or even better yet, let users bring their own. The biggest problem, in my mind, is that there isn't a definitive, strongly-typed, modular listing of all the parameters.  This makes it really hard to put your own thing on top -- you've got to manually go through all the options and put them into your own config library.  And then make sure its up-to-date with every new release of spark. Just as a small example of how the options are hard to track down, some of the options for event logging are listed in SparkContext: https://github.com/apache/spark/blob/424e987dfebbbaa37f4496d44090d469a931ce76/core/src/main/scala/org/apache/spark/SparkContext.scala#L229 and some others are listed in EventLoggingListener: https://github.com/apache/spark/blob/424e987dfebbbaa37f4496d44090d469a931ce76/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala#L60 this also makes it a headache while developing & trying to keep the documentation up-to-date. There are a handful of different libraries that might help out with this: scopt, argot, scallop, sumac.  I'm biased to sumac [since I wrote it ], but probably any of these would let me do whatever customizations I wanted on top, without needing to manually keep every option in sync.  That said, I do think sumac is especially well suited to the way Spark uses configuration -- the nested structure directly maps to the way we have things organized currently.  so eg. everything related to event logging would get placed in a class like: class EventLoggingOpts {var enabled = false var compress = false var testing = false var overwrite = false var buffer: Bytes = 100.kilobytes } Another plus is that you get fail-fast behavior -- if you put in some unparseable value, the job will fail immediately, rather than 1 hour in when you first try to access the value. In any case, my main point is just that I think we should try to make our config more compatible with external config tools, rather than trying to build own.  And after that, I'd just like to throw Sumac into the ring as a contender :) On Fri, Mar 13, 2015 at 1:26 PM, Reynold Xin  wrote: did you run dev/change-version-to-2.11.sh before compiling?  When I ran this on current master, it mostly worked: dev/change-version-to-2.11.sh mvn -Pyarn -Phadoop-2.4 -Pscala-2.11 -DskipTests clean package There was a failure in building catalyst, but core built just fine for me. The error I got was: [INFO] ------------------------------------------------------------------------ [INFO] Building Spark Project Catalyst 1.4.0-SNAPSHOT [INFO] ------------------------------------------------------------------------ [WARNING] The POM for org.scalamacros:quasiquotes_2.11:jar:2.0.1 is missing, no dependency information available I'm not sure if catalyst is supposed to work w/ scala-2.11 or not ... I wouldn't be surprised if the way macros should be used has changed, but its not listed explicitly in the docs as being incompatible: http://spark.apache.org/docs/latest/building-spark.html#building-for-scala-211 On Tue, Apr 7, 2015 at 12:00 AM, mjhb  wrote: any update here?  This is relevant for a currently open PR of mine -- I've got a bunch of new public constants defined w/ format #4, but I'd gladly switch to java enums.  (Even if we are just going to postpone this decision, I'm still inclined to switch to java enums ...) just to be clear about the existing problem with enums & scaladoc: right now, the scaladoc knows about the enum class, and generates a page for it, but it does not display the enum constants.  It is at least labeled as a java enum, though, so a savvy user could switch to the javadocs to see the constants. On Mon, Mar 23, 2015 at 4:50 PM, Imran Rashid  wrote: That limit doesn't have anything to do with the amount of available memory.  Its just a tuning parameter, as one version is more efficient for smaller files, the other is better for bigger files.  I suppose the comment is a little better in FileSegmentManagedBuffer: https://github.com/apache/spark/blob/master/network/common/src/main/java/org/apache/spark/network/buffer/FileSegmentManagedBuffer.java#L62 On Tue, Apr 14, 2015 at 12:01 AM, Kannan Rajah  wrote: (+dev) Hi Justin, short answer: no, there is no way to do that. I'm just guessing here, but I imagine this was done to eliminate serialization problems (eg., what if we got an error trying to serialize the user exception to send from the executors back to the driver?). Though, actually that isn't a great explanation either, since even when the info gets back to the driver, its broken into a few string fields (eg., we have the class name of the root exception), but eventually it just gets converted to one big string. I've cc'ed dev b/c I think this is an oversight in Spark.  It makes it really hard to write an app to deal gracefully with various exceptions -- all you can do is look at the string in SparkException (which could change arbitrarily between versions, in addition to just being a pain to work with).  We should probably add much more fine-grained subclasses of SparkException, at the very least distinguishing errors in user code vs. errors in spark.  I could imagine there might be a few other cases we'd like to distinguish more carefully as well. Any thoughts from other devs? thanks Imran On Tue, Apr 14, 2015 at 4:46 PM, Justin Yip  wrote: These are great questions -- I dunno the answer to most of them, but I'll try to at least give my take on "What should be rejected and why?"For new features, I'm often really confused by our guidelines on what to include and what to exclude.  Maybe we should ask that all new features make it clear why they should *not* just be a separate package. Bug fixes are also a little tricky.  On the one hand, its hard to say no to them -- everyone wants all the bugs fixed.  But I think its actually a lot harder for someone that isn't experienced with spark to fix a bug in a clean way, when they don't know the code base.  Often the proposed fixes are just kludges tacked on somewhere rather than addressing the real problem.  It might help to clearly say that the most useful thing they can do is submit bug reports with simple steps to reproduce, or even better to submit a failing test case.  Of course submitting a patch is great too, but we could be clear that patches would only be accepted if they fit in the long-term design for spark. I really feel that saying "no" more directly would be very helpful. Actually I think one of the most discouraging things we can do is give a "soft no" -- say "oh that sounds interesting", but then let the PR languish. thanks for pushing on this Sean, really useful to have this discussion. On Tue, Apr 14, 2015 at 10:02 AM, Sean Owen  wrote: The temp file creation is controlled by a hadoop OutputCommitter, which is normally FileOutputCommitter by default.  Its used in SparkHadoopWriter (which in turn is used by PairRDDFunctions.saveAsHadoopDataset). You could change the output committer to not use tmp files (eg. use this from Aaron Davidson: https://gist.github.com/aarondav/c513916e72101bbe14ec). On Wed, Apr 15, 2015 at 12:33 AM, Gil Vernik  wrote: +1 testing is super important, it'll be good to give recognition for it. On Mon, May 4, 2015 at 5:46 PM, Patrick Wendell  wrote: On Fri, May 8, 2015 at 4:16 AM, Steve Loughran  Would there be a place in the code tree for some tests to run against we could re-open https://issues.apache.org/jira/browse/SPARK-4746 part of the point of going with tags instead of just unit / integration dichotomy was to give us flexibility to add things like this the basic prototyping for it is done, needs to be brought up to date and polished. -1 discovered I accidentally removed master & worker json endpoints, will restore https://issues.apache.org/jira/browse/SPARK-7760 On Tue, May 19, 2015 at 11:10 AM, Patrick Wendell  Please vote on releasing the following candidate as Apache Spark version Hi, I was just fixing a problem with too short a timeout on one of the unit tests I added (https://issues.apache.org/jira/browse/SPARK-7919), and I was wondering if this is a common problem w/ a lot of our flaky tests. Its really hard to know what to set the timeouts to -- you set the timeout so it runs comfortably on your dev machine, but then discover its far too short for the build machines.  so up it some more, and then it turns out there are some even slower build machines, and you need to make it longer still ... Scalatest actually has a feature to handle this.  You can wrap all timeouts in scaled(...), and then you can pass in a flag with how much the timeouts should get scaled by. http://doc.scalatest.org/2.0/index.html#org.scalatest.concurrent.ScaledTimeSpans eg., I was looking at DriverSuite -- its been turned off because it was too flaky, though the 60 second timeout already seems super long. (https://issues.apache.org/jira/browse/SPARK-7415) does anybody know if timeouts are a common source of flaky tests?  Is it worth trying to change existing timeouts in tests to use scaled() so keep tests passing on slow build machines?  Does it make sense for all future tests to always use scaled() on timeouts? thanks, Imran +1 (partially b/c I would like jira admin myself) On Tue, Jun 23, 2015 at 3:47 AM, Sean Owen  wrote: Hi Stephen, I'm not sure which link you are referring to for the example code -- but yes, the recommendation is that you create the enum in Java, eg. see https://github.com/apache/spark/blob/v1.4.0/core/src/main/java/org/apache/spark/status/api/v1/StageStatus.java Then nothing special is required to use it in scala.  This method both uses the overall type of the enum in the return value, and uses specific values in the body: https://github.com/apache/spark/blob/v1.4.0/core/src/main/scala/org/apache/spark/status/api/v1/AllStagesResource.scala#L114 (I did delete the branches for the code that is *not* recommended anymore) Imran On Wed, Jul 1, 2015 at 5:53 PM, Stephen Boesch  wrote: Hi Mike, are you sure there the size isn't off 2x somehow?  I just tried to reproduce with a simple test in BlockManagerSuite: test("large block") {store = makeBlockManager(4e9.toLong) val arr = new Array[Double](1 << 28) println(arr.size) val blockId = BlockId("rdd_3_10") val result = store.putIterator(blockId, Iterator(arr), StorageLevel.MEMORY_AND_DISK) result.foreach{println} } it fails at 1 << 28 with nearly the same message, but its fine for (1 << 28) - 1 with a reported block size of 2147483680.  Not exactly the same as what you did, but I expect it to be close enough to exhibit the same error. On Tue, Jul 28, 2015 at 12:37 PM, Mike Hynes  wrote: org.apache.spark.storage.BlockInfo.markReady(BlockInfo.scala:55) org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:815) org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:638) org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:996) org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:99) org.apache.spark.broadcast.TorrentBroadcast.         at org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34) org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:63) org.apache.spark.SparkContext.broadcast(SparkContext.scala:1297) org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix.multiply(IndexedRowMatrix.scala:184) sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$runMain(SparkSubmit.scala:666) org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:178) org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:203) org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:118) Hi Mike, I dug into this a little more, and it turns out in this case there is a pretty trivial fix -- the problem you are seeing is just from integer overflow before casting to a long in SizeEstimator.  I've opened https://issues.apache.org/jira/browse/SPARK-9437 for this. For now, I think your workaround is to break into multiple arrays, and broadcast them each separately.  If its any consolation, you would have to do this anyway if you tried to broadcast more than Int.MAX_VALUE doubles in any case.  The JVM only allows arrays up to that length.  So even if spark didn't have this limitation, you could go up to an array of 2^31 doubles, which would be 16GB, before having to break into multiple arrays. There *are* a number of places in spark where things are limited to 2GB, and there are a handful of open issues to deal with it.  However I think this one is pretty easy to fix (I must have looked at this a half-dozen times before and never realized the fix was so simple before ...).  However it could be we'll run into something else after this particular issue w/ SizeEstimator is fixed.  This certainly won't work with HttpBroadcast, but I think it might just work as long as you stick with TorrentBroadcast. imran On Tue, Jul 28, 2015 at 10:56 PM, Mike Hynes  wrote: yikes. Was this a one-time thing?  Or does it happen consistently?  can you turn on debug logging for o.a.s.scheduler (dunno if it will help, but maybe ...) On Tue, Aug 11, 2015 at 8:59 AM, Akhil Das  Hi oh I see, you are defining your own RDD & Partition types, and you had a bug where partition.index did not line up with the partitions slot in rdd.getPartitions.  Is that correct? On Thu, Aug 13, 2015 at 2:40 AM, Akhil Das  I figured that out, And these are my findings: On Thu, Mar 17, 2016 at 2:55 PM, Cody Koeninger  wrote: Certainly PMC votes are not necessary on *every* code deletion.  I dont'think there is a very clear rule on when such discussion is warranted, just a soft expectation that committers understand which changes require more discussion before getting merged.  I believe the only formal requirement for a PMC vote is when there is a release.  But I think as a community we'd much rather deal with these issues ahead of time, rather than having contentious discussions around releases because some are strongly opposed to changes that have already been merged. I'm all for the idea of removing these modules in general (for all of the reasons already mentioned), but it seems that there are important questions about how the new packages get distributed and how they are managed that merit further discussion. I'm somewhat torn on the question of the sub-project vs independent, and how its governed.  I think Steve has summarized the tradeoffs very well.  I do want to emphasize, though, that if they are entirely external from the ASF, the artifact ids and the package names must change at the very least. On Fri, Mar 18, 2016 at 3:15 PM, Shane Curcuru  wrote: Assuming the Spark PMC has no plan on releasing the code, why would we keep it in our codebase?  It only makes the codebase harder to navigate.  It would be easy for someone to stumble on that code and expect it be part of the release.  Seems like a general code-hygiene practice, eg. just like not leaving a giant commented-out block of old code. (But as you can see from the rest of the thread, I think that discussion on whether it should still be part of Apache Spark is ongoing ...) Hi Nezih, I just reported a somewhat similar issue, and I have a potential fix -- SPARK-14560, looks like you are already watching it :).  You can try out that patch, you have to explicitly enable the change in behavior with "spark.shuffle.spillAfterRead=true".  Honestly, I don't think these issues are the same, as I've always seen that case lead to acquiring 0 bytes, while in your case you are requesting GBs and getting something pretty close, so my hunch is that it is different ... but might be worth a shot to see if it is the issue. Turning on debug logging for TaskMemoryManager might help track the root cause -- you'll get information on which consumers are using memory and when there are spill attempts.  (Note that even if the patch I have for SPARK-14560 doesn't fix your issue, it might still make those debug logs a bit more clear, since it'll report memory used by Spillables.) Imran On Mon, Apr 4, 2016 at 10:52 PM, Nezih Yigitbasi  wrote: +1 (binding) on removal of maintainers I dont' have a strong opinion yet on how to have a system for finding the right reviewers.  I agree it would be nice to have something to help you find reviewers, though I'm a little skeptical of anything automatic. On Thu, May 19, 2016 at 10:34 AM, Matei Zaharia  Hi folks, +1 (binding) On Mon, May 23, 2016 at 8:13 AM, Tom Graves  +1 (binding) I've been a bit on the fence on this, but I agree that Luciano makes a compelling reason for why we really should publish things to maven central.  Sure we slightly increase the risk somebody refers to the preview release too late, but really that is their own fault. And I also I agree with comments from Sean and Mark that this is *not* a "Databricks vs The World" scenario at all. On Mon, Jun 6, 2016 at 2:13 PM, Luciano Resende  On Mon, Jun 6, 2016 at 12:05 PM, Reynold Xin  wrote: Hi all, just a heads up, I introduced a flaky test, BlacklistIntegrationSuite, a week ago or so.  I *thought* I had solved the problems, but turns out there was more flakiness remaining.  for now I've just turned the tests off, so if you this has led to failures for you, just re-trigger your build. Opened a jira to fix it for real here if you're interested: https://issues.apache.org/jira/browse/SPARK-15783 Again apologies for the flakiness in the build. Imran Some new features are about to land in spark to improve Spark's ability to handle bad executors and nodes.  These are some significant changes, and we'd like to gather more input from the community about it, especially folks that use *large clusters*. We've spent a lot of time discussing the right approach for this feature; there are some tough tradeoffs that need to be made, and though we made our best efforts at the right balance, more community input would be very helpful.  The feature is marked experimental and off by default for now while we see how it works for the broader community. There is a design doc attached to SPARK-8425 with a more thorough explanation, but I wanted to highlight some key aspects.  Before these changes, spark was particularly vulnerable to flaky hardware on one node. The most common example is one bad disk somewhere in your cluster. After the changes go in, Spark will avoid scheduling tasks on executors and nodes with previous failures.  A brief summary of the behavior when blacklisting is turned on: 1) when a task fails, it won't ever be scheduled on the same executor.  If it fails on another executor on the same node, that task won't ever be scheduled on the same node again. 2) If an executor has two failed tasks within one task set, it is blacklisted for the entire task set.  Similarly for a node. 3) After a taskset completes _successfully_, Spark check's whether it should blacklist any executors or nodes for the entire application. Executors or nodes that have more than 2 failures in *successful* task sets are blacklisted for the application.  They are blacklisted for a configurable timeout, with a default of 1 hour. There are a number of confs added to control the cutoffs used, but we hope these defaults are sensible for most users. These changes will land in two patches.  SPARK-17675 adds the changes described within a taskset.  SPARK-8425 will follow soon after that with application-level blacklisting.  There are other follow up changes, eg. adding this info to the UI, but the core functionality is in those. Choosing the right behavior is particularly tough because Spark doesn't know why tasks fail -- is it a real hardware issue?  Or is there just temporary resource contention, perhaps due to an ill-behaved application? We needed to balance: 1) Robustness -- making sure Spark would survive a bad node under a wide variety of problems 2) Resource utilization  -- avoid blacklisting resources when they are perfectly fine 3) Ease of use -- the feature needs to be easy to configure, and have sensible logging etc. when there are issues. Hopefully the approach delivers a good balance for the majority of use cases, but we're particularly interested in hearing more from use cases with large clusters where hardware failures are encountered more frequently. A number of people have put in a lot of effort on driving discussion and implementation of this issue, in particular: Kay Ousterhout, Tom Graves, Mark Hamstra, Mridul Muralidharan, Wei Mao and Jerry Shao (sorry if I missed anybody!). thanks, Imran Hi Jacek, doesn't look like there is any good reason -- Mark Hamstra might know this best.  Feel free to open a jira & pr for it, you can ping Mark, Kay Ousterhout, and me (@squito) for review. Imran On Thu, Oct 13, 2016 at 7:56 AM, Jacek Laskowski  wrote: Hi Jacek, I'm not entirely sure I understand your question, but the reason preferredLocs can be transient is b/c that is used to define where the scheduler (on the driver) should prefer to assign the task.  But no matter the value, the task could still get assigned anywhere.  By the time that task has been assigned a location, and its running on an executor, it doesn't matter anymore. preferredLocations are entirely independent of having the map task know where to fetch its input shuffle data, and where the shuffle map task writes it output data.  All of that info goes through MapOutputTracker. hope that helps, Imran On Tue, Jan 3, 2017 at 5:27 AM, Jacek Laskowski  wrote: one is used when exactly one task has finished -- that means you now have free resources on just that one executor, so you only need to look for something to schedule on that one. the other one is used when you want to schedule everything you can across the entire cluster.  For example, you have just submitted a new taskset, so you want to try to use any idle resources across the entire cluster.  Or, for delay scheduling, you periodically retry all idle resources, in case they locality delay has expired. you could eliminate the version which takes an executorId, and always make offers across all idle hosts -- it would still be correct.  Its a small efficiency improvement to avoid having to go through the list of all resources. On Thu, Jan 26, 2017 at 5:48 AM, Jacek Laskowski  wrote: it is a small difference but think about what this means with a cluster where you have 10k tasks (perhaps 1k executors with 10 cores each). When you have one task complete, you have to go through 1k more executors. On top of that, with a large cluster, task completions happen far more frequently, since each core in your cluster is finishing tasks independently, and sending those updates back to the driver -- eg., you expect to get 10k updates from one "wave" of tasks on your cluster.  So you avoid going through a list of 1k executors 10k times in just one wave of tasks. On Thu, Jan 26, 2017 at 9:12 AM, Jacek Laskowski  wrote: