for generating idea file you may need this plugin for sbt https://github.com/mpeltonen/sbt-idea -- Nan Zhu School of Computer Science, McGill University On Sunday, October 27, 2013 at 11:32 PM, Reynold Xin wrote: why you need gcc to compile spark? -- Nan Zhu On Sunday, January 5, 2014 at 6:18 PM, q q wrote: change spark.speculation to spark.speculation.switch? maybe we can restrict that all properties in Spark should be "three levels"On Sat, Jan 18, 2014 at 2:10 PM, Mridul Muralidharan  Hi, Hi, all Is it always necessary to run sbt assembly when you want to test some code, Sometimes you just repeatedly change one or two lines for some failed test case, it is really time-consuming to sbt assembly every time any faster way? Best, -- Nan Zhu Thank you very much, Reynold -- Nan Zhu On Thursday, February 6, 2014 at 7:50 PM, Reynold Xin wrote: +1 -- Nan Zhu On Friday, February 7, 2014 at 1:59 AM, Henry Saputra wrote: Hi, all Sometimes, you report an issue, you try to fix by yourself, then you found that the involved scope of the code base was not understood correctly, or your description of the issue may not be so complete/accurate but currently, it seems that the reporter has to comment under the original post, I think it would be nice if the reporter can revise their original issue description in the above cases Is it possible to do it? Best, -- Nan Zhu If we reply these emails, will the reply be posted on pull request discussion board automatically? if yes, that would be very nice -- Nan Zhu On Friday, February 7, 2014 at 9:23 PM, Henry Saputra wrote: Hi, all I have a question when trying to write some test cases for the PR The key functionality in my PR involves actor communication between master and worker, like the worker does something and returns the result to the master via a message, I want to test if the master can do the right thing according to the number of workers existing in the cluster and the return result from the worker, Is there any way to test this via some test cases? Thank you Best, -- Nan Zhu Good, thank you very much Patrick Best, -- Nan Zhu On Sunday, February 9, 2014 at 1:23 PM, Patrick Wendell wrote: Good, thank you very much Patrick Best, -- Nan Zhu On Sunday, February 9, 2014 at 1:23 PM, Patrick Wendell wrote: +1 -- Nan Zhu On Tuesday, February 11, 2014 at 12:30 AM, Ameet Talwalkar wrote: I think the mail list block the images? -- Nan Zhu On Monday, February 17, 2014 at 7:06 PM, Bryn Keller wrote: after you login, can you access:https://spark-project.atlassian.net/secure/CreateIssue!default.jspa? -- Nan Zhu Sent with Sparrow (http://www.sparrowmailapp.com/?sig) On Monday, February 17, 2014 at 8:00 PM, Bryn Keller wrote: apache JIRA is running? Best, -- Nan Zhu On Monday, February 17, 2014 at 9:33 PM, Bryn Keller wrote: Congratulations to all! -- Nan Zhu On Thursday, February 20, 2014 at 3:18 PM, Konstantin Boudnik wrote: piggybank-like +1 -- Nan Zhu On Sunday, February 23, 2014 at 1:06 AM, Mridul Muralidharan wrote: String, it should be get the following helper function private[spark] def getKeyClass() = implicitly[ClassTag[K]].runtimeClass private[spark] def getValueClass() = implicitly[ClassTag[V]].runtimeClass and this is what I run scala> val a = sc.textFile("/Users/nanzhu/code/incubator-spark/LICENSE", 2).map(line => ("a", "b")) scala> a.saveAsNewAPIHadoopFile("/Users/nanzhu/code/output_rdd") java.lang.InstantiationException at sun.reflect.InstantiationExceptionConstructorAccessorImpl.newInstance(InstantiationExceptionConstructorAccessorImpl.java:48) at java.lang.reflect.Constructor.newInstance(Constructor.java:526) at java.lang.Class.newInstance(Class.java:374) at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:632) at org.apache.spark.rdd.PairRDDFunctions.saveAsNewAPIHadoopFile(PairRDDFunctions.scala:590) at $iwC$$iwC$$iwC$$iwC.:15) at $iwC$$iwC$$iwC.:20) at $iwC$$iwC.:22) at $iwC.:24) at :26) at .:30) at .) at .:7) at .) at $print() at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:606) at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:774) at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1042) at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:611) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:642) at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:606) at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:790) at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:835) at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:747) at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:595) at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:602) at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:605) at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:928) at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:878) at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:878) at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135) at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:878) at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:970) at org.apache.spark.repl.Main$.main(Main.scala:31) at org.apache.spark.repl.Main.main(Main.scala) -- Nan Zhu On Sunday, February 23, 2014 at 11:06 AM, Nick Pentreath wrote: OK, I know where I was wrong Best, -- Nan Zhu Sent with Sparrow (http://www.sparrowmailapp.com/?sig) On Sunday, February 23, 2014 at 12:50 PM, Nan Zhu wrote: I just misread the API doc, and forgot to pass the type information when calling this Best, -- Nan Zhu On Monday, February 24, 2014 at 8:17 AM, Mridul Muralidharan wrote: yet another email about forgotten PR I think Sean would like to start some discussion on the current situation where committers are facing a flood of PRs recently (as he said in the discussion thread about how to prevent the blob of RDD API)? Best, -- Nan Zhu On Monday, February 24, 2014 at 4:07 PM, Andrew Ash wrote: Do you mean this https://cwiki.apache.org/confluence/display/SPARK/Committers? -- Nan Zhu On Monday, February 24, 2014 at 4:18 PM, Andrew Ash wrote: Hi, Patrick, How to deal with the active pull requests in the old repository? The contributors have to do something? Best, -- Nan Zhu On Wednesday, February 26, 2014 at 5:37 PM, Patrick Wendell wrote: Hey All, The github incubator-spark mirror has been migrated to [1] by Apache infra and we've migrated Jenkins to reflect the new changes. This means the existing "incubator-spark" mirror is becoming outdated and no longer correctly displays pull request diff's. We've asked apache infra to see if they can migrate existing pull requests to incubator-spark. However since this relies on coordinating with github, I'm not entirely sure whether they can do this or what the timeline would be. In the mean time it would be good for people to open new pull requests against [1]. For pull requests that were *just* about to be merged, we can go manually merge them, but ones that require feedback and more rounds of testing will need to be done on the new one since incubator-spark is now out of date. Sorry about this inconvenience, it is a one-time transition and we won't ever have to do it again. [1] https://github.com/apache/spark - Patrick I think they are working on it? https://issues.apache.org/jira/browse/SPARK Best, -- Nan Zhu On Friday, February 28, 2014 at 2:29 PM, Evan Chan wrote: Hi, all I understand that you are very busy, But it seems that this PR has been there for a long while, and there have been some discussions in its incubator-spark version: https://github.com/apache/incubator-spark/pull/636 The current URL: https://github.com/apache/spark/pull/12 Thank you very much! -- Nan Zhu Yeah, I tested that, I had my SPARK_HOME point to a very old location, after I fixed that, everything goes well Thank you so much for pointing this out Best, -- Nan Zhu On Friday, March 14, 2014 at 6:41 PM, Michael Armbrust wrote: I assume the Jenkins is not working now? Best, -- Nan Zhu On Tuesday, March 25, 2014 at 6:42 PM, Michael Armbrust wrote: I just found that the Jenkins is not working from this afternoon for one PR, the first time build failed after 90 minutes, the second time it has run for more than 2 hours, no result is returned Best, -- Nan Zhu On Tuesday, March 25, 2014 at 10:06 PM, Patrick Wendell wrote: yes, it sends for every PR you were involved I think Patrick is doing something on Jenkins, he just stopped some testing jobs manually Best, -- Nan Zhu On Thursday, March 27, 2014 at 11:07 PM, Mridul Muralidharan wrote: master is managing the resources in the cluster, e.g. ensuring all components can work together, master/worker/driver e.g. you have to submit your application with the path: driver -> master -> worker then the driver take most of the responsibility of running your application, e.g. scheduling jobs/tasks the driver is more like a user-facing component, while master is more transparent to the user Best, -- Nan Zhu On Monday, March 31, 2014 at 10:48 AM, Dan wrote: master is managing the resources in the cluster, e.g. ensuring all components can work together, master/worker/driver e.g. you have to submit your application with the path: driver -> master -> worker then the driver take most of the responsibility of running your application, e.g. scheduling jobs/tasks the driver is more like a user-facing component, while master is more transparent to the user Best, -- Nan Zhu On Monday, March 31, 2014 at 10:48 AM, Dan wrote: I met this issue when Jenkins seems to be very busy.... On Monday, April 7, 2014, Kay Ousterhout  wrote: I thought those are files of spark.apache.org? -- Nan Zhu On Monday, April 21, 2014 at 9:09 PM, Xiangrui Meng wrote: SPARK_HADOOP_VERSION=2.3.0 sbt/sbt assembly and copy the generated jar to lib/ directory of my application, it seems that sbt cannot find the dependencies in the jar? but everything works with the pre-built jar files downloaded from the link provided by Patrick Best, -- Nan Zhu On Thursday, May 1, 2014 at 11:16 PM, Madhu wrote: SPARK_HADOOP_VERSION=2.3.0 sbt/sbt assembly and copy the generated jar to lib/ directory of my application, it seems that sbt cannot find the dependencies in the jar? but everything works with the pre-built jar files downloaded from the link provided by Patrick Best, -- Nan Zhu On Thursday, May 1, 2014 at 11:16 PM, Madhu wrote: Ah, I see, thanks -- Nan Zhu On Tuesday, May 13, 2014 at 12:59 PM, Mark Hamstra wrote: +1, replaced rc3 with rc5, all applications are working fine Best, -- Nan Zhu On Tuesday, May 13, 2014 at 8:03 PM, Madhu wrote: just curious, where is rc4 VOTE? I searched my gmail but didn't find that? On Tue, May 13, 2014 at 9:49 AM, Sean Owen  wrote: +1, replaced rc3 with rc5, all applications are working fine Best, -- Nan Zhu On Tuesday, May 13, 2014 at 8:03 PM, Madhu wrote: en, you have to put spark-assembly-*.jar to the lib directory of your application Best, -- Nan Zhu On Monday, May 19, 2014 at 9:48 PM, nit wrote: en, you have to put spark-assembly-*.jar to the lib directory of your application Best, -- Nan Zhu On Monday, May 19, 2014 at 9:48 PM, nit wrote: just rerun my test on rc5 everything works build applications with sbt and the spark-*.jar which is compiled with Hadoop 2.3 +1 -- Nan Zhu On Sunday, May 18, 2014 at 11:07 PM, witgo wrote: If local[2] is expected, then the streaming doc is actually misleading? as the given example is import org.apache.spark.api.java.function._ import org.apache.spark.streaming._ import org.apache.spark.streaming.api._ // Create a StreamingContext with a local master val ssc = new StreamingContext("local", "NetworkWordCount", Seconds(1)) http://spark.apache.org/docs/latest/streaming-programming-guide.html I created a JIRA and a PR https://github.com/apache/spark/pull/924 -- Nan Zhu On Friday, May 30, 2014 at 1:53 PM, Patrick Wendell wrote: Hi, all Any admin wants to review these two PRs? https://github.com/apache/spark/pull/637 (to make DAGScheduler self-contained) https://github.com/apache/spark/pull/731 (enable multiple executors per Worker) Thanks -- Nan Zhu Hi, Just found it occasionally https://issues.apache.org/jira/browse/SPARK-1471 Best, -- Nan Zhu Hi, all Any admin can assign this issue https://issues.apache.org/jira/browse/SPARK-2126 to me? I have started working on this Thanks, -- Nan Zhu Ah, sorry, sorry It's executorState under deploy package On Monday, July 14, 2014, Patrick Wendell  wrote: I resolved the issue by setting an internal maven repository to contain the Spark-1.0.1 jar compiled from branch-0.1-jdbc and replacing the dependency to the central repository with our own repository I believe there should be some more lightweight way Best, -- Nan Zhu On Monday, July 14, 2014 at 6:36 AM, Nan Zhu wrote: Awesome! On Saturday, July 19, 2014, Patrick Wendell  wrote: Good news, we will see the official version containing JDBC in very soon! Also, I have several pending PRs, can anyone continue the review process in this week? Avoid overwriting already-set SPARK_HOME in spark-submit: https://github.com/apache/spark/pull/1331 fix locality inversion bug in TaskSetManager: https://github.com/apache/spark/pull/1313 (Matei and Mridulm are working on it) Allow multiple executor per worker in Standalone mode: https://github.com/apache/spark/pull/731 Ensure actor is self-contained  in DAGScheduler: https://github.com/apache/spark/pull/637 Best, -- Nan Zhu On Sunday, July 27, 2014 at 2:31 PM, Patrick Wendell wrote: Hi, all It seems that the JDBC test cases are failed unexpectedly in Jenkins? [info] - test query execution against a Hive Thrift server *** FAILED *** [info] java.sql.SQLException: Could not open connection to jdbc:hive2://localhost:45518/: java.net.ConnectException: Connection refused [info] at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:146) [info] at org.apache.hive.jdbc.HiveConnection.(HiveConnection.java:123) [info] at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105) [info] at java.sql.DriverManager.getConnection(DriverManager.java:571) [info] at java.sql.DriverManager.getConnection(DriverManager.java:215) [info] at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.getConnection(HiveThriftServer2Suite.scala:131) [info] at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.createStatement(HiveThriftServer2Suite.scala:134) [info] at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite$$anonfun$1.apply$mcV$sp(HiveThriftServer2Suite.scala:110) [info] at org.apache.spark.sql.hive.thri ftserver.HiveThriftServer2Suite$$anonfun$1.apply(HiveThriftServer2Suite.scala:107) [info] at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite$$anonfun$1.apply(HiveThriftServer2Suite.scala:107) [info] ... [info] Cause: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused [info] at org.apache.thrift.transport.TSocket.open(TSocket.java:185) [info] at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:248) [info] at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) [info] at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:144) [info] at org.apache.hive.jdbc.HiveConnection.(HiveConnection.java:123) [info] at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105) [info] at java.sql.DriverManager.getConnection(DriverManager.java:571) [info] at java.sql.DriverManager.getConnection(DriverManager.java:215) [info] at org.apache.spark.sql.hive.thriftserver.H iveThriftServer2Suite.getConnection(HiveThriftServer2Suite.scala:131) [info] at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.createStatement(HiveThriftServer2Suite.scala:134) [info] ... [info] Cause: java.net.ConnectException: Connection refused [info] at java.net.PlainSocketImpl.socketConnect(Native Method) [info] at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339) [info] at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200) [info] at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182) [info] at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) [info] at java.net.Socket.connect(Socket.java:579) [info] at org.apache.thrift.transport.TSocket.open(TSocket.java:180) [info] at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:248) [info] at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) [info] at org.apache.hive.jdbc.HiveConn ection.openTransport(HiveConnection.java:144) [info] ... [info] CliSuite: Executing: create table hive_test1(key int, val string);, expecting output: OK [warn] four warnings found [warn] Note: /home/jenkins/workspace/SparkPullRequestBuilder@4/core/src/test/java/org/apache/spark/JavaAPISuite.java uses or overrides a deprecated API. [warn] Note: Recompile with -Xlint:deprecation for details. [info] - simple commands *** FAILED *** [info] java.lang.AssertionError: assertion failed: Didn't find "OK" in the output: [info] at scala.Predef$.assert(Predef.scala:179) [info] at org.apache.spark.sql.hive.thriftserver.TestUtils$class.waitForQuery(TestUtils.scala:70) [info] at org.apache.spark.sql.hive.thriftserver.CliSuite.waitForQuery(CliSuite.scala:25) [info] at org.apache.spark.sql.hive.thriftserver.TestUtils$class.executeQuery(TestUtils.scala:62) [info] at org.apache.spark.sql.hive.thriftserver.CliSuite.executeQuery(CliSuite.scala:25) [info] at org.apache.spark.sql.hive.thriftserver.CliSuite $$anonfun$1.apply$mcV$sp(CliSuite.scala:53) [info] at org.apache.spark.sql.hive.thriftserver.CliSuite$$anonfun$1.apply(CliSuite.scala:51) [info] at org.apache.spark.sql.hive.thriftserver.CliSuite$$anonfun$1.apply(CliSuite.scala:51) [info] at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22) [info] at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22) [log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. 14/07/27 17:06:43 INFO ClientBase: Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties info] ... [info] ScalaTest [info] Run completed in 41 seconds, 789 milliseconds. [info] Total number of tests run: 2 [info] Suites: completed 2, aborted 0 [info] Tests: succeeded 0, failed 2, canceled 0, ignored 0, pending 0 [info] *** 2 TESTS FAILED *** Best, -- Nan Zhu Sent with Sparrow (http://www.sparrowmailapp.com/?sig) +1 tested thrift server with our in-house application, everything works fine -- Nan Zhu On Wednesday, September 3, 2014 at 4:43 PM, Matei Zaharia wrote: Hi, all I just modified some document, but still failed to pass tests? https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/19950/consoleFull Anyone can look at the problem? Best, -- Nan Zhu Hi, Sean, Thanks for the reply Here are the updated files: https://github.com/apache/spark/pull/2312/files just two md files... Best, -- Nan Zhu On Sunday, September 7, 2014 at 4:30 PM, Sean Owen wrote: Just noticed these lines in the jenkins log ========================================================================= Running Apache RAT checks ========================================================================= Attempting to fetch rat Launching rat from /home/jenkins/workspace/SparkPullRequestBuilder/lib/apache-rat-0.10.jar Error: Invalid or corrupt jarfile /home/jenkins/workspace/SparkPullRequestBuilder/lib/apache-rat-0.10.jar RAT checks passed. Something wrong? Best, -- Nan Zhu On Monday, September 29, 2014 at 4:43 PM, shane knapp wrote: Great! Congratulations! -- Nan Zhu On Friday, October 10, 2014 at 11:19 AM, Mridul Muralidharan wrote: Hi, I just submitted a patch https://github.com/apache/spark/pull/2864/files with one line change but the Jenkins told me it's failed to compile on the unrelated files? https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/21935/console Best, Nan yes, I can compile locally, too but it seems that Jenkins is not happy now...https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/ All failed to compile Best, -- Nan Zhu On Monday, October 20, 2014 at 7:56 PM, Ted Yu wrote: I agree with Sean I just compiled spark core successfully with 7u71 in Mac OS X On Tue, Oct 21, 2014 at 1:11 PM, Josh Rosen  wrote: BTW, this PR https://github.com/apache/spark/pull/2524 is related to a blocker level bug, and this is actually close to be merged (have been reviewed for several rounds) I would appreciated if anyone can continue the process, @mateiz -- Nan Zhu http://codingcat.me On Thursday, November 20, 2014 at 10:17 AM, Corey Nolet wrote: Dear Spark Users and Developers, We (Distributed (Deep) Machine Learning Community (http://dmlc.ml/)) are happy to announce the release of XGBoost4J (http://dmlc.ml/2016/03/14/xgboost4j-portable-distributed-xgboost-in-spark-flink-and-dataflow.html), a Portable Distributed XGBoost in Spark, Flink and Dataflow XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. It has been the winning solution for many machine learning scenarios, ranging from Machine Learning Challenges (https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions) to Industrial User Cases (https://github.com/dmlc/xgboost/tree/master/demo#usecases) XGBoost4J is a new package in XGBoost aiming to provide the clean Scala/Java APIs and the seamless integration with the mainstream data processing platform, like Apache Spark. With XGBoost4J, users can run XGBoost as a stage of Spark job and build a unified pipeline from ETL to Model training to data product service within Spark, instead of jumping across two different systems, i.e. XGBoost and Spark. (Example: https://github.com/dmlc/xgboost/blob/master/jvm-packages/xgboost4j-example/src/main/scala/ml/dmlc/xgboost4j/scala/example/spark/DistTrainWithSpark.scala) Today, we release the first version of XGBoost4J to bring more choices to the Spark users who are seeking the solutions to build highly efficient data analytic platform and enrich the Spark ecosystem. We will keep moving forward to integrate with more features of Spark. Of course, you are more than welcome to join us and contribute to the project! For more details of distributed XGBoost, you can refer to the recently published paper: http://arxiv.org/abs/1603.02754 Best, -- Nan Zhu http://codingcat.me