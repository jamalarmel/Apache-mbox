Hi Sean, Sorry I didn't see this thread earlier!  (Thanks Ameet for pinging me.) Short version: That exception should not be thrown, so there is a bug somewhere.  The intended logic for handling high-arity categorical features is about the best one can do, as far as I know. Bug finding: For my checking purposes, which branch of Spark are you using, and do you have the options being submitted to DecisionTree? High-arity categorical features: As you have figured out, if you use a categorical feature with just a few categories, it is treated as "unordered" so that we explicitly consider all exponentially many ways to split the categories into 2 groups.  If you use one with many categories, then it is necessary to impose an order.  (The communication increases linearly in the number of possible splits, so it would blow up if we considered all exponentially many splits.)  This order is chosen separately for each node, so it is not a uniform order imposed over the entire tree. This actually means that it is not a heuristic for regression and binary classification; i.e., it chooses the same split as if we had explicitly considered all of the possible splits.  For multiclass classification, it is a heuristic, but I don't know of a better solution. I'll check the code, but if you can forward info about the bug, that would be very helpful. Thanks! Joseph On Mon, Oct 13, 2014 at 1:16 AM, Sean Owen  wrote: I think this is the fix: In this file: mllib/src/main/scala/org/apache/spark/mllib/tree/impl/DTStatsAggregator.scala methods "getFeatureOffset" and "getLeftRightFeatureOffsets" have sanity checks ("require") which are correct for DecisionTree but not for RandomForest.  You can remove those.  I've sent a PR with this and a few other small fixes: https://github.com/apache/spark/pull/2785 I hope this fixes the bug! On Mon, Oct 13, 2014 at 11:19 AM, Sean Owen  wrote: Hi Deb, Thanks for pointing it out!  I don't know of a JIRA for it now, so it would be great if you could open one.  I'm looking into the bug... Joseph On Tue, Nov 4, 2014 at 4:42 PM, Debasish Das  Hi, I'm making a JIRA and will then do a quick PR for it.  (Thanks both for pointing out the bug & fix!) On Tue, Nov 4, 2014 at 10:55 PM, Sean Owen  wrote: Could we move discussion of the design and implementation to the JIRA and/or a work-in-progress PR (tagged with [WIP])?  That will help leave a record for the future. Thanks! Joseph On Wed, Nov 19, 2014 at 9:59 PM, Ashutosh  Done. Thanks. Added you as a collaborator. So that you can add code in it. Hi Yu, Thanks for bringing it up for clarification.  Here's a rough draft of a section for the soon-to-be-updated programming guide, which will have more info on the spark.ml package. Joseph ## spark.mllib vs. spark.ml Spark 1.2 will include a new machine learning package called spark.ml, currently an alpha component but potentially a successor to spark.mllib. The spark.ml package aims to replace the old APIs with a cleaner, more uniform set of APIs which will help users create full machine learning pipelines. (More info about pipelines will be included in the updated programming guide for Spark 1.2.) ### Development plan With Spark 1.2, spark.mllib is still the primary machine learning package, and spark.ml is an alpha component for testing the new API.  The primary parts of this API are: * the Pipeline concept for constructing complicated ML workflows consisting of Estimators and Transformers, * SchemaRDD as an ML dataset, * and constructs for specifying parameters for algorithms and pipelines. If all goes well, spark.ml will become the primary ML package at the time of the Spark 1.3 release.  Initially, simple wrappers will be used to port algorithms to spark.ml, but eventually, code will be moved to spark.ml and spark.mllib will be deprecated. ### Advice to developers During the next development cycle, new algorithms should be contributed to spark.mllib.  Optionally, wrappers for new (and old) algorithms can be contributed to spark.ml. Users will be able to use algorithms from either of the two packages; the only difficulty will be the differences in APIs between the two packages. On Thu, Nov 27, 2014 at 6:41 AM, Yu Ishikawa  Hi all, Hi, I'd recommend starting by checking out the existing helper functionality for these tasks.  There are helper methods to do K-fold cross-validation in MLUtils: https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/util/MLUtils.scala The experimental spark.ml API in the Spark 1.2 release (in branch-1.2 and master) has a CrossValidator class which does this more automatically: https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/tuning/CrossValidator.scala There are also a few evaluation metrics implemented: https://github.com/apache/spark/tree/master/mllib/src/main/scala/org/apache/spark/mllib/evaluation There definitely could be more metrics and/or better APIs to make it easier to evaluate models on RDDs.  If you spot such cases, I'd recommend opening up JIRAs for the new features or improvements to get some feedback before sending PRs: https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark Hope this helps & looking forward to the contributions! Joseph On Thu, Dec 11, 2014 at 4:41 AM, kidynamit  wrote: Thanks to everyone in the community for past collaborations, and I look forward to continuing in the future! Joseph On Tue, Feb 3, 2015 at 6:23 PM, Shixiong Zhu  wrote: Hi Mike, glmnet has definitely been very successful, and it would be great to see how we can improve optimization in MLlib!  There is some related work ongoing; here are the JIRAs: GLMNET implementation in Spark LinearRegression with L1/L2 (elastic net) using OWLQN in new ML package The GLMNET JIRA has actually been closed in favor of the latter JIRA. However, if you're getting good results in your experiments, could you please post them on the GLMNET JIRA and link them from the other JIRA?  If it's faster and more scalable, that would be great to find out. As far as where the code should go and the APIs, that can be discussed on the JIRA. I hope this helps, and I'll keep an eye out for updates on the JIRAs! Joseph On Thu, Feb 19, 2015 at 10:59 AM,  wrote: Hi Mike, I'm not aware of a "standard" big dataset, but there are a number available: * The YearPredictionMSD dataset from the LIBSVM datasets is sizeable (in # instances but not # features): www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/regression.html * I've used this text dataset from which one can generate lots of n-gram features (but not many instances): http://www.ark.cs.cmu.edu/10K/ * I've seen some papers use the KDD Cup datasets, which might be the best option I know of.  The KDD Cup 2012 track 2 one seems promising. Good luck! Joseph On Tue, Feb 24, 2015 at 1:56 PM,  wrote: Some of this discussion seems valuable enough to preserve on the JIRA; can we move it there (and copy any relevant discussions from previous emails as needed)? On Wed, Feb 25, 2015 at 10:35 AM,  wrote: Hi, There are no examples currently.  For unsupervised learning, I think the pattern is straightforward.  It would follow the pattern from supervised learning, but without the label input column and with a model having a different transform() behavior. Reinforcement learning might take a bit more design since I haven't seen work on it so far.  I'd recommend making a Discussion JIRA to post a set of requirements and get feedback on a design.  Reinforcement learning would be great to have in MLlib. Joseph On Mon, Mar 9, 2015 at 5:21 AM, Egor Pahomov  wrote: +1 Tested on Mac OS X On Mon, Mar 9, 2015 at 3:30 PM, Xiangrui Meng  wrote: The checks against maxCategories are not for statistical purposes; they are to make sure communication does not blow up.  There currently are not checks to make sure that there are enough entries for statistically significant results.  That is up to the user. I do like the idea of adding a warning.  A reasonable fix for now might be to print a logWarning message and add a note to the documentation.  On the JIRA, we could also discuss whether the result should be set to some value to indicate a meaningless test (e.g., a very bad fixed pValue). I made a JIRA to track this issue: SPARK-6312 Joseph On Thu, Mar 12, 2015 at 12:13 AM, Chunnan Yao  wrote: Thanks!  I added it to a few other items: https://issues.apache.org/jira/browse/SPARK-6337 On Fri, Mar 20, 2015 at 5:53 PM, Muttineni, Vinay  Hey guys, It would be nice to see how big a performance hit we take from combining binary & multiclass logistic loss/gradient.  If it's not a big hit, then it might be simpler from an outside API perspective to keep them in 1 class (even if it's more complicated within). Joseph On Wed, Mar 25, 2015 at 8:15 AM, Debasish Das  Hi, Makes sense! On Wed, Mar 25, 2015 at 2:46 PM, Debasish Das  Cool...Thanks...It will be great if they move in two code paths just for It looks like SPARK-3250 was applied to the sample() which GradientDescent uses, and that should kick in for your minibatchFraction <= 0.4.  Based on your numbers, aggregation seems like the main issue, though I hesitate to optimize aggregation based on local tests for data sizes that small. The first thing I'd check for is unnecessary object creation, and to profile in a cluster or larger data setting. On Wed, Apr 1, 2015 at 10:09 AM, Ulanov, Alexander  Sorry for bothering you again, but I think that it is an important issue When you say "It seems that instead of sample it is better to shuffle data and then access it sequentially by mini-batches," are you sure that holds true for a big dataset in a cluster?  As far as implementing it, I haven't looked carefully at GapSamplingIterator (in RandomSampler.scala) myself, but that looks like it could be modified to be deterministic. Hopefully someone else can comment on aggregation in local mode.  I'm not sure how much effort has gone into optimizing for local mode. Joseph On Thu, Apr 2, 2015 at 11:33 AM, Ulanov, Alexander   Hi Joseph, I'll add a note that this is just for ML, not other parts of Spark.  (We can discuss more on the JIRA.) Thanks! Joseph On Mon, Apr 6, 2015 at 9:46 PM, Yu Ishikawa  Hi all, +1 tested ML-related items on Mac OS X On Wed, Apr 8, 2015 at 7:59 PM, Krishna Sankar  wrote: +1 On Wed, Apr 15, 2015 at 5:40 PM, Tom Graves  +1 tested on spark on yarn on hadoop 2.6 cluster with security. Hi Jim, You're right; that should be unpersisted.  Could you please create a JIRA and submit a patch? Thanks! Joseph On Wed, Apr 22, 2015 at 6:00 PM, jimfcarroll  wrote: Hi Chunnan, There is currently Scala documentation for the constructor parameters: https://github.com/apache/spark/blob/04525c077c638a7e615c294ba988e35036554f5f/mllib/src/main/scala/org/apache/spark/mllib/linalg/Vectors.scala#L515 There is one benefit to not checking for validity (ordering) within the constructor: If you need to translate between SparseVector and some other library's type (e.g., Breeze), you can do so with a few reference copies, rather than iterating through or copying the actual data.  It might be good to provide this check within Vectors.sparse(), but we'd need to check through MLlib for uses of Vectors.sparse which expect it to be a cheap operation.  What do you think? It is documented in the programming guide too: https://github.com/apache/spark/blob/04525c077c638a7e615c294ba988e35036554f5f/docs/mllib-data-types.md But perhaps that should be more prominent. If you think it would be helpful, then please do make a JIRA about adding a check to Vectors.sparse(). Joseph On Wed, Apr 22, 2015 at 8:29 AM, Chunnan Yao  wrote: I saw the PR already, but only saw this just now.  I think both persists are useful based on my experience, but it's very hard to say in general. On Thu, Apr 23, 2015 at 12:22 PM, jimfcarroll  wrote: A KMeansModel was trained in the previous step, and it was saved to "modelFile" as a Java object file.  This step is loading the model back and reconstructing the KMeansModel, which can then be used to classify new tweets into different clusters. Joseph On Thu, May 7, 2015 at 12:40 PM, anshu shukla  Can anyone please explain - Hi Tarek, Thanks for your interest & for checking the guidelines first!  On 2 points: Algorithm: PCA is of course a critical algorithm.  The main question is how your algorithm/implementation differs from the current PCA.  If it's different and potentially better, I'd recommend opening up a JIRA for explaining & discussing it. Java/Scala: We really do require that algorithms be in Scala, for the sake of maintainability.  The conversion should be doable if you're willing since Scala is a pretty friendly language.  If you create the JIRA, you could also ask for help there to see if someone can collaborate with you to convert the code to Scala. Thanks! Joseph On Mon, May 18, 2015 at 3:13 AM, Tarek Elgamal  Hi, Hi Trevor, I may be repeating what Ram said, but to 2nd it, a few points: We do want MLlib to become an extensive and rich ML library; as you said, scikit-learn is a great example.  To make that happen, we of course need to include important algorithms.  "Important" is hazy, but roughly means being useful to a large number of users, improving a large number of use cases (above what is currently available), and being well-established and tested. Others and I may not be familiar with Tarek's algorithm (since it is so new), so it will be important to discuss details on JIRA to establish the cases in which the algorithm improves over current PCA.  That may require discussion, community testing, etc.  If we establish that it is a clear improvement in a large domain, then it could be valuable to have in MLlib proper.  It's always going to be hard to tell where to draw the line, so less common algorithms will require more testing before we commit to including them in MLlib. I like the Spark package suggestion since it would allow users immediately start using the code, while the discussion on JIRA happens.  (Plus, if package users find it useful, they can report that on the JIRA.) Joseph On Wed, May 20, 2015 at 10:01 AM, Ram Sriharsha  Hi Trevor I believe it works with a mix of DenseVector and SparseVector types. Joseph On Wed, May 20, 2015 at 10:06 AM, Debasish Das  Hi, That's a good question; I could imagine it being much more efficient if kept in a BlockMatrix and using BLAS2 ops. On Sat, May 23, 2015 at 8:09 PM, Debasish Das  Hi, Hi Lorenz, I'm not aware of people working on hierarchical topic models for MLlib, but that would be cool to see.  Hopefully other devs know more! Glad that the current LDA is helpful! Joseph On Wed, Jun 3, 2015 at 6:43 AM, Lorenz Fischer  Hi All +1 On Sat, Jun 6, 2015 at 7:55 PM, Guoqiang Li  wrote: +1 On Sat, Jun 6, 2015 at 9:01 AM, Patrick Wendell  wrote: Hi Peter, We've tried to be cautious about making APIs public without need, to allow for changes needed in the future which we can't foresee now.  Marking classes as final is part of that.  While marking things as Experimental or DeveloperApi is a sort of warning, we've often felt that even changing those Experimental/Developer APIs is dangerous since people can come to rely on those APIs. However, customization is a very valid use case, and I agree that the classes should be opened up in the future.  I hope that, as the Pipelines API graduates from alpha, more users will give feedback about them, and that will give us enough confidence in the API stability to make the classes non-final. Joseph On Mon, Jun 8, 2015 at 9:17 AM, Peter Rudenko  Hi, previously all the models in ml package were private to package, so if +1 for checking out the Wiki on Contributing to Spark.  It gives helpful pointers about finding starter JIRAs, the discussion & code review process, and how we prioritize algorithms & other contributions.  After you read that, I would recommend searching JIRA for issues which catch your interest. Thanks! Joseph On Sat, Jun 13, 2015 at 3:55 AM, Akhil Das  This is a good start, if you haven't seen this already Hi Isca, Could you please give more details?  Data size, model parameters, stack traces / logs, etc. to help get a better picture? Thanks, Joseph On Wed, Jun 17, 2015 at 9:56 AM, Isca Harmatz  wrote: +1 On Tue, Jun 30, 2015 at 5:27 PM, Reynold Xin  wrote: Thanks for bringing this up!  I talked with Michael Armbrust, and it sounds like this is a from a bug in DataFrame caching: https://issues.apache.org/jira/browse/SPARK-9141 It's marked as a blocker for 1.5. Joseph On Tue, Jul 28, 2015 at 2:36 AM, Justin Uang  wrote: I agree that it's high time to start changing/removing target versions, especially if component maintainers have a good idea of what is not needed for 1.5.  I'll start doing that on ML. On Mon, Aug 3, 2015 at 12:05 PM, Sean Owen  wrote: Eron, Thanks for sending out this list!  We can make some of the critical ones public for 1.5, but they will be marked DeveloperApi since they may require changes in the future.  Just made the JIRA: [https://issues.apache.org/jira/browse/SPARK-9704] and I'll send a PR soon. Joseph On Mon, Aug 3, 2015 at 4:51 PM, Eron Wright  wrote: We tend to resist opening up APIs unless there's a strong reason to and we feel reasonably confident that the API will remain stable.  That allows us to make fixes if we realize there are issues with those APIs.  But if you have an important use case, I'd recommend opening up a JIRA to discuss it. Joseph On Wed, Sep 9, 2015 at 2:01 AM, Maandy  wrote: I've tended to use Strings.  Params can be created with a validator (isValid) which can ensure users get an immediate error if they try to pass an unsupported String.  Not as nice as compile-time errors, but easier on the APIs. On Mon, Sep 14, 2015 at 6:07 PM, Feynman Liang  We usually write a Java test suite which exercises the public API (e.g. @Alexander  It's worked for us to use Param[String] directly.  (I think it's b/c String is exactly java.lang.String, rather than a Scala version of it, so it's still Java-friendly.)  In other classes, I've added a static list (e.g., NaiveBayes.supportedModelTypes), though there isn't consistent coverage on that yet. @Stephen  It could be used, but I prefer String for spark.ml since it's easier to maintain consistent APIs across languages.  That's what we've used so far, at least. On Wed, Sep 16, 2015 at 6:00 PM, Stephen Boesch  wrote: Is this specific to tallSkinnyQR, or can you trigger it with other operations?  It would be helpful to see enough of the trace to know where the parser is being used. On Tue, Sep 22, 2015 at 8:27 AM,  wrote: Hi YiZhi Liu, The spark.ml classes are part of the higher-level "Pipelines" API, which works with DataFrames.  When creating this API, we decided to separate it from the old API to avoid confusion.  You can read more about it here: http://spark.apache.org/docs/latest/ml-guide.html For (3): We use Breeze, but we have to modify it in order to do distributed optimization based on Spark. Joseph On Tue, Oct 6, 2015 at 11:47 PM, YiZhi Liu  wrote: Thanks for bringing this up!  We need to add PMML export methods to the spark.ml API.  I just made a JIRA for tracking that: https://issues.apache.org/jira/browse/SPARK-11171 Joseph On Thu, Oct 15, 2015 at 2:58 AM, Fazlan Nazeem  wrote: Hi, it'd be great to share your implementation with the community.  I'd recommend: (1) Share it immediately by creating a Spark package: http://spark-packages.org/ You can use this helper package to create your own: http://spark-packages.org/package/databricks/sbt-spark-package After you create and test it, I'd recommend emailing the user list to announce it and see if others have feedback. (2) If you'd like to get it into MLlib itself, I'd recommend creating a JIRA first to discuss the algorithm, its design, and a timeline/priority for getting it into Spark.  (I'd say it looks useful, but it's hard to guess the use cases & priority within the whole community.  We'd want to collect user feedback on JIRA, the mailing list, and/or your Spark Package in order to gauge use cases and priority.) Thanks! Joseph On Thu, Oct 15, 2015 at 7:58 AM, Kybe67  wrote: +1 tested on OS X On Sat, Nov 7, 2015 at 10:25 AM, Reynold Xin  wrote: That sounds useful; would you mind submitting a JIRA (and a PR if you're willing)? Thanks, Joseph On Fri, Oct 23, 2015 at 12:43 PM, Robert Dodier  Hi, One comment about """1) I agree the sorting method you suggested is a very efficient way to handle the unordered categorical variables in binary classification and regression. I propose we have a Spark ML Transformer to do the sorting and encoding, bringing the benefits to many tree based methods. How about I open a jira for this? """--> MLlib trees do this currently, so you could check out that code as an example. I'm not sure how this would work as a generic transformer, though; it seems more like an internal part of space-partitioning algorithms. On Tue, Oct 27, 2015 at 5:04 PM, Meihua Wu  Hi DB Tsai, Hi, Could you please submit this via JIRA as a bug report?  It will be very helpful if you include the Spark version, system details, and other info too. Thanks! Joseph On Thu, Nov 19, 2015 at 1:21 PM, Rachana Srivastava  wrote: Yes, please, could you send a JIRA (and PR)?  A custom error message would be better. Thank you! Joseph On Fri, Nov 20, 2015 at 2:39 PM, BenFradet  Hey there, It should work with 1.5+. On Thu, Nov 26, 2015 at 12:53 PM, Ndjido Ardo Bar  wrote: You can do grid search if you set the evaluator to a MulticlassClassificationEvaluator, which expects a prediction column, not a rawPrediction column.  There's a JIRA for making BinaryClassificationEvaluator accept prediction instead of rawPrediction. Joseph On Tue, Dec 1, 2015 at 5:10 AM, Benjamin Fradet  Someone correct me if I'm wrong but no there isn't one that I am aware of. If you're working on a feature, please comment on the JIRA first (to avoid conflicts / duplicate work).  Could you please copy what your wrote to the JIRA to discuss there? Thanks, Joseph On Wed, Dec 2, 2015 at 4:51 AM, caiquermarques95  wrote: Thanks for reporting this!  I just added a JIRA: https://issues.apache.org/jira/browse/SPARK-12159 That would be great if you could send a PR for it; thanks! Joseph On Sat, Dec 5, 2015 at 5:02 AM, Benjamin Fradet  Hi, +1 Ran all tests locally on Mac OS X, and MLlib with large workloads on a cluster. On Sat, Dec 12, 2015 at 6:58 PM, Burak Yavuz  wrote: Hi Eugene, The maxDepth parameter exists because the implementation uses Integer node IDs which correspond to positions in the binary tree.  This simplified the implementation.  I'd like to eventually modify it to avoid depending on tree node IDs, but that is not yet on the roadmap. There is not an analogous limit for the GLMs you listed, but I'm not very familiar with the perceptron implementation. Joseph On Mon, Dec 14, 2015 at 10:52 AM, Eugene Morozov  wrote: +1 On Wed, Dec 16, 2015 at 5:26 PM, Reynold Xin  wrote: This method is tested in the Spark 1.5 unit tests, so I'd guess it's a problem with the Parquet dependency.  What version of Parquet are you building Spark 1.5 off of?  (I'm not that familiar with Parquet issues myself, but hopefully a SQL person can chime in.) On Tue, Dec 15, 2015 at 3:23 PM, Rachana Srivastava  wrote: Hi Li, I'm wondering if you're running into the same bug reported here: https://issues.apache.org/jira/browse/SPARK-12488 I haven't figured out yet what is causing it.  Do you have a small corpus which reproduces this error, and which you can share on the JIRA?  If so, that would help a lot in debugging this failure. Thanks! Joseph On Sun, Dec 27, 2015 at 7:26 PM, Li Li  wrote: Hi, This is more a question for the user list, not the dev list, so I'll CC user. If you're using mllib.clustering.LDAModel (RDD API), then can you make sure you're using a LocalLDAModel (or convert to it from DistributedLDAModel)? You can then call topicDistributions() on the new data. If you're using ml.clustering.LDAModel (DataFrame API), then you can call transform() on new data. Does that work? Joseph On Tue, Jan 19, 2016 at 6:21 AM, doruchiulan  wrote: JIRA created!  https://issues.apache.org/jira/browse/SPARK-13089 Feel free to pick it up if you're interested.  : ) Joseph On Wed, Jan 27, 2016 at 8:43 AM, Vinayak Agrawal  wrote: Congrats & welcome! On Mon, Feb 8, 2016 at 12:19 PM, Ram Sriharsha  great job guys! congrats and welcome! Spark devs & users, I want to bring attention to a proposal to merge the MLlib (spark.ml) concepts of Estimator and Model in Spark 2.0.  Please comment & discuss on SPARK-14033  (not in this email thread). *TL;DR:* *Proposal*: Merge Estimator and Model under a single abstraction (Estimator). *Goals*: Simplify API by combining the tightly coupled concepts of Estimator & Model.  Match other ML libraries like scikit-learn.  Simplify mutability semantics. *Details*: See https://issues.apache.org/jira/browse/SPARK-14033 for a design document (Google doc & PDF). Thanks in advance for feedback! Joseph The indexing I mentioned is more restrictive than that: each index corresponds to a unique position in a binary tree.  (I.e., the first index of row 0 is 1, the first of row 1 is 2, the first of row 2 is 4, etc., IIRC) You're correct that this restriction could be removed; with some careful thought, we could probably avoid using indices altogether.  I just created https://issues.apache.org/jira/browse/SPARK-14043  to track this. On Mon, Mar 21, 2016 at 11:22 AM, Eugene Morozov  wrote: There have been some comments about using Pipelines outside of ML, but I have not yet seen a real need for it.  If a user does want to use Pipelines for non-ML tasks, they still can use Transformers + PipelineModels.  Will that work? On Fri, Mar 25, 2016 at 8:05 AM, Jacek Laskowski  wrote: It's possible this was caused by incorrect Graph creation, fixed in [SPARK-13355]. Could you retry your dataset using the current master to see if the problem is fixed?  Thanks! On Tue, Jan 19, 2016 at 5:31 AM, Li Li  wrote: That sounds useful.  Would you mind creating a JIRA for it?  Thanks! Joseph On Mon, Apr 11, 2016 at 2:06 AM, Rahul Tanwani  Hi, Sounds good to me.  I'd request we be strict during this process about requiring *no* changes to the example itself, which will make review easier. On Tue, Apr 19, 2016 at 11:12 AM, Bryan Cutler  wrote: Thanks for your work on this.  Can we continue discussing on the JIRA? On Sun, Apr 24, 2016 at 9:39 AM, Caique Marques  Hello, everyone! Do you have code which can reproduce this performance drop in treeReduce? It would be helpful to debug.  In the 1.6 release, we profiled it via the various MLlib algorithms and did not see performance drops. It's not just renumbering the partitions; it is reducing the number of partitions by a factor of 1.0/scale (where scale > 1).  This creates a "tree"-structured aggregation so that more of the work of merging during aggregation is done on the workers, not the driver. On Wed, Apr 27, 2016 at 4:46 AM, Guillaume Pitel  wrote: Here's a JIRA for it: https://issues.apache.org/jira/browse/SPARK-13346 I don't have a great method currently, but hacks can get around it: convert the DataFrame to an RDD and back to truncate the query plan lineage. Joseph On Wed, May 11, 2016 at 12:46 PM, Ulanov, Alexander  wrote: Sorry for the slow response.  I agree with Hamel on #1. GraphFrames are mostly wrappers for GraphX algorithms.  There are a few which are not: * BFS: This is an iterative DataFrame alg.  Though it has unit tests, I have not pushed it in scaling to see how far it can go. * Belief Propagation example: This uses the conversion to and from an RDD. Not great, but it's really just an example for now. I definitely want to get this issue fixed ASAP! On Sun, May 15, 2016 at 7:15 AM, Hamel Kothari  I don't know about the second one but for question #1: Congrats & welcome! On Tue, Jun 7, 2016 at 7:15 AM, Xiangrui Meng  wrote: Hi Pranay, Yes, you can do this.  The DAG structure should be specified via the various Transformers' input and output columns, where a Transformer can have multiple input and/or output columns.  Most of the classification and regression Models are good examples of Transformers with multiple input and output columns. Hope this helps! Joseph On Wed, Jun 8, 2016 at 9:59 PM, Pranay Tonpay  wrote: One more note: When you specify the stages in the Pipeline, they need to be in topological order according to the DAG. On Sun, Jun 12, 2016 at 10:47 AM, Joseph Bradley  Hi Pranay, Hi Harmeet, I'll add one more item to the other advice: The community is in the process of putting together a roadmap JIRA for 2.1 for ML: https://issues.apache.org/jira/browse/SPARK-15581 This JIRA lists some of the major items and links to a few umbrella JIRAs with subtasks.  I'd expect this roadmap to change a little more as it is still being formed, but I hope it provides some guidance.  Feel free to ping on specific JIRAs to ask about their current importance and to see who else is working on them. Thanks! Joseph On Fri, Jun 17, 2016 at 3:32 PM, Michael Armbrust  Another good signal is the "target version" (which by convention is only +1 Mainly tested ML/Graph/R.  Perf tests from Tim Hunter showed minor speedups from 1.6 for common ML algorithms. On Thu, Jul 21, 2016 at 9:41 AM, Ricardo Almeida  wrote: Welcome Felix! On Mon, Aug 15, 2016 at 6:16 AM, mayur bhole  Congrats Felix! +1 for 4 months.  With QA taking about a month, that's very reasonable. My main ask (especially for MLlib) is for contributors and committers to take extra care not to delay on updating the Programming Guide for new APIs.  Documentation debt often collects and has to be paid off during QA, and a longer cycle will exacerbate this problem. On Wed, Sep 28, 2016 at 7:30 AM, Tom Graves  +1 to 4 months. +1 On Thu, Sep 29, 2016 at 2:11 PM, Dongjoon Hyun  wrote: There are plans...but not concrete ones yet: https://issues.apache.org/jira/browse/SPARK-8515 I agree categorical data handling is a pain point and that we need to improve it! On Tue, Sep 13, 2016 at 4:45 PM, Danil Kirsanov  NominalAttribute in MLib is used to represent categorical data internally. Congrats! On Tue, Oct 4, 2016 at 4:09 PM, Kousuke Saruta  Congratulations Xiao! Could you please file a bug report JIRA and also include more info about what you ran? * Random forest Param settings * dataset dimensionality, partitions, etc. Thanks! On Tue, Oct 4, 2016 at 10:44 PM, Samkit Shah  wrote: +1 On Thu, Nov 3, 2016 at 9:51 PM, Kousuke Saruta  +1 (non-binding) +1 On Fri, Nov 4, 2016 at 11:20 AM, Michael Armbrust  +1 Hi Georg, It's true we need better documentation for this.  I'd recommend checking out simple algorithms within Spark for examples: ml.feature.Tokenizer ml.regression.IsotonicRegression You should not need to put your library in Spark's namespace.  The shared Params in SPARK-7146 are not necessary to create a custom algorithm; they are just niceties. Though there aren't great docs yet, you should be able to follow existing examples.  And I'd like to add more docs in the future! Good luck, Joseph On Wed, Nov 16, 2016 at 6:29 AM, Georg Heiler  HI, To committers and contributors active in MLlib, Thanks everyone who has started helping with the QA tasks in SPARK-18316! I'd like to request that we stop committing non-critical changes to MLlib, including the Python and R APIs, since still-changing public APIs make it hard to QA.  We need have already started to sign off on some QA tasks, but we may need to re-open them if changes are committed, especially if those changes are to public APIs.  There's no need to push Python and R wrappers into 2.1 at the last minute. Let's focus on completing QA, after which we can resume committing API changes to master (not branch-2.1). Thanks everyone! Joseph -- Joseph Bradley Software Engineer - Machine Learning Databricks, Inc. [image: http://databricks.com] We returned a DataFrame since it is a nicer API, but I agree forcing RDD operations is not ideal.  I'd be OK with adding a new method, but I agree with Felix that we cannot break the API for something like this. On Thu, Jan 5, 2017 at 12:44 PM, Felix Cheung  Given how Word2Vec is used the pipeline model in the new ml -- Joseph Bradley Software Engineer - Machine Learning Databricks, Inc. [image: http://databricks.com] Would it be more robust to use the Path when creating the FileSystem? https://github.com/graphframes/graphframes/issues/160 On Thu, Jan 5, 2017 at 4:57 PM, Felix Cheung  This is likely a factor of your hadoop config and Spark rather then -- Joseph Bradley Software Engineer - Machine Learning Databricks, Inc. [image: http://databricks.com] Hi all, This is a general call for thoughts about the process for the MLlib roadmap proposed in SPARK-18813.  See the section called "Roadmap process."Summary: * This process is about committers indicating intention to shepherd and review. * The goal is to improve visibility and communication. * This is fairly orthogonal to the SIP discussion since this proposal is more about setting release targets than about proposing future plans. Thanks! Joseph -- Joseph Bradley Software Engineer - Machine Learning Databricks, Inc. [image: http://databricks.com] Hi Seth, The proposal is geared towards exactly the issue you're describing: providing more visibility into the capacity and intentions of committers. If there are things you'd add to it or change to improve further, it would be great to hear ideas!  The past roadmap JIRA has some more background discussion which is worth looking at too. Let's break off the MLlib mission discussion into another thread.  I'll start one now. Thanks, Joseph On Thu, Jan 19, 2017 at 1:51 PM, Felix Cheung  Hi Seth -- Joseph Bradley Software Engineer - Machine Learning Databricks, Inc. [image: http://databricks.com] This thread is split off from the "Feedback on MLlib roadmap process proposal" thread for discussing the high-level mission and goals for MLlib.  I hope this thread will collect feedback and ideas, not necessarily lead to huge decisions. Copying from the previous thread: *Seth:* """I would love to hear some discussion on the higher level goal of Spark MLlib (if this derails the original discussion, please let me know and we can discuss in another thread). The roadmap does contain specific items that help to convey some of this (ML parity with MLlib, model persistence, etc...), but I'm interested in what the "mission" of Spark MLlib is. We often see PRs for brand new algorithms which are sometimes rejected and sometimes not. Do we aim to keep implementing more and more algorithms? Or is our focus really, now that we have a reasonable library of algorithms, to simply make the existing ones faster/better/more robust? Should we aim to make interfaces that are easily extended for developers to easily implement their own custom code (e.g. custom optimization libraries), or do we want to restrict things to out-of-the box algorithms? Should we focus on more flexible, general abstractions like distributed linear algebra? I was not involved in the project in the early days of MLlib when this discussion may have happened, but I think it would be useful to either revisit it or restate it here for some of the newer developers. """*Mingjie:* """+1 general abstractions like distributed linear algebra. """I'll add my thoughts, starting with our past *t**rajectory*: * Initially, MLlib was mainly trying to build a set of core algorithms. * Two years ago, the big effort was adding Pipelines. * In the last year, big efforts have been around completing Pipelines and making the library more robust. I agree with Seth that a few *immediate goals* are very clear: * feature parity for DataFrame-based API * completing and improving testing for model persistence * Python, R parity *In the future*, it's harder to say, but if I had to pick my top 2 items, I'd list: *(1) Making MLlib more extensible* It will not be feasible to support a huge number of algorithms, so allowing users to customize their ML on Spark workflows will be critical.  This is IMO the most important thing we could do for MLlib. Part of this could be building a healthy community of Spark Packages, and we will need to make it easier for users to write their own algorithms and packages to facilitate this.  Part of this could be allowing users to customize existing algorithms with custom loss functions, etc. *(2) Consistent improvements to core algorithms* A less exciting but still very important item will be constantly improving the core set of algorithms in MLlib. This could mean speed, scaling, robustness, and usability for the few algorithms which cover 90% of use cases. There are plenty of other possibilities, and it will be great to hear the community's thoughts! Thanks, Joseph -- Joseph Bradley Software Engineer - Machine Learning Databricks, Inc. [image: http://databricks.com] *Re: performance measurement framework* We (Databricks) used to use spark-perf , but that was mainly for the RDD-based API.  We've now switched to spark-sql-perf , which does include some ML benchmarks despite the project name.  I'll see about updating the project README to document how to run MLlib tests. On Tue, Jan 24, 2017 at 6:02 PM, bradc  wrote: -- Joseph Bradley Software Engineer - Machine Learning Databricks, Inc. [image: http://databricks.com] Public service announcement: Our doc build has worked with Java 8 for brief time periods, but new changes keep breaking the Java 8 unidoc build. Please be aware of this, and try to test doc changes with Java 8!  In general, it is stricter than Java 7 for docs. A shout out to @HyukjinKwon and others who have made many fixes for this! See these sample PRs for some issues causing failures (especially around links): https://github.com/apache/spark/pull/16741 https://github.com/apache/spark/pull/16604 Thanks, Joseph -- Joseph Bradley Software Engineer - Machine Learning Databricks, Inc. [image: http://databricks.com] Congrats and welcome! On Mon, Feb 13, 2017 at 6:54 PM, Takuya UESHIN  Thank you very much everyone! -- Joseph Bradley Software Engineer - Machine Learning Databricks, Inc. [image: http://databricks.com] The current draft LGTM.  I agree some of the various concerns may need to be addressed in the future, depending on how SPIPs progress in practice. If others agree, let's put it to a vote and revisit the proposal in a few months. Joseph On Fri, Feb 24, 2017 at 5:35 AM, Cody Koeninger  wrote: -- Joseph Bradley Software Engineer - Machine Learning Databricks, Inc. [image: http://databricks.com]