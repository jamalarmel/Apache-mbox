For my own understanding, is this summary correct? Spark will move to scala 2.10, which means it can support akka 2.3-M1, which supports protobuf 2.5, which will allow Spark to run on Hadoop 2.2. What will be the first Spark version with these changes?  Are the Akka features that Spark relies on stable in 2.3-M1? thanks, Sandy On Tue, Nov 5, 2013 at 12:12 AM, Liu, Raymond  wrote: Hi All, We're working on a Spark application that could make use of a computing quantiles in a streaming fashion.  Something in the vein of what DataFu has for Pig http://linkedin.github.io/datafu/docs/current/datafu/pig/stats/StreamingQuantile.html . Does anything like this exist in the Spark ecosystem?  If not, would there be a good place to contribute this if we write it? thanks, Sandy Thanks all for the suggestions.  Exactly what I was looking for. -Sandy On Thu, Dec 5, 2013 at 5:00 AM, Sam Bessalah  wrote: As a sidenote, it would be nice to make sure that whatever done here will work with the YARN Application History Server (YARN-321), a generic history server that functions similarly to MapReduce's JobHistoryServer.  It will eventually have the ability to store application-specific data. -Sandy On Tue, Jan 7, 2014 at 2:51 PM, Tom Graves  wrote: Hey, YARN-321 is targeted for the Hadoop 2.4.  The minimum feature set doesn't include application-specific data, so that probably won't be part of 2.4 unless other things delay the release for a while.  There are no APIs for it yet and pluggable UIs have been discussed but not agreed upon.  I think requirements from Spark could be useful in helping shape what gets done there. -Sandy On Tue, Jan 7, 2014 at 4:13 PM, Patrick Wendell  wrote: Hey All, I ran into an issue when trying to run SparkPi as described in the Spark on YARN doc. 14/01/18 10:52:09 ERROR spark.SparkContext: Error adding jar (java.io.FileNotFoundException: spark-examples-assembly-0.9.0-incubating-SNAPSHOT.jar (No such file or directory)), was the --addJars option used? Is addJars not needed here? Here's the doc: SPARK_JAR=./assembly/target/scala-2.9.3/spark-assembly-0.8.1-incubating-hadoop2.0.5-alpha.jar \ ./spark-class org.apache.spark.deploy.yarn.Client \ --jar examples/target/scala-2.9.3/spark-examples-assembly-0.8.1-incubating.jar \ --class org.apache.spark.examples.SparkPi \ --args yarn-standalone \ --num-workers 3 \ --master-memory 4g \ --worker-memory 2g \ --worker-cores 1 thanks, Sandy Has anybody tested against YARN 2.2?  I tried it out against a pseudo-distributed cluster and ran into an issue I just filed as SPARK-1031 . thanks, Sandy On Sun, Jan 19, 2014 at 12:55 AM, Reynold Xin  wrote: Thanks for all this Patrick. I like Heiko's proposal that requires every pull request to reference a JIRA.  This is how things are done in Hadoop and it makes it much easier to, for example, find out whether an issue you came across when googling for an error is in a release. I agree with Mridul about binary compatibility.  It can be a dealbreaker for organizations that are considering an upgrade. The two ways I'm aware of that cause binary compatibility are scala version upgrades and messing around with inheritance.  Are these not avoidable at least for minor releases? -Sandy On Thu, Feb 6, 2014 at 12:49 AM, Mridul Muralidharan  The reason I explicitly mentioned about binary compatibility was Not codifying binary compatibility as a hard rule sounds fine to me.  Would it make sense to put something in that . I.e. avoid making needless changes to class hierarchies. Whether Spark considers itself stable or not, users are beginning to treat it so.  A responsible project will acknowledge this and provide the stability needed by its user base.  I think some projects have made the mistake of waiting too long to release a 1.0.0.  It allows them to put off making the hard decisions, but users and downstream projects suffer. If Spark needs to go through dramatic changes, there's always the option of a 2.0.0 that allows for this. -Sandy On Thu, Feb 6, 2014 at 10:56 AM, Matei Zaharia  I think it's important to do 1.0 next. The project has been around for 4 Bleh, hit send to early again.  My second paragraph was to argue for 1.0.0 instead of 0.10.0, not to hammer on the binary compatibility point. On Thu, Feb 6, 2014 at 11:04 AM, Sandy Ryza  wrote: *Would it make sense to put in something that strongly discourages binary incompatible changes when possible? On Thu, Feb 6, 2014 at 11:03 AM, Sandy Ryza  wrote: If the APIs are usable, stability and continuity are much more important than perfection.  With many already relying on the current APIs, I think trying to clean them up will just cause pain for users and integrators. Hadoop made this mistake when they decided the original MapReduce APIs were ugly and introduced a new set of APIs to do the same thing.  Even though this happened in a pre-1.0 release, three years down the road, both the old and new APIs are still supported, causing endless confusion for users.  If individual functions or configuration properties have unclear names, they can be deprecated and replaced, but redoing the APIs or breaking compatibility at this point is simply not worth it. On Thu, Feb 6, 2014 at 12:39 PM, Imran Rashid  wrote: +1 On Thu, Feb 6, 2014 at 4:10 PM, Mridul Muralidharan  +1 +1 On Mon, Feb 10, 2014 at 9:57 PM, Mark Hamstra  +1 Hadoop subprojects (MR, YARN, HDFS) each have a "dev" list that contains discussion as well as a single email whenever a JIRA is filed, and an "issues" list with all the JIRA activity.  I think this works out pretty well.  Subscribing just to the dev list, I can keep up with changes that are going to be made and follow the ones I care about.  And the issues list is there if I want the firehose. Is Apache actually prescriptive that a list with "dev" in its name needs to contain all discussion?  If so, most projects I've followed are violating this. On Fri, Feb 21, 2014 at 7:54 PM, Kay Ousterhout  It looks like there's at least one other apache project, jclouds, that Hey All, I've encountered some confusion about how to run a Spark app from a compiled jar and wanted to bring up the recommended way. It seems like the current standard options are: * Build an uber jar that contains the user jar and all of Spark. * Explicitly include the locations of the Spark jars on the client machine in the classpath. Both of these options have a couple issues. For the uber jar, this means unnecessarily sending all of Spark (and its dependencies) to every executor, as well as including Spark twice in the executor classpaths.  This also requires recompiling binaries against the latest version whenever the cluster version is upgraded, lest executor classpaths include two different versions of Spark at the same time. Explicitly including the Spark jars in the classpath is a huge pain because their locations can vary significantly between different installations and platforms, and makes the invocation more verbose. What seems ideal to me is a script that takes a user jar, sets up the Spark classpath, and runs it.  This means only the user jar gets shipped across the cluster, but the user doesn't need to figure out how to get the Spark jars onto the client classpath.  This is similar to the "hadoop jar"command commonly used for running MapReduce jobs. The spark-class script seems to do almost exactly this, but I've been told it's meant only for internal Spark use (with the possible exception of yarn-standalone mode). It doesn't take a user jar as an argument, but one can be added by setting the SPARK_CLASSPATH variable.  This script could be stabilized for user use. Another option would be to have a "spark-app" script that does what spark-class does, but also masks the decision of whether to run the driver in the client process or on the cluster (both standalone and YARN have modes for both of these). Does this all make sense? -Sandy Is the client=driver mode still a supported option (outside of the REPLs), at least for the medium term?  My impression from reading the docs is that it's the most common, if not recommended, way to submit jobs.  If that's the case, I still think it's important, or at least helpful, to have something for this mode that addresses the issues below. On Sat, Feb 22, 2014 at 10:48 PM, Matei Zaharia  Hey Sandy, That makes sense to me.  I filed SPARK-1126 to create a spark-jar script that provides a layer over these. On Sun, Feb 23, 2014 at 6:58 PM, Matei Zaharia  Yes, it is a supported option. I'm just wondering whether we want to To perhaps restate what some have said, Maven is by far the most common build tool for the Hadoop / JVM data ecosystem.  While Maven is less pretty than SBT, expertise in it is abundant.  SBT requires contributors to projects in the ecosystem to learn yet another tool.  If we think of Spark as a project in that ecosystem that happens to be in Scala, as opposed to a Scala project that happens to be part of that ecosystem, Maven seems like the better choice to me. On a CDH-specific note, in building CDH, one of the reasons Maven is helpful to us is that it makes it easy to harmonize dependency versions across projects.  We modify project poms to include the "CDH" pom as a root pom, allowing each project to reference variables defined in the root pom like ${cdh.slf4j.version}.  Is there a way to make an SBT project inherit from a Maven project that would allow this kind of thing? -Sandy On Tue, Feb 25, 2014 at 4:23 PM, Evan Chan  wrote: @patrick - It seems like my point about being able to inherit the root pom was addressed and there's a way to handle this. The larger point I meant to make is that Maven is by far the most common build tool in projects that are likely to share contributors with Spark.  I personally know 10 people I can go to if I run into a Maven problem, and maybe 1 if I run into an SBT problem.  Google is also much more likely to be able to answer a question about Maven than SBT.  I don't know of any particular functionality Maven has that SBT is lacking, but if it was up to me I'd still lean towards Maven on the grounds that it's way easier to find familiarity and expertise with. On Wed, Feb 26, 2014 at 9:42 AM, Patrick Wendell  wrote: Hi Lars, Unfortunately, due to some incompatible changes we pulled in to be closer to YARN trunk, Spark-on-YARN does not work against CDH 4.4+ (but does work against CDH5) -Sandy On Tue, Mar 4, 2014 at 6:33 AM, Tom Graves  wrote: In the mean time, you don't need to wait for the task to be assigned to you to start work.  If you're worried about someone else picking it up, you can drop a short comment on the JIRA saying that you're working on it. On Wed, Mar 12, 2014 at 3:25 PM, Konstantin Boudnik  wrote: Hi Priya, Here's a good place to start: https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark -Sandy On Fri, Apr 11, 2014 at 12:05 PM, priya arora  Hi, Hey all, After a shuffle / groupByKey, Hadoop MapReduce allows the values for a key to not all fit in memory.  The current ShuffleFetcher.fetch API, which doesn't distinguish between keys and values, only returning an Iterator[P], seems incompatible with this. Any thoughts on how we could achieve parity here? -Sandy The issue isn't that the Iterator[P] can't be disk-backed.  It's that, with a groupBy, each P is a (Key, Values) tuple, and the entire tuple is read into memory at once.  The ShuffledRDD is agnostic to what goes inside P. On Sun, Apr 20, 2014 at 11:36 AM, Mridul Muralidharan  An iterator does not imply data has to be memory resident. Thanks Matei and Mridul - was basically wondering whether we would be able to change the shuffle to accommodate this after 1.0, and from your answers it sounds like we can. On Mon, Apr 21, 2014 at 12:31 AM, Mridul Muralidharan  As Matei mentioned, the Values is now an Iterable : which can be disk If it's not done already, would it make sense to codify this philosophy somewhere?  I imagine this won't be the first time this discussion comes up, and it would be nice to have a doc to point to.  I'd be happy to take a stab at this. On Mon, Apr 21, 2014 at 10:54 AM, Xiangrui Meng  wrote: How do I get permissions to edit the wiki? On Mon, Apr 21, 2014 at 3:19 PM, Xiangrui Meng  wrote: I thought this might be a good thing to add to the wiki's "How to contribute" page, as it's not tied to a release. On Mon, Apr 21, 2014 at 6:09 PM, Xiangrui Meng  wrote: Thanks Matei.  I added a section "How to contribute" page. On Mon, Apr 21, 2014 at 7:25 PM, Matei Zaharia  The wiki is actually maintained separately in Hi AJ, You might find this helpful - http://blog.cloudera.com/blog/2014/04/how-to-run-a-simple-apache-spark-app-in-cdh-5/ -Sandy On Sat, May 3, 2014 at 8:42 AM, Ajay Nair  wrote: Hi AJ, You might find this helpful - http://blog.cloudera.com/blog/2014/04/how-to-run-a-simple-apache-spark-app-in-cdh-5/ -Sandy On Sat, May 3, 2014 at 8:42 AM, Ajay Nair  wrote: +1 (non-binding) * Built the release from source. * Compiled Java and Scala apps that interact with HDFS against it. * Ran them in local mode. * Ran them against a pseudo-distributed YARN cluster in both yarn-client mode and yarn-cluster mode. On Tue, May 13, 2014 at 9:09 PM, witgo  wrote: +1 Reran my tests from rc5: * Built the release from source. * Compiled Java and Scala apps that interact with HDFS against it. * Ran them in local mode. * Ran them against a pseudo-distributed YARN cluster in both yarn-client mode and yarn-cluster mode. On Sat, May 17, 2014 at 10:08 AM, Andrew Or  wrote: I spoke with DB offline about this a little while ago and he confirmed that he was able to access the jar from the driver. The issue appears to be a general Java issue: you can't directly instantiate a class from a dynamically loaded jar. I reproduced it locally outside of Spark with: --- URLClassLoader urlClassLoader = new URLClassLoader(new URL[] { new File("myotherjar.jar").toURI().toURL() }, null); Thread.currentThread().setContextClassLoader(urlClassLoader); MyClassFromMyOtherJar obj = new MyClassFromMyOtherJar(); --- I was able to load the class with reflection. On Sun, May 18, 2014 at 11:58 AM, Patrick Wendell  @db - it's possible that you aren't including the jar in the classpath Hey Xiangrui, If the jars are placed in the distributed cache and loaded statically, as the primary app jar is in YARN, then it shouldn't be an issue.  Other jars, however, including additional jars that are sc.addJar'd and jars specified with the spark-submit --jars argument, are loaded dynamically by executors with a URLClassLoader.  These jars aren't next to the executors when they start - the executors fetch them from the driver's HTTP server. On Sun, May 18, 2014 at 4:05 PM, Xiangrui Meng  wrote: It just hit me why this problem is showing up on YARN and not on standalone. The relevant difference between YARN and standalone is that, on YARN, the app jar is loaded by the system classloader instead of Spark's custom URL classloader. On YARN, the system classloader knows about [the classes in the spark jars, the classes in the primary app jar].   The custom classloader knows about [the classes in secondary app jars] and has the system classloader as its parent. A few relevant facts (mostly redundant with what Sean pointed out): * Every class has a classloader that loaded it. * When an object of class B is instantiated inside of class A, the classloader used for loading B is the classloader that was used for loading A. * When a classloader fails to load a class, it lets its parent classloader try.  If its parent succeeds, its parent becomes the "classloader that loaded it". So suppose class B is in a secondary app jar and class A is in the primary app jar: 1. The custom classloader will try to load class A. 2. It will fail, because it only knows about the secondary jars. 3. It will delegate to its parent, the system classloader. 4. The system classloader will succeed, because it knows about the primary app jar. 5. A's classloader will be the system classloader. 6. A tries to instantiate an instance of class B. 7. B will be loaded with A's classloader, which is the system classloader. 8. Loading B will fail, because A's classloader, which is the system classloader, doesn't know about the secondary app jars. In Spark standalone, A and B are both loaded by the custom classloader, so this issue doesn't come up. -Sandy On Mon, May 19, 2014 at 7:07 PM, Patrick Wendell  wrote: sortByKey currently requires partitions to fit in memory, but there are plans to add external sort On Tue, May 20, 2014 at 10:10 AM, Madhu  wrote: sortByKey currently requires partitions to fit in memory, but there are plans to add external sort On Tue, May 20, 2014 at 10:10 AM, Madhu  wrote: There is: SPARK-545 On Tue, May 20, 2014 at 10:16 AM, Andrew Ash  wrote: There is: SPARK-545 On Tue, May 20, 2014 at 10:16 AM, Andrew Ash  wrote: +1 On Tue, May 20, 2014 at 5:26 PM, Andrew Or  wrote: This will solve the issue for jars added upon application submission, but, on top of this, we need to make sure that anything dynamically added through sc.addJar works as well. To do so, we need to make sure that any jars retrieved via the driver's HTTP server are loaded by the same classloader that loads the jars given on app submission.  To achieve this, we need to either use the same classloader for both system jars and user jars, or make sure that the user jars given on app submission are under the same classloader used for dynamically added jars. On Tue, May 20, 2014 at 5:59 PM, Xiangrui Meng  wrote: Is that an assumption we can make?  I think we'd run into an issue in this situation: *In primary jar:* def makeDynamicObject(clazz: String) = Class.forName(clazz).newInstance() *In app code:* sc.addJar("dynamicjar.jar") ... rdd.map(x => makeDynamicObject("some.class.from.DynamicJar")) It might be fair to say that the user should make sure to use the context classloader when instantiating dynamic classes, but I think it's weird that this code would work on Spark standalone but not on YARN. -Sandy On Wed, May 21, 2014 at 2:10 PM, Xiangrui Meng  wrote: +1 On Mon, May 26, 2014 at 7:38 AM, Tathagata Das Please vote on releasing the following candidate as Apache Spark version They should be - in the sense that the docs now recommend using spark-submit and thus include entirely different invocations. On Fri, May 30, 2014 at 12:46 AM, Reynold Xin  wrote: Hi Xiaokai, I think MLLib is definitely interested in supporting additional GLMs.  I'm not aware of anybody working on this at the moment. -Sandy On Tue, Jun 17, 2014 at 5:00 PM, Xiaokai Wei  wrote: Hi Anish, Spark, like MapReduce, makes an effort to schedule tasks on the same nodes and racks that the input blocks reside on. -Sandy On Tue, Jul 8, 2014 at 12:27 PM, anishsneh@yahoo.co.in  wrote: Having a common framework for clustering makes sense to me.  While we should be careful about what algorithms we include, having solid implementations of minibatch clustering and hierarchical clustering seems like a worthwhile goal, and we should reuse as much code and APIs as reasonable. On Tue, Jul 8, 2014 at 1:19 PM, RJ Nowling  wrote: Woot! On Thu, Jul 10, 2014 at 11:15 AM, Patrick Wendell  Just a heads up, we merged Prashant's work on having the sbt build read all Stephen, Often the shuffle is bound by writes to disk, so even if disks have enough space to store the uncompressed data, the shuffle can complete faster by writing less data. Reynold, This isn't a big help in the short term, but if we switch to a sort-based shuffle, we'll only need a single LZFOutputStream per map task. On Mon, Jul 14, 2014 at 3:30 PM, Stephen Haberman  wrote: To add, we've made some effort to yarn-alpha to work with the 2.0.x line, but this was a time when YARN went through wild API changes.  The only line that the yarn-alpha profile is guaranteed to work against is the 0.23 line. On Thu, Jul 17, 2014 at 12:40 AM, Sean Owen  wrote: Hi RJ, Spark Shell instantiates a SparkContext for you named "sc".  In other apps, the user instantiates it themself and can give the variable whatever name they want, e.g. "spark". -Sandy On Mon, Jul 21, 2014 at 8:36 AM, RJ Nowling  wrote: It could make sense to add a skipHeader argument to SparkContext.textFile? On Mon, Jul 21, 2014 at 10:37 PM, Reynold Xin  wrote: Yeah, the input format doesn't support this behavior.  But it does tell you the byte position of each record in the file. On Mon, Jul 21, 2014 at 10:55 PM, Reynold Xin  wrote: I'm working on a patch that switches this stuff out with the Hadoop FileSystem StatisticsData, which will both give an accurate count and allow us to get metrics while the task is in progress.  A hitch is that it relies on https://issues.apache.org/jira/browse/HADOOP-10688, so we still might want a fallback for versions of Hadoop that don't have this API. On Sat, Jul 26, 2014 at 10:47 AM, Reynold Xin  wrote: +user list bcc: dev list It's definitely possible to implement credit fraud management using Spark. A good start would be using some of the supervised learning algorithms that Spark provides in MLLib (logistic regression or linear SVMs). Spark doesn't have any HMM implementation right now.  Sean Owen has a great talk on performing anomaly detection with KMeans clustering in Spark - https://www.youtube.com/watch?v=TC5cKYBZAeI -Sandy On Mon, Jul 28, 2014 at 7:15 AM, jitendra shelar  wrote: Hi Jun, Spark currently doesn't have that feature, i.e. it aims for a fixed number of executors per application regardless of resource usage, but it's definitely worth considering.  We could start more executors when we have a large backlog of tasks and shut some down when we're underutilized. The fine-grained task scheduling is blocked on work from YARN that will allow changing the CPU allocation of a YARN container dynamically.  The relevant JIRA for this dependency is YARN-1197, though YARN-1488 might serve this purpose as well if it comes first. -Sandy On Thu, Aug 7, 2014 at 10:56 PM, Jun Feng Liu  wrote: I think that would be useful work.  I don't know the minute details of this code, but in general TaskSchedulerImpl keeps track of pending tasks.  Tasks are organized into TaskSets, each of which corresponds to a particular stage.  Each TaskSet has a TaskSetManager, which directly tracks the pending tasks for that stage. -Sandy On Fri, Aug 8, 2014 at 12:37 AM, Jun Feng Liu  wrote: Hi Chutium, This is currently being addressed in https://github.com/apache/spark/pull/1825 -Sandy On Fri, Aug 8, 2014 at 2:26 PM, chutium  wrote: Hi Stephen, Have you tried the --jars option (with jars separated by commas)?  It should make the given jars available both to the driver and the executors. I believe one caveat currently is that if you give it a folder it won't pick up all the jars inside. -Sandy On Fri, Aug 15, 2014 at 4:07 PM, Stephen Boesch  wrote: Hi Debasish, The fix is to raise spark.yarn.executor.memoryOverhead until this goes away.  This controls the buffer between the JVM heap size and the amount of memory requested from YARN (JVMs can take up memory beyond their heap size). You should also make sure that, in the YARN NodeManager configuration, yarn.nodemanager.vmem-check-enabled is set to false. -Sandy On Wed, Aug 20, 2014 at 12:27 AM, Debasish Das  I could reproduce the issue in both 1.0 and 1.1 using YARN...so this is This doesn't help for every dependency, but Spark provides an option to build the assembly jar without Hadoop and its dependencies.  We make use of this in CDH packaging. -Sandy On Tue, Sep 2, 2014 at 2:12 AM, scwf  wrote: Hi Deb, The current state of the art is to increase spark.yarn.executor.memoryOverhead until the job stops failing.  We do have plans to try to automatically scale this based on the amount of memory requested, but it will still just be a heuristic. -Sandy On Tue, Sep 9, 2014 at 7:32 AM, Debasish Das  Hi Sandy, I would expect 2 GB would be enough or more than enough for 16 GB executors (unless ALS is using a bunch of off-heap memory?).  You mentioned earlier in this thread that the property wasn't showing up in the Environment tab. Are you sure it's making it in? -Sandy On Tue, Sep 9, 2014 at 11:58 AM, Debasish Das  Hmm...I did try it increase to few gb but did not get a successful run That's right On Tue, Sep 9, 2014 at 2:04 PM, Debasish Das  Last time it did not show up on environment tab but I will give it another After the change to broadcast all task data, is there any easy way to discover the serialized size of the data getting sent down for a task? thanks, -Sandy It used to be available on the UI, no? On Thu, Sep 11, 2014 at 6:26 PM, Reynold Xin  wrote: Hmm, well I can't find it now, must have been hallucinating.  Do you know off the top of your head where I'd be able to find the size to log it? On Thu, Sep 11, 2014 at 6:33 PM, Reynold Xin  wrote: Hi Jun, I believe that's correct that Spark authentication only works against YARN. -Sandy On Thu, Sep 11, 2014 at 2:14 AM, Jun Feng Liu  wrote: Hey All, A couple questions came up about shared variables recently, and I wanted to confirm my understanding and update the doc to be a little more clear. *Broadcast variables* Now that tasks data is automatically broadcast, the only occasions where it makes sense to explicitly broadcast are: * You want to use a variable from tasks in multiple stages. * You want to have the variable stored on the executors in deserialized form. * You want tasks to be able to modify the variable and have those modifications take effect for other tasks running on the same executor (usually a very bad idea). Is that right? *Accumulators* Values are only counted for successful tasks.  Is that right?  KMeans seems to use it in this way.  What happens if a node goes away and successful tasks need to be resubmitted?  Or the stage runs again because a different job needed it. thanks, Sandy Thanks for the heads up Cody.  Any indication of what was going wrong? On Mon, Sep 22, 2014 at 7:16 AM, Cody Koeninger  wrote: Hi Ye, I think git blame shows me because I fixed the formatting in core/pom.xml, but I don't actually know the original reason for setting SPARK_CLASSPATH there. Do the tests run OK if you take it out? -Sandy On Thu, Sep 25, 2014 at 1:59 AM, Ye Xianjin  wrote: It looks like the difference between the proposed Spark model and the CloudStack / SVN model is: * In the former, maintainers / partial committers are a way of centralizing oversight over particular components among committers * In the latter, maintainers / partial committers are a way of giving non-committers some power to make changes -Sandy On Thu, Nov 6, 2014 at 5:17 PM, Corey Nolet  wrote: Hey all, Was messing around with Spark and Google FlatBuffers for fun, and it got me thinking about Spark and serialization.  I know there's been work / talk about in-memory columnar formats Spark SQL, so maybe there are ways to provide this flexibility already that I've missed?  Either way, my thoughts: Java and Kryo serialization are really nice in that they require almost no extra work on the part of the user.  They can also represent complex object graphs with cycles etc. There are situations where other serialization frameworks are more efficient: * A Hadoop Writable style format that delineates key-value boundaries and allows for raw comparisons can greatly speed up some shuffle operations by entirely avoiding deserialization until the object hits user code. Writables also probably ser / deser faster than Kryo. * "No-deserialization" formats like FlatBuffers and Cap'n Proto address the tradeoff between (1) Java objects that offer fast access but take lots of space and stress GC and (2) Kryo-serialized buffers that are more compact but take time to deserialize. The drawbacks of these frameworks are that they require more work from the user to define types.  And that they're more restrictive in the reference graphs they can represent. In large applications, there are probably a few points where a "specialized" serialization format is useful. But requiring Writables everywhere because they're needed in a particularly intense shuffle is cumbersome. In light of that, would it make sense to enable varying Serializers within an app? It could make sense to choose a serialization framework both based on the objects being serialized and what they're being serialized for (caching vs. shuffle).  It might be possible to implement this underneath the Serializer interface with some sort of multiplexing serializer that chooses between subserializers. Nothing urgent here, but curious to hear other's opinions. -Sandy Ah awesome.  Passing customer serializers when persisting an RDD is exactly one of the things I was thinking of. -Sandy On Fri, Nov 7, 2014 at 1:19 AM, Matei Zaharia  Yup, the JIRA for this was https://issues.apache.org/jira/browse/SPARK-540 Currently there are no mandatory profiles required to build Spark.  I.e. "mvn package" just works.  It seems sad that we would need to break this. On Wed, Nov 12, 2014 at 10:59 PM, Patrick Wendell  I think printing an error that says "-Pscala-2.10 must be enabled" is https://github.com/apache/spark/pull/3239 addresses this On Thu, Nov 13, 2014 at 10:05 AM, Marcelo Vanzin  Hello there, Quizhang, This is a known issue that ExternalAppendOnlyMap can do tons of tiny spills in certain situations. SPARK-4452 aims to deal with this issue, but we haven't finalized a solution yet. Dinesh's solution should help as a workaround, but you'll likely experience suboptimal performance when trying to merge tons of small files from disk. -Sandy On Wed, Nov 19, 2014 at 10:10 PM, Dinesh J. Weerakkody  wrote: I think that if we were able to maintain the full set of created RDDs as well as some scheduler and block manager state, it would be enough for most apps to recover. On Wed, Dec 10, 2014 at 5:30 AM, Jun Feng Liu  wrote: +1 (non-binding).  Tested on Ubuntu against YARN. On Thu, Dec 11, 2014 at 9:38 AM, Reynold Xin  wrote: Hi Lochana, We haven't yet added this in 1.2. https://issues.apache.org/jira/browse/SPARK-4081 tracks adding categorical feature indexing, which one-hot encoding can be built on. https://issues.apache.org/jira/browse/SPARK-1216 also tracks a version of this prior to the ML pipelines work. -Sandy On Fri, Dec 12, 2014 at 6:16 PM, Lochana Menikarachchi  Do we have one-hot encoding in spark MLLib 1.1.1 or 1.2.0 ? It wasn't Hi Harikrishna, A good place to start is taking a look at the wiki page on contributing: https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark -Sandy On Fri, Dec 19, 2014 at 2:43 PM, Harikrishna Kamepalli  wrote: Hi Dirceu, Does the issue not show up if you run "map(f => f(1).asInstanceOf[Int]).sum" on the "train" RDD?  It appears that f(1) is an String, not an Int.  If you're looking to parse and convert it, "toInt"should be used instead of "asInstanceOf". -Sandy On Wed, Jan 21, 2015 at 8:43 AM, Dirceu Semighini Filho  wrote: Both SchemaRDD and DataFrame sound fine to me, though I like the former slightly better because it's more descriptive. Even if SchemaRDD's needs to rely on Spark SQL under the covers, it would be more clear from a user-facing perspective to at least choose a package name for it that omits "sql". I would also be in favor of adding a separate Spark Schema module for Spark SQL to rely on, but I imagine that might be too large a change at this point? -Sandy On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia  (Actually when we designed Spark SQL we thought of giving it another name, JIRA updates don't go to this list, they go to issues@spark.apache.org.  I don't think many are signed up for that list, and those that are probably have a flood of emails anyway. So I'd definitely be in favor of any JIRA cleanup that you're up for. -Sandy On Fri, Feb 6, 2015 at 6:45 AM, Sean Owen  wrote: +1 to what Andrew said, I think both make sense in different situations and trusting developer discretion here is reasonable. On Mon, Feb 9, 2015 at 1:48 PM, Andrew Or  wrote: +1 (non-binding, doc and packaging issues aside) Built from source, ran jobs and spark-shell against a pseudo-distributed YARN cluster. On Sun, Mar 8, 2015 at 2:42 PM, Krishna Sankar  wrote: Regarding Patrick's question, you can just do "new Configuration(oldConf)"to get a cloned Configuration object and add any new properties to it. -Sandy On Wed, Mar 25, 2015 at 4:42 PM, Imran Rashid  wrote: I definitely see the value in this.  However, I think at this point it would be an incompatible behavioral change.  People often use count in Spark to exercise their DAG.  Omitting processing steps that were previously included would likely mislead many users into thinking their pipeline was running faster. It's possible there might be room for something like a new smartCount API or a new argument to count that allows it to avoid unnecessary transformations. -Sandy On Sat, Mar 28, 2015 at 6:10 AM, Sean Owen  wrote: +1 Built against Hadoop 2.6 and ran some jobs against a pseudo-distributed YARN cluster. -Sandy On Wed, Apr 8, 2015 at 12:49 PM, Patrick Wendell  wrote: Hi Kannan, Both in MapReduce and Spark, the amount of shuffle data a task produces can exceed the tasks memory without risk of OOM. -Sandy On Tue, Apr 14, 2015 at 6:47 AM, Imran Rashid  wrote: I think there are maybe two separate things we're talking about? 1. Design discussions and in-progress design docs. My two cents are that JIRA is the best place for this.  It allows tracking the progression of a design across multiple PRs and contributors.  A piece of useful feedback that I've gotten in the past is to make design docs immutable.  When updating them in response to feedback, post a new version rather than editing the existing one.  This enables tracking the history of a design and makes it possible to read comments about previous designs in context.  Otherwise it's really difficult to understand why particular approaches were chosen or abandoned. 2. Completed design docs for features that we've implemented. Perhaps less essential to project progress, but it would be really lovely to have a central repository to all the projects design doc.  If anyone wants to step up to maintain it, it would be cool to have a wiki page with links to all the final design docs posted on JIRA. -Sandy On Fri, Apr 24, 2015 at 12:01 PM, Punyashloka Biswal  wrote: My only issue with Google Docs is that they're mutable, so it's difficult to follow a design's history through its revisions and link up JIRA comments with the relevant version. -Sandy On Mon, Apr 27, 2015 at 7:54 AM, Steve Loughran  One thing to consider is that while docs as PDFs in JIRAs do document the Spark currently doesn't allocate any memory off of the heap for shuffle objects.  When the in-memory data gets too large, it will write it out to a file, and then merge spilled filed later. What exactly do you mean by store shuffle data in HDFS? -Sandy On Tue, Apr 14, 2015 at 10:15 AM, Kannan Rajah  wrote: Hi Twinkle, Registering the class makes it so that writeClass only writes out a couple bytes, instead of a full String of the class name. -Sandy On Thu, Apr 30, 2015 at 4:13 AM, twinkle sachdeva  wrote: +1 (non-binding) Launched against a pseudo-distributed YARN cluster running Hadoop 2.6.0 and ran some jobs. -Sandy On Sat, May 30, 2015 at 3:44 PM, Krishna Sankar  wrote: +1 (non-binding) Built from source and ran some jobs against a pseudo-distributed YARN cluster. -Sandy On Fri, Jun 5, 2015 at 11:05 AM, Ram Sriharsha  +1 , tested  with hadoop 2.6/ yarn on centos 6.5 after building  w/ -Pyarn Hi Yash, One of the main advantages is that, if you turn dynamic allocation on, and executors are discarded, your application is still able to get at the shuffle data that they wrote out. -Sandy On Thu, Jun 25, 2015 at 11:08 PM, yash datta  wrote: Hi Su, Spark can't read excel files directly.  Your best best is probably to export the contents as a CSV and use the "csvFile" API. -Sandy On Mon, Jul 13, 2015 at 9:22 AM, spark user  Hi +1 On Sat, Jul 18, 2015 at 4:00 PM, Mridul Muralidharan  Thanks for detailing, definitely sounds better. Hi Tom, Not sure how much this helps, but are you aware that you can build Spark with the -Phadoop-provided profile to avoid packaging Hadoop dependencies in the assembly jar? -Sandy On Fri, Aug 14, 2015 at 6:08 AM, Thomas Dudziak  wrote: Cool, thanks! On Mon, Aug 24, 2015 at 2:07 PM, Reynold Xin  wrote: I see that there's an 1.5.0-rc2 tag in github now.  Is that the official RC2 tag to start trying out? -Sandy On Mon, Aug 24, 2015 at 7:23 AM, Sean Owen  wrote: Hi Justin, The Dataset API proposal is available here: https://issues.apache.org/jira/browse/SPARK-9999. -Sandy On Tue, Nov 3, 2015 at 1:41 PM, Justin Uang  wrote: To answer your fourth question from Cloudera's perspective, we would never support a customer running Spark 2.0 on a Hadoop version < 2.6. -Sandy On Fri, Nov 20, 2015 at 1:39 PM, Reynold Xin  wrote: