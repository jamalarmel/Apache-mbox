Hi, My limited understanding of Spark tells me that a task is the least possible working unit and Spark itself won't give you much. It wouldn't expect so since "acount" is a business entity not Spark's one. What about using mapPartitions* to know the details of partitions and do whatever you want (log to stdout or whatever)? Just a thought. Pozdrawiam, Jacek -- Jacek Laskowski | https://medium.com/@jaceklaskowski/ | http://blog.jaceklaskowski.pl Mastering Spark https://jaceklaskowski.gitbooks.io/mastering-apache-spark/ Follow me at https://twitter.com/jaceklaskowski Upvote at http://stackoverflow.com/users/1305344/jacek-laskowski Hi, While toying with Spark Standalone I've noticed the following messages in the logs of the master: INFO Master: Registering worker 192.168.1.6:59919 with 2 cores, 2.0 GB RAM INFO Master: localhost:59920 got disassociated, removing it. ... WARN Master: Removing worker-20151210090708-192.168.1.6-59919 because we got no heartbeat in 60 seconds INFO Master: Removing worker worker-20151210090708-192.168.1.6-59919 on 192.168.1.6:59919 Why does the message "WARN Master: Removing worker-20151210090708-192.168.1.6-59919 because we got no heartbeat in 60 seconds" appear when the worker should've been gone already (as pointed out in "INFO Master: localhost:59920 got disassociated, removing it.")? Could it be that the ids are different - 192.168.1.6:59919 vs localhost:59920? I started master using "./sbin/start-master.sh -h localhost" and the workers "./sbin/start-slave.sh spark://localhost:7077". p.s. Are such questions appropriate for this mailing list? Pozdrawiam, Jacek -- Jacek Laskowski | https://medium.com/@jaceklaskowski/ | http://blog.jaceklaskowski.pl Mastering Spark https://jaceklaskowski.gitbooks.io/mastering-apache-spark/ Follow me at https://twitter.com/jaceklaskowski Upvote at http://stackoverflow.com/users/1305344/jacek-laskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi, I'm on yesterday's master HEAD. Pozdrawiam, Jacek -- Jacek Laskowski | https://medium.com/@jaceklaskowski/ | http://blog.jaceklaskowski.pl Mastering Spark https://jaceklaskowski.gitbooks.io/mastering-apache-spark/ Follow me at https://twitter.com/jaceklaskowski Upvote at http://stackoverflow.com/users/1305344/jacek-laskowski Hi, While reviewing DAGScheduler, and where failedStages internal collection of failed staged ready for resubmission is used, I came across a question for which I'm looking an answer to. Any hints would be greatly appreciated. When resubmitFailedStages [1] is executed, and there are any failed stages, they are resubmitted using submitStage [2], but before it happens, failedStages is cleared [3] so when submitStage is called that will ultimately call submitMissingTasks for the stage, it checks whether the stage is in failedStages (among the other sets for waiting and running stages) [4]. My naive understanding is that the call to submitStage is a no-op in this case, i.e. nothing really happens and the if expression will silently finish without doing anything useful until some other event happens that changes the status of the failed stages into waiting ones. Is my understanding incorrect? Where? Could the call to submitStage be superfluous? Please guide in the right direction. Thanks. [1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L734 [2] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L743 [3] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L741 [4] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L919 Pozdrawiam, Jacek Jacek Laskowski | https://medium.com/@jaceklaskowski/ Mastering Apache Spark ==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/ Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi, With today's sources I'm facing "NoClassDefFoundError: org/spark-project/guava/collect/Maps" while starting standalone Master using ./sbin/start-master.sh. Anyone's working on it? File an issue? Spark Command: /Library/Java/JavaVirtualMachines/Current/Contents/Home/bin/java -cp /Users/jacek/dev/oss/spark/conf/:/Users/jacek/dev/oss/spark/assembly/target/scala-2.11/spark-assembly-2.0.0-SNAPSHOT-hadoop2.7.1.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-api-jdo-3.2.6.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-core-3.2.10.jar:/Users/jacek/dev/oss/spark/lib_managed/jars/datanucleus-rdbms-3.2.9.jar -Xms1g -Xmx1g org.apache.spark.deploy.master.Master --ip japila.local --port 7077 --webui-port 8080 ======================================== Setting default log level to "WARN". To adjust logging level use sc.setLogLevel(newLevel). Exception in thread "main" java.lang.NoClassDefFoundError: org/spark-project/guava/collect/Maps at org.apache.hadoop.metrics2.lib.MetricsRegistry.(MetricsRegistry.java:42) at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.(MetricsSystemImpl.java:94) at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.(MetricsSystemImpl.java:141) at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.(DefaultMetricsSystem.java:38) at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.(DefaultMetricsSystem.java:36) at org.apache.hadoop.security.UserGroupInformation$UgiMetrics.create(UserGroupInformation.java:120) at org.apache.hadoop.security.UserGroupInformation.(UserGroupInformation.java:236) at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2156) at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2156) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2156) at org.apache.spark.SecurityManager.(SecurityManager.scala:214) at org.apache.spark.deploy.master.Master$.startRpcEnvAndEndpoint(Master.scala:1108) at org.apache.spark.deploy.master.Master$.main(Master.scala:1093) at org.apache.spark.deploy.master.Master.main(Master.scala) Caused by: java.lang.ClassNotFoundException: org.spark-project.guava.collect.Maps at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) ... 15 more Pozdrawiam, Jacek Jacek Laskowski | https://medium.com/@jaceklaskowski/ Mastering Apache Spark ==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/ Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi, I think the change is related: https://github.com/apache/spark/commit/659fd9d04b988d48960eac4f352ca37066f43f5c as it touches the dependency in pom.xml. Pozdrawiam, Jacek Jacek Laskowski | https://medium.com/@jaceklaskowski/ Mastering Apache Spark ==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/ Follow me at https://twitter.com/jaceklaskowski Figured it out and reported https://issues.apache.org/jira/browse/SPARK-12736. Fix's coming... Pozdrawiam, Jacek Jacek Laskowski | https://medium.com/@jaceklaskowski/ Mastering Apache Spark ==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/ Follow me at https://twitter.com/jaceklaskowski Hi, https://github.com/apache/spark/pull/10674 Please review and merge at your convenience. Thanks! Pozdrawiam, Jacek Jacek Laskowski | https://medium.com/@jaceklaskowski/ Mastering Apache Spark ==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/ Follow me at https://twitter.com/jaceklaskowski Hi, Tried to build the sources today with Scala 2.11 twice and it failed. No local changes. Restarted zinc. Can anyone else confirm it? Since the error is buried in the logs I'm asking now without offering more information (before I catch the cause) so I or *the issue* get corrected (whatever first :)). Thanks! Pozdrawiam, Jacek Jacek Laskowski | https://medium.com/@jaceklaskowski/ Mastering Apache Spark ==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/ Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi, My very rough investigation has showed that the commit to may have broken the build was https://github.com/apache/spark/commit/555127387accdd7c1cf236912941822ba8af0a52 (nongli committed with rxin 7 hours ago). Found a fix and building the source again... Pozdrawiam, Jacek Jacek Laskowski | https://medium.com/@jaceklaskowski/ Mastering Apache Spark ==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/ Follow me at https://twitter.com/jaceklaskowski Hi, Pull request submitted https://github.com/apache/spark/pull/10946/files. Please review and merge. Pozdrawiam, Jacek Jacek Laskowski | https://medium.com/@jaceklaskowski/ Mastering Apache Spark ==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/ Follow me at https://twitter.com/jaceklaskowski Hi, Is there a reason to use conf to read SPARK_WORKER_MEMORY not System.getenv as for the other env vars? https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/worker/WorkerArguments.scala#L45 Pozdrawiam, Jacek Jacek Laskowski | https://medium.com/@jaceklaskowski/ Mastering Apache Spark ==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/ Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi devs, Following up on this, it appears that spark.worker.ui.port can only be set in --properties-file. I wonder why conf/spark-defaults.conf is *not* used for the spark.worker.ui.port property? Any reason for the decision? Pozdrawiam, Jacek Jacek Laskowski | https://medium.com/@jaceklaskowski/ Mastering Apache Spark ==> https://jaceklaskowski.gitbooks.io/mastering-apache-spark/ Follow me at https://twitter.com/jaceklaskowski Hi Praveen, I've spent few hours on the changes related to streaming dataframes (included in the SPARK-8360) and concluded that it's currently only possible to read.stream(), but not write.stream() since there are no streaming Sinks yet. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi Praveen, I don't really know. I think TD or Michael should know as they personally involved in the task (as far as I could figure it out from the JIRA and the changes). Ping people on the JIRA so they notice your question(s). Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, While reviewing ClassificationModel custom implementations, I found that out of 4 "production" models two are final while the other two are not. Is there any reason for this? ** `DecisionTreeClassificationModel` (`final`) ** `RandomForestClassificationModel` (`final`) ** `LogisticRegressionModel` ** `NaiveBayesModel` Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi, After few weeks with spark.ml now, I came to conclusion that Transformer concept from Pipeline API (spark.ml/MLlib) should be part of DataFrame (SQL) where they fit better. Are there any plans to migrate Transformer API (ML) to DataFrame (SQL)? Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi, Came across `private class ColumnPruner` with "TODO(ekl) make this a public transformer" in scaladoc, cf. https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/feature/RFormula.scala#L317. Why is this private and is there a JIRA for the TODO(ekl)? Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi Joseph, Thanks for the response. I'm one who doesn't understand all the hype/need for Machine Learning...yet and through Spark ML(lib) glasses I'm looking at ML space. In the meantime I've got few assignments (in a project with Spark and Scala) that have required quite extensive dataset manipulation. It was when I sinked into using DataFrame/Dataset for data manipulation not RDD (I remember talking to Brian about how RDD is an "assembly" language comparing to the higher-level concept of DataFrames with Catalysts and other optimizations). After few days with DataFrame I learnt he was so right! (sorry Brian, it took me longer to understand your point). I started using DataFrames in far too many places than one could ever accept :-) I was so...carried away with DataFrames (esp. show vs foreach(println) and UDFs via udf() function) And then, when I moved to Pipeline API and discovered Transformers. And PipelineStage that can create pipelines of DataFrame manipulation. They read so well that I'm pretty sure people would love using them more often, but...they belong to MLlib so they are part of ML space (not many devs tackled yet). I applied the approach to using withColumn to have better debugging experience (if I ever need it). I learnt it after having watched your presentation about Pipeline API. It was so helpful in my RDD/DataFrame space. So, to promote a more extensive use of Pipelines, PipelineStages, and Transformers, I was thinking about moving that part to SQL/DataFrame API where they really belong. If not, I think people might miss the beauty of the very fine and so helpful Transformers. Transformers are *not* a ML thing -- they are DataFrame thing and should be where they really belong (for their greater adoption). What do you think? Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, Although I'm not that much experienced member of ASF, I share your concerns. I haven't looked at the issue from this point of view, but after having read the thread I think PMC should've signed off the migration of ASF-owned code to a non-ASF repo. At least a vote is required (and this discussion is a sign that the process has not been conducted properly as people have concerns, me including). Thanks Mridul! Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, (since 2.0.0-SNAPSHOT it's more for dev not user) With today's master I'm getting the following: scala> ds res14: org.apache.spark.sql.Dataset[(String, Int)] = [_1: string, _2: int] // WHY?! scala> ds.groupBy(_._1) :26: error: missing parameter type for expanded function ((x$1) => x$1._1) ds.groupBy(_._1) ^ scala> ds.filter(_._1.size > 10) res23: org.apache.spark.sql.Dataset[(String, Int)] = [_1: string, _2: int] It's even on the slide of Michael in https://youtu.be/i7l3JQRx7Qw?t=7m38s from Spark Summit East?! Am I doing something wrong? Please guide. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi Spark devs, I'm unsure if what I'm seeing is correct. I'd appreciate any input to...rest my nerves :-) I did `import org.apache.spark._` by mistake, but since it's valid, I'm wondering why does Spark shell imports sql at all since it's available after the import?! (it's today's build) scala> sql("SELECT * FROM dafa").show(false) :30: error: reference to sql is ambiguous; it is imported twice in the same scope by import org.apache.spark._ and import sqlContext.sql sql("SELECT * FROM dafa").show(false) ^ scala> :imports 1) import sqlContext.implicits._  (52 terms, 31 are implicit) 2) import sqlContext.sql          (1 terms) scala> sc.version res19: String = 2.0.0-SNAPSHOT Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi, In https://github.com/apache/spark/blob/master/streaming/src/test/scala/org/apache/spark/streaming/DStreamClosureSuite.scala#L190: { return; ssc.sparkContext.emptyRDD[Int] } What is this return inside for? I don't understand the line and am about to propose a change to remove it. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi Ted, Yeah, I saw the line, but forgot it's a test that may have been testing that closures should not have return. More clear now. Thanks! Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, Just checked out the latest sources and got this... /Users/jacek/dev/oss/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/stringExpressions.scala:626: error: annotation argument needs to be a constant; found: "_FUNC_(str) - ".+("Returns str, with the first letter of each word in uppercase, all other letters in ").+("lowercase. Words are delimited by white space.") "Returns str, with the first letter of each word in uppercase, all other letters in " + ^ It's in https://github.com/apache/spark/commit/c59abad052b7beec4ef550049413e95578e545be. Is this a real issue with the build now or is this just me? I may have seen a similar case before, but can't remember what the fix was. Looking into it. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi, Is this me or the build is broken today? I'm looking for help as it looks scary. $ ./build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.7.2 -Phive -Phive-thriftserver -DskipTests clean install [INFO] --- scala-maven-plugin:3.2.2:testCompile (scala-test-compile-first) @ spark-mllib-local_2.11 --- [INFO] Using zinc server for incremental compilation [warn] Pruning sources from previous analysis, due to incompatible CompileSetup. [info] Compiling 1 Scala source to /Users/jacek/dev/oss/spark/mllib-local/target/scala-2.11/test-classes... [error] missing or invalid dependency detected while loading class file 'SparkFunSuite.class'. [error] Could not access type Logging in package org.apache.spark.internal, [error] because it (or its dependencies) are missing. Check your build definition for [error] missing or conflicting dependencies. (Re-run with `-Ylog-classpath` to see the problematic classpath.) [error] A full rebuild may help if 'SparkFunSuite.class' was compiled against an incompatible version of org.apache.spark.internal. [error] one error found [error] Compile failed at Apr 9, 2016 2:27:30 PM [0.475s] [INFO] ------------------------------------------------------------------------ [INFO] Reactor Summary: [INFO] [INFO] Spark Project Parent POM ........................... SUCCESS [  4.338 s] [INFO] Spark Project Test Tags ............................ SUCCESS [  5.238 s] [INFO] Spark Project Sketch ............................... SUCCESS [  6.158 s] [INFO] Spark Project Networking ........................... SUCCESS [ 10.397 s] [INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [  7.263 s] [INFO] Spark Project Unsafe ............................... SUCCESS [ 10.448 s] [INFO] Spark Project Launcher ............................. SUCCESS [ 11.028 s] [INFO] Spark Project Core ................................. SUCCESS [02:04 min] [INFO] Spark Project GraphX ............................... SUCCESS [ 16.973 s] [INFO] Spark Project Streaming ............................ SUCCESS [ 38.458 s] [INFO] Spark Project Catalyst ............................. SUCCESS [01:18 min] [INFO] Spark Project SQL .................................. SUCCESS [01:24 min] [INFO] Spark Project ML Local Library ..................... FAILURE [  1.083 s] [INFO] Spark Project ML Library ........................... SKIPPED [INFO] Spark Project Tools ................................ SKIPPED [INFO] Spark Project Hive ................................. SKIPPED [INFO] Spark Project Docker Integration Tests ............. SKIPPED [INFO] Spark Project REPL ................................. SKIPPED [INFO] Spark Project YARN Shuffle Service ................. SKIPPED [INFO] Spark Project YARN ................................. SKIPPED [INFO] Spark Project Hive Thrift Server ................... SKIPPED [INFO] Spark Project Assembly ............................. SKIPPED [INFO] Spark Project External Flume Sink .................. SKIPPED [INFO] Spark Project External Flume ....................... SKIPPED [INFO] Spark Project External Flume Assembly .............. SKIPPED [INFO] Spark Project External Kafka ....................... SKIPPED [INFO] Spark Project Examples ............................. SKIPPED [INFO] Spark Project External Kafka Assembly .............. SKIPPED [INFO] Spark Project Java 8 Tests ......................... SKIPPED [INFO] ------------------------------------------------------------------------ [INFO] BUILD FAILURE [INFO] ------------------------------------------------------------------------ [INFO] Total time: 06:39 min [INFO] Finished at: 2016-04-09T14:27:30-04:00 [INFO] Final Memory: 79M/893M [INFO] ------------------------------------------------------------------------ [ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:3.2.2:testCompile (scala-test-compile-first) on project spark-mllib-local_2.11: Execution scala-test-compile-first of goal net.alchim31.maven:scala-maven-plugin:3.2.2:testCompile failed. CompileFailed -> [Help 1] [ERROR] [ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch. [ERROR] Re-run Maven using the -X switch to enable full debug logging. [ERROR] [ERROR] For more information about the errors and possible solutions, please read the following articles: [ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/PluginExecutionException [ERROR] [ERROR] After correcting the problems, you can resume the build with the command [ERROR]   mvn  -rf :spark-mllib-local_2.11 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi Ted et al, [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 10:41 min [INFO] Finished at: 2016-04-09T19:21:02-04:00 [INFO] Final Memory: 106M/961M [INFO] ------------------------------------------------------------------------ Thank you so much for the prompt solution! And that's while I was driving from Toronto to Mississauga. Thanks! Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, While reviewing explain(extended: Boolean) - https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L408 - I've noticed that: 1. It first creates ExplainCommand that does sqlContext.executePlan(logicalPlan) in run https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/command/commands.scala#L247 2. And then calls sqlContext.executePlan(explain) https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L410 Why are there two sqlContext.executePlan's? It appears that we calls the former to execute the latter (?) I'm confused. Please explain :) I'd appreciate. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi, While working with structured streaming (aka SparkSQL Streams :)) I thought about adding implicit def toProcessingTime(duration: Duration) = ProcessingTime(duration) What do you think? I think it'd improve the API: .trigger(ProcessingTime(10 seconds)) vs .trigger(10 seconds) (since it's not a release feature I didn't mean to file an issue in JIRA - please guide if needed). Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org When you say "in the future", do you have any specific timeframe in mind? You got me curious :) Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, I'd love having a more elaborate toString to StreamExecution: scala> sqlContext.streams.active.foreach(println) Continuous Query - memStream [state = ACTIVE] Continuous Query - hello2 [state = ACTIVE] Continuous Query - hello [state = ACTIVE] Any work in this area? trigger is something it could have. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi Yanbo, https://issues.apache.org/jira/browse/SPARK-14730 Thanks! Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Thanks Reynold! I was going to ask about that one as it breaks the build for me. [info] Compiling 1 Scala source to /Users/jacek/dev/oss/spark/sql/hivecontext-compatibility/target/scala-2.11/classes... [error] /Users/jacek/dev/oss/spark/sql/hivecontext-compatibility/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala:32: overriding value sparkSession in class SQLContext of type org.apache.spark.sql.SparkSession; [error]  value sparkSession has weaker access privileges; it should not be private [error]     @transient private val sparkSession: SparkSession, [error]                            ^ [error] one error found [error] Compile failed at Apr 26, 2016 6:28:01 AM [0.449s] Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski [INFO] ------------------------------------------------------------------------ [INFO] BUILD SUCCESS [INFO] ------------------------------------------------------------------------ [INFO] Total time: 10:53 min [INFO] Finished at: 2016-04-26T06:56:49+02:00 [INFO] Final Memory: 107M/890M [INFO] ------------------------------------------------------------------------ Thanks Reynold! :) Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, While reviewing TaskSchedulerImpl I've noticed that rootPool is created and initialized in TaskSchedulerImpl#initialize [1], but seems legit to do it as part of TaskSchedulerImpl's instantiation. What is the reason for creating and initializing rootPool late in TaskSchedulerImpl's lifecycle? [1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala#L131-L142 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi, I'm wondering why LiveListenerBus has two AtomicBoolean flags [1]? Could it not have just one, say started? Why does Spark have to check the stopped state? [1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/LiveListenerBus.scala#L49-L51 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi Jim, There's no C++ API in Spark to access the off-heap data. Moreover, I also think "off-heap" has an overloaded meaning in Spark - for tungsten and to persist your data off-heap (it's all about memory but for different purposes and with client- and internal API). That's my limited understanding of the things (and I'm not even sure how trustworthy it is). Use with extreme caution. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, While reviewing where SPARK_YARN_MODE is used and how, I found one "weird" place where the "yarn-client" is checked against - see https://github.com/apache/spark/blob/master/repl/scala-2.10/src/main/scala/org/apache/spark/repl/SparkILoop.scala#L946. Since yarn-client (and yarn-cluster) are no longer in use, I'm pretty sure it's of no use and could be safely removed. If not, we should do something with it anyway. Please guide before I file a JIRA issue. Thanks. p.s. On to hunting SPARK_YARN_MODE... Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi, Congrats Yanbo! p.s. It should go to user@, too. Jacek Hi, Just noticed that yarn.Client#populateClasspath uses Path.SEPARATOR [1] to build a CLASSPATH entry while another similar-looking line uses buildPath method [2]. Could a pull request with a change to use buildPath at [1] be accepted? I'm always confused how to fix such small changes. [1] https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L1298 [2] Path.SEPARATOR Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi Steve and Sean, Didn't expect such a warm welcome from Sean and you! Since I'm with Spark on YARN these days, let me see what I can do to make it nicer. Thanks! I'm going to change Spark to use buildPath first. And then propose another patch to use Environment.CLASS_PATH_SEPARATOR instead. And only then I could work on https://issues.apache.org/jira/browse/YARN-5247. Is this about changing the annotation(s) only? Thanks for your support! Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski That's be awesome to have another 2.0 RC! I know many people who'd consider it as a call to action to play with 2.0. +1000 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski +1 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, Whenever I see `backend.reviveOffers()` I'm struggling myself with properly explaining what it does. My understanding is that it requests a SchedulerBackend (that's responsible for talking to a cluster manager) to...that's the moment I'm not sure about. How would you explain `backend.reviveOffers()`? p.s. I understand that it's somehow related to how Mesos manages resources where it offers resources, but can't find anything related to `reviving offers` in Mesos docs :(Please guide. Thanks! Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi, After reviewing makeOffer and launchTasks in CoarseGrainedSchedulerBackend I came to the following conclusion: Scheduling in Spark relies on cores only (not memory), i.e. the number of tasks Spark can run on an executor is constrained by the number of cores available only. When submitting Spark application for execution both -- memory and cores -- can be specified explicitly. Would you agree? Do I miss anything important? I was very surprised when I found it out as I thought that memory would also have been a limiting factor. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi, I've just noticed that there is the private[spark] val SHUFFLE_SERVICE_ENABLED in package object config [1] [1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/internal/config/package.scala#L74-L75 However MesosCoarseGrainedSchedulerBackend [2], BlockManager [3] and Utils [4] are all using their own copies. Would that be acceptable* to send a pull request to get rid of this redundancy? [*] I'm staring at @srowen for his nodding in agreement :-) [2] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosCoarseGrainedSchedulerBackend.scala#L71 [3] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/storage/BlockManager.scala#L73-L74 [4] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/Utils.scala#L748 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Thanks Sean. I'm going to create a JIRA for it and start the work under it. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, While reviewing the release notes for 1.6.2 I stumbled upon https://issues.apache.org/jira/browse/SPARK-13522. It's got Target Version/s: 2.0.0 with Fix Version/s: 1.6.2, 2.0.0. What's the meaning of Target Version/s in Spark? Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi, That makes sense. Thanks Dongjoon for the very prompt response! Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi Sean, devs, How is this possible that Fix Version/s is 2.0.1 given 2.0.0 was not released yet? Why is that that master is not what's going to be released so eventually becomes 2.0.0? I don't get it. Appreciate any guidance. Thanks. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, Thanks Sean! It makes sense. I'm not fully convinced that's how it should be, so I apologize if I ever ask about the version management in Spark again :) Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, Always release from master. What could be the gotchas? Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, Why would I need to start 2.1? If it's ready for master, why could it be not part of 2.0? "Release early and often" is what would benefit Spark a lot. The time to ship 2.0 is far too long I think. And I know companies that won't use 2.0 because...it's "0" version :-(Jacek Hi Sean, What's wrong with the following release procedure? 1. Use master to create RC (versions - master: 2.0.0-SNAPSHOT branch: 2.0.0-RC1) 2. Add new features to master (versions - master: 2.0.0-SNAPSHOT branch: 2.0.0-RC1) 3. RC passes a vote => ship it (versions - master: 2.1.0-SNAPSHOT branch: 2.0.0-RC1 + branch: 2.0.0) <-- master changes to another SNAPSHOT + copy of 2.0.0-RC1 to 2.0.0 4. RC doesn't pass a vote => cut another RC (versions - master: 2.0.0-SNAPSHOT branch: 2.0.0-RC2) Repeat 2 + 3 + 4 until RC passes a vote. master is PRed as usual. What am I missing? I must be missing something, but can't see it. You're right, it has nothing to do with pace of release but the project needs frequent releases say quarterly. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, It's with the master built today. Why can't I call ds.foreachPartition(println)? Is using type annotation the only way to go forward? I'd be so sad if that's the case. scala> ds.foreachPartition(println) :28: error: overloaded method value foreachPartition with alternatives: (func: org.apache.spark.api.java.function.ForeachPartitionFunction[Record])Unit (f: Iterator[Record] => Unit)Unit cannot be applied to (Unit) ds.foreachPartition(println) ^ scala> sc.version res9: String = 2.0.0-SNAPSHOT Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Sort of. Your example works, but could you do a mere ds.foreachPartition(println)? Why not? What should I even see the Java version? scala> val ds = spark.range(10) ds: org.apache.spark.sql.Dataset[Long] = [id: bigint] scala> ds.foreachPartition(println) :26: error: overloaded method value foreachPartition with alternatives: (func: org.apache.spark.api.java.function.ForeachPartitionFunction[Long])Unit (f: Iterator[Long] => Unit)Unit cannot be applied to (Unit) ds.foreachPartition(println) ^ Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski ds is Dataset and the problem is that println (or any other one-element function) would not work here (and perhaps other methods with two variants - Java's and Scala's). Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Well, there is foreach for Java and another foreach for Scala. That's what I can understand. But while supporting two language-specific APIs -- Scala and Java -- Dataset API lost support for such simple calls without type annotations so you have to be explicit about the variant (since I'm using Scala I want to use Scala API right). It appears that any single-argument-function operators in Datasets are affected :(My question was to know whether there are works to fix it (if possible -- I don't know if it is). Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi Reynold, Is this already reported and tracked somewhere. I'm quite sure that people will be asking about the reasons Spark does this. Where are such issues reported usually? Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Thanks Cody, Reynold, and Ryan! Learnt a lot and feel "corrected". Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, Use jps -lm and see the processes on the machine(s) to kill. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, It appears you're running local mode (local[*] assumed) so killing spark-shell *will* kill the one and only executor -- the driver :) Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, Then use --master with spark standalone, yarn, or mesos. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, Read the doc http://spark.apache.org/docs/latest/spark-standalone.html which seems to be the cluster manager the OP uses. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski And the reason is that not all Spark installations are for YARN as the cluster manager. Jacek Hi, It seems that the current master is broken twice. I've just sent a PR for the first one. Please review and merge. https://github.com/apache/spark/pull/14315 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, Since spark.driver.appUIAddress is only used in Spark on YARN to "announce" the web UI's address, I think the setting should rather be called spark.yarn.driver.appUIAddress (for consistency with the other YARN-specific settings). What do you think? I'd like to hear your thoughts before filling an JIRA issue. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Kill 'em all -- one by one slowly yet gradually! :) Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, When ApplicationMaster runs it registers a shutdown hook [1] that (quoting the comment [2] from the code): And so it gets priority lower than SparkContext [3], i.e. val priority = ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY - 1 But, reading ShutdownHookManager.addShutdownHook says [4]: My understanding is that one comment is no longer true (if it has ever been). Please help me understand that part of the code. Thanks. [1] https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala#L206 [2] https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala#L204 [3] https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala#L205 [4] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/ShutdownHookManager.scala#L146-L147 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, I've been reviewing YarnAllocator.updateResourceRequests and think the other branch is...too verbose (and may be deceiving that it's more complex than it really is). I hope to get corrected. The source code of the method is https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala#L294. The method is about requesting new or cancelling outstanding YARN container requests for executors. Right? It starts by checking the executor container requests using getPendingAllocate [1] and branches by missing (the other branch uses numPendingAllocate also which is going to be crucial in my understanding of the method). So, in the other branch, when the number of outstanding YARN containers is too much [2], the method calls [3] amClient.getMatchingRequests(RM_REQUEST_PRIORITY, ANY_HOST, resource) which is exactly getPendingAllocate [4] (!) Is that correct? If my understanding is correct, the code does not need to call getPendingAllocate in the branch since it had already requested it in [1] (at the very beginning) and since we're inside the branch the code does not have to check "matchingRequests.isEmpty" either. My understanding is that the code should be as simple as the following: } else if (numPendingAllocate > 0 && missing < 0) {val numToCancel = math.min(numPendingAllocate, -missing) logInfo(s"Canceling requests for $numToCancel executor container(s) to have a new desired " + s"total $targetNumExecutors executors.") pendingAllocate.take(numToCancel).foreach(amClient.removeContainerRequest) } i.e. just a single pendingAllocate.take... Is that correct? THANKS a lot for reading thus far!!! I greatly appreciate your time and effort to help me understand Spark. p.s. I'm ready with a PR. [1] https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala#L295 [2] https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala#L355 [3] https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala#L360 [4] https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala#L200 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Congrats Felix! (I don't believe SparkR is ever getting close to how excellent Spark/Scala is but it's worth seeing some competition in this area :)) Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi Chris, With my ASF member hat on... Oh, come on, Chris. It's not "in violation of ASF policies"whatsoever. Policies are for ASF developers not for users. Honestly, I was surprised to read the note in Mark Hamstra's email. It's very restrictive but it says about what committers and PMCs should do not users: "Do not include any links on the project website that might encourage non-developers to download and use nightly builds, snapshots, release candidates, or any other similar package."Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Thanks Sean. That reflects my sentiments so well! Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi Tim, AWESOME. Thanks a lot for releasing it. That makes me even more eager to see it in Spark's codebase (and replacing the current RDD-based API)! Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, I'm working with today's build and am facing the issue: scala> Seq(A(4)).toDS 16/08/16 19:26:26 ERROR RetryingHMSHandler: AlreadyExistsException(message:Database default already exists) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891) ... res1: org.apache.spark.sql.Dataset[A] = [id: int] scala> spark.version res2: String = 2.1.0-SNAPSHOT See the complete stack trace at https://gist.github.com/jaceklaskowski/a969fdd5c2c9cdb736bf647b01257a3e. I'm quite positive that it didn't happen a day or two ago. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, I'm wondering how far off base I am with the question: Is a LogicalPlan in #SparkSQL similar to a RDD in #ApacheSpark Core in that they both seem a metadata of the computation that eventually gets executed to produce records? What am I missing if anything? How imprecise I am by comparing LogicalPlan to RDD? I'm considering a Dataset a pair of a LogicalPlan and an Encoder where the encoder is to handle the data in a more sophisticated, low-level way while LogicalPlan is how the data is computed/retrieved. Please help me understand the concepts in a more accurate way. Thanks! Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi devs, While reviewing the code in Catalyst for doing query parsing I found that UnresolvedStar has this typo in the exception [1]. I do understand that it's a very trivial issue but I thought I'd write a test for it as part of the change so I could improve my understanding of the low-level bits and bytes of Catalyst. scala> ds.select("hello.*") org.apache.spark.sql.AnalysisException: cannot resolve 'hello.*' give input columns ''; at org.apache.spark.sql.catalyst.analysis.UnresolvedStar.expand(unresolved.scala:249) What test in Spark is the closest proximity to the class that I could extend to assert the exception's message? How to run the test? Is sbt catalyst/testOnly [testName] enough? Please guide. Thanks. [1] https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/unresolved.scala#L249 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, Just noticed that LogicalPlan.isStreaming flag [1] is inconsistent name-wise to analyzed and resolved flags. Why is isStreaming not a mere "streaming"? That would make sense to me (esp. given the others). (I did manage to find a reason why it could be isStreaming but would like to hear the real reason). Thanks. [1] https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala#L46 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, Analyzer.resolver returns a Resolver per CatalystConf setting [1] that seems a duplicate of CatalystConf.resolver. Unless I'm mistaken, the code could get simpler and be as follows: def resolver: Resolver = conf.resolver Would you agree? I'm ready with a PR if so (plus some other typo fixes). [1] https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L67-L73 [2] https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/CatalystConf.scala#L43-L45 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi Michael, Congrats! BTW What I like about the change the most is that it uses the pluggable interface for TaskScheduler and SchedulerBackend (as introduced by YARN). Think Standalone should follow the steps. WDYT? Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, I've been playing with UDFs and why they're a blackbox for Spark's optimizer and started with filters to showcase the optimizations in play. My current understanding is that the predicate pushdowns are supported by the following data sources: 1. Hive tables 2. Parquet files 3. ORC files 4. JDBC While working on examples I came to a conclusion that not only does predicate pushdown work for the data sources mentioned above but solely for DataFrames. That was quite interesting since I was so much into Datasets as strongly type-safe data abstractions in Spark SQL. Can you help me to find the truth? Any links to videos, articles, commits and such to further deepen my understanding of optimizations in Spark SQL 2.0? I'd greatly appreciate. The following query pushes the filter down to Parquet (see PushedFilters attribute at the bottom) scala> cities.filter('name === "Warsaw").queryExecution.executedPlan res30: org.apache.spark.sql.execution.SparkPlan = *Project [id#196L, name#197] +- *Filter (isnotnull(name#197) && (name#197 = Warsaw)) +- *FileScan parquet [id#196L,name#197] Batched: true, Format: ParquetFormat, InputPaths: file:/Users/jacek/dev/oss/spark/cities.parquet, PartitionFilters: [], PushedFilters: [IsNotNull(name), EqualTo(name,Warsaw)], ReadSchema: struct Why does this not work for Datasets? Is the function/lambda too complex? Are there any examples where it works for Datasets? Are we perhaps trading strong type-safety over optimizations like predicate pushdown (and the feature's are yet to come in the next releases of Spark 2)? scala> cities.as[(Long, String)].filter(_._2 == "Warsaw").queryExecution.executedPlan res31: org.apache.spark.sql.execution.SparkPlan = *Filter .apply +- *FileScan parquet [id#196L,name#197] Batched: true, Format: ParquetFormat, InputPaths: file:/Users/jacek/dev/oss/spark/cities.parquet, PartitionFilters: [], PushedFilters: [], ReadSchema: struct Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi Reynold, That's what I was told few times already (most notably by Adam on twitter), but couldn't understand what it meant exactly. It's only now when I understand what you're saying, Reynold :) Does this put DataFrame's Column-based or SQL-based queries usually faster than Datasets with Encoders? How much I'm wrong to claim that for parquet files, Hive tables, and JDBC tables using DataFrame + Columns/SQL-based queries is usually faster than Datasets? Is that Datasets only shine for strongly typed queries with data sources with no support for such optimizations like filter pushdown? I'm tempted to say that for some data sources DataFrames are faster than Datasets...always. True? What am I missing? https://twitter.com/jaceklaskowski/status/770554918419755008 Thanks a lot, Reynold, for helping me out to get the gist of it all! Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, Definitely well deserved. Don't check your emails for the 2 weeks. Not even for a minute :-) Jacek Hi, I'm concerned with the OOME in local mode with the version built today: scala> val intsMM = 1 to math.pow(10, 3).toInt intsMM: scala.collection.immutable.Range.Inclusive = Range(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 1... scala> val df = intsMM.toDF("n").withColumn("m", 'n % 2) df: org.apache.spark.sql.DataFrame = [n: int, m: int] scala> df.groupBy('m).agg(sum('n)).show ... 16/09/06 22:28:02 ERROR Executor: Exception in task 6.0 in stage 0.0 (TID 6) java.lang.OutOfMemoryError: Unable to acquire 262144 bytes of memory, got 0 ... Please see https://gist.github.com/jaceklaskowski/906d62b830f6c967a7eee5f8eb6e9237 and let me know if I should file an issue. I don't think 10^3 elements and groupBy should kill spark-shell. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi Josh, Yes, that seems to be the issue. As I commented out in the JIRA, just yesterday (after I had sent the email), such simple queries like the following killed spark-shell: Seq(1).toDF.groupBy('value).count.show Hoping to see it get resolved soon. If there's anything I could help you with to fix/reproduce the issue, let me know. I wish I knew how to write a unit test for this. Where in the code to look for inspiration? Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, I'm wondering what's the rationale for checking the path option eagerly in FileStreamSource? My thinking is that until start is called there's no processing going on that is supposed to happen on executors (not the driver) with the path available. I could (and perhaps should) use dfs but IMHO that just hides the real question of the text source eagerness. Please help me understand the rationale of the choice. Thanks! scala> spark.version res0: String = 2.1.0-SNAPSHOT scala> spark.readStream.format("text").load("/var/logs") org.apache.spark.sql.AnalysisException: Path does not exist: /var/logs; at org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:229) at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:81) at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:81) at org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:30) at org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:142) at org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:153) ... 48 elided Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi Steve, Thank you for more source-oriented answer. Helped but didn't explain the reason for such eagerness. The file(s) might not be on the driver but on executors only where the Spark job(s) run. I don't see why Spark should check the file(s) regardless of glob pattern being used. You see my way of thinking? Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, The code is not consistent with @scala.annotation.varargs annotation. There are classes with @scala.annotation.varargs like DataFrameReader or functions as well as examples of @_root_.scala.annotation.varargs, e.g. Window or UserDefinedAggregateFunction. I think it should be consistent and @scala.annotation.varargs only. WDYT? Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, I've stumbled upon CatalogImpl.makeDataset [1] -- the only private[sql] method in the CatalogImpl object -- that looks like SparkSession.createDataset [2]. What do you think about removing CatalogImpl.makeDataset? If not, what's so special about one over the other to keep them both? I'd appreciate your help with this. Thanks. [1] https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/internal/CatalogImpl.scala#L385 [2] https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/SparkSession.scala#L413 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, Just noticed in assembly/target/scala-2.11/jars that similar libraries have different versions: -rw-r--r--  1 jacek  staff   1230201 17 wrz 09:51 netty-3.8.0.Final.jar -rw-r--r--  1 jacek  staff   2305335 17 wrz 09:51 netty-all-4.0.41.Final.jar and -rw-r--r--  1 jacek  staff    218076 17 wrz 09:51 parquet-hadoop-1.8.1.jar -rw-r--r--  1 jacek  staff   2796935 17 wrz 09:51 parquet-hadoop-bundle-1.6.0.jar and -rw-r--r--  1 jacek  staff     46983 17 wrz 09:51 jackson-annotations-2.6.5.jar -rw-r--r--  1 jacek  staff    258876 17 wrz 09:51 jackson-core-2.6.5.jar -rw-r--r--  1 jacek  staff    232248 17 wrz 09:51 jackson-core-asl-1.9.13.jar -rw-r--r--  1 jacek  staff   1171380 17 wrz 09:51 jackson-databind-2.6.5.jar -rw-r--r--  1 jacek  staff     18336 17 wrz 09:51 jackson-jaxrs-1.9.13.jar -rw-r--r--  1 jacek  staff    780664 17 wrz 09:51 jackson-mapper-asl-1.9.13.jar -rw-r--r--  1 jacek  staff     41263 17 wrz 09:51 jackson-module-paranamer-2.6.5.jar -rw-r--r--  1 jacek  staff    515604 17 wrz 09:51 jackson-module-scala_2.11-2.6.5.jar -rw-r--r--  1 jacek  staff     27084 17 wrz 09:51 jackson-xc-1.9.13.jar and -rw-r--r--  1 jacek  staff    188671 17 wrz 09:51 commons-beanutils-1.7.0.jar -rw-r--r--  1 jacek  staff    206035 17 wrz 09:51 commons-beanutils-core-1.8.0.jar and -rw-r--r--  1 jacek  staff    445288 17 wrz 09:51 antlr-2.7.7.jar -rw-r--r--  1 jacek  staff    164368 17 wrz 09:51 antlr-runtime-3.4.jar -rw-r--r--  1 jacek  staff    302248 17 wrz 09:51 antlr4-runtime-4.5.3.jar Even if that does not cause any class mismatches, it might be worth to exclude them to minimize the size of the Spark distro. What do you think? Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi Sean, Thanks a lot for help understanding the different jars. Do you think there's anything that should be reported as an enhancement/issue/task in JIRA? Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, I'm surprised too. Here's the entire stack trace for reference. I'd also like to know what causes the issue. Caused by: java.lang.NoClassDefFoundError: Could not initialize class Main$ at Main$$anonfun$main$1.apply(test.scala:14) at Main$$anonfun$main$1.apply(test.scala:14) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370) at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:231) at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319) at org.apache.spark.rdd.RDD.iterator(RDD.scala:283) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70) at org.apache.spark.scheduler.Task.run(Task.scala:86) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:277) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi Sean, "Boy it's a long story"...yeah, you tell me! :) I don't seem to find anything worth reporting so...let's keep these possible discrepancies in mind and be back to them when they hit us. Thanks a lot, Sean. Your patience with dealing with people here and on JIRA has always made me wish for having it. Kudos! Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, I've just discovered* that I can SerDe my case classes. What a nice feature which I can use in spark-shell, too! Thanks a lot for offering me so much fun! What I don't really like about the code is the following part (esp. that it conflicts with the implicit for Column): import org.apache.spark.sql.catalyst.dsl.expressions._ // in spark-shell there are competing implicits // That's why DslSymbol is used explicitly in the following line scala> val attrs = Seq(DslSymbol('id).long, DslSymbol('name).string) attrs: Seq[org.apache.spark.sql.catalyst.expressions.AttributeReference] = List(id#8L, name#9) scala> val jacekReborn = personExprEncoder.resolveAndBind(attrs).fromRow(row) jacekReborn: Person = Person(0,Jacek) Since I've got the "schema" as the case class Person I'd like to avoid creating attrs manually. Is there a way to avoid the step and use a "reflection"-like approach so the attrs are built out of the case class? Also, since we're at it, why's resolveAndBind required? Is this for names and their types only? Thanks for your help (and for such fantastic project Apache Spark!) [*] yeah, took me a while, but the happiness is stronger and I'll remember longer! :-) Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org +1 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, Just came across the Expression trait [1] that can be check for determinism by the method deterministic [2] and trait Nondeterministic [3]. Why both? [1] https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala#L53 [2] https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala#L80 [3] https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Expression.scala#L271 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, Not that it could fix the issue but no -Pmesos? Jacek Hi, I keep asking myself why are you guys not including -Pmesos in your builds? Is this on purpose or have you overlooked it? Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, That's even more interesting. How's so since the profile got added a week ago or later and RC2 was cut two/three days ago? Anyone know? Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi Sean, Sure, but then the question is why it's not a part of 2.0.1? I thought it was considered ready for prime time and so should be shipped in 2.0.1. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi Sean, So, another question would be when is the change going to be released then? What's the version for the master? The next release's 2.0.2 so it's not for mesos profile either :(Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi Sean, I remember a similar discussion about the releases in Spark and I must admit it again -- I simply don't get it. I seem to not have paid enough attention to details to appreciate it. I apologize for asking the very same questions again and again. Sorry. Re the next release, I was referring to JIRA where 2.0.2 came up quite recently for issues not included in 2.0.1. This disjoint between releases and JIRA versions causes even more frustration whenever I'm asked what and when the next release is going to be. It's not as simple as I think it should be (for me). (I really hope it's only me with this mental issue) Unless I'm mistaken, -Pmesos won't get included in 2.0.x releases unless someone adds it to branch-2.0. Correct? Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski +1 Ship it! Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, Perhaps nitpicking...you've been warned. While reviewing expressions in Catalyst I've noticed some inconsistency, i.e. Nondeterministic trait has two methods deterministic and foldable final override while LeafExpression does not have children final (at the very least). My thinking is that LeafExpression is to mark left expressions so children is assumed to be Nil. Should children be final in LeafExpression? Why not? #curious Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi Marcelo, The reason I asked about the mesos profile was that I thought it was part of the branch already and wondered why nobody used it to compile Spark with all the code available. I do understand no code changes were introduced during this profile maintenance, but with the profile that code does not get compiled unless you enable the profile explicitly. I've learnt it's not part of the release, though. Thanks for all the clarifications! I appreciate your patience dealing with my questions a lot! Thanks. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, I'm doubtful that dynamic allocation / killing executors work in Standalone and YARN as far as web UI's concerned (perhaps it's just web UI). I can successfully request as many executors as I want using sc.requestTotalExecutors and they show up nicely in the web UI as ACTIVE, but whenever I request sc.killExecutor(s) I can see the following in YARN logs: INFO YarnAllocator: Received 2 containers from YARN, launching executors on 2 of them. INFO YarnAllocator: Driver requested a total number of 3 executor(s). INFO YarnAllocator: Canceling requests for 1 executor container(s) to have a new desired total 3 executors. INFO YarnAllocator: Driver requested a total number of 2 executor(s). DEBUG YarnAllocator: Completed 1 containers DEBUG YarnAllocator: Finished processing 1 completed containers. Current running executor count: 2. INFO YarnAllocator: Driver requested a total number of 1 executor(s). INFO YarnAllocator: Will request 1 executor container(s), each with 1 core(s) and 1408 MB memory (including 384 MB of overhead) INFO YarnAllocator: Submitted 1 unlocalized container requests. DEBUG YarnAllocator: Completed 2 containers DEBUG YarnAllocator: Finished processing 2 completed containers. Current running executor count: 0. DEBUG YarnAllocator: Allocated containers: 1. Current executor count: 0. Cluster resources: <memory:5120, vCores:1>. INFO YarnAllocator: Launching container container_1475155700518_0003_01_000006 on host 192.168.65.1 INFO YarnAllocator: Received 1 containers from YARN, launching executors on 1 of them. INFO YarnAllocator: Driver requested a total number of 0 executor(s). DEBUG YarnAllocator: Completed 1 containers DEBUG YarnAllocator: Finished processing 1 completed containers. Current running executor count: 0. But the web UI shows all the executors as ACTIVE in Status column in Executors tab. Can anyone confirm that dynamic allocation works fine and web UI shows the current status of executors? What other information do you want me to offer to verify it. I'm doubtful that web UI shows what it's supposed to show regarding executors. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, Is there a reason why DAGScheduler.handleJobCancellation checks the active job id in jobIdToStageIds [1] while looking the job up in jobIdToActiveJob [2]? Perhaps synchronized earlier yet still inconsistent. [1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1372 [2] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1376 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Thanks Imran! Not only did the response come so promptly, but also it's something I could work on (and have another Spark contributor badge unlocked)! Thanks. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, While reviewing SparkUI I found two artifacts -- appUIAddress + appName (with the entire SparkUITab) -- that I believe are not needed at all as they seem to introduce nothing. Please have a look at https://github.com/apache/spark/pull/15603 and let me know your thoughts. I'd appreciate your comments to learn Spark better. Thanks. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, Just noticed the messages from the recent build of my pull request in Jenkins: [info] Warning: Unknown ScalaCheck args provided: -oDF I think we should fix it, right? Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, Any reason for withExpr duplication in Column [1] and functions [2] objects? It looks like it could be less private and be at least private[sql]? private def withExpr(newExpr: Expression): Column = new Column(newExpr) [1] https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Column.scala#L152 [2] https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L60 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, While reviewing ShuffleExchange physical operator [1] I found the following: override def nodeName: String = {val extraInfo = coordinator match {case Some(exchangeCoordinator) if exchangeCoordinator.isEstimated => s"(coordinator id: ${System.identityHashCode(coordinator)})"case Some(exchangeCoordinator) if !exchangeCoordinator.isEstimated => s"(coordinator id: ${System.identityHashCode(coordinator)})"case None => ""} It *appears* that the first two cases give the same result so...a minor duplication perhaps? (Makes reading the code slightly more involved). [1] https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/exchange/ShuffleExchange.scala#L46-L53 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, It's just a (minor?) example of how to use catalyst.dsl package [1], but am currently reviewing deserialize [2] and got a question. CatalystSerde.deserialize [3] is exactly the deserialize operator (referred above) and since CatalystSerde.deserialize's used in few places like Dataset.rdd [4] as follows: val deserialized = CatalystSerde.deserialize[T](logicalPlan) I'm wondering why the deserialize dsl operator is not used instead that would make the line as follows: val deserialized = deserialize(logicalPlan) which looks so much nicer to my eyes. Any reason for using CatalystSerde.deserialize[T](logicalPlan) instead of this seemingly simpler deserialize operator? Is this because it's just a matter of find-and-replace and no one had time for this? Please help me understand this area better. Thanks! [1] https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala [2] https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala#L304 [3] https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/object.scala#L32 [4] https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala#L2498 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi Hyukjin, What's worked for me so the Spark committers have accepted was to use the first group SQL, MLlib, Core, Python, Scheduler, Build, Docs, Streaming, Mesos, Web UI, YARN, GraphX, R with all the letters uppercase. It's less to remember so I'd vote for keeping it in use (or be acceptable). Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, After SPARK-12588 Remove HTTPBroadcast [1], the one and only implementation of BroadcastFactory is TorrentBroadcastFactory. No code in Spark 2 uses BroadcastFactory (but TorrentBroadcastFactory) however the scaladoc says [2]: /** * An interface for all the broadcast implementations in Spark (to allow * multiple broadcast implementations). SparkContext uses a user-specified * BroadcastFactory implementation to instantiate a particular broadcast for the * entire Spark job. */ which is not correct since there is no way to plug in a custom user-specified BroadcastFactory. My first impression was to remove the seemingly-pluggable interface BroadcastFactory completely since it's no longer pluggable and may imply it is still pluggable. But then I thought you, Spark devs, could argue it's just about fixing the scaladoc (and leaving the interface intact). I'm for removing the BroadcastFactory interface completely and leaving TorrentBroadcastFactory alone (without extending something that's not extendable despite being an interface) or...bringing spark.broadcast.factory Spark property back to life in BroadcastManager so it is indeed possible to plug a custom BroadcastFactory (and hence Broadcast) in. WDYT? [1] https://issues.apache.org/jira/browse/SPARK-12588 [2] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/broadcast/BroadcastFactory.scala#L25-L30 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, (what a timing. Just reviewed CC yesterday!) In ALS they trigger cleaning up shufflemapstages themselves so if I understood the issue the streaming part could do it too. Jacek Hi, I've been reviewing how MapOutputTracker works and can't understand the comment [1]: // Synchronize on the returned array because, on the driver, it gets mutated in place How is this possible since "the returned array" is a local value? I'm stuck and would appreciate help. Thanks! (It also says "Called from executors" [2] so how could the driver be involved?!) [1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/MapOutputTracker.scala#L145 [2] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/MapOutputTracker.scala#L133 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, While reviewing ShuffleManager I've noticed that registerShuffle method [1] takes shuffleId and ShuffleDependency which seems a code duplication since ShuffleDependency has shuffleId. Any reason for having shuffleId specified explicitly? [1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala#L35 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, I'm wondering what's so special about 200 to have it the default value of spark.shuffle.sort.bypassMergeThreshold? Is this arbitrary number? Is there any theory behind it? Is the number of partitions in Spark SQL, i.e. 200, somehow related to spark.shuffle.sort.bypassMergeThreshold? scala> spark.range(5).groupByKey(_ % 5).count.rdd.getNumPartitions res3: Int = 200 I'd appreciate any guidance to get the gist of this seemingly magic number. Thanks! Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, Just found out that ShuffleMapTask has transient locs and preferredLocs attributes which means that when ShuffleMapTask is serialized (as a broadcast variable) the information is gone. Does this mean that the attributes could have not been defined at all since Spark uses SortShuffleManager (and BlockManagerMaster on the driver) to track the shuffle locations (MapStatuses)? Is my understanding correct? What am I missing? (I'm exploring shuffle system currently and would appreciate comments a lot!) Thanks! Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi Imran, Yes, you're right. I stand corrected! Thanks. This is the part that opened my eyes: That's why a task does not have to have it after deserialization (!) Thanks a lot. On to digging deeper... Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, Just noticed that both MapOutputTrackerMaster [1] and MapOutputTrackerWorker [2] use Java's ConcurrentHashMap for mapStatuses [3] which makes this abstract mapStatuses attribute less abstract. I think it was the outcome of some refactoring that led to a small duplication (and makes the distinction between MapOutputTrackerMaster and MapOutputTrackerWorker...spurious?). Why do you think about removing the duplication? (There's another change with post(message: GetMapOutputMessage) to use consistently instead of mapOutputRequests.offer(message) that would make the change initially small slightly bigger and perhaps more acceptable). [1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/MapOutputTracker.scala#L292 [2] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/MapOutputTracker.scala#L596 [3] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/MapOutputTracker.scala#L84 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, Just got this this morning using the fresh build of Spark 2.2.0-SNAPSHOT (with a few local modifications): scala> Seq(0 to 8).toDF scala.MatchError: scala.collection.immutable.Range.Inclusive (of class scala.reflect.internal.Types$ClassNoArgsTypeRef) at org.apache.spark.sql.catalyst.ScalaReflection$.org$apache$spark$sql$catalyst$ScalaReflection$$serializerFor(ScalaReflection.scala:520) at org.apache.spark.sql.catalyst.ScalaReflection$.serializerFor(ScalaReflection.scala:463) at org.apache.spark.sql.catalyst.encoders.ExpressionEncoder$.apply(ExpressionEncoder.scala:71) at org.apache.spark.sql.SQLImplicits.newIntSequenceEncoder(SQLImplicits.scala:168) ... 48 elided Is this something I've introduced, a known issue or a bug? ./bin/spark-shell --version Welcome to ____              __ / __/__  ___ _____/ /__ _\ \/ _ \/ _ `/ __/  '_/ /___/ .__/\_,_/_/ /_/\_\   version 2.2.0-SNAPSHOT /_/ Using Scala version 2.11.8, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_112 Branch master Compiled by user jacek on 2017-01-09T05:01:47Z Revision 19d9d4c855eab8f647a5ec66b079172de81221d0 Url https://github.com/apache/spark.git Type --help for more information. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, Just noticed that TaskContext#getPartitionId [1] is not used and moreover the scaladoc is incorrect: "It will return 0 if there is no active TaskContext for cases like local execution."since there are no local execution. (I've seen the comment in the code before but can't find it now). The reason to remove it is that Structured Streaming is giving new birth to the method in ForeachSink [2] which may look like a "resurrection". There's simply TaskContext.get.partitionId. What do you think? [1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/TaskContext.scala#L41 [2] https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/ForeachSink.scala#L50 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, Yes, correct. I was too forceful in discouraging people using it. I think @deprecated would be a right direction. What should be the next step? I think I should file an JIRA so it's in a release notes. Correct? I was very surprised to have noticed its resurrection in the very latest module of Spark - Structured Streaming - that will be an inspiration for others to learn Spark. Jacek Hi Sean, Can you elaborate on " it's actually used by Spark"? Where exactly? I'd like to be corrected. What about the scaladoc? Since the method's a public API, I think it should be fixed, shouldn't it? Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, Given [1]: I believe the comment in [2]: Correct? Deserves a pull request to get rid of the seemingly incorrect scaladoc? p.s. How to know when the deprecation was introduced? The last change is for executor blacklisting so git blame does not show what I want :(Any ideas? [1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkConf.scala#L641 [2] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala#L32 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, I'm trying to get the gist of clientMode input parameter for RpcEnv.create [1]. It is disabled (i.e. false) by default. I've managed to find out that, in the "general" case, it's enabled for executors and disabled for the driver. (it's also used for Spark Standalone's master and workers but it's infra and I'm not interested in exploring it currently). I've however noticed that in YARN the clientMode parameter means something more, i.e. whether the Spark application runs in client or cluster deploy mode. In YARN my understanding of the parameter is that clientMode is enabled explicitly when Spark on YARN's ApplicationMaster creates the `sparkYarnAM` RPC Environment [2] (when executed for client deploy mode [3]) This is because in client deploy mode the driver runs on some other node and the AM acts simply as a proxy to Spark executors that run in their own YARN containers. This is (also?) because in client deploy mode in Spark on YARN we have separate JVM processes for the driver, the AM and Spark executors. The distinction is Is the last two paragraphs correct? I'd appreciate if you could fix and fill out the gaps where necessary. Thanks a lot to make it so much easier for me and...participants of my Spark workshops ;-) [1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rpc/RpcEnv.scala#L42 [2] https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala#L434 [3] https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala#L254 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi, Just noticed that [1] (and also [3]) is very cautious with $ and $$ to expand environment variables. javaOpts += "-Djava.io.tmpdir=" + new Path(YarnSparkHadoopUtil.expandEnvironment(Environment.PWD), // <-- here YarnConfiguration.DEFAULT_CONTAINER_TEMP_DIR ) Few lines below in the same method [2] the code doesn't seem to bother to call $ directly : Client.buildPath(Environment.PWD.$(), uri.getPath()) Why? Any particular reason for the difference in the two lines? #curious [1] https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala#L156 [2] https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala#L203 [3] https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala#L210 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Wow! At long last. Congrats Burak and Holden! p.s. I was a bit worried that the process of accepting new committers is equally hard as passing Sean's sanity checks for PRs, but given this it's so much easier it seems :D Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, Why are there two (almost) identical makeOffers in CoarseGrainedSchedulerBackend [1] and [2]? I can't seem to figure out why they are there and am leaning towards considering one a duplicate. WDYT? [1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala#L211 [2] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala#L229 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi Imran, Thanks a lot for your detailed explanation, but IMHO the difference is so small that I'm surprised it merits two versions -- both check whether an executor is alive -- executorIsAlive(executorId) vs executorDataMap.filterKeys(executorIsAlive) A bit fishy, isn't it? But, on the other hand, since no one has considered it a small duplication it could be perfectly fine (it did make the code a bit less obvious to me). Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi Imran, Ok, that makes sense for performance reasons. Thanks for bearing with me and explaining that code with so much patience. Appreciated! Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi Nicholas, Interesting. Just on the past Monday I was introducing spark and ran into it but thought it's my poor English skills :-) Thanks for spotting it! (I also think that the entire welcome page begs for a face lifting - it's from pre-2.0 days) Jacek Hi Sean, Given that 3.0.0 is coming, removing the unused versions would be a huge benefit from maintenance point of view. I'd support removing support for 2.5 and earlier. Speaking of Hadoop support, is anyone considering 3.0.0 support? Can't find any JIRA for this. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, Just to throw few zlotys to the conversation, I believe that Spark Standalone does not enforce any memory checks to limit or even kill executors beyond requested memory (like YARN). I also found that memory does not have much of use while scheduling tasks and CPU matters only. My understanding of `spark.memory.offHeap.enabled` is `false` is that it does not disable off heap memory used in Java NIO for buffers in shuffling, RPC, etc. so the memory is always (?) more than you request for mx using executor-memory. Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski Hi, I'm wondering why YARN computes the initial number of executors (in YarnSparkHadoopUtil.getInitialTargetExecutorNumber [1]) if Core's Utils.getDynamicAllocationInitialExecutors [2] could do? I'm to send a PR to remove the duplication as it's tricky enough to keep right in one place given all the Spark properties and their relationship: * spark.dynamicAllocation.minExecutors * spark.dynamicAllocation.initialExecutors * spark.executor.instances WDYT? [1] https://github.com/apache/spark/blob/master/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnSparkHadoopUtil.scala#L270 [2] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/Utils.scala#L2516 Pozdrawiam, Jacek Laskowski ---- https://medium.com/@jaceklaskowski/ Mastering Apache Spark 2.0 https://bit.ly/mastering-apache-spark Follow me at https://twitter.com/jaceklaskowski --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org