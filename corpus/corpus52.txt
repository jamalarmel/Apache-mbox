On Thu, Apr 23, 2015 at 5:26 PM, Sean Owen  wrote: +1, this will help with giving people the "bit of credit", but I guess it also helps on recognizing the community contributors towards becoming committers much easier. -- Luciano Resende http://people.apache.org/~lresende http://twitter.com/lresende1975 http://lresende.blogspot.com/ +1 (non-binding) mostly looking in the legal aspects of the release. On Wed, Jul 8, 2015 at 10:55 PM, Patrick Wendell  wrote: -- Luciano Resende http://people.apache.org/~lresende http://twitter.com/lresende1975 http://lresende.blogspot.com/ On Fri, Aug 21, 2015 at 9:28 AM, Sean Owen  wrote: Hi Sean, Were you able to resolve this ? I am trying it on Linux (Ubuntu 14.04.3 LTS) and when building with a clean maven repo I am getting issues where it can't find lib_managed/jar when trying to build the Launcher. But looks like you went a bit further on this ? Running org.apache.spark.launcher.SparkSubmitCommandBuilderSuite Tests run: 7, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 0.018 sec <<< FAILURE! - in org.apache.spark.launcher.SparkSubmitCommandBuilderSuite testDriverCmdBuilder(org.apache.spark.launcher.SparkSubmitCommandBuilderSuite) Time elapsed: 0.005 sec  <<< ERROR! java.lang.IllegalStateException: Library directory '/home/lresende/dev/spark/source/releases/spark-1.5.0/lib_managed/jars'does not exist. at org.apache.spark.launcher.CommandBuilderUtils.checkState(CommandBuilderUtils.java:249) at org.apache.spark.launcher.AbstractCommandBuilder.buildClassPath(AbstractCommandBuilder.java:218) at org.apache.spark.launcher.AbstractCommandBuilder.buildJavaCommand(AbstractCommandBuilder.java:115) at org.apache.spark.launcher.SparkSubmitCommandBuilder.buildSparkSubmitCommand(SparkSubmitCommandBuilder.java:196) at org.apache.spark.launcher.SparkSubmitCommandBuilder.buildCommand(SparkSubmitCommandBuilder.java:121) at org.apache.spark.launcher.SparkSubmitCommandBuilderSuite.testCmdBuilder(SparkSubmitCommandBuilderSuite.java:174) at org.apache.spark.launcher.SparkSubmitCommandBuilderSuite.testDriverCmdBuilder(SparkSubmitCommandBuilderSuite.java:51) -- Luciano Resende http://people.apache.org/~lresende http://twitter.com/lresende1975 http://lresende.blogspot.com/ The binary archives seems to be having some issues, which seems consistent on few of the different ones (different versions of hadoop) that I tried. tar -xvf spark-1.5.0-bin-hadoop2.6.tgz x spark-1.5.0-bin-hadoop2.6/lib/spark-examples-1.5.0-hadoop2.6.0.jar x spark-1.5.0-bin-hadoop2.6/lib/spark-assembly-1.5.0-hadoop2.6.0.jar x spark-1.5.0-bin-hadoop2.6/lib/spark-1.5.0-yarn-shuffle.jar x spark-1.5.0-bin-hadoop2.6/README.md tar: copyfile unpack (spark-1.5.0-bin-hadoop2.6/python/test_support/sql/orc_partitioned/SUCCESS.crc) failed: No such file or directory tar tzf spark-1.5.0-bin-hadoop2.3.tgz | grep SUCCESS.crc spark-1.5.0-bin-hadoop2.3/python/test_support/sql/orc_partitioned/._SUCCESS.crc This seems similar to a problem Avro release was having recently. On Tue, Aug 25, 2015 at 9:28 PM, Reynold Xin  wrote: -- Luciano Resende http://people.apache.org/~lresende http://twitter.com/lresende1975 http://lresende.blogspot.com/ I was looking for the code mentioned in SPARK-9818 and SPARK-6136 that supposedly is testing MySQL and PostgreSQL using Docker and it seems that this code has been removed. Could anyone provide me a pointer on where are these tests actually located at the moment, and how they are integrated with the Spark build ? My goal is to integrate some DB2 JDBC Dialect tests as mentioned in SPARK-10521 [1] https://issues.apache.org/jira/browse/SPARK-9818 [2] https://issues.apache.org/jira/browse/SPARK-6136 [3] https://issues.apache.org/jira/browse/SPARK-10521 -- Luciano Resende http://people.apache.org/~lresende http://twitter.com/lresende1975 http://lresende.blogspot.com/ Thanks Reynold, Also, what is the status of the associated PR are we planning to merge it soon ? This will help me with the Db2 dialect test framework using Docker. Thanks [1] https://github.com/apache/spark/pull/8101 On Mon, Sep 14, 2015 at 1:47 PM, Reynold Xin  wrote: -- Luciano Resende http://people.apache.org/~lresende http://twitter.com/lresende1975 http://lresende.blogspot.com/ You can use Jira filters to narrow down the scope of issues you want to possible address, for instance, I use this filter to look into open issues, that are unassigned : https://issues.apache.org/jira/issues/?filter=12333428 For a specific release, you can also filter the release, and I Reynold had sent this a few days ago for 1.5.1 https://issues.apache.org/jira/issues/?filter=12333321 On Tue, Sep 22, 2015 at 8:50 AM, Pedro Rodriguez  Where is the best place to look at open issues that haven't been -- Luciano Resende http://people.apache.org/~lresende http://twitter.com/lresende1975 http://lresende.blogspot.com/ For host information, are you looking for something like this (which is available today in Spark 1.5 already) ? # Spark related configuration Sys.setenv("SPARK_MASTER_IP"="127.0.0.1") Sys.setenv("SPARK_LOCAL_IP"="127.0.0.1") #Load libraries library("rJava") library(SparkR, lib.loc="/...../spark-bin/R/lib") #Initalize  spark context sc <- sparkR.init(sparkHome = "/...../spark-bin", sparkPackages="com.databricks:spark-csv_2.11:1.2.0") On Thu, Sep 24, 2015 at 2:09 PM, Hossein  wrote: -- Luciano Resende http://people.apache.org/~lresende http://twitter.com/lresende1975 http://lresende.blogspot.com/ +1 (non-binding) Compiled in Mac OS with : build/mvn -Pyarn,sparkr,hive,hive-thriftserver -Phadoop-2.6 -Dhadoop.version=2.6.0 -DskipTests clean package Checked around R Looked into legal files All looks good. On Thu, Sep 24, 2015 at 12:27 AM, Reynold Xin  wrote: -- Luciano Resende http://people.apache.org/~lresende http://twitter.com/lresende1975 http://lresende.blogspot.com/ I have started looking into PR-8101 [1] and what is required to merge it into trunk which will also unblock me around SPARK-10521 [2]. So here is the minimal plan I was thinking about : - make the docker image version fixed so we make sure we are using the same image all the time - pull the required images on the Jenkins executors so tests are not delayed/timedout because it is waiting for docker images to download - create a profile to run the JDBC tests - create daily jobs for running the JDBC tests In parallel, I learned that Alan Chin from my team is working with the AmpLab team to expand the build capacity for Spark, so I will use some of the nodes he is preparing to test/run these builds for now. Please let me know if there is anything else needed around this. [1] https://github.com/apache/spark/pull/8101 [2] https://issues.apache.org/jira/browse/SPARK-10521 -- Luciano Resende http://people.apache.org/~lresende http://twitter.com/lresende1975 http://lresende.blogspot.com/ Hey Josh, Thanks for helping bringing this up, I have just pushed a WIP PR for bringing the DB2 tests to be running on Docker, and I have a question about how the jdbc drivers are actually being setup for the other datasources (MySQL and PostgreSQL), are these setup directly on the Jenkins slaves ? I didn't see the jars or anything specific on the pom or other files... Thanks On Wed, Oct 21, 2015 at 1:26 PM, Josh Rosen  wrote: -- Luciano Resende http://people.apache.org/~lresende http://twitter.com/lresende1975 http://lresende.blogspot.com/ On Wed, Dec 2, 2015 at 3:02 PM, Reynold Xin  wrote: And I believe anyone with a @apache.org e-mail address can request their own personal license for InteliJ. That's what  I personally did. -- Luciano Resende http://people.apache.org/~lresende http://twitter.com/lresende1975 http://lresende.blogspot.com/ On Mon, Nov 30, 2015 at 1:53 PM, Josh Rosen  The JDBC drivers are currently being pulled in as test-scope dependencies So, the issue I am having now is that the DB2 JDBC is not available in any maven public repository, so the plan I am going in with is : - Before running the DB2 Docker Tests, the client machine needs to download the jdbc driver locally and install it to it's local maven repository (or sbt equivalent)  (instructions to be provided in either readme or pom file) - We would need help with installing the DB2 JDBC on the Jenkins slaves machines - We could also create a new profile for the DB2 Docker Tests, so that this tests are running when this profile is enabled. I could probably think about other options, but they would sound a lot hacky..... Thoughts ? Some suggestions ? -- Luciano Resende http://people.apache.org/~lresende http://twitter.com/lresende1975 http://lresende.blogspot.com/ +1 (non-binding) Tested Standalone mode, SparkR and couple Stream Apps, all seem ok. On Wed, Dec 16, 2015 at 1:32 PM, Michael Armbrust  Please vote on releasing the following candidate as Apache Spark version -- Luciano Resende http://people.apache.org/~lresende http://twitter.com/lresende1975 http://lresende.blogspot.com/ I was looking into the Apache export control page, and didn't see Spark listed there, which from my initial investigation seemed ok because i couldn't find any handling of cryptography in Spark code. Could someone more familiar with the Spark dependency hierarchy confirm that there is no specific crypto code/dependency on Spark. Thank you [1] http://www.apache.org/licenses/exports/ -- Luciano Resende http://people.apache.org/~lresende http://twitter.com/lresende1975 http://lresende.blogspot.com/ There were few more issues, I have started tracking them at https://issues.apache.org/jira/browse/SPARK-13189 On Thu, Feb 4, 2016 at 2:08 AM, Prashant Sharma  Yes, That should be changed to 2.11.7. Mind sending a patch ? -- Luciano Resende http://people.apache.org/~lresende http://twitter.com/lresende1975 http://lresende.blogspot.com/ On Mon, Feb 8, 2016 at 9:15 AM, Matei Zaharia  Hi all, Congratulations !!! -- Luciano Resende http://people.apache.org/~lresende http://twitter.com/lresende1975 http://lresende.blogspot.com/ On Mon, Feb 22, 2016 at 9:08 PM, Michael Armbrust  An update: people.apache.org has been shut down so the release scripts If you skip uploading to people.a.o, it should still be available in nexus for review. The other option is to add the RC into https://dist.apache.org/repos/dist/dev/ -- Luciano Resende http://people.apache.org/~lresende http://twitter.com/lresende1975 http://lresende.blogspot.com/ If the intention is to actually decouple and give a life of it's own to these connectors, I would have expected that they would still be hosted as different git repositories inside Apache even tough users will not really see much difference as they would still be mirrored in GitHub. This makes it much easier on the legal departments of the upstream consumers and customers as well because the code still follow the so well received and trusted Apache Governance and Apache Release Policies. As for implementation details, we can have multiple repositories if we see a lot of fragmented releases, or a single "connectors" repository which in our side would make administration more easily. On Thu, Mar 17, 2016 at 2:33 PM, Marcelo Vanzin  wrote: Agree that there might be a little overhead, but there are ways to minimize this, and I am sure there are volunteers willing to help in favor of having a more unifying project. Breaking things into multiple projects, and having to manage the matrix of supported versions will be hell worst overhead. Subprojects or even if we send this back to incubator as "connectors project" is better then public github per package in my opinion. Now, if with this move is signalizing to customers that the Streaming API as in 1.x is going away in favor the new structure streaming APIs , then I guess this is a complete different discussion. -- Luciano Resende http://people.apache.org/~lresende http://twitter.com/lresende1975 http://lresende.blogspot.com/ On Fri, Mar 18, 2016 at 7:58 AM, Cody Koeninger  wrote: I am currently not a committer, but If we are willing to go into the direction of having another project as spark-extras, I can help drive the bureaucratic work to make this a reality. -- Luciano Resende http://people.apache.org/~lresende http://twitter.com/lresende1975 http://lresende.blogspot.com/ On Fri, Mar 18, 2016 at 10:07 AM, Marcelo Vanzin  Hi Steve, thanks for the write up. It can have multiple repos, but this still brings the overhead into the PMC to maintain it which was brought on previously on this thread and it might not be the direction the PMC want to take (but I might be mistaken). Another approach is to make this extras, just a subproject, with it's own set of committers etc.... which gives less burden on the Spark PMC. Anyway, my main issue here is not who and how it's going to be managed, but that it continues under Apache governance. -- Luciano Resende http://people.apache.org/~lresende http://twitter.com/lresende1975 http://lresende.blogspot.com/ I believe some of this has been resolved in the context of some parts that had interest in one extra connector, but we still have a few removed, and as you mentioned, we still don't have a simple way or willingness to manage and be current on new packages like kafka. And based on the fact that this thread is still alive, I believe that other community members might have other concerns as well. After some thought, I believe having a separate project (what was mentioned here as Spark Extras) to handle Spark Connectors and Spark add-ons in general could be very beneficial to Spark and the overall Spark community, which would have a central place in Apache to collaborate around related Spark components. Some of the benefits on this approach - Enables maintaining the connectors inside Apache, following the Apache governance and release rules, while allowing Spark proper to focus on the core runtime. - Provides more flexibility in controlling the direction (currency) of the existing connectors (e.g. willing to find a solution and maintain multiple versions of same connectors like kafka 0.8x and 0.9x) - Becomes a home for other types of Spark related connectors helping expanding the community around Spark (e.g. Zeppelin see most of it's current contribution around new/enhanced connectors) What are some requirements for Spark Extras to be successful: - Be up to date with Spark Trunk APIs (based on daily CIs against SNAPSHOT) - Adhere to Spark release cycles (have a very little window compared to Spark release) - Be more open and flexible to the set of connectors it will accept and maintain (e.g. also handle multiple versions like the kafka 0.9 issue we have today) Where to start Spark Extras Depending on the interest here, we could follow the steps of (Apache Arrow) and start this directly as a TLP, or start as an incubator project. I would consider the first option first. Who would participate Have thought about this for a bit, and if we go to the direction of TLP, I would say Spark Committers and Apache Members can request to participate as PMC members, while other committers can request to become committers. Non committers would be added based on meritocracy after the start of the project. Project Name It would be ideal if we could have a project name that shows close ties to Spark (e.g. Spark Extras or Spark Connectors) but we will need permission and support from whoever is going to evaluate the project proposal (e.g. Apache Board) Thoughts ? Does anyone have any big disagreement or objection to moving into this direction ? Otherwise, who would be interested in joining the project, so I can start working on some concrete proposal ? On Sat, Mar 26, 2016 at 6:58 AM, Sean Owen  wrote: -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ +1, I also checked with few projects inside IBM that consume Spark and they seem to be ok with the direction of droping JDK 7. On Mon, Mar 28, 2016 at 11:24 AM, Michael Gummelt  +1 from Mesosphere -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ Reynold, Considering the performance improvements you mentioned in your original e-mail and also considering that few other big data projects have already or are in progress of abandoning JDK 7, I think it would benefit Spark if we go with JDK 8.0 only. Are there users that will be less aggressive ? Yes, but those would most likely be in more stable releases like 1.6.x. On Sun, Apr 3, 2016 at 10:28 PM, Reynold Xin  wrote: -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ After some collaboration with other community members, we have created a initial draft for Spark Extras which is available for review at https://docs.google.com/document/d/1zRFGG4414LhbKlGbYncZ13nyX34Rw4sfWhZRA5YBtIE/edit?usp=sharing We would like to invite other community members to participate in the project, particularly the Spark Committers and PMC (feel free to express interest and I will update the proposal). Another option here is just to give ALL Spark committers write access to "Spark Extras". We also have couple asks from the Spark PMC : - Permission to use "Spark Extras" as the project name. We already checked this with Apache Brand Management, and the recommendation was to discuss and reach consensus with the Spark PMC. - We would also want to check with the Spark PMC that, in case of successfully creation of  "Spark Extras", if the PMC would be willing to continue the development of the remaining connectors that stayed in Spark 2.0 codebase in the "Spark Extras" project. Thanks in advance, and we welcome any feedback around this proposal before we present to the Apache Board for consideration. On Sat, Mar 26, 2016 at 10:07 AM, Luciano Resende  I believe some of this has been resolved in the context of some parts that -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ On Fri, Apr 15, 2016 at 9:18 AM, Sean Owen  wrote: This whole discussion started when some of the connectors were moved from Apache to Github, which makes a statement that The "Spark Governance" of the bits is something very valuable by the community, consumers, and other companies that are consuming open source code. Being an Apache project also allows the project to use and share the Apache infrastructure to run the project. I know the name might be confusing, but I also think that the projects have a very big synergy, more like sibling projects, where "Spark Extras"extends the Spark community and develop/maintain components for, and pretty much only for, Apache Spark.  Based on your comment above, if making the project "Spark-Extras" a more acceptable name, I believe this is ok as well. I also understand that the Spark PMC might have concerns with branding, and that's why we are inviting all members of the Spark PMC to join the project and help oversee and manage the project. -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ On Fri, Apr 15, 2016 at 9:34 AM, Cody Koeninger  wrote: Agree Cody, and I think this is one of the goals of "Spark Extras", centralize the development of these connectors under one central place at Apache, and that's why one of our asks is to invite the Spark PMC to continue developing the remaining connectors that stayed in Spark proper, in "Spark Extras". We will also discuss some process policies on enabling lowering the bar to allow proposal of these other github extensions to be part of "Spark Extras" while also considering a way to move code to a maintenance mode location. -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ On Thu, May 19, 2016 at 3:16 PM, Shankar Venkataraman  wrote: I am not sure if this is exactly the same issue, but while we were doing heavy processing of large history of tweet data via streaming, we were having similar issues due to the load on the executors, and we bumped some configurations to avoid loosing some of these executors (even though there were alive, but busy to heart beat or something) Some of these are described at https://github.com/SparkTC/redrock/blob/master/twitter-decahose/src/main/scala/com/decahose/ApplicationContext.scala -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ On Sunday, May 22, 2016, Matei Zaharia  wrote: Thanks Matei, please note that a formal vote should generally be permitted to run for at least 72 hours to provide an opportunity for all concerned persons to participate regardless of their geographic locations. http://www.apache.org/foundation/voting.html Thank you -- Luciano -- Sent from my Mobile device On Wed, May 25, 2016 at 6:53 AM, Marcin Tustin  Would it be useful to start baking docker images? Would anyone find that a +1, I had done one (still based on 1.6) for some SystemML experiments, I could easily get it based on a nightly build. https://github.com/lresende/docker-spark One question tough, how often should the image be updated ? every night ? every week ? I could see if I can automate the build + publish in a CI job at one of our Jenkins servers (Apache or something)... -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ I recently used labels to mark couple jiras that me and my team have some interest on them, so it's easier to share a query and check the status on them. But I noticed that these labels were removed. Are there any issues with labeling jiras ? Any other suggestions ? -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ On Wed, May 25, 2016 at 2:33 PM, Sean Owen  wrote: We have used for other things in the past, like to identify the big-endian related issues https://issues.apache.org/jira/browse/SPARK-15154?jql=labels%20%3D%20big-endian The issue with maintaining anything locally is that then it's not easily sharable (e.g. I can't just send a link to a query) The question is more like, what issues can be caused by using labels ? -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ On Wed, May 25, 2016 at 2:34 PM, Sean Owen  wrote: +1 -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ On Wed, May 25, 2016 at 3:45 PM, Reynold Xin  wrote: Well, if we consider the worst case scenario, and we have a jira, let's say with a few labels, what harm does it make ? -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ Congratulations Yanbo !!! On Fri, Jun 3, 2016 at 7:48 PM, Matei Zaharia  Hi all, -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ On Mon, Jun 6, 2016 at 9:51 AM, Sean Owen  wrote: In this case, I would only expect the 2.0.0 preview to be treated as just any other release, period. -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ On Mon, Jun 6, 2016 at 10:08 AM, Mark Hamstra  I still don't know where this "severely compromised builds of limited A few months from now, why would a developer choose a preview, alpha, beta compared to the GA 2.0 release ? As for the being stale part, this is true for every release anyone put out there. -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ On Mon, Jun 6, 2016 at 11:12 AM, Matei Zaharia  Is there any way to remove artifacts from Maven Central? Maybe that would So, consider this thread started on another project : https://www.mail-archive.com/dev@bahir.apache.org/msg00038.html What would be your recommendation ? - Start a release based on Apache Spark 2.0.0 preview staging repo ? I would  reject that... - Start a release on a set of artifacts that are going to be deleted ? I would also reject that To me, if companies are using the release on their products, and other projects are relying on the release to provide a way for users to test, this should be considered as any other release, published permanently, which at some point will become obsolete and users will move on to more stable releases. Thanks -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ On Mon, Jun 6, 2016 at 12:05 PM, Reynold Xin  wrote: Thank You !!! On Wed, Jun 22, 2016 at 7:46 AM, Cody Koeninger  wrote: To be fair with the Kafka 0.10 PR assessment : I was expecting somewhat an easy transition from customer using 0.80 to 0.10 connector, but the 0.10 seems to have been treated as a completely new extension, also, there is no python support, no samples on the pr demonstrating how to use security capabilities and no documentation updates. Thanks -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ The Apache Bahir project is voting a release based on Spark 2.0.0-preview. https://www.mail-archive.com/dev@bahir.apache.org/msg00085.html It currently provides the following Apache Spark Streaming connectors: streaming-akka streaming-mqtt streaming-twitter streaming-zeromq While we are continuing to work towards a release to support Spark 2.0.0, we appreciate your help around testing the release and the current Spark Streaming connectors. To add the connectors to your scala application, the best way is to build the source of Bahir with 'mvn clean install' which will make the necessary dependencies available in your local maven repository and will enable you to reference the connectors in your application and also submit  your application to a local Spark test environment utilizing --packages. Build: mvn clean install Add repository to your scala application (build.sbt): resolvers += "Local Maven Repository" at "file://" + Path.userHome.absolutePath + "/.m2/repository"Submit your application to a local Spark test environment: bin/spark-submit --master spark://127.0.0.1:7077 --packages org.apache.bahir:spark-streaming-akka_2.11:2.0.0-preview --class org.apache.spark.examples.streaming.akka.ActorWordCount ~/opensource/apache/bahir/streaming-akka-examples/target/scala-2.11/streaming-akka-examples_2.11-1.0.jar localhost 9999 The Bahir community welcomes questions, comments, bug reports and all your feedback. http://bahir.apache.org/community/ Thanks + 1 (non-binding) Found a minor issue when trying to run some of the docker tests, but nothing blocking the release. Will create a JIRA for that. On Tue, Jul 19, 2016 at 7:35 PM, Reynold Xin  wrote: -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ When are we planning to push the release maven artifacts ? We are waiting for this in order to push an official Apache Bahir release supporting Spark 2.0. On Sat, Jul 23, 2016 at 7:05 AM, Reynold Xin  wrote: -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ On Wed, Sep 7, 2016 at 11:57 AM, Matei Zaharia  I think you should ask legal about how to have some Maven artifacts for As long as they are not part of an "Apache licensed"  distribution. Note that Ganglia seems to have changed license to BSD and we might be able to better support that. I think the key here is "optional", while the Kinesis is optional for Spark (which makes it ok to have it in Spark) it is not optional for Kinesis extension, which thenm IMHO, does not allow us to publish the Kinesis artifact either. But let's wait on the response from Legal before we actually implement a solution. -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ On Wed, Sep 7, 2016 at 12:20 PM, Mridul Muralidharan  It is good to get clarification, but the way I read it, the issue is +1, by providing instructions on how the user would build, and attaching the license details on the instructions, we are then safe on the legal aspects of it. -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ It looks like there is nobody running these tests, and after some dependency upgrades in Spark 2.0 this has stopped working. I have tried to bring up this but I am having some issues with getting the right dependencies loaded and satisfying the docker-client expectations. The question then is: Does the community find value on having these tests available ? Then we can focus on bringing them up and I can go push my previous experiments as a WIP PR. Otherwise we should just get rid of these tests. Thoughts ? On Tue, Sep 6, 2016 at 4:05 PM, Suresh Thalamati  wrote: -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ That might be a reasonable and much more simpler approach to try... but if we resolve these issues, we should make it part of some frequent build to make sure the build don't regress and that the actual functionality don't regress either. Let me look into this again... On Wed, Sep 7, 2016 at 2:46 PM, Josh Rosen  wrote: -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ +1 (non-binding) also verified that the assembly files with license issues are not being published to maven staging repositories. On Thu, Sep 22, 2016 at 11:01 PM, Reynold Xin  wrote: -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ +1 (non-binding) On Sat, Sep 24, 2016 at 3:08 PM, Reynold Xin  wrote: -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ +1 (non-binding) On Wed, Sep 28, 2016 at 7:14 PM, Reynold Xin  wrote: -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ Congratulations Sean !!! On Monday, October 3, 2016, Reynold Xin  wrote: -- Sent from my Mobile device It usually don't take that long to be synced, I still don't see any 2.0.1 related artifacts on maven central http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.apache.spark%22%20AND%20v%3A%222.0.1%22 On Tue, Oct 4, 2016 at 1:23 PM, Reynold Xin  wrote: -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ I have created a Infra jira to track the issue with the maven artifacts for Spark 2.0.1 On Wed, Oct 5, 2016 at 10:18 PM, Shivaram Venkataraman  wrote: -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ +1 (non-binding) On Thu, Oct 27, 2016 at 9:18 AM, Reynold Xin  wrote: -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/ Congrats to both !!! On Tue, Jan 24, 2017 at 10:13 AM, Reynold Xin  wrote: -- Luciano Resende http://twitter.com/lresende1975 http://lresende.blogspot.com/