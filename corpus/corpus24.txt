Actually I sent a pull request for this yesterday https://github.com/mesos/spark/pull/753 Shivaram For the machine learning library that is a part of Spark 0.8 we have been using jblas for local matrix operations. From some limited benchmarking that we did, jblas is not much slower than optimized C++ libraries. http://blog.mikiobraun.de/2009/04/some-benchmark-numbers-for-jblas.html has some more details. For more complex operations than addition and multiplication, mahout-math is a pretty good library. There was a great discussion on pros/cons of different Java/Scala-based matrix libraries in https://github.com/mesos/spark/pull/736 Thanks Shivaram Did you check out spark from the master branch of github.com/mesos/spark ? The package names changed recently so you might need to pull. Also just checking that you did publish-local in Spark (not public-local as specified in the email) ? Thanks Shivaram For some more notes on how to debug this: After you do publish-local in Spark, you should have a file in ~/.ivy2 that you can check for using `ls ~/.ivy2/local/org.apache.spark/spark-core_2.9.3/0.8.0-SNAPSHOT/jars/spark-core_2.9.3.jar` Or `sbt/sbt publish-local` also prints something like this on the console [info]  published spark-core_2.9.3 to /home/shivaram/.ivy2/local/org.apache.spark/spark-core_2.9.3/0.8.0-SNAPSHOT/jars/spark-core_2.9.3.jar After that MLI's build should be able to pick this jar up. Thanks Shivaram They should be in /target/unit-tests.log -- or rather core/target/unit-tests.log for core. I think there is a bug somewhere where sbt puts it in that directory while maven puts it in core/core/target/unit-tests.log. Its configurable in core/src/test/resources/log4j.properties Thanks Shivaram One more thing that would be good to have is a set of command line tools to parse the JSON dump. I think something like a python wrapper script that given a (job-id, stage-id) can parse the JSON and print out the stage / task timings, with histograms etc. From my own perspective, It'll be super useful for generating data that can be used in benchmarks and experiment results etc. Thanks Shivaram I'm happy to announce the developer preview of SparkR, an R frontend for Spark. SparkR presents Spark's API in R and allows you to write code in R and run the computation on a Spark cluster. You can try out SparkR today by installing it from our github repo at https://github.com/amplab-extras/SparkR-pkg . Right now SparkR is available as a standalone package that can be installed to run on an existing Spark installation. Note that SparkR requires Spark >= 0.9 and the default build uses the recent 0.9 release candidate. In the future we will consider merging this with Apache Spark. More details about SparkR and examples of SparkR code can be found at http://amplab-extras.github.io/SparkR-pkg. I would like to thank Zongheng Yang, Matei Zaharia and Matt Massie for their contributions and help in developing SparkR. Comments and pull requests are welcome on github. Thanks Shivaram +1 Hey Patrick FYI the link is still broken for the Apache mirrors in the website. For example I get pointed to http://www.eng.lsu.edu/mirrors/apache/incubator/spark/spark-0.8.1-incubating/spark-0.9.0-incubating.tgz instead of http://www.eng.lsu.edu/mirrors/apache/incubator/spark/spark-0.9.0-incubating/spark-0.9.0-incubating.tgz Thanks Shivaram Works fine now. Shivaram All the emails I get from github seem to have the same subject line "[GitHub] incubator-spark pull request:" and get grouped under the same thread in gmail -- Is there a way to put the pull request title in the email subject ? Thanks Shivaram For the 1st case wouldn't it be better to just wrap the parameters to the next line as we do in other cases ? For example def longMethodName(param1, param2, ...) : Long = {} Are there a lot functions which use the old format ? Can we just stick to the above for new functions ? Thanks Shivaram Yeah that was my proposal - Essentially we can just have two styles: The entire function + parameterList + return type fits in one line or when it doesn't we wrap parameters into lines. I agree that it makes the code a more verbose, but it'll make code style more consistent. Shivaram +1 I ran into a weird bug today where trying to read a file from HDFS built using Hadoop 2 gives an error saying "No FileSystem for scheme: hdfs".  Specifically this only seems to happen when building an assembly jar in the application and not when using sbt's run-main. The project's setup[0] is pretty simple and is only a slight modification of the project used by the release audit tool. The sbt assembly instructions[1] are mostly copied from Spark's sbt build files. We run into this in SparkR as well, so it'll be great if anybody has an idea on how to debug this. To repoduce, you can do the following: 1. Launch a Spark EC2 cluster with 0.9.0 with --hadoop-major-version=2 2. Clone https://github.com/shivaram/spark-utils 3. Run release-audits/sbt_app_core/run-hdfs-test.sh Thanks Shivaram [0] https://github.com/shivaram/spark-utils/blob/master/release-audits/sbt_app_core/src/main/scala/SparkHdfsApp.scala [1] https://github.com/shivaram/spark-utils/blob/master/release-audits/sbt_app_core/build.sbt Thanks a lot Jey ! That fixes things. For reference I had to add the following line to build.sbt case m if m.toLowerCase.matches("meta-inf/services.*$")  => MergeStrategy.concat Should we also add this to Spark's assembly build ? Thanks Shivaram Thanks for the pointer -- I guess I should have checked Spark's build script again while debugging. This might be useful to include in a documentation page about how to write and run Spark apps. I think there's are a bunch of such know-how just floating around right now. Shivaram Sorry this request is coming in a bit late, but would it be possible to backport SPARK-979[1] to branch-0.9 ? This is the patch for randomizing executor offers and I would like to use this in a release sooner rather than later. Thanks Shivaram [1] https://github.com/apache/spark/commit/556c56689bbc32c6cec0d07b57bd3ec73ceb243e#diff-8ef3258646b0e6a4793d6ad99848eacd I don't think the attachment came through in the list. Could you upload the results somewhere and link to them ? There is a alpha version of SparkR that you can use from https://github.com/amplab-extras/SparkR-pkg -- It works with Spark 0.9.0 and above and has most of the basic RDD functions implemented. Thanks Shivaram I think it would be very useful to have this. We could put the ui display either behind a flag or a url parameter Shivaram It should be possible to improve cluster launch time if we are careful about what commands we run during setup. One way to do this would be to walk down the list of things we do for cluster initialization and see if there is anything we can do make things faster. Unfortunately this might be pretty time consuming, but I don't know of a better strategy. The place to start would be the setup.sh file at https://github.com/mesos/spark-ec2/blob/v3/setup.sh Here are some things that take a lot of time and could be improved: 1. Creating swap partitions on all machines. We could check if there is a way to get EC2 to always mount a swap partition 2. Copying / syncing things across slaves. The copy-dir script is called too many times right now and each time it pauses for a few milliseconds between slaves [1]. This could be improved by removing unnecessary copies 3. We could make less frequently used modules like Tachyon, persistent hdfs not a part of the default setup. [1] https://github.com/mesos/spark-ec2/blob/v3/copy-dir.sh#L42 Thanks Shivaram One way to do this would be to have a Github hook that parses -1s or +1s and posts a commit status [1] (like say Travis [2]) right next to the PR. Does anybody know of an existing tool that does this ? Shivaram [1] https://github.com/blog/1227-commit-status-api [2] http://blog.travis-ci.com/2012-09-04-pull-requests-just-got-even-more-awesome/ metrics in ADAM ? --  I've run into a similar use-case with having user-defined metrics in long-running tasks and I think a nice way to solve this would be to have user-defined TaskMetrics. To state my problem more clearly, lets say you have two functions you use in a map call and want to measure how much time each of them takes. For example, if you have a code block like the one below and you want to measure how much time f1 takes as a fraction of the task. a.map { l => val f = f1(l) ... some work here ... } It would be really cool if we could do something like a.map { l => val start = System.nanoTime val f = f1(l) TaskMetrics.get("f1-time").add(System.nanoTime - start) } These task metrics have a different purpose from Accumulators in the sense that we don't need to track lineage, perform commutative operations etc. Further we also have a bunch of code in place to aggregate task metrics across a stage etc. So it would be great if we could also populate these in the UI and show median/max etc. I think counters [1] in Hadoop served a similar purpose. Thanks Shivaram [1] https://www.inkling.com/read/hadoop-definitive-guide-tom-white-3rd/chapter-8/counters Thanks Patrick -- It does look like some maven misconfiguration as wget http://repo1.maven.org/maven2/org/scala-lang/scala-library/2.10.2/scala-library-2.10.2.pom works for me. Shivaram Yeah I worked on DistributedR while I was an intern at HP Labs, but it has evolved a lot since then. I don't think its a direct comparison as DistributedR is a pure R implementation in a distributed setting while SparkR is a wrapper around the Scala / Java implementations in Spark. That said, it would be an interesting exercise to compare them and I hope to do it at some point. Shivaram Also I think Jenkins doesn't post build timeouts to github. Is there anyway we can fix that ? Jenkins runs for this PR https://github.com/apache/spark/pull/1960 timed out without notification. The relevant Jenkins logs are at https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18588/consoleFull https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18592/consoleFull https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/18597/consoleFull Hi I'd like to announce a couple of updates to the SparkR project. In order to facilitate better collaboration for new features and development we have a new mailing list, issue tracker for SparkR. - The new JIRA is hosted at https://sparkr.atlassian.net/browse/SPARKR/ and we have migrated all existing Github issues to the JIRA. Please submit any bugs / improvements to this JIRA going forward. - There is a new mailing list sparkr-dev@googlegroups.com that will be used for design discussions for new features and development related issues. We will still be answering to user issues on Apache Spark mailing lists. Please let me know if have any questions. Thanks Shivaram FWIW matrix multiplication is extremely communication intensive when you have two row partitioned matrices and there are often other ways to solve problems. Regardless, it would be good to have a more complete matrix library and it would be good to contribute some of the stuff we have done in the AMPLab to MLLib. Shivaram FWIW I like the multi-line // over /* */ from a purely style standpoint. The Google Java style guide[1] has some comment about code formatting tools working better with /* */ but there doesn't seem to be any strong arguments for one over the other I can find Thanks Shivaram [1] https://google-styleguide.googlecode.com/svn/trunk/javaguide.html#s4.8.6.1-block-comment-style +spark dev list Yes, we should add an Apache license to it -- Feel free to open a PR for it. BTW though it is a part of the mesos github account, it is almost exclusively used by the Spark Project AFAIK. Longer term it may make sense to move it to a more appropriate github account (we could move it to amplab/ for instance as the AMPLab provides Jenkins support etc. too) Thanks Shivaram One reason I wouldn't change the default is that the Hadoop 2 launched by spark-ec2 is not a full Hadoop 2 distribution -- Its more of a hybrid Hadoop version built using CDH4 (it uses HDFS 2, but not YARN AFAIK). Also our default Hadoop version in the Spark build is still 1.0.4 [1], so it makes sense to stick to that in spark-ec2 as well ? [1] https://github.com/apache/spark/blob/master/pom.xml#L122 Thanks Shivaram FWIW there is a PR open to add support for Hadoop 2.4 to spark-ec2 scripts at https://github.com/mesos/spark-ec2/pull/77 -- But it hasnt' received much review or testing to be merged. Thanks Shivaram I haven't looked closely at the sampling issues, but regarding the aggregation latency, there are fixed overheads (in local and distributed mode) with the way aggregation is done in Spark. Launching a stage of tasks, fetching outputs from the previous stage etc. all have overhead, so I would say its not efficient / recommended to run stages where computation is less than 500ms or so. You could increase your batch size based on this and hopefully that will help. Regarding reducing these overheads by an order of magnitude it is a challenging problem given the architecture in Spark -- I have some ideas for this, but they are very much at a research stage. Thanks Shivaram I'm not sure its exactly easy to define 'production' use. One thing we could stress is that spark-ec2 is meant to be run manually (i.e. it outputs errors, asks for prompts etc.) and that automating it is not in our scope right now. Shivaram A related question that has affected me in the past: If we get a PR from a new developer I sometimes find that I am not able to assign an issue to them after merging the PR. Is there a process we need follow to get new contributors on to a particular group in JIRA ? Or does it somehow happen automatically ? Thanks Shivaram My feeling is that we should have a handful of namespaces (say 4 or 5). It becomes too cumbersome to import / remember more package names and having everything in one package makes it hard to read scaladoc etc. Thanks Shivaram I dont know much about Python style, but I think the point Wes made about usability on the JIRA is pretty powerful. IMHO the number of methods on a Spark DataFrame might not be much more compared to Pandas. Given that it looks like users are okay with the possibility of collisions in Pandas I think sticking (1) is not a bad idea. Also is it possible to detect such collisions in Python ? A (4)th option might be to detect that `df` contains a column named `name` and print a warning in `df.name` which tells the user that the method is overriding the column. Thanks Shivaram Agree that toDF is not very useful. In fact it was removed from the namespace in a recent change https://github.com/apache/spark/commit/4e930420c19ae7773b138dfc7db8fc03b4660251 Thanks Shivaram Thanks for catching this. I'll check with Patrick to see why the R API docs are not getting included. Sorry for the delay in getting back on this. So the RDD interface is private in the 1.4 release but as Alek mentioned you can still use it by prefixing `SparkR:::`. Regarding design direction -- there are two JIRAs which cover major features we plan to work on for 1.5. SPARK-6805 tracks porting high-level machine learning operations like `glm` and `kmeans` to SparkR using the ML Pipeline implementation in Scala as the backend. We are also planning to develop a parallel API where users can run native R functions in a distributed setting and SPARK-7264 tracks this effort. If you have specific use cases feel free to chime in on the JIRA or on the dev mailing list. Thanks Shivaram Hi all We recently merged support for launching YARN clusters using Spark EC2 scripts as a part of https://issues.apache.org/jira/browse/SPARK-3674. To use this you can pass in hadoop-major-version as "yarn" to the spark-ec2 script and this will setup Hadoop 2.4 HDFS, YARN and Spark built for YARN on the EC2 cluster. Developers who work on features related to YARN might find this useful for testing / benchmarking Spark with YARN. If anyone has questions or feedback please let me know. Thanks Shivaram We do have a JIRA open for this at https://issues.apache.org/jira/browse/SPARK-6813 but I don't think anybody is actively working on it yet. FWIW I think https://github.com/jimhester/lintr looks more recently updated compared to google-rlint, but we can discuss more about this on the JIRA Every language has its own quirks / features -- so I don't think there exists a document on how to go about doing this for a new language. The most related write up I know of is the wiki page on PySpark internals https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals written by Josh Rosen -- It covers some of the issues like closure capture, serialization, JVM communication that you'll need to handle for a new language. Thanks Shivaram The SparkR code is in the `R` directory i.e. https://github.com/apache/spark/tree/master/R Shivaram We don't use the rscala package in SparkR -- We have an in built R-JVM bridge that is customized to work with various deployment modes. You can find more details in my Spark Summit 2015 talk. Thanks Shivaram You can see the slides, video at https://spark-summit.org/2015/events/sparkr-the-past-the-present-and-the-future/ +1 Tested the EC2 launch scripts and the Spark version and EC2 branch etc. look good. Shivaram Hi Pradeep Thanks for the catch -- Lets open a JIRA and PR for it. I don't think documentation changes affect the release though Patrick can confirm that. Thanks Shivaram There are a couple of PRs open for it (linked from the JIRA) and I am reviewing https://github.com/mesos/spark-ec2/pull/121-- Also the EC2 fixes can be out of band from the release itself, so the fix will make to 1.4.1 once the above PR is merged. Thanks Shivaram callJMethod is a private R function that is defined in https://github.com/apache/spark/blob/a0cc3e5aa3fcfd0fce6813c520152657d327aaf2/R/pkg/R/backend.R#L31 callJMethod serializes the function names, arguments and sends them over a socket to the JVM. This is the socket-based R to JVM bridge described in the Spark Summit talk https://spark-summit.org/2015/events/sparkr-the-past-the-present-and-the-future/ Thanks Shivaram The R and Python implementations differ in how they communicate with the JVM so there is no invariant there per-se. Thanks Shivaram I think moving the repo-location and re-organizing the python code to handle dependencies, testing etc. sounds good to me. However, I think there are a couple of things which I am not sure about 1. I strongly believe that we should preserve existing command-line in ec2/spark-ec2 (i.e. the shell script not the python file). This could be a thin wrapper script that just checks out the or downloads something (similar to say build/mvn). Mainly, I see no reason to break the workflow that users are used to right now. 2. I am also not sure about that moving the issue tracker is necessarily a good idea. I don't think we get a large number of issues due to the EC2 stuff  and if we do have a workflow for launching EC2 clusters, the Spark JIRA would still be the natural place to report issues related to this. At a high level I see the spark-ec2 scripts as an effort to provide a reference implementation for launching EC2 clusters with Apache Spark -- Given this view I am not sure it makes sense to completely decouple this from the Apache project. Thanks Shivaram Both SparkR and the PySpark API call into the JVM Spark API (i.e. JavaSparkContext, JavaRDD etc.). They use different methods (Py4J vs. the R-Java bridge) to call into the JVM based on libraries available / features supported in each language. So for Haskell, one would need to see what is the best way to call the underlying Java API functions from Haskell and get results back. Thanks Shivaram Some replies inline I've created https://github.com/amplab/spark-ec2 and added an initial set of committers. Note that this is not a fork of the existing github.com/mesos/spark-ec2 and users will need to fork from here. This is mostly to avoid the base-fork in pull requests being set incorrectly etc. I'll be migrating some PRs / closing them in the old repo and will also update the README in that repo. Thanks Shivaram Technically I think the project ends in 2017 and I think we will figure out a transition for AMPLab repositories when the project ends. I think it should be pretty simple to transfer ownership to a new organization if / when the time comes around. Thanks Shivaram There is technically no PMC for the spark-ec2 project (I guess we are kind of establishing one right now). I haven't heard anything from the Spark PMC on the dev list that might suggest a need for a vote so far. I will send another round of email notification to the dev list when we have a JIRA / PR that actually moves the scripts (right now the only thing that changed is the location of some scripts in mesos/ to amplab/). Thanks Shivaram Thats part of the confusion we are trying to fix here -- the repository used to live in the mesos github account but was never a part of the Apache Mesos project. It was a remnant part of Spark from when Spark used to live at github.com/mesos/spark. Shivaram Yeah I'll send a note to the mesos dev list just to make sure they are informed. Shivaram Yes - It is still in progress, but I have just not gotten time to get to this. I think getting the repo moved from mesos to amplab in the codebase by 1.5 should be possible. Thanks Shivaram I sent a note to the Mesos developers and created https://github.com/apache/spark/pull/7899 to change the repository pointer. There are 3-4 open PRs right now in the mesos/spark-ec2 repository and I'll work on migrating them to amplab/spark-ec2 later today. My thoughts on moving the python script is that we should have a wrapper shell script that just fetches the latest version of spark_ec2.py for the corresponding Spark branch. We already have separate branches in our spark-ec2 repository for different Spark versions so it can just be a call to `wget https://github.com/amplab/spark-ec2/tree//driver/spark_ec2.py`. Thanks Shivaram PythonRDD.scala has a number of PySpark specific conventions (for example worker reuse, exceptions etc.) and PySpark specific protocols (e.g. for communicating accumulators, broadcasts between the JVM and Python etc.). While it might be possible to refactor the two classes to share some more code I don't think its worth making the code more complex in order to do that. Thanks Shivaram The in-process JNI only works out when the R process comes up first and we launch a JVM inside it. In many deploy modes like YARN (or actually in anything using spark-submit) the JVM comes up first and we launch R after that. Using an inter-process solution helps us cover both use cases Thanks Shivaram Thanks for the catch. Could you send a PR with this diff ? FYI The staging repository published as version 1.5.0 is at https://repository.apache.org/content/repositories/orgapachespark-1136 while the staging repository published as version 1.5.0-rc1 is at https://repository.apache.org/content/repositories/orgapachespark-1137 Thanks Shivaram I've seen similar tar file warnings and in my case it was because I was using the default tar on a Macbook. Using gnu-tar from brew made the warnings go away. Thanks Shivaram Its possible -- in the sense that a lot of designs are possible. But AFAIK there are no clean interfaces for getting all the arguments / SparkConf options from spark-submit and its all the more tricker to handle scenarios where the first JVM has already created a SparkContext that you want to use from R. The inter-process communication is cleaner, pretty lightweight and handles all the scenarios. Thanks Shivaram I think Hao posted a link to the source code in the description of https://issues.apache.org/jira/browse/SPARK-6803 As Rui says it would be good to understand the use case we want to support (supporting CRAN installs could be one for example). I don't think it should be very hard to do as the RBackend itself doesn't use the R source files. The RRDD does use it and the value comes from https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/api/r/RUtils.scala#L29 AFAIK -- So we could introduce a new config flag that can be used for this new mode. Thanks Shivaram I don't think the crux of the problem is about users who download the source -- Spark's source distribution is clearly marked as something that needs to be built and they can run `mvn -DskipTests -Psparkr package` based on instructions in the Spark docs. The crux of the problem is that with a source or binary R package, the client side the SparkR code needs the Spark JARs to be available. So we can't just connect to a remote Spark cluster using just the R scripts as we need the Scala classes around to create a Spark context etc. But this is a use case that I've heard from a lot of users -- my take is that this should be a separate package / layer on top of SparkR. Dan Putler (cc'd) had a proposal on a client package for this and maybe able to add more. Thanks Shivaram I think that getting them from the ASF mirrors is a better strategy in general as it'll remove the overhead of keeping the S3 bucket up to date. It works in the spark-ec2 case because we only support a limited number of Hadoop versions from the tool. FWIW I don't have write access to the bucket and also haven't heard of any plans to support newer versions in spark-ec2. Thanks Shivaram Thanks for investigating this. The right place to add these is the core-site.xml template we have at https://github.com/amplab/spark-ec2/blob/branch-1.5/templates/root/spark/conf/core-site.xml and/or https://github.com/amplab/spark-ec2/blob/branch-1.5/templates/root/ephemeral-hdfs/conf/core-site.xml Feel free to open a PR against the amplab/spark-ec2 repository for this. Thanks Shivaram Yeah we just need to add 1.5.2 as in https://github.com/apache/spark/commit/97956669053646f00131073358e53b05d0c3d5d0#diff-ada66bbeb2f1327b508232ef6c3805a5 to the master branch as well Thanks Shivaram Yeah - that needs to be changed as well. Could you send a PR to fix this ? Shivaram I think its just a bug -- I think we originally followed the Python API (in the original PR [1]) but the Python API seems to have been changed to match Scala / Java in https://issues.apache.org/jira/browse/SPARK-6366 Feel free to open a JIRA / PR for this. Thanks Shivaram [1] https://github.com/amplab-extras/SparkR-pkg/pull/199/files I just ran the tests using a recently synced master branch and the tests seemed to work fine. My guess is some of the Java classes changed and you need to rebuild Spark ? Thanks Shivaram As far as I know the process is just to copy docs/_site from the build to the appropriate location in the SVN repo (i.e. site/docs/2.0.0-preview). Thanks Shivaram Can you open an issue on https://github.com/amplab/spark-ec2 ?  I think we should be able to escape the version string and pass the 2.0.0-preview through the scripts Shivaram -sparkr-dev@googlegroups +dev@spark.apache.org [Please send SparkR development questions to the Spark user / dev mailing lists. Replies inline] The reason this is different in Spark 1.6 is that we added support for automatically deserializing Maps returned from the JVM as environments on the R side. The pull request https://github.com/apache/spark/pull/8711 has some more details. The reason BitSet / ArrayList "work" is that we don't do any special serialization / de-serialization for them. Unfortunately there isn't much more to say than that. The serialization/de-serialization is an internal API and we don't claim to maintain backwards compatibility. You might be able to work around this particular issue by wrapping your Map in a different object. Thanks Shivaram --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hashes, sigs match. I built and ran tests with Hadoop 2.3 ("-Pyarn -Phadoop-2.3 -Phive -Pkinesis-asl -Phive-thriftserver"). I couldn't get the following tests to pass but I think it might be something specific to my setup as Jenkins on branch-2.0 seems quite stable. [error] Failed tests: [error] org.apache.spark.sql.hive.client.VersionsSuite [error] org.apache.spark.sql.hive.HiveSparkSubmitSuite [error] Error during tests: [error] org.apache.spark.sql.hive.HiveExternalCatalogSuite Regarding the open issues, I agree with Sean that most of them seem minor to me and not worth blocking a release for. It would be good to get more details on SPARK-16011 though As for the docs, ideally we should have them in place before the RC but given that this is a recurring issue I'm wondering if having a separate updatable link (like the 2.0.0-rc4-updated that Reynold posted yesterday) can be used. The semantics we could then have are that the docs should be ready when the vote succeeds rather than being ready when the vote starts. Thanks Shivaram +1 SHA and MD5 sums match for all binaries. Docs look fine this time around. Built and ran `dev/run-tests` with Java 7 on a linux machine. No blocker bugs on JIRA and the only critical bug with target as 2.0.0 is SPARK-16633, which doesn't look like a release blocker. I also checked issues which are marked as Critical affecting version 2.0.0 and the only other ones that seem applicable are SPARK-15703 and SPARK-16334. Both of them don't look like blockers to me. Thanks Shivaram I think you can also pass in a zip file using the --files option (http://spark.apache.org/docs/latest/running-on-yarn.html has some examples). The files should then be present in the current working directory of the driver R process. Thanks Shivaram I think takeSample itself runs multiple jobs if the amount of samples collected in the first pass is not enough. The comment and code path at https://github.com/apache/spark/blob/412b0e8969215411b97efd3d0984dc6cac5d31e0/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L508 should explain when this happens. Also you can confirm this by checking if the logWarning shows up in your logs. Thanks Shivaram I looked into this and found the problem. Will send a PR now to fix this. If you are curious about what is happening here: When we build the docs separately we don't have the JAR files from the Spark build in the same tree. We added a new set of docs recently in SparkR called an R vignette that runs Spark and generates docs using outputs from the run.  So this doesn't work when the JARs are not available. Thanks Shivaram +1 I think having a 4 month window instead of a 3 month window sounds good. However I think figuring out a timeline for maintenance releases would also be good. This is a common concern that comes up in many user threads and it'll be better to have some structure around this. It doesn't need to be strict, but something like the first maintenance release for the latest 2.x.0 release within 2 months. And then a second maintenance release within 6 months or something like that. Thanks Shivaram Yeah I see the apache maven repos have the 2.0.1 artifacts at https://repository.apache.org/content/repositories/releases/org/apache/spark/spark-core_2.11/ -- Not sure why they haven't synced to maven central yet Shivaram Thanks Fred - that is very helpful. Could you expand a little bit more on stability ? Is it just bursty workloads in terms of peak vs. average throughput ? Also what level of latencies do you find users care about ? Is it on the order of 2-3 seconds vs. 1 second vs. 100s of milliseconds ? --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Thanks Fred for the detailed reply. The stability points are especially interesting as a goal for the streaming component in Spark. In terms of next steps, one approach that might be helpful is trying to create benchmarks or situations that mimic real-life workloads and then we can work on isolating specific changes that are required etc. It'd also be great to hear other approaches / next steps to concretize some of these goals. Thanks Shivaram +1 - Given that our website is now on github (https://github.com/apache/spark-website), I think we can move most of our wiki into the main website. That way we'll only have two sources of documentation to maintain: A release specific one in the main repo and the website which is more long lived. Thanks Shivaram At the AMPLab we've been working on a research project that looks at just the scheduling latencies and on techniques to get lower scheduling latency. It moves away from the micro-batch model, but reuses the fault tolerance etc. in Spark. However we haven't yet figure out all the parts in integrating this with the rest of structured streaming. I'll try to post a design doc / SIP about this soon. On a related note - are there other problems users face with micro-batch other than latency ? Thanks Shivaram Do we have any query workloads for which we can benchmark these proposals in terms of performance ? Thanks Shivaram FWIW 2.0.1 is also used in the 'Link With Spark' and 'Spark Source Code Management' sections in that page. Shivaram +0 I am not sure how much of a problem this is but the pip packaging seems to have changed the size of the hadoop-2.7 artifact. As you can see in http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc2-bin/, the Hadoop 2.7 build is 359M almost double the size of the other Hadoop versions. This comes from the fact that we build our pip package using the Hadoop 2.7 profile [1] and the pip package is contained inside this tarball. The fix for this is to exclude the pip package from the distribution in [2] Thanks Shivaram [1] https://github.com/apache/spark/blob/202fcd21ce01393fa6dfaa1c2126e18e9b85ee96/dev/create-release/release-build.sh#L242 [2] https://github.com/apache/spark/blob/202fcd21ce01393fa6dfaa1c2126e18e9b85ee96/dev/make-distribution.sh#L240 In addition to usual binary artifacts, this is the first release where we have installable packages for Python [1] and R [2] that are part of the release.  I'm including instructions to test the R package below. Holden / other Python developers can chime in if there are special instructions to test the pip package. To test the R source package you can follow the following commands. 1. Download the SparkR source package from http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc5-bin/SparkR_2.1.0.tar.gz 2. Install the source package with R CMD INSTALL SparkR_2.1.0.tar.gz 3. As the SparkR package doesn't contain Spark JARs (this is due to package size limits from CRAN), we'll need to run [3] export SPARKR_RELEASE_DOWNLOAD_URL="http://people.apache.org/~pwendell/spark-releases/spark-2.1.0-rc5-bin/spark-2.1.0-bin-hadoop2.6.tgz"4. Launch R. You can now use include SparkR with `library(SparkR)` and test it with your applications. 5. Note that the first time a SparkSession is created the binary artifacts will the downloaded. Thanks Shivaram [1] https://issues.apache.org/jira/browse/SPARK-18267 [2] https://issues.apache.org/jira/browse/SPARK-18590 [3] Note that this isn't required once 2.1.0 has been released as SparkR can automatically resolve and download releases. I can't see the resolve button either - Maybe we can forward this to Apache Infra and see if they can close these issues ? Shivaram I'm not sure why the AppVeyor updates are coming to the dev list.  Hyukjin -- Do you know if we made any recent changes that might have caused this ? Thanks Shivaram Thanks for investigating. We should file an INFRA jira about this. Shivaram