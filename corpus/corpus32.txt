Hi dev team
(Apologies for a long email!)
Firstly great news about the inclusion of MLlib into the Spark project!
I've been working on a concept and some code for a machine learning library
on Spark, and so of course there is a lot of overlap between MLlib and what
I've been doing.
I wanted to throw this out there and (a) ask a couple of design and roadmap
questions about MLLib, and (b) talk about how to work together / integrate
my ideas (if at all :)
*Some questions*
*
*
1. What is the general design idea behind MLLib - is it aimed at being a
collection of algorithms, ie a library? Or is it aimed at being a "Mahout
for Spark", i.e. something that can be used as a library as well as a set
of tools for things like running jobs, feature extraction, text processing
etc?
2. How married are we to keeping it within the Spark project? While I
understand the reasoning behind it I am not convinced it's best. But I
guess we can wait and see how it develops
3. Some of the original test code I saw around the Block ALS did use Breeze
(https://github.com/dlwh/breeze) for some of the linear algebra. Now I see
everything is using JBLAS directly and Array[Double]. Is there a specific
reason for this? Is it aimed at creating a separation whereby the linear
algebra backend could be switched out? Scala 2.10 issues?
4. Since Spark is meant to be nicely compatible with Hadoop, do we care
about compatibility/integration with Mahout? This may also encourage Mahout
developers to switch over and contribute their expertise (see for example
Dmitry's work at:
https://github.com/dlyubimov/mahout-commits/commits/dev-0.8.x-scala/math-scala,
where he is doing a Scala/Spark DSL around mahout-math matrices and
distributed operations). Potentially even using mahout-math for linear
algebra routines?
5. Is there a roadmap? (I've checked the JIRA which does have a few
intended models etc). Who are the devs most involved in this project?
6. What are thoughts around API design for models?
*Some thoughts*
*
*
So, over the past couple of months I have been working on a machine
learning library. Initially it was for my own use but I've added a few
things and was starting to think about releasing it (though it's not nearly
ready). The model that I really needed first was ALS for doing
recommendations. So I have ported the ALS code from Mahout to Spark. Well,
"ported" in some sense - mostly I copied the algorithm and data
distribution design, using Spark's primitives and Breeze for all the linear
algebra.
I found it pretty straightforward to port over. So far I have done local
testing only on the Movielens datasets. I have found my RMSE results to
match that of Mahout's. Overall interestingly the wall clock performance is
not as dissimilar as I would have expected. But I would like to now do some
larger-scale tests on a cluster to really do a good comparison.
Obviously with Spark's Block ALS model, my version is now somewhat
superfluous since I expect (and have so far seen in my simple local
experiments) that the block model will significantly outperform. I will
probably be porting my use case over to this in due time once I've done
further testing.
I also found Breeze to be very nice to work with and like the DSL - hence
my question about why not use that? (Especially now that Breeze is actually
just breeze-math and breeze-viz).
Anyway, I then added KMeans (basically just the Spark example with some
Breeze tweaks), and started working on a Linear Model framework. I've also
added a simple framework for arg parsing and config (using Twitter
Algebird's Args and Typesafe Config), and have started on feature
extraction stuff - of particular interest will be text feature extraction
and feature hashing.
This is roughly the idea for a machine learning library on Spark that I
have - call it a design or manifesto or whatever:
- Library available and consistent across Scala, Java and Python (as much
as possible in any event)
- A core library and also a set of stuff for easily running models based on
standard input formats etc
- Standardised model API (even across languages) to the extent possible.
I've based mine so far on Python's scikit-learn (.fit(), .predict() etc).
Why? I believe it's a major strength of scikit-learn, that its API is so
clean, simple and consistent. Plus, for the Python version of the lib,
scikit-learn will no doubt be used wherever possible to avoid re-creating
code
- Models to be included initially:
  - ALS
  - Possibly co-occurrence recommendation stuff similar to Mahout's Taste
  - Clustering (K-Means and others potentially)
  - Linear Models - the idea here is to have something very close to Vowpal
Wabbit, ie a generic SGD engine with various Loss Functions, learning rate
paradigms etc. Furthermore this would allow other models similar to VW such
as online versions of matrix factorisation, neural nets and learning
reductions
  - Possibly Decision Trees / Random Forests
- Some utilities for feature extraction (hashing in particular), and to
make running jobs easy (integration with Spark's ./run etc?)
- Stuff for making pipelining easy (like scikit-learn) and for doing things
like cross-validation in a principled (and parallel) way
- Clean and easy integration with Spark Streaming for online models (e.g. a
linear SGD can be called with fit() on batch data, and then fit() and/or
fit/predict() on streaming data to learn further online etc).
- Interactivity provided by shells (IPython, Spark shell) and also plotting
capability (Matplotlib, and Breeze Viz)
- For Scala, integration with Shark via sql2rdd etc.
- I'd like to create something similar to Scalding's Matrix API based on
RDDs for representing distributed matrices, as well as integrate the ideas
of Dmitry and Mahout's DistributedRowMatrix etc
Here is a rough outline of the model API I have used at the moment:
https://gist.github.com/MLnick/6068841. This works nicely for ALS,
clustering, linear models etc.
So as you can see, mostly overlapping with what MLlib already has or has
planned in some way, but my main aim is frankly to have consistency in the
API, some level of abstraction but to keep things as simple as possible (ie
let Spark handle the complex stuff), and thus hopefully avoid things
becoming just a somewhat haphazard collection of models that is not that
simple to figure out how to use - which is unfortunately what I believe has
happened to Mahout.
So the question then is, how to work together or integrate? I see 3 options:
1. I go my own way (not very appealing obviously)
2. Contribute what I have (or as much as makes sense) to MLlib
3. Create my project as a "front-end" or "wrapper" around MLlib as the
core, effectively providing the API and workflow interface but using MLlib
as the model engine.
#2 is appealing but then a lot depends on the API and framework design and
how much what I have in mind is compatible with the rest of the devs etc
#3 now that I have written it, starts to sound pretty interesting -
potentially we're looking at a "front-end" that could in fact execute
models on Spark (or other engines like Hadoop/Mahout, GraphX etc), while
providing workflows for pipelining transformations, feature extraction,
testing and cross-validation, and data viz.
But of course #3 starts sounding somewhat like what MLBase is aiming to be
(I think)!
At this point I'm willing to show out what I have done so far on a
selective basis - be warned though it is rough and not finished and
somewhat clunky perhaps as it's my first attempt at a library/framework, if
it makes sense. Especially because really the main thing I did was the ALS
port, and with MLlib's version of ALS that may be less useful now in any
case.
It may be that none of this is that useful to others anyway which is fine
as I'll keep developing tools that I need and potentially they will be
useful at some point.
Thoughts, feedback, comments, discussion? I really want to jump into MLlib
and get involved in contributing to standardised machine learning on Spark!
Nick
Hi
Ok, that all makes sense. I can see the benefit of good standard libraries
definitely, and I guess the pieces that felt "missing" to me were what you
are describing as MLI and MLOptimizer.
It seems like the aims of MLI are very much in line with what I have/had in
mind for a ML library/framework. It seems the goals overlap quite a lot.
I guess one "frustration" I have had is that there are all these great BDAS
projects, but we never really know when they will be released and what they
will look like until they are. In this particular case I couldn't wait for
MLlib so ended up doing some work myself to port Mahout's ALS and of course
have ended up duplicating effort (which is not a problem as it was
necessary at the time and has been a great learning experience).
Similarly for GraphX, I would like to develop a project for a Spark-based
version of Faunus (https://github.com/thinkaurelius/faunus) for batch
processing of data in our Titan graph DB. For now I am working with
Bagel-based primitives and Spark RDDs directly, but would love to use
GraphX, but have no idea when it will be released and have little
involvement until it is.
(I use "frustration" in the nicest way here - I love the BDAS concepts and
all the projects coming out, I just want them all to be released NOW!! :)
So yes I would love to be involved in MLlib and MLI work to the extent I
can assist and the work is aligned with what I need currently in my
projects (this is just from a time allocation viewpoint - I'm sure much of
it will be complementary).
Anyway, it seems to me the best course of action is as follows:
   - I'll get involved in MLlib and see how I can contribute there. Some
   things that jump out:
   - implicit preference capability for ALS model since as far as I can see
      currently it handles explicit prefs only? (Implicit prefs here:
      http://68.180.206.246/files/HuKorenVolinsky-ICDM08.pdf which is
      typically better if we don't have actual rating data but instead "view",
      "click", "play" or whatever)
      - RMSE and other evaluation metrics for ALS as well as test/train
      split / cross-val stuff?
      - linear model additions, like new loss functions for hinge loss,
      least squares etc for SGD, as well as learning rate stuff (
      http://arxiv.org/pdf/1305.6646) and regularisers (L1/L2/Elasic Net) -
      i.e. bring the SGD stuff in line with Vowpal Wabbit / sklearn (if that's
      desirable, my view is yes)
      - what about sparse weight and feature vectors for linear models/SGD?
      Together with hashing allows very large models while still being
efficient,
      and with L1 reg is particularly useful.
      - finally what about online models? ie SGD models currently are
      "static" ie once trained can only predict, whereas SGD can of course keep
      learning. Or does one simply re-train with the previous initial weight
      vector (I guess that can work just as well)... Also on this
topic training
      / predicting on Streams as well as RDDs
   - I can put up what I have done to a BitBucket account and grant access
   to whichever devs would like to take a look. The only reason I don't just
   throw it up on GitHub is that frankly it is not really ready and is not a
   fully-fledged project yet (I think anyway). Possibly some of this can be
   useful (not that there's all that much there apart from the ALS (but it
   does solve for both explicit and implicit preference data as per Mahout's
   implementation), KMeans (simpler than the one in MLlib as I didn't yet get
   around to doing KMeans++ init) and the arg-parsing / jobrunner (which may
   or may not be interesting both for ML and for Spark jobs in general)).
Let me know your thoughts
Nick
On Wed, Jul 24, 2013 at 10:09 PM, Ameet Talwalkar
 Hi Nick,
Quite interesting, and timely given current thinking around MLlib and MLI
http://orbi.ulg.ac.be/bitstream/2268/154357/1/paper.pdf
I do really like the way they have approached their API - and so far MLlib
seems to be following a (roughly) similar approach.
Interesting in particular they obviously go for mutable models instead of
the Estimator / Predictor interface MLlib currently has. Not sure which of
these is "best" really, they each have their pros & cons.
N
Hi
I submitted my license agreement and account name request a while back, but
still haven't received any correspondence. Just wondering what I need to do
in order to follow this up?
Thanks
Nick
Hi
I know everyone's pretty busy with getting 0.8.0 out, but as and when folks
have time it would be great to get your feedback on this PR adding support
for the 'implicit feedback' model variant to ALS:
https://github.com/apache/incubator-spark/pull/4
In particular any potential efficiency improvements, issues, and testing it
out locally and on a cluster and on some datasets!
Comments & feedback welcome.
Many thanks
Nick
There was another discussion on the old dev list about this:
https://groups.google.com/forum/#!msg/spark-developers/GL2_DwAeh5s/9rwQ3iDa2t4J
I tend to agree with having configuration sitting in JSON (or properties
files) and using the Typesafe Config library which can parse both.
Something I've used in my apps is along these lines:
https://gist.github.com/MLnick/6578146
It's then easy to have default config overridden with CLI for example:
val conf = cliConf.withFallback(defaultConf)
I'd be happy to be involved in working on this if there is a consensus
about best approach
N
On Mon, Sep 16, 2013 at 9:29 AM, Mike  wrote:
Hi Spark Devs
I was wondering what appetite there may be to add the ability for PySpark
users to create RDDs from (somewhat) arbitrary Hadoop InputFormats.
In my data pipeline for example, I'm currently just using Scala (partly
because I love it but also because I am heavily reliant on quite custom
Hadoop InputFormats for reading data). However, many users may prefer to
use PySpark as much as possible (if not for everything). Reasons might
include the need to use some Python library. While I don't do it yet, I can
certainly see an attractive use case for using say scikit-learn / numpy to
do data analysis & machine learning in Python. Added to this my cofounder
knows Python well but not Scala so it can be very beneficial to do a lot of
stuff in Python.
For text-based data this is fine, but reading data in from more complex
Hadoop formats is an issue.
The current approach would of course be to write an ETL-style Java/Scala
job and then process in Python. Nothing wrong with this, but I was thinking
about ways to allow Python to access arbitrary Hadoop InputFormats.
Here is a quick proof of concept: https://gist.github.com/MLnick/7150058
This works for simple stuff like SequenceFile with simple Writable
key/values.
To work with more complex files, perhaps an approach is to manipulate
Hadoop JobConf via Python and pass that in. The one downside is of course
that the InputFormat (well actually the Key/Value classes) must have a
toString that makes sense so very custom stuff might not work.
I wonder if it would be possible to take the objects that are yielded via
the InputFormat and convert them into some representation like ProtoBuf,
MsgPack, Avro, JSON, that can be read relatively more easily from Python?
Another approach could be to allow a simple "wrapper API" such that one can
write a wrapper function T => String and pass that into an
InputFormatWrapper that takes an arbitrary InputFormat and yields Strings
for the keys and values. Then all that is required is to compile that
function and add it to the SPARK_CLASSPATH and away you go!
Thoughts?
Nick
Hi Spark Devs
If you could pick one language binding to add to Spark what would it be?
Probably Clojure or JRuby if JVM is of interest.
I'm quite excited about Julia as a language for scientific computing (
http://julialang.org). The Julia community have been very focused on things
like interop with R, Matlab, and probably mostly Python (see
https://github.com/stevengj/PyCall.jl and
https://github.com/stevengj/PyPlot.jl for example).
Anyway, this is a bit of a thought experiment but I'd imagine a Julia API
would be similar in principle to the Python API. On the Spark Java side, it
would likely be almost the same. On the Julia side I'd imagine the major
sticking point would be serialisation (eg PyCloud equivalent code).
I actually played around with PyCall and was able to call PySpark from the
Julia console. You're able to run arbitrary Python PySpark code (though the
syntax is a bit ugly) and it seemed to mostly work.
However, when I tried to pass in a Julia function or closure, it failed at
the serialization step.
So one option would be to figure out how to serialize the required things
on the Julia side and to use PyCall for interop. This could add a fair bit
of overhead Julia  Python  Java so perhaps not worth it, but still
the idea of being able to use Spark for the distributed computing part and
to be able to mix n match Python code/libraries and Julia code/libraries
for things like stats/machine learning is very appealing!
Thoughts?
Nick
Thanks Josh, Patrick for the feedback.
Based on Josh's pointers I have something working for JavaPairRDD ->
PySpark RDD[(String, String)]. This just calls the toString method on each
key and value as before, but without the need for a delimiter. For
SequenceFile, it uses SequenceFileAsTextInputFormat which itself calls
toString to convert to Text for keys and values. We then call toString
(again) ourselves to get Strings to feed to writeAsPickle.
Details here: https://gist.github.com/MLnick/7230588
This also illustrates where the "wrapper function" api would fit in. All
that is required is to define a T => String for key and value.
I started playing around with MsgPack and can sort of get things to work in
Scala, but am struggling with getting the raw bytes to be written properly
in PythonRDD (I think it is treating them as pickled byte arrays when they
are not, but when I removed the 'stripPickle' calls and amended the length
(-6) I got "UnpicklingError: invalid load key, ' '. ").
Another issue is that MsgPack does well at writing "structures" - like Java
classes with public fields that are fairly simple - but for example the
Writables have private fields so you end up with nothing being written.
This looks like it would require custom "Templates" (serialization
functions effectively) for many classes, which means a lot of custom code
for a user to write to use it. Fortunately for most of the common Writables
a toString does the job. Will keep looking into it though.
Anyway, Josh if you have ideas or examples on the "Wrapper API from Python"
that you mentioned, I'd be interested to hear them.
If you think this is worth working up as a Pull Request covering
SequenceFiles and custom InputFormats with default toString conversions and
the ability to specify Wrapper functions, I can clean things up more, add
some functionality and tests, and also test to see if common things like
the "normal" Writables and reading from things like HBase and Cassandra can
be made to work nicely (any other common use cases that you think make
sense?).
Thoughts, comments etc welcome.
Nick
On Fri, Oct 25, 2013 at 11:03 PM, Patrick Wendell  As a starting point, a version where people just write their own "wrapper"
Wow Josh, that looks great. I've been a bit swamped this week but as soon
as I get a chance I'll test out the PR in more detail and port over the
InputFormat stuff to use the new framework (including the changes you
suggested).
I can then look deeper into the MsgPack functionality to see if it can be
made to work in a generic enough manner without requiring huge amounts of
custom Templates to be written by users.
Will feed back asap.
N
On Thu, Nov 7, 2013 at 5:03 AM, Josh Rosen  wrote:
CC'ing Spark Dev list
I have been thinking about this for quite a while and would really love to
see this happen.
Most of my pipeline ends up in Scala/Spark these days - which I love, but
it is partly because I am reliant on custom Hadoop input formats that are
just way easier to use from Scala/Java - but I still use Python a lot for
data analysis and interactive work. There is some good stuff happening with
Breeze in Scala and MLlib in Spark (and IScala) but the breadth just
doesn't compare as yet - not to mention IPython and plotting!
There is a PR that was just merged into PySpark to allow arbitrary
serialization protocols between the Java and Python layers. I hope to try
to use this to allow PySpark users to pull data from arbitrary Hadoop
InputFormats with minimum fuss. This I believe will open the way for many
(including me!) to use PySpark directly for virtually all distributed data
processing without "needing" to use Java (
https://github.com/apache/incubator-spark/pull/146) (
http://mail-archives.apache.org/mod_mbox/incubator-spark-dev/201311.mbox/browser
).
Linked to this is what I believe is huge potential to add distributed
PySpark versions of many algorithms in scikit-learn (and elsewhere). The
idea as intimated above, would be to have estimator classes with sklearn
compatible APIs. They may in turn use sklearn algorithms themselves (eg:
this shows how easy it would be for linear models:
https://gist.github.com/MLnick/4707012).
I'd be very happy to try to find some time to work on such a library (I had
started one in Scala that was going to contain a Python library also, but
I've just not had the time available and with Spark MLlib appearing and the
Hadoop stuff I had what I needed for my systems).
The main benefit I see is that sklearn already has:
- many algorithms to work with
- great, simple API
- very useful stuff like preprocessing, vectorizing and feature hashing
(very important for large scale linear models)
- obviously the nice Python ecosystem stuff like plotting, IPython
notebook, pandas, scikit-statsmodels and so on.
The easiest place to start in my opinion is to take a few of the basic
models in the PySpark examples and turn them into production-ready code
that utilises sklearn or other good libraries as much as possible.
(I think this library would live outside of both Spark and sklearn, at
least until it is clear where it should live).
I would be happy to help and provide Spark-related advice even if I cannot
find enough time to work on many algorithms. Though I do hope to find more
time toward the end of the year and early next year.
N
On Wed, Nov 27, 2013 at 12:42 AM, Uri Laserson  Hi all,
Hi Spark Devs
An idea developed recently out of a scikit-learn mailing list discussion (
http://sourceforge.net/mailarchive/forum.php?thread_name=CAFvE7K5HGKYH9Myp7imrJ-nU%3DpJgeGqcCn3JC0m4MmGWZi35Hw%40mail.gmail.com&forum_name=scikit-learn-general)
to have a coding sprint around Strata in Feb, focused on integration
between scikit-learn and PySpark for large-scale machine learning tasks.
Cloudera has kindly agreed to host the sprint, most likely in San
Francisco. Ideally it would be focused and capped at around 10 people. The
idea is not meant to be a teaching workshop for
newcomers but more as a prototyping session, so ideally it would be great
to have developers and users with deep knowledge of PySpark (Josh
especially :) and/or scikit-learn, attend.
Hopefully we can get some people from the Spark community involved, and
Olivier will drum up support from the scikit-learn community.
All the best and hope to see you there (though likely I will only be able
to join remotely).
Nick
Hi devs
I came across Dill (
http://trac.mystic.cacr.caltech.edu/project/pathos/wiki/dill) for Python
serialization. Was wondering if it may be a replacement to the cloudpickle
stuff (and remove that piece of code that needs to be maintained within
PySpark)?
Josh have you looked into Dill? Any thoughts?
N
Hi Spark Devs,
Hoping someone cane help me out. No matter what I do, I cannot get Intellij
to build Spark from source. I am using IDEA 13. I run sbt gen-idea and
everything seems to work fine.
When I try to build using IDEA, everything compiles but I get the error
below.
Have any of you come across the same?
======
Internal error: (java.lang.AssertionError)
java/nio/channels/FileChannel$MapMode already declared as
ch.epfl.lamp.fjbg.JInnerClassesAttribute$Entry@1b5b798b
java.lang.AssertionError: java/nio/channels/FileChannel$MapMode already
declared as ch.epfl.lamp.fjbg.JInnerClassesAttribute$Entry@1b5b798b
at
ch.epfl.lamp.fjbg.JInnerClassesAttribute.addEntry(JInnerClassesAttribute.java:74)
at
scala.tools.nsc.backend.jvm.GenJVM$BytecodeGenerator$$anonfun$addInnerClasses$3.apply(GenJVM.scala:738)
at
scala.tools.nsc.backend.jvm.GenJVM$BytecodeGenerator$$anonfun$addInnerClasses$3.apply(GenJVM.scala:733)
at
scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:59)
at scala.collection.immutable.List.foreach(List.scala:76)
at
scala.tools.nsc.backend.jvm.GenJVM$BytecodeGenerator.addInnerClasses(GenJVM.scala:733)
at
scala.tools.nsc.backend.jvm.GenJVM$BytecodeGenerator.emitClass(GenJVM.scala:200)
at
scala.tools.nsc.backend.jvm.GenJVM$BytecodeGenerator.genClass(GenJVM.scala:355)
at
scala.tools.nsc.backend.jvm.GenJVM$JvmPhase$$anonfun$run$4.apply(GenJVM.scala:86)
at
scala.tools.nsc.backend.jvm.GenJVM$JvmPhase$$anonfun$run$4.apply(GenJVM.scala:86)
at
scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:104)
at
scala.collection.mutable.HashMap$$anon$2$$anonfun$foreach$3.apply(HashMap.scala:104)
at scala.collection.Iterator$class.foreach(Iterator.scala:772)
at scala.collection.mutable.HashTable$$anon$1.foreach(HashTable.scala:157)
at
scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:190)
at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:45)
at scala.collection.mutable.HashMap$$anon$2.foreach(HashMap.scala:104)
at scala.tools.nsc.backend.jvm.GenJVM$JvmPhase.run(GenJVM.scala:86)
at scala.tools.nsc.Global$Run.compileSources(Global.scala:953)
at scala.tools.nsc.Global$Run.compile(Global.scala:1041)
at xsbt.CachedCompiler0.run(CompilerInterface.scala:123)
at xsbt.CachedCompiler0.liftedTree1$1(CompilerInterface.scala:99)
at xsbt.CachedCompiler0.run(CompilerInterface.scala:99)
at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:601)
at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)
at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)
at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
at
sbt.compiler.AggressiveCompile$$anonfun$6$$anonfun$compileScala$1$1$$anonfun$apply$3$$anonfun$apply$1.apply$mcV$sp(AggressiveCompile.scala:106)
at
sbt.compiler.AggressiveCompile$$anonfun$6$$anonfun$compileScala$1$1$$anonfun$apply$3$$anonfun$apply$1.apply(AggressiveCompile.scala:106)
at
sbt.compiler.AggressiveCompile$$anonfun$6$$anonfun$compileScala$1$1$$anonfun$apply$3$$anonfun$apply$1.apply(AggressiveCompile.scala:106)
at
sbt.compiler.AggressiveCompile.sbt$compiler$AggressiveCompile$$timed(AggressiveCompile.scala:173)
at
sbt.compiler.AggressiveCompile$$anonfun$6$$anonfun$compileScala$1$1$$anonfun$apply$3.apply(AggressiveCompile.scala:105)
at
sbt.compiler.AggressiveCompile$$anonfun$6$$anonfun$compileScala$1$1$$anonfun$apply$3.apply(AggressiveCompile.scala:102)
at scala.Option.foreach(Option.scala:236)
at
sbt.compiler.AggressiveCompile$$anonfun$6$$anonfun$compileScala$1$1.apply(AggressiveCompile.scala:102)
at
sbt.compiler.AggressiveCompile$$anonfun$6$$anonfun$compileScala$1$1.apply(AggressiveCompile.scala:102)
at scala.Option.foreach(Option.scala:236)
at
sbt.compiler.AggressiveCompile$$anonfun$6.compileScala$1(AggressiveCompile.scala:102)
at
sbt.compiler.AggressiveCompile$$anonfun$6.apply(AggressiveCompile.scala:151)
at
sbt.compiler.AggressiveCompile$$anonfun$6.apply(AggressiveCompile.scala:89)
at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:39)
at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:37)
at sbt.inc.Incremental$.cycle(Incremental.scala:75)
at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:34)
at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:33)
at sbt.inc.Incremental$.manageClassfiles(Incremental.scala:42)
at sbt.inc.Incremental$.compile(Incremental.scala:33)
at sbt.inc.IncrementalCompile$.apply(Compile.scala:27)
at sbt.compiler.AggressiveCompile.compile2(AggressiveCompile.scala:164)
at sbt.compiler.AggressiveCompile.compile1(AggressiveCompile.scala:73)
at
org.jetbrains.jps.incremental.scala.local.CompilerImpl.compile(CompilerImpl.scala:61)
at
org.jetbrains.jps.incremental.scala.local.LocalServer.compile(LocalServer.scala:26)
at
org.jetbrains.jps.incremental.scala.ScalaBuilder$$anonfun$5$$anonfun$apply$3$$anonfun$apply$4.apply(ScalaBuilder.scala:118)
at
org.jetbrains.jps.incremental.scala.ScalaBuilder$$anonfun$5$$anonfun$apply$3$$anonfun$apply$4.apply(ScalaBuilder.scala:100)
at scala.util.Either$RightProjection.map(Either.scala:536)
at
org.jetbrains.jps.incremental.scala.ScalaBuilder$$anonfun$5$$anonfun$apply$3.apply(ScalaBuilder.scala:100)
at
org.jetbrains.jps.incremental.scala.ScalaBuilder$$anonfun$5$$anonfun$apply$3.apply(ScalaBuilder.scala:99)
at scala.util.Either$RightProjection.flatMap(Either.scala:523)
at
org.jetbrains.jps.incremental.scala.ScalaBuilder$$anonfun$5.apply(ScalaBuilder.scala:99)
at
org.jetbrains.jps.incremental.scala.ScalaBuilder$$anonfun$5.apply(ScalaBuilder.scala:98)
at scala.util.Either$RightProjection.flatMap(Either.scala:523)
at
org.jetbrains.jps.incremental.scala.ScalaBuilder.doBuild(ScalaBuilder.scala:98)
at
org.jetbrains.jps.incremental.scala.ScalaBuilder.build(ScalaBuilder.scala:68)
at
org.jetbrains.jps.incremental.scala.ScalaBuilderService$ScalaBuilderDecorator.build(ScalaBuilderService.java:42)
at
org.jetbrains.jps.incremental.IncProjectBuilder.runModuleLevelBuilders(IncProjectBuilder.java:1086)
at
org.jetbrains.jps.incremental.IncProjectBuilder.runBuildersForChunk(IncProjectBuilder.java:797)
at
org.jetbrains.jps.incremental.IncProjectBuilder.buildTargetsChunk(IncProjectBuilder.java:845)
at
org.jetbrains.jps.incremental.IncProjectBuilder.buildChunkIfAffected(IncProjectBuilder.java:760)
at
org.jetbrains.jps.incremental.IncProjectBuilder.buildChunks(IncProjectBuilder.java:583)
at
org.jetbrains.jps.incremental.IncProjectBuilder.runBuild(IncProjectBuilder.java:344)
at
org.jetbrains.jps.incremental.IncProjectBuilder.build(IncProjectBuilder.java:184)
at org.jetbrains.jps.cmdline.BuildRunner.runBuild(BuildRunner.java:129)
at org.jetbrains.jps.cmdline.BuildSession.runBuild(BuildSession.java:224)
at org.jetbrains.jps.cmdline.BuildSession.run(BuildSession.java:113)
at
org.jetbrains.jps.cmdline.BuildMain$MyMessageHandler$1.run(BuildMain.java:133)
at
org.jetbrains.jps.service.impl.SharedThreadPoolImpl$1.run(SharedThreadPoolImpl.java:41)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
at java.util.concurrent.FutureTask.run(FutureTask.java:166)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:722)
   - Successfully built via sbt/sbt assembly/assembly on Mac OS X, as well
   as on a dev Ubuntu EC2 box
   - Successfully tested via sbt/sbt test locally
   - Successfully built and tested using mvn package locally
   - I've tested my own Spark jobs (built against 0.8.0-incubating) on this
   RC and all works fine, as well as tested with my job server (also built
   against 0.8.0-incubating)
   - Ran a few spark examples and the shell and PySpark shell
   - For my part, tested the MLlib implicit code I added, and checked docs
I'm +1
On Wed, Dec 11, 2013 at 11:04 AM, Prashant Sharma  I hope this PR https://github.com/apache/incubator-spark/pull/252 can
Thanks Evan, I tried it and the new SBT direct import seems to work well,
though I did run into issues with some yarn imports on Spark.
n
On Thu, Dec 12, 2013 at 7:03 PM, Evan Chan  wrote:
One option that is 3rd party that works nicely for the Hadoop project and it's related projects is http://search-hadoop.com - managed by sematext. Perhaps we can plead with Otis to add Spark lists to search-spark.com, or the existing site?
Just throwing it out there as a potential solution to at least searching and navigating the Apache lists
Sent from my iPad
Is Spark active in submitting anything for this?
+1
On Tue, Feb 11, 2014 at 9:17 AM, Matt Massie  wrote:
Thanks Parviz, this looks great and good to see it getting updated. Look
forward to 0.9.0!
A perhaps stupid question - where does the KinesisWordCount example live?
Is that an Amazon example, since I don't see it under the streaming
examples included in the Spark project. If it's a third party example is it
possible to get the code?
Thanks
Nick
On Fri, Feb 14, 2014 at 6:53 PM, Deyhim, Parviz  wrote:
Hi
What KeyClass and ValueClass are you trying to save as the keys/values of
your dataset?
On Sun, Feb 23, 2014 at 10:48 AM, Nan Zhu  wrote:
On the partitioning / id keys. If we would look at hash partitioning, how
feasible will it be to just allow the user and item ids to be strings? A
lot of the time these ids are strings anyway (UUIDs and so on), and it's
really painful to translate between String  Int the whole time.
Are there any obvious blockers to this? I am a bit rusty on the ALS code
but from a quick scan I think this may work. Performance may be an issue
with large String keys... Any majore issues/objections to this thinking?
I may be able to find time to take a stab at this if there is demand.
On Mon, Apr 7, 2014 at 6:08 AM, Xiangrui Meng  wrote:
GraphX, like Spark, will not typically be "real-time" (where by "real-time"
here I assume you mean of the order of a few 10s-100s ms, up to a few
seconds).
Spark can in some cases approach the upper boundary of this definition (a
second or two, possibly less) when data is cached in memory and the
computation is not "too heavy", while Spark Streaming may be able to get
closer to the mid-to-upper boundary of this under similar conditions,
especially if aggregating over relatively small windows.
However, for this use case (while I haven't used GraphX yet) I would say
something like Titan (https://github.com/thinkaurelius/titan/wiki) or a
similar OLTP graph DB may be what you're after. But this depends on what
kind of graph traversal you need.
On Tue, Apr 8, 2014 at 10:02 PM, love2dishtech  Hi,
Seems like you need to initialise a regex pattern for that inputformat. How
is this done? Perhaps via a config option?
In which case you need to first create a hadoop configuration, set the
appropriate config option for the regex, and pass that into
newAPIHadoopFile.
On Tue, Apr 8, 2014 at 10:36 PM, Anurag  wrote:
Likely neither will give real-time for full-graph traversal, no. And once
in memory, GraphX would definitely be faster for "breadth-first" traversal.
But for "vertex-centric" traversals (starting from a vertex and traversing
edges from there, such as "friends of friends" queries etc) then Titan is
optimized for that use case.
On Tue, Apr 8, 2014 at 10:56 PM, Evan Chan  wrote:
I think having the option of seeding the factors from HDFS rather than
random is a good one (well, actually providing additional optional
arguments initialUserFactors and initialItemFactors as RDD[(Int,
Array[Double])])
On Mon, Apr 7, 2014 at 8:09 AM, Debasish Das  Sorry not persist...I meant adding a user parameter k which does checkpoint
I am very much +1 on Sean's comment.
I think the correct abstractions and API for Vectors, Matrices and
distributed matrices (distributed row matrix etc) will, once bedded down
and battle tested in the wild, allow a whole lot of flexibility for
developers of algorithms on top of MLlib core.
This is true whether the algorithm finds itself in MLlib, MLBase, or
resides in a separate contrib project. Just like Spark core sometimes risks
becoming "trying to please everybody" by having the kitchen sink in terms
of Hadoop integration aspects or RDD operations, and thus a spark-contrib
project may make a lot of sense. So too could ml-contrib hold a lot of
algorithms that are not core but still of wide interest. This can include,
for example, models that are still cutting edge and perhaps not as widely
used in production yet, or specialist models that are of interest to a more
niche group.
scikit-learn is very tough about this, requiring a very high bar for
including a new algorithm (many citations, dev support, proof of strong
performance and wide demand). And this leads to a very high quality code
base in general.
I'd say we should (if it hasn't been done already, I may have missed such a
discussion), decide precisely what does constitute MLlib's "1.0.0" goals
for algorithms. I'd say what we have in terms of clustering (K-Means||),
linear models, decision trees and collaborative filtering is pretty much a
good goal. Potentially the Random Forest implementation on top of the DT,
and perhaps another form of recommendation model (such as the co-occurrence
models cf. Mahout's) could be potential candidates for inclusion. I'd also
say any other optimization methods/procedures in addition to SGD and LBFGS
that are very strong and widely used for a variety of (distributed) ML
problems, could be candidates. And finally things like useful utils,
cross-validation and evaluation methods, etc.
So I'd say by all means, please work on a new model such as DBSCAN. Put it
in a new GitHub project, post some detailed performance comparisons vs
MLlib K-Means, and then in future if it gets included in MLlib core it's a
pretty easy to do.
On Mon, Apr 21, 2014 at 6:07 PM, Evan R. Sparks  While DBSCAN and others would be welcome contributions, I couldn't agree
I'd say a section in the "how to contribute" page would be a good place to put this.
In general I'd say that the criteria for inclusion of an algorithm is it should be high quality, widely known, used and accepted (citations and concrete use cases as examples of this), scalable and parallelizable, well documented and with reasonable expectation of dev support
Sent from my iPhone
+1
Built and tested locally on Mac OS X
Built and tested on AWS Ubuntu, with and without Hive support
Ran production jobs including MLlib and SparkSQL/HiveContext successfully
On Wed, May 28, 2014 at 1:09 AM, Holden Karau  wrote:
Seems Twitter has made a bit of progress here:
https://github.com/twitter/summingbird/tree/develop/summingbird-spark
May be of interest and perhaps some devs with experience in both may be
able to help out.
N
We've built a model server internally, based on Scalatra and Akka
Clustering. Our use case is more geared towards serving possibly thousands
of smaller models.
It's actually very basic, just reads models from S3 as strings (!!) (uses
HDFS FileSystem so can read from local, HDFS, S3) and uses Breeze for
linear algebra. (Technically it is also not dependent on Spark, it could be
reading models generated by any computation layer).
It's designed to allow scaling via cluster sharding, by adding nodes (but
could also support a load-balanced approach). Not using persistent actors
as doing a model reload on node failure is not a disaster as we have
multiple levels of fallback.
Currently it is a bit specific to our setup (and only focused on
recommendation models for now), but could with some work be made generic.
I'm certainly considering if we can find the time to make it a releasable
project.
One major difference to Oryx is that it only handles the model loading and
vector computations, not the filtering-related and other things that come
as part of a recommender system (that is done elsewhere in our system). It
also does not handle the ingesting of data at all.
On Sun, Oct 19, 2014 at 7:10 AM, Sean Owen  wrote:
Well, when I started development ~2 years ago, Scalatra just appealed more,
being more lightweight (I didn't need MVC just barebones REST endpoints),
and I still find its API / DSL much nicer to work with. Also, the swagger
API docs integration was important to me. So it's more familiarity than any
other reason.
If I were to build a model server from scratch perhaps Spray/Akka HTTP
would be the better way to go purely for integration purposes.
Having said that I think Scalatra is great and performant, so it's not a
no-brainer either way.
On Sun, Oct 19, 2014 at 5:29 PM, Debasish Das  Hi Nick,
The shared-nothing load-balanced server architecture works for all but the
most massive models - and even then a few big EC2 r3 instances should do
the trick.
One nice thing about Akka (and especially the new HTTP) is fault tolerance,
recovery and potential for persistence.
For us arguably the sharding is somewhat overkill initially, but does allow
easy scaling in future where conceivably all models may not fit into single
machine memory.
On Sun, Oct 19, 2014 at 5:46 PM, Sean Owen  wrote:
Looking at
https://github.com/apache/spark/blob/814a9cd7fabebf2a06f7e2e5d46b6a2b28b917c2/mllib/src/main/scala/org/apache/spark/mllib/evaluation/RankingMetrics.scala#L82
For each user in test set, you generate an Array of top K predicted item
ids (Int or String probably), and an Array of ground truth item ids (the
known rated or liked items in the test set for that user), and pass that to
precisionAt(k) to compute MAP@k (Actually this method name is a bit
misleading - it should be meanAveragePrecisionAt where the other method
there is without a cutoff at k. However, both compute MAP).
The challenge at scale is actually computing all the top Ks for each user,
as it requires broadcasting all the item factors (unless there is a smarter
way?)
I wonder if it is possible to extend the DIMSUM idea to computing top K
matrix multiply between the user and item factor matrices, as opposed to
all-pairs similarity of one matrix?
On Thu, Oct 30, 2014 at 5:28 AM, Debasish Das  Is there an example of how to use RankingMetrics ?
Congrats and welcome Sean, Joseph and Cheng!
On Wed, Feb 4, 2015 at 2:10 PM, Sean Owen  wrote:
Imran, on your point to read multiple files together in a partition, is it
not simpler to use the approach of copy Hadoop conf and set per-RDD
settings for min split to control the input size per partition, together
with something like CombineFileInputFormat?
On Tue, Mar 24, 2015 at 5:28 PM, Imran Rashid  wrote:
Hey Spark devs
I've been looking at DF UDFs and UDAFs. The approx distinct is using
hyperloglog,
but there is only an option to return the count as a Long.
It can be useful to be able to return and store the actual data structure
(ie serialized HLL). This effectively allows one to do aggregation /
rollups over columns while still preserving the ability to get distinct
counts.
For example, one can store daily aggregates of events, grouped by various
columns, while storing for each grouping the HLL of say unique users. So
you can get the uniques per day directly but could also very easily do
arbitrary aggregates (say monthly, annually) and still be able to get a
unique count for that period by merging the daily HLLS.
I did this a while back as a Hive UDAF (https://github.com/MLnick/hive-udf)
which returns a Struct field containing a "cardinality" field and a
"binary" field containing the serialized HLL.
I was wondering if there would be interest in something like this? I am not
so clear on how UDTs work with regards to SerDe - so could one adapt the
HyperLogLogUDT to be a Struct with the serialized HLL as a field as well as
count as a field? Then I assume this would automatically play nicely with
DataFrame I/O etc. The gotcha is one needs to then call
"approx_count_field.count" (or is there a concept of a "default field" for
a Struct?).
Also, being able to provide the bitsize parameter may be useful...
The same thinking would apply potentially to other approximate (and
mergeable) data structures like T-Digest and maybe CMS.
Nick
Cloudera's Kudu also looks interesting here (getkudu.io) - Hadoop
input/output format support:
https://github.com/cloudera/kudu/blob/master/java/kudu-mapreduce/src/main/java/org/kududb/mapreduce/KuduTableInputFormat.java
On Mon, Nov 16, 2015 at 7:52 AM, Reynold Xin  wrote:
Hi Spark users & devs
I was just wondering if anyone out there has interest in DynamoDB Streams (
http://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html)
as an input source for Spark Streaming Kinesis?
Because DynamoDB Streams provides an adaptor client that works with the
KCL, making this work is fairly straightforward, but would require a little
bit of work to add it to Spark Streaming Kinesis as an option. It also
requires updating the AWS SDK version.
For those using AWS heavily, there are other ways of achieving the same
outcome indirectly, the easiest of which I've found so far is using AWS
Lambdas to read from the DynamoDB Stream, (optionally) transform the
events, and write to a Kinesis stream, allowing one to just use the
existing Spark integration. Still, I'd like to know if there is sufficient
interest or demand for this among the user base to work on a PR adding
DynamoDB Streams support to Spark.
(At the same time, the implementation details happen to provide an
opportunity to address https://issues.apache.org/jira/browse/SPARK-10969,
though not sure how much need there is for that either?)
N
I'd also like to get Wiki write access - at the least it allows a few of us
to amend the "Powered By" and similar pages when those requests come
through (Sean has been doing a lot of that recently :)
On Mon, Jan 11, 2016 at 11:01 PM, Sean Owen  wrote:
I haven't come across anything, but could you provide more detail on what
issues you're encountering?
On Fri, Jan 15, 2016 at 11:09 AM, Pete Robbins  wrote:
Hi there
Sounds like a fun project :)
I'd recommend getting familiar with the existing k-means implementation as well as bisecting k-means in Spark, and then implementing yours based off that. You should focus on using the new ML pipelines API, and release it as a package on spark-packages.org.
If it got lots of use cases from there, it could be considered for inclusion in ML core in the future.
Good luck!
Sent from my iPhone
---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
Hi Maciej
Yes, that *train* method is intended to be public, but it is marked as
*DeveloperApi*, which means that backward compatibility is not necessarily
guaranteed, and that method may change. Having said that, even APIs marked
as DeveloperApi do tend to be relatively stable.
As the comment mentions:
 * :: DeveloperApi ::
 * An implementation of ALS that supports *generic ID types*, specialized
for Int and Long. This is
 * exposed as a developer API for users who do need other ID types. But it
is not recommended
 * because it increases the shuffle size and memory requirement during
training.
This *train* method is intended for the use case where user and item ids
are not the default Int (e.g. String). As you can see it returns the factor
RDDs directly, as opposed to an ALSModel instance, so overall it is a
little less user-friendly.
The *Float* ratings are to save space and make ALS more efficient overall.
That will not change in 2.0+ (especially since the precision of ratings is
not very important).
Hope that helps.
On Tue, 8 Mar 2016 at 08:20 Maciej Szymkiewicz  Can I ask for a clarifications regarding ml.recommendation.ALS:
Could you provide more details about:
1. Data set size (# ratings, # users and # products)
2. Spark cluster set up and version
Thanks
On Fri, 11 Mar 2016 at 05:53 Deepak Gopalakrishnan  wrote:
Hmmm, something else is going on there. What data source are you reading
from? How much driver and executor memory have you provided to Spark?
On Fri, 11 Mar 2016 at 09:21 Deepak Gopalakrishnan  wrote:
Also adding dev list in case anyone else has ideas / views.
On Sat, 12 Mar 2016 at 12:52, Nick Pentreath  Thanks for the feedback.
No, I didn't yet - feel free to create a JIRA.
On Thu, 17 Mar 2016 at 22:55 Daniel Siegmann  Hi Nick,
Hi there,
In writing some tests for a PR I'm working on, with a more complex array
type in a DF, I ran into this issue (running off latest master).
Any thoughts?
*// create DF with a column of Array[(Int, Double)]*
val df = sc.parallelize(Seq(
(0, Array((1, 6.0), (1, 4.0))),
(1, Array((1, 3.0), (2, 1.0))),
(2, Array((3, 3.0), (4, 6.0))))
).toDF("id", "predictions")
*// extract the field from the Row, and use map to extract first element of
tuple*
*// the type of RDD appears correct*
scala> df.rdd.map { row => row.getSeq[(Int, Double)](1).map(_._1) }
res14: org.apache.spark.rdd.RDD[Seq[Int]] = MapPartitionsRDD[32] at map at
:27
*// however, calling collect on the same expression throws
ClassCastException*
scala> df.rdd.map { row => row.getSeq[(Int, Double)](1).map(_._1) }.collect
16/04/06 13:02:49 ERROR Executor: Exception in task 5.0 in stage 10.0 (TID
74)
java.lang.ClassCastException:
org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema cannot be
cast to scala.Tuple2
at
$line54.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1$$anonfun$apply$1.apply(:27)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
at
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)
at scala.collection.AbstractTraversable.map(Traversable.scala:104)
at
$line54.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$anonfun$1.apply(:27)
at scala.collection.Iterator$$anon$11.next(Iterator.scala:370)
at scala.collection.Iterator$class.foreach(Iterator.scala:742)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1194)
at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)
at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)
at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)
at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:308)
at scala.collection.AbstractIterator.to(Iterator.scala:1194)
at
scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:300)
at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1194)
at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:287)
at scala.collection.AbstractIterator.toArray(Iterator.scala:1194)
at
org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:880)
at
org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:880)
*// can collect the extracted field*
*// again, return type appears correct*
scala> df.rdd.map { row => row.getSeq[(Int, Double)](1) }.collect
res23: Array[Seq[(Int, Double)]] = Array(WrappedArray([1,6.0], [1,4.0]),
WrappedArray([1,3.0], [2,1.0]), WrappedArray([3,3.0], [4,6.0]))
*// trying to apply map to extract first element of tuple fails*
scala> df.rdd.map { row => row.getSeq[(Int, Double)](1)
}.collect.map(_.map(_._1))
java.lang.ClassCastException:
org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema cannot be
cast to scala.Tuple2
  at $anonfun$2$$anonfun$apply$1.apply(:27)
  at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
  at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
  at
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)
  at $anonfun$2.apply(:27)
  at $anonfun$2.apply(:27)
  at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
  at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)
  at
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)
  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
Ah I got it - Seq[(Int, Float)] is actually represented as Seq[Row] (seq of
struct type) internally.
So a further extraction is required, e.g. row => row.getSeq[Row](1).map { r
=> r.getInt(0) }
On Wed, 6 Apr 2016 at 13:35 Nick Pentreath  wrote:
Hey Spark devs
I noticed that we now have a large number of examples for ML & MLlib in the
examples project - 57 for ML and 67 for MLLIB to be precise. This is bound
to get larger as we add features (though I know there are some PRs to clean
up duplicated examples).
What do you think about organizing them into packages to match the use case
and the structure of the code base? e.g.
org.apache.spark.examples.ml.recommendation
org.apache.spark.examples.ml.feature
and so on...
Is it worth doing? The doc pages with include_example would need updating,
and the run_example script input would just need to change the package
slightly. Did I miss any potential issue?
N
You should find that the first set of fits are called on the training set,
and the resulting models evaluated on the validation set. The final best
model is then retrained on the entire dataset. This is standard practice -
usually the dataset passed to the train validation split is itself further
split into a training and test set, where the final best model is evaluated
against the test set.
On Wed, 27 Apr 2016 at 14:30, Dirceu Semighini Filho  wrote:
+1 (binding)
On Mon, 23 May 2016 at 04:19, Matei Zaharia  wrote:
I've filed https://issues.apache.org/jira/browse/SPARK-15525
For now, you would have to check out sbt-antlr4 at
https://github.com/ihji/sbt-antlr4/commit/23eab68b392681a7a09f6766850785afe8dfa53d
(since
I don't see any branches or tags in the github repo for different
versions), and sbt publishLocal to get the dependency locally.
On Wed, 25 May 2016 at 15:13 Yiannis Gkoufas  wrote:
Congratulations Yanbo and welcome
On Sat, 4 Jun 2016 at 10:17, Hortonworks  wrote:
I'm getting the following when trying to run ./dev/run-tests (not happening
on master) from the extracted source tar. Anyone else seeing this?
error: Could not access 'fc0a1475ef'
**********************************************************************
File "./dev/run-tests.py", line 69, in
__main__.identify_changed_files_from_git_commits
Failed example:
    [x.name for x in determine_modules_for_files(
identify_changed_files_from_git_commits("fc0a1475ef",
target_ref="5da21f07"))]
Exception raised:
    Traceback (most recent call last):
      File "/Users/nick/miniconda2/lib/python2.7/doctest.py", line 1315, in
__run
        compileflags, 1) in test.globs
      File "<doctest __main__.identify_changed_files_from_git_commits[0]>",
line 1, in 
        [x.name for x in determine_modules_for_files(
identify_changed_files_from_git_commits("fc0a1475ef",
target_ref="5da21f07"))]
      File "./dev/run-tests.py", line 86, in
identify_changed_files_from_git_commits
        universal_newlines=True)
      File "/Users/nick/miniconda2/lib/python2.7/subprocess.py", line 573,
in check_output
        raise CalledProcessError(retcode, cmd, output=output)
    CalledProcessError: Command '['git', 'diff', '--name-only',
'fc0a1475ef', '5da21f07']' returned non-zero exit status 1
error: Could not access '50a0496a43'
**********************************************************************
File "./dev/run-tests.py", line 71, in
__main__.identify_changed_files_from_git_commits
Failed example:
    'root' in [x.name for x in determine_modules_for_files(
 identify_changed_files_from_git_commits("50a0496a43",
target_ref="6765ef9"))]
Exception raised:
    Traceback (most recent call last):
      File "/Users/nick/miniconda2/lib/python2.7/doctest.py", line 1315, in
__run
        compileflags, 1) in test.globs
      File "<doctest __main__.identify_changed_files_from_git_commits[1]>",
line 1, in 
        'root' in [x.name for x in determine_modules_for_files(
 identify_changed_files_from_git_commits("50a0496a43",
target_ref="6765ef9"))]
      File "./dev/run-tests.py", line 86, in
identify_changed_files_from_git_commits
        universal_newlines=True)
      File "/Users/nick/miniconda2/lib/python2.7/subprocess.py", line 573,
in check_output
        raise CalledProcessError(retcode, cmd, output=output)
    CalledProcessError: Command '['git', 'diff', '--name-only',
'50a0496a43', '6765ef9']' returned non-zero exit status 1
**********************************************************************
1 items had failures:
   2 of   2 in __main__.identify_changed_files_from_git_commits
***Test Failed*** 2 failures.
On Fri, 24 Jun 2016 at 06:59 Yin Huai  wrote:
+1 I don't believe there's any reason for the warnings to still be there
except for available dev time & focus :)
On Wed, 27 Jul 2016 at 21:35, Jacek Laskowski  wrote:
Note that both HashingTF and CountVectorizer are usually used for creating
TF-IDF normalized vectors. The definition (
https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Definition) of term frequency
in TF-IDF is actually the "number of times the term occurs in the document".
So it's perhaps a bit of a misnomer, but the implementation is correct.
On Tue, 2 Aug 2016 at 05:44 Yanbo Liang  wrote:
Spark already supports compiling with Java 8. What refactoring are you
referring to, and where do you expect to see performance gains?
On Sat, 20 Aug 2016 at 12:41, Timur Shenkao  wrote:
Never actually got around to doing this - do folks still think it
worthwhile?
On Thu, 21 Apr 2016 at 00:10 Joseph Bradley  wrote:
(cc'ing dev list also)
I think a more general version of ranking metrics that allows arbitrary
relevance scores could be useful. Ranking metrics are applicable to other
settings like search or other learning-to-rank use cases, so it should be a
little more generic than pure recommender settings.
The one issue with the proposed implementation is that it is not compatible
with the existing cross-validators within a pipeline.
As I've mentioned on the linked JIRAs & PRs, one option is to create a
special set of cross-validators for recommenders, that address the issues
of (a) dataset splitting specific to recommender settings (user-based
stratified sampling, time-based etc) and (b) ranking-based evaluation.
The other option is to have the ALSModel itself capable of generating the
"ground-truth" set within the same dataframe output from "transform" (ie
predict top k) that can be fed into the cross-validator (with
RankingEvaluator) directly. That's the approach I took so far in
https://github.com/apache/spark/pull/12574.
Both options are valid and have their positives & negatives - open to
comments / suggestions.
On Tue, 20 Sep 2016 at 06:08 Jong Wook Kim  wrote:
I have a PR for it - https://github.com/apache/spark/pull/12574
Sadly I've been tied up and haven't had a chance to work further on it.
The main issue outstanding is deciding on the transform semantics as well
as performance testing.
Any comments / feedback welcome especially on transform semantics.
N
check out https://github.com/VinceShieh/Spark-AdaOptimizer
On Wed, 30 Nov 2016 at 10:52 WangJianfei  Hi devs:
Indeed, it's being tracked here:
https://issues.apache.org/jira/browse/SPARK-18230 though no Pr has been
opened yet.
On Tue, 6 Dec 2016 at 13:36 chris snow  wrote:
I went ahead and re-marked all the existing 2.1.1 fix version JIRAs (that
had gone into branch-2.1 since RC1 but before RC2) for Spark ML to 2.1.0
On Thu, 8 Dec 2016 at 09:20 Reynold Xin  wrote:
Yes most likely due to hashing tf returns ml vectors while you need mllib
vectors for row matrix.
I'd recommend using the vector conversion utils (I think in
mllib.linalg.Vectors but I'm on mobile right now so can't recall exactly).
There are until methods for converting single vectors as well as vector
rows of DF. Check the mllib user guide for 2.0 for details.
On Fri, 9 Dec 2016 at 04:42, satyajit vegesna  Hi All,
Yup - it's because almost all model data in spark ML (model coefficients)
is "small" - i.e. Non distributed.
If you look at ALS you'll see there is no repartitioning since the factor
dataframes can be large
On Fri, 13 Jan 2017 at 19:42, Sean Owen  wrote:
Hi Maciej
If you're seeing a regression from 1.6 -> 2.0 *both using DataFrames *then
that seems to point to some other underlying issue as the root cause.
Even though adding checkpointing should help, we should understand why it's
different between 1.6 and 2.0?
On Thu, 2 Feb 2017 at 08:22 Liang-Chi Hsieh  wrote:
The short answer is there is none and highly unlikely to be inside of Spark
MLlib any time in the near future.
The best bets are to look at other DL libraries - for JVM there is
Deeplearning4J and BigDL (there are others but these seem to be the most
comprehensive I have come across) - that run on Spark. Also there are
various flavours of TensorFlow / Caffe on Spark. And of course the libs
such as Torch, Keras, Tensorflow, MXNet, Caffe etc. Some of them have Java
or Scala APIs and some form of Spark integration out there in the community
(in varying states of development).
Integrations with Spark are a bit patchy currently but include the
"XOnSpark" flavours mentioned above and TensorFrames (again, there may be
others).
On Thu, 23 Feb 2017 at 14:23 n1kt0  wrote:
