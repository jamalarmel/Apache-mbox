Hi Suresh, The request here is actually to *not* use the Apache user list while in the incubator. The issue is that we don't want to ask almost 1000 people to migrate twice between two different lists in a short period of time. That seems like the most confusion of all options. If there is a requirement that eventually we get all lists over to apache, then so be it. But having a double migration is something I'd personally strongly object to and I feel that others in the community would also object to this. Let's just defer the migration of the users list until we graduate to a TLP - this is a completely sensible option and will be easy to explain to the broader user community. - Patrick --- sent from my phone Hey All, I'm working on SPARK-800 [1]. The goal is to document a best practice or recommended way of bundling and running Spark jobs. We have a quickstart guide for writing a standlone job, but it doesn't cover how to deal with packaging up your dependencies and setting the correct environment variables required to submit a full job to a cluster. This can be a confusing process for beginners - it would be good to extend the guide to cover this. First though I wanted to sample this list and see how people tend to run Spark jobs inside their org's. Knowing any of the following would be helpful: - Do you create an uber jar with all of your job (and Spark)'s recursive dependencies? - Do you try to use sbt run or maven exec with some way to pass the correct environment variables? - Do people use a modified version of spark's own `run` script? - Do you have some other way of submitting jobs? Any notes would be helpful in compiling this! https://spark-project.atlassian.net/browse/SPARK-800 For the streaming stuff, I'm fairly sure I used Guava (or I at least *want* it to be Guava) so I'm personally in full support of Guava for this. - Patrick Yep - this will *definitely* be in 0.8! In 0.7.X these are actually of the format /app?format=json. In 0.8.X they will be the format you described here "/app/json". Does that work for you? - Patrick Hey Gordon, We don't have a specification of the API right now (in 0.7.X this is a somewhat experimental feature). Each call returns a set of (k, v) pairs and the list of keys is static, so you can just do a function call in the browser to see what it returns and get the schema. The JSON protocol code is here: https://github.com/mesos/spark/blob/branch-0.7/core/src/main/scala/spark/deploy/JsonProtocol.scala For 0.8 this will be more cleanly specified. - Patrick Hey Guys, Thanks for reporting this - I've disabled CAPTCHA on our sign-up page. It looks like Atlassian broke this feature in and won't be fixing it until the next release: https://jira.atlassian.com/browse/JRA-34421. Could you let me know if you are able to sign up now? Thanks! - Patrick Hey Gary - ya we moved away from Spray. It was a bit overkill for what we needed (we just needed some very simple embedded web servers) and they kept doing major API refactorings that made it really hard to keep up-to-date. - Patrick Hey All, Matei asked me to pick this up because he's travelling this week. I cut a second release candidate from the head of the 0.8 branch (on mesos/spark gitub) to address the following issues: - RC is now hosted in an apache web space - RC now includes signature - RC now includes MD5 and SHA512 digests [tgz] http://people.apache.org/~pwendell/spark-rc/spark-0.8.0-src-incubating-RC2.tgz [all files] http://people.apache.org/~pwendell/spark-rc/ It would be great to get feedback on the release structure. I also changed the name to include "src" since we will be releasing both source and binary releases. I was a bit confused about how to attach my GPG key to the spark.asc file. I took the following steps. 1. Greated a GPG key locally 2. Distributed the key to public key servers (gpg --send-key) 3. Add exported key to my apache web space: http://people.apache.org/~pwendell/9E4FE3AF.asc 4. Added the key fingerprint at id.apage.org 5. Create an apache FOAF file with the key signature However, this doesn't seem sufficient to get my key on this page (at least, not yet): http://people.apache.org/keys/group/spark.asc Chris - are there other steps I missed? Is there a manual way to augment this file? - Patrick No these are posted primarily for the purpose of having the Apache mentors look at the bundling format, they are not likely to be the exact commit we release (though this RC was fc6fbfe7d7e9171572c898d9e90301117517e60e). Hey Evan, These are posted primarily for the purpose of having the Apache mentors look at the bundling format, they are not likely to be the exact commit we release. Matei will be merging in some doc stuff before the release, I'm pretty sure that includes your docs. - Patrick Hey Chris, Henry... do you guys have feedback here? This was based largely on your feedback in the last "round" :) Thanks Chris - also it appears that my key has now been added to this file: http://people.apache.org/keys/group/spark.asc - Patrick Matei mentioned to me that he was going to write docs for this. Matei, is that still your intention? - Patrick Henry, Thanks a lot for your feedback. Could you let me know how you ran Apache RAT tool so I can reproduce this? My sense is that the best "next step" is to do a RC that is built against the Apache Git and also includes both `src` and `bin` in addition to cleaned up license files. Some inline responses below. Yes, we'll do both src and binary releases. I'll hash, and sign both. Yes, this might be me for this release because I've got the keys correctly set-up. I'll chat with Matei when he's back. Next RC we will take care of this. See comment above. This is now finished for me :) Thanks Henry. The MLLib files have been fixed since you ran the tool. Hey Chris, The only issue with CHANGES.txt is that we've only recently become more disciplined about tracking issues in JIRA and tracking version numbers when we do make JIRA issues. If we generated a CHANGES.txt based on JIRA, it would be largely incomplete since many changes from the beginning of the release would be missing. What about if I created a CHANGES.txt based on the Git history? Would that be better than not having one at all? - Patrick Ya I amended this for pushing the releases to maven. I actually changed that later to a different URI because it was incorrect (should have been https://) Please vote on releasing the following candidate as Apache Spark (incubating) version 0.8.0. This will be the first incubator release for Spark in Apache. The tag to be voted on is v0.8.0-incubating (commit ffacd17): https://github.com/apache/incubator-spark/releases/tag/v0.8.0-incubating The release files, including signatures, digests, etc can be found at: http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc3/files/ The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-034/org/apache/spark/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc3/docs/ Please vote on releasing this package as Apache Spark 0.8.0-incubating! The vote is open until Saturday, June 13th at 23:00 UTC and passes if a majority of at least 3 +1 IPMC votes are cast. [ ] +1 Release this package as Apache Spark 0.8.0-incubating [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.incubator.apache.org/ Hey guys, we actually decided on a slightly different naming convention for the downloads. I'm going to amend the files in the next few minutes... in case anyone happens to be looking *this instant* (which I doubt) hold off until I update them. Fixed! Please vote on releasing the following candidate as Apache Spark (incubating) version 0.8.0. This will be the first incubator release for Spark in Apache. The tag to be voted on is v0.8.0-incubating (commit ffacd17): https://github.com/apache/incubator-spark/releases/tag/v0.8.0-incubating The release files, including signatures, digests, etc can be found at: http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc3/files/ The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-034/org/apache/spark/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc3/docs/ Please vote on releasing this package as Apache Spark 0.8.0-incubating! The vote is open until Saturday, June 13th at 23:00 UTC and passes if a majority of at least 3 +1 IPMC votes are cast. [ ] +1 Release this package as Apache Spark 0.8.0-incubating [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.incubator.apache.org/ I'll post another RC in a bit which addresses Mark's comments (though please continue to provide feedback on this one!). Suresh - it's signed with the following key: http://people.apache.org/~pwendell/9E4FE3AF.asc Please vote on releasing the following candidate as Apache Spark (incubating) version 0.8.0. This will be the first incubator release for Spark in Apache. The tag to be voted on is v0.8.0-incubating (commit 32fc250): https://github.com/apache/incubator-spark/releases/tag/v0.8.0-incubating The release files, including signatures, digests, etc can be found at: http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc4/files/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-046/org/apache/spark/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc4/docs/ Please vote on releasing this package as Apache Spark 0.8.0-incubating! The vote is open until Tuesday, September 17th at 10:00 UTC and passes if a majority of at least 3 +1 IPMC votes are cast. [ ] +1 Release this package as Apache Spark 0.8.0-incubating [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.incubator.apache.org/ Hey Mark, Could you describe a user whose behavior is changed by this, and how it is changed? This commit actually brings 0.8 in line with the 0.7 and 0.6 branches, where the scala version is hard coded in the released artifacts: http://repo1.maven.org/maven2/org/spark-project/spark-streaming_2.9.3/0.7.3/spark-streaming_2.9.3-0.7.3.pom That seems to me to minimize the changes in user behavior as much as possible. It would be bad if during the 0.8 release the format of our released artifacts changed in a way that caused things to break for users. One example of something that could break is an IDE or some other tool that consumes these builds downstream and isn't aware of scala versioning. We can have a more intricate solution on the master branch if you'd like. This is just a fix to bring the 0.8 branch into line with our existing releases (and since 0.8 only supports scala 2.9.3 anyways, I'm still not sure how this could affect any users adversely). - Patrick Hey Mark, Thanks for providing the detailed explanation. My primary concern was just that this changes the published artifacts in a way that could break downstream consumers of these poms which may assume that artifact id's are immutable within a pom.xml file. For now, let me revert my change and test that a few important things still work (e.g. IDE's, etc). At a minimum I just want to make sure things we are advising people to do don't break under this release. If this doesn't break those things we can  move forward with the parameterized artifacts for 0.8.0. Just a word of caution though, there may be other downstream consumers of the pom files for whom this will cause a problem in the future. If someone presents a compelling reason, we'll have to think about whether we can keep publishing them like this, since this is not technically a valid maven format. - Patrick So Mark does that mean you'd be OK with us hard coding the scala version in branch 0.8.0 build? It just seems like the overall simplest solution for now. Or would this cause a large problem for you guys? We can solve this on master for 0.9, I didn't touch master at all wrt the maven build. - Patrick Hey Mark, OK - I will cut an RC then with the hard-coded versions. I am 100% in support of a better solution to this which we can discuss and add to the master branch. We could also port that solution to the 0.8 branch for the next branch 0.8 release. I do feel that for this particular release it's just less risky to hard code the versions. As Jey mentioned, there could be some unintended consequences of this downstream and I'd prefer to fully explore those before we change the release format. - Patrick Please vote on releasing the following candidate as Apache Spark (incubating) version 0.8.0. This will be the first incubator release for Spark in Apache. The tag to be voted on is v0.8.0-incubating (commit d9e80d5): https://github.com/apache/incubator-spark/releases/tag/v0.8.0-incubating The release files, including signatures, digests, etc can be found at: http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc5/files/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-051/org/apache/spark/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc5/docs/ Please vote on releasing this package as Apache Spark 0.8.0-incubating! The vote is open until Thursday, September 19th at 05:00 UTC and passes if a majority of at least 3 +1 IPMC votes are cast. [ ] +1 Release this package as Apache Spark 0.8.0-incubating [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.incubator.apache.org/ Yes, we've moved onto RC5, thanks. I also wrote an audit script [1] to verify various aspects of the release binaries and ran it on this RC. People are welcome to run this themselves, but I haven't tested it on other machines yet, and some of the Spark tests are very sensitive to the test environment :) Output is pasted below: [1] https://github.com/pwendell/spark-utils/blob/master/release_auditor.py ----------------------------------------------------- ==== Verifying download integrity for artifact: spark-0.8.0-incubating-bin-cdh4-rc5.tgz ==== [PASSED] Artifact signature verified. [PASSED] Artifact MD5 verified. [PASSED] Artifact SHA verified. [PASSED] Tarball contains CHANGES.txt file [PASSED] Tarball contains NOTICE file [PASSED] Tarball contains LICENSE file [PASSED] README file contains disclaimer ==== Verifying download integrity for artifact: spark-0.8.0-incubating-bin-hadoop1-rc5.tgz ==== [PASSED] Artifact signature verified. [PASSED] Artifact MD5 verified. [PASSED] Artifact SHA verified. [PASSED] Tarball contains CHANGES.txt file [PASSED] Tarball contains NOTICE file [PASSED] Tarball contains LICENSE file [PASSED] README file contains disclaimer ==== Verifying download integrity for artifact: spark-0.8.0-incubating-rc5.tgz ==== [PASSED] Artifact signature verified. [PASSED] Artifact MD5 verified. [PASSED] Artifact SHA verified. [PASSED] Tarball contains CHANGES.txt file [PASSED] Tarball contains NOTICE file [PASSED] Tarball contains LICENSE file [PASSED] README file contains disclaimer ==== Verifying build and tests for artifact: spark-0.8.0-incubating-bin-cdh4-rc5.tgz ==== ==> Running build [PASSED] sbt build successful [PASSED] Maven build successful ==> Performing unit tests [PASSED] Tests successful ==== Verifying build and tests for artifact: spark-0.8.0-incubating-bin-hadoop1-rc5.tgz ==== ==> Running build [PASSED] sbt build successful [PASSED] Maven build successful ==> Performing unit tests [PASSED] Tests successful ==== Verifying build and tests for artifact: spark-0.8.0-incubating-rc5.tgz ==== ==> Running build [PASSED] sbt build successful [PASSED] Maven build successful ==> Performing unit tests [PASSED] Tests successful - Patrick Hey folks, just FYI we found one minor issue with this RC (the kafka jar in the stream pom needs to be published as "provided" since it's not available in maven). Please still continue to test this and provide feedback here until the following RC is posted later. - Patrick Please vote on releasing the following candidate as Apache Spark (incubating) version 0.8.0. This will be the first incubator release for Spark in Apache. The tag to be voted on is v0.8.0-incubating (commit 3b85a85): https://github.com/apache/incubator-spark/releases/tag/v0.8.0-incubating The release files, including signatures, digests, etc can be found at: http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc6/files/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-059/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-0.8.0-incubating-rc6/docs/ Please vote on releasing this package as Apache Spark 0.8.0-incubating! The vote is open until Friday, September 20th at 09:00 UTC and passes if a majority of at least 3 +1 IPMC votes are cast. [ ] +1 Release this package as Apache Spark 0.8.0-incubating [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.incubator.apache.org/ Thanks for the feedback guys. I've changed the audit script to fix Andy's suggestion. I also added tests for building sbt and maven projects against the staged repository to test that artifacts are setup correctly in maven. I've posted RC6 which adds a very small change to this RC. This vote is therefore cancelled in favor of RC6. - Patrick Also the URL for the release audits changed slightly: https://github.com/pwendell/spark-utils/blob/master/release-audits/release_auditor.py Hey Mark, Good catches here. Ya the driver suite thing is sorta annoying - we should try to fix that in master. The audit script I wrote first does an sbt/sbt assembly to avoid this. I agree though these shouldn't block the release (if a blocker does come up we can revisit these potentially when cutting a release). - Patrick I'm a +1 as well. FYI this vote ends in 8 hours. Hey Chris the tag in github is 3b85a85, which I listed in the original vote next to the git URL. Is there another type of tag I should be adding? Ah I see - sounds good. For future releases we can use that URL to describe the tag. Hey Roman, We can do this in the future - I wasn't sure exactly what the right standard approach was. Just so I understand, the change you are proposing from what is there now is just to remove rcX from the file-names, correct? - Patrick Henry - one thing is that, because the filenames are not included in the signatures, I could just alter the filenames now to not include -RCX... would that be preferable or would that necessitate another vote? - Patrick The vote is now closed. Below are the vote totals. +1 (7 Total) Andy Konwinski Matei Zaharia Patrick Wendell Konstantin Boudnik Reynold Xin Chris Mattmann* Henry Saputra* 0 (1 Total) Mark Hamstra -1 (0 Total) * = Binding Vote As per the incubator release guide [1] I'll be sending this to the general incubator list for a final vote from IPMC members. [1] http://incubator.apache.org/guides/releasemanagement.html#best-practice-incubator-release-vote - Patrick Hey Henry, Sounds good. I'll send an email to general@ shortly. I didn't realize that this vote technically counts as passing according to those rules (since plenty of PPMC gave +1). Thanks everyone who contributed to this vote. I've created an IPMC vote here: http://mail-archives.apache.org/mod_mbox/incubator-general/201309.mbox/%3CCABPQxsu6XwYMUxWKwRsXOvT%2B3-8%3DTTWwyHwJe_hWVb%3DNPxWuuw%40mail.gmail.com%3E - Patrick Hey we've actually distributed our artifacts through amazon cloudfront in the past (and that is where the website links redirect to). Since the apache mirrors don't distribute signatures anyways, what is the difference between linking to an apache mirror vs using a more robust CDN? If people want to verify the downloads they need to go to the apache root in either case. Is this just a cultural thing or is there some security reason? - Patrick Yep, we definitely need to just directly point people the location at apache.org where they can find the hashes. I just updated the release notes and downloads page to point to that site. I just wanted to point out that mirroring these through a CDN seems philosophically the same as mirroring through Apache, since in neither case do we expect the users to trust the artifact they download. We just need to be more explicit that we are, indeed, mirroring and explain that the trusted root is at apache.org - Patrick Chris et al, I'm -1 on this because it has many negative consequences for our existing users: 1. Users who do automated downloads based on our posted URL's (of which we get many thousands each release) will no longer work. Now if they do "wget XXX" with our posted link, it will fail in a weird way to due to the redirect page. Is there a version of the closer.cgi script which just performs 302 redirects instead of asking me to click on a link? 2. All other users have to click through an additional page to download the software. 3. Amazon Cloudfront is, as a whole, much more reliable and higher bandwidth than the mirror network. These are my concerns, that basically we're causing our users to have a much worse experience. I've identified these concerns with moving to the apache mirror, but perhaps I've overlooked some benefits that would counteract these. Are there benefits? I completely agree that we need to send users to the signatures and hashes at the Apache release site (to verify the release). So I did add the link to this directly adjacent to the download. - Patrick Hey Folks, I updated the site to include the Apache mirror list for each file. I actually put it is the first download location, before the direct download link. I played around with the mirror network a bit, the performance was not bad, based on sampling from a few vantage points. I found it to be always worse then CloudFront, but typically not *much* worse. So I actually think if we can find a way to have a direct link to the nearest Apache mirror, we could just remove the CloudFront link entirely. I looked into it and apparently we're not the first apache project to have this problem. Many of the bigger projects already use some fancy selection to embed a direct link to the closest mirror: http://httpd.apache.org/download.cgi - Patrick I think Ruby integration via JRuby would be a great idea. +1 - josh this is awesome. As a starting point, a version where people just write their own "wrapper"functions to convert various HadoopFiles into String <K, V> files could go a long way. We could even have a few built-in versions, such as dealing with Sequence files that are <String, String>. Basically, the user needs to write a translator in Java/Scala that produces textual records from whatever format that want. Then, they make sure this is included in the classpath when running PySpark. As Josh is saying, I'm pretty sure this is already possible, but we may want to document it for users. In many organizations they might have 1-2 people who can write the Java/Scala to do this but then many more people who are comfortable using python once it's setup. - Patrick Shark is not a great example in general because it uses semi-private internal interfaces that are not guaranteed to be compatible within minor releases. Spark's public, documented API has always (AFAIK) maintained compatibility within minor versions. In fact, we've been diligent to maintain compatibility with major versions as well and there have only been very minute changes in that API. Over time it would be good for Shark to migrate to using higher API's (and we may need to build these). But my point is that the public API has maintained compatibility consistent with the norms discussed here. - Patrick This may have been caused by a recent merge since a bunch of people independently hit it in the last 48 hours. One debugging step would be to narrow it down to which merge caused it. I don't have time personally today, but just a suggestion for ppl for whom this is blocking progress. - Patrick Hey Henry, I did create release notes for this. However, I wanted to "dogfood"them for the 0.8.1 release before I push them publicly, just so I know the thing is actually comprehensive. It's quite complicated and I don't want to publish something that leads people down the wrong path. My thought was I would use these personally for the 0.8.1 release to verify them, then publish them and try to have someone else do the 0.9.0 release (perhaps wishful thinking!). - Patrick Hey Umar, I dug into this a bit today out of curiosity since I also wasn't sure. I updated the in-line documentation here: https://github.com/apache/incubator-spark/pull/209/files The more important metric is `fetchWaitTime` which indicates how much of the task runtime was spent waiting for input data. remoteFetchTime is an aggregation of all of the fetch delays for each block... this second metric is a bit more convoluted because those fetches can actually overlap, so if this is high it doesn't necessarily indicate any latency hit. - Patrick Please vote on releasing the following candidate as Apache Spark (incubating) version 0.8.1. The tag to be voted on is v0.8.1-incubating (commit fba8738): https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=tag;h=720e75581ae5f0c4835513ee06bfa0cb71923c57 The release files, including signatures, digests, etc can be found at: http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc1/ - or - https://dist.apache.org/repos/dist/dev/incubator/spark/spark-0.8.1-incubating-rc1/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-022/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc1-docs/ Please vote on releasing this package as Apache Spark 0.8.1-incubating! The vote is open until Tuesday, December 9th at 21:30 UTC and passes if a majority of at least 3 +1 PPMC votes are cast. [ ] +1 Release this package as Apache Spark 0.8.1-incubating [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.incubator.apache.org/ Hey Guys, Matei found a few small doc fixes so I'm going to cut a new RC today. I'll include the release credits and summary in that e-mail so people know what they are voting in. - Patrick Hey Henry, Are you suggesting we need to change something about or changes file? Or are you just pointing people to the file? - Patrick Please vote on releasing the following candidate as Apache Spark (incubating) version 0.8.1. The tag to be voted on is v0.8.1-incubating (commit bf23794a): https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=tag;h=e6ba91b5a7527316202797fc3dce469ff86cf203 The release files, including signatures, digests, etc can be found at: http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc2/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-024/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc2-docs/ For information about the contents of this release see: draft of release notes draft of release credits https://github.com/apache/incubator-spark/blob/branch-0.8/CHANGES.txt Please vote on releasing this package as Apache Spark 0.8.1-incubating! The vote is open until Wednesday, December 11th at 21:00 UTC and passes if a majority of at least 3 +1 PPMC votes are cast. [ ] +1 Release this package as Apache Spark 0.8.1-incubating [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.incubator.apache.org/ I'm cancelling this vote in favor of a vote on rc2. rc2 includes some documentation clean-up and changes on top of rc1. I also added (in that thread) more color on the general contents of this release. 0.8.1 is a maintenance release, but because the 0.8-branch will be the last branch to support scala 2.9, we've elected to add some larger features and optimizations that otherwise might not have made the cut. The larger changes (H/A mode for the scheduler and shuffle file consolidation) are not enabled by default to avoid major changes for people upgrading from 0.8.0. - Patrick Hey Mark, One constructive action you and other people can take to help us assess the quality and completeness of this release is to download the release, run the tests, run the release in your dev environment, read through the documentation, etc. This is one of the main points of releasing an RC to the community... even if you disagree with some patches that were merged in, this is still a way you can help validate the release. - Patrick Hey Take, Could you start a separate thread to debug your build issue? In that thread, could you paste the exact build command and entire output? The log you posted here suggests the first build detected hadoop 1.0.4 not 2.2.0 based on the assembly file name it is logging. --- sent from my phone Hey Taka, Most likely this is just the assembly task taking a long time as mark said. What happens if you run 'package' or 'compile'? It could be that on Mac's there is something where this is slow... I'm not sure. Also, unless you specify the hadoop version in the build it will build Hadoop 1.0.4. You keep mentioning Hadoop 2.2.0, but you didn't specify that when you built so Spark has no way of knowing what you want. Checkout the README for documentation on how to do that. Also - does this all work well in the 0.8.0 release? If this is not something specific to the 0.8.1 release than it would be good to know. - Patrick For my own part I'll give a +1 to this RC. Hey Mark - ya this would be good to get in. Does merging that particular PR put this in sufficient shape for the 0.8.1 release or are there other open patches we need to look at? - Patrick Looked into this a bit more - I think removing repl-bin is something we should wait until 0.9 to do, because we've published it to maven in 0.8.0 and people might expect it to be there in 0.8.1. Merging the directly referenced pull request (195) seems like a good idea though since it fixes a bug in the script. Is that what you are suggesting? - Patrick Hey Mark, What I'm asking is whether this patch is sufficient to have a working debian build in 0.8.1, or are there other outstanding issues to make it work? By working I mean, within the initial design that was contributed (with repl-bin) it works according to that approach. We can redesign this packaging in 0.9. That will require having a PR against Apache Spark, discussing, etc. But it doesn't need to be on the critical path for this release. - Patrick Hey Mark, Okay if 195 gets this in working order in the branch 0.8 let's just merge that to keep it consistent with our docs and the way this is done in 0.8.0 We can do a broader refactoring in 0.9. Would be great if you could kick off a JIRA discussion or submit a PR relating to that. - Patrick Okay Mark thanks for bringing this up, I'm going to cut a new RC with this fix. Please vote on releasing the following candidate as Apache Spark (incubating) version 0.8.1. The tag to be voted on is v0.8.1-incubating (commit c88a9916): https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=tag;h=ba05afd29c81e152a84461f95b0e61a783897d7a The release files, including signatures, digests, etc can be found at: http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc3/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-025/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc3-docs/ For information about the contents of this release see: draft of release notes draft of release credits https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=blob;f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branch-0.8 Please vote on releasing this package as Apache Spark 0.8.1-incubating! The vote is open until Thursday, December 12th at 06:30 UTC and passes if a majority of at least 3 +1 PPMC votes are cast. [ ] +1 Release this package as Apache Spark 0.8.1-incubating [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.incubator.apache.org/ I'm cancelling this vote in favor of RC3. I'll go ahead and kick this off with a +1. I'm going to -1 this now because we had two issues reported today. They were reported off the list so I'm summarizing here: (1) Raymond Liu found an issue with the Maven build for YARN 2.2+. Previously we had only tested the sbt build since this is what we refer to in the docs, but we'd like to support this for Maven as well. (2) I noticed we were missing some header files from recent patches. This will result in a -1 downstream during an IPMC release, so we should fix it. - Patrick Please vote on releasing the following candidate as Apache Spark (incubating) version 0.8.1. The tag to be voted on is v0.8.1-incubating (commit b87d31d): https://git-wip-us.apache.org/repos/asf/incubator-spark/repo?p=incubator-spark.git;a=commit;h=b87d31dd8eb4b4e47c0138e9242d0dd6922c8c4e The release files, including signatures, digests, etc can be found at: http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc4/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-040/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-0.8.1-incubating-rc4-docs/ For information about the contents of this release see: https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=blob;f=CHANGES.txt;h=ce0aeab524505b63c7999e0371157ac2def6fe1c;hb=branch-0.8 Please vote on releasing this package as Apache Spark 0.8.1-incubating! The vote is open until Saturday, December 14th at 01:00 UTC and passes if a majority of at least 3 +1 PPMC votes are cast. [ ] +1 Release this package as Apache Spark 0.8.1-incubating [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.incubator.apache.org/ Hey Raymond, This won't work because AFAIK akka 2.3-M1 is not binary compatible with akka 2.2.3 (right?). For all of the non-yarn 2.2 versions we need to still use the older protobuf library, so we'd need to support both. I'd also be concerned about having a reference to a non-released version of akka. Akka is the source of our hardest-to-find bugs and simultaneously trying to support 2.2.3 and 2.3-M1 is a bit daunting. Of course, if you are building off of master you can maintain a fork that uses this. - Patrick Also - the code is still there because of a recent merge that took in some newer changes... we'll be removing it for the final merge. Hey Reymond, Let's move this discussion out of this thread and into the associated JIRA. I'll write up our current approach over there. https://spark-project.atlassian.net/browse/SPARK-995 - Patrick Alright I just merged this in - so Spark is officially "Scala 2.10"from here forward. For reference I cut a new branch called scala-2.9 with the commit immediately prior to the merge: https://git-wip-us.apache.org/repos/asf/incubator-spark/repo?p=incubator-spark.git;a=shortlog;h=refs/heads/scala-2.9 - Patrick You can checkout the docs mentioned in the vote thread. There is also a pre-build binary for hadoop2 that is compiled for YARN 2.2 - Patrick The vote is now closed. This vote passes with 4 IPMC +1's and no 0 or -1 votes. +1 (4 Total) Marvin Humphrey Henry Saputra Chris Mattmann Roman Shaposhnik 0 (0 Total) -1 (0 Total) * = Binding Vote Thanks to everyone who helped vet this release. - Patrick Hi everyone, We've just posted Spark 0.8.1, a new maintenance release that contains some bug fixes and improvements to the 0.8 branch. The full release notes are available at [1]. Apart from various bug fixes, 0.8.1 includes support for YARN 2.2, a high availability mode for the standalone scheduler, and optimizations to the shuffle. We recommend that current users update to this release. You can grab the release at [2]. [1] http://spark.incubator.apache.org/releases/spark-release-0-8-1.html [2] http://spark.incubator.apache.org/downloads Thanks to the following people who contributed to this release: Michael Armbrust, Pierre Borckmans, Evan Chan, Ewen Cheslack, Mosharaf Chowdhury, Frank Dai, Aaron Davidson, Tathagata Das, Ankur Dave, Harvey Feng, Ali Ghodsi, Thomas Graves, Li Guoqiang, Stephen Haberman, Haidar Hadi, Nathan Howell, Holden Karau, Du Li, Raymond Liu, Xi Liu, David McCauley, Michael (wannabeast), Fabrizio Milo, Mridul Muralidharan, Sundeep Narravula, Kay Ousterhout, Nick Pentreath, Imran Rashid, Ahir Reddy, Josh Rosen, Henry Saputra, Jerry Shao, Mingfei Shi, Andre Schumacher, Karthik Tunga, Patrick Wendell, Neal Wiggins, Andrew Xia, Reynold Xin, Matei Zaharia, and Wu Zeming - Patrick Andy and Mike, I'd also prefer to just convert the old groups into mirrors. That way people who are still subscribed to them will continue to get e-mails (and most people on the list are read-only users). Ideally we'd have the behavior that users who try to e-mail the google group get a bounce back saying "this is now a read only mirror". That said I have *no idea* of this is possible to set-up nicely within google groups. I defer to Andy! Having the new mirror groups also seems like a decent solution as well... - Patrick Hey All, There is a small API change that we are considering for the external sort patch. Previously we allowed mergeCombiner to be null when map side aggregation was not enabled. This is because it wasn't necessary in that case since mappers didn't ship pre-aggregated values to reducers. Because the external sort capability also relies on the mergeCombiner function to merge partially-aggregated on-disk segments, we now need it all the time, even if map side aggregation is enabled. This is a fairly esoteric thing that I'm not sure anyone other than Shark ever used, but I want to check in case anyone had feelings about this. The relevant code is here: https://github.com/apache/incubator-spark/pull/303/files#diff-f70e97c099b5eac05c75288cb215e080R72 - Patrick Ya we've been trying to standardize on the terminology here (see glossary): http://spark.incubator.apache.org/docs/latest/cluster-overview.html I think "slave" actually isn't mentioned here at all - but references to slave in the codebase are synonymous with "worker". - Patrick Hey All, Today we merged a pull request which changes the organization of the scripts use to run Spark. The change is fairly minor but it will affect anyone who packages spark or uses spark's binary scripts. Spark's scripts are now divided into a /bin and /sbin directories. This will be the layout in Spark 0.9. /bin contains user-facing scripts: /bin/spark-shell /bin/pyspark /bin/spark-class /bin/run-example /sbin contains administrative scripts for launching the standalone cluster manager: /bin/start-master.sh /bin/start-all.sh ...etc This allows administrators to set permissions differently for user and admin scripts. It is also consistent with the way many other projects package binaries. This change was contributed by @xiajunluan, @shane-huang, and @ScrapCodes. https://github.com/apache/incubator-spark/pull/317 Cheers, - Patrick --> Small correction Yes Just to give a bit more color - these exist because the yarn API changed and we'd like to support both for a while. Rather than introduce shims we just factored out the common code and maintain isolated builds for either one. - Patrick Hey All, Due to an ASF requirement, we recently merged a patch which removes the sbt jar from the build. This is necessary because we aren't allowed to distributed binary artifacts with our source packages. This means that instead of building Spark with "sbt/sbt XXX", you'll need to have sbt yourself and just run "sbt XXX" from within the Spark directory. This is similar to the maven build, where we expect users already have maven installed. You can download sbt at http://www.scala-sbt.org/. It's okay to just download the most recent version of sbt, since sbt knows how to fetch other versions of itself and will always use the one we specify in our build file to compile spark. - Patrick We thought about this but elected not to do this for a few reasons. 1. Some people build from machines that do not have internet access for security reasons and retrieve dependency from internal nexus repositories. So having a build dependency that relies on internet downloads is not desirable. 2. It's a hard to ensure stability of a particular URL in perpetuity. This is why maven central and other mirror networks exist. Keep in mind that we can't change the release code ever once we release it, and if something changed about the particular URL it could break the build. - Patrick Hey Holden, That sounds reasonable to me. Where would we get a url we can control though? Right now the project has web space is at incubator.apache... but later this will change to a full apache domain. Is there somewhere in maven central these jars are hosted... that would be the nicest because things like repo1.maven.org basically never changes. - Patrick Hey Sandy, Do you know what the status is for YARN-321 and what version of YARN it's targeted for? Also, is there any kind of documentation or API for this? Does it control the presentation of the data itself (e.g. it actually has its own UI)? @Tom - having an optional history server sounds like a good idea. One question is what format to use for storing the data and how the persisted format relates to XML/HTML generation in the live UI. One idea would be to add JSON as an intermediate format inside of the current WebUI, and then any JSON page could be persisted and rendered by the history server using the same code. Once a SparkContext exits it could dump a series of named paths each with a JSON file. Then the history server could load those paths and pass them through the second rendering stage (JSON => XML) to create each page. It would be good if SPARK-969 had a good design doc before anyone starts working on it. - Patrick I'm also very wary of using a code formatter for the reasons already mentioned by Reynold. Does scaliform have a mode where it just provides style checks rather than reformat the code? This is something we really need for, e.g., reviewing the many submissions to the project. - Patrick Pillis - I agree we need to decouple the representation from a particular history server. But why not provide as simple history server people can (optionally) run if they aren't using Yarn or Mesos? For people running the standalone cluster scheduler this seems important. Giving them only a JSON dump isn't super consumable for most users. - Patrick Please vote on releasing the following candidate as Apache Spark (incubating) version 0.9.0. A draft of the release notes along with the changes file is attached to this e-mail. The tag to be voted on is v0.9.0-incubating (commit 7348893): https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=7348893f0edd96dacce2f00970db1976266f7008 The release files, including signatures, digests, etc can be found at: http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc1/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1001/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc1-docs/ Please vote on releasing this package as Apache Spark 0.9.0-incubating! The vote is open until Sunday, January 19, at 02:00 UTC and passes if a majority of at least 3 +1 PPMC votes are cast. [ ] +1 Release this package as Apache Spark 0.9.0-incubating [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.incubator.apache.org/ Mridul, thanks a *lot* for pointing this out. This is indeed an issue and something which warrants cutting a new RC. - Patrick This vote is cancelled in favor of rc2 which I'll post shortly. Please vote on releasing the following candidate as Apache Spark (incubating) version 0.9.0. A draft of the release notes along with the changes file is attached to this e-mail. The tag to be voted on is v0.9.0-incubating (commit 00c847a): https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=00c847af1d4be2fe5fad887a57857eead1e517dc The release files, including signatures, digests, etc can be found at: http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc2/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1003/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc2-docs/ Please vote on releasing this package as Apache Spark 0.9.0-incubating! The vote is open until Wednesday, January 22, at 07:05 UTC and passes if a majority of at least 3 +1 PPMC votes are cast. [ ] +1 Release this package as Apache Spark 0.9.0-incubating [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.incubator.apache.org/ I'll kick of the voting with a +1. Hey Mridul this was patched and we cut a new release candidate. There were several different config options which had a.b and a.b.c... they should all work in the new RC. Please vote on releasing the following candidate as Apache Spark (incubating) version 0.9.0. A draft of the release notes along with the changes file is attached to this e-mail. The tag to be voted on is v0.9.0-incubating (commit a7760eff): https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=a7760eff4ea6a474cab68896a88550f63bae8b0d The release files, including signatures, digests, etc can be found at: http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc3/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1004/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc3-docs/ Please vote on releasing this package as Apache Spark 0.9.0-incubating! The vote is open until Wednesday, January 22, at 22:15 UTC and passes if a majority of at least 3 +1 PPMC votes are cast. [ ] +1 Release this package as Apache Spark 0.9.0-incubating [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.incubator.apache.org/ This vote is cancelled in favor of rc3 - which fixes the YARN issue Sandy ran into. @taka - thanks for reporting that bug. It's not enough to block this release however. Once a fix exists we can merge it into the 0.9 branch and it will be in 0.9.1 I'll add my +1 as well Attempting to attach the release notes again (I think it may have been blocked previously due to not having an extension). Eventually the notes get posted on the apache website. I attached them to this e-mail so that people can get a sense of what is in the release before they vote on it. Please vote on releasing the following candidate as Apache Spark (incubating) version 0.9.0. A draft of the release notes along with the changes file is attached to this e-mail. The tag to be voted on is v0.9.0-incubating (commit 2da4b0f): https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=tag;h=2da4b0f131dc8dda34ad44b073515fd3811a0660 The release files, including signatures, digests, etc can be found at: http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc4 Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1005/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc4-docs/ Please vote on releasing this package as Apache Spark 0.9.0-incubating! The vote is open until Friday, January 24, at 11:15 UTC and passes if a majority of at least 3 +1 PPMC votes are cast. [ ] +1 Release this package as Apache Spark 0.9.0-incubating [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.incubator.apache.org/ This vote is cancelled in favor of rc4. (Correction to the hash above). The tag to be voted on is v0.9.0-incubating (commit 0771df67): https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=0771df675363c69622404cb514bd751bc90526af Hm - it seems like the CHANGES.txt files are for some reason missing from the packaged artifacts. Please disregard this RC... I'll post a new one in the morning (too late now I'm afraid). -Patrick Okay please defer to the new thread for RC4. Turns out the artifacts were fine I'm just going to re-post this to remove clutter in this thread. Please vote on releasing the following candidate as Apache Spark (incubating) version 0.9.0. A draft of the release notes along with the changes file is attached to this e-mail. The tag to be voted on is v0.9.0-incubating (commit 0771df67): https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=0771df675363c69622404cb514bd751bc90526af The release files, including signatures, digests, etc can be found at: http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc4 Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1005/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc4-docs/ Please vote on releasing this package as Apache Spark 0.9.0-incubating! The vote is open until Friday, January 24, at 11:15 UTC and passes if a majority of at least 3 +1 PPMC votes are cast. [ ] +1 Release this package as Apache Spark 0.9.0-incubating [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.incubator.apache.org/ This isn't a new RC or a result - I just tried to make an new vote thread and the mail clients get confused and merge it in with the old thread :(Ah oops Andy I see - thought you were discussing something different. Please vote on releasing the following candidate as Apache Spark (incubating) version 0.9.0. A draft of the release notes along with the changes file is attached to this e-mail. The tag to be voted on is v0.9.0-incubating (commit 0771df67): https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=0771df675363c69622404cb514bd751bc90526af The release files, including signatures, digests, etc can be found at: http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc4 Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1005/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc4-docs/ Please vote on releasing this package as Apache Spark 0.9.0-incubating! The vote is open until Friday, January 24, at 11:15 UTC and passes if a majority of at least 3 +1 PPMC votes are cast. [ ] +1 Release this package as Apache Spark 0.9.0-incubating [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.incubator.apache.org/ I just explicitly created a new thread, so please defer to that thread. This still pertains to rc4. Thanks I'll kick it off with a +1. Hey Tom, Matei had to remove this because it turns out that there was a fairly serious bug in the Typesafe config library we use for parsing conf files [1]. There wasn't an immediate solution to this so he just removed the capability for this release and we can revisit it in the next release. http://apache-spark-developers-list.1001551.n3.nabble.com/Config-properties-broken-in-master-td208.html - Patrick Btw - to be clear this was an incompatibility between Spark's config names and constraints on names imposed by typesafe. So didn't mean to imply there was something broken in their config library. Today there was a bug reported almost simultaneously by Tom Graves and Matt Massie that was enough to warrant a new RC. I'll post RC5 momentarily and I'm also going to post a result thread for this one (which apparently helps automated scripts that consume the list) to say it was cancelled. The RC4 vote was cancelled in favor of RC5. Please vote on releasing the following candidate as Apache Spark (incubating) version 0.9.0. A draft of the release notes along with the changes file is attached to this e-mail. The tag to be voted on is v0.9.0-incubating (commit 95d28ff3): https://git-wip-us.apache.org/repos/asf?p=incubator-spark.git;a=commit;h=95d28ff3d0d20d9c583e184f9e2c5ae842d8a4d9 The release files, including signatures, digests, etc can be found at: http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc5 Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1006/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-0.9.0-incubating-rc5-docs/ Please vote on releasing this package as Apache Spark 0.9.0-incubating! The vote is open until Monday, January 27, at 07:30 UTC and passes ifa majority of at least 3 +1 PPMC votes are cast. [ ] +1 Release this package as Apache Spark 0.9.0-incubating [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.incubator.apache.org/ I'll kick of the voting with a +1. Hey Taka, If you build a second version you need to clean the existing assembly jar. The reference implementation of the tests are the ones on the U.C. Berkeley Jenkins. These are passing for Branch 0.9 for both Hadoop 1 and Hadoop 2 versions, so I'm inclined to think it's an issue with your test env or setup. https://amplab.cs.berkeley.edu/jenkins/view/Spark/ - Patrick Voting is now closed. This vote passes with 5 binding +1 votes and no 0 or -1 votes. This vote will now go to the IPMC list for a second 72-hour vote. Spark developers are encouraged to comment on the IPMC vote as well. The totals are: +1 Patrick Wendell* Hossein Falaki Reynold Xin* Andy Konwinski* Mark Hamstra* Sean McNamara* 0: (none) -1: (none) Hey Heiko, Spark 0.9 introduced a common config class for Spark applications. It also (initially) supported loading config files in the nested typesafe format, but this was removed last minute due to a bug. In 1.0 we'll probably add support for config files, though it may not support typesafe's tree-style config files because that conflicts with the naming style of several spark options (we have options where x.y and x.y.z are both named keys, and the typesafe parser doesn't allow that). - Patrick Hey Stephen, Yes this runs afoul of good practice in Maven where a given version shouldn't be re-used. As far as I understand though, it is required by the way the Apache release process works. The artifacts and repository content that get voted on need to exactly match the final release. So we can't hold a vote on a version of the code where everything says "-rcx", then we go back and change the source code and do a second push to maven with code that doesn't have an "-rcx" suffix. This would effectively change the code that is being released. I was thinking as a work around that maybe we could publish a second set of staging artifacts that are versioned with -rcX for people to test against. I think as long as we make it clear that these are not the "official artifacts" being voted on it might be okay. I'm not totally sure if this is allowed though. - Patrick I'll add my own +1. They won't work until after the release is finalized. --- sent from my phone Hi Everyone, We've just posted Spark 0.9.0, a major release with several new features and improvements. 0.9.0 is Spark's largest release ever, with contributions from 83 developers. This release expands Spark's standard libraries, introducing a new graph computation package (GraphX) and adding several new features to the machine learning and stream-processing packages. It also makes major improvements to the core engine, including external aggregations, a simplified H/A mode for long lived applications, and hardened YARN support. The full release notes are at: http://spark.incubator.apache.org/releases/spark-release-0-9-0.html You can download the release at: http://spark.incubator.apache.org/downloads Thanks to the following people who contributed to this release: Andrew Ash, Pierre Borckmans, Russell Cardullo, Evan Chan, Vadim Chekan, Lian Cheng, Ewen Cheslack-Postava, Mosharaf Chowdhury, Dan Crankshaw, Haider Haidi, Frank Dai, Tathagata Das, Ankur Dave, Henry Davidge, Aaron Davidson, Kyle Ellrott, Hossein Falaki, Harvey Feng, Ali Ghodsi, Joseph E. Gonzalez, Thomas Graves, Rong Gu, Stephen Haberman, Walker Hamilton, Mark Hamstra, Damien Hardy, Nathan Howell, Grace Huang, Shane Huang, Prabeesh K, Holden Karau, KarthikTunga, Grega Kespret, Marek Kolodziej, Jey Kottalam, Du Li, Haoyuan Li, LiGuoqiang, Raymond Liu, George Loentiev, Akihiro Matsukawa, David McCauley, Mike, Fabrizio (Misto) Milo, Mridul Muralidharan, Tor Myklebust, Sundeep Narravula, Binh Nguyen, Adam Novak, Andrew Or, Kay Ousterhout, Sean Owen, Nick Pentreath, Pillis, Imran Rashid, Ahir Reddy, Luca Rosellini, Josh Rosen, Henry Saputra, Andre Schumacher, Jerry Shao, Prashant Sharma, Shiyun, Wangda Tan, Matthew Taylor, Jyun-Fan Tsai, Takuya Ueshin, Shivaram Venkataraman, Jianping J Wang, Martin Weindel, Patrick Wendell, Neal Wiggins, Andrew Xia, Reynold Xin, Dong Yan, Haitao Yao, Xusen Yin, Fengdong Yu, Matei Zaharia, Wu Zeming, and Nan Zhu - Patrick The link delay was because of propagation time to the mirror network - should be okay now! Ah thanks @shivaram. Check now? Hi Everyone, In an effort to coordinate development amongst the growing list of Spark contributors, I've taken some time to write up a proposal to formalize various pieces of the development process. The next release of Spark will likely be Spark 1.0.0, so this message is intended in part to coordinate the release plan for 1.0.0 and future releases. I'll post this on the wiki after discussing it on this thread as tentative project guidelines. == Spark Release Structure == Starting with Spark 1.0.0, the Spark project will follow the semantic versioning guidelines (http://semver.org/) with a few deviations. These small differences account for Spark's nature as a multi-module project. Each Spark release will be versioned: [MAJOR].[MINOR].[MAINTENANCE] All releases with the same major version number will have API compatibility, defined as [1]. Major version numbers will remain stable over long periods of time. For instance, 1.X.Y may last 1 year or more. Minor releases will typically contain new features and improvements. The target frequency for minor releases is every 3-4 months. One change we'd like to make is to announce fixed release dates and merge windows for each release, to facilitate coordination. Each minor release will have a merge window where new patches can be merged, a QA window when only fixes can be merged, then a final period where voting occurs on release candidates. These windows will be announced immediately after the previous minor release to give people plenty of time, and over time, we might make the whole release process more regular (similar to Ubuntu). At the bottom of this document is an example window for the 1.0.0 release. Maintenance releases will occur more frequently and depend on specific patches introduced (e.g. bug fixes) and their urgency. In general these releases are designed to patch bugs. However, higher level libraries may introduce small features, such as a new algorithm, provided they are entirely additive and isolated from existing code paths. Spark core may not introduce any features. When new components are added to Spark, they may initially be marked as "alpha". Alpha components do not have to abide by the above guidelines, however, to the maximum extent possible, they should try to. Once they are marked "stable" they have to follow these guidelines. At present, GraphX is the only alpha component of Spark. [1] API compatibility: An API is any public class or interface exposed in Spark that is not marked as semi-private or experimental. Release A is API compatible with release B if code compiled against release A *compiles cleanly* against B. This does not guarantee that a compiled application that is linked against version A will link cleanly against version B without re-compiling. Link-level compatibility is something we'll try to guarantee that as well, and we might make it a requirement in the future, but challenges with things like Scala versions have made this difficult to guarantee in the past. == Merging Pull Requests == To merge pull requests, committers are encouraged to use this tool [2] to collapse the request into one commit rather than manually performing git merges. It will also format the commit message nicely in a way that can be easily parsed later when writing credits. Currently it is maintained in a public utility repository, but we'll merge it into mainline Spark soon. [2] https://github.com/pwendell/spark-utils/blob/master/apache_pr_merge.py == Tentative Release Window for 1.0.0 == Feb 1st - April 1st: General development April 1st: Code freeze for new features April 15th: RC1 == Deviations == For now, the proposal is to consider these tentative guidelines. We can vote to formalize these as project rules at a later time after some experience working with them. Once formalized, any deviation to these guidelines will be subject to a lazy majority vote. - Patrick I think we'd mark alpha features as such in the java/scaladoc. This is what scala does with experimental features. Higher level libraries are anything that isn't Spark core. Maybe we can formalize this more somehow. We might be able to annotate the new features as experimental if they end up in a patch release. This could make it more clear. I'd say the default is the minor level. If contributors know it should be added in a maintenance release, it's great if they say so. However I'd say this is also responsibility with the committers, since individual contributors may not know. It will probably be a while before major level patches are being merged :P If people feel that merging the intermediate SNAPSHOT number is significant, let's just defer merging that until this discussion concludes. That said - the decision to settle on 1.0 for the next release is not just because it happens to come after 0.9. It's a conscientious decision based on the development of the project to this point. A major focus of the 0.9 release was tying off loose ends in terms of backwards compatibility (e.g. spark configuration). There was some discussion back then of maybe cutting a 1.0 release but the decision was deferred until after 0.9. @mridul - pleas see the original post for discussion about binary compatibility. I think this is a good idea and something on which there is wide consensus. I separately was going to suggest this in a later e-mail (it's not directly tied to versioning). One of many reasons this is necessary is because it's becoming hard to track which features ended up in which releases. This is clearly a goal but I'm hesitant to codify it until we understand all of the reasons why it might not work. I've heard in general with Scala there are many non-obvious things that can break binary compatibility and we need to understand what they are. I'd propose we add the migration tool [1] here to our build and use it for a few months and see what happens (hat tip to Michael Armbrust). It's easy to formalize this as a requirement later, it's impossible to go the other direction. For Scala major versions it's possible we can cross-build between 2.10 and 2.11 to retain link-level compatibility. It's just entirely uncharted territory and AFAIK no one who's suggesting this is speaking from experience maintaining this guarantee for a Scala project. That would be the strongest convincing reason for me - if someone has actually done this in the past in a Scala project and speaks from experience. Most of use are speaking from the perspective of Java projects where we understand well the trade-off's and costs of maintaining this guarantee. [1] https://github.com/typesafehub/migration-manager - Patrick Just to echo others - The relevant question is whether we want to advertise stable API's for users that we will support for a long time horizon. And doing this is critical to being taken seriously as a mature project. The question is not whether or not there are things we want to improve about Spark (further reduce dependencies, runtime stability, etc) - of course everyone wants to improve those things! In the next few months ahead of 1.0 the plan would be to invest effort in finishing off loose ends in the API and of course, no 1.0 release candidate will pass muster if these aren't addressed. I only see a few fairly small blockers though wrt API issues: - We should mark things that may evolve and change as semi-private developer API's (e.g. the Spark Listener). - We need to standardize the Java API in a way that supports Java 8 lamdbas. Other than that - I don't see many blockers in terms of API changes we might want to make. A lot of those were dealt with in 0.9 specifically to prepare for this. The broader question API "clean-up" brings up a debate about the trade off of compatibility with older pre-1.0 versions of Spark. This is not the primary issue under discussion and can be debated separably. The primary issue at hand is whether to have 1.0 in ~3 months vs pushing it to ~6 months from now or more. - Patrick As a break out from the other thread. I'd like to propose two guidelines for pull requests. These guidelines are to make things easier to track for developers and users, and to help organize the large number of PR's that we are receiving. Thoughts? 1. Pull requests will require associated JIRA's. We will ask people to create a JIRA if there is none yet. 2. Pull request names should ideally convey: (a) The JIRA name (b) A title summarizing the patch (c) [optional prefix] The library it is related to (d) [optional prefix] WIP or RFC if it is not finished. Example names: SPARK-123: Add some feature to Spark [STREAMING] SPARK-123: Add some feature to Spark streaming [MLLIB] [WIP] SPARK-123: Some potentially useful feature for MLLib - Patrick We should document this on the wiki! Hey Paul, Thanks for digging this up. I worked on this feature and the intent was to give users good default behavior if they didn't include any logging configuration on the classpath. The problem with assuming that CL tooling is going to fix the job is that many people link against spark as a library and run their application using their own scripts. In this case the first thing people see when they run an application that links against Spark was a big ugly logging warning. I'm not super familiar with log4j-over-slf4j, but this behavior of returning null for the appenders seems a little weird. What is the use case for using this and not just directly use slf4j-log4j12 like Spark itself does? Did you have a more general fix for this in mind? Or was your plan to just revert the existing behavior... We might be able to add a configuration option to disable this logging default stuff. Or we could just rip it out - but I'd like to avoid that if possible. - Patrick A config option e.g. could just be to add: spark.logging.loadDefaultLogger (default true) If set to true, Spark will try to initialize a log4j logger if none is detected. Otherwise Spark will not modify logging behavior. Then users could just set this to false if they have a logging set-up that conflicts with this. Maybe there is a nicer fix... Hey Guys, Thanks for explainning. Ya this is a problem - we didn't really know that people are using other slf4j backends, slf4j is in there for historical reasons but I think we may assume in a few places that log4j is being used and we should minimize those. We should patch this and get a fix into 0.9.1. So some solutions I see are: (a) Add SparkConf option to disable this. I'm fine with this one. (b) Ask slf4j which backend is active and only try to enforce this default if we know slf4j is using log4j. Do either of you know if this is possible? Not sure if slf4j exposes this. (c) Just remove this default stuff. We'd rather not do this. The goal of this thing is to provide good usability for people who have linked against Spark and haven't done anything to configure logging. For beginners we try to minimize the assumptions about what else they know about, and I've found log4j configuration is a huge mental barrier for people who are getting started. Paul if you submit a patch doing (a) we can merge it in. If you have any idea if (b) is possible I prefer that one, but it may not be possible or might be brittle. - Patrick This also seems relevant - but not my area of expertise (whether this is a valid way to check this). http://stackoverflow.com/questions/10505418/how-to-find-which-library-slf4j-has-bound-itself-to Koert - my suggestion was this. We let users use any slf4j backend they want. If we detect that they are using the log4j backend and *also* they didn't configure any log4j appenders, we set up some nice defaults for them. If they are using another backend, Spark doesn't try to modify the configuration at all. Hey Henry, Let me document this on the wiki. I've already keep pretty thorough docs on this I just need to migrate them to the wiki. I've created a JIRA here: https://spark-project.atlassian.net/browse/SPARK-1066 - Patrick Will, Thanks for these thoughts - this is something we should try to be attentive to in the way we think about versioning. (2)-(5) are pretty consistent with the guidelines we already follow. I think the biggest proposed difference is to be conscious of (1), which at least I had not given much thought to in the past. Specifically, if we make major version upgrades of dependencies within a major release of Spark, it can cause issues for downstream packagers. I can't easily recall how often we do this or whether this will be hard for us to guarantee (maybe others can...). It's something to keep in mind though - thanks for bringing it up. - Patrick Paul, Looking back at your problem. I think it's the one here: http://www.slf4j.org/codes.html#log4jDelegationLoop So let me just be clear what you are doing so I understand. You have some other application that directly calls log4j. So you have to include log4j-over-slf4j to route those logs through slf4j to logback. At the same time you embed Spark in this application. In the past it was fine, but now that Spark programmatic ally initializes log4j, it screws up your application because log4j-over-slf4j doesn't work with applications that do this explicilty as discussed here: http://www.slf4j.org/legacy.html Correct? - Patrick Hey Paul, So if your goal is ultimately to output to logback. Then why don't you just use slf4j and logback-classic.jar as described here [1]. Why involve log4j-over-slf4j at all? Let's say we refactored the spark build so it didn't advertise slf4j-log4j12 as a dependency. Would you still be using log4j-over-slf4j... or is this just a "fix" to deal with the fact that Spark is somewhat log4j dependent at this point. [1] http://www.slf4j.org/manual.html - Patrick Ah okay sounds good. This is what I meant earlier by "You have some other application that directly calls log4j."... i.e. you have for historical reasons installed the log4j-over-slf4j. Would you mind trying out this fix and seeing if it works? This is designed to be a hotfix for 0.9, not a general solution where we rip out log4j from our published dependencies: https://github.com/apache/incubator-spark/pull/560/files - Patrick Hey All, Thanks for everyone who participated in this thread. I've distilled feedback based on the discussion and wanted to summarize the conclusions: - People seem universally +1 on semantic versioning in general. - People seem universally +1 on having a public merge windows for releases. - People seem universally +1 on a policy of having associated JIRA's with features. - Everyone believes link-level compatiblity should be the goal. Some people think we should outright promise it now. Others thing we should either not promise it or promise it later. --> Compromise: let's do one minor release 1.0->1.1 to convince ourselves this is possible (some issues with Scala traits will make this tricky). Then we can codify it in writing. I've created SPARK-1069 [1] to clearly establish that this is the goal for 1.X family of releases. - Some people think we should add particular features before having 1.0. --> Version 1.X indicates API stability rather than a feature set; this was clarified. --> That said, people still have several months to work on features if they really want to get them in for this release. I'm going to integrate this feedback and post a tentative version of the release guidelines to the wiki. With all this said, I would like to move the master version to 1.0.0-SNAPSHOT as the main concerns with this have been addressed and clarified. This merely represents a tentative consensus and the release is still subject to a formal vote amongst PMC members. [1] https://spark-project.atlassian.net/browse/SPARK-1069 - Patrick :P - I'm pretty sure this can be done but it will require some work - we already use the github API in our merge script and we could hook something like that up with the jenkins tests. Henry maybe you could create a JIRA for this for Spark 1.0? - Patrick I ported the release docs to the wiki today. Thanks for reminding me about this Henry: https://cwiki.apache.org/confluence/display/SPARK/Preparing+Spark+Releases - Patrick Done, thanks. Feel free to edit it directly as well :) It's possible to mock out actors... we have a few examples in the code base. One his here: https://github.com/apache/incubator-spark/blob/master/core/src/test/scala/org/apache/spark/deploy/worker/WorkerWatcherSuite.scala Thanks Paul - it isn't mean to be a "full solution" but just a fix for the 0.9 branch - for the full solution there is another PR by Sean Owen. Hey Andrew, The intent was to be consistent with the way the merge messages look before. But I agree it obfuscates the commit messages from the user and hides them further down. I think your proposal is good, but it might be better to use the title of their pull request message rather than the first line of the most recent commit in their branch (not sure what you meant by "commit message"). Maybe you could submit a pull request for this? The script we use to merge things is in dev/merge_spark_pr.py. Another nice thing is if people are formatting their titles with jira's then it will all look nice and pretty... which is kind of the goal. - Patrick +1 To clarify to others, this is an IPCM vote so only the IPCM votes are binding :) I think Aaron just meant 1.0.0 by "the next minor release". Hey Henry, Ya unfortunately I have no idea how to do this! You need to create an account. Anyone can sign up for an account on the JIRA. Ya I ran into this a few months ago. We actually patched the spark build back then. It took me a long time to figure it out. https://github.com/apache/incubator-spark/commit/0c1985b153a2dc2c891ae61c1ee67506926384ae BTW my fix in Spark was later generalized to be equivalent to what you did, which is do this for the entire services directory rather than just FileSystem. +1 overall. Christopher - I agree that once the number of rules becomes large it's more efficient to pursue a "use your judgement" approach. However, since this is only 3 cases I'd prefer to wait to see if it grows. The concern with this approach is that for newer people, contributors, etc it's hard for them to understand what good judgement is. Many are new to scala, so explicit rules are generally better. - Patrick I'd personally like to see this go to a separate list. Until then I'd strongly recommended using filters to get rid of them. In gmail it's trivial... Hey All, It's very high overhead having two build systems in Spark. Before getting into a long discussion about the merits of sbt vs maven, I wanted to pose a simple question to the dev list: Is there anyone who feels that dropping either sbt or maven would have a major consequence for them? And I say "major consequence" meaning something becomes completely impossible now and can't be worked around. This is different from an "inconvenience", i.e., something which can be worked around but will require some investment. I'm posing the question in this way because, if there are features in either build system that are absolutely-un-available in the other, then we'll have to maintain both for the time being. I'm merely trying to see whether this is the case... - Patrick Hey Henry, Yep, I wanted to reboot this since some time has passed and people may have new or changed ways of using the build. Maven makes the Apache publishing fairly seamless, but after the last two releases I believe we could make it work with sbt as well. sbt also supports publishing and other Apache projects such as Kafka publish with sbt. Hey Everyone, We are going to publish artifacts to maven central in the exact same format no matter which build system we use. For normal consumers of Spark {maven vs sbt} won't make a difference. It will make a difference for people who are extended the Spark build to do their own packaging. This is what I'm trying to gauge - does anyone do this in a way where they feel only maven or only sbt supports their particular issue. - Patrick Kos - thanks for chiming in. Could you be more specific about what is available in maven and not in sbt for these issues? I took a look at the bigtop code relating to Spark. As far as I could tell [1] was the main point of integration with the build system (maybe there are other integration points)? The sbt build also allows you to plug in a Hadoop version similar to the maven build. AFIAK we are only using the shade plug-in to deal with conflict resolution in the assembly jar. These are dealt with in sbt via the sbt assembly plug-in in an identical way. Is there a difference? [1] https://git-wip-us.apache.org/repos/asf?p=bigtop.git;a=blob;f=bigtop-packages/src/common/spark/do-component-build;h=428540e0f6aa56cd7e78eb1c831aa7fe9496a08f;hb=master We back port bug fixes into the 0.9 branch as they come in, so if there is a particular fix you want to get you can always build from the head of branch-0.9 and expect only stability improvements compared with Spark 0.9.0. The timing of the maintenance releases depends a bit on what bug fixes come in and their importance. I'm thinking we should propose a release pretty soon (order weeks) since there are some valuable bug fixes that came in this week. - Patrick Hey Chris, Would the following be consistent with the Apache guidelines? (a) We establish a culture of not having overall design discussions on github. Design discussions should to occur on JIRA or on the dev list. IMO this is pretty much already true, but there are a few exceptions. (b) We add a mailing list called github@s.a.o which receives the github traffic. This way everything is available in Apache infra. (c) Because of our use of JIRA it might make sense to have an issues@s.a.o list as well similar to what YARN and other projects use. The github chatter is so noisy that I think, overall, it decreases engagement with the official developer list. This is the opposite of what we want. - Patrick btw - I'd prefer reviews@s.a.o instead of github@ to remain more neutral and flexible. Hey All, I created a JIRA to ask infra to create a dedicated reviews@ mailing list for this purpose. https://issues.apache.org/jira/browse/INFRA-7368 Hopefully they can migrate the github stream to this list so that people can distinguish it from developer discussions. In parallel, we are also trying to see if we can use the github status notifier rather than the constant comments from jenkins. - Patrick Hey Punya, It's sufficient to just ping the request on github rather than e-mail the dev list. Sometimes it can takes a few days for people to get to looking at patches... - Patrick Hey Yao, Would you mind explaining exactly how your company extends the Spark maven build? For instance: (a) You are depending on Spark in your build and your build is using Maven. (b) You have downloaded Spark and forked it's maven build to change around the dependencies. (c) You are writing pom files that extend the Spark pom. If it's just (a) - then whether Spark itself uses sbt/maven will make no difference. We'd publish identical poms. - Patrick Evan - this is a good thing to bring up. Wrt the shader plug-in - right now we don't actually use it for bytecode shading - we simply use it for creating the uber jar with excludes (which sbt supports just fine via assembly). I was wondering actually, do you know if it's possible to added shaded artifacts to the *spark jar* using this plug-in (e.g. not an uber jar)? That's something I could see being really handy in the future. - Patrick What I mean is this. AFIAK the shader plug-in is primarily designed for creating uber jars which contain spark and all dependencies. But since Spark is something people depend on in Maven, what I actually want is to create the normal old Spark jar [1], but then include shaded versions of some of our dependencies inside of it. Not sure if that's even possible. The way we do shading now is we manually publish shaded versions of some dependencies to maven central as their own artifacts. http://search.maven.org/remotecontent?filepath=org/apache/spark/spark-core_2.10/0.9.0-incubating/spark-core_2.10-0.9.0-incubating.jar Hey Andrew, Indeed, sometimes there are patches that sit around a while and in this case it can be because it's unclear to the reviewers whether they are features worth having - or just by accident. To put things in perspective, Spark merges about 80% of the proposed patches (if you look we are on around 600 since moving to the new repo with 100 not merged) - so in general we try hard to be very supportive of community patches, much more than other projects in this space. - Patrick Hey Andrew, Ah, I just meant to say that in cases like this it's usually a mistake...  and we try to (in general) be inclusive about merging patches :) Definitely appreciate you calling this one out... this is what people should do in cases like this. - Patrick Hey Andrew, Ah, I just meant to say that in cases like this it's usually a mistake...  and we try to (in general) be inclusive about merging patches :) Definitely appreciate you calling this one out... this is what people should do in cases like this. - Patrick @mridul - As far as I know both Maven and Sbt use fairly similar processes for building the assembly/uber jar. We actually used to package spark with sbt and there were no specific issues we encountered and AFAIK sbt respects versioning of transitive dependencies correctly. Do you have a specific bug listing for sbt that indicates something is broken? @sandy - It sounds like you are saying that the CDH build would be easier with Maven because you can inherit the POM. However, is this just a matter of convenience for packagers or would standardizing on sbt limit capabilities in some way? I assume that it would just mean a bit more manual work for packagers having to figure out how to set the hadoop version in SBT and exclude certain dependencies. For instance, what does CDH about other components like Impala that are not based on Maven at all? Hey All, The github incubator-spark mirror has been migrated to [1] by Apache infra and we've migrated Jenkins to reflect the new changes. This means the existing "incubator-spark" mirror is becoming outdated and no longer correctly displays pull request diff's. We've asked apache infra to see if they can migrate existing pull requests to incubator-spark. However since this relies on coordinating with github, I'm not entirely sure whether they can do this or what the timeline would be. In the mean time it would be good for people to open new pull requests against [1]. For pull requests that were *just* about to be merged, we can go manually merge them, but ones that require feedback and more rounds of testing will need to be done on the new one since incubator-spark is now out of date. Sorry about this inconvenience, it is a one-time transition and we won't ever have to do it again. [1] https://github.com/apache/spark - Patrick Sorry if this wasn't clear - If you are in the middle of a review close it and re-open it in against [1]. The reason is we can't test your changes against incubator-spark because it no longer exists. [1] https://github.com/apache/spark - Patrick You need to fork the new apache repository. 1. Fork https://github.com/apache/spark/ in github 2. Add your own fork as a remote in your local git ===> git remote add apache-pwendell git@github.com:pwendell/spark.git 3. Push your local branch the fork on github. 4. Make a pull request from your fork on github to apache. Because the repo has migrated, anyone who wants to contribute will need to fork the new repo at some point... - Patrick Hey, Thanks everyone for chiming in on this. I wanted to summarize these issues a bit particularly wrt the constituents involved - does this seem accurate? = Spark Users = In general those linking against Spark should be totally unaffected by the build choice. Spark will continue to publish well-formed poms and jars to maven central. This is a no-op wrt this decision. = Spark Developers = There are two concerns. (a) General day-to-day development and packaging and (b) Spark binaries and packages for distribution. For (a) - sbt seems better because it's just nicer for doing scala development (incremental complication is simple, we have some home-baked tools for compiling Spark vs. the spark deps etc). The arguments that maven has more "general know how", at least so far, haven't affected us in the ~2 years we've maintained both builds - where adding stuff for Maven is typically just as annoying/difficult with sbt. For (b) - Some non-specific concerns were raised about bugs with the sbt assembly package - we should look into this and see what is going on. Maven has better out-of-the-box support for publishing to Maven central, we'd have to do some manual work on our end to make this work well with sbt. = Downstream Integrators = On this one it seems that Maven is the universal favorite, largely because of community awareness of Maven and comfort with Maven builds. Some things like restructuring the Spark build to inherit config values from a vendor build will be not possible with sbt (though fairly straightforward to work around). Other cases where vendors have directly modified or inherited the Spark build won't work anymore if we standardize on SBT. These have no obvious work around at this point as far as I see. - Patrick Hey All, Just a heads up that there are a bunch of updated developer docs on the wiki including posting the dates around the current merge window. Some of the new docs might be useful for developers/committers: https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage Cheers, - Patrick Hey Evan, This is being tracked here: https://spark-project.atlassian.net/browse/SPARK-1190 That patch didn't get merged but I've just opened a new one here: https://github.com/apache/spark/pull/107/files Would you have any interest in testing this? I want to make sure it works for users who are using logback. I'd like to get this merged quickly since it's one of the only remaining blockers for Spark 0.9.1. - Patrick Evan I actually remembered that Paul Brown (who also reported this issue) tested it and found that it worked. I'm going to merge this into master and branch 0.9, so please give it a spin when you have a chance. - Patrick The fix for this was just merged into branch 0.9 (will be in 0.9.1+) and master. Dianna I'm forwarding this to the dev list since it might be useful there as well. Hey All, The Hadoop Summit uses community choice voting to decide which talks to feature. It would be great if the community could help vote for Spark talks so that Spark has a good showing at this event. You can make three votes on each track. Below I've listed Spark talks in each of the tracks - voting closes tomorrow so vote now!! Building a Unified Data Pipeline in Apache Spark bit.ly/O8USIq (Committer Track) Building a Data Processing System for Real Time Auctions bit.ly/1ij3XJJ (Business Apps Track) SparkR: Enabling Interactive Data Science at Scale on Hadoop bit.ly/1kPQUlG (Data Science Track) Recent Developments in Spark MLlib and Beyond bit.ly/1hgZW5D (The Future of Apache Hadoop Track) Cheers, - Patrick Hey Josh, I spent some time playing with the sbt-pom-reader and actually I think it might suite our needs pretty well. Basically our situation is that Maven has more mature support for packaging (shading, official assembly plug-in) and is used by pretty much every downstream packager of Spark. SBT is used by the developers heavily for incremental compilation, the console, etc. So far we've been maintaining entirely isolated builds in sbt and maven which is a huge development cost and something that's lead to divergence over time. The sbt-pom-reader I think could give us the best of both worlds. But it doesn't support profiles in Maven which we use pretty heavily. I played a bit this week and tried to get it to work without much luck: https://github.com/pwendell/sbt-pom-reader/commit/ca1f1f2d6bf8891acb7212facf4807baaca8974d Any pointers or interest in helping us with that feature? I think with that in place we might have a good solution where the dependencies are declared in one place (poms) and we use Maven for packaging but we can use sbt for day to day development. Hey All, We've created a new list called reviews@spark.apache.org which will contain the contents from the github pull requests and comments. Note that these e-mails will no longer appear on the dev list. Thanks to Apache Infra for helping us set this up. To subscribe to this e-mail: reviews-subscribe@spark.apache.org - Patrick Hey Nathan, I don't think this would be possible because there are at least dozens of permutations of Hadoop versions (different vendor distros X different versions X YARN vs not YARN, etc) and maybe hundreds. So publishing new artifacts for each would be really difficult. What is the exact problem you ran into? Maybe we need to improve the documentation to make it more clear how to correctly link against spark/hadoop for user applications. Basically the model we have now is users link against Spark and then link against the hadoop-client relevant to their version of Hadoop. - Patrick Evan - yep definitely open a JIRA. It would be nice to have a contrib repo set-up for the 1.0 release. Hey Tom, Someone recently sent me a personal e-mail reporting some problems with this. I'll ask them to forward it to you/the dev list. Might be worth looking into before merging. Good call on this one. - Patrick Thanks Tom, After I looked more at this patch I don't see how this could have regressed behavior for any users (it seems like it only pertains to warnings and instructions). So maybe the user mistook this patch for a different issue. https://github.com/apache/incubator-spark/pull/553/files - Patrick It has a bunch of packages installed on it for various spark dependencies (libfortran, numpy, scipy) and some helpful tools (dstat, iotop). Hey Evan and TD, Spark's dependency graph in a maintenance release seems potentially harmful, especially upgrading a minor version (not just a patch version) like this. This could affect other downstream users. For instance, now without knowing their fastutil dependency gets bumped and they hit some new problem in fastutil 6.5. - Patrick *Modifying* Spark's dependency graph... Hey TD, This one we just merged into master this morning: https://spark-project.atlassian.net/browse/SPARK-1322 It should definitely go into the 0.9 branch because there was a bug in the semantics of top() which at this point is unreleased in Python. I didn't backport it yet because I figured you might want to do this at a specific time. So please go ahead and backport it. Not sure whether this warrants another RC. - Patrick Yeah sorry guys - Jenkins is having some issues and there isn't a way to fix this that doesn't spam people following github. Apologies! Really - I didn't know this ever was changed. But in any case, I think you can compile with 2.10.4 and run with 2.10.3 and it's fine - right? Mridul, You can unsubscribe yourself from any of these sources, right? - Patrick Ah sorry I see - Jira updates are going to the dev list. Maybe that's not desirable. I think we should send them to the issues@ list. Hey Chris, I don't think our JIRA has been fully migrated to Apache infra, so it's really confusing to send people e-mails referring to the new JIRA since we haven't announced it yet. There is some content there because we've been trying to do the migration, but I'm not sure it's entirely finished. Also, right now our github comments go to a commits@ list. I'm actually -1 copying all of these to JIRA because we do a bunch of review level comments that are going to pollute the JIRA a bunch. In any case, can you revert the change whatever it was that sent these to the dev list? We should have a coordinated plan about this transition and the e-mail changes we plan to make. - Patrick Okay I think I managed to revert this by just removing jira@a.o from our dev list. I'm working with infra to get the following set-up: 1. Don't post github updates to jira comments (they are too low level). If users want these they can subscribe to commits@s.a.o. 2. Jira comment stream will go to issues@s.a.o so people can opt into that. One thing YARN has set-up that might be cool in the future is to e-mail *new* JIRA's to the dev list. That might be cool to set up in the future. Okay cool - sorry about that. Infra should be able to migrate these over to an issues@ list shortly. I'd rather bother a few moderators than the entire dev list... but ya I realize it's annoying :/ Hey All, We've successfully migrated the Spark JIRA to the Apache infrastructure. This turned out to be a huge effort, lead by Andy Konwinski, who deserves all of our deepest appreciation for managing this complex migration Since Apache runs the same JIRA version as Spark's existing JIRA, there is no new software to learn. A few things to note though: - The issue tracker for Spark is now at: https://issues.apache.org/jira/browse/SPARK - You can sign up to receive an e-mail feed of JIRA updates by e-mailing: issues-subscribe@spark.apache.org - DO NOT create issues on the old JIRA. I'll try to disable this so that it is read-only. - You'll need to create an account at the new site if you don't have one already. - We've imported all the old JIRA's. In some cases the import tool can't correctly guess the assignee for the JIRA, so we may have to do some manual assignment. - If you feel like you don't have sufficient permissions on the new JIRA, please send me an e-mail. I tried to add all of the committers as administrators but I may have missed some. Thanks, Patrick TD - I downloaded and did some local testing. Looks good to me! +1 You should cast your own vote - at that point it's enough to pass. - Patrick Checkout this page: http://spark.incubator.apache.org/docs/latest/cluster-overview.html Checkout this page: http://spark.incubator.apache.org/docs/latest/cluster-overview.html Yeah good point. Let's just extend this vote another few days? Ya there is already some fragmentation here. Maven has some "dist" targets and there is also ./make-distribution.sh. And there is a deb target as well - ah didn't see Mark's email. Tom, Given this is a pretty straightforward workaround, what do yo think about the following course of action: (a) We can put the workaround in the docs for 0.9.1. We don't need to do a new RC/vote for this since we can update the published docs independently. (b) We try to get a fix in for this into the 0.9 branch so it can end up in 0.9.2. But this takes the fix off the critical path for this release. - Patrick Hey Evan, Ya thanks this is a pretty small patch. Should definitely be do-able for 1.0. - Patrick I answered this over on the user list... If you want to submit a hot fix for this issue specifically please do. I'm not sure why it didn't fail our build... TD - do you know what is going on here? I looked into this ab it and at least a few of these that use Thread.sleep() and assume the sleep will be exact, which is wrong. We should disable all the tests that do and probably they should be re-written to virtualize time. - Patrick Hey All, In accordance with the scheduled window for the release I've cut a 1.0 branch. Thanks a ton to everyone for being so active in reviews during the last week. In the last 7 days we've merged 66 new patches, and every one of them has undergone thorough peer-review. Tons of committers have been active in code review - pretty cool! At this point the 1.0 branch transitions to a normal maintenance branch*. Bug fixes, documentation are still welcome or additions to higher level libraries (e.g. MLLib). The focus though is shifting to QA, fixes, and documentation for the release. Thanks again to everyone who participated in the last week! - Patrick *caveat: we will still merge in some API visibility patches and a few remaining loose ends in the next day or two. You'll need to use the associated functionality in Breeze and then create a dense vector from a Breeze vector. I have a JIRA for us to update the examples for 1.0...  I'm hoping Xiangrui can take a look at it. https://issues.apache.org/jira/browse/SPARK-1464 https://github.com/scalanlp/breeze/wiki/Breeze-Linear-Algebra You'll need to use the associated functionality in Breeze and then create a dense vector from a Breeze vector. I have a JIRA for us to update the examples for 1.0...  I'm hoping Xiangrui can take a look at it. https://issues.apache.org/jira/browse/SPARK-1464 https://github.com/scalanlp/breeze/wiki/Breeze-Linear-Algebra There are a few things going on here wrt tests. 1. I fixed up the RAT issues with a hotfix. 2. The Hive tests were actually disabled for a while accidentally. A recent fix correctly re-enabled them. Without Hive Spark tests run in about 40 minutes and with Hive it runs in 1 hour and 15 minutes, so it's a big difference. To ease things I committed a patch today that only runs the Hive tests if the change touches Spark SQL. So this should make it simpler for normal tests. We can actually generalize this to do much finer grained testing, e.g. if something in MLLib changes we don't need to re-run the streaming tests. I've added this JIRA to track it: https://issues.apache.org/jira/browse/SPARK-1455 3. Overall we've experienced more race conditions with tests recently. I noticed a few zombie test processes on Jenkins hogging up 100% of CPU so I think this has triggered several previously unseen races due to CPU contention on the test cluster. I killed them and we'll see if they crop up again. 4. Please try to keep an eye on the length of new tests that get committed. It's common to see people commit tests that e.g. sleep for several seconds or do things that take a long time. Almost always this can be avoided and usually avoiding it makes the test cleaner anyways (e.g. use proper synchronization instead of sleeping). - Patrick Just option [string] is fine. Happy to accept a fix but I'll probably submit one tonight if no one else has. --- sent from my phone Just wanted to mention - one common thing I've seen users do is use groupByKey, then do something that is commutitive and associative once the values are grouped. Really users here should be doing reduceByKey. rdd.groupByKey().map{ case (key, values) => (key, values.sum)) rdd.reduceByKey(_ + _) I've seen this happen particularly for users coming from MapReduce where they are used to having to write their own combiners and it's not intuitive that these functions are very different. Sandy - have you heard from users who have a specific problems they can't solve using an associative function? I'm sure they exist, but I wonder how often it's this vs. they just don't understand they API. I wonder if we should actually warn about this in the groupByKey documentation. - Patrick This is already on the wiki: https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools Hey All, This is not an official vote, but I wanted to cut an RC so that people can test against the Maven artifacts, test building with their configuration, etc. We are still chasing down a few issues and updating docs, etc. If you have issues or bug reports for this release, please send an e-mail to the Spark dev list and/or file a JIRA. Commit: d636772 (v1.0.0-rc3) https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d636772ea9f98e449a038567b7975b1a07de3221 Binaries: http://people.apache.org/~pwendell/spark-1.0.0-rc3/ Docs: http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/ Repository: https://repository.apache.org/content/repositories/orgapachespark-1012/ == API Changes == If you want to test building against Spark there are some minor API changes. We'll get these written up for the final release but I'm noting a few here (not comprehensive): changes to ML vector specification: http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/mllib-guide.html#from-09-to-10 changes to the Java API: http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark coGroup and related functions now return Iterable[T] instead of Seq[T] ==> Call toSeq on the result to restore the old behavior SparkContext.jarOfClass returns Option[String] instead of Seq[String] ==> Call toSeq on the result to restore old behavior Streaming classes have been renamed: NetworkReceiver -> Receiver There are not guarantees. Sorry got cut off. For 0.9.0 and 1.0.0 they are not binary compatible and in a few cases not source compatible. 1.X will be source compatible. We are also planning to support binary compatibility in 1.X but I'm waiting util we make a few releases to officially promise that, since Scala makes this pretty tricky. Hi Dean, We always used the Hadoop libraries here to read and write local files. In Spark 1.0 we started enforcing the rule that you can't over-write an existing directory because it can cause confusing/undefined behavior if multiple jobs output to the directory (they partially clobber each other's output). https://issues.apache.org/jira/browse/SPARK-1100 https://github.com/apache/spark/pull/11 In the JIRA I actually proposed slightly deviating from Hadoop semantics and allowing the directory to exist if it is empty, but I think in the end we decided to just go with the exact same semantics as Hadoop (i.e. empty directories are a problem). - Patrick That suggestion got lost along the way and IIRC the patch didn't have that. It's a good idea though, if nothing else to provide a simple means for backwards compatibility. I created a JIRA for this. It's very straightforward so maybe someone can pick it up quickly: https://issues.apache.org/jira/browse/SPARK-1677 I added a fix for this recently and it didn't require adding -J notation - are you trying it with this patch? https://issues.apache.org/jira/browse/SPARK-1654 ./bin/spark-shell --driver-java-options "-Dfoo=a -Dbar=b"scala> sys.props.get("foo") res0: Option[String] = Some(a) scala> sys.props.get("bar") res1: Option[String] = Some(b) - Patrick Yeah I think the problem is that the spark-submit script doesn't pass the argument array to spark-class in the right way, so any quoted strings get flattened. We do: ORIG_ARGS=$@ $SPARK_HOME/bin/spark-class org.apache.spark.deploy.SparkSubmit $ORIG_ARGS This works: // remove all the code relating to `shift`ing the arguments $SPARK_HOME/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@"Not sure, but I think the issue is that when you make a copy of $@ in bash the type actually changes from an array to something else. My patch fixes this for spark-shell but I didn't realize that spark-submit does the same thing. https://github.com/apache/spark/pull/576/files#diff-bc287993dfd11fd18794041e169ffd72L23 I think we'll need to figure out how to do this correctly in the bash script so that quoted strings get passed in the right way. So I reproduced the problem here: == test.sh == #!/bin/bash for x in "$@"; do echo "arg: $x"done ARGS_COPY=$@ for x in "$ARGS_COPY"; do echo "arg_copy: $x"done == ./test.sh a b "c d e" f arg: a arg: b arg: c d e arg: f arg_copy: a b c d e f I'll dig around a bit more and see if we can fix it. Pretty sure we aren't passing these argument arrays around correctly in bash. Marcelo - Mind trying the following diff locally? If it works I can send a patch: patrick@patrick-t430s:~/Documents/spark$ git diff bin/spark-submit diff --git a/bin/spark-submit b/bin/spark-submit index dd0d95d..49bc262 100755 --- a/bin/spark-submit +++ b/bin/spark-submit @@ -18,7 +18,7 @@ # export SPARK_HOME="$(cd `dirname $0`/..; pwd)"-ORIG_ARGS=$@ +ORIG_ARGS=("$@") while (($#)); do if [ "$1" = "--deploy-mode" ]; then @@ -39,5 +39,5 @@ if [ ! -z $DRIVER_MEMORY ] && [ ! -z $DEPLOY_MODE ] && [ $DEPLOY_MODE = "client"export SPARK_MEM=$DRIVER_MEMORY fi -$SPARK_HOME/bin/spark-class org.apache.spark.deploy.SparkSubmit $ORIG_ARGS +$SPARK_HOME/bin/spark-class org.apache.spark.deploy.SparkSubmit "${ORIG_ARGS[@]}"Dean - our e-mails crossed, but thanks for the tip. Was independently arriving at your solution :) Okay I'll submit something. - Patrick Patch here: https://github.com/apache/spark/pull/609 Andrew, Updating these docs would be great! I think this would be a welcome change. In terms of packaging, it would be good to mention the binaries produced by the upstream project as well, in addition to Mesosphere. - Patrick Please vote on releasing the following candidate as Apache Spark version 1.0.0! The tag to be voted on is v1.0.0-rc5 (commit 18f0623): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=18f062303303824139998e8fc8f4158217b0dbc3 The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.0.0-rc5/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1012/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/ Please vote on releasing this package as Apache Spark 1.0.0! The vote is open until Friday, May 16, at 09:30 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.0.0 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ == API Changes == We welcome users to compile Spark applications against 1.0. There are a few API changes in this release. Here are links to the associated upgrade guides - user facing changes have been kept as small as possible. changes to ML vector specification: http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10 changes to the Java API: http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark changes to the streaming API: http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x changes to the GraphX API: http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091 coGroup and related functions now return Iterable[T] instead of Seq[T] ==> Call toSeq on the result to restore the old behavior SparkContext.jarOfClass returns Option[String] instead of Seq[String] ==> Call toSeq on the result to restore old behavior Hey @witgo - those bugs are not severe enough to block the release, but it would be nice to get them fixed. At this point we are focused on severe bugs with an immediate fix, or regressions from previous versions of Spark. Anything that misses this release will get merged into the branch-1.0 branch and make it into the 1.0.1 release, so people will have access to it. Hey all - there were some earlier RC's that were not presented to the dev list because issues were found with them. Also, there seems to be some issues with the reliability of the dev list e-mail. Just a heads up. I'll lead with a +1 for this. This vote is cancelled in favor of rc6. Please vote on releasing the following candidate as Apache Spark version 1.0.0! This patch has a few minor fixes on top of rc5. I've also built the binary artifacts with Hive support enabled so people can test this configuration. When we release 1.0 we might just release both vanilla and Hive-enabled binaries. The tag to be voted on is v1.0.0-rc6 (commit 54133a): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=54133abdce0246f6643a1112a5204afb2c4caa82 The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.0.0-rc6/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachestratos-1011 The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.0.0-rc6-docs/ Please vote on releasing this package as Apache Spark 1.0.0! The vote is open until Saturday, May 17, at 20:58 UTC and passes if amajority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.0.0 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ == API Changes == We welcome users to compile Spark applications against 1.0. There are a few API changes in this release. Here are links to the associated upgrade guides - user facing changes have been kept as small as possible. changes to ML vector specification: http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10 changes to the Java API: http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark changes to the streaming API: http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x changes to the GraphX API: http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091 coGroup and related functions now return Iterable[T] instead of Seq[T] ==> Call toSeq on the result to restore the old behavior SparkContext.jarOfClass returns Option[String] instead of Seq[String] ==> Call toSeq on the result to restore old behavior I'm cancelling this vote in favor of rc6. [Due to ASF e-mail outage, I'm not if anyone will actually receive this.] Please vote on releasing the following candidate as Apache Spark version 1.0.0! This has only minor changes on top of rc7. The tag to be voted on is v1.0.0-rc8 (commit 80eea0f): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=80eea0f111c06260ffaa780d2f3f7facd09c17bc The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.0.0-rc8/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1016/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/ Please vote on releasing this package as Apache Spark 1.0.0! The vote is open until Monday, May 19, at 10:15 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.0.0 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ == API Changes == We welcome users to compile Spark applications against 1.0. There are a few API changes in this release. Here are links to the associated upgrade guides - user facing changes have been kept as small as possible. changes to ML vector specification: http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#from-09-to-10 changes to the Java API: http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark changes to the streaming API: http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x changes to the GraphX API: http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091 coGroup and related functions now return Iterable[T] instead of Seq[T] ==> Call toSeq on the result to restore the old behavior SparkContext.jarOfClass returns Option[String] instead of Seq[String] ==> Call toSeq on the result to restore old behavior Thanks for your feedback. Since it's not a regression, it won't block the release. Please vote on releasing the following candidate as Apache Spark version 1.0.0! This patch has minor documentation changes and fixes on top of rc6. The tag to be voted on is v1.0.0-rc7 (commit 9212b3e): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=9212b3e5bb5545ccfce242da8d89108e6fb1c464 The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.0.0-rc7/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1015 The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.0.0-rc7-docs/ Please vote on releasing this package as Apache Spark 1.0.0! The vote is open until Sunday, May 18, at 09:12 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.0.0 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ == API Changes == We welcome users to compile Spark applications against 1.0. There are a few API changes in this release. Here are links to the associated upgrade guides - user facing changes have been kept as small as possible. changes to ML vector specification: http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10 changes to the Java API: http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark changes to the streaming API: http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x changes to the GraphX API: http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091 coGroup and related functions now return Iterable[T] instead of Seq[T] ==> Call toSeq on the result to restore the old behavior SparkContext.jarOfClass returns Option[String] instead of Seq[String] ==> Call toSeq on the result to restore old behavior This vote is cancelled in favor of rc7. Hey Everyone, Just a heads up - I've sent other release candidates to the list, but they appear to be getting swallowed (i.e. they are not on nabble). I think there is an issue with Apache mail servers. I'm going to keep trying... if you get duplicate e-mails I apologize in advance. I'll start the voting with a +1. Hey all, My vote threads seem to be running about 24 hours behind and/or getting swallowed by infra e-mail. I sent RC8 yesterday and we might send one tonight as well. I'll make sure to close all existing ones There have been only small "polish" changes in the recent RC's since RC5. So testing any off these should be pretty equivalent. I'll make sure I close all the other threads by tonight. - Patrick Due to the issue discovered by Michael, this vote is cancelled in favor of rc9. I'll start the voting with a +1. Please vote on releasing the following candidate as Apache Spark version 1.0.0! This has one bug fix and one minor feature on top of rc8: SPARK-1864: https://github.com/apache/spark/pull/808 SPARK-1808: https://github.com/apache/spark/pull/799 The tag to be voted on is v1.0.0-rc9 (commit 920f947): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=920f947eb5a22a679c0c3186cf69ee75f6041c75 The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.0.0-rc9/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1017/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.0.0-rc9-docs/ Please vote on releasing this package as Apache Spark 1.0.0! The vote is open until Tuesday, May 20, at 08:56 UTC and passes if amajority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.0.0 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ == API Changes == We welcome users to compile Spark applications against 1.0. There are a few API changes in this release. Here are links to the associated upgrade guides - user facing changes have been kept as small as possible. changes to ML vector specification: http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#from-09-to-10 changes to the Java API: http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark changes to the streaming API: http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x changes to the GraphX API: http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091 coGroup and related functions now return Iterable[T] instead of Seq[T] ==> Call toSeq on the result to restore the old behavior SparkContext.jarOfClass returns Option[String] instead of Seq[String] ==> Call toSeq on the result to restore old behavior Cancelled in favor of rc9. @xiangrui - we don't expect these to be present on the system classpath, because they get dynamically added by Spark (e.g. your application can call sc.addJar well after the JVM's have started). @db - I'm pretty surprised to see that behavior. It's definitely not intended that users need reflection to instantiate their classes - something odd is going on in your case. If you could create an isolated example and post it to the JIRA, that would be great. @db - it's possible that you aren't including the jar in the classpath of your driver program (I think this is what mridul was suggesting). It would be helpful to see the stack trace of the CNFE. - Patrick Hey Matei - the issue you found is not related to security. This patch a few days ago broke builds for Hadoop 1 with YARN support enabled. The patch directly altered the way we deal with commons-lang dependency, which is what is at the base of this stack trace. https://github.com/apache/spark/pull/754 - Patrick Having a user add define a custom class inside of an added jar and instantiate it directly inside of an executor is definitely supported in Spark and has been for a really long time (several years). This is something we do all the time in Spark. DB - I'd hold off on a re-architecting of this until we identify exactly what is causing the bug you are running into. In a nutshell, when the bytecode "new Foo()" is run on the executor, it will ask the driver for the class over HTTP using a custom classloader. Something in that pipeline is breaking here, possibly related to the YARN deployment stuff. Whenever we publish a release candidate, we create a temporary maven repository that host the artifacts. We do this precisely for the case you are running into (where a user wants to build an application against it to test). You can build against the release candidate by just adding that repository in your sbt build, then linking against "spark-core"version "1.0.0". For rc9 the repository is in the vote e-mail: http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-0-0-rc9-td6629.html We're cancelling this RC in favor of rc10. There were two blockers: an issue with Windows run scripts and an issue with the packaging for Hadoop 1 when hive support is bundled. https://issues.apache.org/jira/browse/SPARK-1875 https://issues.apache.org/jira/browse/SPARK-1876 Thanks everyone for the testing. TD will be cutting rc10, since I'm travelling this week (thanks TD!). - Patrick Of these two solutions I'd definitely prefer 2 in the short term. I'd imagine the fix is very straightforward (it would mostly just be remove code), and we'd be making this more consistent with the standalone mode which makes things way easier to reason about. In the long term we'll definitely want to exploit the distributed cache more, but at this point it's premature optimization at a high complexity cost. Writing stuff to HDFS through is so slow anyways I'd guess that serving it directly from the driver is still faster in most cases (though for very large jar sizes or very large clusters, yes, we'll need the distributed cache). - Patrick Hey I just looked at the fix here: https://github.com/apache/spark/pull/848 Given that this is quite simple, maybe it's best to just go with this and just explain that we don't support adding jars dynamically in YARN in Spark 1.0. That seems like a reasonable thing to do. Nilesh - out of curiosity - what operation are you doing on the values for the key? Nilesh - out of curiosity - what operation are you doing on the values for the key? Hey Ankur, That does seem like a good fix, but right now we are only blocking the release on major regressions that affect all components. So I don't think this is sufficient to block it from going forward and cutting a new candidate. This is because we are in the very late stage of the release. We can slot that for the 1.0.1 release and merge it into the 1.0 branch so people can get access to the fix easily. +1 I spun up a few EC2 clusters and ran my normal audit checks. Tests passing, sigs, CHANGES and NOTICE look good Thanks TD for helping cut this RC! [tl;dr stable API's are important - sorry, this is slightly meandering] Hey - just wanted to chime in on this as I was travelling. Sean, you bring up great points here about the velocity and stability of Spark. Many projects have fairly customized semantics around what versions actually mean (HBase is a good, if somewhat hard-to-comprehend, example). What the 1.X label means to Spark is that we are willing to guarantee stability for Spark's core API. This is something that actually, Spark has been doing for a while already (we've made few or no breaking changes to the Spark core API for several years) and we want to codify this for application developers. In this regard Spark has made a bunch of changes to enforce the integrity of our API's: - We went through and clearly annotated internal, or experimental API's. This was a huge project-wide effort and included Scaladoc and several other components to make it clear to users. - We implemented automated byte-code verification of all proposed pull requests that they don't break public API's. Pull requests after 1.0 will fail if they break API's that are not explicitly declared private or experimental. I can't possibly emphasize enough the importance of API stability. What we want to avoid is the Hadoop approach. Candidly, Hadoop does a poor job on this. There really isn't a well defined stable API for any of the Hadoop components, for a few reasons: 1. Hadoop projects don't do any rigorous checking that new patches don't break API's. Of course, the results in regular API breaks and a poor understanding of what is a public API. 2. In several cases it's not possible to do basic things in Hadoop without using deprecated or private API's. 3. There is significant vendor fragmentation of API's. The main focus of the Hadoop vendors is making consistent cuts of the core projects work together (HDFS/Pig/Hive/etc) - so API breaks are sometimes considered "fixed" as long as the other projects work around them (see [1]). We also regularly need to do archaeology (see [2]) and directly interact with Hadoop committers to understand what API's are stable and in which versions. One goal of Spark is to deal with the pain of inter-operating with Hadoop so that application writers don't to. We'd like to retain the property that if you build an application against the (well defined, stable) Spark API's right now, you'll be able to run it across many Hadoop vendors and versions for the entire Spark 1.X release cycle. Writing apps against Hadoop can be very difficult... consider how much more engineering effort we spent maintaining YARN support than Mesos support. There are many factors, but one is that Mesos has a single, narrow, stable API. We've never had to make a change in Mesos due to an API change, for several years. YARN on the other hand, there are at least 3 YARN API's that currently exist, which are all binary incompatible. We'd like to offer apps the ability to build against Spark's API and just let us deal with it. As more vendors packaging Spark, I'd like to see us put tools in the upstream Spark repo that do validation for vendor packages of Spark, so that we don't end up with fragmentation. Of course, vendors can enhance the API and are encouraged to, but we need a kernel of API's that vendors must maintain (think POSIX) to be considered compliant with Apache Spark. I believe some other projects like OpenStack have done this to avoid fragmentation. - Patrick [1] https://issues.apache.org/jira/browse/MAPREDUCE-5830 [2] http://2.bp.blogspot.com/-GO6HF0OAFHw/UOfNEH-4sEI/AAAAAAAAAD0/dEWFFYTRgYw/s1600/output-file.png Congrats everyone! This is a huge accomplishment! I'm thrilled to announce the availability of Spark 1.0.0! Spark 1.0.0 is a milestone release as the first in the 1.0 line of releases, providing API stability for Spark's core interfaces. Spark 1.0.0 is Spark's largest release ever, with contributions from 117 developers. I'd like to thank everyone involved in this release - it was truly a community effort with fixes, features, and optimizations contributed from dozens of organizations. This release expands Spark's standard libraries, introducing a new SQL package (SparkSQL) which lets users integrate SQL queries into existing Spark workflows. MLlib, Spark's machine learning library, is expanded with sparse vector support and several new algorithms. The GraphX and Streaming libraries also introduce new features and optimizations. Spark's core engine adds support for secured YARN clusters, a unified tool for submitting Spark applications, and several performance and stability improvements. Finally, Spark adds support for Java 8 lambda syntax and improves coverage of the Java and Python API's. Those features only scratch the surface - check out the release notes here: http://spark.apache.org/releases/spark-release-1-0-0.html Note that since release artifacts were posted recently, certain mirrors may not have working downloads for a few hours. - Patrick Yeah - Spark streaming needs at least two threads to run. I actually thought we warned the user if they only use one (@tdas?) but the warning might not be working correctly - or I'm misremembering. Hey guys, thanks for the insights. Also, I realize Hadoop has gotten way better about this with 2.2+ and I think it's great progress. We have well defined API levels in Spark and also automated checking of API violations for new pull requests. When doing code reviews we always enforce the narrowest possible visibility: 1. private 2. private[spark] 3. @Experimental or @DeveloperApi 4. public Our automated checks exclude 1-3. Anything that breaks 4 will trigger a build failure. The Scala compiler prevents anyone external from using 1 or 2. We do have "bytecode public but annotated" (3) API's that we might change. We spent a lot of time looking into whether these can offer compiler warnings, but we haven't found a way to do this and do not see a better alternative at this point. Regarding Scala compatibility, Scala 2.11+ is "source code compatible", meaning we'll be able to cross-compile Spark for different versions of Scala. We've already been in touch with Typesafe about this and they've offered to integrate Spark into their compatibility test suite. They've also committed to patching 2.11 with a minor release if bugs are found. Anyways, my point is we've actually thought a lot about this already. The CLASSPATH thing is different than API stability, but indeed also a form of compatibility. This is something where I'd also like to see Spark have better isolation of user classes from Spark's own execution... - Patrick Spark is a bit different than Hadoop MapReduce, so maybe that's a source of some confusion. Spark is often used as a substrate for building different types of analytics applications, so @DeveloperAPI are internal API's that we'd like to expose to application writers, but that might be more volatile. This is like the internal API's in the linux kernel, they aren't stable, but of course we try to minimize changes to them. If people want to write lower-level modules against them, that's fine with us, but they know the interfaces might change. This has worked pretty well over the years, even with many different companies writing against those API's. @Experimental are user-facing features we are trying out. Hopefully that one is more clear. In terms of making a big jar that shades all of our dependencies - I'm curious how that would actually work in practice. It would be good to explore. There are a few potential challenges I see: 1. If any of our dependencies encode class name information in IPC messages, this would break. E.g. can you definitely shade the Hadoop client, protobuf, hbase client, etc and have them send messages over the wire? This could break things if class names are ever encoded in a wire format. 2. Many libraries like logging subsystems, configuration systems, etc rely on static state and initialization. I'm not totally sure how e.g. slf4j initializes itself if you have both a shaded and non-shaded copy of slf4j present. 3. This would mean the spark-core jar would be really massive because it would inline all of our deps. We've actually been thinking of avoiding the current assembly jar approach because, due to scala specialized classes, our assemblies now have more than 65,000 class files in them leading to all kinds of bad issues. We'd have to stick with a big uber assembly-like jar if we decide to shade stuff. 4. I'm not totally sure how this would work when people want to e.g. build Spark with different Hadoop versions. Would we publish different shaded uber-jars for every Hadoop version? Would the Hadoop dep just not be shaded... if so what about all it's dependencies. Anyways just some things to consider... simplifying our classpath is definitely an avenue worth exploring! Can you look at the logs from the executor or in the UI? They should give an exception with the reason for the task failure. Also in the future, for this type of e-mail please only e-mail the "user@" list and not both lists. - Patrick One other consideration popped into my head: 5. Shading our dependencies could mess up our external API's if we ever return types that are outside of the spark package because we'd then be returned shaded types that users have to deal with. E.g. where before we returned an o.a.flume.AvroFlumeEvent we'd have to return a some.namespace.AvroFlumeEvent. Then users downstream would have to deal with converting our types into the correct namespace if they want to inter-operate with other libraries. We generally try to avoid ever returning types from other libraries, but it would be good to audit our API's and see if we ever do this. This is a false error message actually - the Maven build no longer requires SCALA_HOME but the message/check was still there. This was fixed recently in master: https://github.com/apache/spark/commit/d8c005d5371f81a2a06c5d27c7021e1ae43d7193 I can back port that fix into branch-1.0 so it will be in 1.0.1 as well. For other people running into this, you can export SCALA_HOME to any value and it will work. - Patrick I went ahead and created a JIRA for this and back ported the improvement into branch-1.0. This wasn't a regression per-se because the behavior existed in all previous versions, but it's annoying behavior so best to fix it. https://issues.apache.org/jira/browse/SPARK-1984 - Patrick Yeah - check out sparkPreviousArtifact in the build: https://github.com/apache/spark/blob/master/project/SparkBuild.scala#L325 - Patrick Hey All, I wanted to announce the the Spark 1.1 release window: June 1 - Merge window opens July 25 - Cut-off for new pull requests August 1 - Merge window closes (code freeze), QA period starts August 15+ - RC's and voting This is consistent with the "3 month" release cycle we are targeting. I'd really encourage people submitting larger features to do so during the month of June, as features submitted closer to the window closing could end up getting pushed into the next release. I wanted to reflect a bit as well on the 1.0 release. First, thanks to everyone who was involved in this release. It was the largest release ever and it's something we should all be proud of. In the 1.0 release, we cleaned up and consolidated several parts of the Spark code base. In particular, we  consolidated the previously fragmented process of submitting Spark jobs across a wide variety of environments {YARN/Mesos/Standalone, Windows/Unix, Python/Java/Scala}. We also brought the three language API's into much closer alignment. These were difficult (but critical) tasks towards having a stable deployment environment on which higher level libraries can build. These cross-cutting changes also had associated test burden resulting in an extended QA period. The 1.1, 1.2, 1.3, family of releases are intended to be smaller releases, and I'd like to deliver them with very predictable timing to the community. This will mean being fairly strict about freezes and investing in QA infrastructure to allow us to get through voting more quickly. With 1.0 shipped, now is a great time to catch up on code reviews and look at outstanding patches. Despite the large queue, we've actually been consistently merging/closing about 80% of proposed PR's, which is definitely good (for instance, we have 170 outstanding out of 950 proposed), but there remain a lot of people waiting on reviews, and it's something everyone can help with! Thanks again to everyone involved. Looking forward to more great releases! - Patrick Received! It should be 1.1-SNAPSHOT. Feel free to submit a PR to clean up any inconsistencies. Hey There, The best way is to use the v1.0.0 tag: https://github.com/apache/spark/releases/tag/v1.0.0 - Patrick Hey Rahul, The v1.0.0 tag is correct. When we release Spark we create multiple candidates. One of the candidates is promoted to the full release. So rc11 is also the same as the official v1.0.0 release. - Patrick Hey All, Some people may have noticed PR failures due to binary compatibility checks. We've had these enabled in several of the sub-modules since the 0.9.0 release but we've turned them on in Spark core post 1.0.0 which has much higher churn. The checks are based on the "migration manager" tool from Typesafe. One issue is that tool doesn't support package-private declarations of classes or methods. Prashant Sharma has built instrumentation that adds partial support for package-privacy (via a workaround) but since there isn't really native support for this in MIMA we are still finding cases in which we trigger false positives. In the next week or two we'll make it a priority to handle more of these false-positive cases. In the mean time users can add manual excludes to: project/MimaExcludes.scala to avoid triggering warnings for certain issues. This is definitely annoying - sorry about that. Unfortunately we are the first open source Scala project to ever do this, so we are dealing with uncharted territory. Longer term I'd actually like to see us just write our own sbt-based tool to do this in a better way (we've had trouble trying to extend MIMA itself, it e.g. has copy-pasted code in it from an old version of the scala compiler). If someone in the community is a Scala fan and wants to take that on, I'm happy to give more details. - Patrick Paul, Could you give the version of Java that you are building with and the version of Java you are running with? Are they the same? Just off the cuff, I wonder if this is related to: https://issues.apache.org/jira/browse/SPARK-1520 If it is, it could appear that certain functions are not in the jar because they go beyond the extended zip boundary `jar tvf` won't list them. - Patrick Also I should add - thanks for taking time to help narrow this down! Just a heads up - due to an outage at UCB we've lost several of the Jenkins slaves. I'm trying to spin up new slaves on EC2 in order to compensate, but this might fail some ongoing builds. The good news is if we do get it working with EC2 workers, then we will have burst capability in the future - e.g. on release deadlines. So it's not all bad! - Patrick No luck with this tonight - unfortunately our Python tests aren't working well with Python 2.6 and some other issues made it hard to get the EC2 worker up to speed. Hopefully we can have this up and running tomororw. - Patrick Hey just to update people - as of around 1pm PT we were back up and running with Jenkins slaves on EC2. Sorry about the disruption. - Patrick Out of curiosity - are you guys using speculation, shuffle consolidation, or any other non-default option? If so that would help narrow down what's causing this corruption. Just wondering, do you get this particular exception if you are not consolidating shuffle data? I'll make a comment on the JIRA - thanks for reporting this, let's get to the bottom of it. Those are pretty old - but I think the reason Matei did that was to make it less confusing for brand new users. `spark` is actually a valid identifier because it's just a variable name (val spark = new SparkContext()) but I agree this could be confusing for users who want to drop into the shell. Hey All, 1. The original test infrastructure hosted by the AMPLab has been fully restored and also expanded with many more executor slots for tests. Thanks to Matt Massie at the Amplab for helping with this. 2. We now have a nightly build matrix across different Hadoop versions. It appears that the Maven build is failing tests with some of the newer Hadoop versions. If people from the community are interested, diagnosing and fixing test issues would be welcome patches (they are all dependency related). https://issues.apache.org/jira/browse/SPARK-2232 3. Prashant Sharma has spent a lot of time to make it possible for our sbt build to read dependencies from Maven. This will save us a huge amount of headache keeping the builds consistent. I just wanted to give a heads up to users about this - we should retain compatibility with features of the sbt build, but if you are e.g. hooking into deep internals of our build it may affect you. I'm hoping this can be updated and merged in the next week: https://github.com/apache/spark/pull/77 4. We've moved most of the documentation over to recommending users build with Maven when creating official packages. This is just to provide a single "reference build" of Spark since it's the one we test and package for releases, we make sure all recursive dependencies are correct, etc. I'd recommend that all downstream packagers use this build. For day-to-day development I imagine sbt will remain more popular (repl, incremental builds, etc). Prashant's work allows us to get the "best of both worlds" which is great. - Patrick Please vote on releasing the following candidate as Apache Spark version 1.0.1! The tag to be voted on is v1.0.1-rc1 (commit 7feeda3): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7feeda3d729f9397aa15ee8750c01ef5aa601962 The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.0.1-rc1/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1020/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.0.1-rc1-docs/ Please vote on releasing this package as Apache Spark 1.0.1! The vote is open until Monday, June 30, at 03:00 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.0.1 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ === About this release === This release fixes a few high-priority bugs in 1.0 and has a variety of smaller fixes. The full list is here: http://s.apache.org/b45. Some of the more visible patches are: SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame size. SPARK-1790: Support r3 instance types on EC2. This is the first maintenance release on the 1.0 line. We plan to make additional maintenance releases as new fixes come in. - Patrick Hey All, We're going to move onto another rc because of this vote. Unfortunately with the summit activities I haven't been able to usher in the necessary patches and cut the RC. I will do so as soon as possible and we can commence official voting. - Patrick Do those also happen if you run other hadoop versions (e.g. try 1.0.4)? Yeah I created a JIRA a while back to piggy-back the map status info on top of the task (I honestly think it will be a small change). There isn't a good reason to broadcast the entire array and it can be an issue during large shuffles. - Patrick I don't understand problem a.1 is. In this case, we don't need to do caching, right? This would add only a small, constant amount of data to the task. It's strictly better than before. Before if the map output status array was size M x R, we send a single akka message to every node of size M x R... this basically scales quadratically with the size of the RDD. The new approach is constant... it's much better. And the total amount of data send over the wire is likely much less. - Patrick Just a reminder here - we'll soon be merging a patch that changes the SBT build internals significantly. We've tried to make this fully backwards compatible, but there may be issues (which we'll resolve as they arrive). https://github.com/apache/spark/pull/77 - Patrick Sorry all, I sent the wrong pull request to refer to Prashant's work: https://github.com/apache/spark/pull/772 Please vote on releasing the following candidate as Apache Spark version 1.0.1! The tag to be voted on is v1.0.1-rc1 (commit 7d1043c): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7d1043c99303b87aef8ee19873629c2bfba4cc78 The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.0.1-rc2/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1021/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.0.1-rc2-docs/ Please vote on releasing this package as Apache Spark 1.0.1! The vote is open until Monday, July 07, at 20:45 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.0.1 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ === Differences from RC1 === This release includes only one "blocking" patch from rc1: https://github.com/apache/spark/pull/1255 There are also smaller fixes which came in over the last week. === About this release === This release fixes a few high-priority bugs in 1.0 and has a variety of smaller fixes. The full list is here: http://s.apache.org/b45. Some of the more visible patches are: SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame size. SPARK-1790: Support r3 instance types on EC2. This is the first maintenance release on the 1.0 line. We plan to make additional maintenance releases as new fixes come in. This vote is cancelled in favor of RC2. Thanks to everyone who voted. I'll start the voting with a +1 - ran tests on the release candidate and ran some basic programs. RC1 passed our performance regression suite, and there are no major changes from that RC. Just a heads up - I've added some better Jenkins integration that posts more useful messages on pull requests. We'll run this side-by-side with the current Jenkins messages for a while to make sure it's working well. Things may be a bit chatty while we are testing this - we can migrate over as soon as we feel it's stable. - Patrick Hey Gary, The vote technically doesn't close until I send the vote summary e-mail, but I was planning to close and package this tonight. It's too bad if there is a regression, it might be worth holding the release but it really requires narrowing down the issue to get more information about the scope and severity. Could you fork another thread for this? - Patrick There are two differences: 1. We publish hive with a shaded protobuf dependency to avoid conflicts with some Hadoop versions. 2. We publish a proper hive-exec jar that only includes hive packages. The upstream version of hive-exec bundles a bunch of other random dependencies in it which makes it really hard for third-party projects to use it. Hey Gary, Why do you think the akka frame size changed? It didn't change - we added some fixes for cases where users were setting non-default values. Okay just FYI - I'm closing this vote since many people are waiting on the release and I was hoping to package it today. If we find a reproducible Mesos issue here, we can definitely spin the fix into a subsequent release. This vote has passed with 9 +1 votes (5 binding) and 1 -1 vote (0 binding). +1: Patrick Wendell* Mark Hamstra* DB Tsai Krishna Sankar Soren Macbeth Andrew Or Matei Zaharia* Xiangrui Meng* Tom Graves* 0: -1: Gary Malouf I am happy to announce the availability of Spark 1.0.1! This release includes contributions from 70 developers. Spark 1.0.0 includes fixes across several areas of Spark, including the core API, PySpark, and MLlib. It also includes new features in Spark's (alpha) SQL library, including support for JSON data and performance and stability fixes. Visit the release notes[1] to read about this release or download[2] the release today. [1] http://spark.apache.org/releases/spark-release-1-0-1.html [2] http://spark.apache.org/downloads.html I don't think there is a class in Spark named ExecuterStatus (sic) ... or ExecutorStatus. Is this a class you made? Hey Cody, This Jstack seems truncated, would you mind giving the entire stack trace? For the second thread, for instance, we can't see where the lock is being acquired. - Patrick Hey Nishkam, Aaron's fix should prevent two concurrent accesses to getJobConf (and the Hadoop code therein). But if there is code elsewhere that tries to mutate the configuration, then I could see how we might still have the ConcurrentModificationException. I looked at your patch for HADOOP-10456 and the only example you give is of the data being accessed inside of getJobConf. Is it accessed somewhere else too from Spark that you are aware of? https://issues.apache.org/jira/browse/HADOOP-10456 - Patrick Andrew and Gary, Would you guys be able to test https://github.com/apache/spark/pull/1409/files and see if it solves your problem? - Patrick Adding new build modules is pretty high overhead, so if this is a case where a small amount of duplicated code could get rid of the dependency, that could also be a good short-term option. - Patrick Yeah - this is likely caused by SPARK-2471. Thanks DB, Feel free to file sub-jira's under: https://issues.apache.org/jira/browse/SPARK-2487 I've been importing the Maven build into Intellij, it might be worth trying that as well to see if it works. - Patrick Cody - did you mean to send this to the spark dev list? Hey Andrew, Cloning the conf this might be a good/simple fix for this particular problem. It's definitely worth looking into. There are a few things we can probably do in Spark to deal with non-thread-safety inside of the Hadoop FileSystem and Configuration classes. One thing we can do in general is to add barriers around the locations where we knowingly access Hadoop FileSystem and Configuration state from multiple threads (e.g. during our own calls to getRecordReader in this case). But this will only deal with "writer writer" conflicts where we had multiple calls mutating the same object at the same time. It won't deal with "reader writer" conflicts where some of our initialization code touches state that is needed during normal execution of other tasks. - Patrick Hey Andrew, I think you are correct and a follow up to SPARK-2521 will end up fixing this. The desing of SPARK-2521 automatically broadcasts RDD data in tasks and the approach creates a new copy of the RDD and associated data for each task. A natural follow-up to that patch is to stop handling the jobConf separately (since we will now broadcast all referents of the RDD itself) and just have it broadcasted with the RDD. I'm not sure if Reynold plans to include this in SPARK-2521 or afterwards, but it's likely we'd do that soon. - Patrick Hey Stephen, The only change the build was that we ask users to run -Phive and -Pyarn of --with-hive and --with-yarn (which internally just set -Phive and -Pyarn). I don't think this should affect the dependency graph. Just to test this, what happens if you run *without* the CDH profile and build with hadoop version 2.3.0? Does that work? - Patrick +1 - Looked through the release commits - Looked through JIRA issues - Ran the audit tests (one issue with the maven app test, but was also an issue with 0.9.1 so I think it's my environment) - Checked sigs/sums Just a small note, today I committed a tool that will automatically mirror pull requests to JIRA issues, so contributors will no longer have to manually post a pull request on the JIRA when they make one. It will create a "link" on the JIRA and also make a comment to trigger an e-mail to people watching. This should make some things easier, such as avoiding accidental duplicate effort on the same JIRA. - Patrick Yeah it needs to have SPARK-XXX in the title (this is the format we request already). It just works with small synchronization script I wrote that we run every five minutes on Jeknins that uses the Github and Jenkins API: https://github.com/apache/spark/commit/49e472744951d875627d78b0d6e93cd139232929 - Patrick Just a note - there was a fix in branch-1.0 (and Spark 1.0.1) that introduced a new bug worse than the original one. https://issues.apache.org/jira/browse/SPARK-1199 The original bug was an issue with case classes defined in the repl. The fix caused a separate bug which broke most compound statements in the repl (val x = 1; val y = x + 1). I've reverted the original SPARK-1199 fix in the 1.0 branch. Since repl changes are some of the riskier ones in Spark, I'm planning to leave this fix out of 1.0.X entirely. The final, correct fix for this will appear in Spark 1.1. - Patrick I've always operated under the assumption that if a commiter makes a comment on a PR, and that's not addressed, that should block the PR from being merged (even without a specific "-1"). I don't know of any cases where this has intentionally been violated, but I do think this happens accidentally some times. Unfortunately, we are not allowed to use those github hooks because of the way the ASF github integration works. I've lately been using a custom-made tool to help review pull requests. One thing I could do is add a feature here saying which committers have said LGTM on a PR (vs the ones that have commented). We could also indicate the latest test status as Green/Yellow/Red based on the Jenkins comments: http://pwendell.github.io/spark-github-shim/ As a warning to potential users, my tool might crash your browser. - Patrick Shivaram, You should take a look at this patch which adds support for naming accumulators - this is likely to get merged in soon. I actually started this patch by supporting named TaskMetrics similar to what you have there, but then I realized there is too much semantic overlap with accumulators, so I just went that route. For instance, it would be nice if any user-defined metrics are accessible at the driver program. https://github.com/apache/spark/pull/1309 In your example, you could just define an accumulator here on the RDD and you'd see the incremental update in the web UI automatically. - Patrick What if we have a registry for accumulators, where you can access them statically by name? - Patrick The most important issue in this release is actually an ammendment to an earlier fix. The original fix caused a deadlock which was a regression from 1.0.0->1.0.1: Issue: https://issues.apache.org/jira/browse/SPARK-1097 1.0.1 Fix: https://github.com/apache/spark/pull/1273/files (had a deadlock) 1.0.2 Fix: https://github.com/apache/spark/pull/1409/files I failed to correctly label this on JIRA, but I've updated it! Yeah I agree reflection is the best solution. Whenever we do reflection we should clearly document in the code which YARN API version corresponds to which code path. I'm guessing since YARN is adding new features... we'll just have to do this over time. - Patrick Ted - technically I think you are correct, although I wouldn't recommend disabling this lock. This lock is not expensive (acquired once per task, as are many other locks already). Also, we've seen some cases where Hadoop concurrency bugs ended up requiring multiple fixes - concurrency of client access is not well tested in the Hadoop codebase since most of the Hadoop tools to not use concurrent access. So in general it's good to be conservative in what we expect of the Hadoop client libraries. If you'd like to discuss this further, please fork a new thread, since this is a vote thread. Thanks! SPARK-2677 is a long standing issue and not a regression. Also, as far as I can see there is no patch for it or clear understanding of the cause. This type of bug does not warrant holding a release. If we fix SPARK-2677 we can just make another release with the fix. I just made a note on the JIRA - I realize it might not be clear whether it was a regression, but in fact, this issue has been observed in earlier versions as well. Hey All, Just a heads up, we'll cut branch-1.1 on this Friday, August 1st. Once the release branch is cut we'll start community QA and go into the normal triage process for merging patches into that branch. For Spark core, we'll be conservative in merging things past the freeze date (e.g. high priority fixes) to ensure a healthy amount of time for testing. A key focus of this release in core is improving overall stability and resilience of Spark core. As always, I'll encourage of committers/contributors to help review patches this week to so we can get as many things in as possible. People have been quite active recently, which is great! Good luck! - Patrick Hey Ted, We always intend Spark to work with the newer Hadoop versions and encourage Spark users to use the newest Hadoop versions for best performance. We do try to be liberal in terms of supporting older versions as well. This is because many people run older HDFS versions and we want Spark to read and write data from them. So far we've been willing to do this despite some maintenance cost. The reason is that for many users it's very expensive to do a whole-sale upgrade of HDFS, but trying out new versions of Spark is much easier. For instance, some of the largest scale Spark users run fairly old or forked HDFS versions. - Patrick I'm going to revert it again - Cheng can you try to look into this? Thanks. Hey Yu, I think we could definitely put a pointer to documentation in other languages that is hosted somewhere welse, but since we are not in a position to maintain this, I'm not sure we could merge it into the mainline Spark codebase. I'd be interested to know what other projects do about this situation! - Patrick Hey Yu, I think we could definitely put a pointer to documentation in other languages that is hosted somewhere welse, but since we are not in a position to maintain this, I'm not sure we could merge it into the mainline Spark codebase. I'd be interested to know what other projects do about this situation! - Patrick It would be great if the hive team can fix that issue. If not, we'll have to continue forking our own version of Hive to change the way it publishes artifacts. - Patrick Yeah so we need a model for this (Mark - do you have any ideas?). I did this in a personal github repo. I just did it quickly because dependency issues were blocking the 1.0 release: https://github.com/pwendell/hive/tree/branch-0.12-shaded-protobuf I think what we want is to have a semi official github repo with an index to each of the shaded dependencies and what version is included in which branch. - Patrick I've heard from Cloudera that there were hive internal changes between 0.12 and 0.13 that required code re-writing. Over time it might be possible for us to integrate with hive using API's that are more stable (this is the domain of Michael/Cheng/Yin more than me!). It would be interesting to see what the Hulu folks did. - Patrick Yeah for packagers we officially recommend using maven. Spark's dependency graph is very complicated and Maven and SBT use different conflict resolution strategies, so we've opted to official support Maven. SBT is still around though and it's used more often by day-to-day developers. - Patrick https://issues.apache.org/jira/browse/INFRA-8116 Just a heads up, the github mirroring is running behind. You can follow that JIRA to keep up to date on the fix. In the mean time you can use the Apache git itself: https://git-wip-us.apache.org/repos/asf/spark.git Some people have reported issues checking out Apache git as well, but it might work. - Patrick Cody - in your example you are using the '=' character, but in our documentation and tests we use a whitespace to separate the key and value in the defaults file. docs: http://spark.apache.org/docs/latest/configuration.html spark.driver.extraJavaOptions -Dfoo.bar.baz=23 I'm not sure if the java properties file parser will try to interpret the equals sign. If so you might need to do this. spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"Do those work for you? Thanks for digging around here. I think there are a few distinct issues. 1. Properties containing the '=' character need to be escaped. I was able to load properties fine as long as I escape the '='character. But maybe we should document this: == spark-defaults.conf == spark.foo a\=B == shell == scala> sc.getConf.get("spark.foo") res2: String = a=B 2. spark.driver.extraJavaOptions, when set in the properties file, don't affect the driver when running in client mode (always the case for mesos). We should probably document this. In this case you need to either use --driver-java-options or set SPARK_SUBMIT_OPTS. 3. Arguments aren't propagated on Mesos (this might be because of the other issues, or a separate bug). - Patrick The third issue may be related to this: https://issues.apache.org/jira/browse/SPARK-2022 We can take a look at this during the bug fix period for the 1.1 release next week. If we come up with a fix we can backport it into the 1.0 branch also. This is a Scala bug - I filed something upstream, hopefully they can fix it soon and/or we can provide a work around: https://issues.scala-lang.org/browse/SI-8772 - Patrick Andrew - I think Spark is using Guava 14... are you using Guava 16 in your user app (i.e. you inverted the versions in your earlier e-mail)? - Patrick Please don't let this prevent you from merging patches, just keep a list and we can update the JIRA later. - Patrick Hey All, I'm happy to announce branch-1.1 of Spark [1] - this branch will eventually become the 1.1 release. Committers: new patches will need to be explicitly back-ported into this branch in order to appear in the 1.1 release. Thanks so much to all the committers and contributors who were extremely active on review in the last week! We'll now enter the standard triage process: -> In the next 48 hours, smaller features that are very late in review (e.g. loose ends) are okay to go in. -> On Monday we'll package a community build and start community testing. At that point we are solidly "bug fixes only" for most Spark components. -> When deemed appropriate, we'll cut a release candidate for official voting and start the process. As voting goes on, we'll eventually escalate to "regressions only" - i.e. we hold the release only for fixes that regress from earlier version. I'll also try to document the release/triage process on the wiki in a bit more detail. For those interested, here is a good example - the guidelines for the Linux kernel release process: https://www.kernel.org/doc/Documentation/development-process/2.Process Thanks everyone, looking forward to a great 1.1 release! [1] https://git-wip-us.apache.org/repos/asf?p=spark.git;a=shortlog;h=refs/heads/branch-1.1 - Patrick Great idea - I think this is easy to do given the current architecture. We already have access to the commit ID in the same script that posts the comments. 2. "Pin" a message to the start or end of the PR that is updated with This also is a good idea - I think this would be doable since the github API allows us to edit comments, but it's a bit tricker. I think it would require first making an API call to get the "status comment" ID and then updating it. Nick - Any interest in doing these? this is all doable from within the spark repo itself because our QA harness scripts are in there: https://github.com/apache/spark/blob/master/dev/run-tests-jenkins If not, could you make a JIRA for them and put it under "Project Infra". - Patrick I'll let TD chime on on this one, but I'm guessing this would be a welcome addition. It's great to see community effort on adding new streams/receivers, adding a Java API for receivers was something we did specifically to allow this :) - Patrick Hey Anand, Thanks for looking into this - it's great to see momentum towards Scala 2.11 and I'd love if this land in Spark 1.2. For the external dependencies, it would be good to create a sub-task of SPARK-1812 to track our efforts encouraging other projects to upgrade. In certain cases (e.g. Kafka) there is fairly late-stage work on this already, so we can e.g. link to those JIRA's as well. A good starting point is to just go to their dev list and ask what the status is, most Scala projects have put at least some thought into this already. Another thing we can do is submit patches ourselves to those projects to help get them upgraded. The twitter libraries, e.g., tend to be pretty small and also open to external contributions. One other thing in the mix here - Prashant Sharma has also spent some time looking at this, so it might be good for you two to connect (probably off list) and sync up. Prashant has contributed to many Scala projects, so he might have cycles to go and help some of our dependencies get upgraded - but I won't commit to that on his behalf :). Regarding Akka - I shaded and published akka as a one-off thing: https://github.com/pwendell/akka/tree/2.2.3-shaded-proto Over time we've had to publish our own versions of a small number of dependencies. It's somewhat high overhead, but it actually works quite well in terms of avoiding some of the nastier dependency conflicts. At least better than other alternatives I've seen such as using a shader build plug-in. Going forward, I'd actually like to track these in the Spark repo itself. For instance, we have a bash script in the spark repo that can e.g. check out akka, apply a few patches or regular expressions, and then you have a fully shaded dependency that can be published to maven. If you wanted to take a crack at something like that for akka 2.3.4, be my guest. I can help with the actual publishing. - Patrick Sure thing - feel free to ping me off list if you need pointers. The script just does string concatenation and a curl to post the comment... I think it should be pretty accessible! - Patrick For hortonworks, I believe it should work to just link against the corresponding upstream version. I.e. just set the Hadoop version to "2.4.0"Does that work? - Patrick Hi All, I've packaged and published a snapshot release of Spark 1.1 for testing. This is being distributed to the community for QA and preview purposes. It is not yet an official RC for voting. Going forward, we'll do preview releases like this for testing ahead of official votes. The tag of this release is v1.1.0-snapshot1 (commit d428d8): *https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d428d88418d385d1d04e1b0adcb6b068efe9c7b0 * The release files, including signatures, digests, etc can be found at: *http://people.apache.org/~pwendell/spark-1.1.0-snapshot1/ * Release artifacts are signed with the following key: *https://people.apache.org/keys/committer/pwendell.asc * The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1025/ NOTE: Due to SPARK-2899, docs are not yet available for this release. Docs will be posted ASAP. To learn more about Apache Spark, please see http://spark.apache.org/ Minor correction: the encoded URL in the staging repo link was wrong. The correct repo is: https://repository.apache.org/content/repositories/orgapachespark-1025/ In the past I've found if I do a jstack when running some tests, it sits forever inside of a hostname resolution step or something. I never narrowed it down, though. - Patrick Andrew - I think your JIRA may duplicate existing work: https://github.com/apache/spark/pull/1513 The current YARN is equivalent to what is called "fine grained" mode in Mesos. The scheduling of tasks happens totally inside of the Spark driver. Hey sorry about that - what I said was the opposite of what is true. The current YARN mode is equivalent to "coarse grained" mesos. There is no fine-grained scheduling on YARN at the moment. I'm not sure YARN supports scheduling in units other than containers. Fine-grained scheduling requires scheduling at the granularity of individual cores. I dug around this a bit a while ago, I think if someone sat down and profiled the tests it's likely we could find some things to optimize. In particular, there may be overheads in starting up a local spark context that could be minimized and speed up all the tests. Also, there are some tests (especially in Streaming) that take really long, like 60 seconds for a single test (see some of the new flume tests). These could almost certainly be optimized. I think 5 minutes might be out of reach, but something like a 2X improvement might be possible and would be very valuable if accomplished. - Patrick Josh - that was actually fixed recently (we just bind to a random port when running tests). Cheng Lian also has a fix for this. I've asked him to make a PR - he is on China time so it probably won't come until tonight: https://github.com/liancheng/spark/compare/apache:master...liancheng:spark-2894 I spent some time on this and I'm not sure either of these is an option, unfortunately. We typically can't use custom JIRA plug-in's because this JIRA is controlled by the ASF and we don't have rights to modify most things about how it works (it's a large shared JIRA instance used by more than 50 projects). It's worth looking into whether they can do something. In general we've tended to avoid going through ASF infra them whenever possible, since they are generally overloaded and things move very slowly, even if there are outages. Here is the script we use to do the sync: https://github.com/apache/spark/blob/master/dev/github_jira_sync.py It might be possible to modify this to support post-hoc changes, but we'd need to think about how to do so while minimizing function calls to the ASF JIRA API, which I found are very slow. - Patrick I commented on the bug. For driver mode, you'll need to get the corresponding version of spark-submit for Spark 1.0.2. Hi All, I noticed that all PR tests run overnight had failed due to timeouts. The patch that updates the netty shuffle I believe somehow inflated to the build time significantly. That patch had been tested, but one change was made before it was merged that was not tested. I've reverted the patch for now to see if it brings the build times back down. - Patrick We'll need to build timeouts into our own reporting infrastructure - it shouldn't be too bad but we just need to script it. Unfortunately the Jenkins plug-in is either "all or nothing" in what it reports, so we can't have it report timeouts unless we want all the other fairly noisy messages from it. Hey Mingyu, For this reason we are encouraging all users to run spark-submit. In Spark we capture closures and send them over the network from the driver to the executors. These are then deserialized on the executor. So if your driver program has different versions of certain classes than exist on the executor, it doesn't work well. We've even run into stranger issues, where the exact same version of Spark was used at the driver and the executor, but they were compiled at different times. Since Scala doesn't guarantee stable naming for certain types of anonymous classes, the class names didn't match up and it caused errors at runtime. The most straightforward way to deal with this is to inject, at run-time, the exact version of Spark that the cluster expects if you are running the standalone mode. I think we'd be totally open to improving this to provide "API stability"for the case you are working with, i.e. the case where you have spark 1.0.X at the driver and 1.0.Y on the executors. But it will require looking at what exactly causes incompatibility and seeing if there is a solution. In this case I think we changed a publicly exposed class (the RDD class) in some way that caused compatibility issues... even though we didn't change any binary signatures. BTW - this is not relevant to YARN mode where you ship Spark with your job so there is no "cluster version of Spark". - Patrick Hey Nicholas, Yeah so Jenkins has it's own timeout mechanism and it will just kill the entire build after 120 minutes. But since run-tests is sitting in the middle of the tests, it can't actually post a failure message. I think run-tests-jenkins should just wrap the call to run-tests in a call in its own timeout. It might be possible to just use this: http://linux.die.net/man/1/timeout - Patrick Yeah I was thinking something like that. Basically we should just have a variable for the timeout and I can make sure it's under the configured Jenkins time. Hey Gary, There are couple of blockers in Spark core and SQL - but we're quite close. The goal was to have rc1 on Friday (ish) of last week... I think by tonight I will be able to cut one. If not, I'll cut a preview release tonight that does a full package but doesn't trigger an official vote yet so that people can test it. bit.ly/1tgfZrQ - Patrick Hey Deb, Can you be specific what changes you are mentioning? We have not, to my knowledge, made major architectural changes around akka use. I think in general we don't want people to be using Spark's actor system directly - it is an internal communication component in Spark and could e.g. be re-factored later to not use akka at all. Could you elaborate a bit more on your use case? - Patrick Hi All, I've packaged and published a snapshot release of Spark 1.1 for testing. This is very close to RC1 and we are distributing it for testing. Please test this and report any issues on this thread. The tag of this release is v1.1.0-snapshot1 (commit e1535ad3): *https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=e1535ad3c6f7400f2b7915ea91da9c60510557ba * The release files, including signatures, digests, etc can be found at: *http://people.apache.org/~pwendell/spark-1.1.0-snapshot2/ * Release artifacts are signed with the following key: *https://people.apache.org/keys/committer/pwendell.asc * The staging repository for this release can be found at: [NOTE: Apache Sonatype is down preventing us from cutting this] https://repository.apache.org/content/repositories/orgapachespark-1026/ To learn more about Apache Spark, please see http://spark.apache.org/ The docs for this release are also available here: http://people.apache.org/~pwendell/spark-1.1.0-snapshot2-docs/ Hey All, We can sort this out ASAP. Many of the Spark committers were at a company offsite for the last 72 hours, so sorry that it is broken. - Patrick One other idea - when things freeze up, try to run jstack on the spark shell process and on the executors and attach the results. It could be that somehow you are encountering a deadlock somewhere. Hey Nicholas, That seems promising - I prefer having a proper link to having that fairly verbose comment though, because in some cases there will be dozens of comments and it could get lost. I wonder if they could do something where it posts a link instead... - Patrick Hey Amnon, So just to make sure I understand - you also saw the same issue with 1.0.2? Just asking because whether or not this regresses the 1.0.2 behavior is important for our own bug tracking. - Patrick Hey Nicholas, Thanks for bringing this up. There are a few dimensions to this... one is that it's actually precedurally difficult for us to close pull requests. I've proposed several different solutions to ASF infra to streamline the process, but thus far they haven't been open to any of my ideas: https://issues.apache.org/jira/browse/INFRA-7918 https://issues.apache.org/jira/browse/INFRA-8241 The more important thing, maybe, is how we want to deal with this culturally. And I think we need to do a better job of making sure no pull requests go unattended (i.e. waiting for committer feedback). If patches go stale, it should be because the user hasn't responded, not us. Another thing is that we should, IMO, err on the side of explicitly saying "no" or "not yet" to patches, rather than letting them linger without attention. We do get patches where the user is well intentioned, but it is a feature that doesn't make sense to add, or isn't well thought out or explained, or the review effort would be so large it's not within our capacity to look at just yet. Most other ASF projects I know just ignore these patches. I'd prefer if we took the approach of politely explaining why in the current form the patch isn't acceptable and closing it (potentially w/ tips on how to improve it or narrow the scope). - Patrick Hi All, I want to invite users to submit to the Spark "Powered By" page. This page is a great way for people to learn about Spark use cases. Since Spark activity has increased a lot in the higher level libraries and people often ask who uses each one, we'll include information about which components each organization uses as well. If you are interested, simply respond to this e-mail (or e-mail me off-list) with: 1) Organization name 2) URL 3) Which Spark components you use: Core, SQL, Streaming, MLlib, GraphX 4) A 1-2 sentence description of your use case. I'll post any new entries here: https://cwiki.apache.org/confluence/display/SPARK/Powered+By+Spark - Patrick Hey Nishkam, To some extent we already have this process - many community members help review patches and some earn a reputation where committer's will take an LGTM from them seriously. I'd be interested in seeing if any other projects recognize people who do this. - Patrick Please vote on releasing the following candidate as Apache Spark version 1.1.0! The tag to be voted on is v1.1.0-rc1 (commit f0718324): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=f07183249b74dd857069028bf7d570b35f265585 The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.1.0-rc1/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1028/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.1.0-rc1-docs/ Please vote on releasing this package as Apache Spark 1.1.0! The vote is open until Sunday, August 31, at 17:00 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.1.0 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ == What justifies a -1 vote for this release? == This vote is happening very late into the QA period compared with previous votes, so -1 votes should only occur for significant regressions from 1.0.2. Bugs already present in 1.0.X will not block this release. == What default changes should I be aware of? == 1. The default value of "spark.io.compression.codec" is now "snappy"--> Old behavior can be restored by switching to "lzf"2. PySpark now performs external spilling during aggregations. --> Old behavior can be restored by setting "spark.shuffle.spill" to "false". I'll send a bit more later today with feature information for the release. In the mean time I want to put this out there for consideration. - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Mridul - thanks for sending this along and for the debugging comments on the JIRA. I think we have a handle on the issue and we'll patch it and spin a new RC. We can also update the test coverage to cover LZ4. - Patrick Okay I'm cancelling this vote in favor of RC2. Please vote on releasing the following candidate as Apache Spark version 1.1.0! The tag to be voted on is v1.1.0-rc2 (commit 711aebb3): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=711aebb329ca28046396af1e34395a0df92b5327 The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.1.0-rc2/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1029/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.1.0-rc2-docs/ Please vote on releasing this package as Apache Spark 1.1.0! The vote is open until Monday, September 01, at 03:11 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.1.0 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ == Regressions fixed since RC1 == LZ4 compression issue: https://issues.apache.org/jira/browse/SPARK-3277 == What justifies a -1 vote for this release? == This vote is happening very late into the QA period compared with previous votes, so -1 votes should only occur for significant regressions from 1.0.2. Bugs already present in 1.0.X will not block this release. == What default changes should I be aware of? == 1. The default value of "spark.io.compression.codec" is now "snappy"--> Old behavior can be restored by switching to "lzf"2. PySpark now performs external spilling during aggregations. --> Old behavior can be restored by setting "spark.shuffle.spill" to "false". --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I'll kick off the vote with a +1. Hey Sean, The reason there are no longer CDH-specific builds is that all newer versions of CDH and HDP work with builds for the upstream Hadoop projects. I dropped CDH4 in favor of a  newer Hadoop version (2.4) and the Hadoop-without-Hive (also 2.4) build. For MapR - we can't officially post those artifacts on ASF web space when we make the final release, we can only link to them as being hosted by MapR specifically since they use non-compatible licenses. However, I felt that providing these during a testing period was alright, with the goal of increasing test coverage. I couldn't find any policy against posting these on personal web space during RC voting. However, we can remove them if there is one. Dropping CDH4 was more because it is now pretty old, but we can add it back if people want. The binary packaging is a slightly separate question from release votes, so I can always add more binary packages whenever. And on this, my main concern is covering the most popular Hadoop versions to lower the bar for users to build and test Spark. - Patrick Yeah, we can't/won't post MapR binaries on the ASF web space for the release. However, I have been linking to them (at their request) with a clear identifier that it is an incompatible license and a 3rd party build. The only "vendor specific" build property we provide is compatibility with different Hadoop FileSystem clients, since unfortunately there is not a universally adopted client/server protocol. I think our goal has always been to provide a path for using "ASF Spark" with vendor-specific filesystems. Some vendors perform backports or enhancements... and this of course we would never want to manage in the upstream project. In terms of vendor support for this approach - In the early days Cloudera asked us to add CDH4 repository and more recently Pivotal and MapR also asked us to allow linking against their hadoop-client libraries. So we've added these based on direct requests from vendors. Given the ubiquity of the Hadoop FileSystem API, it's hard for me to imagine ruffling feathers by supporting this. But if we get feedback in that direction over time we can of course consider a different approach. - Patrick In some cases IntelliJ's Scala compiler can't compile valid Scala source files. Hopefully they fix (or have fixed) this in a newer version. - Patrick Thanks to Nick Chammas and Cheng Lian who pointed out two issues with the release candidate. I'll cancel this in favor of RC3. Please vote on releasing the following candidate as Apache Spark version 1.1.0! The tag to be voted on is v1.1.0-rc3 (commit b2d0493b): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=b2d0493b223c5f98a593bb6d7372706cc02bebad The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.1.0-rc3/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1030/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.1.0-rc3-docs/ Please vote on releasing this package as Apache Spark 1.1.0! The vote is open until Tuesday, September 02, at 23:07 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.1.0 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ == Regressions fixed since RC1 == - Build issue for SQL support: https://issues.apache.org/jira/browse/SPARK-3234 - EC2 script version bump to 1.1.0. == What justifies a -1 vote for this release? == This vote is happening very late into the QA period compared with previous votes, so -1 votes should only occur for significant regressions from 1.0.2. Bugs already present in 1.0.X will not block this release. == What default changes should I be aware of? == 1. The default value of "spark.io.compression.codec" is now "snappy"--> Old behavior can be restored by switching to "lzf"2. PySpark now performs external spilling during aggregations. --> Old behavior can be restored by setting "spark.shuffle.spill" to "false". --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org For my part I'm +1 on this, though Sean it would be great separately to fix the test environment. For those who voted on rc2, this is almost identical, so feel free to +1 unless you think there are issues with the two minor bug fixes. Yeah, this wasn't detected in our performance tests. We even have a test in PySpark that I would have though might catch this (it just schedules a bunch of really small tasks, similar to the regression case). https://github.com/databricks/spark-perf/blob/master/pyspark-tests/tests.py#L51 Anyways, Josh is trying to repro the regression to see if we can figure out what is going on. If we find something for sure we should add a test. Hey Shane, Thanks for your work so far and I'm really happy to see investment in this infrastructure. This is a key productivity tool for us and something we'd love to expand over time to improve the development process of Spark. - Patrick Thanks everyone for voting on this. There were two minor issues (one a blocker) were found that warrant cutting a new RC. For those who voted +1 on this release, I'd encourage you to +1 rc4 when it comes out unless you have been testing issues specific to the EC2 scripts. This will move the release process along. SPARK-3332 - Issue with tagging in EC2 scripts SPARK-3358 - Issue with regression for m3.XX instances - Patrick I'm cancelling this release in favor of RC4. Happy voting! Please vote on releasing the following candidate as Apache Spark version 1.1.0! The tag to be voted on is v1.1.0-rc4 (commit 2f9b2bd): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=2f9b2bd7844ee8393dc9c319f4fefedf95f5e460 The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.1.0-rc4/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1031/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.1.0-rc4-docs/ Please vote on releasing this package as Apache Spark 1.1.0! The vote is open until Saturday, September 06, at 08:30 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.1.0 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ == Regressions fixed since RC3 == SPARK-3332 - Issue with tagging in EC2 scripts SPARK-3358 - Issue with regression for m3.XX instances == What justifies a -1 vote for this release? == This vote is happening very late into the QA period compared with previous votes, so -1 votes should only occur for significant regressions from 1.0.2. Bugs already present in 1.0.X will not block this release. == What default changes should I be aware of? == 1. The default value of "spark.io.compression.codec" is now "snappy"--> Old behavior can be restored by switching to "lzf"2. PySpark now performs external spilling during aggregations. --> Old behavior can be restored by setting "spark.shuffle.spill" to "false". 3. PySpark uses a new heuristic for determining the parallelism of shuffle operations. --> Old behavior can be restored by setting "spark.default.parallelism" to the number of cores in the cluster. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I'll kick it off with a +1 Hey Nick, Yeah we'll put those in the release notes. Hm yeah it seems that it hasn't been polling since 3:45. Hey There, I believe this is on the roadmap for the 1.2 next release. But Xiangrui can comment on this. - Patrick This vote passes with 8 binding +1 votes and no -1 votes. I'll post the final release in the next 48 hours... just finishing the release notes and packaging (which now takes a long time given the number of contributors!). +1: Reynold Xin* Michael Armbrust* Xiangrui Meng* Andrew Or* Sean Owen Matthew Farrellee Marcelo Vanzin Josh Rosen* Cheng Lian Mubarak Seyed Matei Zaharia* Nan Zhu Jeremy Freeman Denny Lee Tom Graves* Henry Saputra Egor Pahomov Rohit Sinha Kan Zhang Tathagata Das* Reza Zadeh -1: 0: * = binding --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi Everyone, This is a call to the community for comments on SPARK-3445 [1]. In a nutshell, we are trying to figure out timelines for deprecation of the YARN-alpha API's as Yahoo is now moving off of them. It's helpful for us to have a sense of whether anyone else uses these. Please comment on the JIRA if you have feeback, thanks! [1] https://issues.apache.org/jira/browse/SPARK-3445 - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I think what Michael means is people often use this to read existing partitioned Parquet tables that are defined in a Hive metastore rather than data generated directly from within Spark and then reading it back as a table. I'd expect the latter case to become more common, but for now most users connect to an existing metastore. I think you could go this route by creating a partitioned external table based on the on-disk layout you create. The downside is that you'd have to go through a hive metastore whereas what you are doing now doesn't need hive at all. We should also just fix the case you are mentioning where a union is used directly from within spark. But that's the context. - Patrick Hey just a heads up to everyone - running a bit behind on getting the final artifacts and notes up. Finalizing this release was much more complicated than previous ones due to new binary formats (we need to redesign the download page a bit for this to work) and the large increase in contributor count. Next time we can pipeline this work to avoid a delay. I did cut the v1.1.0 tag today. We should be able to do the full announce tomorrow. Thanks, Patrick I am happy to announce the availability of Spark 1.1.0! Spark 1.1.0 is the second release on the API-compatible 1.X line. It is Spark's largest release ever, with contributions from 171 developers! This release brings operational and performance improvements in Spark core including a new implementation of the Spark shuffle designed for very large scale workloads. Spark 1.1 adds significant extensions to the newest Spark modules, MLlib and Spark SQL. Spark SQL introduces a JDBC server, byte code generation for fast expression evaluation, a public types API, JSON support, and other features and optimizations. MLlib introduces a new statistics library along with several new algorithms and optimizations. Spark 1.1 also builds out Spark's Python support and adds new components to the Spark Streaming module. Visit the release notes [1] to read about the new features, or download [2] the release today. [1] http://spark.eu.apache.org/releases/spark-release-1-1-0.html [2] http://spark.eu.apache.org/downloads.html NOTE: SOME ASF DOWNLOAD MIRRORS WILL NOT CONTAIN THE RELEASE FOR SEVERAL HOURS. Please e-mail me directly for any type-o's in the release notes or name listing. Thanks, and congratulations! - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org [moving to user@] This would typically be accomplished with a union() operation. You can't mutate an RDD in-place, but you can create a new RDD with a union() which is an inexpensive operator. We typically post design docs on JIRA's before major work starts. For instance, pretty sure SPARk-1856 will have a design doc posted shortly. Hey All, Wanted to send a quick update about test infrastructure. With the number of contributors we have and the rate of development, maintaining a well-oiled test infra is really important. Every time a flaky test fails a legitimate pull request, it wastes developer time and effort. 1. Master build: Spark's master builds are back to green again in Maven and SBT after a long time of instability. Big thanks to Josh Rosen, Andrew Or, Nick Chammas, Shane Knapp, Sean Owen, and many others who were involved in pinpointing and fixing fairly convoluted test failure issues. 2. Jenkins PRB: The Jenkins Pull Request Builder is mostly functioning again. However, we are working on a simpler technical pipeline for testing patches, as this plug-in has been a constant source of downtime and issues for us, and is very hard to debug. 3. Reverting flaky patches: Going forward - we may revert patches that seem to be the root cause of flaky or failing tests. This is necessary as these days, the test infra being down will block something like 10-30 in-flight patches on a given day. This puts the onus back on the test writer to try and figure out what's going on - we'll of course help debug the issue! 4. Time of tests: With hundreds (thousands?) of tests, we will have a very high bar for tests which take several seconds or longer. Things like Thread.sleep() bloat test time when proper synchronization mechanisms should be used. Expect reviewers to push back on any long-running tests, in many cases they can be re-written to be both shorter and better. Thanks again to everyone putting in effort on this, we've made a ton of progress in the last few weeks. A solid test infra will help us scale and move quickly as Spark development continues to accelerate. - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hey Otis, Could you describe a bit more about what your program is. Is it an open source project? A product? This would help understand a bit where it should go. - Patrick Hi Mohit, Welcome to the Spark community! We normally look at feature proposals using github pull requests mind submitting one? The contribution process is covered here: https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark Yes - I believe we use the local dirs for spilling as well. Hey the numbers you mentioned don't quite line up - did you mean PR 2711? Ah I see it was SPARK-2711 (and PR1707). In that case, it's possible that you are just having more spilling as a result of the patch and so the filesystem is opening more files. I would try increasing the ulimit. How much memory do your executors have? - Patrick Hey Cody, In terms of Spark 1.1.1 - we wouldn't change a default value in a spot release. Changing this to default is slotted for 1.2.0: https://issues.apache.org/jira/browse/SPARK-3280 - Patrick Have you considered running the mima checks locally? We prefer people not use Jenkins for very frequent checks since it takes resources away from other people trying to run tests. Yeah we can also move it first. Wouldn't hurt. Hey Nick, We can always take built-in rules. Back when we added this Prashant Sharma actually did some great work that lets us write our own style rules in cases where rules don't exist. You can see some existing rules here: https://github.com/apache/spark/tree/master/project/spark-style/src/main/scala/org/apache/spark/scalastyle Prashant has over time contributed a lot of our custom rules upstream to stalastyle, so now there are only a couple there. - Patrick Hey All, Just a couple notes. I recently posted a shell script for creating the AMI's from a clean Amazon Linux AMI. https://github.com/mesos/spark-ec2/blob/v3/create_image.sh I think I will update the AMI's soon to get the most recent security updates. For spark-ec2's purpose this is probably sufficient (we'll only need to re-create them every few months). However, it would be cool if someone wanted to tackle providing a more general mechanism for defining Spark-friendly "images" that can be used more generally. I had thought that docker might be a good way to go for something like this - but maybe this packer thing is good too. For one thing, if we had a standard image we could use it to create containers for running Spark's unit test, which would be really cool. This would help a lot with random issues around port and filesystem contention we have for unit tests. I'm not sure if the long term place for this would be inside the spark codebase or a community library or what. But it would definitely be very valuable to have if someone wanted to take it on. - Patrick Actually - weirdly - we can delete old tags and it works with the mirroring. Nick if you put together a list of un-needed tags I can delete them. Another big problem with these patches are that they make it almost impossible to backport changes to older branches cleanly (there becomes like 100% chance of a merge conflict). One proposal is to do this: 1. We only consider new style rules at the end of a release cycle, when there is the smallest chance of wanting to backport stuff. 2. We require that they are submitted in individual patches with a (a) new style rule and (b) the associated changes. Then we can also evaluate on a case-by-case basis how large the change is for each rule. For rules that require sweeping changes across the codebase, personally I'd vote against them. For rules like import ordering that won't cause too much pain on the diff (it's pretty easy to deal with those conflicts) I'd be okay with it. If we went with this, we'd also have to warn people that we might not accept new style rules if they are too costly to enforce. I'm guessing people will still contribute even with those expectations. - Patrick There is a deeper issue here which is AFAIK we don't even store a notion of attempt inside of Spark, we just use a new taskId with the same index. The failure is in the Kinesis compoent, can you reproduce this if you build with -Pkinesis-asl? - Patrick Thanks Shane - we should fix the source code issues in the Kinesis code that made stricter Java compilers reject it. - Patrick I created an issue to fix this: https://issues.apache.org/jira/browse/SPARK-4021 Josh - the errors that broke our build indicated that JDK5 was being used. Somehow the upgrade caused our build to use a much older Java version. See the JIRA for more details. The best documentation about communication interfaces is the SecurityManager doc written by Tom Graves. With this as a starting point I'd recommend digging through the code for each component. https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SecurityManager.scala#L59 Hey Koert, I think disabling the style checks in maven package could be a good idea for the reason you point out. I was sort of mixed on that when it was proposed for this exact reason. It's just annoying to developers. In terms of changing the global limit, this is more religion than anything else, but there are other cases where the current limit is useful (e.g. if you have many windows open in a large screen). - Patrick Hey All, Just a reminder that as planned [1] we'll go into a feature freeze on November 1. On that date I'll cut a 1.2 release branch and make the up-or-down call on any patches that go into that branch, along with individual committers. It is common for us to receive a very large volume of patches near the deadline. The highest priority will be fixes and features that are in review and were submitted earlier in the window. As a heads up, new feature patches that are submitted in the next week have a good chance of being pushed after 1.2. During this coming weeks, I'd like to invite the community to help with code review, testing patches, helping isolate bugs, our test infra, etc. In past releases, community participation has helped increase our ability to merge patches substantially. Individuals really can make a huge difference here! [1] https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Overall I think this would be a good idea. The main blocker is just that I think the Maven build is much slower right now than the SBT build. However, if we were able to e.g. parallelize the test build on Jenkins that might make up for it. I'd actually like to have a trigger where we could tests pull requests with either one. - Patrick Thanks for the update Shane. As a point of process, for things like this where we re debugging specific issues - can we use JIRA instead of notifying everyone on the spark-dev list? I'd prefer if ops/infra announcements on the dev list are restricted to things that are widely applicable to developers (e.g. planned or unplanned maintenance on jenkins), since this last has hundreds of people on it. - Patrick Does Zinc still help if you are just running a single totally fresh build? For the pull request builder we purge all state from previous builds. - Patrick Hey Cheng, Right now we aren't using stable API's to communicate with the Hive Metastore. We didn't want to drop support for Hive 0.12 so right now we are using a shim layer to support compiling for 0.12 and 0.13. This is very costly to maintain. If Hive has a stable meta-data API for talking to a Metastore, we should use that (is HCatalog sufficient for this purpose?). Ideally we would be able to talk to multiple versions of the Hive metastore and we can keep a single internal version of Hive for our use of Serde's, etc. I've created SPARK-4114 for this: https://issues.apache.org/jira/browse/SPARK-4114 This is a very important issue for Spark SQL, so I'd welcome comments on that JIRA from anyone who is familiar with Hive/HCatalog internals. - Patrick Hey Stephen, In some cases in the maven build we now have pluggable source directories based on profiles using the maven build helper plug-in. This is necessary to support cross building against different Hive versions, and there will be additional instances of this due to supporting scala 2.11 and 2.10. In these cases, you may need to add source locations explicitly to intellij if you want the entire project to compile there. Unfortunately as long as we support cross-building like this, it will be an issue. Intellij's maven support does not correctly detect our use of the maven-build-plugin to add source directories. We should come up with a good set of instructions on how to import the pom files + add the few extra source directories. Off hand I am not sure exactly what the correct sequence is. - Patrick Btw - we should have part of the official docs that describes a full "from scratch" build in IntelliJ including any gotchas. Then we can update it if there are build changes that alter it. I created this JIRA for it: https://issues.apache.org/jira/browse/SPARK-4128 I just started a totally fresh IntelliJ project importing from our root pom. I used all the default options and I added "hadoop-2.4, hive, hive-0.13.1" profiles. I was able to run spark core tests from within IntelliJ. Didn't try anything beyond that, but FWIW this worked. - Patrick Cheng - to make it recognize the new HiveShim for 0.12 I had to click on spark-hive under "packages" in the left pane, then go to "Open Module Settings" - then explicitly add the v0.12.0/src/main/scala folder to the sources by navigating to it and then +click to add it as a source. Did you have to do this? Oops - I actually should have added v0.13.0 (i.e. to match whatever I did in the profile). One thing is you need to do a "maven package" before you run tests. The "local-cluster" tests depend on Spark already being packaged. - Patrick Some of our tests actually require spinning up a small multi-process spark cluster. These use the normal deployment codepath for Spark which is that we rely on the spark "assembly jar" to be present. That jar is generated when you run "mvn package" via a special sub project called assembly in our build. This is a bit non-standard. The reason is that some of our tests are really mini integration tests. - Patrick Hey Nick, Unfortunately Citus Data didn't contact any of the Spark or Spark SQL developers when running this. It is really easy to make one system look better than others when you are running a benchmark yourself because tuning and sizing can lead to a 10X performance improvement. This benchmark doesn't share the mechanism in a reproducible way. There are a bunch of things that aren't clear here: 1. Spark SQL has optimized parquet features, were these turned on? 2. It doesn't mention computing statistics in Spark SQL, but it does this for Impala and Parquet. Statistics allow Spark SQL to broadcast small tables which can make a 10X difference in TPC-H. 3. For data larger than memory, Spark SQL often performs better if you don't call "cache", did they try this? Basically, a self-reported marketing benchmark like this that *shocker* concludes this vendor's solution is the best, is not particularly useful. If Citus data wants to run a credible benchmark, I'd invite them to directly involve Spark SQL developers in the future. Until then, I wouldn't give much credence to this or any other similar vendor benchmark. - Patrick == Short version == A recent commit replaces Spark's networking subsystem with one based on Netty rather than raw sockets. Users running off of master can disable this change by setting "spark.shuffle.blockTransferService=nio". We will be testing with this during the QA period for Spark 1.2. The new implementation is designed to increase stability and decrease GC pressure during shuffles. == Long version == For those who haven't been following the associated PR's and JIRA's: We recently merged PR #2753 which creates a "network" package which does not depend on Spark core. #2753 introduces a Netty-based BlockTransferService to replace the NIO-based ConnectionManager, used for transferring shuffle and RDD cache blocks between Executors (in other words, the transport layer of the BlockManager). The new BlockTransferService is intended to provide increased stability, decreased maintenance burden, and decreased garbage collection. By relying on Netty to take care of the low-level networking, the actual transfer code is simpler and easier to verify. By making use of ByteBuf pooling, we can lower both memory usage and memory churn by reusing buffers. This was actually a critical component of the petasort benchmark, where the code originated from. While building this component, we realized it was a good opportunity to extract out the core transport functionality from Spark so we could reuse it for SPARK-3796, which calls for an external service which can serve Spark shuffle files. Thus, we created the "network/common"package, containing the functionality for setting up a simple control plane and an efficient data plane over a network. This part is functionally independent from Spark and is in fact written in Java to further minimize dependencies. PR #3001 finishes the work of creating an external shuffle service by creating a "network/shuffle" package which deals with serving Spark shuffle files from outside of an executor. The intention is that this server can be run anywhere -- including inside the Spark Standalone Worker or the YARN NodeManager, or as a separate service inside Mesos -- and provide the ability to scale up and down executors without losing shuffle data. Thanks to Aaron, Reynold and others who have worked on these improvements over the last month. - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Does this happen if you clean and recompile? I've seen failures on and off, but haven't been able to find one that I could reproduce from a clean build such that we could hand it to the scala team. - Patrick By the way - we can report issues to the Scala/Typesafe team if we have a way to reproduce this. I just haven't found a reliable reproduction yet. - Patrick Hi All, I've just cut the release branch for Spark 1.2, consistent with then end of the scheduled feature window for the release. New commits to master will need to be explicitly merged into branch-1.2 in order to be in the release. This begins the transition into a QA period for Spark 1.2, with a focus on testing and fixes. A few smaller features may still go in as folks wrap up loose ends in the next 48 hours (or for developments in alpha components). To help with QA, I'll try to package up a SNAPSHOT release soon for community testing; this worked well when testing Spark 1.1 before official votes started. I might give it a few days to allow committers to merge in back-logged fixes and other patches that were punted to after the feature freeze. Thanks to everyone who helped author and review patches over the last few weeks! - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I'm a +1 on this as well, I think it will be a useful model as we scale the project in the future and recognizes some informal process we have now. To respond to Sandy's comment: for changes that fall in between the component boundaries or are straightforward, my understanding of this model is you wouldn't need an explicit sign off. I think this is why unlike some other projects, we wouldn't e.g. lock down permissions to portions of the source tree. If some obvious fix needs to go in, people should just merge it. - Patrick I think new committers might or might not be maintainers (it would depend on the PMC vote). I don't think it would affect what you could merge, you can merge in any part of the source tree, you just need to get sign off if you want to touch a public API or make major architectural changes. Most projects already require code review from other committers before you commit something, so it's just a version of that where you have specific people appointed to specific components for review. If you look, most large software projects have a maintainer model, both in Apache and outside of it. Cloudstack is probably the best example in Apache since they are the second most active project (roughly) after Spark. They have two levels of maintainers and much strong language - their language: "In general, maintainers only have commit rights on the module for which they are responsible.". I'd like us to start with something simpler and lightweight as proposed here. Really the proposal on the table is just to codify the current de-facto process to make sure we stick by it as we scale. If we want to add more formality to it or strictness, we can do it later. - Patrick Hey Greg, Regarding subversion - I think the reference is to partial vs full committers here: https://subversion.apache.org/docs/community-guide/roles.html - Patrick In fact, if you look at the subversion commiter list, the majority of people here have commit access only for particular areas of the project: http://svn.apache.org/repos/asf/subversion/trunk/COMMITTERS I bet it doesn't work. +1 on isolating it's inclusion to only the newer YARN API's. - Patrick I think you might be conflating two things. The first error you posted was because YARN didn't standardize the shuffle API in alpha versions so our spark-network-yarn module won't compile. We should just disable that module if yarn alpha is used. spark-network-yarn is a leaf in the intra-module dependency graph, and core doesn't depend on it. This second error is something else. Maybe you are excluding network-shuffle instead of spark-network-yarn? Great - I think that should work, but if there are any issues we can definitely fix them up. I reverted that patch to see if it fixes it. I wonder if we should be linking to that dashboard somewhere from our official docs or the wiki... Hey All, I've just merged a patch that adds support for Scala 2.11 which will have some minor implications for the build. These are due to the complexities of supporting two versions of Scala in a single project. 1. The JDBC server will now require a special flag to build -Phive-thriftserver on top of the existing flag -Phive. This is because some build permutations (only in Scala 2.11) won't support the JDBC server yet due to transitive dependency conflicts. 2. The build now uses non-standard source layouts in a few additional places (we already did this for the Hive project) - the repl and the examples modules. This is just fine for maven/sbt, but it may affect users who import the build in IDE's that are using these projects and want to build Spark from the IDE. I'm going to update our wiki to include full instructions for making this work well in IntelliJ. If there are any other build related issues please respond to this thread and we'll make sure they get sorted out. Thanks to Prashant Sharma who is the author of this feature! - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Yeah Sandy and I were chatting about this today and din't realize -Pscala-2.10 was mandatory. This is a fairly invasive change, so I was thinking maybe we could try to remove that. Also if someone doesn't give -Pscala-2.10 it fails in a way that is initially silent, which is bad because most people won't know to do this. https://issues.apache.org/jira/browse/SPARK-4375 I think printing an error that says "-Pscala-2.10 must be enabled" is probably okay. It's a slight regression but it's super obvious to users. That could be a more elegant solution than the somewhat complicated monstrosity I proposed on the JIRA. I actually do agree with this - let's see if we can find a solution that doesn't regress this behavior. Maybe we can simply move the one kafka example into its own project instead of having it in the examples project. Hey Marcelo, I'm not sure chaining activation works like that. At least in my experience activation based on properties only works for properties explicitly specified at the command line rather than declared elsewhere in the pom. https://gist.github.com/pwendell/6834223e68f254e6945e I any case, I think Prashant just didn't document that his patch required -Pscala-2.10 explicitly, which is what he said further up in the thread. And Sandy has a solution that has better behavior than that, which is nice. - Patrick Ah yeah good call - I so then we'd trigger 2.11-vs-not based on the presence of -Dscala-2.11. Would that fix this issue then? It might be a simpler fix to merge into the 1.2 branch than Sandy's patch since we're pretty late in the game (though that patch does other things separately that I'd like to see end up in Spark soon). --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org A recent patch broke clean builds for me, I am trying to see how widespread this issue is and whether we need to revert the patch. The error I've seen is this when building the examples project: spark-examples_2.10: Could not resolve dependencies for project org.apache.spark:spark-examples_2.10:jar:1.2.0-SNAPSHOT: Could not find artifact jdk.tools:jdk.tools:jar:1.7 at specified path /System/Library/Java/JavaVirtualMachines/1.6.0.jdk/Contents/Home/../lib/tools.jar The reason for this error is that hbase-annotations is using a "system" scoped dependency in their hbase-annotations pom, and this doesn't work with certain JDK layouts such as that provided on Mac OS: http://central.maven.org/maven2/org/apache/hbase/hbase-annotations/0.98.7-hadoop2/hbase-annotations-0.98.7-hadoop2.pom Has anyone else seen this or is it just me? - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org A work around for this fix is identified here: http://dbknickerbocker.blogspot.com/2013/04/simple-fix-to-missing-toolsjar-in-jdk.html However, if this affects more users I'd prefer to just fix it properly in our build. I think in this case we can probably just drop that dependency, so there is a simpler fix. But mostly I'm curious whether anyone else has observed this. Sounds like this is pretty specific to my environment so not a big deal then. However, if we can safely exclude those packages it's worth doing. Neither is strictly optimal which is why we ended up supporting both. Our reference build for packaging is Maven so you are less likely to run into unexpected dependency issues, etc. Many developers use sbt as well. It's somewhat religion and the best thing might be to try both and see which you prefer. - Patrick Hi All, I've just posted a preview of the Spark 1.2.0. release for community regression testing. Issues reported now will get close attention, so please help us test! You can help by running an existing Spark 1.X workload on this and reporting any regressions. As we start voting, etc, the bar for reported issues to hold the release will get higher and higher, so test early! The tag to be is v1.2.0-snapshot1 (commit 38c1fbd96) The release files, including signatures, digests, etc can be found at: http://people.apache.org/~pwendell/spark-1.2.0-snapshot1 The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1038/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.2.0-snapshot1-docs/ == Notes == - Maven artifacts are published for both Scala 2.10 and 2.11. Binary distributions are not posted for Scala 2.11 yet, but will be posted soon. - There are two significant config default changes that users may want to revert if doing A:B testing against older versions. "spark.shuffle.manager" default has changed to "sort" (was "hash") "spark.shuffle.blockTransferService" default has changed to "netty" (was "nio") - This release contains a shuffle service for YARN. This jar is present in all Hadoop 2.X binary packages in "lib/spark-1.2.0-yarn-shuffle.jar"Cheers, Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hey Kevin, If you are upgrading from 1.0.X to 1.1.X checkout the upgrade notes here [1] - it could be that default changes caused a regression for your workload. Do you still see a regression if you restore the configuration changes? It's great to hear specifically about issues like this, so please fork a new thread and describe your workload if you see a regression. The main focus of a patch release vote like this is to test regressions against the previous release on the same line (e.g. 1.1.1 vs 1.1.0) though of course we still want to be cognizant of 1.0-to-1.1 regressions and make sure we can address them down the road. [1] https://spark.apache.org/releases/spark-release-1-1-0.html Hey All, The Apache-->github mirroring is not working right now and hasn't been working fo more than 24 hours. This means that pull requests will not appear as closed even though they have been merged. It also causes diffs to display incorrectly in some cases. If you'd like to follow progress by Apache infra on this issue you can watch this JIRA: https://issues.apache.org/jira/browse/INFRA-8654 - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hey All, Just a heads up. I merged this patch last night which caused the Spark build to break: https://github.com/apache/spark/commit/397d3aae5bde96b01b4968dde048b6898bb6c914 The patch itself was fine and previously had passed on Jenkins. The issue was that other intermediate changes merged since it last passed, and the combination of those changes with the patch caused an issue with our binary compatibility tests. This kind of race condition can happen from time to time. I've merged in a hot fix that should resolve this: https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=0df02ca463a4126e5437b37114c6759a57ab71ee We'll keep an eye this and make sure future builds are passing. - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi All, I noticed people sometimes struggle to get Spark set up in IntelliJ. I'd like to maintain comprehensive instructions on our Wiki to make this seamless for future developers. Due to some nuances of our build, getting to the point where you can build + test every module from within the IDE is not trivial. I created a reference here: https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-BuildingSparkinIntelliJIDEA I'd love people to independently test this and/or share potential improvements. - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org After we merge pull requests in Spark they are closed via a special message we put in each commit description ("Closes #XXX"). This feature stopped working around 21 hours ago causing already-merged pull requests to display as open. I've contacted Github support with the issue. No word from them yet. It is not clear whether this relates to recently delays syncing with Github. - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org There are two distinct topics when it comes to hive integration. Part of the 1.3 roadmap will likely be better defining the plan for Hive integration as Hive adds future versions. 1. Ability to interact with Hive metastore's from different versions ==> I.e. if a user has a metastore, can Spark SQL read the data? This one we want need to solve by asking Hive for a stable metastore thrift API, or adding sufficient features to the HCatalog API so we can use that. 2. Compatibility with HQL over time as Hive adds new features. ==> This relates to how often we update our internal library dependency on Hive and/or build support for new Hive features internally. Hi All, Unfortunately this went back down again. I've opened a new JIRA to track it: https://issues.apache.org/jira/browse/INFRA-8688 - Patrick +1 (binding). Don't see any evidence of regressions at this point. The issue reported by Hector was not related to this rlease. Hey Stephen, Thanks for bringing this up. Technically when we call a release vote it needs to be on the exact commit that will be the final release. However, one thing I've thought of doing for a while would be to publish the maven artifacts using a version tag with $VERSION-rcX even if the underlying commit has $VERSION in the pom files. Some recent changes I've made to the way we do publishing in branch 1.2 should make this pretty easy - it wasn't very easy before because we used maven's publishing plugin which makes modifying the published version tricky. Our current approach is, indeed, problematic because maven artifacts are supposed to be immutable once they have a specific version identifier. I created SPARK-4568 to track this: https://issues.apache.org/jira/browse/SPARK-4568 - Patrick Hey Evan, It might be nice to merge this into existing documentation. In particular, a lot of this could serve to update the current tuning section and programming guides. It could also work to paste this wholesale as a reference for Spark users, but in that case it's less likely to get updated when other things change, or be found by users reading through the spark docs. - Patrick Please vote on releasing the following candidate as Apache Spark version 1.2.0! The tag to be voted on is v1.2.0-rc1 (commit 1056e9ec1): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=1056e9ec13203d0c51564265e94d77a054498fdb The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.2.0-rc1/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1048/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.2.0-rc1-docs/ Please vote on releasing this package as Apache Spark 1.2.0! The vote is open until Tuesday, December 02, at 05:15 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.1.0 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ == What justifies a -1 vote for this release? == This vote is happening very late into the QA period compared with previous votes, so -1 votes should only occur for significant regressions from 1.0.2. Bugs already present in 1.1.X, minor regressions, or bugs related to new features will not block this release. == What default changes should I be aware of? == 1. The default value of "spark.shuffle.blockTransferService" has been changed to "netty"--> Old behavior can be restored by switching to "nio"2. The default value of "spark.shuffle.manager" has been changed to "sort". --> Old behavior can be restored by setting "spark.shuffle.manager" to "hash". == Other notes == Because this vote is occurring over a weekend, I will likely extend the vote if this RC survives until the end of the vote period. - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Thanks for pointing this out, Matei. I don't think a minor typo like this is a big deal. Hopefully it's clear to everyone this is the 1.2.0 release vote, as indicated by the subject and all of the artifacts. Thanks for reporting this. One thing to try is to just do a git clean to make sure you have a totally clean working space ("git clean -fdx"will blow away any differences you have from the repo, of course only do that if you don't have other files around). Can you reproduce this if you just run "sbt/sbt compile"? Also, if you can, can you reproduce it if you checkout only the spark master branch and not merged with your own code? Finally, if you can reproduce it on master, can you perform a bisection to find out which commit caused it? - Patrick Hey Ryan, A few more things here. You should feel free to send patches to Jenkins to test them, since this is the reference environment in which we regularly run tests. This is the normal workflow for most developers and we spend a lot of effort provisioning/maintaining a very large jenkins cluster to allow developers access this resource. A common development approach is to locally run tests that you've added in a patch, then send it to jenkins for the full run, and then try to debug locally if you see specific unanticipated test failures. One challenge we have is that given the proliferation of OS versions, Java versions, Python versions, ulimits, etc. there is a combinatorial number of environments in which tests could be run. It is very hard in some cases to figure out post-hoc why a given test is not working in a specific environment. I think a good solution here would be to use a standardized docker container for running Spark tests and asking folks to use that locally if they are trying to run all of the hundreds of Spark tests. Another solution would be to mock out every system interaction in Spark's tests including e.g. filesystem interactions to try and reduce variance across environments. However, that seems difficult. As the number of developers of Spark increases, it's definitely a good idea for us to invest in developer infrastructure including things like snapshot releases, better documentation, etc. Thanks for bringing this up as a pain point. - Patrick Hey Ryan, The existing JIRA also covers publishing nightly docs: https://issues.apache.org/jira/browse/SPARK-1517 - Patrick Btw - the documnetation on github represents the source code of our docs, which is versioned with each release. Unfortunately github will always try to render ".md" files so it could look to a passerby like this is supposed to represent published docs. This is a feature limitation of github, AFAIK we cannot disable it. The official published docs are associated with each release and available on the apache.org website. I think "/latest" is a common convention for referring to the latest *published release* docs, so probably we can't change that (the audience for /latest is orders of magnitude larger than for snapshot docs). However we could just add /snapshot and publish docs there. - Patrick Hi Ilya - you can just submit a pull request and the way we test them is to run it through jenkins. You don't need to do anything special. Hey All, Just an update. Josh, Andrew, and others are working to reproduce SPARK-4498 and fix it. Other than that issue no serious regressions have been reported so far. If we are able to get a fix in for that soon, we'll likely cut another RC with the patch. Continued testing of RC1 is definitely appreciated! I'll leave this vote open to allow folks to continue posting comments. It's fine to still give "+1" from your own testing... i.e. you can assume at this point SPARK-4498 will be fixed before releasing. - Patrick Also a note on this for committers - it's possible to re-word the title during merging, by just running "git commit -a --amend" before you push the PR. - Patrick Hey Ryan, What if you run a single "mvn install" to install all libraries locally - then can you "mvn compile -pl core"? I think this may be the only way to make it work. - Patrick Hey Jun, The Ooyala server is being maintained by it's original author (Evan Chan) here: https://github.com/spark-jobserver/spark-jobserver This is likely to stay as a standalone project for now, since it builds directly on Spark's public API's. - Patrick Thanks for flagging this. I reverted the relevant YARN fix in Spark 1.2 release. We can try to debug this in master. One thing I created a JIRA for a while back was to have a similar script to "sbt/sbt" that transparently downloads Zinc, Scala, and Maven in a subdirectory of Spark and sets it up correctly. I.e. "build/mvn". Outside of brew for MacOS there aren't good Zinc packages, and it's a pain to figure out how to set it up. https://issues.apache.org/jira/browse/SPARK-4501 Prashant Sharma looked at this for a bit but I don't think he's working on it actively any more, so if someone wanted to do this, I'd be extremely grateful. - Patrick Hey All, Thanks all for the continued testing! The issue I mentioned earlier SPARK-4498 was fixed earlier this week (hat tip to Mark Hamstra who contributed to fix). In the interim a few smaller blocker-level issues with Spark SQL were found and fixed (SPARK-4753, SPARK-4552, SPARK-4761). There is currently an outstanding issue (SPARK-4740[1]) in Spark core that needs to be fixed. I want to thank in particular Shopify and Intel China who have identified and helped test blocker issues with the release. This type of workload testing around releases is really helpful for us. Once things stabilize I will cut RC2. I think we're pretty close with this one. - Patrick Hey Nick, Thanks for bringing this up. I believe these Java tests are running in the sbt build right now, the issue is that this particular bug was flagged by the triggering of a runtime Java "assert" (not a normal Junit test assertion) and those are not enabled in our sbt tests. It would be good to fix it so that assertions run when we do the sbt tests, for some reason I think the sbt tests disable them by default. I think the original issue is fixed now (that Sean found and reported). It would be good to get assertions running in our tests, but I'm not sure I'd block the release on it. The normal JUnit assertions are running correctly. - Patrick Hi Andrew, It looks like somehow you are including jars from the upstream Apache Hive 0.13 project on your classpath. For Spark 1.2 Hive 0.13 support, we had to modify Hive to use a different version of Kryo that was compatible with Spark's Kryo version. https://github.com/pwendell/hive/commit/5b582f242946312e353cfce92fc3f3fa472aedf3 I would look through the actual classpath and make sure you aren't including your own hive-exec jar somehow. - Patrick This vote is closed in favor of RC2. Please vote on releasing the following candidate as Apache Spark version 1.2.0! The tag to be voted on is v1.2.0-rc2 (commit a428c446e2): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=a428c446e23e628b746e0626cc02b7b3cadf588e The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.2.0-rc2/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1055/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.2.0-rc2-docs/ Please vote on releasing this package as Apache Spark 1.2.0! The vote is open until Saturday, December 13, at 21:00 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.2.0 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ == What justifies a -1 vote for this release? == This vote is happening relatively late into the QA period, so -1 votes should only occur for significant regressions from 1.0.2. Bugs already present in 1.1.X, minor regressions, or bugs related to new features will not block this release. == What default changes should I be aware of? == 1. The default value of "spark.shuffle.blockTransferService" has been changed to "netty"--> Old behavior can be restored by switching to "nio"2. The default value of "spark.shuffle.manager" has been changed to "sort". --> Old behavior can be restored by setting "spark.shuffle.manager" to "hash". == How does this differ from RC1 == This has fixes for a handful of issues identified - some of the notable fixes are: [Core] SPARK-4498: Standalone Master can fail to recognize completed/failed applications [SQL] SPARK-4552: Query for empty parquet table in spark sql hive get IllegalArgumentException SPARK-4753: Parquet2 does not prune based on OR filters on partition columns SPARK-4761: With JDBC server, set Kryo as default serializer and disable reference tracking SPARK-4785: When called with arguments referring column fields, PMOD throws NPE - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I believe many apache services are/were down due to an outage. Hey All, It appears that a single test suite is failing after the jenkins upgrade: "org.apache.spark.streaming.rdd.WriteAheadLogBackedBlockRDDSuite". My guess is the suite is not resilient in some way to differences in the environment (JVM, OS version, or something else). I'm going to disable the suite to get the build passing. This should be done in the next 30 minutes or so. - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Ah cool Josh - I think for some reason we are hitting this every time now. Since this is holding up a bunch of other patches, I just pushed something ignoring the tests as a hotfix. Even waiting for a couple hours is really expensive productivity-wise given the frequency with which we run tests. We should just re-enable them when we merge the appropriate fix. Hey Andrew, The list of admins is maintained by the Amplab as part of their donation of this infrastructure. The reason why we need to have admins is that the pull request builder will fetch and then execute arbitrary user code, so we need to do a security audit before we can approve testing new patches. Over time when we get to know users we usually whitelist them so they can test whatever they want. I can see offline if the Amplab would be open to adding you as an admin. I think we've added people over time who are very involved in the community. Just wanted to send this e-mail so people understand how it works. - Patrick Yeah you can do it - just make sure they understand it is a new feature so we're asking them to revisit it. They looked at it in the past and they concluded they couldn't give us access without giving us push access. - Patrick The Partition itself doesn't need to be an iterator - the iterator comes from the result of compute(partition). The Partition is just an identifier for that partition, not the data itself. Take a look at the signature for compute() in the RDD class. https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L97 --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org This vote has PASSED with 12 +1 votes (8 binding) and no 0 or -1 votes: +1: Matei Zaharia* Madhu Siddalingaiah Reynold Xin* Sandy Ryza Josh Rozen* Mark Hamstra* Denny Lee Tom Graves* GuiQiang Li Nick Pentreath* Sean McNamara* Patrick Wendell* 0: -1: I'll finalize and package this release in the next 48 hours. Thanks to everyone who contributed. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I'm closing this vote now, will send results in a new thread. Hey All, Due to the very high volume of contributions, we're switching to an automated process for generating release credits. This process relies on JIRA for categorizing contributions, so it's not possible for us to provide credits in the case where users submit pull requests with no associated JIRA. This needed to be automated because, with more than 1000 commits per release, finding proper names for every commit and summarizing contributions was taking on the order of days of time. For 1.2.0 there were around 100 commits that did not have JIRA's. I'll try to manually merge these into the credits, but please e-mail me directly if you are not credited once the release notes are posted. The notes should be posted within 48 hours of right now. We already ask that users include a JIRA for pull requests, but now it will be required for proper attribution. I've updated the contributing guide on the wiki to reflect this. - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hey Cody, Thanks for reaching out with this. The lead on streaming is TD - he is traveling this week though so I can respond a bit. To the high level point of whether Kafka is important - it definitely is. Something like 80% of Spark Streaming deployments (anecdotally) ingest data from Kafka. Also, good support for Kafka is something we generally want in Spark and not a library. In some cases IIRC there were user libraries that used unstable Kafka API's and we were somewhat waiting on Kafka to stabilize them to merge things upstream. Otherwise users wouldn't be able to use newer Kakfa versions. This is a high level impression only though, I haven't talked to TD about this recently so it's worth revisiting given the developments in Kafka. Please do bring things up like this on the dev list if there are blockers for your usage - thanks for pinging it. - Patrick Update: An Apache infrastructure issue prevented me from pushing this last night. The issue was resolved today and I should be able to push the final release artifacts tonight. I'm happy to announce the availability of Spark 1.2.0! Spark 1.2.0 is the third release on the API-compatible 1.X line. It is Spark's largest release ever, with contributions from 172 developers and more than 1,000 commits! This release brings operational and performance improvements in Spark core including a new network transport subsytem designed for very large shuffles. Spark SQL introduces an API for external data sources along with Hive 13 support, dynamic partitioning, and the fixed-precision decimal type. MLlib adds a new pipeline-oriented package (spark.ml) for composing multiple algorithms. Spark Streaming adds a Python API and a write ahead log for fault tolerance. Finally, GraphX has graduated from alpha and introduces a stable API along with performance improvements. Visit the release notes [1] to read about the new features, or download [2] the release today. For errata in the contributions or release notes, please e-mail me *directly* (not on-list). Thanks to everyone involved in creating, testing, and documenting this release! [1] http://spark.apache.org/releases/spark-release-1-2-0.html [2] http://spark.apache.org/downloads.html --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Thanks for pointing out the tag issue. I've updated all links to point to the correct tag (from the vote thread): a428c446e23e628b746e0626cc02b7b3cadf588e I also couldn't reproduce this issued. Xiangrui asked me to report that it's back and running :) A SparkContext is thread safe, so you can just have different threads that create their own RDD's and do actions, etc. - Patrick Hey Nick, I think Hitesh was just trying to be helpful and point out the policy - not necessarily saying there was an issue. We've taken a close look at this and I think we're in good shape her vis-a-vis this policy. - Patrick Hey Josh, We don't explicitly track contributions to spark-ec2 in the Apache Spark release notes. The main reason is that usually updates to spark-ec2 include a corresponding update to spark so we get it there. This may not always be the case though, so let me know if you think there is something missing we should add. - Patrick s/Josh/Nick/ - sorry! Hi Will, When you call collect() the item you are collecting needs to fit in memory on the driver. Is it possible your driver program does not have enough memory? - Patrick Is it sufficient to set "spark.hadoop.validateOutputSpecs" to false? http://spark.apache.org/docs/latest/configuration.html - Patrick So the behavior of overwriting existing directories IMO is something we don't want to encourage. The reason why the Hadoop client has these checks is that it's very easy for users to do unsafe things without them. For instance, a user could overwrite an RDD that had 100 partitions with an RDD that has 10 partitions... and if they read back the RDD they would get a corrupted RDD that has a combination of data from the old and new RDD. If users want to circumvent these safety checks, we need to make them explicitly disable them. Given this, I think a config option is as reasonable as any alternatives. This is already pretty easy IMO. - Patrick Hi All, A consistent piece of feedback from Spark developers has been that the Maven build is very slow. Typesafe provides a tool called Zinc which improves Scala complication speed substantially with Maven, but is difficult to install and configure, especially for platforms other than Mac OS. I've just merged a patch (authored by Brennon York) that provides an automatically configured Maven instance with Zinc embedded in Spark. E.g.: ./build/mvn -Phive -Phive-thriftserver -Pyarn -Phadoop-2.3 package It is hard to test changes like this across all environments, so please give this a spin and report any issues on the Spark JIRA. It is working correctly if you see the following message during compilation: [INFO] Using zinc server for incremental compilation Note that developers preferring their own Maven installation are unaffected by this and can just ignore this new feature. Cheers, - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi Alessandro, Can you create a JIRA for this rather than reporting it on the dev list? That's where we track issues like this. Thanks!. - Patrick Thanks for reporting this - it definitely sounds like a bug. Please open a JIRA for it. My guess is that we define the start or end time of the job based on the current time instead of looking at data encoded in the underlying event stream. That would cause it to not work properly when loading from historical data. - Patrick Hey Andrew, So the executors in Spark will fetch classes from the driver node for classes defined in the repl from an HTTP server on the driver. Is this happening in the context of a repl session? Also, is it deterministic or does it happen only periodically? The reason all of the other threads are hanging is that there is a global lock around classloading, so they all queue up. Could you attach the full stack trace from the driver? Is it possible that something in the network is blocking the transfer of bytes between these two processes? Based on the stack trace it looks like it sent an HTTP request and is waiting on the result back from the driver. One thing to check is to verify that the TCP connection between them used for the repl class server is still alive from the vantage point of both the executor and driver nodes. Another thing to try would be to temporarily open up any firewalls that are on the nodes or in the network and see if this makes the problem go away (to isolate it to an exogenous-to-Spark network issue). - Patrick Nick - yes. Do you mind moving it? I should have put it in the "Contributing to Spark" page. Actually I went ahead and did it. Priority scheduling isn't something we've supported in Spark and we've opted to support FIFO and Fair scheduling and asked users to try and fit these to the needs of their applications. In practice from what I've seen of priority schedulers, such as the linux CPU scheduler, is that strict priority scheduling is never used in practice because of priority starvation and other issues. So you have this second tier of heuristics that exist to deal with issues like starvation, priority inversion, etc, and these become very complex over time. That said, I looked a this a bit with @kayousterhout and I don't think it would be very hard to implement a simple priority scheduler in the current architecture. My main concern would be additional complexity that would develop over time, based on looking at previous implementations in the wild. Alessandro, would you be able to open a JIRA and list some of your requirements there? That way we could hear whether other people have similar needs. - Patrick FYI our git repo may be down for a few hours today. Akhil, Those are handled by ASF infrastructure, not anyone in the Spark project. So this list is not the appropriate place to ask for help. - Patrick Hey All, Just wanted to ping about a minor issue - but one that ends up having consequence given Spark's volume of reviews and commits. As much as possible, I think that we should try and gear towards "Google Style"LGTM on reviews. What I mean by this is that LGTM has the following semantics: "I know this code well, or I've looked at it close enough to feel confident it should be merged. If there are issues/bugs with this code later on, I feel confident I can help with them."Here is an alternative semantic: "Based on what I know about this part of the code, I don't see any show-stopper problems with this patch". The issue with the latter is that it ultimately erodes the significance of LGTM, since subsequent reviewers need to reason about what the person meant by saying LGTM. In contrast, having strong semantics around LGTM can help streamline reviews a lot, especially as reviewers get more experienced and gain trust from the comittership. There are several easy ways to give a more limited endorsement of a patch: - "I'm not familiar with this code, but style, etc look good" (general endorsement) - "The build changes in this code LGTM, but I haven't reviewed the rest" (limited LGTM) If people are okay with this, I might add a short note on the wiki. I'm sending this e-mail first, though, to see whether anyone wants to express agreement or disagreement with this approach. - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I think the ASF +1 is *slightly* different than Google's LGTM, because it might convey wanting the patch/feature to be merged but not necessarily saying you did a thorough review and stand behind it's technical contents. For instance, I've seen people pile on +1's to try and indicate support for a feature or patch in some projects, even though they didn't do a thorough technical review. This +1 is definitely a useful mechanism. There is definitely much overlap though in the meaning, though, and it's largely because Spark had it's own culture around reviews before it was donated to the ASF, so there is a mix of two styles. Nonetheless, I'd prefer to stick with the stronger LGTM semantics I proposed originally (unlike the one Sandy proposed, e.g.). This is what I've seen every project using the LGTM convention do (Google, and some open source projects such as Impala) to indicate technical sign-off. - Patrick Okay - so given all this I was going to put the following on the wiki tentatively: ## Reviewing Code Community code review is Spark's fundamental quality assurance process. When reviewing a patch, your goal should be to help streamline the committing process by giving committers confidence this patch has been verified by an additional party. It's encouraged to (politely) submit technical feedback to the author to identify areas for improvement or potential bugs. If you feel a patch is ready for inclusion in Spark, indicate this to committers with a comment: "I think this patch looks good". Spark uses the LGTM convention for indicating the highest level of technical sign-off on a patch: simply comment with the word "LGTM". An LGTM is a strong statement, it should be interpreted as the following: "I've looked at this thoroughly and take as much ownership as if I wrote the patch myself". If you comment LGTM you will be expected to help with bugs or follow-up issues on the patch. Judicious use of LGTM's is a great way to gain credibility as a reviewer with the broader community. It's also welcome for reviewers to argue against the inclusion of a feature or patch. Simply indicate this in the comments. - Patrick The wiki does not seem to be operational ATM, but I will do this when it is back up. To respond to the original suggestion by Nick. I always thought it would be useful to have a Docker image on which we run the tests and build releases, so that we could have a consistent environment that other packagers or people trying to exhaustively run Spark tests could replicate (or at least look at) to understand exactly how we recommend building Spark. Sean - do you think that is too high of overhead? In terms of providing images that we encourage as standard deployment images of Spark and want to make portable across environments, that's a much larger project and one with higher associated maintenance overhead. So I'd be interested in seeing that evolve as its own project (spark-deploy) or something associated with bigtop, etc. - Patrick But the issue is when users can't reproduce Jenkins failures. We don't publish anywhere what the exact set of packages and versions is that is installed on Jenkins. And it can change since it's a shared infrastructure with other projects. So why not publish this manifest as a docker file and then have it run on jenkins using that image? My point is that this "VM image + steps" is not public anywhere. Right now the reference build env is an AMI I created and keep adding stuff to when Spark gets new dependencies (e.g. the version of ruby we need to create the docs, new python stats libraries, etc). So if we had a docker image, then I would use that for making the RC's as well and it could serve as a definitive reference for people who want to understand exactly what set of things they need to build Spark. There are actually a good number of packages you need to do a full build of Spark including a compliant python version, Java version, certain python packages, ruby and jekyll stuff for the docs, etc (mentioned a bit earlier). - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Yep, I think it's only useful (and likely to be maintained) if we actually use this on Jenkins. So that was my proposal. Basically give people a docker file so they can understand exactly what versions of everything we use for our reference build. And if they don't want to use docker directly, this will at least serve as an up-to-date list of packages/versions they should try to install locally in whatever environment they have. - Patrick Hey All, I am planning to cut a 1.2.1 RC soon and wanted to notify people. There are a handful of important fixes in the 1.2.1 branch (http://s.apache.org/Mpn) particularly for Spark SQL. There was also an issue publishing some of our artifacts with 1.2.0 and this release would fix it for downstream projects. You can track outstanding 1.2.1 blocker issues here at http://s.apache.org/2v2 - I'm guessing all remaining blocker issues will be fixed today. I think we have a good handle on the remaining outstanding fixes, but please let me know if you think there are severe outstanding fixes that need to be backported into this branch or are not tracked above. Thanks! - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org One thing potentially not clear from this e-mail, there will be a 1:1 correspondence where you can get an RDD to/from a DataFrame. Please vote on releasing the following candidate as Apache Spark version 1.2.1! The tag to be voted on is v1.2.1-rc1 (commit 3e2d7d3): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3e2d7d310b76c293b9ac787f204e6880f508f6ec The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.2.1-rc1/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1061/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.2.1-rc1-docs/ Please vote on releasing this package as Apache Spark 1.2.1! The vote is open until Friday, January 30, at 07:00 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.2.1 [ ] -1 Do not release this package because ... For a list of fixes in this release, see http://s.apache.org/Mpn. To learn more about Apache Spark, please see http://spark.apache.org/ - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hey Sean, The release script generates hashes in two places (take a look a bit further down in the script), one for the published artifacts and the other for the binaries. In the case of the binaries we use SHA512 because, AFAIK, the ASF does not require you to use SHA1 and SHA512 is better. In the case of the published Maven artifacts we use SHA1 because my understanding is this is what Maven requires. However, it does appear that the format is now one that maven cannot parse. Anyways, it seems fine to just change the format of the hash per your PR. - Patrick Yes - the key issue is just due to me creating new keys this time around. Anyways let's take another stab at this. In the mean time, please don't hesitate to test the release itself. - Patrick Hey Sean, Right now we don't publish every 2.11 binary to avoid combinatorial explosion of the number of build artifacts we publish (there are other parameters such as whether hive is included, etc). We can revisit this in future feature releases, but .1 releases like this are reserved for bug fixes. - Patrick Okay - we've resolved all issues with the signatures and keys. However, I'll leave the current vote open for a bit to solicit additional feedback. Hey All, Just a reminder, as always around release time we have a very large volume of patches show up near the deadline. One thing that can help us maximize the number of patches we get in is to have community involvement in performing code reviews. And in particular, doing a thorough review and signing off on a patch with LGTM can substantially increase the odds we can merge a patch confidently. If you are newer to Spark, finding a single area of the codebase to focus on can still provide a lot of value to the project in the reviewing process. Cheers and good luck with everyone on work for this release. - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org This vote is cancelled in favor of RC2. Please vote on releasing the following candidate as Apache Spark version 1.2.1! The tag to be voted on is v1.2.1-rc1 (commit b77f876): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=b77f87673d1f9f03d4c83cf583158227c551359b The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.2.1-rc2/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1062/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.2.1-rc2-docs/ Changes from rc1: This has no code changes from RC1. Only minor changes to the release script. Please vote on releasing this package as Apache Spark 1.2.1! The vote is open until  Saturday, January 31, at 10:04 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.2.1 [ ] -1 Do not release this package because ... For a list of fixes in this release, see http://s.apache.org/Mpn. To learn more about Apache Spark, please see http://spark.apache.org/ --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Minor typo in the above e-mail - the tag is named v1.2.1-rc2 (not v1.2.1-rc1). Yes - it fixes that issue. It's maintained here: https://github.com/pwendell/akka/tree/2.2.3-shaded-proto Over time, this is something that would be great to get rid of, per rxin Hey Jerry, I think standalone mode will still add more features over time, but the goal isn't really for it to become equivalent to what Mesos/YARN are today. Or at least, I doubt Spark Standalone will ever attempt to manage _other_ frameworks outside of Spark and become a general purpose resource manager. In terms of having better support for multi tenancy, meaning multiple *Spark* instances, this is something I think could be in scope in the future. For instance, we added H/A to the standalone scheduler a while back, because it let us support H/A streaming apps in a totally native way. It's a trade off of adding new features and keeping the scheduler very simple and easy to use. We've tended to bias towards simplicity as the main goal, since this is something we want to be really easy "out of the box". One thing to point out, a lot of people use the standalone mode with some coarser grained scheduler, such as running in a cloud service. In this case they really just want a simple "inner" cluster manager. This may even be the majority of all Spark installations. This is slightly different than Hadoop environments, where they might just want nice integration into the existing Hadoop stack via something like YARN. - Patrick It's my fault, I'm sending a hot fix now. Hey All, I made a change to the Jenkins configuration that caused most builds to fail (attempting to enable a new plugin), I've reverted the change effective about 10 minutes ago. If you've seen recent build failures like below, this was caused by that change. Sorry about that. ==== ERROR: Publisher com.google.jenkins.flakyTestHandler.plugin.JUnitFlakyResultArchiver aborted due to exception java.lang.NoSuchMethodError: hudson.model.AbstractBuild.getTestResultAction()Lhudson/tasks/test/AbstractTestResultAction; at com.google.jenkins.flakyTestHandler.plugin.FlakyTestResultAction.(FlakyTestResultAction.java:78) at com.google.jenkins.flakyTestHandler.plugin.JUnitFlakyResultArchiver.perform(JUnitFlakyResultArchiver.java:89) at hudson.tasks.BuildStepMonitor$1.perform(BuildStepMonitor.java:20) at hudson.model.AbstractBuild$AbstractBuildExecution.perform(AbstractBuild.java:770) at hudson.model.AbstractBuild$AbstractBuildExecution.performAllBuildSteps(AbstractBuild.java:734) at hudson.model.Build$BuildExecution.post2(Build.java:183) at hudson.model.AbstractBuild$AbstractBuildExecution.post(AbstractBuild.java:683) at hudson.model.Run.execute(Run.java:1784) at hudson.matrix.MatrixRun.run(MatrixRun.java:146) at hudson.model.ResourceController.execute(ResourceController.java:89) at hudson.model.Executor.run(Executor.java:240) ==== - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org The windows issue reported only affects actually running Spark on Windows (not job submission). However, I agree it's worth cutting a new RC. I'm going to cancel this vote and propose RC3 with a single additional patch. Let's try to vote that through so we can ship Spark 1.2.1. - Patrick This is cancelled in favor of RC2. Please vote on releasing the following candidate as Apache Spark version 1.2.1! The tag to be voted on is v1.2.1-rc3 (commit b6eaf77): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=b6eaf77d4332bfb0a698849b1f5f917d20d70e97 The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.2.1-rc3/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1065/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.2.1-rc3-docs/ Changes from rc2: A single patch fixing a windows issue. Please vote on releasing this package as Apache Spark 1.2.1! The vote is open until Friday, February 06, at 05:00 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.2.1 [ ] -1 Do not release this package because ... For a list of fixes in this release, see http://s.apache.org/Mpn. To learn more about Apache Spark, please see http://spark.apache.org/ --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hey All, Just wanted to announce that we've cut the 1.3 branch which will become the 1.3 release after community testing. There are still some features that will go in (in higher level libraries, and some stragglers in spark core), but overall this indicates the end of major feature development for Spark 1.3 and a transition into testing. Within a few days I'll cut a snapshot package release for this so that people can begin testing. https://git-wip-us.apache.org/repos/asf?p=spark.git;a=shortlog;h=refs/heads/branch-1.3 - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi Markus, That won't be included in 1.2.1 most likely because the release votes have already started, and at that point we don't hold the release except for major regression issues from 1.2.0. However, if this goes through we can backport it into the 1.2 branch and it will end up in a future maintenance release, or you can just build spark from that branch as soon as it's in there. - Patric Personally I have no opinion, but agree it would be nice to standardize. - Patrick I've done this in the past, but back when I wasn't using Zinc it didn't make a big difference. It's worth doing this in our jenkins environment though. - Patrick Per Nick's suggestion I added two components: 1. Spark Submit 2. Spark Scheduler I figured I would just add these since if we decide later we don't want them, we can simply merge them into Spark Core. I'll add a +1 as well. This vote passes with 5 +1 votes (3 binding) and no 0 or -1 votes. +1 Votes: Krishna Sankar Sean Owen* Chip Senkbeil Matei Zaharia* Patrick Wendell* 0 Votes: (none) -1 Votes: (none) I think we already have a YARN component. https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20component%20%3D%20YARN I don't think JIRA allows it to be mandatory, but if it does, that would be useful. Hey All, The tests are in a not-amazing state right now due to a few compounding factors: 1. We've merged a large volume of patches recently. 2. The load on jenkins has been relatively high, exposing races and other behavior not seen at lower load. For those not familiar, the main issue is flaky (non deterministic) test failures. Right now I'm trying to prioritize keeping the PullReqeustBuilder in good shape since it will block development if it is down. For other tests, let's try to keep filing JIRA's when we see issues and use the flaky-test label (see http://bit.ly/1yRif9S): I may contact people regarding specific tests. This is a very high priority to get in good shape. This kind of thing is no one's "fault"but just the result of a lot of concurrent development, and everyone needs to pitch in to get back in a good place. - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I have wondered whether we should sort of deprecated it more officially, since otherwise I think people have the reasonable expectation based on the current code that Spark intends to support "complete" Debian packaging as part of the upstream build. Having something that's sort-of maintained but no one is helping review and merge patches on it or make it fully functional, IMO that doesn't benefit us or our users. There are a bunch of other projects that are specifically devoted to packaging, so it seems like there is a clear separation of concerns here. Hi All, I've just posted the 1.2.1 maintenance release of Apache Spark. We recommend all 1.2.0 users upgrade to this release, as this release includes stability fixes across all components of Spark. - Download this release: http://spark.apache.org/downloads.html - View the release notes: http://spark.apache.org/releases/spark-release-1-2-1.html - Full list of JIRA issues resolved in this release: http://s.apache.org/Mpn Thanks to everyone who helped work on this release! - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Clearly there isn't a strictly optimal commenting format (pro's and cons for both '//' and '/*'). My thought is for consistency we should just chose one and put in the style guide. Ah - we should update it to suggest mailing the dev@ list (and if there is enough traffic maybe do something else). I'm happy to add you if you can give an organization name, URL, a list of which Spark components you are using, and a short description of your use case.. Mark was involved in adding this code (IIRC) and has also been the most active in maintaining it. So I'd be interested in hearing his thoughts on that proposal. Mark - would you be okay deprecating this and having Spark instead work with the upstream projects that focus on packaging? My feeling is that it's better to just have nothing than to have something not usable out-of-the-box (which to your point, is a lot more work). Actually, to correct myself, the assembly jar is in assembly/target/scala-2.11 (I think). Hi Judy, If you have added source files in the sink/ source folder, they should appear in the assembly jar when you build. One thing I noticed is that you are looking inside the "/dist" folder. That only gets populated if you run "make-distribution". The normal development process is just to do "mvn package" and then look at the assembly jar that is contained in core/target. - Patrick Thanks Paolo - I've fixed it. Hey All, I've posted Spark 1.3.0 snapshot 1. At this point the 1.3 branch is ready for community testing and we are strictly merging fixes and documentation across all components. The release files, including signatures, digests, etc can be found at: http://people.apache.org/~pwendell/spark-1.3.0-snapshot1/ The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1068/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.3.0-snapshot1-docs/ Please report any issues with the release to this thread and/or to our project JIRA. Thanks! - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org The map will start with a capacity of 64, but will grow to accommodate new data. Are you using the groupBy operator in Spark or are you using Spark SQL's group by? This usually happens if you are grouping or aggregating in a way that doesn't sufficiently condense the data created from each input partition. - Patrick It will create and connect to new executors. The executors are mostly stateless, so the program can resume with new executors. Yeah my preferred is also having a more open ended "2+" for issues that are clearly desirable but blocked by compatibility concerns. What I would really want to avoid is major feature proposals sitting around in our JIRA and tagged under some 2.X version. IMO JIRA isn't the place for thoughts about very-long-term things. When we get these, I'd be include to either close them as "won't fix" or "later". Hey Niranda, It seems to me a lot of effort to support multiple libraries inside of Spark like this, so I'm not sure that's a great solution. If you are building an application that embeds Spark, is it not possible for you to continue to use Jetty for Spark's internal servers and use tomcat for your own server's? I would guess that many complex applications end up embedding multiple server libraries in various places (Spark itself has different transport mechanisms, etc.) - Patrick Please vote on releasing the following candidate as Apache Spark version 1.3.0! The tag to be voted on is v1.3.0-rc1 (commit f97b0d4a): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=f97b0d4a6b26504916816d7aefcf3132cd1da6c2 The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.3.0-rc1/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1069/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.3.0-rc1-docs/ Please vote on releasing this package as Apache Spark 1.3.0! The vote is open until Saturday, February 21, at 08:03 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.3.0 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ == How can I help test this release? == If you are a Spark user, you can help us test this release by taking a Spark 1.2 workload and running on this release candidate, then reporting any regressions. == What justifies a -1 vote for this release? == This vote is happening towards the end of the 1.3 QA period, so -1 votes should only occur for significant regressions from 1.2.1. Bugs already present in 1.2.X, minor regressions, or bugs related to new features will not block this release. - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hey Committers, Now that Spark 1.3 rc1 is cut, please restrict branch-1.3 merges to the following: 1. Fixes for issues blocking the 1.3 release (i.e. 1.2.X regressions) 2. Documentation and tests. 3. Fixes for non-blocker issues that are surgical, low-risk, and/or outside of the core. If there is a lower priority bug fix (a non-blocker) that requires nontrivial code changes, do not merge it into 1.3. If something seems borderline, feel free to reach out to me and we can work through it together. This is what we've done for the last few releases to make sure rc's become progressively more stable, and it is important towards helping us cut timely releases. Thanks! - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org This is a newer test suite. There is something flaky about it, we should definitely fix it, IMO it's not a blocker though. Works for me. Maybe it's some ephemeral issue? Thanks for finding this. I'm going to leave this open for continued testing... --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I believe the heuristic governing the way that take() decides to fetch partitions changed between these versions. It could be that in certain cases the new heuristic is worse, but it might be good to just look at the source code and see, for your number of elements taken and number of partitions, if there was any effective change in how aggressively spark fetched partitions. This was quite a while ago, but I think the change was made because in many cases the newer code works more efficiently. - Patrick So actually, the list of blockers on JIRA is a bit outdated. These days I won't cut RC1 unless there are no known issues that I'm aware of that would actually block the release (that's what the snapshot ones are for). I'm going to clean those up and push others to do so also. The main issues I'm aware of that came about post RC1 are: 1. Python submission broken on YARN 2. The license issue in MLlib [now fixed]. 3. Varargs broken for Java Dataframes [now fixed] Re: Corey - yeah, as it stands now I try to wait if there are things that look like implicit -1 votes. It's only been reported on this thread by Tom, so far. Hey Cody, What build command are you using? In any case, we can actually comment out the "unused" thing now in the root pom.xml. It existed just to ensure that at least one dependency was listed in the shade plugin configuration (otherwise, some work we do that requires the shade plugin does not happen). However, now there are other things there. If you just comment out the line in the root pom.xml adding this dependency, does it work? - Patrick This has been around for multiple versions of Spark, so I am a bit surprised to see it not working in your build. - Patrick Hey All, Just a quick updated on this thread. Issues have continued to trickle in. Not all of them are blocker level but enough to warrant another RC: I've been keeping the JIRA dashboard up and running with the latest status (sorry, long link): https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20%22Target%20Version%2Fs%22%20%3D%201.3.0%20AND%20(fixVersion%20IS%20EMPTY%20OR%20fixVersion%20!%3D%201.3.0)%20AND%20(Resolution%20IS%20EMPTY%20OR%20Resolution%20IN%20(Done%2C%20Fixed%2C%20Implemented))%20ORDER%20BY%20priority%2C%20component One these are in I will cut another RC. Thanks everyone for the continued voting! - Patrick Yeah calling it Hadoop 2 was a very bad naming choice (of mine!), this was back when CDH4 was the only real distribution available with some of the newer Hadoop API's and packaging. I think to not surprise people using this, it's best to keep v1 as the default. Overall, we try not to change default values too often to make upgrading easy for people. - Patrick This vote is cancelled in favor of RC2. Please vote on releasing the following candidate as Apache Spark version 1.3.0! The tag to be voted on is v1.3.0-rc2 (commit 3af2687): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3af26870e5163438868c4eb2df88380a533bb232 The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.3.0-rc2/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc Staging repositories for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1074/ (published with version '1.3.0') https://repository.apache.org/content/repositories/orgapachespark-1075/ (published with version '1.3.0-rc2') The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.3.0-rc2-docs/ Please vote on releasing this package as Apache Spark 1.3.0! The vote is open until Saturday, March 07, at 04:17 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.3.0 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ == How does this compare to RC1 == This patch includes a variety of bug fixes found in RC1. == How can I help test this release? == If you are a Spark user, you can help us test this release by taking a Spark 1.2 workload and running on this release candidate, then reporting any regressions. If you are happy with this release based on your own testing, give a +1 vote. == What justifies a -1 vote for this release? == This vote is happening towards the end of the 1.3 QA period, so -1 votes should only occur for significant regressions from 1.2.1. Bugs already present in 1.2.X, minor regressions, or bugs related to new features will not block this release. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hey Marcelo, Yes - I agree. That one trickled in just as I was packaging this RC. However, I still put this out here to allow people to test the existing fixes, etc. - Patrick Hey Mingyu, I think it's broken out separately so we can record the time taken to serialize the result. Once we serializing it once, the second serialization should be really simple since it's just wrapping something that has already been turned into a byte buffer. Do you see a specific issue with serializing it twice? I think you need to have two steps if you want to record the time taken to serialize the result, since that needs to be sent back to the driver when the task completes. - Patrick I like #4 as well and agree with Aaron's suggestion. - Patrick Yes - only new or internal API's. I doubt we'd break any exposed APIs for the purpose of clean up. Patrick Please vote on releasing the following candidate as Apache Spark version 1.3.0! The tag to be voted on is v1.3.0-rc2 (commit 4aaf48d4): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4aaf48d46d13129f0f9bdafd771dd80fe568a7dc The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.3.0-rc3/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc Staging repositories for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1078 The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.3.0-rc3-docs/ Please vote on releasing this package as Apache Spark 1.3.0! The vote is open until Monday, March 09, at 02:52 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.3.0 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ == How does this compare to RC2 == This release includes the following bug fixes: https://issues.apache.org/jira/browse/SPARK-6144 https://issues.apache.org/jira/browse/SPARK-6171 https://issues.apache.org/jira/browse/SPARK-5143 https://issues.apache.org/jira/browse/SPARK-6182 https://issues.apache.org/jira/browse/SPARK-6175 == How can I help test this release? == If you are a Spark user, you can help us test this release by taking a Spark 1.2 workload and running on this release candidate, then reporting any regressions. If you are happy with this release based on your own testing, give a +1 vote. == What justifies a -1 vote for this release? == This vote is happening towards the end of the 1.3 QA period, so -1 votes should only occur for significant regressions from 1.2.1. Bugs already present in 1.2.X, minor regressions, or bugs related to new features will not block this release. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org This vote is cancelled in favor of RC3. I'll kick it off with a +1. Hey Sean, For these, the issue is that they are documentation JIRA's, which don't need to be timed exactly with the release vote, since we can update the documentation on the website whenever we want. In the past I've just mentally filtered these out when considering RC's. I see a few options here: 1. We downgrade such issues away from Blocker (more clear, but we risk loosing them in the fray if they really are things we want to have before the release is posted). 2. We provide a filter to the community that excludes 'Documentation'issues and shows all other blockers for 1.3. We can put this on the wiki, for instance. Which do you prefer? - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Sean, The docs are distributed and consumed in a fundamentally different way than Spark code itself. So we've always considered the "deadline" for doc changes to be when the release is finally posted. If there are small inconsistencies with the docs present in the source code for that release tag, IMO that doesn't matter much since we don't even distribute the docs with Spark's binary releases and virtually no one builds and hosts the docs on their own (that I am aware of, at least). Perhaps we can recommend if people want to build the doc sources that they should always grab the head of the most recent release branch, to set expectations accordingly. In the past we haven't considered it worth holding up the release process for the purpose of the docs. It just doesn't make sense since they are consumed "as a service". If we decide to change this convention, it would mean shipping our releases later, since we could't pipeline the doc finalization with voting. - Patrick For now, I'll just put this as critical. We can discuss the documentation stuff offline or in another thread. We probably want to revisit the way we do binaries in general for 1.4+. IMO, something worth forking a separate thread for. I've been hesitating to add new binaries because people (understandably) complain if you ever stop packaging older ones, but on the other hand the ASF has complained that we have too many binaries already and that we need to pare it down because of the large volume of files. Doubling the number of binaries we produce for Scala 2.11 seemed like it would be too much. One solution potentially is to actually package "Hadoop provided"binaries and encourage users to use these by simply setting HADOOP_HOME, or have instructions for specific distros. I've heard that our existing packages don't work well on HDP for instance, since there are some configuration quirks that differ from the upstream Hadoop. If we cut down on the cross building for Hadoop versions, then it is more tenable to cross build for Scala versions without exploding the number of binaries. - Patrick I think it's important to separate the goals from the implementation. I agree with Matei on the goal - I think the goal needs to be to allow people to download Apache Spark and use it with CDH, HDP, MapR, whatever... This is the whole reason why HDFS and YARN have stable API's, so that other projects can build on them in a way that works across multiple versions. I wouldn't want to force users to upgrade according only to some vendor timetable, that doesn't seem from the ASF perspective like a good thing for the project. If users want to get packages from Bigtop, or the vendors, that's totally fine too. My point earlier was - I am not sure we are actually accomplishing that goal now, because I've heard in some cases our "Hadoop 2.X"packages actually don't work on certain distributions, even those that are based on that Hadoop version. So one solution is to move towards "bring your own Hadoop" binaries and have users just set HADOOP_HOME and maybe document any vendor-specific configs that need to be set. That also happens to solve the "too many binaries" problem, but only incidentally. - Patrick I think that yes, longer term we want to have encryption of all communicated data. However Jeff, can you open a JIRA to discuss the design before opening a pull request (it's fine to link to a WIP branch if you'd like)? I'd like to better understand the performance and operational complexity of using SSL for this in comparison with alternatives. It would also be good to look at how the Hadoop encryption works for their shuffle service, in terms of the design decisions made there. - Patrick Hey All, Today there was a JIRA posted with an observed regression around Spark Streaming during certain recovery scenarios: https://issues.apache.org/jira/browse/SPARK-6222 My preference is to go ahead and ship this release (RC3) as-is and if this issue is isolated resolved soon, we can make a patch release in the next week or two. At some point, the cost of continuing to hold the release re/vote is so high that it's better to just ship the release. We can document known issues and point users to a fix once it's available. We did this in 1.2.0 as well (there were two small known issues) and I think as a point of process, this approach is necessary given the size of the project. I wanted to notify this thread though, in case this change anyones opinion on their release vote. I will leave the thread open at least until the end of today. Still +1 on RC3, for me. - Patrick Hey All, Marcelo Vanzin has been working on a patch for a few months that performs cross cutting clean-up and fixes to the way that Spark's launch scripts work (including PySpark, spark submit, the daemon scripts, etc.). The changes won't modify any public API's in terms of how those scripts are invoked. Historically, such patches have been difficult to test due to the number of interactions between components and interactions with external environments. I'd like to welcome people to test and/or code review this patch in their own environment. This patch is the in the very late stages of review and will likely be merged soon into master (eventually 1.4). https://github.com/apache/spark/pull/3916/files I'll ping this thread again once it is merged and we can establish a JIRA to encapsulate any issues. Just wanted to give a heads up as this is one of the larger internal changes we've made to this infrastructure since Spark 1.0 - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Does this matter for our own internal types in Spark? I don't think any of these types are designed to be used in RDD records, for instance. Hi All, I'm happy to announce the availability of Spark 1.3.0! Spark 1.3.0 is the fourth release on the API-compatible 1.X line. It is Spark's largest release ever, with contributions from 172 developers and more than 1,000 commits! Visit the release notes [1] to read about the new features, or download [2] the release today. For errata in the contributions or release notes, please e-mail me *directly* (not on-list). Thanks to everyone who helped work on this release! [1] http://spark.apache.org/releases/spark-release-1-3-0.html [2] http://spark.apache.org/downloads.html --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hey Sean, Yes, go crazy. Once we close the release vote, it's open season to merge backports into that release. - Patrick Cheng - what if you hold shift+refresh? For me the /latest link correctly points to 1.3.0 Hey Xiangrui, Do you want to write up a straw man proposal based on this line of discussion? - Patrick If the official solution from the Scala community is to use Java enums, then it seems strange they aren't generated in scaldoc? Maybe we can just fix that w/ Typesafe's help and then we can use them. My philosophy has been basically what you suggested, Sean. One thing you didn't mention though is if a bug fix seems complicated, I will think very hard before back-porting it. This is because "fixes" can introduce their own new bugs, in some cases worse than the original issue. It's really bad to have some upgrade to a patch release and see a regression - with our current approach this almost never happens. I will usually try to backport up to N-2, if it can be back-ported reasonably easily (for instance, with minor or no code changes). The reason I do this is that vendors do end up supporting older versions, and it's nice for them if some committer has backported a fix that they can then pull in, even if we never ship it. In terms of doing older maintenance releases, this one I think we should do according to severity of issues (for instance, if there is a security issue) or based on general command from the community. I haven't initiated many 1.X.2 releases recently because I didn't see huge demand. However, personally I don't mind doing these if there is a lot of demand, at least for releases where ".0" has gone out in the last six months. Yeah - to Nick's point, I think the way to do this is to pass in a custom conf when you create a Hadoop RDD (that's AFAIK why the conf field is there). Is there anything you can't do with that feature? Hey All, For a while we've published binary packages with different Hadoop client's pre-bundled. We currently have three interfaces to a Hadoop cluster (a) the HDFS client (b) the YARN client (c) the Hive client. Because (a) and (b) are supposed to be backwards compatible interfaces. My working assumption was that for the most part (modulo Hive) our packages work with *newer* Hadoop versions. For instance, our Hadoop 2.4 package should work with HDFS 2.6 and YARN 2.6. However, I have heard murmurings that these are not compatible in practice. So I have three questions I'd like to put out to the community: 1. Have people had difficulty using 2.4 packages with newer Hadoop versions? If so, what specific incompatibilities have you hit? 2. Have people had issues using our binary Hadoop packages in general with commercial or Apache Hadoop distro's, such that you have to build from source? 3. How would people feel about publishing a "bring your own Hadoop"binary, where you are required to point us to a local Hadoop distribution by setting HADOOP_HOME? This might be better for ensuring full compatibility: https://issues.apache.org/jira/browse/SPARK-6511 - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org We can probably better explain that if you are not using HDFS or YARN, you can download any binary. However, my question was about if the existing binaries do not work well with newer Hadoop versions, which I heard some people suggest but I'm looking for more specific issues. I see - if you look, in the saving functions we have the option for the user to pass an arbitrary Configuration. https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala#L894 It seems fine to have the same option for the loading functions, if it's easy to just pass this config into the input format. Yeah I agree that might have been nicer, but I think for consistency with the input API's maybe we should do the same thing. We can also give an example of how to clone sc.hadoopConfiguration and then set some new values: val conf = sc.hadoopConfiguration.clone() .set("k1", "v1") .set("k2", "v2") val rdd = sc.objectFile(..., conf) I have no idea if that's the correct syntax, but something like that seems almost as easy as passing a hashmap with deltas. - Patrick Great - that's even easier. Maybe we could have a simple example in the doc. I think we have a version of mapPartitions that allows you to tell Spark the partitioning is preserved: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L639 We could also add a map function that does same. Or you can just write your map using an iterator. - Patrick Hey Marcelo, Great question. Right now, some of the more active developers have an account that allows them to log into this cluster to inspect logs (we copy the logs from each run to a node on that cluster). The infrastructure is maintained by the AMPLab. I will put you in touch the someone there who can get you an account. This is a short term solution. The longer term solution is to have these scp'd regularly to an S3 bucket or somewhere people can get access to them, but that's not ready yet. - Patrick Please vote on releasing the following candidate as Apache Spark version 1.3.1! The tag to be voted on is v1.3.1-rc1 (commit 0dcb5d9f): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=0dcb5d9f31b713ed90bcec63ebc4e530cbb69851 The list of fixes present in this release can be found at: http://bit.ly/1C2nVPY The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.3.1-rc1/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1080 The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.3.1-rc1-docs/ Please vote on releasing this package as Apache Spark 1.3.1! The vote is open until Wednesday, April 08, at 01:10 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.3.1 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Please vote on releasing the following candidate as Apache Spark version 1.2.2! The tag to be voted on is v1.2.2-rc1 (commit 7531b50): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7531b50e406ee2e3301b009ceea7c684272b2e27 The list of fixes present in this release can be found at: http://bit.ly/1DCNddt The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.2.2-rc1/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1082/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.2.2-rc1-docs/ Please vote on releasing this package as Apache Spark 1.2.2! The vote is open until Thursday, April 08, at 00:30 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.2.2 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I believe TD just forgot to set the fix version on the JIRA. There is a fix for this in 1.3: https://github.com/apache/spark/commit/03e263f5b527cf574f4ffcd5cd886f7723e3756e - Patrick What if you don't run zinc? I.e. just download maven and run that "mvn package...". It might take longer, but I wonder if it will work. The issue is that if you invoke "build/mvn" it will start zinc again if it sees that it is killed. The absolute most "sterile" thing to do is this: 1. Kill any zinc processes. 2. Clean up spark "git clean -fdx" (WARNING: this will delete any staged changes you have, if you have code modifications or extra files around) 3. Run the 2.11 script to change the versions. 4. Run "mvn package" with maven that you installed on your machine. One thing that I think can cause issues is if you run build/mvn with Scala 2.10, then try to run it with 2.11, since I think we may store some downloaded jars relating to zinc that will get screwed up. Not sure that's what is happening, just an idea. The only think that can persist outside of Spark is if there is still a live Zinc process. We took care to make sure this was a generally stateless mechanism. Both the 1.2.X and 1.3.X releases are built with Scala 2.11 for packaging purposes. And these have been built as recently as in the last few days, since we are voting on 1.2.2 and 1.3.1. However there could be issues that only affect certain environments. - Patrick Hmm..  Make sure you are building with the right flags. I think you need to pass -Dscala-2.11 to maven. Take a look at the upstream docs - on my phone now so can't easily access. Hey All, Today SPARK-6737 came to my attention. This is a bug that causes a memory leak for any long running program that repeatedly saves data out to a Hadoop FileSystem. For that reason, it is problematic for Spark Streaming. My sense is that this is severe enough to cut another RC once the fix is merged (which is imminent): https://issues.apache.org/jira/browse/SPARK-6737 I'll leave a bit of time for others to comment, in particular if people feel we should not wait for this fix. - Patrick This vote is cancelled in favor of RC2. Please vote on releasing the following candidate as Apache Spark version 1.3.1! The tag to be voted on is v1.3.1-rc2 (commit 7c4473a): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7c4473aa5a7f5de0323394aaedeefbf9738e8eb5 The list of fixes present in this release can be found at: http://bit.ly/1C2nVPY The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.3.1-rc2/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1083/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.3.1-rc2-docs/ The patches on top of RC1 are: [SPARK-6737] Fix memory leak in OutputCommitCoordinator https://github.com/apache/spark/pull/5397 [SPARK-6636] Use public DNS hostname everywhere in spark_ec2.py https://github.com/apache/spark/pull/5302 [SPARK-6205] [CORE] UISeleniumSuite fails for Hadoop 2.x test with NoClassDefFoundError https://github.com/apache/spark/pull/4933 Please vote on releasing this package as Apache Spark 1.3.1! The vote is open until Saturday, April 11, at 07:00 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.3.1 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hey Denny, I beleive the 2.4 bits are there. The 2.6 bits I had done specially (we haven't merge that into our upstream build script). I'll do it again now for RC2. - Patrick Oh I see - ah okay I'm guessing it was a transient build error and I'll get it posted ASAP. This vote is cancelled in favor of RC3, due to SPARK-6851. Please vote on releasing the following candidate as Apache Spark version 1.3.1! The tag to be voted on is v1.3.1-rc2 (commit 3e83913): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3e8391327ba586eaf54447043bd526d919043a44 The list of fixes present in this release can be found at: http://bit.ly/1C2nVPY The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.3.1-rc3/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1088/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.3.1-rc3-docs/ The patches on top of RC2 are: [SPARK-6851] [SQL] Create new instance for each converted parquet relation [SPARK-5969] [PySpark] Fix descending pyspark.rdd.sortByKey. [SPARK-6343] Doc driver-worker network reqs [SPARK-6767] [SQL] Fixed Query DSL error in spark sql Readme [SPARK-6781] [SQL] use sqlContext in python shell [SPARK-6753] Clone SparkConf in ShuffleSuite tests [SPARK-6506] [PySpark] Do not try to retrieve SPARK_HOME when not needed... Please vote on releasing this package as Apache Spark 1.3.1! The vote is open until Tuesday, April 14, at 07:00 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.3.1 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Would just like to encourage everyone who is active in day-to-day development to give feedback on this (and I will do same). Sean has spent a lot of time looking through different ways we can streamline our dev process. - Patrick +1 from myself as well +1 from me ass well. This vote passes with 10 +1 votes (5 binding) and no 0 or -1 votes. +1: Sean Owen* Reynold Xin* Krishna Sankar Denny Lee Mark Hamstra* Sean McNamara* Sree V Marcelo Vanzin GuoQiang Li Patrick Wendell* 0: -1: I will work on packaging this release in the next 48 hours. - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I'd like to close this vote to coincide with the 1.3.1 release, however, it would be great to have more people test this release first. I'll leave it open for a bit longer and see if others can give a +1. I'm gonna go ahead and close this now - thanks everyone for voting! This vote passes with 7 +1 votes (6 binding) and no 0 or -1 votes. +1: Mark Hamstra* Reynold Xin Kirshna Sankar Sean Owen* Tom Graves* Joseph Bradley* Sean McNamara* 0: -1: Thanks! - Patrick Hi All, I'm happy to announce the Spark 1.3.1 and 1.2.2 maintenance releases. We recommend all users on the 1.3 and 1.2 Spark branches upgrade to these releases, which contain several important bug fixes. Download Spark 1.3.1 or 1.2.2: http://spark.apache.org/downloads.html Release notes: 1.3.1: http://spark.apache.org/releases/spark-release-1-3-1.html 1.2.2:  http://spark.apache.org/releases/spark-release-1-2-2.html Comprehensive list of fixes: 1.3.1: http://s.apache.org/spark-1.3.1 1.2.2: http://s.apache.org/spark-1.2.2 Thanks to everyone who worked on these releases! - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Good catch Olivier - I'll take care of it. Tracking this on SPARK-7027. It could be a good idea to document this a bit. The original goals were to give people an easy way to get started with Spark and also to provide a consistent environment for our own experiments and benchmarking of Spark at the AMPLab. Over time I've noticed a huge amount of scope increase in terms of what people want to do and I do know that many companies run production infrastructure based on launching the EC2 scripts. My feeling is that the general problem of deploying Spark with other applications and frameworks is fairly well covered by projects which specifically focus on packaging and automation (e.g. Whirr, BigTop, etc). So I'd like to see a narrower focus on just getting a vanilla Spark cluster up and running and make it clear that customization and extension of that functionality is really not in scope. This doesn't mean discouraging people from using it for production use cases, but more that they shouldn't expect us to merge and maintain things that seek to do broader integration with other technologies, automation, etc. - Patrick One over arching issue is that it's pretty unclear what "Assigned to X" in JIAR means from a process perspective. Personally I actually feel it's better for this to be more historical - i.e. who ended up submitting a patch for this feature that was merged - rather than creating an exclusive reservation for a particular user to work on something. If an issue is "assigned" to person X, but some other person Y submits a great patch for it, I think we have some obligation to Spark users and to the community to merge the better patch. So the idea of reserving the right to add a feature, it just seems overall off to me. IMO, its fine if multiple people want to submit competing patches for something, provided everyone comments on JIRA saying they are intending to submit a patch, and everyone understands there is duplicate effort. So commenting with an intention to submit a patch, IMO seems like the healthiest workflow since it is non exclusive. To me the main benefit of "assigning" something ahead of time is if you have a committer that really wants to see someone specific work on a patch, it just acts as a strong signal that there is someone endorsed to work on that patch. That doesn't mean no one else can submit a patch, but it is IMO more of a warning that there may be existing work which is likely to be high quality, to avoid duplicated effort. When it was really easy to assign features to themselves, I saw a lot of anti-patterns in the community that seemed unhealthy, specifically: - It was really unclear what it means semantically if someone is assigned to a JIRA. - People assign JIRA's to themselves that aren't a good fit, given the authors level of experience. - People expect if they assign JIRA's to themselves that others won't submit patches, and become upset if they do. - People are discouraged from working on a patch because someone else was officially assigned. - Patrick Hi Vinod, Thanks for you thoughts - However, I do not agree with your sentiment and implications. Spark is broadly quite an inclusive project and we spend a lot of effort culturally to help make newcomers feel welcome. - Patrick It's a bit of a digression - but Steve's suggestion that we have a mailing list for new issues is a great idea and we can do it easily. We could nave new-issues@s.a.o or something (we already have issues@s.a.o). - Patrick Using our ASF git repository as a working area for design docs, it seems potentially concerning to me. It's difficult process wise because all commits need to go through committers and also, we'd pollute our git history a lot with random incremental design updates. The git history is used a lot by downstream packagers, us during our QA process, etc... we really try to keep it oriented around code patches: https://git-wip-us.apache.org/repos/asf?p=spark.git;a=shortlog Committing a polished design doc along with a feature, maybe that's something we could consider. But I still think JIRA is the best location for these docs, consistent with what most other ASF projects do that I know. It is true that in the past we've posted community tutorials on the site. Spark has grown a lot since then and it might be a better fit at this point to curate community tutorials on the wiki (something like the powered by page) and link to them from the documentation website. The documentation page overall could use some love, as it's pretty outdated. - Patrick Hey All, Just a friendly reminder that May 1st is the feature freeze for Spark 1.4, meaning major outstanding changes will need to land in the next week. After May 1st we'll package a release for testing and then go into the normal triage process where bugs are prioritized and some smaller features are allowed on a case by case basis (if they are additive/feature flagged/etc). As always, I'll invite the community to help participate in code review of patches in the next week, since review bandwidth is the single biggest determinant of how many features will get in. Please also keep in mind that most active committers are working overtime (nights/weekends) during this period and will try their best to help usher in as many patches as possible, along with their own code. As a reminder, release window dates are always maintained on the wiki and are updated after each release according to our 3 month release cadence: https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage Thanks - and happy coding! - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I actually don't totally see why we can't use Google Docs provided it is clearly discoverable from the JIRA. It was my understanding that many projects do this. Maybe not (?). If it's a matter of maintaining public record on ASF infrastructure, perhaps we can just automate that if an issue is closed we capture the doc content and attach it to the JIRA as a PDF. My sense is that in general the ASF infrastructure policy is becoming more and more lenient with regards to using third party services, provided the are broadly accessible (such as a public google doc) and can be definitively archived on ASF controlled storage. - Patrick Hey Punya, There is some ongoing work to help make Hive upgrades more manageable and allow us to support multiple versions of Hive. Once we do that, it will be much easier for us to upgrade. https://issues.apache.org/jira/browse/SPARK-6906 - Patrick I'd also support this. In general, I think it's good that we try to have Spark support different versions of things (Hadoop, Hive, etc). But at some point you need to weigh the costs of doing so against the number of users affected. In the case of Java 6, we are seeing increasing cost from this. Some of the newer unsafe code is not supported in Java 6 (and it's a pretty large internal initiative). And the ability to upgrade dependencies is starting to cause pain for users. Sean and I had to "wontfix" an important bug fix for users because the library requires JRE 7. I reverted the patch that I think was causing this: SPARK-5213 Thanks Maybe I can help a bit. What happens when you call .map(my func) is that you create a MapPartitionsRDD that has a reference to that closure in it's compute() function. When a job is run (jobs are run as the result of RDD actions): https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L520 When this happens the RDD will generate ShuffleMapTask's for physically computing the MapPartitionsRDD. The ShuffleMapTask will be shipped to the executor and then run() https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala#L70 The ShuffleMapTask will call rdd.iterator() which will eventually call into compute() https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L240 - Patrick Hi Devs, Just an announcement that I've cut Spark's branch 1.4 to form the basis of the 1.4 release. Other than a few stragglers, this represents the end of active feature development for Spark 1.4. Per usual, if committers are merging any features, please be in touch so I can help coordinate. Any new commits will need to be explicitly merged into this branch. I'd like to invite the community to begin testing this release. This week I can create preview packages to help with it. Please help test Spark 1.4 and report any regressions! - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org If we just set JAVA_HOME in dev/run-test-jenkins, I think it should work. Hey All, Community testing during the QA window is an important part of the release cycle in Spark. It helps us deliver higher quality releases by vetting out issues not covered by our unit tests. I was thinking that from now on, it would be nice to recognize the organizations that donate time to help test release previews and release candidates. So starting with Spark 1.4, barring any major objection, I was planning to highlight those groups as test partners when we make release notes, similar to how we thank contributors for the release. That is, companies or teams that commit to doing nontrivial release testing such as testing real workloads, performance tests, or other integration tests. Hopefully this will provide recognition to those parts of the community and also encourage others to help test Spark releases. - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org If there is broad consensus here to drop Java 1.6 in Spark 1.5, should we do an ANNOUNCE to user and dev? For unknown reasons, pull requests on Jenkins worker 3 have been failing with an exception[1]. After trying to fix this by clearing the ivy and maven caches on the node, I've given up and simply blacklisted that worker. [error] oro#oro;2.0.8!oro.jar origin location must be absolute: file:/home/jenkins/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar java.lang.IllegalArgumentException: oro#oro;2.0.8!oro.jar origin location must be absolute: file:/home/jenkins/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar at org.apache.ivy.util.Checks.checkAbsolute(Checks.java:57) at org.apache.ivy.core.cache.DefaultRepositoryCacheManager.getArchiveFileInCache(DefaultRepositoryCacheManager.java:385) at org.apache.ivy.core.cache.DefaultRepositoryCacheManager.download(DefaultRepositoryCacheManager.java:849) at org.apache.ivy.plugins.resolver.BasicResolver.download(BasicResolver.java:835) at org.apache.ivy.plugins.resolver.RepositoryResolver.download(RepositoryResolver.java:282) at org.apache.ivy.plugins.resolver.ChainResolver.download(ChainResolver.java:219) at org.apache.ivy.plugins.resolver.ChainResolver.download(ChainResolver.java:219) at org.apache.ivy.core.resolve.ResolveEngine.downloadArtifacts(ResolveEngine.java:388) at org.apache.ivy.core.resolve.ResolveEngine.resolve(ResolveEngine.java:331) at org.apache.ivy.Ivy.resolve(Ivy.java:517) at sbt.IvyActions$.sbt$IvyActions$$resolve(IvyActions.scala:266) at sbt.IvyActions$$anonfun$updateEither$1.apply(IvyActions.scala:175) at sbt.IvyActions$$anonfun$updateEither$1.apply(IvyActions.scala:157) at sbt.IvySbt$Module$$anonfun$withModule$1.apply(Ivy.scala:151) at sbt.IvySbt$Module$$anonfun$withModule$1.apply(Ivy.scala:151) at sbt.IvySbt$$anonfun$withIvy$1.apply(Ivy.scala:128) at sbt.IvySbt.sbt$IvySbt$$action$1(Ivy.scala:56) at sbt.IvySbt$$anon$4.call(Ivy.scala:64) ... --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Thanks Shane - really useful as I know that several companies are interested in having in-house replicas of our QA infra. In Spark we sometimes close issues as something other than "Fixed", and this is an important part of maintaining our JIRA. The current resolution types we use are the following: Won't Fix - bug fix or (more often) feature we don't want to add Invalid - issue is underspecified or not appropriate for a JIRA issue Duplicate - duplicate of another JIRA Cannot Reproduce - bug that could not be reproduced Not A Problem - issue purports to represent a bug, but does not I would like to propose adding a few new resolutions. This will require modifying the ASF JIRA, but infra said they are open to proposals as long as they are considered of broad interest. My issue with the current set of resolutions are that "Won't Fix" is a big catch all we use for many different things. Most often it's used for things that aren't even bugs even though it has "Fix" in the name. I'm proposing adding: Inactive - A feature or bug that has had no activity from users or developers in a long time Out of Scope - A feature proposal that is not in scope given the projects goals Later - A feature not on the immediate roadmap, but potentially of interest longer term (this one already exists, I'm just proposing to start using it) I am in no way proposing changes to the decision making model around JIRA's, notably that it is consensus based and that all resolutions are considered tentative and fully reversible. The benefits I see of this change would be the following: 1. Inactive: A way to clear out inactive/dead JIRA's without indicating a decision has been made one way or the other. 2. Out of Scope: It more clearly explains closing out-of-scope features than the generic "Won't Fix". Also makes it more clear to future contributors what is considered in scope for Spark. 3. Later: A way to signal that issues aren't targeted for a near term version. This would help avoid the mess we have now of like 200+ issues targeted at each version and target version being a very bad indicator of actual roadmap. An alternative on this one is to have a version called "Later" or "Parking Lot" but not close the issues. Any thoughts on this? - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Due to an ASF infrastructure change (bug?) [1] the default JIRA resolution status has switched to "Pending Closed". I've made a change to our merge script to coerce the correct status of "Fixed" when resolving [2]. Please upgrade the merge script to master. I've manually corrected JIRA's that were closed with the incorrect status. Let me know if you have any issues. [1] https://issues.apache.org/jira/browse/INFRA-9646 [2] https://github.com/apache/spark/commit/1b9e434b6c19f23a01e9875a3c1966cd03ce8e2d --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hey Kevin and Ron, So is the main shortcoming of the launcher library the inability to get an app ID back from YARN? Or are there other issues here that fundamentally regress things for you. It seems like adding a way to get back the appID would be a reasonable addition to the launcher. - Patrick Hi All - unfortunately the fix introduced another bug, which is that fixVersion was not updated properly. I've updated the script and had one other person test it. So committers please pull from master again thanks! - Patrick Hey Chester, Thanks for sending this. It's very helpful to have this list. The reason we made the Client API private was that it was never intended to be used by third parties programmatically and we don't intend to support it in its current form as a stable API. We thought the fact that it was for internal use would be obvious since it accepts arguments as a string array of CL args. It was always intended for command line use and the stable API was the command line. When we migrated the Launcher library we figured we covered most of the use cases in the off chance someone was using the Client. It appears we regressed one feature which was a clean way to get the app ID. The items you list here 2-6 all seem like new feature requests rather than a regression caused by us making that API private. I think the way to move forward is for someone to design a proper long-term stable API for the things you mentioned here. That could either be by extension of the Launcher library. Marcelo would be natural to help with this effort since he was heavily involved in both YARN support and the launcher. So I'm curious to hear his opinion on how best to move forward. I do see how apps that run Spark would benefit of having a control plane for querying status, both on YARN and elsewhere. - Patrick Yeah I wrote the original script and I intentionally made it easy for other projects to use (you'll just need to tweak some variables at the top). You just need somewhere to run it... we were using a jenkins cluster to run it every 5 minutes. BTW - I looked and there is one instance where it hard cores the string "SPARK-", but that should be easy to change. I'm happy to review a patch that makes that prefix a variable. https://github.com/apache/spark/blob/master/dev/github_jira_sync.py#L71 - Patrick Hi Niranda, Maintenance releases are not done on a predetermined schedule but instead according to which fixes show up and their severity. Since we just did a 1.3.1 release I'm not sure I see 1.3.2 on the immediate horizon. However, the maintenance releases are simply builds at the head of the respective release branches (in this case branch-1.3). They never introduce new API's. If you have a particular bug fix you are waiting for, you can always build Spark off of that branch. - Patrick The PR builder currently builds against Hadoop 2.3. - Patrick Sorry premature send: The PR builder currently builds against Hadoop 2.3 https://github.com/apache/spark/blob/master/dev/run-tests#L54 We can set this to whatever we want. 2.2 might make sense since it's the default in our published artifacts. - Patrick If there is no further feedback on this I will ask ASF Infra to add the new fields "Out of Scope" and "Inactive". - Patrick Please vote on releasing the following candidate as Apache Spark version 1.4.0! The tag to be voted on is v1.4.0-rc1 (commit 777a081): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=777a08166f1fb144146ba32581d4632c3466541e The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-1.4.0-rc1/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: https://repository.apache.org/content/repositories/orgapachespark-1092/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-1.4.0-rc1-docs/ Please vote on releasing this package as Apache Spark 1.4.0! The vote is open until Friday, May 22, at 17:03 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.4.0 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ == How can I help test this release? == If you are a Spark user, you can help us test this release by taking a Spark 1.3 workload and running on this release candidate, then reporting any regressions. == What justifies a -1 vote for this release? == This vote is happening towards the end of the 1.4 QA period, so -1 votes should only occur for significant regressions from 1.3.1. Bugs already present in 1.3.X, minor regressions, or bugs related to new features will not block this release. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org A couple of other process things: 1. Please *keep voting* (+1/-1) on this thread even if we find some issues, until we cut RC2. This lets us pipeline the QA. 2. The SQL team owes a JIRA clean-up (forthcoming shortly)... there are still a few "Blocker's" that aren't. Punya, Let me see if I can publish these under rc1 as well. In the future this will all be automated but current it's a somewhat manual task. - Patrick Hey All, Since we are now voting, please tread very carefully with branch-1.4 merges. For instances, bug fixes that don't represent regressions from 1.3.X, these probably shouldn't be merged unless they are extremely simple and well reviewed. As usual mature/core components (e.g. Spark core) are more sensitive than newer/edge ones (e.g. Dataframes). I'm happy to provide guidance to people if they are on the fence about patches. Ultimately this ends up being a matter of judgement and assessing risk of specific patches. Just ping me on github. - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org HI all, I've created another release repository where the release is identified with the version 1.4.0-rc1: https://repository.apache.org/content/repositories/orgapachespark-1093/ Thanks Andrew, the doc issue should be fixed in RC2 (if not, please chine in!). R was missing in the build envirionment. - Patrick Thanks Ted - there is no need for people to upgrade at this point, since the changes in the release scripts just modify it not to rely on default behavior. Yes - spark packages can include non ASF licenses. This vote is cancelled in favor of RC2. Hey jameszhouyi, Since SPARK-7119 is not a regression from earlier versions, we won't hold the release for it. However, please comment on the JIRA if it is affecting you... it will help us prioritize the bug. - Patrick Please vote on releasing the following candidate as Apache Spark version 1.4.0! The tag to be voted on is v1.4.0-rc2 (commit 03fb26a3): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=03fb26a3e50e00739cc815ba4e2e82d71d003168 The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc2-bin/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: [published as version: 1.4.0] https://repository.apache.org/content/repositories/orgapachespark-1103/ [published as version: 1.4.0-rc2] https://repository.apache.org/content/repositories/orgapachespark-1104/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc2-docs/ Please vote on releasing this package as Apache Spark 1.4.0! The vote is open until Wednesday, May 27, at 08:12 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.4.0 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ == What has changed since RC1 == Below is a list of bug fixes that went into this RC: http://s.apache.org/U1M == How can I help test this release? == If you are a Spark user, you can help us test this release by taking a Spark 1.3 workload and running on this release candidate, then reporting any regressions. == What justifies a -1 vote for this release? == This vote is happening towards the end of the 1.4 QA period, so -1 votes should only occur for significant regressions from 1.3.1. Bugs already present in 1.3.X, minor regressions, or bugs related to new features will not block this release. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi All, This week I got around to setting up nightly builds for Spark on Jenkins. I'd like feedback on these and if it's going well I can merge the relevant automation scripts into Spark mainline and document it on the website. Right now I'm doing: 1. SNAPSHOT's of Spark master and release branches published to ASF Maven snapshot repo: https://repository.apache.org/content/repositories/snapshots/org/apache/spark/ These are usable by adding this repository in your build and using a snapshot version (e.g. 1.3.2-SNAPSHOT). 2. Nightly binary package builds and doc builds of master and release versions. http://people.apache.org/~pwendell/spark-nightly/ These build 4 times per day and are tagged based on commits. If anyone has feedback on these please let me know. Thanks! - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi James, As I said before that is not a blocker issue for this release, thanks. Separately, there are some comments in this code review that indicate you may be facing a bug in your own code rather than with Spark: https://github.com/apache/spark/pull/5688#issuecomment-104491410 Please follow up on that issue outside of the vote thread. Thanks! Please vote on releasing the following candidate as Apache Spark version 1.4.0! The tag to be voted on is v1.4.0-rc3 (commit dd109a8): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=dd109a8746ec07c7c83995890fc2c0cd7a693730 The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc3-bin/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: [published as version: 1.4.0] https://repository.apache.org/content/repositories/orgapachespark-1109/ [published as version: 1.4.0-rc3] https://repository.apache.org/content/repositories/orgapachespark-1110/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc3-docs/ Please vote on releasing this package as Apache Spark 1.4.0! The vote is open until Tuesday, June 02, at 00:32 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.4.0 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ == What has changed since RC1 == Below is a list of bug fixes that went into this RC: http://s.apache.org/vN == How can I help test this release? == If you are a Spark user, you can help us test this release by taking a Spark 1.3 workload and running on this release candidate, then reporting any regressions. == What justifies a -1 vote for this release? == This vote is happening towards the end of the 1.4 QA period, so -1 votes should only occur for significant regressions from 1.3.1. Bugs already present in 1.3.X, minor regressions, or bugs related to new features will not block this release. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Thanks for all the discussion on the vote thread. I am canceling this vote in favor of RC3. Hey Bobby, Those are generic warnings that the hadoop libraries throw. If you are using MapRFS they shouldn't matter since you are using the MapR client and not the default hadoop client. Do you have any issues with functionality... or was it just seeing the warnings that was the concern? Thanks for helping test! - Patrick This vote is cancelled in favor of RC4. Thanks everyone for the thorough testing of this RC. We are really close, but there were a few blockers found. I've cut a new RC to incorporate those issues. The following patches were merged during the RC3 testing period: (blockers) 4940630 [SPARK-8020] [SQL] Spark SQL conf in spark-defaults.conf make metadataHive get constructed too early 6b0f615 [SPARK-8038] [SQL] [PYSPARK] fix Column.when() and otherwise() 78a6723 [SPARK-7978] [SQL] [PYSPARK] DecimalType should not be singleton (other fixes) 9d6475b [SPARK-6917] [SQL] DecimalType is not read back when non-native type exists 97d4cd0 [SPARK-8049] [MLLIB] drop tmp col from OneVsRest output cbaf595 [SPARK-8014] [SQL] Avoid premature metadata discovery when writing a HadoopFsRelation with a save mode other than Append fa292dc [SPARK-8015] [FLUME] Remove Guava dependency from flume-sink. f71a09d [SPARK-8037] [SQL] Ignores files whose name starts with dot in HadoopFsRelation 292ee1a [SPARK-8021] [SQL] [PYSPARK] make Python read/write API consistent with Scala 87941ff [SPARK-8023][SQL] Add "deterministic" attribute to Expression to avoid collapsing nondeterministic projects. e6d5895 [SPARK-7965] [SPARK-7972] [SQL] Handle expressions containing multiple window expressions and make parser match window frames in case insensitive way 8ac2376 [SPARK-8026][SQL] Add Column.alias to Scala/Java DataFrame API efc0e05 [SPARK-7982][SQL] DataFrame.stat.crosstab should use 0 instead of null for pairs that don't appear cbfb682a [SPARK-8028] [SPARKR] Use addJar instead of setJars in SparkR a7c8b00 [SPARK-7958] [STREAMING] Handled exception in StreamingContext.start() to prevent leaking of actors a76c2e1 [SPARK-7899] [PYSPARK] Fix Python 3 pyspark/sql/types module conflict f1d4e7e [SPARK-7227] [SPARKR] Support fillna / dropna in R DataFrame. 01f38f7 [SPARK-7979] Enforce structural type checker. 2c45009 [SPARK-7459] [MLLIB] ElementwiseProduct Java example 8938a74 [SPARK-7962] [MESOS] Fix master url parsing in rest submission client. 1513cff [SPARK-7957] Preserve partitioning when using randomSplit 9a88be1 [SPARK-6013] [ML] Add more Python ML examples for spark.ml 2bd4460 [SPARK-7954] [SPARKR] Create SparkContext in sparkRSQL init Please vote on releasing the following candidate as Apache Spark version 1.4.0! The tag to be voted on is v1.4.0-rc3 (commit 22596c5): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h= 22596c534a38cfdda91aef18aa9037ab101e4251 The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc4-bin/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: [published as version: 1.4.0] https://repository.apache.org/content/repositories/orgapachespark-1111/ [published as version: 1.4.0-rc4] https://repository.apache.org/content/repositories/orgapachespark-1112/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc4-docs/ Please vote on releasing this package as Apache Spark 1.4.0! The vote is open until Saturday, June 06, at 05:00 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.4.0 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ == What has changed since RC3 == In addition to may smaller fixes, three blocker issues were fixed: 4940630 [SPARK-8020] [SQL] Spark SQL conf in spark-defaults.conf make metadataHive get constructed too early 6b0f615 [SPARK-8038] [SQL] [PYSPARK] fix Column.when() and otherwise() 78a6723 [SPARK-7978] [SQL] [PYSPARK] DecimalType should not be singleton == How can I help test this release? == If you are a Spark user, you can help us test this release by taking a Spark 1.3 workload and running on this release candidate, then reporting any regressions. == What justifies a -1 vote for this release? == This vote is happening towards the end of the 1.4 QA period, so -1 votes should only occur for significant regressions from 1.3.1. Bugs already present in 1.3.X, minor regressions, or bugs related to new features will not block this release. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org He all - a tiny nit from the last e-mail. The tag is v1.4.0-rc4. The exact commit and all other information is correct. (thanks Shivaram who pointed this out). I will give +1 as well. Hey All, Just a request here - it would be great if people could create JIRA's for any and all merged pull requests. The reason is that when patches get reverted due to build breaks or other issues, it is very difficult to keep track of what is going on if there is no JIRA. Here is a list of 5 patches we had to revert recently that didn't include a JIRA: Revert "[MINOR] [BUILD] Use custom temp directory during build."Revert "[SQL] [TEST] [MINOR] Uses a temporary log4j.properties in HiveThriftServer2Test to ensure expected logging behavior"Revert "[BUILD] Always run SQL tests in master build."Revert "[MINOR] [CORE] Warn users who try to cache RDDs with dynamic allocation on."Revert "[HOT FIX] [YARN] Check whether `/lib` exists before listing its files"The cost overhead of creating a JIRA relative to other aspects of development is very small. If it's *really* a documentation change or something small, that's okay. But anything affecting the build, packaging, etc. These all need to have a JIRA to ensure that follow-up can be well communicated to all Spark developers. Hopefully this is something everyone can get behind, but opened a discussion here in case others feel differently. - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi All, Thanks for the continued voting! I'm going to leave this thread open for another few days to continue to collect feedback. - Patrick This vote passes! Thanks to everyone who voted. I will get the release artifacts and notes up within a day or two. +1 (23 votes): Reynold Xin* Patrick Wendell* Matei Zaharia* Andrew Or* Timothy Chen Calvin Jia Burak Yavuz Krishna Sankar Hari Shreedharan Ram Sriharsha* Kousuke Saruta Sandy Ryza Marcelo Vanzin Bobby Chowdary Mark Hamstra Guoqiang Li Joseph Bradley Sean McNamara Tathagata Das* Ajay Singal Wang, Daoyuan Denny Lee Forest Fang 0: -1: * Binding Hey Hector, It's not a bad idea. I think we'd want to do this by virtue of allowing custom repositories, so users can add bintray or others. - Patrick Hi All, I'm happy to announce the availability of Spark 1.4.0! Spark 1.4.0 is the fifth release on the API-compatible 1.X line. It is Spark's largest release ever, with contributions from 210 developers and more than 1,000 commits! A huge thanks go to all of the individuals and organizations involved in development and testing of this release. Visit the release notes [1] to read about the new features, or download [2] the release today. For errata in the contributions or release notes, please e-mail me *directly* (not on-list). Thanks to everyone who helped work on this release! [1] http://spark.apache.org/releases/spark-release-1-4-0.html [2] http://spark.apache.org/downloads.html --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hey Sean, Thanks for bringing this up - I went through and fixed about 10 of them. Unfortunately there isn't a hard and fast way to resolve them. I found all of the following: - Features that missed the release and needed to be retargeted to 1.5. - Bugs that missed the release and needed to be retargeted to 1.4.1. - Issues that were not properly targeted (e.g. someone randomly set the target version) and should probably be untargeted. I'd like to encourage others to do this, especially the more active developers on different components (Streaming, ML, etc). One other question is what the semantics of target version are, which I don't think we've defined clearly. Is it the target of the person contributing the feature? Or in some sense the target of the committership? My preference would be that targeting a JIRA has some strong semantics - i.e. it means the commiter targeting has mentally allocated time to review a patch for that feature in the timeline of that release. I.e. prefer to have fewer targeted JIRA's for a release, and also expect to get most of the targeted features merged into a release. In the past I think targeting has meant different things to different people. - Patrick Please vote on releasing the following candidate as Apache Spark version 1.4.1! This release fixes a handful of known issues in Spark 1.4.0, listed here: http://s.apache.org/spark-1.4.1 The tag to be voted on is v1.4.1-rc1 (commit 60e08e5): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h= 60e08e50751fe3929156de956d62faea79f5b801 The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-releases/spark-1.4.1-rc1-bin/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: [published as version: 1.4.1] https://repository.apache.org/content/repositories/orgapachespark-1118/ [published as version: 1.4.1-rc1] https://repository.apache.org/content/repositories/orgapachespark-1119/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-releases/spark-1.4.1-rc1-docs/ Please vote on releasing this package as Apache Spark 1.4.1! The vote is open until Saturday, June 27, at 06:32 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.4.1 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hey Sean, This is being shipped now because there is a severe bug in 1.4.0 that can cause data corruption for Parquet users. There are no blockers targeted for 1.4.1 - so I don't see that JIRA is inconsistent with shipping a release now. The goal of having every single targeted JIRA cleared by the time we start voting, I don't think there is broad consensus and cultural adoption of that principle yet. So I do not take it as a signal this release is premature (the story has been the same for every previous release we've ever done). The fact that we hit 90/124 of issues targeted at this release means we are targeting such that we get around 70% of issues merged. That actually doesn't seem so bad to me since there is some uncertainty in the process. B - Patrick Hey Tom - no one voted on this yet, so I need to keep it open until people vote. But I'm not aware of specific things we are waiting for. Anyone else? - Patrick Hey Krishna - this is still the current release candidate. - Patrick Hey Sean - yes I think that is an issue. Our published poms need to have the dependency versions inlined. We probably need to revert that bit of the build patch. - Patrick This vote is cancelled in favor of RC2. Thanks very much to Sean Owen for triaging an important bug associated with RC1. I took a look at the branch-1.4 contents and I think its safe to cut RC2 from the head of that branch (i.e no very high risk patches that I could see). JIRA management around the time of the RC voting is an interesting topic, Sean I like your most recent proposal. Maybe we can put that on the wiki or start a DISCUSS thread to cover that topic. Please vote on releasing the following candidate as Apache Spark version 1.4.1! This release fixes a handful of known issues in Spark 1.4.0, listed here: http://s.apache.org/spark-1.4.1 The tag to be voted on is v1.4.1-rc2 (commit 07b95c7): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h= 07b95c7adf88f0662b7ab1c47e302ff5e6859606 The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-releases/spark-1.4.1-rc2-bin/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: [published as version: 1.4.1] https://repository.apache.org/content/repositories/orgapachespark-1120/ [published as version: 1.4.1-rc2] https://repository.apache.org/content/repositories/orgapachespark-1121/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-releases/spark-1.4.1-rc2-docs/ Please vote on releasing this package as Apache Spark 1.4.1! The vote is open until Monday, July 06, at 22:00 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.4.1 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org What if you use the built-in maven (i.e. build/mvn). It might be that we require a newer version of maven than you have. The release itself is built with maven 3.3.3: https://github.com/apache/spark/blob/master/build/mvn#L72 - Patrick Can you try using the built in maven "build/mvn..."? All of our builds are passing on Jenkins so I wonder if it's a maven version issue: https://amplab.cs.berkeley.edu/jenkins/view/Spark-QA-Compile/ - Patrick Thanks - it appears this is just a legitimate issue with the build, affecting all versions of Maven. Let's continue the disucssion on the other thread relating to the master build. Okay I did some forensics with Sean Owen. Some things about this bug: 1. The underlying cause is that we added some code to make the tests of sub modules depend on the core tests. For unknown reasons this causes Spark to hit MSHADE-148 for *some* combinations of build profiles. 2. MSHADE-148 can be worked around by disabling building of "dependency reduced poms" because then the buggy code path is circumvented. Andrew Or did this in a patch on the 1.4 branch. However, that is not a tenable option for us because our *published* pom files require dependency reduction to substitute in the scala version correctly for the poms published to maven central. 3. As a result, Andrew Or reverted his patch recently, causing some package builds to start failing again (but publishing works now). 4. The reason this is not detected in our test harness or release build is that it is sensitive to the profiles enabled. The combination of profiles we enable in the test harness and release builds do not trigger this bug. The best path I see forward right now is to do the following: 1. Disable creation of dependency reduced poms by default (this doesn't matter for people doing a package build) so typical users won't have this bug. 2. Add a profile that re-enables that setting. 3. Use the above profile when publishing release artifacts to maven central. 4. Hope that we don't hit this bug for publishing. - Patrick Patch that added test-jar dependencies: https://github.com/apache/spark/commit/bfe74b34 Patch that originally disabled dependency reduced poms: https://github.com/apache/spark/commit/984ad60147c933f2d5a2040c87ae687c14eb1724 Patch that reverted the disabling of dependency reduced poms: https://github.com/apache/spark/commit/bc51bcaea734fe64a90d007559e76f5ceebfea9e Hi Tomo, For now you can do that as a work around. We are working on a fix for this in the master branch but it may take a couple of days since the issue is fairly complicated. - Patrick Hey All, This vote is cancelled in favor of RC3. - Patrick Please vote on releasing the following candidate as Apache Spark version 1.4.1! This release fixes a handful of known issues in Spark 1.4.0, listed here: http://s.apache.org/spark-1.4.1 The tag to be voted on is v1.4.1-rc3 (commit 3e8ae38): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h= 3e8ae38944f13895daf328555c1ad22cd590b089 The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-releases/spark-1.4.1-rc3-bin/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: [published as version: 1.4.1] https://repository.apache.org/content/repositories/orgapachespark-1123/ [published as version: 1.4.1-rc3] https://repository.apache.org/content/repositories/orgapachespark-1124/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-releases/spark-1.4.1-rc3-docs/ Please vote on releasing this package as Apache Spark 1.4.1! The vote is open until Friday, July 10, at 20:00 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.4.1 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Yeah - we can fix the docs separately from the release. - Patrick Hey All, The issue that Josh pointed out is not just a test failure, it's an issue with an important bug fix that was not correctly back-ported into the 1.4 branch. Unfortunately the overall state of the 1.4 branch tests on Jenkins was not in great shape so this was missed earlier on. Given that this is fixed now, I have prepared another RC and am leaning towards restarting the vote. If anyone feels strongly one way or the other let me know, otherwise I'll restart it in a few hours. I figured since this will likely finalize over the weekend anyways, it's not so bad to wait 1 additional day in order to get that fix. - Patrick This vote is cancelled in favor of RC4. - Patrick Please vote on releasing the following candidate as Apache Spark version 1.4.1! This release fixes a handful of known issues in Spark 1.4.0, listed here: http://s.apache.org/spark-1.4.1 The tag to be voted on is v1.4.1-rc4 (commit dbaa5c2): https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h= dbaa5c294eb565f84d7032e387e4b8c1a56e4cd2 The release files, including signatures, digests, etc. can be found at: http://people.apache.org/~pwendell/spark-releases/spark-1.4.1-rc4-bin/ Release artifacts are signed with the following key: https://people.apache.org/keys/committer/pwendell.asc The staging repository for this release can be found at: [published as version: 1.4.1] https://repository.apache.org/content/repositories/orgapachespark-1125/ [published as version: 1.4.1-rc4] https://repository.apache.org/content/repositories/orgapachespark-1126/ The documentation corresponding to this release can be found at: http://people.apache.org/~pwendell/spark-releases/spark-1.4.1-rc4-docs/ Please vote on releasing this package as Apache Spark 1.4.1! The vote is open until Sunday, July 12, at 06:55 UTC and passes if a majority of at least 3 +1 PMC votes are cast. [ ] +1 Release this package as Apache Spark 1.4.1 [ ] -1 Do not release this package because ... To learn more about Apache Spark, please see http://spark.apache.org/ --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org +1 Hey Sean B., Thanks for bringing this to our attention. I think putting them on the developer wiki would substantially decrease visibility in a way that is not beneficial to the project - this feature was specifically requested by developers from other projects that integrate with Spark. If the concern underlying that policy is that snapshot builds could be misconstrued as formal releases, I think it would work to put a very clear disclaimer explaining the difference directly adjacent to the link. That's arguably more explicit than just moving the same text to a different page. The formal policy asks us not to include links "that encourage non-developers to download" the builds. Stating clearly that the audience for those links is developers, in my interpretation that would satisfy the letter and spirit of this policy. - Patrick Thanks Sean O. I was thinking something like "NOTE: Nightly builds are meant for development and testing purposes. They do not go through Apache's release auditing process and are not official releases."- Patrick I think we can close this vote soon. Any addition votes/testing would be much appreciated! Hey Sean B, Would you mind outlining for me how we go about changing this policy - I think it's outdated and doesn't make much sense. Ideally I'd like to propose a vote to modify the text slightly such that our current behavior is seen as complaint. Specifically: - What concrete steps can I take to change the policy? - Who has the authority to change this document? - You keep mentioning the incubator@ list, why is this the place for such policy to be discussed or decided on? - What is the reasonable amount of time frame in which the policy change is likely to be decided? We've had a few times people from the various parts of the ASF come and say we are in violation of a policy. And sometimes other ASF people come and then get in a fight on our mailing list, and there is back and fourth, and it turns out there isn't so much a widely followed policy as a doc somewhere that is really old and not actually universally followed. It's difficult for us in such situations to now how to proceed and how much autonomy we as a PMC have to make decisions about our own project. - Patrick This vote passes with 14 +1 (7 binding) votes and no 0 or -1 votes. +1 (14): Patrick Wendell Reynold Xin Sean Owen Burak Yavuz Mark Hamstra Michael Armbrust Andrew Or York, Brennon Krishna Sankar Luciano Resende Holden Karau Tom Graves Denny Lee Sean McNamara - Patrick Thanks shane for the updates and helping get this up and running. - Patrick One related note here is that we have a Java version of this that is an abstract class - in the doc it says that it exists more or less to allow for binary compatibility (it says it's for Java users, but really Scala could use this also): https://github.com/apache/spark/blob/master/core/src/main/java/org/apache/spark/JavaSparkListener.java#L23 I think it might be reasonable that the Scala trait provides only source compatibitly and the Java class provides binary compatibility. - Patrick Actually the java one is a concrete class. Hi All, I'm happy to announce the Spark 1.4.1 maintenance release. We recommend all users on the 1.4 branch upgrade to this release, which contain several important bug fixes. Download Spark 1.4.1 - http://spark.apache.org/downloads.html Release notes - http://spark.apache.org/releases/spark-release-1-4-1.html Comprehensive list of fixes - http://s.apache.org/spark-1.4.1 Thanks to the 85 developers who worked on this release! Please contact me directly for errata in the release notes. - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org +1 from me too Sean B., Thank you for giving a thorough reply. I will work with Sean O. and see what we can change to make us more in line with the stated policy. I did some research and it appears that some time between October [1] and December [2] 2006, this page was modified to include stricter policy surrounding nightly builds. Actually, the original version of the policy page encouraged projects to post nightly builds for the benefit of all developers, just as we have been doing. If you detect frustration from the Spark community, it's because this type of situation occurs with some regularity. In this case: (a) A policy exists from ~10 years ago, presumably because some project back then had problematic release management practices and so a "policy" needed to be created to solve a problem. (b) The policy is outdated now, and no one is 100% sure why it was created (likely many of the people are no longer involved in the ASF who helped craft it). (c) The steps for how to change it are unclear and there isn't clear ownership of the policy document. I think it's unavoidable given the decentralized organization structure of the ASF, but I just want to be up front about our perspective and why you might sense some frustration. [1] https://web.archive.org/web/20061020220358/http://www.apache.org/dev/release.html [2] https://web.archive.org/web/20061231050046/http://www.apache.org/dev/release.html - Patrick I think we should just revert this patch on all affected branches. No reason to leave the builds broken until a fix is in place. - Patrick Hey Bharath, There was actually an incompatible change to the build process that broke several of the Jenkins builds. This should be patched up in the next day or two and nightly builds will resume. - Patrick Hi All, A few times I've been asked about backporting and when to backport and not backport fix patches. Since I have managed this for many of the past releases, I wanted to point out the way I have been thinking about it. If we have some consensus I can put it on the wiki. The trade off when backporting is you get to deliver the fix to people running older versions (great!), but you risk introducing new or even worse bugs in maintenance releases (bad!). The decision point is when you have a bug fix and it's not clear whether it is worth backporting. I think the following facets are important to consider: (a) Backports are an extremely valuable service to the community and should be considered for any bug fix. (b) Introducing a new bug in a maintenance release must be avoided at all costs. It over time would erode confidence in our release process. (c) Distributions or advanced users can always backport risky patches on their own, if they see fit. For me, the consequence of these is that we should backport in the following situations: - Both the bug and the fix are well understood and isolated. Code being modified is well tested. - The bug being addressed is high priority to the community. - The backported fix does not vary widely from the master branch fix. We tend to avoid backports in the converse situations: - The bug or fix are not well understood. For instance, it relates to interactions between complex components or third party libraries (e.g. Hadoop libraries). The code is not well tested outside of the immediate bug being fixed. - The bug is not clearly a high priority for the community. - The backported fix is widely different from the master branch fix. These are clearly subjective criteria, but ones worth considering. I am always happy to help advise people on specific patches if they want a soundingboard to understand whether it makes sense to backport. - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I've disabled the test and filed a JIRA: https://issues.apache.org/jira/browse/SPARK-9335 Hi All, If there is a build break (i.e. a compile issue or consistently failing test) that somehow makes it into master, the best protocol is: 1. Revert the offending patch. 2. File a JIRA and assign it to the committer of the offending patch. The JIRA should contain links to broken builds. It's not worth waiting any time to try and figure out how to fix it, or blocking on tracking down the commit author. This is because every hour that we have the PRB broken is a major cost in terms of developer productivity. - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Thanks ted for pointing this out. CC to Ryan and TD Yeah this could make sense - allowing data sources to register a short name. What mechanism did you have in mind? To use the jar service loader? The only issue is that there could be conflicts since many of these are third party packages. If the same name were registered twice I'm not sure what the best behavior would be. Ideally in my mind if the same shortname were registered twice we'd force the user to use a fully qualified name and say the short name is ambiguous. Patrick Hey All, I've mostly kept quiet since I am not very active in maintaining this code anymore. However, it is a bit odd that the project is split-brained with a lot of the code being on github and some in the Spark repo. If the consensus is to migrate everything to github, that seems okay with me. I would vouch for having user continuity, for instance still have a "shim" ec2/spark-ec2 script that could perhaps just download and unpack the real script from github. - Patrick Hey All, I got it up and running - it was a newly surfaced bug in the build scripts. - Patrick Yeah the best bet is to use ./build/mvn --force (otherwise we'll still use your system maven). - Patrick Hey Meihua, If you are a user of Spark, one thing that is really helpful is to run Spark 1.5 on your workload and report any issues, performance regressions, etc. - Patrick Hey All, Was wondering if people would be willing to avoid merging build changes until we have put the tests in better shape. The reason is that build changes are the most likely to cause downstream issues with the test matrix and it's very difficult to reverse engineer which patches caused which problems when the tests are not in a stable state. For instance, the updates to Hive 1.2.1 caused cascading failures that have lasted several days now and in the mean time a few other build related patches were also merged - as these pile up it gets harder for us to have confidence those other patches didn't introduce problems. https://amplab.cs.berkeley.edu/jenkins/view/Spark-QA-Test/ - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org There is already code in place that restricts which tests run depending on which code is modified. However, changes inside of Spark's core currently require running all dependent tests. If you have some ideas about how to improve that heuristic, it would be great. - Patrick Hi All, For pull requests that modify the build, you can now test different build permutations as part of the pull request builder. To trigger these, you add a special phrase to the title of the pull request. Current options are: [test-maven] - run tests using maven and not sbt [test-hadoop1.0] - test using older hadoop versions (can use 1.0, 2.0, 2.2, and 2.3). The relevant source code is here: https://github.com/apache/spark/blob/master/dev/run-tests-jenkins#L193 This is useful because it allows up-front testing of build changes to avoid breaks once a patch has already been merged. I've documented this on the wiki: https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools - Patrick --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I think it would be a big improvement to get rid of it. It's not how jars are supposed to be packaged and it has caused problems in many different context over the years. For me a key step in moving away would be to fully audit/understand all compatibility implications of removing it. If other people are supportive of this plan I can offer to help spend some time thinking about any potential corner cases, etc. - Patrick Hey Richard, My assessment (just looked before I saw Sean's email) is the same as his. The NOTICE file embeds other projects' licenses. If those licenses themselves have pointers to other files or dependencies, we don't embed them. I think this is standard practice. - Patrick Ah - I can update it. Usually i do it after the release is cut. It's just a standard 3 month cadence. BTW - the merge window for 1.6 is September+October. The QA window is November and we'll expect to ship probably early december. We are on a 3 month release cadence, with the caveat that there is some pipelining... as we finish release X we are already starting on release X+1. - Patrick Hey Holden, It would be helpful if you could outline the set of features you'd imagine being part of Spark in a short doc. I didn't see a README on the existing repo, so it's hard to know exactly what is being proposed. As a general point of process, we've typically avoided merging modules into Spark that can exist outside of the project. A testing utility package that is based on Spark's public API's seems like a really useful thing for the community, but it does seem like a good fit for a package library. At least, this is my first question after taking a look at the project. In any case, getting some high level view of the functionality you imagine would be helpful to give more detailed feedback. - Patrick I would push back slightly. The reason we have the PR builds taking so long is death by a million small things that we add. Doing a full 2.11 compile is order minutes... it's a nontrivial increase to the build times. It doesn't seem that bad to me to go back post-hoc once in a while and fix 2.11 bugs when they come up. It's on the order of once or twice per release and the typesafe guys keep a close eye on it (thanks!). Compare that to literally thousands of PR runs and a few minutes every time, IMO it's not worth it. I think Daniel is correct here. The source artifact incorrectly includes jars. It is inadvertent and not part of our intended release process. This was something I noticed in Spark 1.5.0 and filed a JIRA and was fixed by updating our build scripts to fix it. However, our build environment was not using the most current version of the build scripts. See related links: https://issues.apache.org/jira/browse/SPARK-10511 https://github.com/apache/spark/pull/8774/files I can update our build environment and we can repackage the Spark 1.5.1 source tarball. To not include sources. - Patrick *to not include binaries. Oh I see - yes it's the build/. I always thought release votes related to a source tag rather than specific binaries. But maybe we can just fix it in 1.5.2 if there is concern about mutating binaries. It seems reasonable to me. For tests... in the past we've tried to avoid having jars inside of the source tree, including some effort to generate jars on the fly which a lot of our tests use. I am not sure whether it's a firm policy that you can't have jars in test folders, though. If it is, we could probably do some magic to get rid of these few ones that have crept in. - Patrick Yeah I mean I definitely think we're not violating the *spirit* of the "no binaries" policy, in that we do not include any binary code that is used at runtime. This is because the binaries we distribute relate only to build and testing. Whether we are violating the *letter* of the policy, I'm not so sure. In the very strictest interpretation of "there cannot be any binary files in your downloaded tarball" - we aren't honoring that. We got a lot of people complaining about the sbt jar for instance when we were in the incubator. I found those complaints a little pedantic, but we ended up removing it from our source tree and adding things to download it for the user. - Patrick I would tend to agree with this approach. We should audit all @Experimenetal labels before the 1.6 release and clear them out when appropriate. - Patrick Hi Jakob, There is a temporary issue with the Scala 2.11 build in SBT. The problem is this wasn't previously covered by our automated tests so it broke without us knowing - this has been actively discussed on the dev list in the last 24 hours. I am trying to get it working in our test harness today. In terms of fixing the underlying issues, I am not sure whether there is a JIRA for it yet, but we should make one if not. Does anyone know? - Patrick Jakob this is now being tested by our harness. I've created a JIRA for the issue, if you want to take a stab at fixing these, that would be great: https://issues.apache.org/jira/browse/SPARK-11110 - Patrick Hey Shane, It also appears that every Spark build is failing right now. Could it be related to your changes? - Patrick This is what I'm looking at: https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/ I think many of them are coming form the Spark 1.4 builds: https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Test%20(Dashboard)/job/Spark-1.4-Maven-pre-YARN/3900/console I verified that the issue with build binaries being present in the source release is fixed. Haven't done enough vetting for a full vote, but did verify that. I believe this is some bug in our tests. For some reason we are using way more memory than necessary. We'll probably need to log into Jenkins and heap dump some running tests and figure out what is going on. Hey Jakob, The builds in Spark are largely maintained by me, Sean, and Michael Armbrust (for SBT). For historical reasons, Spark supports both a Maven and SBT build. Maven is the build of reference for packaging Spark and is used by many downstream packagers and to build all Spark releases. SBT is more often used by developers. Both builds inherit from the same pom files (and rely on the same profiles) to minimize maintenance complexity of Spark's very complex dependency graph. If you are looking to make contributions that help with the build, I am happy to point you towards some things that are consistent maintenance headaches. There are two major pain points right now that I'd be thrilled to see fixes for: 1. SBT relies on a different dependency conflict resolution strategy than maven - causing all kinds of headaches for us. I have heard that newer versions of SBT can (maybe?) use Maven as a dependency resolver instead of Ivy. This would make our life so much better if it were possible, either by virtue of upgrading SBT or somehow doing this ourselves. 2. We don't have a great way of auditing the net effect of dependency changes when people make them in the build. I am working on a fairly clunky patch to do this here: https://github.com/apache/spark/pull/8531 It could be done much more nicely using SBT, but only provided (1) is solved. Doing a major overhaul of the sbt build to decouple it from pom files, I'm not sure that's the best place to start, given that we need to continue to support maven - the coupling is intentional. But getting involved in the build in general would be completely welcome. - Patrick I think we'd have to standardize on Maven-style resolution, or I'd at least like to see that path explored first. The issue is if we switch the standard now, it could cause compatibility breaks for applications on top of Spark. I think there are a few minor differences in the dependency graph that arise from this. For a given user, the probability it affects them is low - it needs to conflict with a library a user application is using. However the probability it affects *some users* is very high and we do see small changes crop up fairly frequently. My feeling is mostly pragmatic... if we can get things working to standardize on Maven-style resolution by upgrading SBT, let's do it. If that's not tenable, we can evaluate alternatives. - Patrick In terms of advertising to people the status of the release and whether an RC is likely to go out, the best mechanism I can think of is our current mechanism of using JIRA and respecting the semantics of a blocker JIRA. We could do a better job though creating a JIRA dashboard for each release and linking to it publicly so it's very clear to people what is going on. I have always used one privately when managing previous releases, but no reason we can't put one up on the website or wiki. IMO a mailing list is not a great mechanism for the fine-grained work of release management because of the sheer complexity and volume of finalizing a spark release. Being a release manager means tracking over a course of several weeks typically dozens of distinct issues and trying to prioritize them, get more clarity from the report of those issues, possibly reaching out to people on the phone or in person to get more details, etc. You want a mutable dashboard where you can convey the current status clearly. What might be good in the early stages is a weekly e-mail to the dev@ list just refreshing what is on the JIRA and letting people know how things are looking. So someone just passing by has some idea of how things are going and can chime in, etc. Once an RC is cut then we do mostly rely on the mailing list for discussion. At that point the number of known issues is small enough I think to discuss in an all-to-all fashion. - Patrick