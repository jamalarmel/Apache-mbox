Hi, I am not noticing any DSGD implementation of ALS in Spark. There are two ALS implementations. org.apache.spark.examples.SparkALS does not run on large matrices and seems more like a demo code. org.apache.spark.mllib.recommendation.ALS looks feels more robust version and I am experimenting with it. References here are Jellyfish, Twitter's implementation of Jellyfish called Scalafish, Google paper called Sparkler and similar idea put forward by IBM paper by Gemulla et al. (large-scale matrix factorization with distributed stochastic gradient descent) https://github.com/azymnis/scalafish Are there any plans of adding DSGD in Spark or there are any existing JIRA ? Thanks. Deb Hi Ameet, Matrix factorization is a non-convex problem and ALS solves it using 2 convex problems, DSGD solves the problem by finding a local minima. I am experimenting with Spark Parallel ALS but I intend to port Scalafish https://github.com/azymnis/scalafish to Spark as well. For bigger matrices jury is not out that which algorithms provides a better local optima with an iteration bound. It is also highly dependent on datasets I believe. Thanks. Deb Hi DB, Xiangrui, Mallet from cmu also has bfgs cg and a good optimization package.  Do you know if cpl license si Continuation on last email sent by mistake: Is cpl license is compatible with apache ? http://opensource.org/licenses/cpl1.0.php Mallet jars are available on maven. They have hessian based solvers which looked interesting along with bfgs and cg. Definitely the lbfgs f2j looks promising as the base fortran code is perhaps the best bfgs code I have used. Also the bounded bfgs is useful for many cases to enforce constraints on say pca.... Note that right now the version is not blas optimized. With jblas or netlib-java discussions that's going on it can be improved. Also it runs on a single thread which can be improved...so there is scope for further improvements in the code. Basically Xiangrui, is there a push back on making optimizers part of spark mllib ? I am exploring cg and qp solvers for spark mllib as well and I am developing these as part of mllib optimization. I was hoping we should be able to publish mllib as a maven artifact later. Thanks. Deb Hi DB, I am considering building on your PR and add Mallet as the dependency so that we can run some basic comparisons test on large scale sparse datasets that I have. In the meantime, let's discuss if there are other optimization packages that we should try. My wishlist has bounded bfgs as well and I will add it to the PR. About the PR getting merged to mllib, we can plan that later. Thanks. Deb Hi DB, I am considering building on your PR and add Mallet as the dependency so that we can run some basic comparisons test on large scale sparse datasets that I have. In the meantime, let's discuss if there are other optimization packages that we should try. My wishlist has bounded bfgs as well and I will add it to the PR. About the PR getting merged to mllib, we can plan that later. Thanks. Deb Hi DB, Could you please point me to your spark PR ? Thanks. Deb Hi, We have a mirror repo of spark at our internal stash. We are adding changes to a fork of the mirror so that down the line we can push the contributions back to Spark git. I am not sure what's the exact the development methodology we should follow as things are a bit complicated due to enterprise stash. Any suggestions would be great. I have follow usecases: 1. Ask for a pull request and push the changes from stash mirror upstream to the spark apache git. 2. I am working on 2 PR that are currently open. What's the best way to work on those ? Can I get those PR through the stash mirror of Spark git or those PRs are only available on github ? Thanks. Deb Stash is an enterprise git from atlassian.. I got it...Basically the PRs are managed by github and if I have to work on a PR, I should rather make use of my github account... Thanks for the clarification. Hi DB, 1. Could you point to the BFGS repositories used to publish artifacts to maven central ? What's the best way to add changes to it ? I fork the repo at my github ? Basically as I mentioned before I need to add lbfgs-b, orthant wise for L1 handling and few variants of line search to lbfgs... 2. For the spark pull request, what's the best way to get your branch dbtsai-lbfgs ? On my github spark mirror I can make a branch called debasish-lbfgs and merge your code to it ? I need sparse support to the logistic regression classifier using lbfgs solve and therefore I will need Xiangrui's branch as well... https://github.com/apache/incubator-spark/pull/575 Thanks. Deb Hi Reynold, I checked and atlassian stash also has pull request feature... In the spark README it says: "Contributions via GitHub pull requests are gladly accepted from their original author."What happens if a spark pull requests comes from stash ? will you guys accept it or all pull requests has to come through github ? I could merge for example @dbtsai github lbfgs branch to my branch at stash... Thanks. Deb Hi DB, I have forked the LBFGS repo. The features that I want in it are the following: a.  OWL-QN for solving L1 natively in BFGS b.  Bound constraints in BFGS : I saw you have converted the fortran code. Is there a license issue ? I can help in getting that up to speed as well. c. Few variants of line searches : I will discuss on it. For the dbtsai-lbfgs branch seems like it already got merged by Jenkins. Is this getting merged to the master or there will be revisions on it ? https://github.com/apache/spark/pull/53 Thanks. Deb Yeah we should move f2j L-BFGS and L-BFGS-B to breeze..they already have 2 line searches..also the OWL-QN outline... Hi Xiangrui, What's the plan on the PR ? https://github.com/apache/incubator-spark/pull/575 Will you add breeze as a dependency for the sparse support ? I looked at your branch https://github.com/mengxr/incubator-spark/tree/sparse and the code is using mahout wrapper. I can add a branch which updates GLM with breeze sparse matrices in case you are fine with breeze license and other issues that we discussed on the PR. Thanks. Deb Hi David, Few questions on breeze solvers: 1. I feel the right place of adding useful things from RISO LBFGS (based on Professor Nocedal's fortran code) will be breeze. It will involve stress testing breeze LBFGS on large sparse datasets and contributing fixes to existing breeze LBFGS with the learning from RISO LBFGS. You agree on that right ? 2. Normally for doing experiments like 1, I fix the line search. What's your preferred line search in breeze BFGS ? I will also use that. More Thuente and Strong wolfe with backtracking helped me in the past. 3. The paper that you sent also says on L-BFGS-B is better on box constraints. I feel It's worthwhile to have both the solvers because many practical problems need box constraints or complex constraints can be reformulated in the realm of unconstrained and box constraints. Example use-cases for me are automatic feature extraction from photo/video frames using factorization. I will compare L-BFGS-B vs constrained QN method that you have in Breeze within an analysis similar to 1. 4. Do you have a road-map on adding CG solvers in breeze ? Linear CG solver to compare with BLAS posv seems like a good usecase for me in mllib ALS. DB sent a paper by Professor Ng which shows the effectiveness of CG and BFGS over SGD in the email chain. I believe on non-convex problems like Matrix Factorization, CG family might converge to a better solution than BFGS. No way to know till we experiment on the datasets :-) Thanks. Deb Hi David, Few questions on breeze solvers: 1. I feel the right place of adding useful things from RISO LBFGS (based on Professor Nocedal's fortran code) will be breeze. It will involve stress testing breeze LBFGS on large sparse datasets and contributing fixes to existing breeze LBFGS with the learning from RISO LBFGS. You agree on that right ? 2. Normally for doing experiments like 1, I fix the line search. What's your preferred line search in breeze BFGS ? I will also use that. More Thuente and Strong wolfe with backtracking helped me in the past. 3. The paper that you sent also says on L-BFGS-B is better on box constraints. I feel It's worthwhile to have both the solvers because many practical problems need box constraints or complex constraints can be reformulated in the realm of unconstrained and box constraints. Example use-cases for me are automatic feature extraction from photo/video frames using factorization. I will compare L-BFGS-B vs constrained QN method that you have in Breeze within an analysis similar to 1. 4. Do you have a road-map on adding CG solvers in breeze ? Linear CG solver to compare with BLAS posv seems like a good usecase for me in mllib ALS. DB sent a paper by Professor Ng which shows the effectiveness of CG and BFGS over SGD in the email chain. I believe on non-convex problems like Matrix Factorization, CG family might converge to a better solution than BFGS. No way to know till we experiment on the datasets :-) Thanks. Deb David, There used to be standard BFGS testcases in Professor Nocedal's package...did you stress test the solver with them ? If not I will shoot him an email for them. Thanks. Deb David, There used to be standard BFGS testcases in Professor Nocedal's package...did you stress test the solver with them ? If not I will shoot him an email for them. Thanks. Deb Hi, I am running ALS on a sparse problem (10M x 1M) and I am getting the following error: org.jblas.exceptions.LapackArgumentException: LAPACK DPOSV: Leading minor of order i of A is not positive definite. at org.jblas.SimpleBlas.posv(SimpleBlas.java:373) at org.jblas.Solve.solvePositive(Solve.java:68) This error from blas shows up if the hessian matrix is not positive definite... I checked that rating matrix is all > 0 Is there some sort of specific initialization of factor matrices done which can make the hessian matrix non positive definite ? I am printing out the eigen vectors and fullXtX matrix to understand it more but any help will be appreciated. Thanks. Deb Hi, I am running ALS on a sparse problem (10M x 1M) and I am getting the following error: org.jblas.exceptions.LapackArgumentException: LAPACK DPOSV: Leading minor of order i of A is not positive definite. at org.jblas.SimpleBlas.posv(SimpleBlas.java:373) at org.jblas.Solve.solvePositive(Solve.java:68) This error from blas shows up if the hessian matrix is not positive definite... I checked that rating matrix is all > 0 but of course like netflix they are not bounded within 1 and 5.... Is there some sort of specific initialization of factor matrices done which can make the hessian matrix non positive definite ? I am printing out the eigen vectors and fullXtX matrix to understand it more but any help will be appreciated. Thanks. Deb Hi Sebastian, Yes Mahout ALS and Oryx runs fine on the same matrix because Sean calls QR decomposition. But the ALS objective should give us strictly positive definite matrix..I am thinking more on it.. There are some random factor assignment step but that also initializes factors with normal(0,1)...which I think is not a big deal... About QR decomposition, jblas has Solve.solve and Solve.solvePositive...solve should also run fine but it does a LU factorization and better way will be to do a QR decomposition. Seems breeze has QR decomposition and we can make use of that...But QR by default is also not correct since if the matrix is positive definite BLAS psov (Solve.solvePositive) is much faster due to cholesky computation.. I believe we need a singular value check and based on that we should call solvePositive/solve or qr from breeze.. There is also a specialized version of TSQR (tall and skinny QR decomposition) from Chris which might be good to evaluate as well: https://github.com/ccsevers/scalding-linalg I am going to debug it further and try to understand why I am getting non positive definite matrix and publish the findings... Any suggestions on how to proceed further on this ? Should I ask for a PR and we can discuss more on it ? Thanks. Deb Yes that will be really cool if the data has linearly independent rows ! I have to debug it more but I got it running with jblas Solve.solve.. I will try breeze QR decomposition next. Have you guys tried adding bound constraints in QR decomposition / BLAS posv other than projecting to positive space at each iteration ? Also for the experiments what's the usual procudure for documenting them for public feedback ? On the github PR ? Thanks Deb Bound constraints in QR decomposition / BLAS posv other than projecting to positive space at each iteration ? Common usecases are feature generation from photos/videos etc... I saw a paper on projecting to positive space from 70s...there are some improvements later using projected gradients but those are for first order solves... Matei, If the data has linearly dependent rows ALS should have a failback mechanism. Either remove the rows and then call BLAS posv or call BLAS gesv or Breeze QR decomposition. I can share the analysis over email. Thanks. Deb Hi Xiangrui, I used lambda = 0.1...It is possible that 2 users ranked in movies in a very similar way... I agree that increasing lambda will solve the problem but you agree this is not a solution...lambda should be tuned based on sparsity / other criteria and not to make a linearly dependent hessian matrix linearly independent... Thanks. Deb Nope..I did not test implicit feedback yet...will get into more detailed debug and generate the testcase hopefully next week... Hi, I gave my spark job 16 gb of memory and it is running on 8 executors. The job needs more memory due to ALS requirements (20M x 1M matrix) On each node I do have 96 gb of memory and I am using 16 gb out of it. I want to increase the memory but I am not sure what is the right way to do that... On 8 executor if I give 96 gb it might be a issue due to GC... Ideally on 8 nodes, I would run with 48 executors and each executor will get 16 gb of memory..Total  48 JVMs... Is it possible to increase executors per node ? Thanks. Deb Nope...with the cleaner dataset I am not noticing issues with the dposv and this dataset is even bigger...20 M users and 1 M products...I don't think other than cholesky anything else will get us the efficiency we need... For my usecase we also need to see the effectiveness of positive factors and I am doing variable projection as a start.. If possible could you please point me to the PRs related to ALS improvements ? Are they all added to the master ? There are at least 3 PRs that Sean and you contributed recently related to ALS efficiency. A JIRA or gist will definitely help a lot. Thanks. Deb Awesome news ! It will be great if there are any examples or usecases to look at ? We are looking into shark/ooyala job server to give in memory sql analytics, model serving/scoring features for dashboard apps... Does this feature has different usecases than shark or more cleaner as hive dependency is gone? Thanks. Deb On Mar 21, 2014 10:13 AM, "Matei Zaharia"  wrote: Hi, For our usecases we are looking into 20 x 1M matrices which comes in the similar ranges as outlined by the paper over here: http://sandeeptata.blogspot.com/2012/12/sparkler-large-scale-matrix.html Is the exponential runtime growth in spark ALS as outlined by the blog still exists in recommendation.ALS ? I am running a spark cluster of 10 nodes with total memory of around 1 TB with 80 cores.... With rank = 50, the memory requirements for ALS should be 20Mx50 doubles on every worker which is around 8 GB.... Even if both the factor matrices are cached in memory I should be bounded by ~ 9 GB but even with 32 GB per worker I see GC errors... I am debugging the scalability and memory requirements of the algorithm further but any insights will be very helpful... Also there are two other issues: 1. If GC errors are hit, that worker JVM goes down and I have to restart it manually. Is this expected ? 2. When I try to make use of all 80 cores on the cluster I get some issues related to java.io.File not found exception on /tmp/ ? Is there some OS limit that how many cores can simultaneously access /tmp from a process ? Thanks. Deb Thanks Sean. Looking into executor memory options now... I am at incubator_spark head. Does that has all the fixes or I need spark head ? I can deploy the spark head as well... I am not running implicit feedback yet...I remember memory enhancements were mainly for implicit right ? For ulimit let me look into centos settings....I am curious how map-reduce resolves it....by using 1 core from 1 process ? I am running 2 tb yarn jobs as well for etl, pre processing etc....have not seen the too many files opened yet.... when there is gc error, the worker dies....that's mystery as well...any insights from spark core team ? Yarn container gets killed if gc boundaries are about to hit....similar ideas can be used here as well ? Also which tool do we use for memory debugging in spark ? On Mar 26, 2014 1:45 AM, "Sean Owen"  wrote: Just a clarification, I am using Spark ALS explicit feedback on standalone cluster without deploying zookeeper master HA option yet... When in the standalone spark cluster, worker fails due to GC error, the worker dies as well and I have to restart the worker....Understanding this issue will be useful as we deploy the solution... Hi Matei, I am hitting similar problems with 10 ALS iterations...I am running with 24 gb executor memory on 10 nodes for 20M x 3 M matrix with rank =50 The first iteration of flatMaps run fine which means that the memory requirements are good per iteration... If I do check-pointing on RDD, most likely rest 9 iterations will also run fine and I will get the results... Is there a plan to add checkpoint option to ALS for such large factorization jobs ? Thanks. Deb Hi David, I have started to experiment with BFGS solvers for Spark GLM over large scale data... I am also looking to add a good QP solver in breeze that can be used in Spark ALS for constraint solves...More details on that soon... I could not load up breeze 0.7 code onto eclipse...There is a folder called natives in the master but there is no code in that....all the code is in src/main/scala... I added the eclipse plugin: addSbtPlugin("com.github.mpeltonen" % "sbt-idea" % "1.6.0") addSbtPlugin("com.typesafe.sbteclipse" % "sbteclipse-plugin" % "2.2.0") But it seems the project is set to use idea... Could you please explain the dev methodology for breeze ? My idea is to do solver work in breeze as that's the right place and get it into Spark through Xiangrui's WIP on Sparse data and breeze support... Thanks. Deb I added eclipse support in my qp branch: https://github.com/debasish83/breeze/tree/qp For the QP solver I will look into this solver http://www.joptimizer.com/ Right now my plan is to use Professor Boyd's ECOS solver which is also designed in the very similar lines but has been tested to solve even cone programs... https://github.com/ifa-ethz/ecos Any idea whether I should add C native code using jniloader as the first version or rewrite using breeze.optimize style and call netlib-java calls for native support (ldl, cholesky etc)... I still have to think how much cone support we will need...In ALS for example X^TX = I and Y^Y=I are interesting constraints for orthogonality...and they are quadratic constraints...With BFGS and CG, it is difficult to handle quadratic constraints... Thanks for the pointers. That's the plan...I am going to have default ALS, variable projection at each ALS step, and then using the constrained first order solver that you have... I am hoping ECOS QP will beat all of them...I don't know...some of these obvious assumptions does not work when the data is large... About ECOS, we are trying to get it Apache licensed from Professor Boyd / Alex Domahidi who is the primary author. Let's see how that goes... I am not sure how easy it will be for projected gradient solver to handle quadratic constraints...For simple bounds I feel it will work fine...On Mar 31, 2014 10:34 AM, "David Hall"  wrote: Hi, Also posted it on user but then I realized it might be more involved. In my ALS runs I am noticing messages that complain about heart beats: 14/04/04 20:43:09 WARN BlockManagerMasterActor: Removing BlockManager BlockManagerId(17, machine1, 53419, 0) with no recent heart beats: 48476ms exceeds 45000ms 14/04/04 20:43:09 WARN BlockManagerMasterActor: Removing BlockManager BlockManagerId(12, machine2, 60714, 0) with no recent heart beats: 45328ms exceeds 45000ms 14/04/04 20:43:09 WARN BlockManagerMasterActor: Removing BlockManager BlockManagerId(19, machine3, 39496, 0) with no recent heart beats: 53259ms exceeds 45000ms Is this some issue with the underlying jvm over which akka is run ? Can I increase the heartbeat somehow to get these messages resolved ? Any more insight about the possible cause for the heartbeat will be helpful... Thanks. Deb Thanks Patrick...I searched in the archives and found the answer...tuning the akka and gc params.... I am synced with apache/spark master but getting error in spark/sql compilation... Is the master broken ? [info] Compiling 34 Scala sources to /home/debasish/spark_deploy/sql/core/target/scala-2.10/classes... [error] /home/debasish/spark_deploy/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.scala:106: value getGlobal is not a member of object java.util.logging.Logger [error]       logger.setParent(Logger.getGlobal) [error]                               ^ [error] one error found [error] (sql/compile:compile) Compilation failed [error] Total time: 171 s, completed Apr 5, 2014 4:58:41 PM Thanks. Deb I can compile with Java 7...let me try that... I verified this is happening for both CDH4.5 and 1.0.4...My deploy environment is Java 6...so Java 7 compilation is not going to help... Is this the PR which caused it ? Andre Schumacher fbebaed    Spark parquet improvements A few improvements to the Parquet support for SQL queries: - Instead of files a ParquetRelation is now backed by a directory, which simplifies importing data from other sources - InsertIntoParquetTable operation now supports switching between overwriting or appending (at least in HiveQL) - tests now use the new API - Parquet logging can be set to WARNING level (Default) - Default compression for Parquet files (GZIP, as in parquet-mr) Author: Andre Schumacher &...    2 days ago    SPARK-1383 I will go to a stable checkin before this @patrick our cluster still has java6 deployed...and I compiled using jdk6... Sean is looking into it...this api is in java7 but not java6... With jdk7 I could compile it fine: java version "1.7.0_51"Java(TM) SE Runtime Environment (build 1.7.0_51-b13) Java HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode) What happens if I say take the jar and try to deploy it on ancient centos6 default on cluster ? java -version java version "1.6.0_31"Java(TM) SE Runtime Environment (build 1.6.0_31-b04) Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode) Breeze compilation also fails with jdk6, runs fine with jdk7 and breeze jar is already included in spark mllib with Xiangrui's Sparse vector checkin.... Does that mean that classes compiled and generated using jdk7 will run fine on jre6 ? I am confused Hi, I deployed apache/spark master today and recently there were many ALS related checkins and enhancements.. I am running ALS with explicit feedback and I remember most enhancements were related to implicit feedback... With 25 factors my runs were successful but with 50 factors I am getting array index out of bound... Note that I was hitting gc errors before with an older version of spark but it seems like the sparse matrix partitioning scheme has changed now...data caching looks much balanced now...earlier one node was becoming bottleneck...Although I ran with 64g memory per node... There are around 3M products, 25M users... Anyone noticed this bug or something similar ? 14/04/05 23:03:15 WARN TaskSetManager: Loss was due to java.lang.ArrayIndexOutOfBoundsException java.lang.ArrayIndexOutOfBoundsException: 81029 at org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateBlock$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(ALS.scala:450) at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141) at org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateBlock$1.apply$mcVI$sp(ALS.scala:446) at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141) at org.apache.spark.mllib.recommendation.ALS.org $apache$spark$mllib$recommendation$ALS$$updateBlock(ALS.scala:445) at org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:416) at org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:415) at org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31) at org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31) at scala.collection.Iterator$$anon$11.next(Iterator.scala:328) at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:149) at org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:147) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:147) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229) at org.apache.spark.rdd.RDD.iterator(RDD.scala:220) at org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229) at org.apache.spark.rdd.RDD.iterator(RDD.scala:220) at org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229) at org.apache.spark.rdd.RDD.iterator(RDD.scala:220) at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229) at org.apache.spark.rdd.RDD.iterator(RDD.scala:220) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102) at org.apache.spark.scheduler.Task.run(Task.scala:52) at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211) at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:43) at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:396) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408) at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:42) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:662) Thanks. Deb Hi Koert, How do I specify that in sbt ? Is this the correct way ? javacOptions ++= Seq("-target", "1.6", "-source","1.6") Breeze project for examples compiles fine with jdk7, fails with jdk6 and the function it fails on: error] /home/debasish/github/breeze/ src/main/scala/breeze/util/package.scala:200: value valueOf is not a member of object java.util.BitSet [error]       java.util.BitSet.valueOf(bs.toBitMask) is not available in jdk6... http://docs.oracle.com/javase/6/docs/api/java/util/BitSet.html I have no clue how with target 1.6 solves the issue...are you saying jdk7 will put a function that's closest to java.util.BitSet.valueOf ? Thanks. Deb Yeah spark builds are fine... For solvers we are planning to use breeze optimization since it has most of the core functions we will need and we can enhance it further (QP solver for example) Right now sparse kmeans in spark mllib uses breeze and that might not even need this line of code....But still I thought Xiangrui should be aware of this issue... At the head I see persist option in implicitPrefs but more cases like the ones mentioned above why don't we use similar technique and take an input that which iteration should we persist in explicit runs as well ? for (iter <- 1 to iterations) {// perform ALS update logInfo("Re-computing I given U (Iteration %d/%d)".format(iter, iterations)) products = updateFeatures(users, userOutLinks, productInLinks, partitioner, rank, lambda, alpha, YtY = None) logInfo("Re-computing U given I (Iteration %d/%d)".format(iter, iterations)) users = updateFeatures(products, productOutLinks, userInLinks, partitioner, rank, lambda, alpha, YtY = None) } Say if I want to persist at every k iterations out of N iterations of ALS explicit, there shoud be an option to do that...implicit right now uses persist at each iteration... Does this option make sense or you guys want this issue to be fixed in a different way... I definitely see that for my 25M x 3M run, with 64 gb executor memory, something is going wrong after 5-th iteration and I wanted to run for 10 iterations... So my k is 4/5 for this particular problem... I can ask for the PR after testing the fix on the dataset I have...I will also try to see if we can make such datasets public for more research... For the LDA problem mentioned earlier in this email chain, k is 10...NMF can generate topics similar to LDA as well...Carrot2 project uses it... Hi Xiangrui, With 4 ALS iterations it runs fine...If I run 10 I am failing...I believe I have to cut the lineage chain and call checkpoint....Trying to follow the other email chain on checkpointing... Thanks. Deb Sorry not persist...I meant adding a user parameter k which does checkpoint after every k iterations...out of N ALS iterations...We have hdfs installed so not a big deal...is there an issue of adding this user parameter in ALS.scala ? If it is then I can add it to our internal branch... For me tipping k seems like 4...With 4 iterations I can write out the factors...if I run with 10 iterations, after 4 I can see that it restarts the sparse matrix partition...tries to run all the iterations over again and fails due to array index out of bound which does not seems like a real bug... Not sure if it can be reproduced in movielens as the dataset I have is 25M x 3M (and counting)...whille movielens is tall and thin.... Another idea would be to give an option to restart ALS with previous factors...that way ALS core algorithm does not need to change and it might be more useful...and that way we can point to a location from where the old factors can be load...I think @sean used similar idea in Oryx generations... Let me know which way you guys prefer....I can add it in... Sorry not persist...I meant adding a user parameter k which does checkpoint after every k iterations...out of N ALS iterations...We have hdfs installed so not a big deal...is there an issue of adding this user parameter in ALS.scala ? If it is then I can add it to our internal branch... For me tipping k seems like 4...With 4 iterations I can write out the factors...if I run with 10 iterations, after 4 I can see that it restarts the sparse matrix partition...tries to run all the iterations over again and fails due to array index out of bound which does not seems like a real bug... Not sure if it can be reproduced in movielens as the dataset I have is 25M x 3M (and counting)...whille movielens is tall and thin.... Another idea would be to give an option to restart ALS with previous factors...that way ALS core algorithm does not need to change and it might be more useful...and that way we can point to a location from where the old factors can be load...I think @sean used similar idea in Oryx generations... Let me know which way you guys prefer....I can add it in... Nick, I already have this code which calls dictionary generation and then maps string etc to ints...I think the core algorithm should stay in ints...if you like I can add this code in MFUtils.scala....that's the convention I followed similar to MLUtils.scala...actually these functions should be even made part of MLUtils.scala... Only thing is that the join should be an option which makes it application dependent...sometimes people would like to do map side joins if their dictionaries are small...in my case user dictionary has 25M rows and product dictionary has 3M rows...so join optimization did not help... Thanks. Deb I am using master... No negative indexes... If I run with 4 iterations it runs fine and I can generate factors... With 10 iterations run fails with array index out of bound... 25m users and 3m products are within int limits.... Does it help if I can point the logs for both the runs to you ? I will debug it further today... On Apr 7, 2014 9:54 AM, "Xiangrui Meng"  wrote: I got your checkin....I need to run logistic regression SGD vs BFGS for my current usecases but your next checkin will update the logistic regression with LBFGS right ? Are you adding it to regression package as well ? Thanks. Deb By the way...what's the idea...the labeled data set is a RDD which is cached on all nodes.. The bfgs solver is maintained on the master or each worker is supposed to maintain it's own bfgs... Hi DB, Are we going to clean up the function: class LogisticRegressionWithSGD private (var stepSize: Double, var numIterations: Int, var regParam: Double, var miniBatchFraction: Double) extends GeneralizedLinearAlgorithm[LogisticRegressionModel] with Serializable {val gradient = new LogisticGradient() val updater = new SimpleUpdater() override val optimizer = new GradientDescent(gradient, updater) Or add a new one ? class LogisticRegressionWithBFGS ? The WithABC is optional since optimizer could be picked up either based on a flag...there are only 3 options for optimizor: 1. GradientDescent 2. Quasi Newton 3. Newton May be we add an enum for optimization type....and then under GradientDescent family people can add their variants of SGD....Not sure if ConjugateGradient comes under 1 or 2....may be we need 4 options... Thanks. Deb By the way these changes are needed in mllib.regression as well.... Right now my usecases need BFGS support in logistic regression and MLOR so we can focus on cleaning up the classification package first ? Yup that's what I expected...L-BFGS solver is in the master and gradient computation per RDD is done on each of the workers... This miniBatchFraction is also a heuristic which I don't think makes sense for LogisticRegressionWithBFGS...does it ? Have you experimented with it ? For logistic regression at least given enough iterations/tolerance that you are giving, BFGS in both ways should converge to same solution.... Hi, I saw in the code that spark jars are published on sonatype but I was wondering if you guys have published spark jars to artifactory as...Cloudera uses artifactory... Somehow I can publish maven projects to artifactory but after following the sbt link: http://www.scala-sbt.org/release/docs/Detailed-Topics/Publishing.html And trying various things, still I have not been able to publish the jars on artifactory... Any github example that publishes sbt project to artifactory would be really helpful... Thanks. Deb Hi, Why mllib vector is using double as default ? /** * Represents a numeric vector, whose index type is Int and value type is Double. */ trait Vector extends Serializable {/** * Size of the vector. */ def size: Int /** * Converts the instance to a double array. */ def toArray: Array[Double] Don't we need a template on float/double ? This will give us memory savings... Thanks. Deb Is this a breeze issue or breeze can take templates on float / double ? If breeze can take templates then it is a minor fix for Vectors.scala right ? Thanks. Deb Is any one facing issues due to this ? If not then I guess doubles are fine... For me it's not a big deal as there is enough memory available... Hi, I need to change the toString on LabeledPoint to libsvm format so that I can dump RDD[LabeledPoint] as a format that could be read by sparse glmnet-R and other packages to benchmark mllib classification accuracy... Basically I have to change the toString of LabeledPoint and toString of SparseVector.... Should I add it as a PR or is it already being added ? I added these functions toLibSvm in my internal util class for now... def toLibSvm(labelPoint: LabeledPoint): String = {labelPoint.label.toString + " " + toLibSvm(labelPoint.features .asInstanceOf[SparseVector]) } def toLibSvm(features: SparseVector): String = {val indices = features.indices val values = features.values indices.zip(values).mkString(" ").replace(',', ':').replace("(", "").replace(")","") } Thanks. Deb Hi, I need to change the toString on LabeledPoint to libsvm format so that I can dump RDD[LabeledPoint] as a format that could be read by sparse glmnet-R and other packages to benchmark mllib classification accuracy... Basically I have to change the toString of LabeledPoint and toString of SparseVector.... Should I add it as a PR or is it already being added ? I added these functions toLibSvm in my internal util class for now... def toLibSvm(labelPoint: LabeledPoint): String = {labelPoint.label.toString + " " + toLibSvm(labelPoint.features.asInstanceOf[SparseVector]) } def toLibSvm(features: SparseVector): String = {val indices = features.indices val values = features.values indices.zip(values).mkString("").replace(',', ':').replace("(", "").replace(")","") } Thanks. Deb Hi, I see ALS is still using Array[Int] but for other mllib algorithm we moved to Vector[Double] so that it can support either dense and sparse formats... ALS can stay in Array[Int] due to the Netflix format for input datasets which is well defined but it helps if we move ALS to Vector[Double] as well...that way all algorithms will be consistent... The second issue is that toString on SparseVector does not write libsvm format but something not very generic...can we change the SparseVector.toString to write as libsvm output ? I am dumping a sample of dataset to see how mllib glm compares with the glmnet-R package for QoR... Thanks. Deb Hi, Is there a PR for multinomial logistic regression which does one-vs-all and compare it to the other possibilities ? @dbtsai in your strata presentation you used one vs all ? Did you add some constraints on the fact that you penalize if mis-predicted labels are not very far from the true label ? Thanks. Deb Hi, In the sparse vector the toString API is as follows: override def toString: String = {"(" + size + "," + indices.zip(values).mkString("[", "," ,"]") + ")"} Does it make sense to keep it consistent with libsvm format ? What does each line of libsvm format looks like ? Thanks. Deb Hi, I see ALS is still using Array[Int] but for other mllib algorithm we moved to Vector[Double] so that it can support either dense and sparse formats... I know ALS can stay in Array[Int] due to the Netflix format for input datasets which is well defined but it helps if we move ALS to Vector[Double] as well...that way all algorithms will be consistent... Does it make sense ? Thanks. Deb Hi Patrick, We maintain internal Spark mirror in sync with Spark github master... What's the way to get the 1.0.0 stable release from github to deploy on our production cluster ? Is there a tag for 1.0.0 that I should use to deploy ? Thanks. Deb Hi, We are adding a constrained ALS solver in Spark to solve matrix factorization use-cases which needs additional constraints (bounds, equality, inequality, quadratic constraints) We are using a native version of a primal dual SOCP solver due to its small memory footprint and sparse ccs matrix computation it uses...The solver depends on AMD and LDL packages from Timothy Davis for sparse ccs matrix algebra (released under lgpl)... Due to GPL dependencies, it won't be possible to release the code as Apache license for now...If we get good results on our use-cases, we will plan to write a version in breeze/modify joptimizer for sparse ccs operations... I derived ConstrainedALS from Spark mllib ALS and I am comparing the performance with default ALS and non-negative ALS as baseline. Plan is to release the code as GPL license for community review...I have kept the package structure as org.apache.spark.mllib.recommendation There are some private functions defined in ALS, which I would like to reuse....Is it possible to take the private out from the following functions: 1. makeLinkRDDs 2. makeInLinkBlock 3. makeOutLinkBlock 4. randomFactor 5. unblockFactors I don't want to copy any code.... I can ask for a PR to make these changes... Thanks. Deb Hi Xiangrui, For orthogonality properties in the factors we need a constraint solver other than the usuals (l1, upper and lower bounds, l2 etc) The interface of constraint solver is standard and I can add it in mllib optimization.... But I am not sure how will I call the gpl licensed ipm solver from mllib....assume the solver interface is as follows: Qpsolver (densematrix h, array [double] f, int linearEquality, int linearInequality, bool lb, bool ub) And then I have functions to update equalities, inequalities, bounds etc followed by the run which generates the solution.... For l1 constraints I have to use epigraph formulation which needs a variable transformation before the solve.... I was thinking that for the problems that does not need constraints people will use ALS.scala and ConstrainedALS.scala will have the constrained formulations.... I can point you to the code once it is ready and then you can guide me how to refactor it to mllib als ? Thanks. Deb Hi Deb, Why do you want to make those methods public? If you only need to replace the solver for subproblems. You can try to make the solver pluggable. Now it supports least squares and non-negative least squares. You can define an interface for the subproblem solvers and maintain the IPM solver at your own code base, if the only information you need is Y^T Y and Y^T b. Btw, just curious, what is the use case for quadratic constraints? Best, Xiangrui Hi Xiangrui, It's not the linear constraint, It is quadratic inequality with slack, first order taylor approximation of off diagonal cross terms and a cyclic coordinate descent, which we think will yield orthogonality....It's still under works... Also we want to put a L1 constraint as set of linear equations when solving for ALS... I will create the JIRA...as I see it, this will evolve to a generic constraint solver for machine learning problems that has a QP structure....ALS is one example....another example is kernel SVMs... I did not know that lgpl solver can be added to the classpath....if it can be then definitely we should add these in ALS.scala... Thanks. Deb Hi, I am bit confused wiht the code here: // Solve the least-squares problem for each user and return the new feature vectors Array.range(0, numUsers).map { index => // Compute the full XtX matrix from the lower-triangular part we got above fillFullMatrix(userXtX(index), fullXtX) // Add regularization var i = 0 while (i < rank) {fullXtX.data(i * rank + i) += lambda i += 1 } // Solve the resulting matrix, which is symmetric and positive-definite algo match {case ALSAlgo.Implicit => Solve.solvePositive(fullXtX.addi(YtY.get.value), userXy(index)).data case ALSAlgo.Explicit => Solve.solvePositive(fullXtX, userXy (index)).data } } Sorry last one went out by mistake: Is not for users (0 to numUsers), fullXtX is same ? In the ALS formulation this is W^TW or H^TH which should be same for all the users ? Why we are reading userXtX(index) and adding it to fullXtX in the loop over all numUsers ? // Solve the least-squares problem for each user and return the new feature vectors Array.range(0, numUsers).map { index => // Compute the full XtX matrix from the lower-triangular part we got above fillFullMatrix(userXtX(index), fullXtX) // Add regularization var i = 0 while (i < rank) {fullXtX.data(i * rank + i) += lambda i += 1 } // Solve the resulting matrix, which is symmetric and positive-definite algo match {case ALSAlgo.Implicit => Solve.solvePositive(fullXtX.addi(YtY.get.value), userXy(index)).data case ALSAlgo.Explicit => Solve.solvePositive(fullXtX, userXy (index)).data } } I got it...ALS formulation is solving the matrix completion problem.... To convert the problem to matrix factorization or take user feedback (missing entries means the user hate the site ?), we should put 0 to the missing entries (or may be -1)...in that case we have to use computeYtY and accumulate over users in each block to generate full gram matrix...and after that while computing userXy(index) we have to be careful in putting 0/-1 for rest of the features... Is implicit feedback trying to do something like this ? Basically I am trying to see if it is possible to cache the gram matrix and it's cholesky factorization, and then call the QpSolver multiple times with updated gradient term...I am expecting better runtimes than dposv when ranks are high... But seems like that's not possible without a broadcast step which might kill all the runtime gain... Hi, In my experiments with Jellyfish I did not see any substantial RMSE loss over DSGD for Netflix dataset... So we decided to stick with ALS and implemented a family of Quadratic Minimization solvers that stays in the ALS realm but can solve interesting constraints(positivity, bounds, L1, equality constrained bounds etc)...We are going to show it at the Spark Summit...Also ALS structure is favorable to matrix factorization use-cases where missing entries means zero and you want to compute a global gram matrix using broadcast and use that for each Quadratic Minimization for all users/products... Implementing DSGD in the data partitioning that Spark ALS uses will be straightforward but I would be more keen to see a dataset where DSGD is showing you better RMSEs than ALS.... If you have a dataset where DSGD produces much better result could you please point it to us ? Also you can use Jellyfish to run DSGD benchmarks to compare against ALS...It is multithreaded and if you have good RAM, you should be able to run fairly large datasets... Be careful about the default Jellyfish...it has been tuned for netflix dataset (regularization, rating normalization etc)...So before you compare RMSE make sure ALS and Jellyfish is running same algorithm (L2 regularized Quadratic Loss).... Thanks. Deb Hi, In my experiments with Jellyfish I did not see any substantial RMSE loss over DSGD for Netflix dataset... So we decided to stick with ALS and implemented a family of Quadratic Minimization solvers that stays in the ALS realm but can solve interesting constraints(positivity, bounds, L1, equality constrained bounds etc)...We are going to show it at the Spark Summit...Also ALS structure is favorable to matrix factorization use-cases where missing entries means zero and you want to compute a global gram matrix using broadcast and use that for each Quadratic Minimization for all users/products... Implementing DSGD in the data partitioning that Spark ALS uses will be straightforward but I would be more keen to see a dataset where DSGD is showing you better RMSEs than ALS.... If you have a dataset where DSGD produces much better result could you please point it to us ? Also you can use Jellyfish to run DSGD benchmarks to compare against ALS...It is multithreaded and if you have good RAM, you should be able to run fairly large datasets... Be careful about the default Jellyfish...it has been tuned for netflix dataset (regularization, rating normalization etc)...So before you compare RMSE make sure ALS and Jellyfish is running same algorithm (L2 regularized Quadratic Loss).... Thanks. Deb Hi, I am looking for an efficient linear CG to be put inside the Quadratic Minimization algorithms we added for Spark mllib. With a good linear CG, we should be able to solve kernel SVMs with this solver in mllib... I use direct solves right now using cholesky decomposition which has higher complexity as matrix sizes become large... I found out some jblas example code: https://github.com/mikiobraun/jblas-examples/blob/master/src/CG.java I was wondering if mllib developers have any experience using this solver and if this is better than apache commons linear CG ? Thanks. Deb Thanks David...Let me try it...I am keen to see the results first and later will look into runtime optimizations... Deb Factorization problems are non-convex and so both ALS and DSGD will converge to local minima and it is not clear which minima will be better than the other until we run both the algorithms and see... So I will still say get a DSGD version running in the test setup while you experiment with the Spark ALS...so that you can see if on your particular dataset DSGD is converging to a better minima... If you want I can put the DSGD code base that I used for experimentation on github...I am not sure if Professor Re already put it on github... Hi, I am coming up with an iterative solver for Equality and bound constrained quadratic minimization... I have the cholesky versions running but cholesky does not scale for large dimensions but works fine for matrix factorization use-cases where ranks are low.. Minimize 0.5x'Px + q'x s.t Aeq x = beq lb <= x <= ub Based on your decomposition you will end up using linear CG  in x-update or NLCG/BFGS with bounds...I am not sure which one is better unless I see both of them running on datasets.... I am hoping we can re-use the solver for SVM variants... Could you please point to some implementation references for Nystrom approximations or kernel mappings ? Thanks. Deb Thanks Tom for the pointers... I have a IPM running on the JVM which uses SOCP formulation for the quadratic program I wrote above We are going to show the details of it at the Summit....IPM runtimes and accuracy give a baseline for the problem that we are solving... Now we are trying to see how to come up with efficient versions for cases where the constraints are not that many or not very complicated...which is what most ML problems have...The idea is to use ADMM decomposition for them... If the constraints are LP style complex, it is better to use the IPM with the SOCP directly... Let me look into the blog and update you more...with jblas and breeze-netlib-java I doubt there is any numerics that we cannot do on JVM !...With these packages we call BLAS libraries from fortran...Missing is sparse linear algebra from Tim Davis which will be exposed to the JVM in the IPM package that I built... Hi Xiangrui, Could you please point to the IPM solver that you have positive results with ? I was planning to compare with CVX, KNITRO from Professor Nocedal and MOSEK probably...I don't have CPLEX license so I won't be able to do that comparison... My experiments so far tells me that ADMM based solver is faster than IPM for simpler constraints but then perhaps I did not choose the correct IPM.... Proximal algorithm paper also shows very similar results compared to CVX: http://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf Thanks. Deb Hi Xiangrui, I did some out-of-box comparisons with ECOS and PDCO from SOL. Both of them seems to be running at par but I will do more detailed analysis. I used pdco's testQP randomized problem generation. pdcotestQP(m, n) means m constraints and n variables For ECOS runtime reference here is the paper http://web.stanford.edu/~boyd/papers/pdf/ecos_ecc.pdf It runs at par with gurobi but slower than MOSEK. Note that MOSEK is also a SOCP solver. K>> pdcotestQP(50, 100) ECOSQP: Converting QP to SOCP... ECOSQP: Time for Cholesky: 0.00 seconds Conversion completed. Calling ECOS... ECOS - (c) A. Domahidi, Automatic Control Laboratory, ETH Zurich, 2012-2014. OPTIMAL (within feastol=1.0e-05, reltol=1.0e-06, abstol=1.0e-06). Runtime: 0.010340 seconds. -------------------------------------------------------- pdco.m                            Version of 23 Nov 2013 Primal-dual barrier method to minimize a convex function subject to linear constraints Ax + r = b,  bl <= x <= bu Michael Saunders       SOL and ICME, Stanford University Contributors:     Byunggyoo Kim (SOL), Chris Maes (ICME) Santiago Akle (ICME), Matt Zahr (ICME) -------------------------------------------------------- m        =       50     n        =      100      nnz(A)  =      483 Method   =       21     (1=chol  2=QR  3=LSMR  4=MINRES  21=SQD(LU) 22=SQD(MA57)) Elapsed time is 0.050226 seconds. 2. With a larger problem with 50 equality and 1000 variables: K>> pdcotestQP(50, 1000) ECOSQP: Converting QP to SOCP... ECOSQP: Time for Cholesky: 0.05 seconds Conversion completed. Calling ECOS... ECOS - (c) A. Domahidi, Automatic Control Laboratory, ETH Zurich, 2012-2014. OPTIMAL (within feastol=1.0e-05, reltol=1.0e-06, abstol=1.0e-06). Runtime: 6.333036 seconds. -------------------------------------------------------- pdco.m                            Version of 23 Nov 2013 Primal-dual barrier method to minimize a convex function subject to linear constraints Ax + r = b,  bl <= x <= bu Michael Saunders       SOL and ICME, Stanford University Contributors:     Byunggyoo Kim (SOL), Chris Maes (ICME) Santiago Akle (ICME), Matt Zahr (ICME) -------------------------------------------------------- The objective is defined by a function handle: @(x)deal(0.5*(x'*H*x)+c'*x,H*x+c,H) The matrix A is an explicit sparse matrix m        =       50     n        =     1000      nnz(A)  =     4842 Method   =       21     (1=chol  2=QR  3=LSMR  4=MINRES  21=SQD(LU) 22=SQD(MA57)) Elapsed time is 7.531934 seconds. I will change the Method = 21 (LU) to 1 (chol) and that should help PDCO. If both the IPMs are at par it's still a good idea to choose ECOS as the generic IPM since it can solve conic programs which are a superset of quadratic programs (robust portfolio optimization from quantitative finance is an example of standard conic program). For the runtime comparisons with ADMM based decomposition for simpler constraints, I am doing further profiling and see if the jnilib is causing any performance issues for ECOS calls... Please look at the proximal algorithm references http://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf. For many problems like L1 constraint / positivity / bounds / huber / hyperplane projection etc, the proximal operator is simple to evaluate and for these cases ADMM decomposition has been shown to run faster than standard constraint solvers like IPM. I am not very surprised that Sparse NMF runs in 4X runtime of least squares using ADMM decomposition. Distributed consensus is another ADMM decomposition which we are working with right now. We will have some results on that soon. There the idea is to use ADMM so that accumulator need not collect dense gradient vectors from each worker. This development will further help the treeReduce work. Should I open up Spark JIRA's so that we can document Quadratic Minimization related runtime experiments/benchmarks and share the code for review ? Most likely the core solvers will go to breeze and in Spark mllib optimization, I will add a QpSolver object which will call the underlying breeze solvers based on the problem complexity....the ecos jnilib can be part of breeze natives as it is GPL licensed (same as netlib-java jniloader). I will push the jnilib as a PR to ecos repository https://github.com/ifa-ethz/ecos Thanks. Deb Hi Denis, Are you using matrix factorization to generate the latent factors ? Thanks. Deb Thanks for the pointer... Looks like you are using EM algorithm for factorization which looks similar to multiplicative update rules Do you think using mllib ALS implicit feedback, you can scale the problem further ? We can handle L1, L2, equality and positivity constraints in ALS now...As long as you can find the gradient and hessian from the KL divergence loss, you can use that in place of gram matrix that is used in ALS right now If you look in topic modeling work in Solr (Carrot is the package), they use ALS to generate the topics...that algorithm looks like a simplified version of what you are attempting here... May be the EM algorithm for topic modeling is efficient than ALS but from looking at it I don't see how...I see lot of broadcasts...while in implicit feedback you need one broadcast of gram matrix... I looked further and realized that ECOS used a mex file while PDCO is using pure Matlab code. So the out-of-box runtime comparison is not fair. I am trying to generate PDCO C port. Like ECOS, PDCO also makes use of sparse support from Tim Davis. Thanks. Deb Hi, I thought OWLQN is already merged to mllib optimization but I don't see it in the master yet... Are there any issues in merging it in ? I see there are some merge conflicts right now... https://github.com/apache/spark/pull/840/ Thanks. Deb Hi, Is sbt still used for master compilation ? I could compile for 2.3.0-cdh5.0.2 using maven following the instructions from the website: http://spark.apache.org/docs/latest/building-with-maven.html But when I am trying to use sbt for local testing and then I am getting some weird errors...Is sbt still used by developers ? I am using JDK7... org.xml.sax.SAXParseException; lineNumber: 4; columnNumber: 57; Element type "settings" must be followed by either attribute specifications, ">" or "/>". at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198) at com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.fatalError(ErrorHandlerWrapper.java:177) at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:441) at com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368) at com.sun.org.apache.xerces.internal.impl.XMLScanner.reportFatalError(XMLScanner.java:1436) at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.seekCloseOfStartTag(XMLDocumentFragmentScannerImpl.java:1394) at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanStartElement(XMLDocumentFragmentScannerImpl.java:1327) at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$ContentDriver.scanRootElementHook(XMLDocumentScannerImpl.java:1292) at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:3122) at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$PrologDriver.next(XMLDocumentScannerImpl.java:880) at com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606) at com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510) at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848) at com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777) at com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141) at com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1213) at com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:649) at com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl.parse(SAXParserImpl.java:333) at scala.xml.factory.XMLLoader$class.loadXML(XMLLoader.scala:40) at scala.xml.XML$.loadXML(XML.scala:57) at scala.xml.factory.XMLLoader$class.load(XMLLoader.scala:52) at scala.xml.XML$.load(XML.scala:57) at com.typesafe.sbt.pom.MavenHelper$$anonfun$settingsXml$1.apply(MavenHelper.scala:225) at com.typesafe.sbt.pom.MavenHelper$$anonfun$settingsXml$1.apply(MavenHelper.scala:224) at sbt.Using.apply(Using.scala:25) at com.typesafe.sbt.pom.MavenHelper$.settingsXml(MavenHelper.scala:224) at com.typesafe.sbt.pom.MavenHelper$.settingsXmlServers(MavenHelper.scala:245) at com.typesafe.sbt.pom.MavenHelper$.createSbtCredentialsFromSettingsXml(MavenHelper.scala:291) at com.typesafe.sbt.pom.MavenHelper$$anonfun$pullSettingsFromPom$10.apply(MavenHelper.scala:83) at com.typesafe.sbt.pom.MavenHelper$$anonfun$pullSettingsFromPom$10.apply(MavenHelper.scala:83) at sbt.Scoped$RichInitialize$$anonfun$map$1$$anonfun$apply$3.apply(Structure.scala:177) at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:45) at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:45) at sbt.std.Transform$$anon$4.work(System.scala:64) at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:237) at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:237) at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18) at sbt.Execute.work(Execute.scala:244) at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:237) at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:237) at sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:160) at sbt.CompletionService$$anon$2.call(CompletionService.scala:30) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) at java.util.concurrent.FutureTask.run(FutureTask.java:262) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) at java.lang.Thread.run(Thread.java:745) Thanks. Deb I am at the reservoir sampling commit: commit 586e716e47305cd7c2c3ff35c0e828b63ef2f6a8 Author: Reynold Xin Date:   Fri Jul 18 12:41:50 2014 -0700 sbt/sbt -Dhttp.nonProxyHosts=132.197.10.21 [info] Set current project to spark-mllib (in build file:/Users/v606014/spark-master/) [trace] Stack trace suppressed: run last mllib/*:credentials for the full output. [trace] Stack trace suppressed: run last core/*:credentials for the full output. [error] (mllib/*:credentials) org.xml.sax.SAXParseException; lineNumber: 4; columnNumber: 57; Element type "settings" must be followed by either attribute specifications, ">" or "/>". [error] (core/*:credentials) org.xml.sax.SAXParseException; lineNumber: 4; columnNumber: 57; Element type "settings" must be followed by either attribute specifications, ">" or "/>". [error] Total time: 0 s, completed Jul 19, 2014 6:09:24 PM I figured out the cause...brew is updated to scala 2.11 and I got the latest scala version... Once I reverted back to 2.10.4, HEAD and 1.0.1 tag compiles fine... Hi, I have deployed spark stable 1.0.1 on the cluster but I have new code that I added in mllib-1.1.0-SNAPSHOT. I am trying to access the new code using spark-submit as follows: spark-job --class com.verizon.bda.mllib.recommendation.ALSDriver --executor-memory 16g --total-executor-cores 16 --jars spark-mllib_2.10-1.1.0-SNAPSHOT.jar,scopt_2.10-3.2.0.jar sag-core-0.0.1-SNAPSHOT.jar --rank 25 --numIterations 10 --lambda 1.0 --qpProblem 2 inputPath outputPath I can see the jars are getting added to httpServer as expected: 14/08/02 12:50:04 INFO SparkContext: Added JAR file:/vzhome/v606014/spark-glm/spark-mllib_2.10-1.1.0-SNAPSHOT.jar at http://10.145.84.20:37798/jars/spark-mllib_2.10-1.1.0-SNAPSHOT.jar with timestamp 1406998204236 14/08/02 12:50:04 INFO SparkContext: Added JAR file:/vzhome/v606014/spark-glm/scopt_2.10-3.2.0.jar at http://10.145.84.20:37798/jars/scopt_2.10-3.2.0.jar with timestamp 1406998204237 14/08/02 12:50:04 INFO SparkContext: Added JAR file:/vzhome/v606014/spark-glm/sag-core-0.0.1-SNAPSHOT.jar at http://10.145.84.20:37798/jars/sag-core-0.0.1-SNAPSHOT.jar with timestamp 1406998204238 But the job still can't access code form mllib-1.1.0 SNAPSHOT.jar...I think it's picking up the mllib from cluster which is at 1.0.1... Please help. I will ask for a PR tomorrow but internally we want to generate results from the new code. Thanks. Deb Let me try it... Will this be fixed if I generate a assembly file with mllib-1.1.0 SNAPSHOT jar and other dependencies with the rest of the application code ? I created the assembly file but still it wants to pick the mllib from the cluster: jar tf ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar | grep QuadraticMinimizer org/apache/spark/mllib/optimization/QuadraticMinimizer$$anon$1.class /Users/v606014/dist-1.0.1/bin/spark-submit --master spark://TUSCA09LMLVT00C.local:7077 --class ALSDriver ./target/ml-0.0.1-SNAPSHOT-jar-with-dependencies.jar inputPath outputPath Exception in thread "main" java.lang.NoSuchMethodError: org.apache.spark.mllib.recommendation.ALS.setLambdaL1(D)Lorg/apache/spark/mllib/recommendation/ALS; Now if I force it to use the jar that I gave using spark.files.userClassPathFirst, then it fails on some serialization issues... A simple solution is to cherry pick the files I need from spark branch to the application branch but I am not sure that's the right thing to do... The way userClassPathFirst is behaving, there might be bugs in it... Any suggestions will be appreciated.... Thanks. Deb Hi Xiangrui, I used your idea and kept a cherry picked version of ALS.scala in my application and call it ALSQp.scala...this is a OK workaround for now till a version adds up to master for example... For the bug with userClassPathFirst, looks like Koert already found this issue in the following JIRA: https://issues.apache.org/jira/browse/SPARK-1863 By the way the userClassPathFirst feature is very useful since I am sure the deployed version of spark on a production cluster will always be the last stable (core at 1.0.1 in my case) and people would like to deploy SNAPSHOT versions of libraries that build on top of spark core (mllib, streaming etc)... Another way is to have a build option that deploys only the core and not the libraries built upon core... Do we have an option like that in make-distribution script ? Thanks. Deb Hi Xiangrui, Maintaining another file will be a pain later so I deployed spark 1.0.1 without mllib and then my application jar bundles mllib 1.1.0-SNAPSHOT along with the code changes for quadratic optimization... Later the plan is to patch the snapshot mllib with the deployed stable mllib... There are 5 variants that I am experimenting with around 400M ratings (daily data, monthly data I will update in few days)... 1. LS 2. NNLS 3. Quadratic with bounds 4. Quadratic with L1 5. Quadratic with equality and positivity Now the ALS 1.1.0 snapshot runs fine but after completion on this step ALS.scala:311 // Materialize usersOut and productsOut. usersOut.count() I am getting from one of the executors: java.lang.ClassCastException: scala.Tuple1 cannot be cast to scala.Product2 I am debugging it further but I was wondering if this is due to RDD compatibility within 1.0.1 and 1.1.0-SNAPSHOT ? I have built the jars on my Mac which has Java 1.7.0_55 but the deployed cluster has Java 1.7.0_45. The flow runs fine on my localhost spark 1.0.1 with 1 worker. Can that Java version mismatch cause this ? Stack traces are below Thanks. Deb Executor stacktrace: org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:156) org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:154) scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:154) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126) org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:123) scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772) scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108) scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771) org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:123) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158) org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99) org.apache.spark.scheduler.Task.run(Task.scala:51) org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183) java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) java.lang.Thread.run(Thread.java:744) Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634) at scala.Option.foreach(Option.scala:236) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634) at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498) at akka.actor.ActorCell.invoke(ActorCell.scala:456) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237) at akka.dispatch.Mailbox.run(Mailbox.scala:219) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) Ok...let me look into it a bit more and most likely I will deploy the Spark v1.1 and then use mllib 1.1 SNAPSHOT jar with it so that we follow your guideline of not running newer spark component on older version of spark core... That should solve this issue unless it is related to Java versions.... I am also keen to see the final recommendation within L1 and Positivity....I will compute the metrics Our plan is to use scalable matrix factorization as an engine to do clustering, feature extraction, topic modeling and auto encoders (single layer to start with). So these algorithms are not really constrained to recommendation use-cases... I did not play with Hadoop settings...everything is compiled with 2.3.0CDH5.0.2 for me... I did try to bump the version number of HBase from 0.94 to 0.96 or 0.98 but there was no profile for CDH in the pom...but that's unrelated to this ! Hi Patrick, I am testing the 1.1 branch but I see lot of protobuf warnings while building the jars: [warn] Class com.google.protobuf.Parser not found - continuing with a stub. [warn] Class com.google.protobuf.Parser not found - continuing with a stub. [warn] Class com.google.protobuf.Parser not found - continuing with a stub. [warn] Class com.google.protobuf.Parser not found - continuing with a stub. I am compiling for 2.3.0cdh5.0.2...Later when running the jobs I am getting a protobuf error: Exception in thread "main" java.lang.VerifyError: class org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$AddBlockRequestProto overrides final method getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet; Is there a protobuf issue on this branch ? Also on the branch at least I am noticing the following: Welcome to ____              __ / __/__  ___ _____/ /__ _\ \/ _ \/ _ `/ __/  '_/ /___/ .__/\_,_/_/ /_/\_\   version 1.0.0-SNAPSHOT /_/ Won't it be 1.1.0-SNAPSHOT ? Thanks. Deb Hi Xiangrui, Based on your suggestion I moved core and mllib both to 1.1.0-SNAPSHOT...I am still getting class cast exception: Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 249 in stage 52.0 failed 4 times, most recent failure: Lost task 249.3 in stage 52.0 (TID 10002, tblpmidn06adv-hdp.tdc.vzwcorp.com): java.lang.ClassCastException: scala.Tuple1 cannot be cast to scala.Product2 I am running ALS.scala merged with my changes. I will try the mllib jar without my changes next... Can this be due to the fact that my jars are compiled with Java 1.7_55 but the cluster JRE is at 1.7_45. Thanks. Deb I validated that I can reproduce this problem with master as well (without adding any of my mllib changes)... I separated mllib jar from assembly, deploy the assembly and then I supply the mllib jar as --jars option to spark-submit... I get this error: 14/08/09 12:49:32 INFO DAGScheduler: Failed to run count at ALS.scala:299 Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 238 in stage 40.0 failed 4 times, most recent failure: Lost task 238.3 in stage 40.0 (TID 10002, tblpmidn05adv-hdp.tdc.vzwcorp.com): java.lang.ClassCastException: scala.Tuple1 cannot be cast to scala.Product2 org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5$$anonfun$apply$4.apply(CoGroupedRDD.scala:159) scala.collection.Iterator$$anon$11.next(Iterator.scala:328) org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:138) org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159) org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:158) scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772) scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771) org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:158) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:129) org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126) scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772) scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108) scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771) org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:126) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:61) org.apache.spark.rdd.RDD.iterator(RDD.scala:227) org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62) org.apache.spark.scheduler.Task.run(Task.scala:54) org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199) java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) java.lang.Thread.run(Thread.java:744) Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1153) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1142) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1141) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1141) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682) at scala.Option.foreach(Option.scala:236) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:682) at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1359) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498) at akka.actor.ActorCell.invoke(ActorCell.scala:456) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237) at akka.dispatch.Mailbox.run(Mailbox.scala:219) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) I will try now with mllib inside the assembly....If that works then something is weird here ! Including mllib inside assembly worked fine...If I deploy only the core and send mllib as --jars then this problem shows up... Xiangrui could you please comment if it is a bug or expected behavior ? I will create a JIRA if this needs to be tracked... Actually nope it did not work fine... With multiple ALS iteration, I am getting the same error (with or without my mllib changes).... Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 206 in stage 52.0 failed 4 times, most recent failure: Lost task 206.3 in stage 52.0 (TID 9999, tblpmidn42adv-hdp.tdc.vzwcorp.com): java.lang.ClassCastException: scala.Tuple1 cannot be cast to scala.Product2 org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5$$anonfun$apply$4.apply(CoGroupedRDD.scala:159) scala.collection.Iterator$$anon$11.next(Iterator.scala:328) org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:138) org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:159) org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$5.apply(CoGroupedRDD.scala:158) scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772) scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771) org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:158) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:129) org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$2.apply(CoGroupedRDD.scala:126) scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772) scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108) scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771) org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:126) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33) org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) org.apache.spark.rdd.RDD.iterator(RDD.scala:229) org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68) org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) org.apache.spark.scheduler.Task.run(Task.scala:54) org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:199) java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) java.lang.Thread.run(Thread.java:744) Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1153) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1142) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1141) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1141) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:682) at scala.Option.foreach(Option.scala:236) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:682) at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1359) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498) at akka.actor.ActorCell.invoke(ActorCell.scala:456) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237) at akka.dispatch.Mailbox.run(Mailbox.scala:219) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) The behavior is consistent in standalone and yarn mode... I am at the following checkin: commit ec79063fad44751a6689f5e58d47886babeaecff I also tested yarn deployment and I will use standalone mode to deploy stable spark release (1.0.1 right now) and all the mllib changes I can test on our datasets through yarn deployment...it works fine... By the way, let me try if I can reproduce this issue on MovieLensALS locally....Most likely it is a bug Thanks. Deb I figured out the issue....the driver memory was at 512 MB and for our datasets, the following code needed more memory... // Materialize usersOut and productsOut. usersOut.count() productsOut.count() Thanks. Deb Hi, Is there a JIRA for this bug ? I have seen it multiple times during our ALS runs now...some runs don't show while some runs fail due to the error msg https://github.com/GrahamDennis/spark-kryo-serialisation/blob/master/README.md One way to circumvent this is to not use kryo but then I am not sure how performance will get impacted... Thanks. Deb Hi, We are running the snapshots (new spark features) on YARN and I was wondering if the webui is available on YARN mode... The deployment document does not mention webui on YARN mode... Is it available ? Thanks. Deb Hi, During the 4th ALS iteration, I am noticing that one of the executor gets disconnected: 14/08/19 23:40:00 ERROR network.ConnectionManager: Corresponding SendingConnectionManagerId not found 14/08/19 23:40:00 INFO cluster.YarnClientSchedulerBackend: Executor 5 disconnected, so removing it 14/08/19 23:40:00 ERROR cluster.YarnClientClusterScheduler: Lost executor 5 on tblpmidn42adv-hdp.tdc.vzwcorp.com: remote Akka client disassociated 14/08/19 23:40:00 INFO scheduler.DAGScheduler: Executor lost: 5 (epoch 12) Any idea if this is a bug related to akka on YARN ? I am using master Thanks. Deb I could reproduce the issue in both 1.0 and 1.1 using YARN...so this is definitely a YARN related problem... At least for me right now only deployment option possible is standalone... Hi, There have been some recent changes in the way akka is used in spark and I feel they are major changes... Is there a design document / JIRA / experiment on large datasets that highlight the impact of changes (1.0 vs 1.1) ? Basically it will be great to understand where akka is used in the code base... If I don't have to broadcast big variables but use akka's programming model (use actors directly) on Spark's actorsystem is that allowed ? I understand that it might look hacky :-) Thanks. Deb Hi Patrick, Last few days I came across some bugs which got exposed due to ALS runs on large scale data...although it was not related to the akka changes but during the debug I found across some akka related changes that might have an impact of overall performance...one example is the following: https://github.com/apache/spark/pull/1907 @dbtsai explained it to me a bit yesterday that in 1.1 RDDs are no longer sent through akka msgs but over http-channels...If there is a document detailing the architecture that is currently in-place (like how the core changed from 1.0 to 1.1) it will help a lot in debugging the jobs which are built upon the libraries like mllib and optimize them further for efficiency... For using the Spark actor system directly: I spent few weeks December 2013 to make the Scalafish code (https://github.com/azymnis/scalafish) operational on 10 nodes...It uses scalding for matrix partitioning and actorSystem to coordinate the updates...It is a cool use of akka but getting an actor system operational is difficult... Since Spark already has tested version of actor system running on both standalone and yarn modes, I am planning to port scalafish to spark using actor model...That's one of the use-cases I am looking for... Another use-case that I am considering is to send msgs directly from kafka queues to spark actorSystem for processing to get Storm like latency...basically window sizes of 1-2 ms and no overhead of using an RDD if possible... Thanks. Deb Yeah that's the one we discussed...sorry I pointed to a different one that I was reading... Sandy, I put spark.yarn.executor.memoryOverhead 1024 on spark-defaults.conf but I don't see environment variable on spark properties on the webui->environment Does it need to go in spark-env.sh ? Thanks. Deb Hi Sandy, Any resolution for YARN failures ? It's a blocker for running spark on top of YARN. Thanks. Deb Hmm...I did try it increase to few gb but did not get a successful run yet... Any idea if I am using say 40 executors, each running 16GB, what's the typical spark.yarn.executor.memoryOverhead for say 100M x 10 M large matrices with say few billion ratings... Last time it did not show up on environment tab but I will give it another shot...Expected behavior is that this env variable will show up right ? You should look into Evan Spark's talk from Spark Summit 2014 http://spark-summit.org/2014/talk/model-search-at-scale I am not sure if some of it is already open sourced through MLBase... Hi, Inside mllib I am running tests using: mvn -Dhadoop.version=2.3.0-cdh5.1.0 -Phadoop-2.3 -Pyarn install The locat tests run fine but cluster tests are failing.. LBFGSClusterSuite: - task size should be small *** FAILED *** org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: FAILED Do I need to start a localhost spark cluster first before running these tests that has ClusterSuite in them ? Thanks. Deb I have done mvn clean several times... Consistently all the mllib tests that are using LocalClusterSparkContext.scala, they fail ! Hi, I have added some changes to ALS tests and I am re-running tests as: mvn -Dhadoop.version=2.3.0-cdh5.1.0 -Phadoop-2.3 -Pyarn -DwildcardSuites=org.apache.spark.mllib.recommendation.ALSSuite test I have some INFO logs in the code which I want to see on my console. They work fine if I add println. I copied conf/log4j.properties.template to conf/log4j.properties The options are: log4j.rootCategory=INFO, console log4j.appender.console=org.apache.log4j.ConsoleAppender log4j.appender.console.target=System.err I still don't see the INFO msgs on the console. Any idea if I am setting up my log4j properties correctly ? Thanks. Deb Thanks Sean...trying them out... Awesome news Matei ! Congratulations to the databricks team and all the community members... Hi, If I take the Movielens data and run the default ALS with regularization as 0.0, I am hitting exception from LAPACK that the gram matrix is not positive definite. This is on the master branch. This is how I run it : ./bin/spark-submit --total-executor-cores 1 --master spark:// tusca09lmlvt00c.uswin.ad.vzwcorp.com:7077 --jars /Users/v606014/.m2/repository/com/github/scopt/scopt_2.10/3.2.0/scopt_2.10-3.2.0.jar --class org.apache.spark.examples.mllib.MovieLensALS ./examples/target/spark-examples_2.10-1.1.0-SNAPSHOT.jar --rank 20 --numIterations 20 --lambda 0.0 --kryo hdfs://localhost:8020/sandbox/movielens/ Error from LAPACK: WARN TaskSetManager: Lost task 0.0 in stage 11.0 (TID 22, tusca09lmlvt00c.uswin.ad.vzwcorp.com): org.jblas.exceptions.LapackArgumentException: LAPACK DPOSV: Leading minor of order i of A is not positive definite. ||r - wi'hj||^{2} has to be positive definite... I think the tests are not running any 0.0 regularization tests otherwise we should have caught it as well... For the sparse coding NMF variant that I am running, I have to turn off L2 regularization when I run a L1 on products to extract sparse topics... Thanks. Deb But do you expect the mllib code to fail if I run with 0.0 regularization ? I think ||r - wi'hj||^{2} is positive definite...It can become positive semi definite only if there are dependent rows in the matrix... @sean is that right ? We had this discussion before as well... @xiangrui should we add this epsilon inside ALS code itself ? So that if user by mistake put 0.0 as regularization, LAPACK failures does not show up... @sean For the proximal algorithms I am using Cholesky for L1 and LU for equality and bound constraints (since the matrix is quasi definite)...I am right now experimenting with the nesterov acceleration...I should definitely use QR in place of LU...I am already BLAS solves from netlib which is not in jblas so this should be fine as well... Details are over here: https://github.com/apache/spark/pull/2705 Just checked, QR is exposed by netlib: import org.netlib.lapack.Dgeqrf For the equality and bound version, I will use QR...it will be faster than the LU that I am using through jblas.solveSymmetric... Hi, I am validating the proximal algorithm for positive and bound constrained ALS and I came across the bug detailed in the JIRA while running ALS with NNLS: https://issues.apache.org/jira/browse/SPARK-3987 ADMM based proximal algorithm came up with correct result... Thanks. Deb Hi, Is someone working on a project on integrating Oryx model serving layer with Spark ? Models will be built using either Streaming data / Batch data in HDFS and cross validated with mllib APIs but the model serving layer will give API endpoints like Oryx and read the models may be from hdfs/impala/SparkSQL One of the requirement is that the API layer should be scalable and elastic...as requests grow we should be able to add more nodes...using play and akka clustering module... If there is a ongoing project on github please point to it... Is there a plan of adding model serving and experimentation layer to mllib ? Thanks. Deb Hi Nick, Any specific reason of choosing scalatra and not play/spray (now that they are getting integrated) ? Sean, Would you be interested in a play and akka clustering based module in oryx2 and see how it compares against the servlets ? I am interested to understand the scalability.... Thanks. Deb Hi, In the current factorization flow, we cross validate on the test dataset using the RMSE number but there are some other measures which are worth looking into. If we consider the problem as a regression problem and the ratings 1-5 are considered as 5 classes, it is possible to generate a confusion matrix using MultiClassMetrics.scala If the ratings are only 0/1 (like from the spotify demo from spark summit) then it is possible to use Binary Classification Metrices to come up with the ROC curve... For topK user/products we should also look into prec@k and pdcg@k as the metric.. Does it make sense to add the multiclass metric and prec@k, pdcg@k in examples.MovielensALS along with RMSE ? Thanks. Deb Makes sense for the binary and ranking problem but for example linear regression for multi-class also optimizes on RMSE but we still measure the prediction efficiency using some measure on confusion matrix...Is not the same idea should hold for ALS as well ? Is there an example of how to use RankingMetrics ? Let's take the user, document example...we get user x topic and document x topic matrices as the model... Now for each user, we can generate topK document by doing a sort on (1 x topic)dot(topic x document) and picking topK... Is it possible to validate such a topK finding algorithm using RankingMetrics ? I thought topK will save us...for each user we have 1xrank...now our movie factor is a RDD...we pick topK movie factors based on vector norm...with K = 50, we will have 50 vectors * num_executors in a RDD...with the user 1xrank we do a distributed dot product using RowMatrix APIs... May be we can't find topK using vector norm on movie factors... I am working on it...I will open up a JIRA once I see some results.. Idea is to come up with a test train set based on users...basically for each user, we come up with 80% train data and 20% test data... Now we pick up a K (each user should have a different K based on the movies he watched so some multiplier) and then we get topK for each user and see the confusion matrix for each user... This data will also go to RankingMetrics I think...one is ground truth array and the other is our prediction...I would like to see the raw confusions as well.. These measures are necessary to validate any of the topic modeling algorithms as well... Is there a better place for it other than mllib examples ? Does it make sense to have a user specific K or K is considered same over all users ? Intuitively the users who watches more movies should get a higher K than the others... Hi, I am testing MatrixFactorizationModel.predict(user: Int, product: Int) but the code fails on userFeatures.lookup(user).head In computeRmse MatrixFactorizationModel.predict(RDD[(Int, Int)]) has been called and in all the test-cases that API has been used... I can perhaps refactor my code to do the same but I was wondering whether people test the lookup(user) version of the code.. Do I need to cache the model to make it work ? I think right now default is STORAGE_AND_DISK... Thanks. Deb Hi, I build the master today and I was testing IR statistics on movielens dataset (open up a PR in a bit)... Right now in the master examples.MovieLensALS, case class Params extends AbstractParam[Params] On my localhost spark, if I run as follows it fails: ./bin/spark-submit --master spark:// tusca09lmlvt00c.uswin.ad.vzwcorp.com:7077 --jars /Users/v606014/.m2/repository/com/github/scopt/scopt_2.10/3.2.0/scopt_2.10-3.2.0.jar --total-executor-cores 4 --executor-memory 4g --driver-memory 1g --class org.apache.spark.examples.mllib.MovieLensALS ./examples/target/spark-examples_2.10-1.2.0-SNAPSHOT.jar --kryo --lambda 0.065 hdfs://localhost:8020/sandbox/movielens/ 2014-11-04 16:00:18.691 java[1811:1903] Unable to load realm mapping info from SCDynamicStore 14/11/04 16:00:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 14/11/04 16:00:21 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1, tusca09lmlvt00c.uswin.ad.vzwcorp.com): java.io.InvalidClassException: org.apache.spark.examples.mllib.MovieLensALS$Params; no valid constructor java.io.ObjectStreamClass$ExceptionInfo.newInvalidClassException(ObjectStreamClass.java:150) java.io.ObjectStreamClass.checkDeserialize(ObjectStreamClass.java:768) java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1772) java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990) java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915) java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798) java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350) java.io.ObjectInputStream.readObject(ObjectInputStream.java:370) org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62) org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87) org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:57) org.apache.spark.scheduler.Task.run(Task.scala:56) org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:186) java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) java.lang.Thread.run(Thread.java:745) If I remove AbstractParams from examples.MovieLensALS and recompile then code runs fine: ./bin/spark-submit --master spark:// tusca09lmlvt00c.uswin.ad.vzwcorp.com:7077 --jars /Users/v606014/.m2/repository/com/github/scopt/scopt_2.10/3.2.0/scopt_2.10-3.2.0.jar --total-executor-cores 4 --executor-memory 4g --driver-memory 1g --class org.apache.spark.examples.mllib.MovieLensALS ./examples/target/spark-examples_2.10-1.2.0-SNAPSHOT.jar --kryo --lambda 0.065 hdfs://localhost:8020/sandbox/movielens/ 2014-11-04 16:26:25.359 java[2892:1903] Unable to load realm mapping info from SCDynamicStore 14/11/04 16:26:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable Got 1000209 ratings from 6040 users on 3706 movies. Training: 800650, test: 199559. Test RMSE = 0.8525220763317215. 14/11/04 16:26:41 ERROR ConnectionManager: Corresponding SendingConnection to ConnectionManagerId(tusca09lmlvt00c.uswin.ad.vzwcorp.com,50749) not found 14/11/04 16:26:42 WARN ConnectionManager: All connections not cleaned up Is this a known issue for mllib examples ? If there is no open JIRA for this, I can open it up... Thanks. Deb I reproduced the problem in mllib tests ALSSuite.scala using the following functions: val arrayPredict = userProductsRDD.map{case(user,product) => val recommendedProducts = model.recommendProducts(user, products) val productScore = recommendedProducts.find{x=>x.product == product } require(productScore != None) productScore.get }.collect arrayPredict.foreach { elem => if (allRatings.get(elem.user, elem.product) != elem.rating) fail("Prediction APIs don't match") } If the usage of model.recommendProducts is correct, the test fails with the same error I sent before... org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 316.0 failed 1 times, most recent failure: Lost task 0.0 in stage 316.0 (TID 79, localhost): scala.MatchError: null org.apache.spark.rdd.PairRDDFunctions.lookup(PairRDDFunctions.scala:825) org.apache.spark.mllib.recommendation.MatrixFactorizationModel.recommendProducts(MatrixFactorizationModel.scala:81) It is a blocker for me and I am debugging it. I will open up a JIRA if this is indeed a bug... Do I have to cache the models to make userFeatures.lookup(user).head to work ? model.recommendProducts can only be called from the master then ? I have a set of 20% users on whom I am performing the test...the 20% users are in a RDD...if I have to collect them all to master node and then call model.recommendProducts, that's a issue... Any idea how to optimize this so that we can calculate MAP statistics on large samples of data ? I tested 2 different implementations to generate the predicted ranked list...The first version uses a cartesian of user and product features and then generates a predicted value for each (user,product) key... The second version does a collect on the skinny matrix (most likely products) and then broadcasts it to every node which computes the predicted value... cartesian is slower than the broadcast version...but in the broadcast version also the shuffle time is significant..Bottleneck is the groupBy on (user,product) composite key followed by local sort to generate topK... The third version I thought of was to use topK predict API but this works only if topK is bounded by a small number...If topK is large (say 100K) it does not work since then it is bounded by master memory... The block-wise cross product idea will optimize the groupBy right ? we break user and feature matrices into blocks (re-use ALS partitioning) and then in place of using (user,product) as a key use (userBlock, productBlock) as key...Does this help improve in shuffle size ? Hi, I am noticing the first step for Spark jobs does a TimSort in 1.2 branch...and there is some time spent doing the TimSort...Is this assigning the RDD blocks to different nodes based on a sort order ? Could someone please point to a JIRA about this change so that I can read more about it ? Thanks. Deb Andrew, I put up 1.1.1 branch and I am getting shuffle failures while doing flatMap followed by groupBy...My cluster memory is less than the memory I need and therefore flatMap does around 400 GB of shuffle...memory is around 120 GB... 14/11/13 23:10:49 WARN TaskSetManager: Lost task 22.1 in stage 191.0 (TID 4084, istgbd020.hadoop.istg.verizon.com): FetchFailed(null, shuffleId=4, mapId=-1, reduceId=22) I searched on user-list and this issue has been found over there: http://apache-spark-user-list.1001560.n3.nabble.com/Issues-with-partitionBy-FetchFailed-td14760.html I wanted to make sure whether 1.1.1 does not have the same bug...-1 from me till we figure out the root cause... Thanks. Deb Hi, I have a rdd whose key is a userId and value is (movieId, rating)... I want to sample 80% of the (movieId,rating) that each userId has seen for train, rest is for test... val indexedRating = sc.textFile(...).map{x=> Rating(x(0), x(1), x(2)) val keyedRatings = indexedRating.map{x => (x.product, (x.user, x.rating))} val keyedTraining = keyedRatings.sample(true, 0.8, 1L) val keyedTest = keyedRatings.subtract(keyedTraining) blocks = sc.maxParallelism println(s"Rating keys ${keyedRatings.groupByKey(blocks).count()}") println(s"Training keys ${keyedTraining.groupByKey(blocks).count()}") println(s"Test keys ${keyedTest.groupByKey(blocks).count()}") My expectation was that the println will produce exact number of keys for keyedRatings, keyedTraining and keyedTest but this is not the case... On MovieLens for example I am noticing the following: Rating keys 3706 Training keys 3676 Test keys 3470 I also tried sampleByKey as follows: val keyedRatings = indexedRating.map{x => (x.product, (x.user, x.rating))} val fractions = keyedRatings.map{x=> (x._1, 0.8)}.collect.toMap val keyedTraining = keyedRatings.sampleByKey(false, fractions, 1L) val keyedTest = keyedRatings.subtract(keyedTraining) Still I get the results as: Rating keys 3706 Training keys 3682 Test keys 3459 Any idea what's is wrong here... Are my assumptions about behavior of sample/sampleByKey on a key-value RDD correct ? If this is a bug I can dig deeper... Thanks. Deb Sean, I thought sampleByKey (stratified sampling) in 1.1 was designed to solve the problem that randomSplit can't sample by key... Xiangrui, What's the expected behavior of sampleByKey ? In the dataset sampled using sampleByKey the keys should match the input dataset keys right ? If it is a bug, I can open up a JIRA and look into it... Thanks. Deb For mllib PR, I will add this logic: "If a user is missing in training and appears in test, we can simply ignore it."I was struggling since users appear in test on which the model was not trained on... For our internal tests we want to cross validate on every product / user as all of them are equally important and so I have to come up with a sampling strategy for every user/product... In general for stratified sampling what's the bound on strata ? Like number of classes in a labeled dataset ~ 100 ? -1 from me...same FetchFailed issue as what Hector saw... I am running Netflix dataset and dumping out recommendation for all users. It shuffles around 100 GB data on disk to run a reduceByKey per user on utils.BoundedPriorityQueue...The code runs fine with MovieLens1m dataset... I gave Spark 10 nodes, 8 cores, 160 GB of memory. Fails with the following FetchFailed errors. 14/11/23 11:51:22 WARN TaskSetManager: Lost task 28.0 in stage 188.0 (TID 2818, tblpmidn08adv-hdp.tdc.vzwcorp.com): FetchFailed(BlockManagerId(1, tblpmidn03adv-hdp.tdc.vzwcorp.com, 52528, 0), shuffleId=35, mapId=28, reduceId=28) It's a consistent behavior on master as well. I tested it both on YARN and Standalone. I compiled spark-1.1 branch (assuming it has all the fixes from RC2 tag. I am now compiling spark-1.0 branch and see if this issue shows up there as well. If it is related to hash/sort based shuffle most likely it won't show up on 1.0. Thanks. Deb Actually +1 from me... This is a recommendAll feature we are testing which is really compute intensive... For ranking metric calculation I was trying to run through the Netflix matrix and generate a ranked list of recommendation for all 17K products and perhaps it needs more compute than what is needed. I was running 6 nodes, 120 cores, 240 GB...It needed to shuffle around 100 GB over 6 nodes... A version with topK runs fine where K = (some multipler on number of movies each user saw and we cross validate on that) Running the following JIRA on Netflix dataset (the dataset is distributed with Jellyfish code http://i.stanford.edu/hazy/victor/Hogwild/), will reproduce the failure... https://issues.apache.org/jira/browse/SPARK-4231 The failed job I will debug more and figure out the real cause. If needed I will open up new JIRAs. Hi, It seems there are multiple places where we would like to compute row similarity (accurate or approximate similarities) Basically through RowMatrix columnSimilarities we can compute column similarities of a tall skinny matrix Similarly we should have an API in RowMatrix called rowSimilarities where we can compute similar rows in a map-reduce fashion. It will be useful for following use-cases: 1. Generate topK users for each user from matrix factorization model 2. Generate topK products for each product from matrix factorization model 3. Generate kernel matrix for use in spectral clustering 4. Generate kernel matrix for use in kernel regression/classification I am not sure if there are already good implementation for map-reduce row similarity that we can use (ideas like fastfood and kitchen sink felt more like for classification use-case but for recommendation also user similarities show up which is unsupervised)... Is there a JIRA tracking it ? If not I can open one and we can discuss further on it. Thanks. Deb I added code to compute topK products for each user and topK user for each product in SPARK-3066.. That is different than row similarity calculation as we need both user and product factors to calculate the topK recommendations.. For (1) and (2) we are trying to answer similarUsers to given a user and similarProducts to a given product.... similarProducts to a given product is straightforward to compute through columnSimilarities/dimsum when products are skinny... similarUser to a given user will need a map-reduce implementation of row similarity since the matrix is tall... I don't see a JIRA for that yet...Are there any good reference for map reduce implementation of row similarity ? Hi, Will it be possible to merge this PR to 1.3 ? https://github.com/apache/spark/pull/3098 The batch prediction API in ALS will be useful for us who want to cross validate on prec@k and MAP... Thanks. Deb Hi, I am bit confused on the mllib design in the master. I thought that core algorithms will stay in mllib and ml will define the pipelines over the core algorithm but looks like in master ALS is moved from mllib to ml... I am refactoring my PR to a factorization package and I want to build it on top of ml.recommendation.ALS (possibly extend from ml.recommendation.ALS since first version will use very similar RDD handling as ALS and a proximal solver that's being added to breeze) https://issues.apache.org/jira/browse/SPARK-2426 https://github.com/scalanlp/breeze/pull/321 Basically I am not sure if we should merge it with recommendation.ALS since this is more generic than recommendation. I am considering calling it ConstrainedALS where user can specify different constraint for user and product factors (Similar to GraphLab CF structure). I am also working on ConstrainedALM where the underlying algorithm is no longer ALS but nonlinear alternating minimization with constraints. https://github.com/scalanlp/breeze/pull/364 This will let us do large rank matrix completion where there is no need to construct gram matrices. I will open up the JIRA soon after getting initial results I am bit confused that where should I add the factorization package. It will use the current ALS test-cases and I have to construct more test-cases for sparse coding and PLSA formulations. Thanks. Deb It will be really help us if we merge it but I guess it is already diverged from the new ALS...I will also take a look at it again and try update with the new ALS... There is a usability difference...I am not sure if recommendation.ALS would like to add both userConstraint and productConstraint ? GraphLab CF for example has it and we are ready to support all the features for modest ranks where gram matrices can be made... For large ranks I am still working on the code You have a JIRA for it... https://issues.apache.org/jira/browse/SPARK-3066 I added the PR on the JIRA... Hi, Some of my jobs failed due to no space left on device and on those jobs I was monitoring the shuffle space...when the job failed shuffle space did not clean and I had to manually clean it... Is there a JIRA already tracking this issue ? If no one has been assigned to it, I can take a look. Thanks. Deb Any reason why the regularization path cannot be implemented using current owlqn pr ? We can change owlqn in breeze to fit your needs... On Feb 24, 2015 3:27 PM, "Joseph Bradley"  wrote: Hi David, We are stress testing breeze.optimize.proximal and nnls...if you are cutting a release now, we will need another release soon once we get the runtime optimizations in place and merged to breeze. Thanks. Deb On Mar 15, 2015 9:39 PM, "David Hall"  wrote: dgemm dgemv and dot come to Breeze and Spark through netlib-java.... Right now both in dot and dgemv Breeze does a extra memory allocate but we already found the issue and we are working on adding a common trait that will provide a sink operation (basically memory will be allocated by user)...adding more BLAS operators in breeze will also help in general as lot more operations are defined over there... Hi Xiangrui, I am facing some minor issues in implementing Alternating Nonlinear Minimization as documented in this JIRA due to the ALS code being in ml package: https://issues.apache.org/jira/browse/SPARK-6323 I need to use Vectors.fromBreeze / Vectors.toBreeze but they are package private on mllib. For now I removed private but not sure that's the correct way... I also need to re-use lot of building blocks from ml.ALS and so I am writing ALM in ml package... I thought the plan was to still write core algorithms in mllib and pipeline integration in ml...It will be great if you can move the ALS object from ml to mllib and that way I can also move ALM to mllib (which I feel is the right place)...Of course the Pipeline based flow will stay in ml package... We can decide later if ALM needs to be in recommendation or a better place is package called factorization but the idea is that ALM will support MAP (and may be KL divergence loss) with sparsity constraints (probability simplex and bounds are fine for what I am focused at right now)... Thanks. Deb Hi, Right now LogisticGradient implements both binary and multi-class in the same class using an if-else statement which is a bit convoluted. For Generalized matrix factorization, if the data has distinct ratings I want to use LeastSquareGradient (regression has given best results to date) but if the data has binary labels 0/1 based on domain knowledge (implicit for example, visits no-visits) I want to use a LogisticGradient without any overhead for multi-class if-else... I can compare the performance of LeastSquareGradient and multi-class LogisticGradient on the recommendation metrics but it will be great if we can separate binary and multi-class in Separate classes....MultiClassLogistic can extend BinaryLogistic but mixing them in the same class is an overhead for users (like me) who wants to use BinaryLogistic for his application.. Thanks. Deb Cool...Thanks...It will be great if they move in two code paths just for the sake of code clean-up For alm I have started experimenting with the following: 1. rmse and map improvement from loglikelihood loss over least square loss. 2. Factorization for datasets that are not ratings (basically improvement over implicit ratings) 3. Sparse topic generation using plsa. We are directly optimizing likelihood under constraints here and so I feel it will improve upon EM algorithm. Also the current LDA does not produce sparse topics and ALM results can augment LDA flow. I am understanding LDA flow to see if the sparsity and loglikelihood optimization can be added there. I will understand more as I see the result. I am not sure if it is supported by public packages like graphlab or scikit but the plsa papers show interesting results. On Mar 30, 2015 2:31 PM, "Xiangrui Meng"  wrote: Hi, We recently added ADMM based proximal algorithm in breeze.optimize.proximal.NonlinearMinimizer which uses a combination of BFGS and proximal algorithms (soft thresholding for L1 for example) to solve large scale constrained optimization problem of form f(x) + g(z). Its usage is similar to current lbfgs flow. We also added a library of commonly used proximal algorithms in breeze. I am experimenting with L1 but I remember there were various admm based PR before. Please feel free to use the solver for your usecases and let us know if it was helpful. The solver is available in mllib master as we already integrated breeze 0.11.2 to mllib. Thanks. Deb I opened it up today but it should help you: https://github.com/apache/spark/pull/6213 Hi, For indexedrowmatrix and rowmatrix, both take RDD(vector)....is it possible that it has intermixed dense and sparse vector....basically I am considering a gemv flow when indexedrowmatrix has dense flag true, dot flow otherwise... Thanks. Deb Hi, I am on last week's master but all the examples that set up the following .set("spark.kryoserializer.buffer", "8m") are failing with the following error: Exception in thread "main" java.lang.IllegalArgumentException: spark.kryoserializer.buffer must be less than 2048 mb, got: + 8192 mb. looks like buffer.mb is deprecated...Is "8m" is not the right syntax to get 8mb kryo buffer or it shuld be "8mb"Thanks. Deb Hi, Is it possible to add GPL/LGPL code on spark packages or it must be licensed under Apache as well ? I want to expose Professor Tim Davis's LGPL library for sparse algebra and ECOS GPL library through the package. Thanks. Deb Tried "8mb"...still I am failing on the same error... Hi, What was the motivation to write power iteration clustering using graphx and not a vector matrix multiplication over similarity matrix represented as say coordinate matrix ? We can use gemv in that flow to block the computation. Over graphx can we do all k eigen vector computation together because I don't see that in a vector matrix multiply flow ? On the other side vector matrix multiply flow is generic for kernel regression or classification flows. Thanks. Deb Yup netlib lgpl right now is activated through a profile...if we can reuse the same idea then csparse can also be added to spark with a lgpl flag. But again as Sean said its tricky. Better to keep it on spark packages for users to try. I am May 3rd commit: commit 49549d5a1a867c3ba25f5e4aec351d4102444bc0 Author: WangTaoTheTonic Date:   Sun May 3 00:47:47 2015 +0100 [SPARK-7031] [THRIFTSERVER] let thrift server take SPARK_DAEMON_MEMORY and SPARK_DAEMON_JAVA_OPTS Ok I thought we tried that and found graphx based flow was faster due to some inherent problem structure (graphx can compute K eigenvectors at the same time) I will report some stats on row similarities experiments on vector blocked index row matrix multiply vs current pic flow... In general for implicit feedback in als you have to do a blocked gram matrix calculation which might not fit in graphx flow and lot of blocked operations can be used...but if your loss is likelihood or kl divergence or just simple sgd update rules and not least square then graphx idea makes sense... Lda flow uses similar idea as the loss function is defined on sparse ratings... Hi, We want to keep the model created and loaded in memory through Spark batch context since blocked matrix operations are required to optimize on runtime. The data is streamed in through Kafka / raw sockets and Spark Streaming Context. We want to run some prediction operations with the streaming data and model loaded in memory through batch context. Do I need to open up a API on top of the batch context or it is possible to use a RDD created by batch context through streaming context ? Most likely not since both streaming context and batch context can't exist in the same spark job but I am curious. If I have to open up an API, does it makes sense to come up with a generic serving api for mllib and let all mllib algorithms expose a serving API ? The API can be spawned using Spark's actor system itself specially since spray is merging to akka-httpx and akka is a dependency in spark already. May be it's not a good idea since it needs maintaining another actor system for the API. Thanks. Deb Hi, I have some impala created parquet tables which hive 0.13.2 can read fine. Now the same table when I want to read using Spark SQL 1.3 I am getting exception class exception that parquet.hive.serde.ParquetHiveSerde not found. I am assuming that hive somewhere is putting the parquet-hive-bundle.jar in hive classpath but I tried putting the parquet-hive-bundle.jar in spark-1.3/conf/hive-site.xml through auxillary jar but even that did not work. Any input on fixing this will be really helpful. Thanks. Deb Congratulations to All. DB great work in bringing quasi newton methods to Spark ! Hi, The demo of end-to-end ML pipeline including the model server component at Spark Summit was really cool. I was wondering if the Model Server component is based upon Velox or it uses a completely different architecture. https://github.com/amplab/velox-modelserver We are looking for an open source version of model server to build upon. Thanks. Deb Hi, I have Impala created table with the following io format and serde: inputFormat:parquet.hive.DeprecatedParquetInputFormat, outputFormat:parquet.hive.DeprecatedParquetOutputFormat, serdeInfo:SerDeInfo(name:null, serializationLib:parquet.hive.serde.ParquetHiveSerDe, parameters:{}) I am trying to read this table on Spark SQL 1.3 and see if caching improves my query latency but I am getting exception: java.lang.ClassNotFoundException: Class parquet.hive.serde.ParquetHiveSerDe not found I understand that in hive 0.13 (which I am using) parquet.hive.serde.ParquetHiveSerDe is deprecated but it seems Impala still used it to write the table. I also tried to provide the bundle jar with --jars option to Spark 1.3 Shell / SQL which has org.apache.parquet.hive.serde.ParquetHiveSerDe but I am confused how to configure to serde in SQLContext ? The table which has the following io format and serde can be read fine by Spark SQL 1.3: inputFormat=org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat, outputFormat=org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat, serializationLib=org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe Thanks. Deb Hi, Akka cluster uses gossip protocol for Master election. The approach in Spark right now is to use Zookeeper for high availability. Interestingly Cassandra and Redis clusters are both using Gossip protocol. I am not sure what is the default behavior right now. If the master dies and zookeeper selects a new master, the whole depedency graph will be re-executed or only the unfinished stages will be restarted ? Also why the zookeeper based HA was preferred in Spark ? I was wondering if there is JIRA to add gossip protocol for Spark Master election ? In the code I see zookeeper, filesystem, custom and default is MonarchyLeader. So looks like Spark is designed to add new leaderElectionAgent. Thanks. Deb Hi, Implicit factorization is important for us since it drives recommendation when modeling user click/no-click and also topic modeling to handle 0 counts in document x word matrices through NMF and Sparse Coding. I am a bit confused on this code: val c1 = alpha * math.abs(rating) if (rating > 0) ls.add(srcFactor, (c1 + 1.0)/c1, c1) When the alpha = 1.0 (high confidence) and rating is > 0 (true for word counts), why this formula does not become same as explicit formula: ls.add(srcFactor, rating, 1.0) For modeling document, I believe implicit Y'Y needs to stay but we need explicit ls.add(srcFactor, rating, 1.0) I am understanding confidence code further. Please let me know if the idea of mapping implicit to handle 0 counts in document word matrix makes sense. Thanks. Deb Yeah, I think the idea of confidence is a bit different than what I am looking for using implicit factorization to do document clustering. I basically need (r_ij - w_ih_j)^2 for all observed ratings and (0 - w_ih_j)^2 for all the unobserved ratings...Think about the document x word matrix where r_ij is the count that's observed, 0 are the word counts that are not in particular document. The broadcasted value of gram matrix w_i'wi or h_j'h_j will also count the r_ij those are observed...So I might be fine using the broadcasted gram matrix and use the linear term as \sum (-r_ijw_i) or \sum (-rijh_j)... I will think further but in the current implicit formulation with confidence, looks like I am really factorizing a 0/1 matrix with weights 1 + alpha*rating for  . It's a bit different from LSA model. I will think further but in the current implicit formulation with confidence, looks like I am factorizing a 0/1 matrix with weights 1 + alpha*rating for observed (1) values and 1 for unobserved (0) values. It's a bit different from LSA model. We got good clustering results from Implicit factorization using alpha = 1.0 since I thought to have a confidence of 1 + rating to observed entries and 1 to unobserved entries. I used positivity / sparse coding basically to force sparsity on document / topic matrix...But then I got confused because I am modifying the real counts from dataset (does not matter much for in practical sense since we really don't have true documents) I mean gram matrix is the key here but then how much weight to give on real counts also matters...I have not yet started looking into perplexity but that will give me further insights... In your experience with using implicit factorization for document clustering, how did you tune alpha ? Using perplexity measures or just something simple like 1 + rating since the ratings are always positive in this case.... Rdd nesting can lead to recursive nesting...i would like to know the usecase and why join can't support it...you can always expose an api over a rdd and access that in another rdd mappartition...use a external data source like hbase cassandra redis to support the api... For ur case group by and then pass the logic...collect each group sample in a seq and then lookup if u r doing one at a time...if doing all try joining it...pattern is common if every key is a iid and you a cross validating a model for each key on 80% train 20% test... We are looking to fit it in pipeline flow...with minor mods it will fit..