I personally with Evan in that I prefer map with getOrElse over fold with
options (but that just my personal preference) :)
On Thu, Dec 26, 2013 at 7:58 PM, Reynold Xin  wrote:
-- 
Cell : 425-233-8271
Could we ship a shell script which downloads the sbt jar if not present
(like for example https://github.com/holdenk/slashem/blob/master/sbt )?
On Sat, Jan 4, 2014 at 12:02 AM, Patrick Wendell  wrote:
-- 
Cell : 425-233-8271
That makes sense, I think we could structure a script in such a way that it
would overcome these problems though and probably provide a fair a mount of
benefit for people who just want to get started quickly.
The easiest would be to have it use the system sbt if present and then fall
back to downloading the sbt jar. As far as stability of the URL goes we
could solve this by either having it point at a domain we control, or just
with an clear error message indicating it failed to download sbt and the
user needs to install sbt.
If a restructured script in that manner would be useful I could whip up a
pull request :)
On Sat, Jan 4, 2014 at 10:56 AM, Patrick Wendell  wrote:
-- 
Cell : 425-233-8271
Sure, unique from MLI-2?
On Thu, Mar 6, 2014 at 2:15 PM, mengxr  wrote:
-- 
Cell : 425-233-8271
+1 (I did some very basic testing with PySpark & Pandas on rc11)
On Tue, May 27, 2014 at 3:53 PM, Mark Hamstra  +1
-- 
Cell : 425-233-8271
Hi Sean,
I've pushed a PR for this https://github.com/apache/spark/pull/2893 :)
Cheers,
Holden :)
On Tue, Oct 21, 2014 at 4:41 AM, Sean Owen  wrote:
-- 
Cell : 425-233-8271
Hi,
Many tests in pyspark are implemented as doctests and the python
unittesting framework is also used for additional tests.
Cheers,
Holden :)
On Wed, Oct 22, 2014 at 4:13 PM, catchmonster  wrote:
-- 
Cell : 425-233-8271
Not exactly but it means someone has come up with what they think a
solution to the problem is and that they've submitted some code for
consideration/review.
On Wednesday, July 8, 2015, Chandrashekhar Kotekar  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Linked In: https://www.linkedin.com/in/holdenkarau
+1 - compiled on ubuntu & centos, spark-perf run against yarn in client
mode on a small cluster comparing 1.4.0 & 1.4.1 (for core) doesn't have any
huge jumps (albeit with a small scaling factor).
On Wed, Jul 8, 2015 at 11:58 PM, Patrick Wendell  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Linked In: https://www.linkedin.com/in/holdenkarau
Has anyone else run into "impossible to get artifacts when data has not
been loaded. IvyNode = org.scala-lang#scala-library;2.10.3" during
hive/update when building with sbt. Working around it is pretty simple
(just add it as a dependency), but I'm wondering if its impacting anyone
else and I should make a PR for it or if its something funky with my local
build setup.
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Linked In: https://www.linkedin.com/in/holdenkarau
Hi Spark Devs,
So this has been brought up a few times before, and generally on the user
list people get directed to use spark-testing-base. I'd like to start
moving some of spark-testing-base's functionality into Spark so that people
don't need a library to do what is (hopefully :p) a very common requirement
across all Spark projects.
To that end I was wondering what peoples thoughts are on where this should
live inside of Spark. I was thinking it could either be a separate testing
project (like sql or similar), or just put the bits to enable testing
inside of each relevant project.
I was also thinking it probably makes sense to only move the unit testing
parts at the start and leave things like integration testing in a testing
project since that could vary depending on the users environment.
What are peoples thoughts?
Cheers,
Holden :)
I'll put together a google doc and send that out (in the meantime a quick
guide of sort of how the current package can be used is in the blog post I
did at
http://blog.cloudera.com/blog/2015/09/making-apache-spark-testing-easy-with-spark-testing-base/
)  If people think its better to keep as a package I am of course happy to
keep doing that. It feels a little strange to have something as core as
being able to test your code live outside.
On Tue, Oct 6, 2015 at 3:44 PM, Patrick Wendell  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Linked In: https://www.linkedin.com/in/holdenkarau
So here is a quick description of the current testing bits (I can expand on
it if people are interested) http://bit.ly/pandaPandaPanda .
On Tue, Oct 6, 2015 at 3:49 PM, Holden Karau  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Linked In: https://www.linkedin.com/in/holdenkarau
I don't think there has been much work done with ScalaJS and Spark (outside
of the April fools press release), but there is a live Web UI project out
of hammerlab with Ryan Williams https://github.com/hammerlab/spree which
you may want to take a look at.
On Mon, Oct 12, 2015 at 2:36 PM, Jakob Odersky  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Linked In: https://www.linkedin.com/in/holdenkarau
Hi YiZhi,
I've been waiting on the shared param to go in (I think it was kmeans) so
we could have a common API. I think the issue is SPARK-7852 but I am on
mobile right now.
Cheers,
Holden :)
On Monday, November 2, 2015, DB Tsai  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Thats correct :)
On Mon, Nov 2, 2015 at 8:04 PM, YiZhi Liu  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
I'd be more than happy to help review the docs if that would be useful :)
On Sat, Dec 5, 2015 at 2:21 PM, Joseph Bradley  Thanks for reporting this!  I just added a JIRA:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
I've run into some problems with the Python tests in the past when I
haven't built with hive support, you might want to build your assembly with
hive support and see if that helps.
On Thursday, February 18, 2016, Jason White  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Great - I'll update the wiki.
On Thu, Feb 18, 2016 at 8:34 PM, Jason White  Compiling with `build/mvn -Pyarn -Phadoop-2.4 -Phive -Dhadoop.version=2.4.0
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Any chance I could also get write access to the wiki? I'd like to update
some of the PySpark documentation in the wiki.
On Tue, Jan 12, 2016 at 10:14 AM, shane knapp  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Or wait I don't have access to the wiki - if anyone can give me wiki access
I'll update the instructions.
On Thu, Feb 18, 2016 at 8:45 PM, Holden Karau  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
One minor downside to having both 2.10 and 2.11 (and eventually 2.12) is
deprecation warnings in our builds that we can't fix without introducing a
wrapper/ scala version specific code. This isn't a big deal, and if we drop
2.10 in the 3-6 month time frame talked about we can cleanup those warnings
once we get there.
On Fri, Apr 1, 2016 at 10:00 PM, Raymond Honderdors  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Personally I'd rather err on the side of keeping PRs open, but I understand
wanting to keep the open PRs limited to ones which have a reasonable chance
of being merged.
What about if we filtered for non-mergeable PRs or instead left a comment
asking the author to respond if they are still available to move the PR
forward - and close the ones where they don't respond for a week?
Just a suggestion.
On Monday, April 18, 2016, Ted Yu  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
They are different, also this might be better suited for the user list.
Persist by default will cache in memory on one machine, although you can
specify a different storage level. Checkpoint on the other hand will write
out to a persistent store and get rid of the dependency graph used to
compute the RDD (so it is often seen in iterative algorithms which may
build very large or complex dependency graphs over time).
On Saturday, April 30, 2016, Renyi Xiong  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
I've been doing some looking at EclairJS (Spark + Javascript) which takes a
really interesting approach. The driver program is run in node and the
workers are run in nashorn. I was wondering if anyone has given much though
to optionally exposing an interface for PySpark in a similar fashion. For
some UDFs and UDAFs we could keep the data entirely in the JVM, and still
go back to our old PipelinedRDD based interface for operations which
require native libraries or otherwise aren't supported in Jython. Have I
had too much coffee and this is actually a bad idea or is this something
people think would be worth investigating some?
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
+1 non-binding (as a contributor anything which speed things up is worth a
try, and git blame is a good enough substitute for the list when figuring
out who to ping on a PR).
On Monday, May 23, 2016, Imran Rashid  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Generally this means numpy isn't installed on the system or your PYTHONPATH
has somehow gotten pointed somewhere odd,
On Wed, Jun 1, 2016 at 8:31 AM, Bhupendra Mishra  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
I think your error could possibly be different - looking at the original
JIRA the issue was happening on HDFS and you seem to be experiencing the
issue on s3n, and while I don't have full view of the problem I could see
this being s3 specific (read-after-write on s3 is trickier than
read-after-write on HDFS).
On Thursday, June 9, 2016, Sunil Kumar  Hi,
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
I'd do some searching and see if there is a JIRA related to this problem
on s3 and if you don't find one go ahead and make one. Even if it is an
intrinsic problem with s3 (and I'm not super sure since I'm just reading
this on mobile) - it would maybe be a good thing for us to document.
On Thursday, June 9, 2016, Sunil Kumar  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
PySpark RDDs are (on the Java side) are essentially RDD of pickled objects
and mostly (but not entirely) opaque to the JVM. It is possible (by using
some internals) to pass a PySpark DataFrame to a Scala library (you may or
may not find the talk I gave at Spark Summit useful
https://www.youtube.com/watch?v=V6DkTVvy9vk as well as some of the Python
examples in
https://github.com/high-performance-spark/high-performance-spark-examples
). Good luck! :)
On Wed, Jun 22, 2016 at 7:07 PM, Daniel Imberman  Hi All,
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Looking at the Sink in 2.0 there is a warning (added in SPARK-16020 without
a lot of details) that says "Note: You cannot apply any operators on `data`
except consuming it (e.g., `collect/foreach`)." but I'm wondering if this
restriction is perhaps too broadly worded? Provided that we consume the
data in a blocking fashion could we apply some other transformation
beforehand? Or is there a better way to get equivalent foreachRDD
functionality with the structured streaming API?
On somewhat of tangent - would it maybe make sense to mark transformations
on Datasets which are not supported for Streaming use (e.g. toJson etc.)?
Cheers,
Holden :)
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Ok, that makes sense (the JIRA where the restriction note was added didn't
have a lot of details). So for now, would converting to an RDD inside of a
custom Sink and then doing your operations on that be a reasonable work
around?
On Tuesday, June 28, 2016, Michael Armbrust  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
2.0.1 just means that the fix will be included in 2.0.1 (eg its not in the
current 2.0.0 RC).
On Saturday, July 2, 2016, Jacek Laskowski  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Hi PySpark Devs,
The Py4j developer has a survey up for Py4J users -
https://github.com/bartdag/py4j/issues/237 it might be worth our time to
provide some input on how we are using and would like to be using Py4J if
binary transfer was improved. I'm happy to fill it out with my thoughts -
but if other people are interested too maybe we could work on a response
together?
Cheers,
Holden :)
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
There are also the spark-perf and spark-sql-perf projects in the Databricks
github (although I see an open issue for Spark 2.0 support in one of them).
On Friday, July 8, 2016, Ted Yu  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
This has also been reported on the user@ by a few people - other apache
projects (arrow & hadoop) don't seem to be affected so maybe it was a just
bad update for the Spark website?
On Wed, Jul 13, 2016 at 12:05 PM, Dongjoon Hyun  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
-1 : The docs don't seem to be fully built (e.g.
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc4-docs/streaming-programming-guide.html
is a zero byte file currently) - although if this is a transient apache
issue no worries.
On Thu, Jul 14, 2016 at 11:59 AM, Reynold Xin  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Ah in that case: 0
On Tue, Jul 19, 2016 at 3:26 PM, Jonathan Kelly  The docs link from Reynold's initial email is apparently no longer valid.
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
+1 (non-binding)
Built locally on Ubuntu 14.04, basic pyspark sanity checking & tested with
a simple structured streaming project (spark-structured-streaming-ml) &
spark-testing-base & high-performance-spark-examples (minor changes
required from preview version but seem intentional & jetty conflicts with
out of date testing library - but not a Spark problem).
On Fri, Jul 22, 2016 at 12:45 PM, Luciano Resende  + 1 (non-binding)
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Now that the 2.0 release is out the door and I've got some cycles to do
some cleanups -  I'd like to know what other people think of the internal
deprecation warnings we've introduced in a lot of a places in our code.
Once before I did some minor refactoring so the Python code which had to
use the deprecated code to expose the deprecated API wouldn't gum up the
build logs - but is there interest in doing that or are we more interested
in not paying attention to the deprecation warnings for internal Spark
components (e.g.
https://twitter.com/thepracticaldev/status/725769766603001856 )?
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Great it sounds like is a general consensus that these are worth fixing so
I'll start tackling some of these so we can hopefully get down to the point
that we will notice new warnings and be able to consider them.
On Wed, Jul 27, 2016 at 4:11 PM, Sean Owen  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Hi Neil,
Thanks for your interest in participating in Apache Spark! You can create
JIRAs - but first you will need to signup for an Apache JIRA account.
Generally we can't assign JIRAs to ourselves - but you can leave a comment
saying your interested in working. I think for R a good place to get
started is possibly plumbing through existing functionality from Spark -
but thats based on my work in PySpark so this might not actually be the
best place for Spark R. If you havesn't already looked at
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark its
a good place to start.
Cheers,
Holden :)
On Wed, Jul 27, 2016 at 8:32 PM, Neil Chang  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
I believe it was intentional with the idea that it would be more unified
between Java and Scala APIs. If your talking about the javadoc mention in
https://github.com/apache/spark/pull/14466/files - I believe the += is
meant to refer to what the internal implementation of the add function can
be for someone extending the accumulator (but it certainly could cause
confusion).
Reynold can provide a more definitive answer in this case.
On Tue, Aug 2, 2016 at 1:46 PM, Bryan Cutler  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Ah in that case the programming guides text is still talking about the
deprecated accumulator API despite having an updated code sample (the way
it suggests making an accumulator is also deprecated). I think the fix is
updating the programming guide rather than adding += to the API.
On Wednesday, August 3, 2016, Bryan Cutler  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Spark does not currently support Apache Arrow - probably a good place to
chat would be on the Arrow mailing list where they are making progress
towards unified JVM & Python/R support which is sort of a precondition of a
functioning Arrow interface between Spark and Python.
On Fri, Aug 5, 2016 at 12:40 PM, jpivarski@gmail.com  In a few earlier posts [ 1
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
I don't think there is an approximate timescale right now and its likely
any implementation would depend on a solid Java implementation of Arrow
being ready first (or even a guarantee that it necessarily will - although
I'm interested in making it happen in some places where it makes sense).
On Fri, Aug 5, 2016 at 2:18 PM, Jim Pivarski  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Hi Everyone (that cares about structured streaming and ML),
Seth and I have been giving some thought to support structured streaming in
machine learning - we've put together an early design doc (its been in JIRA
(SPARK-16424)  for
awhile, but incase you missed it) we'd love your comments.
Also if anyone happens to be in San Francisco this week and would like to
chat over coffee or bubble tea (I know some people on the list already get
bobba tea as a perk so in that case we can get pastries) feel free to reach
out to me on or off-list (
https://docs.google.com/document/d/1snh7x7b0dQIlTsJNHLr-IxIFgP43RfRV271YK2qGiFQ/edit?usp=sharing
) and we will update the list with the result of any such discussion.
Cheers,
Holden :)
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Congratulations :D :) Yay!
On Tue, Oct 4, 2016 at 11:14 AM, Suresh Thalamati  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Hi Python Spark Developers & Users,
As Datasets/DataFrames are becoming the core building block of Spark, and
as someone who cares about Python Spark performance, I've been looking more
at PySpark UDF performance.
I've got an early WIP/request for comments pull request open
 with a corresponding design
document
and
JIRA (SPARK-15369)  that
allows for selective UDF evaluation in Jython . Now
that Spark 2.0.1 is out I'd really love peoples input or feedback on this
proposal so I can circle back with a more complete PR :) I'd love to hear
from people using PySpark if this is something which looks interesting (as
well as the PySpark developers) for some of the open questions :)
For users: If you have simple Python UDFs (or even better UDFs and
datasets) that you can share for bench-marking it would be really useful to
be able to add them to the bench-marking I've been looking at in the design
doc. It would also be useful to know if some, many, or none, of your UDFs
can be evaluated by Jython. If you have UDF you aren't comfortable sharing
on-list feel free to each out to me directly.
Some general open questions:
1) The draft PR does some magic** to allow being passed in functions at
least some of the time - is that something which people are interested in
or would it be better to leave the magic out and just require a string
representing the lambda be passed in?
2) Would it be useful to provide easy steps to use JyNI 
 (its LGPL licensed  so I
don't think we we can include it out of the bo
x - but we could try
and make it easy for users to link with if its important)?
3) While we have a 2x speedup for tokenization/wordcount (getting close to
native scala perf) - what is performance like for other workloads (please
share your desired UDFs/workloads for my evil bench-marking plans)?
4) What does the eventual Dataset API look like for Python? (This could
partially influence #1)?
5) How important it is to not add the Jython dependencies to the weight for
non-Python users (and if desired which work around to chose - maybe
something like spark-hive?)
6) Do you often chain PySpark UDF operations and is that something we
should try and optimize for in Jython as well?
7) How many of your Python UDFs can / can not be evaluated in Jython for
one reason or another?
8) Do your UDFs depend on Spark accumulators or broadcast values?
9) What am I forgetting in my coffee fueled happiness?
Cheers,
Holden :)
*Bench-marking has been very limited 2~3X improvement likely different for
"real" work loads (unless you really like doing wordcount :p :))
** Note: magic depends on dill .
P.S.
I leave you with this optimistic 80s style intro screen
 :)
Also if anyone happens to be going to PyData DC 
this weekend I'd love to chat with you in person about this (and of course
circle it back to the mailing list).
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
We could certainly do that system - but given the current somewhat small
set of active committers its clearly not scaling very well. There are many
developers  in Spark like Hyukjin, Cody, and myself who care about specific
areas and can verify if an issue is still present in mainline.
That being said if the general view is that only committers should resolve
JIRAs I'm happy to back off and leave that to the current committers (or we
could try ping them to close issues which I think are resolved instead of
closing them myself but given how many pings I sometimes have to make to
get an issue looked at I'm hesitant to suggest this system).
I'll hold off on my JIRA review for a bit while we get this sorted :)
On Sat, Oct 8, 2016 at 7:47 AM, Ted Yu  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
I think it is really important to ensure that someone with a good
understanding of Kafka is empowered around this component with a formal
voice around - but I don't have much dev experience with our Kafka
connectors so I can't speak to the specifics around it personally.
More generally, I also feel pretty strongly about commit bits, and while
I've been going back through the old Python JIRAs and PRs it's seems we are
leaving some good stuff out just because of reviewer bandwidth (not to
mention the people that get turned away from contributing more after their
first interaction or lack their of). Certainly the Python reviewer(s) knows
their stuff - but it feels like for Python there just isn't enough
committer time available to handle the contributor interest. Although - to
be fair - this may be one of those cases where as we add more committers we
will have more contributors never having enough time, but I see that as a
positive cycle we should embrace.
I'm curious - are developers working more in other components feeling
similarly? I've sort of assumed so personally - but it would be nice to
hear others experiences as well.
Of course my disclaimer from the original conversation applies
- I do very much "have a horse in the race" so I will avoid proposing new
criteria. I working on Spark is a core part of what I do most days, and
once my day job with Spark is done I go and do even more Spark like working
on a new Spark book focused on performance right now - and I very much do
want to see a healthy community flourish around Spark :)
More thoughts in-line:
On Sat, Oct 8, 2016 at 5:03 PM, Cody Koeninger  wrote:
I'm happy to hear this is something being actively discussed by the PMC.
I'm also glad the PMC took the time to create some documentation around
what it takes to be a committer - but, to me, it seems like there are maybe
some additional requirements or nuances to the requirements/process which
haven't quite been fully captured in the current wiki and I look forward to
seeing the result of the conversation and the clarity or changes it can
bring to the process.
I realize the default for the PMC may be to have the conversation around
this on private@ - but I think the dev (and maybe even user) community as a
whole is rather interested and we all could benefit by working together on
this (or at least being aware of the PMCs thoughts around this).With the
decisions and discussions around the committer process happen on the
private mailing list (or in person) its really difficult as an outsider (or
contributor interested in being a committer) feel that one has a good
understanding of what is going on. Sean Owen and Matei each provided some
insight from their points of view in Cody's initial thread
along with some additional thoughts in this thread by Matei, but I'd really
love to hear more (from both of them as well as the rest of the PMC). I
also think it would be useful to hear from people with experience in other
projects with what their best practices are around similar processes and
doing this (or parts of it) on the dev@ or user@ list will be able to
provide a wider variety of experiences to share as the PMC considers the
best approach.
Of course I completely understand and respect the PMCs choice around which
parts of the conversation belong where, I'd just like to encourage a
default to slightly more open if possible.
P.S.
I want to thank everyone who has taken the time to read and respond, its
really good that we as a community are able to have these sometimes
difficult discussions and try and grow from them.
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
This is a thing I often have people ask me about, and then I do my best
dissuade them from using Spark in the "hot path" and it's normally
something which most people eventually accept. Fred might have more
information for people for whom this is a hard requirement though.
On Thursday, October 13, 2016, Cody Koeninger  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Hi Krishna,
Thanks for your interest contributing to PySpark! I don't personally use
either of those IDEs so I'll leave that part for someone else to answer -
but in general you can find the building spark documentation at
http://spark.apache.org/docs/latest/building-spark.html which includes
notes on how to run the Python tests as well. You will also probably want
to check out the contributing to Spark guide at
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark.
Cheers,
Holden :)
On Tue, Oct 18, 2016 at 2:16 AM, Krishna Kalyan  Hello,
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Right now the wiki isn't particularly accessible to updates by external
contributors. We've already got a contributing to spark page which just
links to the wiki - how about if we just move the wiki contents over? This
way contributors can contribute to our documentation about how to
contribute probably helping clear up points of confusion for new
contributors which the rest of us may be blind to.
If we do this we would probably want to update the wiki page to point to
the documentation generated from markdown. It would also mean that the
results of any update to the contributing guide take a full release cycle
to be visible. Another alternative would be opening up the wiki to a
broader set of people.
I know a lot of people are probably getting ready for Spark Summit EU (and
I hope to catch up with some of y'all there) but I figured this a
relatively minor proposal.
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
I think what Reynold means is that if its easy for a developer to build
this convenience function using the current Spark API it probably doesn't
need to go into Spark unless its being done to provide a similar API to a
system we are attempting to be semi-compatible with (e.g. if a
corresponding convenience function existed in the pandas API).
On Tue, Oct 18, 2016 at 7:03 AM, roehst  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
I'd also like to add Python 2.6 to the list of things. We've considered
dropping it before but never followed through to the best of my knowledge
(although on mobile right now so can't double check).
On Tuesday, October 25, 2016, Sean Owen  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
I believe Bryan is also working on this a little - and I'm a little busy
with the other stuff but would love to stay in the loop on Arrow progress :)
On Monday, October 31, 2016, mariusvniekerk  So i've been working on some very very early stage apache arrow
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
On that note there is some discussion on the Jira -
https://issues.apache.org/jira/browse/SPARK-13534 :)
On Mon, Oct 31, 2016 at 8:32 PM, Holden Karau  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Hi Spark Developers & Maintainers,
I know we've been talking a lot about what we want changes we want in
PySpark to help keep it interesting and usable (see
http://apache-spark-developers-list.1001551.n3.nabble.com/Python-Spark-Improvements-forked-from-Spark-Improvement-Proposals-td19422.html).
One of the underlying challenges that we haven't explicitly discussed is
that a reason behind the slow pace of a lot of the PySpark development is
the lack of dedicated Python reviewers.
For changes which are based around parity with an existing component,
Python contributors like myself can sometimes get reviewers from the
component (like ML) to take a look at our Python changes - but for core
changes it's even harder to get reviewers.
The general Python PR review dashboard
 shows the a number of PRs
languishing - but to specifically call out a few:
   -
   pip installability - https://github.com/apache/spark/pull/15659
   -
   KMeans summary in Python - https://github.com/apache/spark/pull/13557
   -
   The various Anaconda/Virtualenv support PRs (none of them have had any
   luck with committer bandwidth)
   -
   PySpark ML models should have params finally starting to get committer
   review - but blocked for months (
   https://github.com/apache/spark/pull/14653 )
   -
   Python meta algorithms in Scala -
   https://github.com/apache/spark/pull/13794 (out of sync with master but
   waiting for months for a committer to say if they are interested in the
   feature or not)
For those following a lot of Python JIRAs you also probably noticed a lot
of Python related JIRAs being re-targeted for future versions that keep
getting bumped back.
The lack of core Python reviewers will make things like Arrow integration
difficult to achieve unless the situation changes.
This isn't meant to say that the current Python reviewers aren't good -
there just isn't enough Python committer bandwidth available to move these
things forward. The normal solution to this is adding more committers with
that focus area.
I'd love to hear y'alls thoughts on this.
Cheers,
Holden :)
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
So according the documentation it mostly uses blame lines which _might_ not
be the best fit for Spark (since many of the people in the blame lines
aren't going to have permission to commit the code). (Although it's
possible that the algorithm that is actually used does more than the one
described in the documentation).
I'd love to know what peoples experiences of using this with other projects
with a similar structure as Spark (small set of committers to a large set
of contributors).
Even with that reservation it could be interesting to try out since this is
a really big problem for new contributors looking to participate in Spark
and the spark-prs dashboard doesn't seem to be catching everything.
On Sun, Nov 6, 2016 at 6:25 PM, Nicholas Chammas  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
+1 it seems like I'm missing a number of my GitHub email notifications
lately (although since I run my own mail server and forward I've been
assuming it's my own fault).
I've also had issues with having greatly delayed notifications on some of
my own pull requests but that might be unrelated.
On Thu, Nov 17, 2016 at 8:20 AM Reynold Xin  wrote:
That's awesome thanks for doing the migration :)
On Wed, Nov 23, 2016 at 3:29 AM Sean Owen  wrote:
Thanks for the specific mention of the new PySpark packaging Shivaram,
For *nix (Linux, Unix, OS X, etc.) Python users interested in helping test
the new artifacts you can do as follows:
Setup PySpark with pip by:
1. Download the artifact from
http://home.apache.org/~pwendell/spark-releases/spark-2.1.0-rc5-bin/pyspark-2.1.0+hadoop2.7.tar.gz
2. (Optional): Create a virtual env (e.g. virtualenv /tmp/pysparktest;
source /tmp/pysparktest/bin/activate)
3. (Possibly required depending on pip version): Upgrade pip to a recent
version (e.g. pip install --upgrade pip)
3. Install the package with pip install pyspark-2.1.0+hadoop2.7.tar.gz
4. If you have SPARK_HOME set to any specific path unset it to force the
pip installed pyspark to run with its provided jars
In the future we hope to publish to PyPI allowing you to skip the download
step, but there just wasn't a chance to get that part included for this
release. If everything goes smoothly hopefully we can add that soon (see
SPARK-18128 ) :)
Some things to verify:
1) Verify you can start the PySpark shell (e.g. run pyspark)
2) Verify you can start PySpark from python (e.g. run python, verify you
can import pyspark and construct a SparkContext).
3) Verify you PySpark programs works with pip installed PySpark as well as
regular spark (e.g. spark-submit my-workload.py)
4) Have a different version of Spark downloaded locally as well? Verify
that launches and runs correctly & pip installed PySpark is not taking
precedence (make sure to use the fully qualified path when executing).
Some things that are explicitly not supported in pip installed PySpark:
1) Starting a new standalone cluster with pip installed PySpark (connecting
to an existing standalone cluster is expected to work)
2) non-Python Spark interfaces (e.g. don't pip install pypsark for SparkR,
use the SparkR packaging instead :)).
3) PyPi - if things go well coming in a future release (track the progress
on https://issues.apache.org/jira/browse/SPARK-18128)
4) Python versions prior to 2.7
5) Full Windows support - later follow up task (if your interested in this
please chat with me or see https://issues.apache.org/jira/browse/SPARK-18136
)
Post verification cleanup:
1. Uninstall the pip installed PySpark since it is just an RC and you don't
want it getting in the way later (e.g. pip uninstall pypsark-2.1.0 )
2 (Optional). deactivate your pip environment
If anyone has any questions about the new PySpark packaging I'm more than
happy to chat :)
Cheers,
Holden :)
On Thu, Dec 15, 2016 at 9:44 PM, Reynold Xin  wrote:
-- 
Twitter: https://twitter.com/holdenkarau
Hi Gilad,
Spark uses the sample standard variance inside of the StandardScaler (see
https://spark.apache.org/docs/2.0.2/api/scala/index.html#org.apache.spark.mllib.feature.StandardScaler
) which I think would explain the results you are seeing you are seeing. I
believe the scalers are intended to be used on larger sized datasets You
can verify this yourself doing the same computation in Python and see the
scaling using the sample deviation result in the values you are seeing from
Spark.
Cheers,
Holden :)
On Sun, Jan 8, 2017 at 12:06 PM, Gilad Barkan  Hi
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Hi Georg,
Thanks for the question along with the code (as well as posting to stack
overflow). In general if a question is well suited for stackoverflow its
probably better suited to the user@ list instead of the dev@ list so I've
cc'd the user@ list for you.
As far as handling empty partitions when working mapPartitions (and
similar), the general approach is to return an empty iterator of the
correct type when you have an empty input iterator.
It looks like your code is doing this, however it seems like you likely
have a bug in your application logic (namely it assumes that if a partition
has a record missing a value it will either have had a previous row in the
same partition which is good OR that the previous partition is not empty
and has a good row - which need not necessarily be the case). You've
partially fixed this problem by going through and for each partition
collecting the last previous good value, and then if you don't have a good
value at the start of a partition look up the value in the collected array.
However, if this also happens at the same time the previous partition is
empty, you will need to go and lookup the previous previous partition value
until you find the one you are looking for. (Note this assumes that the
first record in your dataset is valid, if it isn't your code will still
fail).
Your solution is really close to working but just has some minor
assumptions which don't always necessarily hold.
Cheers,
Holden :)
On Sun, Jan 8, 2017 at 8:30 PM, Liang-Chi Hsieh  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
If you want to check if it's your modifications or just in mainline, you
can always just checkout mainline or stash your current changes to rebuild
(this is something I do pretty often when I run into bugs I don't think I
would have introduced).
On Mon, Jan 9, 2017 at 1:01 AM Liang-Chi Hsieh  wrote:
I'd be happy to help with reviewing Python test improvements. Maybe make an
umbrella JIRA and do one sub components at a time?
On Thu, Jan 12, 2017 at 12:20 PM Saikat Kanjilal  Following up, any thoughts on next steps for this?
Also thanks everyone :) Looking forward to helping out (and if anyone wants
to get started contributing to PySpark please ping me :))
On Tue, Jan 24, 2017 at 3:24 PM, Burak Yavuz  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
I'm in mobile right now but there is a JIRA to add it to the models first
and on that JIRA people are discussing single element transform as a
possibility -
https://issues.apache.org/jira/plugins/servlet/mobile#issue/SPARK-10413
There might be others as well that just aren't as fresh in my memory.
On Mon, Feb 6, 2017 at 8:46 AM Aseem Bansal  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Congratulations Takuya-san :D!
On Mon, Feb 13, 2017 at 11:16 AM, Reynold Xin  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Hi PySpark Developers,
Cloudpickle is a core part of PySpark, and is originally copied from (and
improved from) picloud. Since then other projects have found cloudpickle
useful and a fork of cloudpickle  was
created and is now maintained as its own library
 (with better test coverage and
resulting bug fixes I understand). We've had a few PRs backporting fixes
from the cloudpickle project into Spark's local copy of cloudpickle - how
would people feel about moving to taking an explicit (pinned) dependency on
cloudpickle?
We could add cloudpickle to the setup.py and a requirements.txt file for
users who prefer not to do a system installation of PySpark.
Py4J is maybe even a simpler case, we currently have a zip of py4j in our
repo but could instead have a pinned version required. While we do depend
on a lot of py4j internal APIs, version pinning should be sufficient to
ensure functionality (and simplify the update process).
Cheers,
Holden :)
-- 
Twitter: https://twitter.com/holdenkarau
It's a good question. Py4J seems to have been updated 5 times in 2016 and
is a bit involved (from a review point of view verifying the zip file
contents is somewhat tedious).
cloudpickle is a bit difficult to tell since we can have changes to
cloudpickle which aren't correctly tagged as backporting changes from the
fork (and this can take awhile to review since we don't always catch them
right away as being backports).
Another difficulty with looking at backports is that since our review
process for PySpark has historically been on the slow side, changes
benefiting systems like dask or IPython parallel were not backported to
Spark unless they caused serious errors.
I think the key benefits are better test coverage of the forked version of
cloudpickle, using a more standardized packaging of dependencies, simpler
updates of dependencies reduces friction to gaining benefits from other
related projects work - Python serialization really isn't our secret sauce.
If I'm missing any substantial benefits or costs I'd love to know :)
On Mon, Feb 13, 2017 at 3:03 PM, Reynold Xin  wrote:
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
It's at the bottom of every message (although some mail clients hide it for
some reason), send an email to dev-unsubscribe@spark.apache.org
On Sat, Feb 18, 2017 at 11:07 AM Pritish Nawlakhe  wrote:
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Hi Spark Devs,
Spark 2.1 has been out since end of December
and we've got quite a few fixes merged for 2.1.1
.
On the Python side one of the things I'd like to see us get out into a
patch release is a packaging fix (now merged) before we upload to PyPI &
Conda, and we also have the normal batch of fixes like toLocalIterator for
large DataFrames in PySpark.
I've chatted with Felix & Shivaram who seem to think the R side is looking
close to in good shape for a 2.1.1 release to submit to CRAN (if I've
miss-spoken my apologies). The two outstanding issues that are being
tracked for R are SPARK-18817, SPARK-19237.
Looking at the other components quickly it seems like structured streaming
could also benefit from a patch release.
What do others think - are there any issues people are actively targeting
for 2.1.1? Is this too early to be considering a patch release?
Cheers,
Holden
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
I'd be happy to do the work of coordinating a 2.1.1 release if that's a
thing a committer can do (I think the release coordinator for the most
recent Arrow release was a committer and the final publish step took a PMC
member to upload but other than that I don't remember any issues).
On Mon, Mar 13, 2017 at 1:05 PM Sean Owen  wrote:
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
