The terms worker and slave seem to be used interchangeably.  Are they the
same?
Worker is used more frequently in the codebase:
aash@aash-mbp ~/git/spark$ git grep -i worker | wc -l
     981
aash@aash-mbp ~/git/spark$ git grep -i slave | wc -l
     348
aash@aash-mbp ~/git/spark$
Does it make sense to unify on one or the other?
That matches my mental model -- glad to hear we're converging on one though
for consistency.
Thanks guys!
On Fri, Jan 3, 2014 at 1:45 AM, Patrick Wendell  wrote:
+1 on bundling a script similar to that one
On Sat, Jan 4, 2014 at 4:48 AM, Holden Karau  wrote:
Can you paste the exception you're seeing?
Sent from my mobile phone
On Jan 24, 2014 2:36 PM, "Kapil Malik"  wrote:
Agree on timeboxed releases as well.
Is there a vision for where we want to be as a project before declaring the
first 1.0 release?  While we're in the 0.x days per semver we can break
backcompat at will (though we try to avoid it where possible), and that
luxury goes away with 1.x  I just don't want to release a 1.0 simply
because it seems to follow after 0.9 rather than making an intentional
decision that we're at the point where we can stand by the current APIs and
binary compatibility for the next year or so of the major release.
Until that decision is made as a group I'd rather we do an immediate
version bump to 0.10.0-SNAPSHOT and then if discussion warrants it later,
replace that with 1.0.0-SNAPSHOT.  It's very easy to go from 0.10 to 1.0
but not the other way around.
https://github.com/apache/incubator-spark/pull/542
Cheers!
Andrew
On Wed, Feb 5, 2014 at 9:49 PM, Heiko Braun  +1 on time boxed releases and compatibility guidelines
Hi Spark devs,
Occasionally when hitting Ctrl-C in the scala spark shell on 0.9.0 one of
my workers goes dead in the spark master UI.  I'm using the standalone
cluster and didn't ever see this while using 0.8.0 so I think it may be a
regression.
When I prod on the hung CoarseGrainedExecutorBackend JVM with jstack and
jmap -heap, it doesn't respond unless I add the -F force flag.  The heap
isn't full, but there are some interesting bits in the jstack.  Poking
around a little, I think there may be some kind of deadlock in the shutdown
hooks.
Below are the threads I think are most interesting:
Thread 14308: (state = BLOCKED)
 - java.lang.Shutdown.exit(int) @bci=96, line=212 (Interpreted frame)
 - java.lang.Runtime.exit(int) @bci=14, line=109 (Interpreted frame)
 - java.lang.System.exit(int) @bci=4, line=962 (Interpreted frame)
 -
org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$receive$1.applyOrElse(java.lang.Object,
scala.Function1) @bci=352, line=81 (Interpreted frame)
 - akka.actor.ActorCell.receiveMessage(java.lang.Object) @bci=25, line=498
(Interpreted frame)
 - akka.actor.ActorCell.invoke(akka.dispatch.Envelope) @bci=39, line=456
(Interpreted frame)
 - akka.dispatch.Mailbox.processMailbox(int, long) @bci=24, line=237
(Interpreted frame)
 - akka.dispatch.Mailbox.run() @bci=20, line=219 (Interpreted frame)
 - akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec()
@bci=4, line=386 (Interpreted frame)
 - scala.concurrent.forkjoin.ForkJoinTask.doExec() @bci=10, line=260
(Compiled frame)
 -
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(scala.concurrent.forkjoin.ForkJoinTask)
@bci=10, line=1339 (Compiled frame)
 -
scala.concurrent.forkjoin.ForkJoinPool.runWorker(scala.concurrent.forkjoin.ForkJoinPool$WorkQueue)
@bci=11, line=1979 (Compiled frame)
 - scala.concurrent.forkjoin.ForkJoinWorkerThread.run() @bci=14, line=107
(Interpreted frame)
Thread 3865: (state = BLOCKED)
 - java.lang.Object.wait(long) @bci=0 (Interpreted frame)
 - java.lang.Thread.join(long) @bci=38, line=1280 (Interpreted frame)
 - java.lang.Thread.join() @bci=2, line=1354 (Interpreted frame)
 - java.lang.ApplicationShutdownHooks.runHooks() @bci=87, line=106
(Interpreted frame)
 - java.lang.ApplicationShutdownHooks$1.run() @bci=0, line=46 (Interpreted
frame)
 - java.lang.Shutdown.runHooks() @bci=39, line=123 (Interpreted frame)
 - java.lang.Shutdown.sequence() @bci=26, line=167 (Interpreted frame)
 - java.lang.Shutdown.exit(int) @bci=96, line=212 (Interpreted frame)
 - java.lang.Terminator$1.handle(sun.misc.Signal) @bci=8, line=52
(Interpreted frame)
 - sun.misc.Signal$1.run() @bci=8, line=212 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=744 (Interpreted frame)
Thread 3987: (state = BLOCKED)
 - java.io.UnixFileSystem.list(java.io.File) @bci=0 (Interpreted frame)
 - java.io.File.list() @bci=29, line=1116 (Interpreted frame)
 - java.io.File.listFiles() @bci=1, line=1201 (Compiled frame)
 - org.apache.spark.util.Utils$.listFilesSafely(java.io.File) @bci=1,
line=466 (Interpreted frame)
 - org.apache.spark.util.Utils$.deleteRecursively(java.io.File) @bci=9,
line=478 (Compiled frame)
 -
org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(java.io.File)
@bci=4, line=479 (Compiled frame)
 -
org.apache.spark.util.Utils$$anonfun$deleteRecursively$1.apply(java.lang.Object)
@bci=5, line=478 (Compiled frame)
 -
scala.collection.IndexedSeqOptimized$class.foreach(scala.collection.IndexedSeqOptimized,
scala.Function1) @bci=22, line=33 (Compiled frame)
 - scala.collection.mutable.WrappedArray.foreach(scala.Function1) @bci=2,
line=34 (Compiled frame)
 - org.apache.spark.util.Utils$.deleteRecursively(java.io.File) @bci=19,
line=478 (Interpreted frame)
 -
org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$2.apply(java.io.File)
@bci=14, line=141 (Interpreted frame)
 -
org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$2.apply(java.lang.Object)
@bci=5, line=139 (Interpreted frame)
 -
scala.collection.IndexedSeqOptimized$class.foreach(scala.collection.IndexedSeqOptimized,
scala.Function1) @bci=22, line=33 (Compiled frame)
 - scala.collection.mutable.ArrayOps$ofRef.foreach(scala.Function1) @bci=2,
line=108 (Interpreted frame)
 - org.apache.spark.storage.DiskBlockManager$$anon$1.run() @bci=39,
line=139 (Interpreted frame)
I think what happened here is that thread 14308 received the akka
"shutdown" message and called System.exit().  This started thread 3865,
which is the JVM shutting itself down.  Part of that process is running the
shutdown hooks, so it started thread 3987.  That thread is the shutdown
hook from addShutdownHook() in DiskBlockManager.scala, which looks like
this:
  private def addShutdownHook() {
    localDirs.foreach(localDir => Utils.registerShutdownDeleteDir(localDir))
    Runtime.getRuntime.addShutdownHook(new Thread("delete Spark local
dirs") {
      override def run() {
        logDebug("Shutdown hook called")
        localDirs.foreach { localDir =>
          try {
            if (!Utils.hasRootAsShutdownDeleteDir(localDir))
Utils.deleteRecursively(localDir)
          } catch {
            case t: Throwable =>
              logError("Exception while deleting local spark dir: " +
localDir, t)
          }
        }
        if (shuffleSender != null) {
          shuffleSender.stop()
        }
      }
    })
  }
It goes through and deletes the directories recursively.  I was thinking
there might be some issues with concurrently-running shutdown hooks
deleting things out from underneath each other (shutdown hook javadocs say
they're all started in parallel if multiple hooks are added) causing the
File.list() in that last thread to take quite some time.
While I was looking through the stacktrace the JVM finally exited (after
15-20min at least) so I won't be able to debug more until this bug strikes
again.
Any ideas on what might be going on here?
Thanks!
Andrew
Got a repro locally on my MBP (the other was on a CentOS machine).
Build spark, run a master and a worker with the sbin/start-all.sh script,
then run this in a shell:
import org.apache.spark.storage.StorageLevel._
val s = sc.parallelize(1 to 1000000000).persist(MEMORY_AND_DISK_SER);
s.count
After about a minute, this line appears in the shell logging output:
14/02/06 02:44:44 WARN BlockManagerMasterActor: Removing BlockManager
BlockManagerId(0, aash-mbp.dyn.yojoe.local, 57895, 0) with no recent heart
beats: 57510ms exceeds 45000ms
Ctrl-C the shell.  In jps there is now a worker, a master, and a
CoarseGrainedExecutorBackend.
Run jstack on the CGEBackend JVM, and I got the attached stacktraces.  I
waited around for 15min then kill -9'd the JVM and restarted the process.
I wonder if what's happening here is that the threads that are spewing data
to disk (as that parallelize and persist would do) can write to disk faster
than the cleanup threads can delete from disk.
What do you think of that theory?
Andrew
On Thu, Feb 6, 2014 at 2:30 AM, Mridul Muralidharan  shutdown hooks should not take 15 mins are you mentioned !
Per the book Java Concurrency in Practice the already-running threads
continue running while the shutdown hooks run.  So I think the race between
the writing thread and the deleting thread could be a very real possibility
:/
http://stackoverflow.com/a/3332925/120915
On Thu, Feb 6, 2014 at 2:49 AM, Andrew Ash  wrote:
I think the solution where we stop the writing threads and then let the
deleting threads completely clean up is the best option since the final
state doesn't have half-deleted temp dirs scattered across the cluster.
How feasible do you think it'd be to interrupt the other threads?
On Thu, Feb 6, 2014 at 10:54 AM, Mridul Muralidharan  Looks like a pathological corner case here - where the the delete
There is probably just one threadpool that has task threads -- is it
possible to enumerate and interrupt just those?  We may need to keep string
a reference to that threadpool through to the shutdown thread to make that
happen.
On Thu, Feb 6, 2014 at 10:36 PM, Mridul Muralidharan  Ideally, interrupting the thread writing to disk should be sufficient
That's genius.  Of course when a worker is told to shutdown it should
interrupt its worker threads -- I think that would address this issue.
Are you thinking to put
running.map(_.jobId).foreach { handleJobCancellation }
at the top of the StopDAGScheduler block?
On Thu, Feb 6, 2014 at 11:05 PM, Tathagata Das
 Its highly likely that the executor with the threadpool that runs the tasks
I think we can enumerate all current threads with the ThreadMXBean, filter
to those threads with the name of executor pool in them, and interrupt them.
http://docs.oracle.com/javase/6/docs/api/java/lang/management/ManagementFactory.html#getThreadMXBean%28%29
The executor threads are currently named according to the pattern "Executor
task launch worker-X"
On Thu, Feb 6, 2014 at 11:45 PM, Tathagata Das
 That definitely sound more reliable. Worth trying out if there is a
An additional shutdown hook to stop the threadpool is much more elegant
than the name matching and thread interrupting I was thinking about.  That
Javadoc looks like it's a best-effort shutdown and won't hard kill threads,
but that's at least a step forward from current behavior.
http://docs.oracle.com/javase/7/docs/api/java/util/concurrent/ThreadPoolExecutor.html#shutdownNow()
On Fri, Feb 7, 2014 at 12:13 AM, Tathagata Das
 Or we can try adding a shutdown hook in the
Agreed.  Also I'm happy to test any patches since I have a consistent repro
now (see one of my first responses in this thread)
On Fri, Feb 7, 2014 at 12:51 AM, Mridul Muralidharan  This looks like the most reasonable approach to resolve this !
JIRA I'd guess -- I observed the same thing. Reporters should be able to
edit their own bug!
Sent from my mobile phone
On Feb 7, 2014 11:53 AM, "Henry Saputra"  wrote:
+1 on moving this stuff to a separate mailing list.  It's Apache policy
that discussion is archived, but it's not policy that it must be
interleaved with other dev discussion.  Let's move it to a
spark-github-discuss list (or a different name) and people who care to see
it can subscribe.
On Fri, Feb 7, 2014 at 5:19 PM, Reynold Xin  wrote:
The current script for merging a GitHub PR squashes the commits and sticks
a "Merge pull request #123 from abc/def" at the top of the commit message.
 However this obscures the original commit message when doing a short
gitlog (first line only) so the recent history is much less meaningful than
before.
Compare recent history A:
* 919bd7f Prashant Sharma 86 minutes ago  (origin/master, origin/HEAD)Merge
pull request #567 from ScrapCodes/style2.
* 2182aa3 Martin Jaggi 8 hours ago Merge pull request #566 from
martinjaggi/copy-MLlib-d.
* afc8f3c qqsun8819 10 hours ago Merge pull request #551 from
qqsun8819/json-protocol.
* 94ccf86 Patrick Wendell 10 hours ago Merge pull request #569 from
pwendell/merge-fixes.
* b69f8b2 Patrick Wendell 14 hours ago Merge pull request #557 from
ScrapCodes/style. Closes #557.
* b6dba10 CodingCat 24 hours ago Merge pull request #556 from
CodingCat/JettyUtil. Closes #556.
| * de22abc jyotiska 24 hours ago  (origin/branch-0.9)Merge pull request
#562 from jyotiska/master. Closes #562.
* | 2ef37c9 jyotiska 24 hours ago Merge pull request #562 from
jyotiska/master. Closes #562.
| * 2e3d1c3 Patrick Wendell 24 hours ago Merge pull request #560 from
pwendell/logging. Closes #560.
* | b6d40b7 Patrick Wendell 24 hours ago Merge pull request #560 from
pwendell/logging. Closes #560.
* | f892da8 Patrick Wendell 25 hours ago Merge pull request #565 from
pwendell/dev-scripts. Closes #565.
* | c2341c9 Mark Hamstra 32 hours ago Merge pull request #542 from
markhamstra/versionBump. Closes #542.
| * 22e0a3b Qiuzhuang Lian 35 hours ago Merge pull request #561 from
Qiuzhuang/master. Closes #561.
* | f0ce736 Qiuzhuang Lian 35 hours ago Merge pull request #561 from
Qiuzhuang/master. Closes #561.
* | 7805080 Jey Kottalam 35 hours ago Merge pull request #454 from
jey/atomic-sbt-download. Closes #454.
* | fabf174 Martin Jaggi 2 days ago Merge pull request #552 from
martinjaggi/master. Closes #552.
* | 3a9d82c Andrew Ash 3 days ago Merge pull request #506 from
ash211/intersection. Closes #506.
| * ce179f6 Andrew Or 3 days ago Merge pull request #533 from
andrewor14/master. Closes #533.
To B:
If you go back some time in history, you get a much more branched history,
like this:
| * | | | | | | | | 0984647 Patrick Wendell 4 weeks ago Enable compression
by default for spills
|/ / / / / / / / /
| * | | | | | | | 4e497db Tathagata Das 4 weeks ago Removed
StreamingContext.registerInputStream and registerOutputStream - they were
useless as InputDStream has been made to register itself. Also made DS
* | | | | | | | |   fdaabdc Patrick Wendell 4 weeks ago Merge pull request
#380 from mateiz/py-bayes
|\ \ \ \ \ \ \ \ \
| | | | | * | | | | c2852cf Frank Dai 4 weeks ago Indent two spaces
* | | | | | | | | |   4a805af Patrick Wendell 4 weeks ago Merge pull
request #367 from ankurdave/graphx
|\ \ \ \ \ \ \ \ \ \
| * | | | | | | | | | 80e73ed Joseph E. Gonzalez 4 weeks ago Adding minimal
additional functionality to EdgeRDD
* | | | | | | | | | |   945fe7a Patrick Wendell 4 weeks ago Merge pull
request #408 from pwendell/external-serializers
|\ \ \ \ \ \ \ \ \ \ \
| | * | | | | | | | | | 4bafc4f Joseph E. Gonzalez 4 weeks ago adding
documentation about EdgeRDD
* | | | | | | | | | | |   68641bc Patrick Wendell 4 weeks ago Merge pull
request #413 from rxin/scaladoc
|\ \ \ \ \ \ \ \ \ \ \ \
| | | | | | | | * | | | | 12386b3 Frank Dai 4 weeks ago Since getLong() and
getInt() have side effect, get back parentheses, and remove an empty line
| | | | | | | | * | | | | 0d94d74 Frank Dai 4 weeks ago Code clean up for
mllib
* | | | | | | | | | | | |   0ca0d4d Patrick Wendell 4 weeks ago Merge pull
request #401 from andrewor14/master
|\ \ \ \ \ \ \ \ \ \ \ \ \
| | | | * | | | | | | | | | af645be Ankur Dave 4 weeks ago Fix all code
examples in guide
| | | | * | | | | | | | | | 2cd9358 Ankur Dave 4 weeks ago Finish
6f6f8c928ce493357d4d32e46971c5e401682ea8
* | | | | | | | | | | | | |   08b9fec Patrick Wendell 4 weeks ago Merge
pull request #409 from tdas/unpersist
Ignoring the merge commits here, the commit messages are much better here
than in the current setup because they're what the original author wrote.
 Not a pretty generic "merged pull request #123 from ash211/patch5" or
similar.
Looking at one of those squashed commits, we can see the commit message:
$ git show afc8f3c
commit afc8f3cb9a7afe3249500a7d135b4a54bb3e58c4
Author: qqsun8819 
Date:   Sun Feb 9 13:57:29 2014 -0800
    Merge pull request #551 from qqsun8819/json-protocol.
    [SPARK-1038] Add more fields in JsonProtocol and add tests that verify
the JSON itself
    This is a PR for SPARK-1038. Two major changes:
    1 add some fields to JsonProtocol which is new and important to
standalone-related data structures
    2 Use Diff in liftweb.json to verity the stringified Json output for
detecting someone mod type T to Option[T]
    Author: qqsun8819 
    Closes #551 and squashes the following commits:
    fdf0b4e [qqsun8819] [SPARK-1038] 1. Change code style for more readable
according to rxin review 2. change submitdate hard-coded string to a date
object toString for more complexiblity
    095a26f [qqsun8819] [SPARK-1038] mod according to  review of pwendel,
use hard-coded json string for json data validation. Each test use its own
json string
    0524e41 [qqsun8819] Merge remote-tracking branch 'upstream/master' into
json-protocol
    d203d5c [qqsun8819] [SPARK-1038] Add more fields in JsonProtocol and
add tests that verify the JSON itself
I'd like to propose modifying the git merge/squash script to place that
first line ("Merge pull request #551 from qqsun8819/json-protocol") farther
down in the squashed commit message, to right above the "Closes #551 and
squashes the following commits:" line.
That way the author's original one-line commit message title remains intact.
Thoughts?
Thanks!
Andrew
P.S. These graphs are made with this hlog alias:
hlog = log --date-order --all --graph --format=\"%C(green)%h%Creset
%C(yellow)%an%Creset %C(blue bold)%ar%Creset %C(red bold)%d%Creset%s\"
I didn't realize that it was the PR title I was looking at above and not
the git commit message summary, so the script is already doing as you
suggest.
A commit message is the text that describes what the change was in the
commit.  There's a standard format (linked in the below PR) that people
tend to follow now.
https://github.com/apache/incubator-spark/pull/574
On Mon, Feb 10, 2014 at 12:03 AM, Patrick Wendell  Hey Andrew,
Spark 0.9.0
Hi Spark devs,
I'm pretty sure this stacktrace is a bug in the way Spark is using the type
system but I don't quite know what it is.  Something to do with type bounds
judging from my Googling.
Can someone with more Scala-foo than me please take a look?  In the
meantime I'll be avoiding top() for a bit.
Thanks!
Andrew
This stracktrace came about when I called
val myRDD: RDD[(String,Int)] = ...
myRDD.reduceByKey(_+_).top(100)
But my toy example doesn't trigger the repro:
sc.parallelize(Seq( ("A",10), ("B",5), ("A",4), ("C", 15)
)).reduceByKey(_+_).top(2)
14/02/14 03:15:07 ERROR OneForOneStrategy:
scala.collection.immutable.$colon$colon cannot be cast to
org.apache.spark.util.BoundedPriorityQueue
java.lang.ClassCastException: scala.collection.immutable.$colon$colon
cannot be cast to org.apache.spark.util.BoundedPriorityQueue
        at org.apache.spark.rdd.RDD$$anonfun$top$2.apply(RDD.scala:873)
        at org.apache.spark.rdd.RDD$$anonfun$6.apply(RDD.scala:671)
        at org.apache.spark.rdd.RDD$$anonfun$6.apply(RDD.scala:668)
        at
org.apache.spark.scheduler.JobWaiter.taskSucceeded(JobWaiter.scala:56)
        at
org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:859)
        at
org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:616)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:207)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at
scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Hi dachuan,
At first glance that does look like a bug.  I've opened a pull request with
the change here:
https://github.com/apache/incubator-spark/pull/608
Is that the fix you're proposing?
Thanks!
Andrew
On Mon, Feb 17, 2014 at 6:56 AM, dachuan  wrote:
Hi dachuan,
Aaron merged this in, so it should be fixed in the next release!
Thanks for the patch!
Andrew
On Mon, Feb 17, 2014 at 9:39 AM, dachuan  wrote:
The Create Issue button is in the top header bar in the center, right of
Agil and Capture.  Here's what my screen looks like:  Is that button not
there for you?
[image: Inline image 1]
On Mon, Feb 17, 2014 at 3:57 PM, Bryn Keller  wrote:
Hi dev list,
I'm running into an issue where I'm seeing different results from Spark
when I run with spark.shuffle.spill=false vs leaving it at the default
(true).
It's on internal data so I can't share my exact repro, but here's roughly
what I'm doing:
val rdd = sc.textFile(...)
  .map(l => ... (col1, col2))  // parse CSV into Tuple2[String,String]
  .distinct
  .join(
    sc.textFile(...)
       .map(l => ... (col1, col2))  // parse CSV into Tuple2[String,String]
       .distinct
  )
  .map{ case (k,(v1,v2)) => Seq(v1,k,v2).mkString("|") }
Then I output:
(rdd.count, rdd.distinct.count)
When I run with spark.shuffle.spill=false I get this:
(3192729,3192729)
And with spark.shuffle.spill=true I get this:
(3192931,3192726)
Has anyone else seen any bugs in join-heavy operations while using
spark.shuffle.spill=true?
My current theory is that I have a hashcode collision between rows (unusual
I know) and that the AppendOnlyMap does equality based on
hashcode()+equals() and ExternalAppendOnlyMap does equality based just on
hashcode().
Would appreciate some additional eyes on this problem for sure.
Right now I'm looking through the source and tests for AppendOnlyMap and
ExternalAppendOnlyMap to see if anything jumps out at me.
Thanks!
Andrew
I confirmed also that the spill to disk _was_ occurring:
14/02/18 22:50:50 WARN collection.ExternalAppendOnlyMap: Spilling in-memory
map of 634 MB to disk (1 time so far)
14/02/18 22:50:50 WARN collection.ExternalAppendOnlyMap: Spilling in-memory
map of 581 MB to disk (1 time so far)
On Tue, Feb 18, 2014 at 8:07 PM, Andrew Ash  wrote:
To keep this thread from getting lost, I've opened a ticket here:
https://spark-project.atlassian.net/browse/SPARK-1107
On Fri, Feb 7, 2014 at 12:53 AM, Andrew Ash  wrote:
I'm using Kryo with these options:
-Dspark.shuffle.spill=false -Dspark.storage.memoryFraction=0.4
-Dspark.serializer=org.apache.spark.serializer.KryoSerializer
-Dspark.kryo.registrator=com.andrewash.CustomKryoRegistrator
The data is being read from a .lzo file and written back to another .lzo
file if that affects things.  Does that cover the compression and
serialization libraries question?
I can give master a shot with my repro but it may be some time now that I
have a workaround.  I'm trying to turn something around quickly and have my
own bugs to debug as well :)
Thanks!
Andrew
On Tue, Feb 18, 2014 at 9:02 PM, Andrew Or  wrote:
I found Haskell's convention of including type signatures as documentation
to be worthwhile.
http://www.haskell.org/haskellwiki/Type_signatures_as_good_style
I'd support a guideline to include type signatures where they're unclear
but would prefer to leave it quite vague.  In my experience, the lightest
process is the best process for contributions.  Strict rules here _will_
drive away contributors.
On Wed, Feb 19, 2014 at 10:42 AM, Reynold Xin  wrote:
I'm fine with keeping the GitHub traffic if we can
a) take away the Jenkins build started / build finished / build succeeded /
build failed messages.  Those aren't "dev discussion" and are very noisy.
 I don't think they help anyone, and people who care about those for a
particular PR (because they're a reviewer or author on it) are already
subscribed through GitHub.
b) change the format of the emails that are sent out; I find them very
poorly formatted.  I'd prefer no deep tab for the message.
http://mail-archives.apache.org/mod_mbox/incubator-spark-dev/201402.mbox/%3C20140210192901.CE834922554@tyr.zones.apache.org%3E
FWIW I'm filtering all emails from git@git.apache.org straight to trash
right now because of the noise.
On Thu, Feb 20, 2014 at 12:51 PM, Mattmann, Chris A (3980)  wrote:
Hi Spark devs,
Kyle identified a deficiency in Spark where generating iterators are
unrolled into memory and then flushed to disk rather than sent straight to
disk when possible.
He's had a patch sitting ready for code review for quite some time now (100
days) but no response.
Is this something that an admin would be able to review?  I for one would
find this quite valuable.
Thanks!
Andrew
https://spark-project.atlassian.net/browse/SPARK-942
https://github.com/apache/incubator-spark/pull/180
Would love to have a discussion since I know the core contributors are
facing a barrage of PRs and things are falling through the cracks.
Is there a list of who can commit to core Spark somewhere?  Maybe that list
should be expanded or there should be a rotation of PR duty of some sort.
One of the perils of having a vibrant, organic community is that you get
way more contributions than you expected!
On Mon, Feb 24, 2014 at 1:16 PM, Nan Zhu  wrote:
Yep that's the one thanks! That's quite a few more people than I thought
Sent from my mobile phone
On Feb 24, 2014 1:20 PM, "Nan Zhu"  wrote:
Spark devs,
I picked up somewhere that the Spark 0.9.0 release included Twitter's chill
library of default-registered Kryo serialization classes.  Is that the case?
If so I'd like to mention in the data serialization docs that many things
are registered by default and include a link to the relevant Chill
documentation of what those default-registered classes are.  Chill isn't
mentioned in the below docs right now.
http://spark.incubator.apache.org/docs/latest/tuning.html#data-serialization
Thanks!
Andrew
https://github.com/apache/incubator-spark/pull/647
I was going to say in the addition that twitter/chill was new in 0.9.0 but
decided against it since there aren't any other references to specific
spark versions in the documentation (and I'm not confident that was
actually the new version).
On Mon, Feb 24, 2014 at 8:30 PM, Reynold Xin  wrote:
I've always felt that the Spark team was extremely responsive to PRs and
I've been very impressed over the past year with your output.  As Matei
said, probably the best thing to do here is to be more diligent about
closing PRs that are old/abandoned so that every PR is active.  Whenever I
comment I try to make it clear who has the next action to get the PR merged.
I definitely don't want you to think that I'm critiquing the process!  The
reason I brought this up in the first place was because I thought we were
about to lose a contributor because something fell through the cracks,
which would be unfortunate.
On Tue, Feb 25, 2014 at 6:32 PM, Patrick Wendell  wrote:
I've always felt that the Spark team was extremely responsive to PRs and
I've been very impressed over the past year with your output.  As Matei
said, probably the best thing to do here is to be more diligent about
closing PRs that are old/abandoned so that every PR is active.  Whenever I
comment I try to make it clear who has the next action to get the PR merged.
I definitely don't want you to think that I'm critiquing the process!  The
reason I brought this up in the first place was because I thought we were
about to lose a contributor because something fell through the cracks,
which would be unfortunate.
On Tue, Feb 25, 2014 at 6:32 PM, Patrick Wendell  wrote:
Understood of course.
Did the data fit comfortably in memory or did you experience memory
pressure?  I've had to do a fair amount of tuning when under memory
pressure in the past (0.7.x) and was hoping that the handling of this
scenario is improved in later Spark versions.
On Thu, Mar 20, 2014 at 11:28 AM, Reynold Xin  wrote:
Are you using the PatternInputFormat from this blog post?
https://hadoopi.wordpress.com/2013/05/31/custom-recordreader-processing-string-pattern-delimited-records/
If so you need to set the pattern in the configuration before attempting to
read data with that InputFormat:
String regex = "^[A-Za-z]{3},\\s\\d{2}\\s[A-Za-z]{3}.*";
 Configuration conf = new Configuration(true);
 conf.set("record.delimiter.regex", regex);
On Tue, Apr 8, 2014 at 1:36 PM, Anurag  wrote:
Anurag,
There is another method called newAPIHadoopRDD that takes in a
Configuration object rather than a path.  Give that a shot?
https://spark.apache.org/docs/latest/api/core/index.html#org.apache.spark.SparkContext
On Tue, Apr 8, 2014 at 1:47 PM, Anurag  wrote:
The docs for how to run Spark on Mesos have changed very little since
0.6.0, but setting it up is much easier now than then.  Does it make sense
to revamp with the below changes?
You no longer need to build mesos yourself as pre-built versions are
available from Mesosphere: http://mesosphere.io/downloads/
And the instructions guide you towards compiling your own distribution of
Spark, when you can use the prebuilt versions of Spark as well.
I'd like to split that portion of the documentation into two sections, a
build-from-scratch section and a use-prebuilt section.  The new outline
would look something like this:
*Running Spark on Mesos*
Installing Mesos
- using prebuilt (recommended)
 - pointer to mesosphere's packages
- from scratch
 - (similar to current)
Connecting Spark to Mesos
- loading distribution into an accessible location
- Spark settings
Mesos Run Modes
- (same as current)
Running Alongside Hadoop
- (trim this down)
Does that work for people?
Thanks!
Andrew
PS Basically all the same:
http://spark.apache.org/docs/0.6.0/running-on-mesos.html
http://spark.apache.org/docs/0.6.2/running-on-mesos.html
http://spark.apache.org/docs/0.7.3/running-on-mesos.html
http://spark.apache.org/docs/0.8.1/running-on-mesos.html
http://spark.apache.org/docs/0.9.1/running-on-mesos.html
https://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/running-on-mesos.html
As an example of where it sometimes doesn't work, in older versions of Kryo
/ Chill the Joda LocalDate class didn't serialize properly --
https://groups.google.com/forum/#!topic/cascalog-user/35cdnNIamKU
On Mon, May 12, 2014 at 4:39 PM, Reynold Xin  wrote:
This is the issue where .sortByKey() launches a cluster job when it
shouldn't because it's a transformation not an action.
https://issues.apache.org/jira/browse/SPARK-1021
I'd appreciate a fix too but don't currently have any thoughts on how to
proceed forward.
Andrew
On Thu, May 8, 2014 at 2:16 PM, Mark Hamstra  I'm trying to decide whether attacking the underlying issue of
An RDD can hold objects of any type.  If you generally think of it as a
distributed Collection, then you won't ever be that far off.
As far as serialization, the contents of an RDD must be serializable.
 There are two serialization libraries you can use with Spark: normal Java
serialization or Kryo serialization.  See
https://spark.apache.org/docs/latest/tuning.html#data-serialization for
more details.
If you are using Java serialization then just implementing the Serializable
interface will work.  If you're using Kryo, then
The point that it works fine with local mode and tests but fails in Mesos,
that makes me think there's an issue with the Mesos cluster deployment.
 First, does it work properly in standalone mode?  Second, how are you
getting the Clojure libraries onto the Mesos executors?  Are they included
in your executor URI bundle, or otherwise passing a parameter that points
to the clojure jars?
Cheers,
Andrew
On Thu, May 8, 2014 at 9:55 AM, Soren Macbeth  wrote:
As far as I know, the upstream doesn't release binaries, only source code.
 The downloads page  for 0.18.0 only
has a source tarball.  Is there a binary release somewhere from Mesos that
I'm missing?
On Sun, May 11, 2014 at 2:16 PM, Patrick Wendell  wrote:
For trimming the Running Alongside Hadoop section I mostly think there
should be a separate Spark+HDFS section and have the CDH+HDP page be merged
into that one, but I supposed that's a separate docs change.
On Sun, May 11, 2014 at 4:28 PM, Andy Konwinski  Thanks for suggesting this and volunteering to do it.
Hi Spark devs,
First of all, huge congrats on the parquet integration with SparkSQL!  This
is an incredible direction forward and something I can see being very
broadly useful.
I was doing some preliminary tests to see how it works with one of my
workflows, and wanted to share some numbers that people might want to know
about.
I also wanted to point out that .count() doesn't seem integrated with the
rest of the optimization framework, and some big gains could be possible.
So, the numbers:
I took a table extracted from a SQL database and stored in HDFS:
   - 115 columns (several always-empty, mostly strings, some enums, some
   numbers)
   - 253,887,080 rows
   - 182,150,295,881 bytes (raw uncompressed)
   - 42,826,820,222 bytes (lzo compressed with .index file)
And I converted it to Parquet using SparkSQL's SchemaRDD.saveAsParquet()
call:
   - Converting from .lzo in HDFS to .parquet in HDFS took 635s using 42
   cores across 4 machines
   - 17,517,922,117 bytes (parquet per SparkSQL defaults)
So storing in parquet format vs lzo compresses the data down to less than
50% of the .lzo size, and under 10% of the raw uncompressed size.  Nice!
I then did some basic interactions on it:
*Row count*
   - LZO
      - lzoFile("/path/to/lzo").count
      - 31.632305953s
   - Parquet
      - sqlContext.parquetFile("/path/to/parquet").count
      - 289.129487003s
Reassembling rows from the separate column storage is clearly really
expensive.  Median task length is 33s vs 4s, and of that 33s in each task
(319 tasks total) about 1.75 seconds are spent in GC (inefficient object
allocation?)
*Count number of rows with a particular key:*
   - LZO
   - lzoFile("/path/to/lzo").filter(_.split("\\|")(0) == "1234567890").count
      - 73.988897511s
       - Parquet
   - sqlContext.parquetFile("/path/to/parquet").where('COL ===
      1234567890).count
      - 293.410470418s
       - Parquet (hand-tuned to count on just one column)
   - sqlContext.parquetFile("/path/to/parquet").where('COL ===
      1234567890).select('IDCOL).count
      - 1.160449187s
It looks like currently the .count() on parquet is handled incredibly
inefficiently and all the columns are materialized.  But if I select just
that relevant column and then count, then the column-oriented storage of
Parquet really shines.
There ought to be a potential optimization here such that a .count() on a
SchemaRDD backed by Parquet doesn't require re-assembling the rows, as
that's expensive.  I don't think .count() is handled specially in
SchemaRDDs, but it seems ripe for optimization.
*Count number of distinct values in a column*
   - LZO
   - lzoFile("/path/to/lzo").map(sel(0)).distinct.count
      - 115.582916866s
       - Parquet
   - sqlContext.parquetFile("/path/to/parquet").select('COL).distinct.count
      - 16.839004826 s
It turns out column selectivity is very useful!  I'm guessing that if I
could get byte counts read out of HDFS, that would just about match up with
the difference in read times.
Any thoughts on how to embed the knowledge of my hand-tuned additional
.select('IDCOL)
into Catalyst?
Thanks again for all the hard work and prep for the 1.0 release!
Andrew
I have a draft of my proposed changes here:
https://github.com/apache/spark/pull/756
https://issues.apache.org/jira/browse/SPARK-1818
Thanks!
Andrew
On Mon, May 12, 2014 at 9:57 PM, Andrew Ash  wrote:
These numbers were run on git commit 756c96 (a few days after the 1.0.0-rc3
tag).  Do you have a link to the patch that avoids scanning all columns for
count(*) or count(1)?  I'd like to give it a shot.
Andrew
On Mon, May 12, 2014 at 11:41 PM, Reynold Xin  wrote:
In Scala, if you override .equals() you also need to override .hashCode(),
just like in Java:
http://www.scala-lang.org/api/2.10.3/index.html#scala.AnyRef
I suspect if your .hashCode() delegates to just the hashcode of s then
you'd be good.
On Tue, May 13, 2014 at 3:30 PM, Michael Malak  Is it permissible to use a custom class (as opposed to e.g. the built-in
Thanks for filing -- I'm keeping my eye out for updates on that ticket.
Cheers!
Andrew
On Tue, May 13, 2014 at 2:40 PM, Michael Armbrust  >
+1 on the next release feeling more like a 0.10 than a 1.0
On May 17, 2014 4:38 AM, "Mridul Muralidharan"  wrote:
Hi Liquan,
There is some working being done on implementing linear algebra algorithms
on Spark for use in higher-level machine learning algorithms.  That work is
happening in the MLlib project, which has a
org.apache.spark.mllib.linalgpackage you may find useful.
See
https://github.com/apache/spark/tree/master/mllib/src/main/scala/org/apache/spark/mllib/linalg
MLlib) both the IndexedRowMatrix and RowMatrix implement a multiply
operation:
aash@aash-mbp~/git/spark/mllib/src/main/scala/org/apache/spark/mllib/linalg$
git grep
'def multiply'
distributed/IndexedRowMatrix.scala:  def multiply(B: Matrix):
IndexedRowMatrix = {
distributed/RowMatrix.scala:  def multiply(B: Matrix): RowMatrix = {
aash@aash-mbp~/git/spark/mllib/src/main/scala/org/apache/spark/mllib/linalg$
Can you look into using that code and let us know if it meets your needs?
Thanks!
Andrew
On Sat, May 17, 2014 at 10:28 PM, Liquan Pei  wrote:
Hi Spark devs,
Is the algorithm for
TorrentBroadcastthe
same as Cornet from the below paper?
http://www.mosharaf.com/wp-content/uploads/orchestra-sigcomm11.pdf
If so it would be nice to include a link to the paper in the Javadoc for
the class.
Thanks!
Andrew
Sounds like the problem is that classloaders always look in their parents
before themselves, and Spark users want executors to pick up classes from
their custom code before the ones in Spark plus its dependencies.
Would a custom classloader that delegates to the parent after first
checking itself fix this up?
On Mon, May 19, 2014 at 12:17 AM, DB Tsai  wrote:
Sandy, is there a Jira ticket for that?
On Tue, May 20, 2014 at 10:12 AM, Sandy Ryza  sortByKey currently requires partitions to fit in memory, but there are
Sandy, is there a Jira ticket for that?
On Tue, May 20, 2014 at 10:12 AM, Sandy Ryza  sortByKey currently requires partitions to fit in memory, but there are
Voted :)
https://issues.apache.org/jira/browse/SPARK-983
On Tue, May 20, 2014 at 10:21 AM, Sandy Ryza  There is: SPARK-545
Voted :)
https://issues.apache.org/jira/browse/SPARK-983
On Tue, May 20, 2014 at 10:21 AM, Sandy Ryza  There is: SPARK-545
Hi Gerard,
I agree that your second option seems preferred.  You shouldn't have to
specify a SPARK_HOME if the executor is going to use the spark.executor.uri
instead.  Can you send in a pull request that includes your proposed
changes?
Andrew
On Wed, May 21, 2014 at 10:19 AM, Gerard Maas  wrote:
Fixing the immediate issue of requiring SPARK_HOME to be set when it's not
actually used is a separate ticket in my mind from a larger cleanup of what
SPARK_HOME means across the cluster.
I think you should file a new ticket for just this particular issue.
On Thu, May 22, 2014 at 11:03 AM, Gerard Maas  wrote:
Hi Nilesh,
That change from Matei to change (Key, Seq[Value]) into (Key,
Iterable[Value]) was to enable the optimization in future releases without
breaking the API.  Currently though, all values on a single key are still
held in memory on a single machine.
The way I've gotten around this is by introducing another value to my Key
that goes from (Key) to (Key, randomValue % 10) for example.  Using this
you can further shard an individual key and keep from holding as much data
in memory at once.  The workaround is an ugly hack, but if it works then it
works.
Hope that helps!
Andrew
On Sun, May 25, 2014 at 6:55 PM, Nilesh  wrote:
Hi Nilesh,
That change from Matei to change (Key, Seq[Value]) into (Key,
Iterable[Value]) was to enable the optimization in future releases without
breaking the API.  Currently though, all values on a single key are still
held in memory on a single machine.
The way I've gotten around this is by introducing another value to my Key
that goes from (Key) to (Key, randomValue % 10) for example.  Using this
you can further shard an individual key and keep from holding as much data
in memory at once.  The workaround is an ugly hack, but if it works then it
works.
Hope that helps!
Andrew
On Sun, May 25, 2014 at 6:55 PM, Nilesh  wrote:
I can confirm that the commit is included in the 1.0.0 release candidates
(it was committed before branch-1.0 split off from master), but I can't
confirm that it works in PySpark.  Generally the Python and Java interfaces
lag a little behind the Scala interface to Spark, but we're working to keep
that diff much smaller going forward.
Can you try the same thing in Scala?
On Thu, May 29, 2014 at 8:54 AM, dataginjaninja  wrote:
I can confirm that the commit is included in the 1.0.0 release candidates
(it was committed before branch-1.0 split off from master), but I can't
confirm that it works in PySpark.  Generally the Python and Java interfaces
lag a little behind the Scala interface to Spark, but we're working to keep
that diff much smaller going forward.
Can you try the same thing in Scala?
On Thu, May 29, 2014 at 8:54 AM, dataginjaninja  wrote:
Hi Spark users,
In past Spark releases I always had to add jars to multiple places when
using the spark-shell, and I'm looking to cut down on those.  The --jars
option looks like it does what I want, but it doesn't work.  I did a quick
experiment on latest branch-1.0 and found this:
*# 0) jar not added anywhere*
./bin/spark-shell --master spark://aash-mbp.local:7077
spark> import org.joda.time.DateTime
[fails -- expected because the .jar isn't anywhere]
*# 1) just --jars*
./bin/spark-shell --master spark://aash-mbp.local:7077 --jars
/tmp/joda-time-2.3.jar
spark> import org.joda.time.DateTime
[fails -- but might work on non-standalone clusters?]
*# 2) using --jars and sc.addJar()*
./bin/spark-shell --master spark://aash-mbp.local:7077 --jars
/tmp/joda-time-2.3.jar
spark> sc.addJar("/tmp/joda-time-2.3.jar")
spark> import org.joda.time.DateTime
[fails -- shouldn't sc.addJar() make imports possible?]
*# 3) just --driver-class-path*
./bin/spark-shell --master spark://aash-mbp.local:7077 --driver-class-path
/tmp/joda-time-2.3.jar
spark> import org.joda.time.DateTime
spark> new DateTime()
res0: org.joda.time.DateTime = 2014-05-29T11:10:56.745-07:00
spark> sc.parallelize(1 to 10).map(k => new DateTime()).collect
[fails -- expected because jar wasn't ever sent to executors, only driver]
*# 4) using --driver-class-path and sc.addJar()*
./bin/spark-shell --master spark://aash-mbp.local:7077 --driver-class-path
/tmp/joda-time-2.3.jar
spark> import org.joda.time.DateTime
spark> sc.addJar("/tmp/joda-time-2.3.jar")
spark> new DateTime()
res0: org.joda.time.DateTime = 2014-05-29T11:10:56.745-07:00
spark> sc.parallelize(1 to 10).map(k => new DateTime()).collect
[success!]
Looking at the documentation for --jars, it looks like --jars doesn't work
with standalone in cluster deployment mode.  Here are the relevant doc
entries:
  --jars JARS                 A comma-separated list of local jars to
include on the
                              driver classpath and that SparkContext.addJar
will work
                              with. Doesn't work on standalone with
'cluster' deploy mode.
  --driver-class-path         Extra class path entries to pass to the
driver. Note that
                              jars added with --jars are automatically
included in the
                              classpath.
For the --jars comment about not working with standalone, is this something
that can be fixed to make the "1) just --jars" path above work?  Or is
there some larger architecture reason that --jars can't work with
standalone mode?
Appreciate it!
Andrew
// observed in Spark 1.0
Scala devs,
I was observing an unusual NPE in my code recently, and came up with the
below minimal test case:
class Super extends Serializable {
    lazy val superVal: String = null
}
class Sub extends Super {
    lazy val subVal: String = {
        try {
            "literal"
        } catch {
            case _:Throwable => return superVal
        }
    }
}
Save this to a file, open the Spark shell, and load with ":l
/tmp/test.scala"
I got the below really unusual exception.  It goes away though when
removing the try/catch inside subVal and just returning either a straight
literal or superVal.
Is this a bug in Spark, or Scala, or my code, or what?  I think it might be
related to the Spark Repl doing magic but I'm unsure what.
Cheers!
Andrew
scala> :l /tmp/test.scala
Loading /tmp/test.scala...
defined class Super
     while compiling: 
        during phase: mixin
     library version: version 2.10.4
    compiler version: version 2.10.4
  reconstructed args:
  last tree to typer: Apply(constructor $read)
              symbol: constructor $read in class $read (flags: )
   symbol definition: def (): $line9.$read
                 tpe: $line9.$read
       symbol owners: constructor $read -> class $read -> package $line9
      context owners: class iwC$Sub -> package $line9
== Enclosing template or block ==
Template( // val <local Sub>: , tree.tpe=$line9.iwC$Sub
  "$line5.$read$$iwC$$iwC$$iwC$$iwC$Super" // parents
  ValDef(
    private
    "_"
    
    
  )
  // 6 statements
  ValDef( // lazy private[this] var subVal: String
    private   lazy 
    "subVal "
     // tree.tpe=String
    
  )
  DefDef( // lazy val subVal(): String
       lazy
    "subVal"
    []
    List(Nil)
     // tree.tpe=String
    Block( // tree.tpe=String
      ValDef( // val nonLocalReturnKey1: Object
         
        "nonLocalReturnKey1"
         // tree.tpe=Object
        Apply( // def (): Object in class Object, tree.tpe=Object
          new Object."" // def (): Object in class Object,
tree.tpe=()Object
          Nil
        )
      )
      Try( // tree.tpe=String
        Block( // tree.tpe=String
          Assign( // tree.tpe=Unit
            $read$$iwC$$iwC$$iwC$$iwC$Sub.this."subVal " // lazy
private[this] var subVal: String, tree.tpe=String
            Block( // tree.tpe=String
              {}
              Apply( // final private[this] def
liftedTree1$1(nonLocalReturnKey1$1: Object): String, tree.tpe=String
                $read$$iwC$$iwC$$iwC$$iwC$Sub.this."liftedTree1$1" // final
private[this] def liftedTree1$1(nonLocalReturnKey1$1: Object): String,
tree.tpe=(nonLocalReturnKey1$1: Object)String
                "nonLocalReturnKey1" // val nonLocalReturnKey1: Object,
tree.tpe=Object
              )
            )
          )
          $read$$iwC$$iwC$$iwC$$iwC$Sub.this."subVal " // lazy
private[this] var subVal: String, tree.tpe=String
        )
        CaseDef( // tree.tpe=String
          Bind( // val ex: runtime.NonLocalReturnControl,
tree.tpe=runtime.NonLocalReturnControl
            "ex"
            Typed( // tree.tpe=runtime.NonLocalReturnControl
              "_" // tree.tpe=runtime.NonLocalReturnControl
               // tree.tpe=runtime.NonLocalReturnControl
            )
          )
          If( // tree.tpe=String
            Apply( // final def eq(x$1: Object): Boolean in class Object,
tree.tpe=Boolean
              ex.key()."eq" // final def eq(x$1: Object): Boolean in class
Object, tree.tpe=(x$1: Object)Boolean
              "nonLocalReturnKey1" // val nonLocalReturnKey1: Object,
tree.tpe=Object
            )
            Apply( // final def $asInstanceOf[T0 >: ? <: ?](): T0 in class
Object, tree.tpe=String
              TypeApply( // final def $asInstanceOf[T0 >: ? <: ?](): T0 in
class Object, tree.tpe=()String
                ex.value()."$asInstanceOf" // final def $asInstanceOf[T0 >:
? <: ?](): T0 in class Object, tree.tpe=[T0 >: ? <: ?]()T0
                 // tree.tpe=String
              )
              Nil
            )
            Throw("ex")tree.tpe=Nothing
          )
        )
      )
    )
  )
  ValDef( // protected val $outer: $line9.iwC
    protected   
    "$outer "
     // tree.tpe=$line9.iwC
    
  )
  DefDef( // val $outer(): $line9.iwC
       
    "$line9$$read$$iwC$$iwC$$iwC$$iwC$Sub$$$outer"
    []
    List(Nil)
     // tree.tpe=$line9.iwC
    $read$$iwC$$iwC$$iwC$$iwC$Sub.this."$outer " // protected val $outer:
$line9.iwC, tree.tpe=$line9.iwC
  )
  DefDef( // final private[this] def liftedTree1$1(nonLocalReturnKey1$1:
Object): String
     private final   
    "liftedTree1"
    []
    // 1 parameter list
    ValDef( // nonLocalReturnKey1$1: Object
       
      "nonLocalReturnKey1$1"
       // tree.tpe=Object
      
    )
     // tree.tpe=String
    Try( // tree.tpe=String
      "literal"
      CaseDef( // tree.tpe=Nothing
        Typed( // tree.tpe=Throwable
          "_" // tree.tpe=Throwable
           // tree.tpe=Throwable
        )
        Throw( // tree.tpe=Nothing
          Apply( // def (key: Object,value: Object):
scala.runtime.NonLocalReturnControl in class NonLocalReturnControl,
tree.tpe=scala.runtime.NonLocalReturnControl
            new runtime.NonLocalReturnControl."" // def (key:
Object,value: Object): scala.runtime.NonLocalReturnControl in class
NonLocalReturnControl, tree.tpe=(key: Object, value:
Object)scala.runtime.NonLocalReturnControl
            // 2 arguments
            "nonLocalReturnKey1$1" // nonLocalReturnKey1$1: Object,
tree.tpe=Object
            Apply( // lazy val superVal(): String, tree.tpe=String
              $read$$iwC$$iwC$$iwC$$iwC$Sub.this."superVal" // lazy val
superVal(): String, tree.tpe=()String
              Nil
            )
          )
        )
      )
    )
  )
  DefDef( // def (arg$outer: $line9.iwC): $line9.iwC$Sub
    
    ""
    []
    // 1 parameter list
    ValDef( // $outer: $line9.iwC
       
      "$outer"
       // tree.tpe=$line9.iwC
      
    )
     // tree.tpe=$line9.iwC$Sub
    Block( // tree.tpe=Unit
      // 2 statements
      If( // tree.tpe=Unit
        Apply( // final def eq(x$1: Object): Boolean in class Object,
tree.tpe=Boolean
          "$outer"."eq" // final def eq(x$1: Object): Boolean in class
Object, tree.tpe=(x$1: Object)Boolean
          null
        )
        Throw( // tree.tpe=Nothing
          Apply( // def (): NullPointerException in class
NullPointerException, tree.tpe=NullPointerException
            new NullPointerException."" // def ():
NullPointerException in class NullPointerException,
tree.tpe=()NullPointerException
            Nil
          )
        )
        Assign( // tree.tpe=Unit
          $read$$iwC$$iwC$$iwC$$iwC$Sub.this."$outer " // protected val
$outer: $line9.iwC, tree.tpe=$line9.iwC
          "$outer" // $outer: $line9.iwC, tree.tpe=$line9.iwC
        )
      )
      Apply( // def (arg$outer: $line5.iwC): $line5.iwC$Super,
tree.tpe=$line5.iwC$Super
        $read$$iwC$$iwC$$iwC$$iwC$Sub.super."" // def
(arg$outer: $line5.iwC): $line5.iwC$Super, tree.tpe=(arg$outer:
$line5.iwC)$line5.iwC$Super
        Apply( // val $iw(): $line5.iwC, tree.tpe=$line5.iwC
$outer.$line9$$read$$iwC$$iwC$$iwC$$iwC$$$outer().$VAL1().$iw().$iw().$iw()."$iw"
// val $iw(): $line5.iwC, tree.tpe=()$line5.iwC
          Nil
        )
      )
      ()
    )
  )
)
== Expanded type of tree ==
TypeRef(TypeSymbol(class $read extends Serializable))
unhandled exception while transforming 
error: uncaught exception during compilation: java.lang.NullPointerException
java.lang.NullPointerException
at scala.reflect.internal.Trees$class.Select(Trees.scala:1066)
 at scala.reflect.internal.SymbolTable.Select(SymbolTable.scala:13)
at
scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$scala$tools$nsc$transform$Mixin$MixinTransformer$$dd$1$2.apply(Mixin.scala:908)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$scala$tools$nsc$transform$Mixin$MixinTransformer$$dd$1$2.apply(Mixin.scala:904)
 at scala.reflect.internal.Trees$class.deriveDefDef(Trees.scala:1598)
at scala.reflect.internal.SymbolTable.deriveDefDef(SymbolTable.scala:13)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer.scala$tools$nsc$transform$Mixin$MixinTransformer$$dd$1(Mixin.scala:904)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$addCheckedGetters$1$1.apply(Mixin.scala:945)
at
scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$addCheckedGetters$1$1.apply(Mixin.scala:945)
 at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
 at scala.collection.immutable.List.foreach(List.scala:318)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
 at scala.collection.AbstractTraversable.map(Traversable.scala:105)
at
scala.tools.nsc.transform.Mixin$MixinTransformer.addCheckedGetters$1(Mixin.scala:945)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer.addNewDefs(Mixin.scala:1013)
at
scala.tools.nsc.transform.Mixin$MixinTransformer.scala$tools$nsc$transform$Mixin$MixinTransformer$$postTransform(Mixin.scala:1147)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$transform$1.apply(Mixin.scala:1261)
at
scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$transform$1.apply(Mixin.scala:1261)
 at scala.reflect.internal.SymbolTable.atPhase(SymbolTable.scala:207)
at scala.reflect.internal.SymbolTable.afterPhase(SymbolTable.scala:216)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:1261)
at
scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:471)
 at scala.reflect.api.Trees$Transformer.transformTemplate(Trees.scala:2904)
at
scala.reflect.internal.Trees$$anonfun$itransform$4.apply(Trees.scala:1280)
 at
scala.reflect.internal.Trees$$anonfun$itransform$4.apply(Trees.scala:1279)
at scala.reflect.api.Trees$Transformer.atOwner(Trees.scala:2936)
 at scala.reflect.internal.Trees$class.itransform(Trees.scala:1278)
at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:13)
 at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:13)
at scala.reflect.api.Trees$Transformer.transform(Trees.scala:2897)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:1258)
at
scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:471)
 at
scala.reflect.api.Trees$Transformer$$anonfun$transformStats$1.apply(Trees.scala:2927)
at
scala.reflect.api.Trees$Transformer$$anonfun$transformStats$1.apply(Trees.scala:2925)
 at scala.collection.immutable.List.loop$1(List.scala:170)
at scala.collection.immutable.List.mapConserve(List.scala:186)
 at scala.reflect.api.Trees$Transformer.transformStats(Trees.scala:2925)
at
scala.reflect.internal.Trees$$anonfun$itransform$7.apply(Trees.scala:1298)
 at
scala.reflect.internal.Trees$$anonfun$itransform$7.apply(Trees.scala:1298)
at scala.reflect.api.Trees$Transformer.atOwner(Trees.scala:2936)
 at scala.reflect.internal.Trees$class.itransform(Trees.scala:1297)
at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:13)
 at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:13)
at scala.reflect.api.Trees$Transformer.transform(Trees.scala:2897)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:1258)
at
scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:471)
 at scala.tools.nsc.ast.Trees$Transformer.transformUnit(Trees.scala:227)
at scala.tools.nsc.transform.Transform$Phase.apply(Transform.scala:30)
 at scala.tools.nsc.Global$GlobalPhase.applyPhase(Global.scala:464)
at scala.tools.nsc.Global$GlobalPhase$$anonfun$run$1.apply(Global.scala:431)
 at
scala.tools.nsc.Global$GlobalPhase$$anonfun$run$1.apply(Global.scala:431)
at scala.collection.Iterator$class.foreach(Iterator.scala:727)
 at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
at scala.tools.nsc.Global$GlobalPhase.run(Global.scala:431)
 at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
 at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
at
org.apache.spark.repl.SparkIMain.compileSourcesKeepingRun(SparkIMain.scala:468)
 at
org.apache.spark.repl.SparkIMain$ReadEvalPrint.compileAndSaveRun(SparkIMain.scala:859)
at
org.apache.spark.repl.SparkIMain$ReadEvalPrint.compile(SparkIMain.scala:815)
 at
org.apache.spark.repl.SparkIMain$Request.compile$lzycompute(SparkIMain.scala:1009)
at org.apache.spark.repl.SparkIMain$Request.compile(SparkIMain.scala:1004)
 at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:644)
at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:609)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:796)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:753)
at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:601)
 at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:608)
at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:611)
 at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$2.apply(SparkILoop.scala:621)
 at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$2.apply(SparkILoop.scala:618)
 at
scala.reflect.io.Streamable$Chars$class.applyReader(Streamable.scala:104)
at scala.reflect.io.File.applyReader(File.scala:82)
 at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(SparkILoop.scala:618)
 at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1$$anonfun$apply$mcV$sp$1.apply(SparkILoop.scala:618)
 at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1$$anonfun$apply$mcV$sp$1.apply(SparkILoop.scala:618)
at org.apache.spark.repl.SparkILoop.savingReplayStack(SparkILoop.scala:150)
 at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1.apply$mcV$sp(SparkILoop.scala:617)
at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1.apply(SparkILoop.scala:617)
 at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1.apply(SparkILoop.scala:617)
at org.apache.spark.repl.SparkILoop.savingReader(SparkILoop.scala:155)
 at org.apache.spark.repl.SparkILoop.interpretAllFrom(SparkILoop.scala:616)
at
org.apache.spark.repl.SparkILoop$$anonfun$loadCommand$1.apply(SparkILoop.scala:681)
 at
org.apache.spark.repl.SparkILoop$$anonfun$loadCommand$1.apply(SparkILoop.scala:680)
at org.apache.spark.repl.SparkILoop.withFile(SparkILoop.scala:674)
 at org.apache.spark.repl.SparkILoop.loadCommand(SparkILoop.scala:680)
at
org.apache.spark.repl.SparkILoop$$anonfun$standardCommands$7.apply(SparkILoop.scala:294)
 at
org.apache.spark.repl.SparkILoop$$anonfun$standardCommands$7.apply(SparkILoop.scala:294)
at
scala.tools.nsc.interpreter.LoopCommands$LineCmd.apply(LoopCommands.scala:81)
 at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:748)
at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:601)
 at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:608)
at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:611)
 at
org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:936)
at
org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:884)
 at
org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:884)
at
scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
 at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:884)
at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:983)
 at org.apache.spark.repl.Main$.main(Main.scala:31)
at org.apache.spark.repl.Main.main(Main.scala)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
 at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
 at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:256)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:54)
 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Abandoning crashed session.
scala>
Ah nevermind, the fix is to get rid of "return" from my method.  There's
probably a bug somewhere related to the repl taking bad input more cleanly,
but this isn't the end of the world once you figure out what the issue is.
Thanks for the time,
Andrew
On Mon, Jun 2, 2014 at 11:35 PM, Andrew Ash  wrote:
I have a use case that would greatly benefit from RDDs having a .scanLeft()
method.  Are the project developers interested in adding this to the public
API?
Looking through past message traffic, this has come up a few times.  The
recommendation from the list before has been to implement a parallel prefix
scan.
http://comments.gmane.org/gmane.comp.lang.scala.spark.user/1880
https://groups.google.com/forum/#!topic/spark-users/ts-FdB50ltY
The algorithm Reynold sketched in the first link leads to this working
implementation:
val vector = sc.parallelize(1 to 20, 3)
val sums = 0 +: vector.mapPartitionsWithIndex{ case(partition, iter) =>
Iterator(iter.sum) }.collect.scanLeft(0)(_+_).drop(1)
val prefixScan = vector.mapPartitionsWithIndex { case(partition, iter) =>
  val base = sums(partition)
  println(partition, base)
  iter.scanLeft(base)(_+_).drop(1)
}.collect
I'd love to have that replaced with this:
val vector = sc.parallelize(1 to 20, 3)
val cumSum: RDD[Int] = vector.scanLeft(0)(_+_)
Any thoughts on whether this contribution would be accepted?  What pitfalls
exist that I should be thinking about?
Thanks!
Andrew
I that something that documentation on the method can solve?
On Thu, Jun 5, 2014 at 10:47 AM, Reynold Xin  wrote:
I would appreciate seeing the specs you came up with as well but don't need
to particularly quickly.  I'll wait until seeing the PR to comment on the
specifics, but have some questions about the thought process that went into
configuring the hardware.
Is the idea to see how you spec'd out memory/disk/processor for each
physical machine?  How did you optimize the ratios between the three for
Spark specifically?  Were you able to test this configuration against
others to optimize for price per performance?
Sorry for the barrage of questions -- I know talking hardware tends to
bring out the crazy in people.
Cheers,
Andrew
On Thu, Jun 5, 2014 at 4:04 PM, Krishna Sankar  wrote:
I can't run sbt/sbt gen-idea on a clean checkout of Spark master.
I get resolution errors on junit#junit;4.10!junit.zip(source)
As shown below:
aash@aash-mbp /tmp/git/spark$ sbt/sbt gen-idea
Using /Library/Java/JavaVirtualMachines/jdk1.7.0_45.jdk/Contents/Home as
default JAVA_HOME.
Note, this will be overridden by -java-home if it is set.
[info] Loading project definition from
/private/tmp/git/spark/project/project
[info] Loading project definition from /private/tmp/git/spark/project
[info] Set current project to root (in build file:/private/tmp/git/spark/)
[info] Creating IDEA module for project 'assembly' ...
[info] Updating {file:/private/tmp/git/spark/}core...
[info] Resolving org.fusesource.jansi#jansi;1.4 ...
[warn] [FAILED     ] junit#junit;4.10!junit.zip(source):  (0ms)
[warn] ==== local: tried
[warn]   /Users/aash/.ivy2/local/junit/junit/4.10/sources/junit.zip
[warn] ==== public: tried
[warn]   http://repo1.maven.org/maven2/junit/junit/4.10/junit-4.10.zip
[warn] ==== Maven Repository: tried
[warn]   http://repo.maven.apache.org/maven2/junit/junit/4.10/junit-4.10.zip
[warn] ==== Apache Repository: tried
[warn]
https://repository.apache.org/content/repositories/releases/junit/junit/4.10/junit-4.10.zip
[warn] ==== JBoss Repository: tried
[warn]
https://repository.jboss.org/nexus/content/repositories/releases/junit/junit/4.10/junit-4.10.zip
[warn] ==== MQTT Repository: tried
[warn]
https://repo.eclipse.org/content/repositories/paho-releases/junit/junit/4.10/junit-4.10.zip
[warn] ==== Cloudera Repository: tried
[warn]
http://repository.cloudera.com/artifactory/cloudera-repos/junit/junit/4.10/junit-4.10.zip
[warn] ==== Pivotal Repository: tried
[warn]   http://repo.spring.io/libs-release/junit/junit/4.10/junit-4.10.zip
[warn] ==== Maven2 Local: tried
[warn]   file:/Users/aash/.m2/repository/junit/junit/4.10/junit-4.10.zip
[warn] ::::::::::::::::::::::::::::::::::::::::::::::
[warn] ::              FAILED DOWNLOADS            ::
[warn] :: ^ see resolution messages for details  ^ ::
[warn] ::::::::::::::::::::::::::::::::::::::::::::::
[warn] :: junit#junit;4.10!junit.zip(source)
[warn] ::::::::::::::::::::::::::::::::::::::::::::::
sbt.ResolveException: download failed: junit#junit;4.10!junit.zip(source)
By bumping the junit dependency to 4.11 I'm able to generate the IDE files.
 Are other people having this problem or does everyone use the maven
configuration?
Andrew
Hi qingyang,
This looks like an issue with the open source version of the Java runtime
(called OpenJDK) that causes the JVM to fail.  Can you try using the JVM
released by Oracle and see if it has the same issue?
Thanks!
Andrew
On Mon, Jun 16, 2014 at 9:24 PM, qingyang li  hi, I encounter  jvm problem when integreation spark with mesos,
Maybe it's a Mac OS X thing?
On Mon, Jun 16, 2014 at 9:57 PM, Ted Yu  wrote:
Hi Xiaokai,
Also take a look through Xiangrui's slides from HadoopSummit a few weeks
back: http://www.slideshare.net/xrmeng/m-llib-hadoopsummit  The roadmap
starting at slide 51 will probably be interesting to you.
Andrew
On Tue, Jun 17, 2014 at 7:37 PM, Sandy Ryza  wrote:
Thanks for helping shepherd the voting on 1.0.1 Patrick.
I'd like to call attention to
https://issues.apache.org/jira/browse/SPARK-2157 and
https://github.com/apache/spark/pull/1107 -- "Ability to write tight
firewall rules for Spark"
I'm currently unable to run Spark on some projects because our cloud ops
team is uncomfortable with the firewall situation around Spark at the
moment.  Currently Spark starts listening on random ephemeral ports and
does server to server communication on them.  This keeps the team from
writing tight firewall rules between the services -- they get real queasy
when asked to open inbound connections to the entire ephemeral port range
of a cluster.  We can tighten the size of the ephemeral range using kernel
settings to mitigate the issue, but it doesn't actually solve the problem.
The PR above aims to make every listening port on JVMs in a Spark
standalone cluster configurable with an option.  If not set, the current
behavior stands (start listening on an ephemeral port).  Is this something
the Spark team would consider merging into 1.0.1?
Thanks!
Andrew
On Sun, Jun 29, 2014 at 10:54 PM, Patrick Wendell  Hey All,
Ok that's reasonable -- it's certainly more of an enhancement than a
critical bug-fix.  I would like to get this in for 1.1.0 though, so let's
talk through the right way to do that on the PR.
In the meantime the best alternative is running with lax firewall settings,
which can be somewhat mitigated by modifying the ephemeral port range.
Thanks!
Andrew
On Sun, Jun 29, 2014 at 11:14 PM, Reynold Xin  wrote:
I observed a deadlock here when using the AvroInputFormat as well. The
short of the issue is that there's one configuration object per JVM, but
multiple threads, one for each task. If each thread attempts to add a
configuration option to the Configuration object at once you get issues
because HashMap isn't thread safe.
More details to come tonight. Thanks!
On Jul 14, 2014 4:11 PM, "Nishkam Ravi"  wrote:
2014-07-11 15:54:08
Full thread dump Java HotSpot(TM) 64-Bit Server VM (24.60-b09 mixed mode):
"Attach Listener" daemon prio=10 tid=0x00007f9210001000 nid=0x5a5a waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
"sparkExecutor-akka.actor.default-dispatcher-25" daemon prio=10 tid=0x00007f9100001000 nid=0x7ff waiting on condition [0x00007f92d630c000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"sparkExecutor-akka.actor.default-dispatcher-26" daemon prio=10 tid=0x00007f9204005000 nid=0x7fe waiting on condition [0x00007f92d5f08000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"sparkExecutor-akka.actor.default-dispatcher-24" daemon prio=10 tid=0x00007f9144196000 nid=0x7fd waiting on condition [0x00007f92d5e07000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"sparkExecutor-akka.actor.default-dispatcher-23" daemon prio=10 tid=0x00007f9204003800 nid=0x7f7 waiting on condition [0x00007f92d6009000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"sparkExecutor-akka.actor.default-dispatcher-22" daemon prio=10 tid=0x00007f9204002000 nid=0x7f6 waiting on condition [0x00007f92d610a000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"sparkExecutor-akka.actor.default-dispatcher-21" daemon prio=10 tid=0x00007f91cc001000 nid=0x7f5 waiting on condition [0x00007f92d620b000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"sparkExecutor-akka.actor.default-dispatcher-20" daemon prio=10 tid=0x00007f9204001000 nid=0x7f4 waiting on condition [0x00007f92d79f8000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"sparkExecutor-akka.actor.default-dispatcher-19" daemon prio=10 tid=0x00007f91c4001000 nid=0x7f3 waiting on condition [0x00007f92d77f6000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"sparkExecutor-akka.actor.default-dispatcher-18" daemon prio=10 tid=0x00007f91fc03a800 nid=0x7e0 waiting on condition [0x00007f92d78f7000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"spark-akka.actor.default-dispatcher-16" daemon prio=10 tid=0x00007f913c01f800 nid=0x7df waiting on condition [0x00007f92d76f5000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"spark-akka.actor.default-dispatcher-15" daemon prio=10 tid=0x00007f91b4001000 nid=0x7de waiting on condition [0x00007f92d75f4000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"spark-akka.actor.default-dispatcher-14" daemon prio=10 tid=0x00007f91ac004000 nid=0x7dd waiting on condition [0x00007f92d7af9000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"Connection manager future execution context-1" daemon prio=10 tid=0x00007f91b8001000 nid=0x7dc waiting on condition [0x00007f92dc63e000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a java.util.concurrent.SynchronousQueue$TransferStack)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
	at java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:359)
	at java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:942)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:1068)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1130)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
"org.apache.hadoop.hdfs.PeerCache@4404df37" daemon prio=10 tid=0x00007f9138020800 nid=0x57ec waiting on condition [0x00007f92d5d06000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hdfs.PeerCache.run(PeerCache.java:252)
	at org.apache.hadoop.hdfs.PeerCache.access$000(PeerCache.java:39)
	at org.apache.hadoop.hdfs.PeerCache$1.run(PeerCache.java:135)
	at java.lang.Thread.run(Thread.java:745)
"RESULT_TASK cleanup timer" daemon prio=10 tid=0x00007f9140005800 nid=0x55a8 in Object.wait() [0x00007f92d6cd4000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on  (a java.util.TaskQueue)
	at java.lang.Object.wait(Object.java:503)
	at java.util.TimerThread.mainLoop(Timer.java:526)
	- locked  (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)
"Executor task launch worker-6" daemon prio=10 tid=0x00007f91f01fe000 nid=0x54b1 runnable [0x00007f92d74f1000]
   java.lang.Thread.State: RUNNABLE
	at java.util.HashMap.transfer(HashMap.java:601)
	at java.util.HashMap.resize(HashMap.java:581)
	at java.util.HashMap.addEntry(HashMap.java:879)
	at java.util.HashMap.put(HashMap.java:505)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:803)
	at org.apache.hadoop.conf.Configuration.set(Configuration.java:783)
	at org.apache.hadoop.conf.Configuration.setClass(Configuration.java:1662)
	at org.apache.hadoop.ipc.RPC.setProtocolEngine(RPC.java:193)
	at org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol(NameNodeProxies.java:343)
	at org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:168)
	at org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:129)
	at org.apache.hadoop.hdfs.DFSClient.(DFSClient.java:436)
	at org.apache.hadoop.hdfs.DFSClient.(DFSClient.java:403)
	at org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:125)
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2262)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:86)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2296)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2278)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:316)
	at org.apache.hadoop.fs.Path.getFileSystem(Path.java:194)
	at org.apache.avro.mapred.FsInput.(FsInput.java:37)
	at org.apache.avro.mapred.AvroRecordReader.(AvroRecordReader.java:43)
	at org.apache.avro.mapred.AvroInputFormat.getRecordReader(AvroInputFormat.java:52)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.(HadoopRDD.scala:156)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:149)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:64)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
	at org.apache.spark.scheduler.Task.run(Task.scala:53)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
	at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
	at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
"qtp1108297380-55" daemon prio=10 tid=0x00007f91f012f000 nid=0x54aa waiting on condition [0x00007f92d7bfa000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)
"qtp1108297380-54" daemon prio=10 tid=0x00007f91f012c800 nid=0x54a9 waiting on condition [0x00007f92d7cfb000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)
"qtp1108297380-53" daemon prio=10 tid=0x00007f91f012a800 nid=0x54a8 waiting on condition [0x00007f92d7dfc000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)
"qtp1108297380-52" daemon prio=10 tid=0x00007f91f0128800 nid=0x54a7 waiting on condition [0x00007f92d7efd000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)
"qtp1108297380-51" daemon prio=10 tid=0x00007f91f0127000 nid=0x54a6 waiting on condition [0x00007f92d7ffe000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)
"qtp1108297380-50" daemon prio=10 tid=0x00007f91f0120800 nid=0x54a5 waiting on condition [0x00007f92dc139000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)
"qtp1108297380-49" daemon prio=10 tid=0x00007f91f011f000 nid=0x54a4 waiting on condition [0x00007f92dc23a000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:226)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2082)
	at org.eclipse.jetty.util.BlockingArrayQueue.poll(BlockingArrayQueue.java:342)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.idleJobPoll(QueuedThreadPool.java:526)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.access$600(QueuedThreadPool.java:44)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:572)
	at java.lang.Thread.run(Thread.java:745)
"qtp1108297380-48 Acceptor0 SocketConnector@0.0.0.0:57403" daemon prio=10 tid=0x00007f91f011e000 nid=0x54a3 runnable [0x00007f92dc33b000]
   java.lang.Thread.State: RUNNABLE
	at java.net.PlainSocketImpl.socketAccept(Native Method)
	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:398)
	at java.net.ServerSocket.implAccept(ServerSocket.java:530)
	at java.net.ServerSocket.accept(ServerSocket.java:498)
	at org.eclipse.jetty.server.bio.SocketConnector.accept(SocketConnector.java:117)
	at org.eclipse.jetty.server.AbstractConnector$Acceptor.run(AbstractConnector.java:938)
	at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
	at java.lang.Thread.run(Thread.java:745)
"MAP_OUTPUT_TRACKER cleanup timer" daemon prio=10 tid=0x00007f91f00f8000 nid=0x54a2 in Object.wait() [0x00007f92dc43c000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on  (a java.util.TaskQueue)
	at java.util.TimerThread.mainLoop(Timer.java:552)
	- locked  (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)
"HTTP_BROADCAST cleanup timer" daemon prio=10 tid=0x00007f91f00f1000 nid=0x54a1 in Object.wait() [0x00007f92dc53d000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on  (a java.util.TaskQueue)
	at java.util.TimerThread.mainLoop(Timer.java:552)
	- locked  (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)
"BROADCAST_VARS cleanup timer" daemon prio=10 tid=0x00007f91f00e1000 nid=0x549f in Object.wait() [0x00007f92dc73f000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on  (a java.util.TaskQueue)
	at java.util.TimerThread.mainLoop(Timer.java:552)
	- locked  (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)
"BLOCK_MANAGER cleanup timer" daemon prio=10 tid=0x00007f91f00df800 nid=0x549e in Object.wait() [0x00007f92dc840000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on  (a java.util.TaskQueue)
	at java.util.TimerThread.mainLoop(Timer.java:552)
	- locked  (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)
"connection-manager-thread" daemon prio=10 tid=0x00007f91f00d8000 nid=0x549d runnable [0x00007f92dc941000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked  (a sun.nio.ch.Util$2)
	- locked  (a java.util.Collections$UnmodifiableSet)
	- locked  (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:102)
	at org.apache.spark.network.ConnectionManager.run(ConnectionManager.scala:283)
	at org.apache.spark.network.ConnectionManager$$anon$3.run(ConnectionManager.scala:98)
"SHUFFLE_BLOCK_MANAGER cleanup timer" daemon prio=10 tid=0x00007f91f00a8800 nid=0x549c in Object.wait() [0x00007f92dca42000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on  (a java.util.TaskQueue)
	at java.util.TimerThread.mainLoop(Timer.java:552)
	- locked  (a java.util.TaskQueue)
	at java.util.TimerThread.run(Timer.java:505)
"Hashed wheel timer #2" daemon prio=10 tid=0x00007f9198001000 nid=0x549b waiting on condition [0x00007f92dcb43000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:503)
	at org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:401)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at java.lang.Thread.run(Thread.java:745)
"New I/O server boss #12" daemon prio=10 tid=0x00007f91a80df000 nid=0x549a runnable [0x00007f92dcc44000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked  (a sun.nio.ch.Util$2)
	- locked  (a java.util.Collections$UnmodifiableSet)
	- locked  (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:102)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
"New I/O worker #11" daemon prio=10 tid=0x00007f91a80b4800 nid=0x5499 runnable [0x00007f92dcd45000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked  (a sun.nio.ch.Util$2)
	- locked  (a java.util.Collections$UnmodifiableSet)
	- locked  (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
"New I/O worker #10" daemon prio=10 tid=0x00007f91a808a000 nid=0x5498 runnable [0x00007f92dce46000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked  (a sun.nio.ch.Util$2)
	- locked  (a java.util.Collections$UnmodifiableSet)
	- locked  (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
"New I/O boss #9" daemon prio=10 tid=0x00007f91a806f800 nid=0x5497 runnable [0x00007f92dcf47000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked  (a sun.nio.ch.Util$2)
	- locked  (a java.util.Collections$UnmodifiableSet)
	- locked  (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
"New I/O worker #8" daemon prio=10 tid=0x00007f91a8044800 nid=0x5496 runnable [0x00007f92dd048000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked  (a sun.nio.ch.Util$2)
	- locked  (a java.util.Collections$UnmodifiableSet)
	- locked  (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
"New I/O worker #7" daemon prio=10 tid=0x00007f91a801a000 nid=0x5495 runnable [0x00007f92dd149000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked  (a sun.nio.ch.Util$2)
	- locked  (a java.util.Collections$UnmodifiableSet)
	- locked  (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
"spark-akka.actor.default-dispatcher-6" daemon prio=10 tid=0x00007f91b0001000 nid=0x5494 waiting on condition [0x00007f92dd24a000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"spark-akka.actor.default-dispatcher-5" daemon prio=10 tid=0x00007f91f004b800 nid=0x5493 waiting on condition [0x00007f92dd34b000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"spark-akka.actor.default-dispatcher-4" daemon prio=10 tid=0x00007f91bc001000 nid=0x5492 waiting on condition [0x00007f92dd44c000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"spark-akka.actor.default-dispatcher-3" daemon prio=10 tid=0x00007f91f0049800 nid=0x5491 waiting on condition [0x00007f92dd54d000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"spark-akka.actor.default-dispatcher-2" daemon prio=10 tid=0x00007f91f0049000 nid=0x5490 waiting on condition [0x00007f92dd64e000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"spark-scheduler-1" daemon prio=10 tid=0x00007f91f0045800 nid=0x548f waiting on condition [0x00007f92dd74f000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at akka.actor.LightArrayRevolverScheduler.waitNanos(Scheduler.scala:226)
	at akka.actor.LightArrayRevolverScheduler$$anon$12.nextTick(Scheduler.scala:393)
	at akka.actor.LightArrayRevolverScheduler$$anon$12.run(Scheduler.scala:363)
	at java.lang.Thread.run(Thread.java:745)
"sparkExecutor-akka.actor.default-dispatcher-17" daemon prio=10 tid=0x00007f91c0001000 nid=0x548e waiting on condition [0x00007f92dd850000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"sparkExecutor-akka.actor.default-dispatcher-16" daemon prio=10 tid=0x00007f91d0001000 nid=0x548d waiting on condition [0x00007f92dd951000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"sparkExecutor-akka.actor.default-dispatcher-15" daemon prio=10 tid=0x00007f91c8006800 nid=0x548c waiting on condition [0x00007f92dda52000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"sparkExecutor-akka.actor.default-dispatcher-14" daemon prio=10 tid=0x00007f91fc017000 nid=0x548b waiting on condition [0x00007f92ddb53000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"Hashed wheel timer #1" daemon prio=10 tid=0x00007f91e0001000 nid=0x548a waiting on condition [0x00007f92ddc54000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:503)
	at org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:401)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at java.lang.Thread.run(Thread.java:745)
"sparkExecutor-akka.actor.default-dispatcher-13" daemon prio=10 tid=0x00007f91f8160000 nid=0x5489 waiting on condition [0x00007f92ddd55000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"New I/O server boss #6" daemon prio=10 tid=0x00007f91f809b000 nid=0x5488 runnable [0x00007f92de276000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked  (a sun.nio.ch.Util$2)
	- locked  (a java.util.Collections$UnmodifiableSet)
	- locked  (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:102)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.select(NioServerBoss.java:163)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.NioServerBoss.run(NioServerBoss.java:42)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
"New I/O worker #5" daemon prio=10 tid=0x00007f91f8096800 nid=0x5487 runnable [0x00007f92de377000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked  (a sun.nio.ch.Util$2)
	- locked  (a java.util.Collections$UnmodifiableSet)
	- locked  (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
"New I/O worker #4" daemon prio=10 tid=0x00007f91f8098000 nid=0x5486 runnable [0x00007f92de478000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked  (a sun.nio.ch.Util$2)
	- locked  (a java.util.Collections$UnmodifiableSet)
	- locked  (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
"New I/O boss #3" daemon prio=10 tid=0x00007f91f8094800 nid=0x5485 runnable [0x00007f92de579000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked  (a sun.nio.ch.Util$2)
	- locked  (a java.util.Collections$UnmodifiableSet)
	- locked  (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
"New I/O worker #2" daemon prio=10 tid=0x00007f91f8034000 nid=0x5484 runnable [0x00007f92de67a000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked  (a sun.nio.ch.Util$2)
	- locked  (a java.util.Collections$UnmodifiableSet)
	- locked  (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
"New I/O worker #1" daemon prio=10 tid=0x00007f91f8036800 nid=0x5483 runnable [0x00007f92de77b000]
   java.lang.Thread.State: RUNNABLE
	at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
	at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)
	at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)
	at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)
	- locked  (a sun.nio.ch.Util$2)
	- locked  (a java.util.Collections$UnmodifiableSet)
	- locked  (a sun.nio.ch.EPollSelectorImpl)
	at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)
	at org.jboss.netty.channel.socket.nio.SelectorUtil.select(SelectorUtil.java:64)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.select(AbstractNioSelector.java:409)
	at org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:206)
	at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:90)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:178)
	at org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
	at org.jboss.netty.util.internal.DeadLockProofWorker$1.run(DeadLockProofWorker.java:42)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
"sparkExecutor-akka.actor.default-dispatcher-5" daemon prio=10 tid=0x00007f91f8011000 nid=0x5482 waiting on condition [0x00007f92de87c000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"sparkExecutor-akka.actor.default-dispatcher-4" daemon prio=10 tid=0x00007f91fc003000 nid=0x5481 waiting on condition [0x00007f92de97d000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"sparkExecutor-akka.actor.default-dispatcher-3" daemon prio=10 tid=0x00007f9f844e7800 nid=0x5480 waiting on condition [0x00007f92dea7e000]
   java.lang.Thread.State: TIMED_WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.idleAwaitWork(ForkJoinPool.java:2135)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2067)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"sparkExecutor-akka.actor.default-dispatcher-2" daemon prio=10 tid=0x00007f9f844dd000 nid=0x547f waiting on condition [0x00007f92deb7f000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinPool)
	at scala.concurrent.forkjoin.ForkJoinPool.scan(ForkJoinPool.java:2075)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
"sparkExecutor-scheduler-1" daemon prio=10 tid=0x00007f9f8446d800 nid=0x547e sleeping[0x00007f92dec80000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at akka.actor.LightArrayRevolverScheduler.waitNanos(Scheduler.scala:226)
	at akka.actor.LightArrayRevolverScheduler$$anon$12.nextTick(Scheduler.scala:393)
	at akka.actor.LightArrayRevolverScheduler$$anon$12.run(Scheduler.scala:363)
	at java.lang.Thread.run(Thread.java:745)
"Service Thread" daemon prio=10 tid=0x00007f9f840a9800 nid=0x547c runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
"C2 CompilerThread1" daemon prio=10 tid=0x00007f9f840a7000 nid=0x547b waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
"C2 CompilerThread0" daemon prio=10 tid=0x00007f9f840a4000 nid=0x547a waiting on condition [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
"Signal Dispatcher" daemon prio=10 tid=0x00007f9f840a2000 nid=0x5479 runnable [0x0000000000000000]
   java.lang.Thread.State: RUNNABLE
"Finalizer" daemon prio=10 tid=0x00007f9f84084800 nid=0x5478 in Object.wait() [0x00007f92eca38000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on  (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
	- locked  (a java.lang.ref.ReferenceQueue$Lock)
	at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:151)
	at java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:209)
"Reference Handler" daemon prio=10 tid=0x00007f9f84082800 nid=0x5477 in Object.wait() [0x00007f92ecb39000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on  (a java.lang.ref.Reference$Lock)
	at java.lang.Object.wait(Object.java:503)
	at java.lang.ref.Reference$ReferenceHandler.run(Reference.java:133)
	- locked  (a java.lang.ref.Reference$Lock)
"main" prio=10 tid=0x00007f9f8400b000 nid=0x5468 waiting on condition [0x00007f9f8a605000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for   (a java.util.concurrent.CountDownLatch$Sync)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:834)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:994)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1303)
	at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:236)
	at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(ActorSystem.scala:760)
	at akka.actor.ActorSystemImpl$TerminationCallbacks.ready(ActorSystem.scala:729)
	at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala:86)
	at scala.concurrent.Await$$anonfun$ready$1.apply(package.scala:86)
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
	at scala.concurrent.Await$.ready(package.scala:86)
	at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala:598)
	at akka.actor.ActorSystemImpl.awaitTermination(ActorSystem.scala:599)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$.run(CoarseGrainedExecutorBackend.scala:112)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$.main(CoarseGrainedExecutorBackend.scala:126)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend.main(CoarseGrainedExecutorBackend.scala)
"VM Thread" prio=10 tid=0x00007f9f8407e000 nid=0x5476 runnable 
"GC task thread#0 (ParallelGC)" prio=10 tid=0x00007f9f84020800 nid=0x5469 runnable 
"GC task thread#1 (ParallelGC)" prio=10 tid=0x00007f9f84022800 nid=0x546a runnable 
"GC task thread#2 (ParallelGC)" prio=10 tid=0x00007f9f84024800 nid=0x546b runnable 
"GC task thread#3 (ParallelGC)" prio=10 tid=0x00007f9f84026000 nid=0x546c runnable 
"GC task thread#4 (ParallelGC)" prio=10 tid=0x00007f9f84028000 nid=0x546d runnable 
"GC task thread#5 (ParallelGC)" prio=10 tid=0x00007f9f8402a000 nid=0x546e runnable 
"GC task thread#6 (ParallelGC)" prio=10 tid=0x00007f9f8402c000 nid=0x546f runnable 
"GC task thread#7 (ParallelGC)" prio=10 tid=0x00007f9f8402d800 nid=0x5470 runnable 
"GC task thread#8 (ParallelGC)" prio=10 tid=0x00007f9f8402f800 nid=0x5471 runnable 
"GC task thread#9 (ParallelGC)" prio=10 tid=0x00007f9f84031800 nid=0x5472 runnable 
"GC task thread#10 (ParallelGC)" prio=10 tid=0x00007f9f84033000 nid=0x5473 runnable 
"GC task thread#11 (ParallelGC)" prio=10 tid=0x00007f9f84035000 nid=0x5474 runnable 
"GC task thread#12 (ParallelGC)" prio=10 tid=0x00007f9f84037000 nid=0x5475 runnable 
"VM Periodic Task Thread" prio=10 tid=0x00007f9f840b4000 nid=0x547d waiting on condition 
JNI global references: 188
Hi Patrick, thanks for taking a look.  I filed as
https://issues.apache.org/jira/browse/SPARK-2546
Would you recommend I pursue the cloned Configuration object approach now
and send in a PR?
Reynold's recent announcement of the broadcast RDD object patch may also
have implications of the right path forward here.  I'm not sure I fully
understand the implications though:
https://github.com/apache/spark/pull/1452
"Once this is committed, we can also remove the JobConf broadcast in
HadoopRDD."
Thanks!
Andrew
On Tue, Jul 15, 2014 at 5:20 PM, Patrick Wendell  wrote:
Sounds good -- I added comments to the ticket.
Since SPARK-2521 is scheduled for a 1.1.0 release and we can work around
with spark.speculation, I don't personally see a need for a 1.0.2 backport.
Thanks looking through this issue!
On Thu, Jul 17, 2014 at 2:14 AM, Patrick Wendell  wrote:
Personally I'd find the method useful -- I've often had a .csv file with a
header row that I want to drop so filter it out, which touches all
partitions anyway.  I don't have any comments on the implementation quite
yet though.
On Mon, Jul 21, 2014 at 8:24 AM, Erik Erlandson  wrote:
Hi everyone,
I'm seeing the below exception coming out of Spark 1.0.1 when I call it
from my application.  I can't share the source to that application, but the
quick gist is that it uses Spark's Java APIs to read from Avro files in
HDFS, do processing, and write back to Avro files.  It does this by
receiving a REST call, then spinning up a new JVM as the driver application
that connects to Spark.  I'm using CDH4.4.0 and have enabled Kryo and also
speculation.  The cluster is running in standalone mode on a 6 node cluster
in AWS (not using Spark's EC2 scripts though).
The below stacktraces are reliably reproduceable on every run of the job.
 The issue seems to be that on deserialization of a task result on the
driver, Kryo spits up while reading the ClassManifest.
I've tried swapping in Kryo 2.23.1 rather than 2.21 (2.22 had some
backcompat issues) but had the same error.
Any ideas on what can be done here?
Thanks!
Andrew
In the driver (Kryo exception while deserializing a DirectTaskResult):
INFO   | jvm 1    | 2014/07/30 20:52:52 | 20:52:52.667 [Result resolver
thread-0] ERROR o.a.spark.scheduler.TaskResultGetter - Exception while
getting task result
INFO   | jvm 1    | 2014/07/30 20:52:52 |
com.esotericsoftware.kryo.KryoException: Buffer underflow.
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
com.esotericsoftware.kryo.io.Input.require(Input.java:156)
~[kryo-2.21.jar:na]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
com.esotericsoftware.kryo.io.Input.readInt(Input.java:337)
~[kryo-2.21.jar:na]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:762)
~[kryo-2.21.jar:na]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:624) ~[kryo-2.21.jar:na]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
com.twitter.chill.ClassManifestSerializer.read(ClassManifestSerializer.scala:26)
~[chill_2.10-0.3.6.jar:0.3.6]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
com.twitter.chill.ClassManifestSerializer.read(ClassManifestSerializer.scala:19)
~[chill_2.10-0.3.6.jar:0.3.6]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
~[kryo-2.21.jar:na]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:147)
~[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:79)
~[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.scheduler.TaskSetManager.handleSuccessfulTask(TaskSetManager.scala:480)
~[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.scheduler.TaskSchedulerImpl.handleSuccessfulTask(TaskSchedulerImpl.scala:316)
~[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:68)
[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1160)
[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:46)
[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[na:1.7.0_65]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[na:1.7.0_65]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]
In the DAGScheduler (job gets aborted):
org.apache.spark.SparkException: Job aborted due to stage failure:
Exception while getting task result:
com.esotericsoftware.kryo.KryoException: Buffer underflow.
    at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026)
    at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    at
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
    at scala.Option.foreach(Option.scala:236)
    at
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634)
    at
org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
    at akka.actor.ActorCell.invoke(ActorCell.scala:456)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
    at akka.dispatch.Mailbox.run(Mailbox.scala:219)
    at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
    at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
    at
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
    at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
In an Executor (running tasks get killed):
14/07/29 22:57:38 INFO broadcast.HttpBroadcast: Started reading broadcast
variable 0
14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
153
14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
147
14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
141
14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
135
14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
150
14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
144
14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
138
14/07/29 22:57:39 INFO storage.MemoryStore: ensureFreeSpace(241733) called
with curMem=0, maxMem=30870601728
14/07/29 22:57:39 INFO storage.MemoryStore: Block broadcast_0 stored as
values to memory (estimated size 236.1 KB, free 28.8 GB)
14/07/29 22:57:39 INFO broadcast.HttpBroadcast: Reading broadcast variable
0 took 0.91790748 s
14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0 locally
14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0 locally
14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0 locally
14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0 locally
14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0 locally
14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0 locally
14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 135
org.apache.spark.TaskKilledException
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 144
org.apache.spark.TaskKilledException
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 150
org.apache.spark.TaskKilledException
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 138
org.apache.spark.TaskKilledException
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 141
org.apache.spark.TaskKilledException
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
After several days of debugging, we think the issue is that we have
conflicting versions of Guava.  Our application was running with Guava 14
and the Spark services (Master, Workers, Executors) had Guava 16.  We had
custom Kryo serializers for Guava's ImmutableLists, and commenting out
those register calls did the trick.
Have people had issues with Guava version mismatches in the past?
I've found @srowen's Guava 14 -> 11 downgrade PR here
https://github.com/apache/spark/pull/1610 and some extended discussion on
https://issues.apache.org/jira/browse/SPARK-2420 for Hive compatibility
On Thu, Jul 31, 2014 at 10:47 AM, Andrew Ash  wrote:
The original version numbers I reported were indeed what we had, so let me
clarify the situation.
Our application had Guava 14 because that's what Spark depends on.  But we
had added an in-house library to the Hadoop cluster and also the Spark
cluster to add a new FileSystem (think hdfs://, s3n://, etc) that was using
Guava 16.  So the Guava 16 from our additional FileSystem overrode the
Guava 11 jar from the CDH4.4.0 lib directory and the Guava 14 class files
that are bundled in the Spark assembly jar.  That mismatch between 16 on
the cluster and 14 on the driver caused us problems with ImmutableLists,
which must have changed in a way between 14 and 16 that aren't binary
compatible in Kryo serialization.
At least that's our current understanding of the bug we experienced.
On Fri, Aug 1, 2014 at 11:13 PM, Patrick Wendell  wrote:
Hi Spark devs,
Several people on the mailing list have seen issues with
FileNotFoundExceptions related to _temporary in the name.  I've personally
observed this several times, as have a few of my coworkers on various Spark
clusters.
Any ideas what might be going on?
I've collected the various stack traces from various mailing list posts and
put their stacktraces and a link back to the original report at
https://issues.apache.org/jira/browse/SPARK-2984
Thanks!
Andrew
Hi Spark devs,
I'm seeing a stacktrace where the classloader that reads from the REPL is
hung, and blocking all progress on that executor.  Below is that hung
thread's stacktrace, and also the stacktrace of another hung thread.
I thought maybe there was an issue with the REPL's JVM on the other side,
but didn't see anything useful in that stacktrace either.
Any ideas what I should be looking for?
Thanks!
Andrew
"Executor task launch worker-0" daemon prio=10 tid=0x00007f780c208000
nid=0x6ae9 runnable [0x00007f78c2eeb000]
   java.lang.Thread.State: RUNNABLE
        at java.net.SocketInputStream.socketRead0(Native Method)
        at java.net.SocketInputStream.read(SocketInputStream.java:152)
        at java.net.SocketInputStream.read(SocketInputStream.java:122)
        at java.io.BufferedInputStream.fill(BufferedInputStream.java:235)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:275)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
        - locked  (a java.io.BufferedInputStream)
        at sun.net.www.http.HttpClient.parseHTTPHeader(HttpClient.java:687)
        at sun.net.www.http.HttpClient.parseHTTP(HttpClient.java:633)
        at
sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1323)
        - locked  (a
sun.net.www.protocol.http.HttpURLConnection)
        at java.net.URL.openStream(URL.java:1037)
        at
org.apache.spark.repl.ExecutorClassLoader.findClassLocally(ExecutorClassLoader.scala:86)
        at
org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:63)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        - locked  (a
org.apache.spark.repl.ExecutorClassLoader)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:270)
        at org.apache.avro.util.ClassUtils.forName(ClassUtils.java:102)
        at org.apache.avro.util.ClassUtils.forName(ClassUtils.java:82)
        at
org.apache.avro.specific.SpecificData.getClass(SpecificData.java:132)
        at
org.apache.avro.specific.SpecificDatumReader.setSchema(SpecificDatumReader.java:69)
        at
org.apache.avro.file.DataFileStream.initialize(DataFileStream.java:126)
        at
org.apache.avro.file.DataFileReader.(DataFileReader.java:97)
        at
org.apache.avro.file.DataFileReader.openReader(DataFileReader.java:59)
        at
org.apache.avro.mapred.AvroRecordReader.(AvroRecordReader.java:41)
        at
org.apache.avro.mapred.AvroInputFormat.getRecordReader(AvroInputFormat.java:71)
        at
org.apache.spark.rdd.HadoopRDD$$anon$1.(HadoopRDD.scala:193)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
And the other threads are stuck on the Class.forName0() method too:
"Executor task launch worker-4" daemon prio=10 tid=0x00007f780c20f000
nid=0x6aed waiting for monitor entry [0x00007f78c2ae8000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:270)
        at org.apache.avro.util.ClassUtils.forName(ClassUtils.java:102)
        at org.apache.avro.util.ClassUtils.forName(ClassUtils.java:79)
        at
org.apache.avro.specific.SpecificData.getClass(SpecificData.java:132)
        at
org.apache.avro.specific.SpecificDatumReader.setSchema(SpecificDatumReader.java:69)
        at
org.apache.avro.file.DataFileStream.initialize(DataFileStream.java:126)
        at
org.apache.avro.file.DataFileReader.(DataFileReader.java:97)
        at
org.apache.avro.file.DataFileReader.openReader(DataFileReader.java:59)
        at
org.apache.avro.mapred.AvroRecordReader.(AvroRecordReader.java:41)
        at
org.apache.avro.mapred.AvroInputFormat.getRecordReader(AvroInputFormat.java:71)
        at
org.apache.spark.rdd.HadoopRDD$$anon$1.(HadoopRDD.scala:193)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:184)
        at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:93)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
asdf
Hi Paul,
I agree that jumping straight from reading N rows from 1 partition to N
rows from ALL partitions is pretty aggressive.  The exponential growth
strategy of doubling the partition count every time seems better -- 1, 2,
4, 8, 16, ... will be much more likely to prevent OOMs than the 1 -> ALL
strategy.
Andrew
On Fri, Aug 22, 2014 at 9:50 AM, pnepywoda  wrote:
Yep, anyone can create a bug at https://issues.apache.org/jira/browse/SPARK
Then if you make a pull request on GitHub and have the bug number in the
header like "[SPARK-1234] Make take() less OOM-prone", then the PR gets
linked to the Jira ticket.  I think that's the best way to get feedback on
a fix.
On Fri, Aug 22, 2014 at 12:52 PM, pnepywoda  wrote:
Filed as https://issues.apache.org/jira/browse/SPARK-3211
On Fri, Aug 22, 2014 at 1:06 PM, Andrew Ash  wrote:
FWIW we use CDH4 extensively and would very much appreciate having a
prebuilt version of Spark for it.
We're doing a CDH 4.4 to 4.7 upgrade across all the clusters now and have
plans for a 5.x transition after that.
On Aug 28, 2014 11:57 PM, "Sean Owen"  wrote:
Hi npanj,
I'm seeing the same exception now on the Spark 1.1.0 release.  Did you ever
get this figured out?
Andrew
On Thu, Aug 21, 2014 at 2:14 PM, npanj  wrote:
I should clarify: I'm not using GraphX, it's a different
application-specific Kryo registrator that causes the same stacktrace
ending in PARSING_ERROR:
com.esotericsoftware.kryo.KryoException: java.io.IOException: failed to
uncompress the chunk: PARSING_ERROR(2)
com.esotericsoftware.kryo.io.Input.fill(Input.java:142)
com.esotericsoftware.kryo.io.Input.require(Input.java:169)
com.esotericsoftware.kryo.io.Input.readInt(Input.java:325)
com.esotericsoftware.kryo.io.Input.readFloat(Input.java:624)
com.esotericsoftware.kryo.serializers.DefaultSerializers$FloatSerializer.read(DefaultSerializers.java:127)
com.esotericsoftware.kryo.serializers.DefaultSerializers$FloatSerializer.read(DefaultSerializers.java:117)
com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:109)
com.esotericsoftware.kryo.serializers.CollectionSerializer.read(CollectionSerializer.java:18)
com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:732)
... my registrator
Ankur from my read of the ticket there's not a root cause identified for
those PARSING_ERROR exceptions in GraphX yet?
Andrew
On Mon, Sep 15, 2014 at 2:10 PM, Ankur Dave  wrote:
Another data point on the 1.1.0 FetchFailures:
Running this SQL command works on 1.0.2 but fails on 1.1.0 due to the
exceptions mentioned earlier in this thread: "SELECT stringCol,
SUM(doubleCol) FROM parquetTable GROUP BY stringCol"
The FetchFailure exception has the remote block manager that failed to
produce the shuffle.  I enabled GC logging and repeated, and the
CoarseGrainedExecutorBackend JVM is just pounding in full GCs:
943.047: [Full GC [PSYoungGen: 5708288K->5536188K(8105472K)] [ParOldGen:
20971043K->20971202K(20971520K)] 26679331K->26507390K(29076992K)
[PSPermGen: 52897K->52897K(57344K)], 48.4514680 secs] [Times: user=602.38
sys=4.43, real=48.44 secs]
991.591: [Full GC [PSYoungGen: 5708288K->5591884K(8105472K)] [ParOldGen:
20971202K->20971044K(20971520K)] 26679490K->26562928K(29076992K)
[PSPermGen: 52897K->52897K(56832K)], 51.8109380 secs] [Times: user=645.44
sys=5.03, real=51.81 secs]
1043.431: [Full GC [PSYoungGen: 5708288K->5606238K(8105472K)] [ParOldGen:
20971044K->20971100K(20971520K)] 26679332K->26577339K(29076992K)
[PSPermGen: 52908K->52908K(56320K)], 85.9367800 secs] [Times: user=1074.29
sys=9.49, real=85.92 secs]
1129.419: [Full GC [PSYoungGen: 5708288K->5634246K(8105472K)] [ParOldGen:
20971100K->20971471K(20971520K)] 26679388K->26605717K(29076992K)
[PSPermGen: 52912K->52912K(55808K)], 52.2114100 secs] [Times: user=652.29
sys=4.94, real=52.21 secs]
1181.671: [Full GC [PSYoungGen: 5708288K->5656389K(8105472K)] [ParOldGen:
20971471K->20971125K(20971520K)] 26679759K->26627514K(29076992K)
[PSPermGen: 52961K->52961K(55296K)], 65.3284620 secs] [Times: user=818.58
sys=6.71, real=65.31 secs]
1247.034: [Full GC [PSYoungGen: 5708288K->5672356K(8105472K)] [ParOldGen:
20971125K->20971417K(20971520K)] 26679413K->26643774K(29076992K)
[PSPermGen: 52982K->52982K(54784K)], 91.2656940 secs] [Times: user=1146.94
sys=9.83, real=91.25 secs]
1338.318: [Full GC [PSYoungGen: 5708288K->5683177K(8105472K)] [ParOldGen:
20971417K->20971364K(20971520K)] 26679705K->26654541K(29076992K)
[PSPermGen: 52982K->52982K(54784K)], 68.9840690 secs] [Times: user=866.72
sys=7.31, real=68.97 secs]
1407.319: [Full GC [PSYoungGen: 5708288K->5691352K(8105472K)] [ParOldGen:
20971364K->20971041K(20971520K)] 26679652K->26662394K(29076992K)
[PSPermGen: 52985K->52985K(54272K)], 58.2522860 secs] [Times: user=724.33
sys=5.74, real=58.24 secs]
1465.572: [Full GC [PSYoungGen: 5708288K->5691382K(8105472K)] [ParOldGen:
20971041K->20971041K(20971520K)] 26679329K->26662424K(29076992K)
[PSPermGen: 52986K->52986K(54272K)], 17.8034740 secs] [Times: user=221.43
sys=0.72, real=17.80 secs]
1483.377: [Full GC [PSYoungGen: 5708288K->5691383K(8105472K)] [ParOldGen:
20971041K->20971041K(20971520K)] 26679329K->26662424K(29076992K)
[PSPermGen: 52987K->52987K(54272K)], 64.3194300 secs] [Times: user=800.32
sys=6.65, real=64.31 secs]
1547.700: [Full GC [PSYoungGen: 5708288K->5692228K(8105472K)] [ParOldGen:
20971041K->20971029K(20971520K)] 26679329K->26663257K(29076992K)
[PSPermGen: 52991K->52991K(53760K)], 54.8107170 secs] [Times: user=681.07
sys=5.41, real=54.80 secs]
1602.519: [Full GC [PSYoungGen: 5708288K->5695801K(8105472K)] [ParOldGen:
20971029K->20971401K(20971520K)] 26679317K->26667203K(29076992K)
[PSPermGen: 52993K->52993K(53760K)], 71.7970690 secs] [Times: user=896.22
sys=7.61, real=71.79 secs]
I repeated the job, this time taking jmap -histos as it went along.  The
last histo I was able to get before the JVM locked up (getting a histo on a
JVM in GC storms is very difficult) is here:
 num     #instances         #bytes  class name
----------------------------------------------
   1:      31437598     2779681704  [B
   2:      62794123     1507058952  scala.collection.immutable.$colon$colon
   3:      31387645     1506606960
 org.apache.spark.sql.catalyst.expressions.Cast
   4:      31387645     1506606960
 org.apache.spark.sql.catalyst.expressions.SumFunction
   5:      31387645     1255505800
 org.apache.spark.sql.catalyst.expressions.Literal
   6:      31387645     1255505800
 org.apache.spark.sql.catalyst.expressions.Coalesce
   7:      31387645     1255505800
 org.apache.spark.sql.catalyst.expressions.MutableLiteral
   8:      31387645     1255505800
 org.apache.spark.sql.catalyst.expressions.Add
   9:      31391224     1004519168  java.util.HashMap$Entry
  10:      31402978      756090664  [Ljava.lang.Object;
  11:      31395785      753498840  java.lang.Double
  12:      31387645      753303480
 [Lorg.apache.spark.sql.catalyst.expressions.AggregateFunction;
  13:      31395808      502332928
 org.apache.spark.sql.catalyst.expressions.GenericRow
  14:      31387645      502202320
 org.apache.spark.sql.catalyst.expressions.Cast$$anonfun$castToDouble$5
  15:           772      234947960  [Ljava.util.HashMap$Entry;
  16:           711      106309792  [I
  17:        106747       13673936  
  18:        106747       12942856  
  19:          8186        9037880  
  20:          8186        8085000  
  21:        222494        5339856  scala.Tuple2
  22:        213731        5129544  java.lang.Long
  23:          6609        4754144  
  24:         36254        3348992  [C
  25:            73        2393232
 [Lscala.concurrent.forkjoin.ForkJoinTask;
  26:         88787        2130888  org.apache.spark.storage.ShuffleBlockId
  27:          4826        1812216  
  28:          8688        1020352  java.lang.Class
  29:         15957         869016  [[I
For me at least a symptom is large GC storms on the executors.  Is anyone
observing these FetchFailures on a consistent basis that doesn't also see
heavy GC?
Hope this helps with debugging.
Andrew
On Mon, Sep 22, 2014 at 3:36 AM, David Rowe  wrote:
Hi Cody,
I wasn't aware there were different versions of the parquet format.  What's
the difference between "raw parquet" and the Hive-written parquet files?
As for your migration question, the approaches I've often seen are
convert-on-read and convert-all-at-once.  Apache Cassandra for example does
both -- when upgrading between Cassandra versions that change the on-disk
sstable format, it will do a convert-on-read as you access the sstables, or
you can run the upgradesstables command to convert them all at once
post-upgrade.
Andrew
On Fri, Oct 3, 2014 at 4:33 PM, Cody Koeninger  wrote:
Hi Gurvinder,
Is there a SPARK ticket tracking the issue you describe?
On Mon, Oct 6, 2014 at 2:44 AM, Gurvinder Singh  On 10/06/2014 08:19 AM, Fairiz Azizi wrote:
Hi Spark devs,
I've heard a few times that keeping support for Java 6 is a priority for
Apache Spark.  Given that Java 6 has been publicly EOL'd since Feb 2013
 and the last
public update was Apr 2013
, why
are we still maintaing support for 6?  The only people using it now must be
paying for the extended support to continue receiving security fixes.
Bumping the lower bound of Java versions up to Java 7 would allow us to
upgrade from Jetty 8 to 9, which is currently a conflict with the
Dropwizard framework and a personal pain point.
Java 6 vs 7 for Spark links:
Try with resources
 for
SparkContext et al
Upgrade to Jetty 9
Warn when not compiling with Java6
Who are the people out there that still need Java 6 support?
Thanks!
Andrew
Hi Jay,
I just came across SPARK-720 Statically guarantee serialization will succeed
 which sounds like exactly
what you're referring to.  Like Reynold I think it's not possible at this
time but it would be good to get your feedback on that ticket.
Andrew
On Sun, Nov 16, 2014 at 4:37 PM, Reynold Xin  wrote:
Filed as https://issues.apache.org/jira/browse/SPARK-4437
On Sun, Nov 16, 2014 at 4:49 PM, Reynold Xin  wrote:
I'm interested in understanding this as well.  One of the main ways Tachyon
is supposed to realize performance gains without sacrificing durability is
by storing the lineage of data rather than full copies of it (similar to
Spark).  But if Spark isn't sending lineage information into Tachyon, then
I'm not sure how this isn't a durability concern.
On Wed, Dec 10, 2014 at 5:47 AM, Jun Feng Liu  wrote:
Jenkins is a really valuable tool for increasing quality of incoming
patches to Spark, but I've noticed that there are often a lot of patches
waiting for testing because they haven't been approved for testing.
Certain users can instruct Jenkins to run on a PR, or add other users to a
whitelist. How does governance work for that list of admins?  Meaning who
is currently on it, and what are the requirements to be on that list?
Can I be permissioned to allow Jenkins to run on certain PRs?  I've often
come across well-intentioned PRs that are languishing because Jenkins has
yet to run on them.
Andrew
Hi Alex,
SparkContext.submitJob() is marked as experimental -- most client programs
shouldn't be using it.  What are you looking to do?
For multiplexing jobs, one thing you can do is have multiple threads in
your client JVM each submit jobs on your SparkContext job.  This is
described here in the docs:
http://spark.apache.org/docs/latest/job-scheduling.html#scheduling-within-an-application
Andrew
On Mon, Dec 22, 2014 at 1:32 PM, Alessandro Baretta  Fellow Sparkers,
Hi Spark devs,
I'm interested in having a committer look at a PR [1] for Mesos, but
there's not an entry for Mesos in the maintainers specialties on the wiki
[2].  Which Spark committers have expertise in the Mesos features?
Thanks!
Andrew
[1] https://github.com/apache/spark/pull/3074
[2]
https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-ReviewProcessandMaintainers
What Reynold is describing is a performance optimization in implementation,
but the semantics of the join (cartesian product plus relational algebra
filter) should be the same and produce the same results.
On Thu, Jan 15, 2015 at 1:36 PM, Reynold Xin  wrote:
In addition to the references you have at the end of the presentation,
there's a great set of practical examples based on the learnings from Qt
posted here: http://www21.in.tum.de/~blanchet/api-design.pdf
Chapter 4's way of showing a principle and then an example from Qt is
particularly instructional.
On Tue, Jan 27, 2015 at 1:05 AM, Reynold Xin  wrote:
Sam, I see your PR was merged -- many thanks for sending it in and getting
it merged!
In general for future reference, the most effective way to contribute is
outlined on this wiki page:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
On Mon, Feb 9, 2015 at 1:04 AM, Akhil Das  You can open a Jira issue pointing this PR to get it processed faster. :)
Hi Spark devs,
I'm creating a streaming export functionality for RDDs and am having some
trouble with large partitions.  The RDD.toLocalIterator() call pulls over a
partition at a time to the driver, and then streams the RDD out from that
partition before pulling in the next one.  When you have large partitions
though, you can OOM the driver, especially when multiple of these exports
are happening in the same SparkContext.
One idea I had was to repartition the RDD so partitions are smaller, but
it's hard to know a priori what the partition count should be, and I'd like
to avoid paying the shuffle cost if possible -- I think repartition to a
higher partition count forces a shuffle.
Is it feasible to rework this so the executor -> driver transfer in
.toLocalIterator is a steady stream rather than a partition at a time?
Thanks!
Andrew
I think a cheap way to repartition to a higher partition count without
shuffle would be valuable too.  Right now you can choose whether to execute
a shuffle when going down in partition count, but going up in partition
count always requires a shuffle.  For the need of having a smaller
partitions to make .toLocalIterator more efficient, no shuffle on increase
of partition count is necessary.
Filed as https://issues.apache.org/jira/browse/SPARK-5997
On Wed, Feb 18, 2015 at 3:21 PM, Mingyu Kim  wrote:
I'm interested in seeing this data transfer occurring over encrypted
communication channels as well.  Many customers require that all network
transfer occur encrypted to prevent the "soft underbelly" that's often
found inside a corporate network.
On Fri, Mar 6, 2015 at 4:20 PM, turp1twin  wrote:
Does the Apache project team have any ability to measure download counts of
the various releases?  That data could be useful when it comes time to
sunset vendor-specific releases, like CDH4 for example.
On Mon, Mar 9, 2015 at 5:34 AM, Mridul Muralidharan  In ideal situation, +1 on removing all vendor specific builds and
Would it be valuable to create a .withColumns([colName], [ColumnObject])
method that adds in bulk rather than iteratively?
Alternatively effort might be better spent in making .withColumn() singular
faster.
On Tue, Jun 2, 2015 at 3:46 PM, Reynold Xin  wrote:
I would guess that many tickets targeted at 1.4.1 were set that way during
the tail end of the 1.4.0 voting process as people realized they wouldn't
make the .0 release in time.  In that case, they were likely aiming for a
1.4.x release, not necessarily 1.4.1 specifically.  Maybe creating a
"1.4.x" target in Jira in addition to 1.4.0, 1.4.1, 1.4.2, etc would make
it more clear that these tickets are targeted at "some 1.4 update release"
rather than specifically "the 1.4.1 update".
On Thu, Jun 25, 2015 at 5:38 AM, Sean Owen  wrote:
Spark 2.x has to be the time for Java 8.
I'd rather increase JVM major version on a Spark major version than on a
Spark minor version, and I'd rather Spark do that upgrade for the 2.x
series than the 3.x series (~2yr from now based on the lifetime of Spark
1.x).  If we wait until the next opportunity for a breaking change to Spark
(3.x) we might be upgrading to Java 9 at that point rather than Java 8.
If Spark users need Java 7 they are free to continue using the 1.x series,
the same way that folks who need Java 6 are free to continue using 1.4
On Thu, Mar 24, 2016 at 11:46 AM, Stephen Boesch  wrote:
