As a slf4j user, FWIW, I think this approach is fine. Just note that you will have to handle log4j classes via reflection if they are not going to always be on the user classpath. Is it sufficient to bundle log4j.properties? no programmatic config mess then. I don't know log4j enough to know if that accomplishes the goal. While we're on the topic, I see that additional slf4j shims are in place to route java.util.logging calls through slf4j and therefore on to log4j or whatever for centralized config. The same ought to be done for commons-logging, no? and we also should really exclude log4j and include log4j-to-slf4j if we really want people to be able to use something besides log4j. I can easily whip up a PR if anyone thinks that's a good idea. Two builds is indeed a pain, since it's an ongoing chore to keep them in sync. For example, I am already seeing that the two do not quite declare the same dependencies (see recent patch). I think publishing artifacts to Maven central should be considered a hard requirement if it isn't already one from the ASF, and it may be? Certainly most people out there would be shocked if you told them Spark is not in the repo at all. And that requires at least maintaining a pom that declares the structure of the project. This does not necessarily mean using Maven to build, but is a reason that removing the pom is going to make this a lot harder for people to consume as a project. Maven has its pros and cons but there are plenty of people lurking around who know it quite well. Certainly it's easier for the Hadoop people to understand and work with. On the other hand, it supports Scala although only via a plugin, which is weaker support. sbt seems like a fairly new, basic, ad-hoc tool. Is there an advantage to it, other than being Scala (which is an advantage)? -- Sean Owen | Director, Data Science | London Thank you for bringing this up. I think the current committers are bravely facing down a flood of PRs, and this (among other things) is a step that needs to be taken to scale up and keep this fun. I'd love to have a separate discussion about more steps, but for here I offer two bits of advice from experience: First, you guys most certainly can and should say 'no' to some changes. It's part of keeping the project coherent. It's always good to try to include all contributions, but, appreciating contributions does not always mean accepting them. I have seen projects turned to mush by the 'anything's welcome' mentality. Push back on contributors to contribute the thing you think is right. Please keep the API succinct, yes. Second, contrib/ modules are problematic. It becomes a ball of legacy code that you still have to keep maintaining to compile and run. In a world of Github, I think 'contrib' stuff just belongs in other repos. I know it sounds harmless to have a contrib, but I think you'd find the consensus here is that contrib is a mistake. $0.02 -- -- Sean Owen | Director, Data Science | London Side point -- "provides" scope is not the same as an exclude. "provides" means, this artifact is used directly by this code (compile time), but it is not necessary to package it, since it will be available from a runtime container. Exclusions make an artifact, that would otherwise be available, unavailable at both compile time and run-time. SBT appears to have syntax for both, just like Maven. Surely these have the same meanings in SBT, and excluding artifacts is accomplished with exclude and excludeAll, as seen in the Spark build? The assembly and shader stuff in Maven is more about controlling exactly how it's put together into an artifact, at the level of files even, to stick a license file in or exclude some data file cruft or rename dependencies. exclusions and shading are necessary evils to be used as sparingly as possible. Dependency graphs get nuts fast here, and Spark is already quite big. (Hence my recent PR to start touching it up -- more coming for sure.) I also favor Maven. I don't the the logic is "because it's common". As Sandy says, it's because of the things that brings: more plugins, easier to consume by more developers, etc. These are, however, just some reasons 'for', and have to be considered against the other pros and cons. The choice of build tools affects users in a way that choice of, say, version control does not.  As long as a usable pom pops out somehow, and artifacts are on Maven Central, I think all users will be happy. Whether SBT or Maven is the single source of truth is not that important (and yes there should be only one). What is the benefit to SBT? It seems like it's mostly replicating Maven functionality. I don't doubt it, just wondering. Is it just that it's in Scala too? They seem pretty equivalent. Don't underestimate the power of Maven plugins though... especially when trying to make all the release artifacts and push it! There are known incantations for this, hard won by years of anguish with Maven. My hunch is it will come down to whether it's easier to make the pom from SBT, or the SBT build from a pom. -- Sean Owen | Director, Data Science | London Hmm, Will Xt*X be positive definite in all cases? For example it's not if X has linearly independent rows? (I'm not going to guarantee 100% that I haven't missed something there.) Even though your data is huge, if it was generated by some synthetic process, maybe it is very low rank? QR decomposition is pretty good here, yes. -- Sean Owen | Director, Data Science | London I do not think there is an advantage in projecting into only the nonnegative quadrant (meaning all of X and Y are nonnegative right?) The argument I have seen is simply interpretability but this doesn't matter here. I think it would be a great exercise to see if the QR decomposition is as fast, and if it produces results that are quantifiably different. Same for these other constraints you suggest. Presumably if you can show an improvement, like that the more general solver is just as performant, that's worth a PR. -- Sean Owen | Director, Data Science | London Repo is fine: http://repository.cloudera.com/artifactory/cloudera-repos/ This artifact has never been in the Cloudera repo actually. As I mentioned I am able to build successfully even if the repo is completely gone, as a result. The Spark build actually does not use the Cloudera repo, as Patrick says. It's there for convenience. Whatever is going on -- you could just remove the repo declaration for the moment. What I *do* see is that if you access it over https, it says the cert is expired: https://repository.cloudera.com/artifactory/cloudera-repos/ I will ask Andrew B about that since that's of course no good. That's consistent with "peer not authenticated" but still not clear why it only fails the build for you. What Maven version? One solution is to use a HTTP URL for the repo. I can suggest another solution to whatever this is, which is to put the Eclipse repo up in the parent pom, and ahead of the Cloudera repo. This causes it to be tried first, which is appropriate. Any +1 for either of those changes? -- Sean Owen | Director, Data Science | London Yes, I'm using Maven 3.2.1. Actually, scratch that, it fails for me too once it gets down into the MQTT module, with a clearer error: sun.security.validator.ValidatorException: PKIX path validation failed: java.security.cert.CertPathValidatorException: timestamp check failed for project org.eclipse.paho:mqtt-client:jar:0.4.0: NotAfter: Fri Mar 14 Yep, expired 3 hours ago. HTTP is a fine workaround for now (or just remove the declaration entirely). I suppose it's slightly better to use HTTPS in the long-run, to ensure the authenticity of the source. Of course the repo's cert is going to be fixed shortly. So I suggest HTTPS should be used for all repos, and it's not right now in the build. (Although, by default, Maven uses an HTTP URL for its own repo, hmm.) But I still think it's more sensible to move repo declarations up into the parent, where their order can be controlled as desired. Child pom repos come after parent repos and that, while it rarely makes any difference, isn't actually desirable. I'll prep a PR but wait for someone else to second a change like that. -- Sean Owen | Director, Data Science | London PS the Cloudera cert issue was cleared up a few hours ago; give it a spin. Much of this sounds related to the memory issue mentioned earlier in this thread. Are you using a build that has fixed that? That would be by far most important here. If the raw memory requirement is 8GB, the actual heap size necessary could be a lot larger -- object overhead, all the other stuff in memory, overheads within the heap allocation, etc. So I would expect total memory requirement to be significantly more than 9GB. Still, this is the *total* requirement across the cluster. Each worker is just loading part of the matrix. If you have 10 workers I would imagine it roughly chops the per-worker memory requirement by 10x. This in turn depends on also letting workers use more than their default amount of memory. May need to increase executor memory here. Separately, I have observed issues with too many files open and lots of /tmp files. You may have to use ulimit to increase the number of open files allowed. That method was added in Java 7. The project is on Java 6, so I think this was just an inadvertent error in a recent PR (it was the 'Spark parquet improvements' one). I'll open a hot-fix PR after looking for other stuff like this that might have snuck in. -- Sean Owen | Director, Data Science | London Will do. I'm just finishing a recompile to check for anything else like this. The reason is because the tests run with Java 7 (like lots of us do including me) so it used the Java 7 classpath and found the class. It's possible to use Java 7 with the Java 6 -bootclasspath. Or just use Java 6. -- Sean Owen | Director, Data Science | London That's a Breeze question, no? you should not need to compile Breeze yourself to compile Spark -- why do that? That method indeed only exists in Java 7. But Breeze seems to target Java 6 as expected: https://github.com/scalanlp/breeze/blob/master/build.sbt#L59 I see this particular line of code was added after the last release: https://github.com/scalanlp/breeze/commit/ff46ddfa66f98b8c8b0ef5b65a6e7a9f86b5a5c4 So it's possible it's an issue lurking in Breeze of just the same form we just saw. It's worth opening an issue since, indeed, I would expect exactly the compile error you see with Java 6. But it should not stop you from building Spark. scala.None certainly isn't new in 2.10.4; it's ancient : http://www.scala-lang.org/api/2.10.3/index.html#scala.None$ Surely this is some other problem? Good call -- indeed that same Files class has a move() method that will try to use renameTo() and then fall back to copy() and delete() if needed for this very reason. I remember that too, and it has been fixed already in master, but maybe it was not included in 0.9.1: https://github.com/apache/spark/blob/master/project/SparkBuild.scala#L367 -- Sean Owen | Director, Data Science | London I remember that too, and it has been fixed already in master, but maybe it was not included in 0.9.1: https://github.com/apache/spark/blob/master/project/SparkBuild.scala#L367 -- Sean Owen | Director, Data Science | London Nobody asked me, and this is a comment on a broader question, not this one, but: In light of a number of recent items about adding more algorithms, I'll say that I personally think an explosion of algorithms should come after the MLlib "core" is more fully baked. I'm thinking of finishing out the changes to vectors and matrices, for example. Things are going to change significantly in the short term as people use the algorithms and see how well the abstractions do or don't work. I've seen another similar project suffer mightily from too many algorithms too early, so maybe I'm just paranoid. Anyway, long-term, I think lots of good algorithms is a right and proper goal for MLlib, myself. Consistent approaches, representations and APIs will make or break MLlib much more than having or not having a particular algorithm. With the plumbing in place, writing the algo is the fun easy part. -- Sean Owen | Director, Data Science | London #1 and #2 are not relevant the issue of jar size. These can be problems in general, but don't think there have been issues attributable to file clashes. Shading has mechanisms to deal with this anyway. #3 is a problem in general too, but is not specific to shading. Where versions collide, build processes like Maven and shading must be used to resolve them. But this happens regardless of whether you shade a fat jar. #4 is a real problem specific to Java 6. It does seem like it will be important to identify and remove more unnecessary dependencies to work around it. But shading per se is not the problem, and it is important to make a packaged jar for the app. What are you proposing? Dependencies to be removed? On this note, non-binding commentary: Releases happen in local minima of change, usually created by internally enforced code freeze. Spark is incredibly busy now due to external factors -- recently a TLP, recently discovered by a large new audience, ease of contribution enabled by Github. It's getting like the first year of mainstream battle-testing in a month. It's been very hard to freeze anything! I see a number of non-trivial issues being reported, and I don't think it has been possible to triage all of them, even. Given the high rate of change, my instinct would have been to release 0.10.0 now. But won't it always be very busy? I do think the rate of significant issues will slow down. Version ain't nothing but a number, but if it has any meaning it's the semantic versioning meaning. 1.0 imposes extra handicaps around striving to maintain backwards-compatibility. That may end up being bent to fit in important changes that are going to be required in this continuing period of change. Hadoop does this all the time unfortunately and gets away with it, I suppose -- minor version releases are really major. (On the other extreme, HBase is at 0.98 and quite production-ready.) Just consider this a second vote for focus on fixes and 1.0.x rather than new features and 1.x. I think there are a few steps that could streamline triage of this flood of contributions, and make all of this easier, but that's for another thread. I might be stating the obvious for everyone, but the issue here is not reflection or the source of the JAR, but the ClassLoader. The basic rules are this. "new Foo" will use the ClassLoader that defines Foo. This is usually the ClassLoader that loaded whatever it is that first referenced Foo and caused it to be loaded -- usually the ClassLoader holding your other app classes. ClassLoaders can have a parent-child relationship. ClassLoaders always look in their parent before themselves. (Careful then -- in contexts like Hadoop or Tomcat where your app is loaded in a child ClassLoader, and you reference a class that Hadoop or Tomcat also has (like a lib class) you will get the container's version!) When you load an external JAR it has a separate ClassLoader which does not necessarily bear any relation to the one containing your app classes, so yeah it is not generally going to make "new Foo" work. Reflection lets you pick the ClassLoader, yes. I would not call setContextClassLoader. I don't think a customer classloader is necessary. Well, it occurs to me that this is no new problem. Hadoop, Tomcat, etc all run custom user code that creates new user objects without reflection. I should go see how that's done. Maybe it's totally valid to set the thread's context classloader for just this purpose, and I am not thinking clearly. It's an Iterator in both Java and Scala. In both cases you need to copy the stream of values into something List-like to sort it. An Iterable would not change that (not sure the API can promise many iterations anyway). If you just want the equivalent of "toArray", you can use a utility method in Commons Collections or Guava. Guava's Lists.newArrayList(Iterator) does nicely, which you can then Collections.sort() with a Comparator and the return its iterator() I dug this up too, remembering a similar question: http://mail-archives.apache.org/mod_mbox/spark-user/201312.mbox/%3C529F819F.3060901@vu.nl%3E http://spark.apache.org/docs/0.9.1/api/core/index.html#org.apache.spark.rdd.PairRDDFunctions It becomes automagically available when your RDD contains pairs. I'd like to resurrect this thread: http://mail-archives.apache.org/mod_mbox/spark-user/201403.mbox/%3C6D657D19-1ECF-4E92-BF15-CC4762EF98BF@thekratos.com%3E Basically when you call this particular Java-flavored overloading of KafkaUtils.createStream: https://github.com/apache/spark/blob/master/external/kafka/src/main/scala/org/apache/spark/streaming/kafka/KafkaUtils.scala#L133 ... you get java.lang.NoSuchMethodException: java.lang.Object.(kafka.utils.VerifiableProperties) at java.lang.Class.getConstructor0(Class.java:2763) at java.lang.Class.getConstructor(Class.java:1693) at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:108) This doesn't appear to be a version issue. It doesn't appear when calling other versions of this method. Other overloadings work (well, have other issues). Something is making it try to instantiate java.lang.Object as if it's a Decoder class. I am wondering about this code at https://github.com/apache/spark/blob/master/external/kafka/src/main/scala/org/apache/spark/streaming/kafka/KafkaUtils.scala#L148 implicit val keyCmd: Manifest[U] = implicitly[Manifest[AnyRef]].asInstanceOf[Manifest[U]] implicit val valueCmd: Manifest[T] = implicitly[Manifest[AnyRef]].asInstanceOf[Manifest[T]] ... where U and T are key/value Decoder types. I don't know enough Scala to fully understand this, but is it possible this causes the reflective call later to lose the type and try to instantiate Object? The AnyRef made me wonder. @tdas I'm hoping you might have some insight as it came in this commit in January: https://github.com/apache/spark/commit/aa99f226a691ddcb4442d60f4cd4908f434cc4ce I'll file a JIRA if it's legitimate; just asking first. This class was introduced in Servlet 3.0. We have in the dependency tree some references to Servlet 2.5 and Servlet 3.0. The latter is a superset of the former. So we standardized on depending on Servlet 3.0. At least, that seems to have been successful in the Maven build, but this is just evidence that the SBT build ends up including Servlet 2.5 dependencies. You shouldn't have to work around it in this way. Let me see if I can debug and propose the right exclusion for SBT. (Related: is the SBT build going to continue to live separately from Maven, or is it going to be auto-generated? that is -- worth fixing this?) Guys I'm struggling to debug some strange behavior in a simple Streaming + Java + Kafka example -- in fact, a simplified version of JavaKafkaWordcount, that is just calling print() on a sequence of messages. Data is flowing, but it only appears to work for a few periods -- sometimes 0 -- before ceasing to call any actions. Sorry for lots of log posting but it may illustrate to someone who knows this better what is happening: Key action in the logs seems to be as follows -- it works a few times: ... 2014-05-30 13:53:50 INFO  ReceiverTracker:58 - Stream 0 received 0 blocks 2014-05-30 13:53:50 INFO  JobScheduler:58 - Added jobs for time 1401454430000 ms ------------------------------------------- Time: 1401454430000 ms ------------------------------------------- 2014-05-30 13:53:50 INFO  JobScheduler:58 - Starting job streaming job 1401454430000 ms.0 from job set of time 1401454430000 ms 2014-05-30 13:53:50 INFO  JobScheduler:58 - Finished job streaming job 1401454430000 ms.0 from job set of time 1401454430000 ms 2014-05-30 13:53:50 INFO  JobScheduler:58 - Total delay: 0.004 s for time 1401454430000 ms (execution: 0.000 s) 2014-05-30 13:53:50 INFO  MappedRDD:58 - Removing RDD 2 from persistence list 2014-05-30 13:53:50 INFO  BlockManager:58 - Removing RDD 2 2014-05-30 13:53:50 INFO  BlockRDD:58 - Removing RDD 1 from persistence list 2014-05-30 13:53:50 INFO  BlockManager:58 - Removing RDD 1 2014-05-30 13:53:50 INFO  KafkaInputDStream:58 - Removing blocks of RDD BlockRDD[1] at BlockRDD at ReceiverInputDStream.scala:69 of time 1401454430000 ms 2014-05-30 13:54:00 INFO  ReceiverTracker:58 - Stream 0 received 0 blocks 2014-05-30 13:54:00 INFO  JobScheduler:58 - Added jobs for time 1401454440000 ms ... Then works with some additional, different output in the logs -- here you see output is flowing too: ... 2014-05-30 13:54:20 INFO  ReceiverTracker:58 - Stream 0 received 2 blocks 2014-05-30 13:54:20 INFO  JobScheduler:58 - Added jobs for time 1401454460000 ms 2014-05-30 13:54:20 INFO  JobScheduler:58 - Starting job streaming job 1401454460000 ms.0 from job set of time 1401454460000 ms 2014-05-30 13:54:20 INFO  SparkContext:58 - Starting job: take at DStream.scala:593 2014-05-30 13:54:20 INFO  DAGScheduler:58 - Got job 1 (take at DStream.scala:593) with 1 output partitions (allowLocal=true) 2014-05-30 13:54:20 INFO  DAGScheduler:58 - Final stage: Stage 1(take at DStream.scala:593) 2014-05-30 13:54:20 INFO  DAGScheduler:58 - Parents of final stage: List() 2014-05-30 13:54:20 INFO  DAGScheduler:58 - Missing parents: List() 2014-05-30 13:54:20 INFO  DAGScheduler:58 - Computing the requested partition locally 2014-05-30 13:54:20 INFO  BlockManager:58 - Found block input-0-1401454458400 locally 2014-05-30 13:54:20 INFO  SparkContext:58 - Job finished: take at DStream.scala:593, took 0.007007 s 2014-05-30 13:54:20 INFO  SparkContext:58 - Starting job: take at DStream.scala:593 2014-05-30 13:54:20 INFO  DAGScheduler:58 - Got job 2 (take at DStream.scala:593) with 1 output partitions (allowLocal=true) 2014-05-30 13:54:20 INFO  DAGScheduler:58 - Final stage: Stage 2(take at DStream.scala:593) 2014-05-30 13:54:20 INFO  DAGScheduler:58 - Parents of final stage: List() 2014-05-30 13:54:20 INFO  DAGScheduler:58 - Missing parents: List() 2014-05-30 13:54:20 INFO  DAGScheduler:58 - Computing the requested partition locally 2014-05-30 13:54:20 INFO  BlockManager:58 - Found block input-0-1401454459400 locally 2014-05-30 13:54:20 INFO  SparkContext:58 - Job finished: take at DStream.scala:593, took 0.002217 s ------------------------------------------- Time: 1401454460000 ms ------------------------------------------- 99,true,-0.11342268416043325 17,false,1.6732879882133793 ... Then keeps repeating the following with no more evidence that the print() action is being called: ... 2014-05-30 13:54:20 INFO  JobScheduler:58 - Finished job streaming job 1401454460000 ms.0 from job set of time 1401454460000 ms 2014-05-30 13:54:20 INFO  MappedRDD:58 - Removing RDD 8 from persistence list 2014-05-30 13:54:20 INFO  JobScheduler:58 - Total delay: 0.019 s for time 1401454460000 ms (execution: 0.015 s) 2014-05-30 13:54:20 INFO  BlockManager:58 - Removing RDD 8 2014-05-30 13:54:20 INFO  BlockRDD:58 - Removing RDD 7 from persistence list 2014-05-30 13:54:20 INFO  BlockManager:58 - Removing RDD 7 2014-05-30 13:54:20 INFO  KafkaInputDStream:58 - Removing blocks of RDD BlockRDD[7] at BlockRDD at ReceiverInputDStream.scala:69 of time 1401454460000 ms 2014-05-30 13:54:20 INFO  MemoryStore:58 - ensureFreeSpace(100) called with curMem=201, maxMem=2290719129 2014-05-30 13:54:20 INFO  MemoryStore:58 - Block input-0-1401454460400 stored as bytes to memory (size 100.0 B, free 2.1 GB) 2014-05-30 13:54:20 INFO  BlockManagerInfo:58 - Added input-0-1401454460400 in memory on 192.168.1.10:60886 (size: 100.0 B, free: 2.1 GB) 2014-05-30 13:54:20 INFO  BlockManagerMaster:58 - Updated info of block input-0-1401454460400 2014-05-30 13:54:20 WARN  BlockManager:70 - Block input-0-1401454460400 already exists on this machine; not re-adding it 2014-05-30 13:54:20 INFO  BlockGenerator:58 - Pushed block input-0-1401454460400 2014-05-30 13:54:21 INFO  MemoryStore:58 - ensureFreeSpace(100) called with curMem=301, maxMem=2290719129 2014-05-30 13:54:21 INFO  MemoryStore:58 - Block input-0-1401454461400 stored as bytes to memory (size 100.0 B, free 2.1 GB) 2014-05-30 13:54:21 INFO  BlockManagerInfo:58 - Added input-0-1401454461400 in memory on 192.168.1.10:60886 (size: 100.0 B, free: 2.1 GB) 2014-05-30 13:54:21 INFO  BlockManagerMaster:58 - Updated info of block input-0-1401454461400 2014-05-30 13:54:21 WARN  BlockManager:70 - Block input-0-1401454461400 already exists on this machine; not re-adding it 2014-05-30 13:54:21 INFO  BlockGenerator:58 - Pushed block input-0-1401454461400 2014-05-30 13:54:22 INFO  MemoryStore:58 - ensureFreeSpace(99) called with curMem=401, maxMem=2290719129 2014-05-30 13:54:22 INFO  MemoryStore:58 - Block input-0-1401454462400 stored as bytes to memory (size 99.0 B, free 2.1 GB) 2014-05-30 13:54:22 INFO  BlockManagerInfo:58 - Added input-0-1401454462400 in memory on 192.168.1.10:60886 (size: 99.0 B, free: 2.1 GB) ... Occasionally it says: ... 2014-05-30 13:54:30 INFO  ReceiverTracker:58 - Stream 0 received 10 blocks 2014-05-30 13:54:30 INFO  JobScheduler:58 - Added jobs for time 1401454470000 ms 2014-05-30 13:54:30 INFO  JobScheduler:58 - Starting job streaming job 1401454470000 ms.0 from job set of time 1401454470000 ms 2014-05-30 13:54:30 INFO  SparkContext:58 - Starting job: take at DStream.scala:593 2014-05-30 13:54:30 INFO  DAGScheduler:58 - Got job 3 (take at DStream.scala:593) with 1 output partitions (allowLocal=true) 2014-05-30 13:54:30 INFO  DAGScheduler:58 - Final stage: Stage 3(take at DStream.scala:593) 2014-05-30 13:54:30 INFO  DAGScheduler:58 - Parents of final stage: List() 2014-05-30 13:54:30 INFO  DAGScheduler:58 - Missing parents: List() 2014-05-30 13:54:30 INFO  DAGScheduler:58 - Computing the requested partition locally 2014-05-30 13:54:30 INFO  BlockManager:58 - Found block input-0-1401454460400 locally 2014-05-30 13:54:30 INFO  SparkContext:58 - Job finished: take at DStream.scala:593, took 0.003993 s 2014-05-30 13:54:30 INFO  SparkContext:58 - Starting job: take at DStream.scala:593 2014-05-30 13:54:30 INFO  DAGScheduler:58 - Got job 4 (take at DStream.scala:593) with 9 output partitions (allowLocal=true) 2014-05-30 13:54:30 INFO  DAGScheduler:58 - Final stage: Stage 4(take at DStream.scala:593) 2014-05-30 13:54:30 INFO  DAGScheduler:58 - Parents of final stage: List() 2014-05-30 13:54:30 INFO  DAGScheduler:58 - Missing parents: List() 2014-05-30 13:54:30 INFO  DAGScheduler:58 - Submitting Stage 4 (MappedRDD[12] at map at MappedDStream.scala:35), which has no missing parents 2014-05-30 13:54:30 INFO  DAGScheduler:58 - Submitting 9 missing tasks from Stage 4 (MappedRDD[12] at map at MappedDStream.scala:35) 2014-05-30 13:54:30 INFO  TaskSchedulerImpl:58 - Adding task set 4.0 with 9 tasks ... Output is definitely continuing to be written to Kafka; you can even see that it seems to be acknolwedging that the stream is seeing more data. The same happens with operations like saving to file. It looks like the action is no longer scheduled. Does that ring any bells? I'm at a loss! I suspect Patrick is right about the cause. The Maven artifact that was released does contain this class (phew) http://search.maven.org/#artifactdetails%7Corg.apache.spark%7Cspark-core_2.10%7C1.0.0%7Cjar As to the hadoop1 / hadoop2 artifact question -- agree that is often done. Here the working theory seems to be to depend on the one artifact (whose API should be identical regardless of dependencies) and then customize the hadoop-client dep. Here, there are not two versions deployed to Maven at all. (The user@ list might be a bit better but I can see why it might look like a dev@ question.) Did you import org.apache.spark.mllib.linalg.Vector ? I think you are picking up Scala's Vector class instead. (BCC dev@) The example is out of date with respect to current Vector class. The zeros() method is on "Vectors". There is not currently a += operation for Vector anymore. To be fair the example doesn't claim this illustrates use of the Spark Vector class but it did work with the now-deprecated Vector. Make sure you still have AccumulableParam imported. You could make a PR to adjust the example to something that works with the newer class once you have it working. Declare this class with "extends Serializable", meaning java.io.Serializable? It correctly points to 1.0.1 but you may need to refresh in your browser to see the update. Agree. You end up with a "core" and a "corer core" to distinguish between and it ends up just being more complicated. This sounds like something that doesn't need a module. Are you setting -Pyarn-alpha? ./sbt/sbt -Pyarn-alpha, followed by "projects", shows it as a module. You should only build yarn-stable *or* yarn-alpha at any given time. I don't remember the modules changing in a while. 'yarn-alpha' is for YARN before it stabilized, circa early Hadoop 2.0.x. 'yarn-stable' is for beta and stable YARN, circa late Hadoop 2.0.x and onwards. 'yarn'is code common to both, so should compile with yarn-alpha. What's the compile error, and are you setting yarn.version? the default is to use hadoop.version, but that defaults to 1.0.4 and there is no such YARN. Unless I missed it, I only see compile errors in yarn-stable, and you are trying to compile vs YARN alpha versions no? This looks like a Jetty version problem actually. Are you bringing in something that might be changing the version of Jetty used by Spark? It depends a lot on how you are building things. Good to specify exactly how your'e building here. Looks like a real problem. I see it too. I think the same workaround found in ClientBase.scala needs to be used here. There, the fact that this field can be a String or String[] is handled explicitly. In fact I think you can just call to ClientBase for this? PR it, I say. CC tmalaska since he touched the line in question. This is a fun one. So, here's the line of code added last week: val channelFactory = new NioServerSocketChannelFactory (Executors.newCachedThreadPool(), Executors.newCachedThreadPool()); Scala parses this as two statements, one invoking a no-arg constructor and one making a tuple for fun. Put it on one line and it's fine. It works with newer Netty since there is a no-arg constructor. It fails with older Netty, which is what you get with older Hadoop. The fix is obvious. I'm away and if nobody beats me to a PR in the meantime, I'll propose one as an addendum to the recent JIRA. Sean * Should be an easy rebase for your PR, so I went ahead just to get this fixed up: https://github.com/apache/spark/pull/1466 This build invocation works just as you have it, for me. (At least, it gets through Hive; Examples fails for a different unrelated reason.) commons-logging 1.0.4 exists in Maven for sure. Maybe there is some temporary problem accessing Maven's repo? Good idea, although it gets difficult in the context of multiple distributions. Say change X is not present in version A, but present in version B. If you depend on X, what version can you look for to detect it? The distribution will return "A" or "A+X" or somesuch, but testing for "A" will give an incorrect answer, and the code can't be expected to look for everyone's "A+X" versions. Actually inspecting the code is more robust if a bit messier. Right, the scenario is, for example, that a class is added in release 2.5.0, but has been back-ported to a 2.4.1-based release. 2.4.1 isn't missing anything from 2.4.1. But a version of "2.4.1" doesn't tell you whether or not the class is there reliably. By the way, I just found there is already such a class, org.apache.hadoop.util.VersionInfo: https://github.com/apache/hadoop-common/blob/release-2.4.1/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/VersionInfo.java It appears to have been around for a long time. Theoretical problems aside, there may be cases where querying the version is a fine and reliable solution. The reason for org.spark-project.hive is that Spark relies on hive-exec, but the Hive project does not publish this artifact by itself, only with all its dependencies as an uber jar. Maybe that's been improved. If so, you need to point at the new hive-exec and perhaps sort out its dependencies manually in your build. Yes, it is published. As of previous versions, at least, hive-exec included all of its dependencies *in its artifact*, making it unusable as-is because it contained copies of dependencies that clash with versions present in other artifacts, and can't be managed with Maven mechanisms. I am not sure why hive-exec was not published normally, with just its own classes. That's why it was copied, into an artifact with just hive-exec code. You could do the same thing for hive-exec 0.13.1. Or maybe someone knows that it's published more 'normally' now. I don't think hive-metastore is related to this question? I am no expert on the Hive artifacts, just remembering what the issue was initially in case it helps you get to a similar solution. How about using a JIRA status like "Documentation Required" to mean "burden's on you to elaborate with a motivation and/or PR". This could both prompt people to do so, and also let one see when a JIRA has been waiting on the reporter for months, rather than simply never been looked at, and should thus time out and be closed. Both of these would probably help the JIRA backlog. You missed the mllib artifact? that would certainly explain it! all I see is core. For any Hadoop 2.4 distro, yes, set hadoop.version but also set -Phadoop-2.4. http://spark.apache.org/docs/latest/building-with-maven.html I think your best bet by far is to consume the Maven build as-is from within Eclipse. I wouldn't try to export a project config from the build as there is plenty to get lost in translation. Certainly this works well with IntelliJ, and by the by, if you have a choice, I would strongly recommend IntelliJ over Eclipse for working with Maven and Scala. (Don't use gen-idea, just open it directly as a Maven project in IntelliJ.) It's definitely just a typo. The ordered categories are A, C, B so the other split can't be A | B, C. Just open a PR. A common approach is to separate unit tests from integration tests. Maven has support for this distinction. I'm not sure it helps a lot though, since it only helps you to not run integration tests all the time. But lots of Spark tests are integration-test-like and are important to run to know a change works. I haven't heard of a plugin to run different test suites remotely on many machines, but I would not be surprised if it exists. The Jenkins servers aren't CPU-bound as far as I can tell. It's that the tests spend a lot of time waiting for bits to start up or complete. That implies the existing tests could be sped up by just running in parallel locally. I recall someone recently proposed this? And I think the problem with that is simply that some of the tests collide with each other, by opening up the same port at the same time for example. I know that kind of problem is being attacked even right now. But if all the tests were made parallel friendly, I imagine parallelism could be enabled and speed up builds greatly without any remote machines. I don't think this will work just by changing the version. Have a look at: https://issues.apache.org/jira/browse/SPARK-2706 Maven is just telling you that there is no version 1.1.0 of yarn-parent, and indeed, it has not been released. To build the branch you would need to "mvn install" to compile and make available local copies of artifacts along the way. (You may have these for 1.1.0-SNAPSHOT locally already). Use Maven, not SBT, for building releases. Yes, master hasn't compiled for me for a few days. It's fixed in: https://github.com/apache/spark/pull/1726 https://github.com/apache/spark/pull/2075 Could a committer sort this out? Sean The examples aren't runnable quite like this. It's intended that they are submitted to a cluster with spark-submit, which would among other things provide Spark at runtime. I think you might get them to run this way if you set master to "local[*]" and indeed made a run profile that also included Spark on the classpath. You would never modify the .iml files anyway. You can change the Maven pom.xml files if you were to need to modify a dependency scope. +1 I tested the source and Hadoop 2.4 release. Checksums and signatures are OK. Compiles fine with Java 8 on OS X. Tests... don't fail any more than usual. FWIW I've also been using the 1.1.0-SNAPSHOT for some time in another project and have encountered no problems. I notice that the 1.1.0 release removes the CDH4-specific build, but adds two MapR-specific builds. Compare with https://dist.apache.org/repos/dist/release/spark/spark-1.0.2/ I commented on the commit: https://github.com/apache/spark/commit/ceb19830b88486faa87ff41e18d03ede713a73cc I'm in favor of removing all vendor-specific builds. This change *looks* a bit funny as there was no JIRA (?) and appears to swap one vendor for another. Of course there's nothing untoward going on, but what was the reasoning? It's best avoided, and MapR already distributes Spark just fine, no? This is a gray area with ASF projects. I mention it as well because it came up with Apache Flink recently (http://mail-archives.eu.apache.org/mod_mbox/incubator-flink-dev/201408.mbox/%3CCANC1h_u%3DN0YKFu3pDaEVYz5ZcQtjQnXEjQA2ReKmoS%2Bye7%3Do%3DA%40mail.gmail.com%3E) Another vendor rightly noted this could look like favoritism. They changed to remove vendor releases. (Copying my reply since I don't know if it goes to the mailing list) Great, thanks for explaining the reasoning. You're saying these aren't going into the final release? I think that moots any issue surrounding distributing them then. This is all I know of from the ASF: https://community.apache.org/projectIndependence.html I don't read it as expressly forbidding this kind of thing although you can see how it bumps up against the spirit. There's not a bright line -- what about Tomcat providing binaries compiled for Windows for example? does that favor an OS vendor? what you want with snapshots and RCs. The only issue there is maybe releasing something different than was in the RC; is that at all confusing? Just needs a note. I think this theoretical issue doesn't exist if these binaries aren't released, so I see no reason to not proceed. The rest is a different question about whether you want to spend time maintaining this profile and candidate. The vendor already manages their build I think and -- and I don't know -- may even prefer not to have a different special build floating around. There's also the theoretical argument that this turns off other vendors from adopting Spark if it's perceived to be too connected to other vendors. I'd like to maximize Spark's distribution and there's some argument you do this by not making vendor profiles. But as I say a different question to just think about over time... (oh and PS for my part I think it's a good thing that CDH4 binaries were removed. I wasn't arguing for resurrecting them) This isn't possible since the two versions of YARN are mutually incompatible at compile-time. However see my comments about how this could be restructured to be a little more standard, and so that IntelliJ would parse it out of the box. Still I imagine it is not worth it if YARN alpha will go away at some point and IntelliJ can easily be told where the extra src/ is. Yes, alpha and stable need to stay in two separate modules. I think this is a little less standard than simply having three modules: common, stable, alpha. All the signatures are correct. The licensing all looks fine. The source builds fine. Now, let me ask about unit tests, since I had a more detailed look, which I should have done before. dev/run-tests fails two tests (1 Hive, 1 Kafka Streaming) for me locally on 1.1.0-rc3. Does anyone else see that? It may be my env. Although I still see the Hive failure on Debian too: [info] - SET commands semantics for a HiveContext *** FAILED *** [info]   Expected Array("spark.sql.key.usedfortestonly=test.val.0", "spark.sql.key.usedfortestonlyspark.sql.key.usedfortestonly=test.val.0test.val.0"), but got Array("spark.sql.key.usedfortestonlyspark.sql.key.usedfortestonly=test.val.0test.val.0", "spark.sql.key.usedfortestonly=test.val.0") (HiveQuerySuite.scala:541) Python lint checks fail for files in python/build/py4j. These aren't Spark files and are only present in this location in the release. The check should simply be updated later to ignore this. Not a blocker. Evidently, the SBT tests pass, usually, in master: https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/SparkPullRequestBuilder/ But Maven tests have not passed in master for a long time: https://amplab.cs.berkeley.edu/jenkins/view/Spark/ I can reproduce this with Maven for 1.1.0-rc3. It feels funny to ship with a repeatable Maven build failure, since Maven is the build of record for release. Whatever is being tested is probably OK since SBT passes, so it need not block release. I'll look for a fix as well. A simple "sbt test" always fails for me, and that just may be because the build is now only meaningful with further configuration. SBT tests are mostly passing if not consistently for all profiles: https://amplab.cs.berkeley.edu/jenkins/view/Spark/  These also sort of feel funny, although nothing seems like an outright blocker. I guess I'll add a non-binding +0 -- none of these are necessarily a blocker but adds up to feeling a bit iffy about the state of tests in the context of a release. Fantastic. As it happens, I just fixed up Mahout's tests for Java 8 and observed a lot of the same type of failure. I'm about to submit PRs for the two issues I identified. AFAICT these 3 then cover the failures I mentioned: https://issues.apache.org/jira/browse/SPARK-3329 https://issues.apache.org/jira/browse/SPARK-3330 https://issues.apache.org/jira/browse/SPARK-3331 I'd argue that none necessarily block a release, since they just represent a problem with test-only code in Java 8, with the test-only context of Jenkins and multiple profiles, and with a trivial configuration in a style check for Python. Should be fixed but none indicate a bug in the release. Hm, are you suggesting that the Spark distribution be a bag of 100 JARs? It doesn't quite seem reasonable. It does not remove version conflicts, just pushes them to run-time, which isn't good. The assembly is also necessary because that's where shading happens. In development, you want to run against exactly what will be used in a real Spark distro. +1 signatures still fine, tests still pass. On Mac OS X I get the following failure but I think it's spurious. Only mentioning it to see if anyone else sees it. It doesn't happen on Linux. [error] Test org.apache.spark.streaming.kafka.JavaKafkaStreamSuite.testKafkaStream failed: junit.framework.AssertionFailedError: expected: but was: [error]     at junit.framework.Assert.fail(Assert.java:50) [error]     at junit.framework.Assert.failNotEquals(Assert.java:287) [error]     at junit.framework.Assert.assertEquals(Assert.java:67) [error]     at junit.framework.Assert.assertEquals(Assert.java:199) [error]     at junit.framework.Assert.assertEquals(Assert.java:205) [error]     at org.apache.spark.streaming.kafka.JavaKafkaStreamSuite.testKafkaStream(JavaKafkaStreamSuite.java:129) [error] Dumb question -- are you using a Spark build that includes the Kinesis dependency? that build would have resolved conflicts like this for you. Your app would need to use the same version of the Kinesis client SDK, ideally. All of these ideas are well-known, yes. In cases of super-common dependencies like Guava, they are already shaded. This is a less-common source of conflicts so I don't think http-client is shaded, especially since it is not used directly by Spark. I think this is a case of your app conflicting with a third-party dependency? I think OSGi is deemed too over the top for things like this. It would help to point to your change. Are you sure it was only docs and are you sure you're rebased, submitting against the right branch? Jenkins is saying you are changing public APIs; it's not reporting test failures. But it could well be a test/Jenkins problem. FWIW consensus from Cloudera folk seems to be that there's no need or demand on this end for YARN alpha. It wouldn't have an impact if it were removed sooner even. It will be a small positive to reduce complexity by removing this support, making it a little easier to develop for current YARN APIs. What has worked for me is to bundle log4j.properties in the root of the application's .jar file, since log4j will look for it there, and configuring log4j will turn off Spark's default log4j configuration. I don't think conf/log4j.properties is going to do anything by itself, but -Dlog4j.configuration=/path/to/file should cause it read a config file on the file system. But for messing with a local build of Spark, just edit core/src/main/resources/org/apache/spark/log4j-defaults.properties and rebuild. Yes I think your syntax is OK; here's some of mine where I turn off a bunch of INFO messages: log4j.rootLogger=INFO, stdout log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.Target=System.out log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=%d{ISO8601} %-5p %c{1}:%L %m%n log4j.logger.org.apache.hadoop=WARN log4j.logger.org.apache.kafka=WARN log4j.logger.kafka=WARN log4j.logger.akka=WARN log4j.logger.org.apache.spark=WARN log4j.logger.org.apache.spark.storage.BlockManager=ERROR log4j.logger.org.apache.zookeeper=WARN log4j.logger.org.eclipse.jetty=WARN log4j.logger.org.I0Itec.zkclient=WARN I'm having trouble getting decision forests to work with categorical features. I have a dataset with a categorical feature with 40 values. It seems to be treated as a continuous/numeric value by the implementation. Digging deeper, I see there is some logic in the code that indicates that categorical features over N values do not work unless the number of bins is at least 2*((2^N - 1) - 1) bins. I understand this as the naive brute force condition, wherein the decision tree will test all possible splits of the categorical value. However, this gets unusable quickly as the number of bins should be tens or hundreds at best, and this requirement rules out categorical values over more than 10 or so features as a result. But, of course, it's not unusual to have categorical features with high cardinality. It's almost common. There are some pretty fine heuristics for selecting 'bins' over categorical features when the number of bins is far fewer than the complete, exhaustive set. Before I open a JIRA or continue, does anyone know what I am talking about, am I mistaken? Is this a real limitation and is it worth pursuing these heuristics? I can't figure out how to proceed with decision forests in MLlib otherwise. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I'm looking at this bit of code in DecisionTreeMetadata ... val maxCategoriesForUnorderedFeature = ((math.log(maxPossibleBins / 2 + 1) / math.log(2.0)) + 1).floor.toInt // Decide if some categorical features should be treated as unordered features, //  which require 2 * ((1 << numCategories - 1) - 1) bins. // We do this check with log values to prevent overflows in case numCategories is large. // The next check is equivalent to: 2 * ((1 << numCategories - 1) - 1) <= maxBins if (numCategories <= maxCategoriesForUnorderedFeature) {unorderedFeatures.add(featureIndex) numBins(featureIndex) = numUnorderedBins(numCategories) } else {numBins(featureIndex) = numCategories } } So if I have a feature with 40 values and less than about a trillion bins, it gets treated as a continuous feature, which is meaningless. It shortly throws an exception though since other parts of the code expect this to be a categorical feature. I think there's a bug here somewhere but wasn't sure whether it was just 'not implemented' yet and so needs a better error message (and should be implemented), or something else preventing this from working as expected. I'll wait a beat to get more info and then if needed open a JIRA. Thanks all. Hm, no I don't think I'm quite right there. There's an issue but that's not quite it. So I have a categorical feature with 40 value, and 300 bins. The error I see in the end is: java.lang.IllegalArgumentException: requirement failed: DTStatsAggregator.getLeftRightFeatureOffsets is for unordered features only, but was called for ordered feature 4. at scala.Predef$.require(Predef.scala:233) at org.apache.spark.mllib.tree.impl.DTStatsAggregator.getLeftRightFeatureOffsets(DTStatsAggregator.scala:143) at org.apache.spark.mllib.tree.DecisionTree$.org$apache$spark$mllib$tree$DecisionTree$$mixedBinSeqOp(DecisionTree.scala:363) ... So this categorical is treated as an ordered feature because you would have to have a load of bins to match the condition in the code I quote. But something isn't expecting that. Is that much worth a JIRA? Hm, but what's the theory of giving meaning to the ordering of the arbitrary categorical values? is that what this is trying to do, or is it in fact ordering by some function of the target (average value for regression, average 1-vs-all entropy for classification)? I suppose I didn't expect to encounter logic like this. Great, we'll confer then. I'm using master / 1.2.0-SNAPSHOT. I'll send some details directly under separate cover. It Gramian is at least positive semidefinite and will be definite if the matrix is non singular, yes. That's usually but not always true. The lambda*I matrix is positive definite, well, when lambda is positive. Adding that makes it definite. At least, lambda=0 could be rejected as invalid. But this goes back to using the Cholesky decomposition. Why not use QR? It doesn't require definite. It should be a little more accurate. On these smallish dense matrices I don't think it is much slower. I have not benchmarked that but I opted for QR in a different implementation and it has worked fine. Now I have to go hunt for how the QR decomposition is exposed in BLAS... Looks like its GEQRF which JBLAS helpfully exposes. Debasish you could try it for fun at least. On Oct 15, 2014 8:06 PM, "Debasish Das"  wrote: Yes, that is exactly what the next 2.x version does. Still in progress but the recommender app and framework are code - complete. It is not even specific to MLlib and could plug in other model build functions. The current 1.x version will not use MLlib. Neither uses Play but is intended to scale just by adding web servers however you usually do. See graphflow too. Briefly, re: Oryx2, since the intent is for users to write their own serving apps, I though JAX-RS would be more familiar to more developers. I don't know how hard/easy REST APIs are in JAX-RS vs anything else but I suspect it's not much different. The interesting design decision that impacts scale is: do you distribute scoring of each request across a cluster? the servlet-based design does not and does everything in-core, in-memory. Pros: Dead simple architecture. Hard to beat for low latency. Anything more complex is big overkill for most models (RDF, k-means) -- except recommenders. Cons: For recommenders, harder to scale since everything is in-memory. And that's a big "but". Oh right, we're talking about the bundled sbt of course. And I didn't know Maven wasn't installed anymore! This one can be resolved, I think, with a bit of help from someone who understands SBT + plugin config: https://issues.apache.org/jira/browse/SPARK-3359 Just a matter of figuring out how to set a property on the plugin. This would make Java 8 javadoc work much more nicely. Minor but useful! --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Here's a crude benchmark on a Linux box (GCE n1-standard-4). zinc gets the assembly build in range of SBT's time. mvn -DskipTests clean package 15:27 (start zinc) 8:18 (rebuild) 7:08 ./sbt/sbt -DskipTests clean assembly 5:10 (start zinc) 5:11 (rebuild) 5:06 The dependencies were already downloaded, and the whole build was cleaned in between. These are smallish in comparison with time to run tests. I admit I didn't run them here in the interest of time and because I assumed zinc doesn't help that. You are right that this is a bit weird compared to the Maven lifecycle semantics. Maven wants assembly to come after tests but here tests want to launch the final assembly as part of some tests. Yes you would not normally have to do this in 2 stages. The pretty standard metric for recommenders is mean average precision, and RankingMetrics will already do that as-is. I don't know that a confusion matrix for this binary classification does much. MAP is effectively an average over all k from 1 to min(# recommendations, # items rated) Getting first recommendations right is more important than the last. This might be a question for Xiangrui. Recently I was using BinaryClassificationMetrics to build an AUC curve for a classifier over a reasonably large number of points (~12M). The scores were all probabilities, so tended to be almost entirely unique. The computation does some operations by key, and this ran out of memory. It's something you can solve with more than the default amount of memory, but in this case, it seemed unuseful to create an AUC curve with such fine-grained resolution. I ended up just binning the scores so there were ~1000 unique values and then it was fine. Does that sound generally useful as some kind of parameter? or am I missing a trick here. Sean --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Agree, just rounding only makes sense if the values are sort of evenly distributed -- in my case they were in 0,1. I will put it on my to-do list to look at, yes. Thanks for the confirmation. Let me crash this thread to suggest this *might* be related to this problem I'm trying to solve: https://issues.apache.org/jira/browse/SPARK-4196 Basically the question there is: this blank Configuration object gets made on the driver in the saveAsNewAPIHadoopFiles call, and seems to need to be serialized to use it in foreachRDD. This fails for me and at least 2 other users I know. But I feel like I am missing something. If you're investigating handling of Configuration when enabling checkpointing with the getOrCreate method, have a look and see if you have any comments vis-a-vis this JIRA. I don't think it's anything to do with AbstractParams. The problem is MovieLensALS$Params, which is a case class without default constructor. It is not Serializable. However you can see it gets used in an RDD function: val fields = line.split("::") if (params.implicitPrefs) {It it just a matter of rejiggering the code to not pass params. Have at it; I'm happy to do it too. Naturally, this sounds great. FWIW my only but significant worry about Spark is scaling up to meet unprecedented demand in the form of questions and contribution. Clarifying responsibility and ownership helps more than it hurts by adding process. This is related but different topic, but, I wonder out loud what this can do to help clear the backlog -- ~*1200* open JIRAs and ~300 open PRs, most of which have de facto already fallen between some cracks. This harms the usefulness of these tools and processes. I'd love to see this translate into triage / closing of most of it by maintainers, and new actions and strategies for increasing 'throughput' in review and/or helping people make better contributions in the first place. (Different topic, indulge me one more reply --) Yes the number of JIRAs/PRs closed is unprecedented too and that deserves big praise. The project has stuck to making all changes and discussion in this public process, which is so powerful. Adjusted for the sheer inbound volume, Spark is doing a much better job than other projects; I would not hold them up as a benchmark of 'good enough', to be honest. JIRA is usually under-managed and it's a pet issue of mine. My motive is that core contributor / committer time is very valuable and in short supply. On the one hand we could use lots more of it to shepherd changes and fix bugs in the core that only the very experienced can. On the other hand, you all deserve time to work on your own changes, build a business, etc. So I harp on JIRA management as a way to save time: - Merging PRs sooner means less rebasing / retesting - Bouncing back bad PRs/JIRAs early teaches everyone what's acceptable as a good PR/JIRA and prevents the noise in the first place - Resolving issues soon prevents duplicates from being filed - Recording 'WontFix' resolutions early heads off repeated discussion/work on out of scope topics I have more concrete ideas about managing this but it's not for now. For now, thanks for zapping some old JIRAs this morning and for endorsing the idea of staying on top of the issue list in general. As a long-time fan I hope I can help from the sidelines by also closing JIRAs I'm all but certain are stale, and review minor PRs to clear the way for maintainers to take on the more important work. I noticed that this doesn't compile: mvn -Pyarn-alpha -Phadoop-0.23 -Dhadoop.version=0.23.7 -DskipTests clean package [error] warning: [options] bootstrap class path not set in conjunction with -source 1.6 [error] /Users/srowen/Documents/spark/network/yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java:26: error: cannot find symbol [error] import org.apache.hadoop.yarn.server.api.AuxiliaryService; [error]                                         ^ [error]   symbol:   class AuxiliaryService [error]   location: package org.apache.hadoop.yarn.server.api [error] /Users/srowen/Documents/spark/network/yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java:27: error: cannot find symbol [error] import org.apache.hadoop.yarn.server.api.ApplicationInitializationContext; [error]                                         ^ ... Should it work? if not shall I propose to enable the service only with -Pyarn? --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hm. Problem is, core depends directly on it: [error] /Users/srowen/Documents/spark/core/src/main/scala/org/apache/spark/SecurityManager.scala:25: object sasl is not a member of package org.apache.spark.network [error] import org.apache.spark.network.sasl.SecretKeyHolder [error]                                 ^ [error] /Users/srowen/Documents/spark/core/src/main/scala/org/apache/spark/SecurityManager.scala:147: not found: type SecretKeyHolder [error] private[spark] class SecurityManager(sparkConf: SparkConf) extends Logging with SecretKeyHolder {[error] ^ [error] /Users/srowen/Documents/spark/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala:29: object RetryingBlockFetcher is not a member of package org.apache.spark.network.shuffle [error] import org.apache.spark.network.shuffle.{RetryingBlockFetcher, BlockFetchingListener, OneForOneBlockFetcher} [error]        ^ [error] /Users/srowen/Documents/spark/core/src/main/scala/org/apache/spark/deploy/worker/StandaloneWorkerShuffleService.scala:23: object sasl is not a member of package org.apache.spark.network [error] import org.apache.spark.network.sasl.SaslRpcHandler [error] ... [error] /Users/srowen/Documents/spark/core/src/main/scala/org/apache/spark/storage/BlockManager.scala:124: too many arguments for constructor ExternalShuffleClient: (x$1: org.apache.spark.network.util.TransportConf, x$2: String)org.apache.spark.network.shuffle.ExternalShuffleClient [error]     new ExternalShuffleClient(SparkTransportConf.fromSparkConf(conf), securityManager, [error]     ^ [error] /Users/srowen/Documents/spark/core/src/main/scala/org/apache/spark/storage/BlockManager.scala:39: object protocol is not a member of package org.apache.spark.network.shuffle [error] import org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo [error]                                         ^ [error] /Users/srowen/Documents/spark/core/src/main/scala/org/apache/spark/storage/BlockManager.scala:214: not found: type ExecutorShuffleInfo [error]     val shuffleConfig = new ExecutorShuffleInfo([error] ... More refactoring needed? Either to support YARN alpha as a separate shuffle module, or sever this dependency? Of course this goes away when yarn-alpha goes away too. Oops, that was my mistake. I moved network/shuffle into yarn, when it's just that network/yarn should be removed from yarn-alpha. That makes yarn-alpha work. I'll run tests and open a quick JIRA / PR for the change. - Tip: when you rebase, IntelliJ will temporarily think things like the Kafka module are being removed. Say 'no' when it asks if you want to remove them. - Can we go straight to Scala 2.11.4? LICENSE and NOTICE are fine. Signature and checksum is fine. I unzipped and built the plain source distribution, which built. However I am seeing a consistent test failure with "mvn -DskipTests clean package; mvn test". In the Hive module: - SET commands semantics for a HiveContext *** FAILED *** Expected Array("spark.sql.key.usedfortestonly=test.val.0", "spark.sql.key.usedfortestonlyspark.sql.key.usedfortestonly=test.val.0test.val.0"), but got Array("spark.sql.key.usedfortestonlyspark.sql.key.usedfortestonly=test.val.0test.val.0", "spark.sql.key.usedfortestonly=test.val.0") (HiveQuerySuite.scala:544) Anyone else seeing this? Ah right. This is because I'm running Java 8. This was fixed in SPARK-3329 (https://github.com/apache/spark/commit/2b7ab814f9bde65ebc57ebd04386e56c97f06f4a#diff-7bfd8d7c8cbb02aa0023e4c3497ee832). Consider back-porting it if other reasons arise, but this is specific to tests and to Java 8. I don't think it's necessary. You're looking at the hadoop-2.4 specialization needed beyond that. The profile sets hadoop.version to 2.4.0 by default, but this can be overridden. Yeah I think someone even just suggested that today in a separate thread? couldn't hurt to just add an example. FWIW I do not see this on master with "mvn -DskipTests clean package". I'm on OS X 10.10 and I build with Java 8 by default. No, the Maven build is the main one.  I would use it unless you have a need to use the SBT build in particular. I thought I'd ask first since there's a good chance this isn't a problem, but, I'm having a problem wherein the first batch that Spark Streaming processes fails (due to an app problem), but then, stop() blocks a very long time. This bit of JobGenerator.stop() executes, since the message appears in the logs: def haveAllBatchesBeenProcessed = {lastProcessedBatch != null && lastProcessedBatch.milliseconds == stopTime } logInfo("Waiting for jobs to be processed and checkpoints to be written") while (!hasTimedOut && !haveAllBatchesBeenProcessed) {Thread.sleep(pollTime) } // ... 10x batch duration wait here, before seeing the next line log: logInfo("Waited for jobs to be processed and checkpoints to be written") I think that lastProcessedBatch is always null since no batch ever succeeds. Of course, for all this code knows, the next batch might succeed and so is there waiting for it. But it should proceed after one more batch completes, even if it failed? JobGenerator.onBatchCompleted is only called for a successful batch. Can it be called if it fails too? I think that would fix it. Should the condition also not be lastProcessedBatch.milliseconds <= stopTime instead of == ? Thanks for any pointers. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Yeah, my comment was mostly reflecting the fact that mvn is what creates the releases and is the 'build of reference', from which the SBT build is generated. The docs were recently changed to suggest that Maven is the default build and SBT is for advanced users. I find Maven plays nicer with IDEs, or at least, IntelliJ. SBT is faster for incremental compilation and better for anyone who knows and can leverage SBT's model. If someone's new to it all, I dunno, they're likelier to have fewer problems using Maven to start? YMMV. I use randomSplit to make a train/CV/test set in one go. It definitely produces disjoint data sets and is efficient. The problem is you can't do it by key. I am not sure why your subtract does not work. I suspect it is because the values do not partition the same way, or they don't evaluate equality in the expected way, but I don't see any reason why. Tuples work as expected here. +1 (non binding) Signatures and license looks good. I built the plain-vanilla distribution and ran tests. While I still see the Java 8 + Hive test failure, I think we've established this is ignorable. 10M is tiny compared to all of the overhead of running a lot complex Scala based app in a JVM. I think you may be bumping up against practical minimum sizes and that you may find it is not really the data size? I don't think it really scales down this far. Stephen you can publish the artifact to your repo under a different name, right? IIRC Maven will take care of the pom change along the way. Yes you would not ever want to mess with changing an artifact after it's published. http://maven.apache.org/plugins/maven-install-plugin/examples/specific-local-repo.html That's how I thought people did this, if they needed to do more than test a tarball. +1 non-binding I downloaded the plain source artifact and verified signatures. Licesning and artifact looks OK. All tests pass with a plain package/test run, and also with -Phive -Pyarn -Dhadop.version=2.5.2, which covers two fairly different scenarios. I'm having no problems with the build or zinc on my Mac. I use zinc from "brew install zinc". Yes, they are compiled to classes in JVM bytecode just the same. You may find the generated code from Scala looks a bit strange and uses Scala-specific classes, but it's certainly possible to treat them like other Java classes. You just run it once with "zinc -start" and leave it running as a background process on your build machine. You don't have to do anything for each build. (Nit: CDH *5.1.x*, including 5.1.3, is derived from Hadoop 2.3.x. 5.3 is based on 2.5.x) https://github.com/apache/spark/blob/master/network/shuffle/src/main/java/org/apache/spark/network/shuffle/protocol/BlockTransferMessage.java#L70 public byte[] toByteArray() {ByteBuf buf = Unpooled.buffer(encodedLength()); buf.writeByte(type().id); encode(buf); assert buf.writableBytes() == 0 : "Writable bytes remain: " + buf.writableBytes(); return buf.array(); } Running the Java tests at last might have turned up a little bug here, but wanted to check. This makes a buffer to hold enough bytes to encode the message. But it writes 1 byte, plus the message. This makes the buffer expand, and then does have nonzero capacity afterwards, so the assert fails. So just needs a "+ 1" in the size? --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Yep, will do. The test does catch it -- it's just not being executed. I think I have a reasonable start on re-enabling surefire + Java tests for SPARK-4159. I'm not so sure about SBT, but I'm looking at the output now and do not see things like JavaAPISuite being run. I see them compiled. That I'm not as sure how to fix. I think I have a solution for Maven on SPARK-4159. Oops, yes I see Java tests run with SBT now. You're right, it must be because of the assertion. I can try to add '-ea' to the SBT build as a closely-related change for SPARK-4159. FWIW this error is the only one I saw once the Maven tests ran the Java tests. Signatures and checksums are OK. License and notice still looks fine. The plain-vanilla source release compiles with Maven 3.2.1 and passes tests, on OS X 10.10 + Java 8. Tag 1.2.0 is older than 1.2.0-rc2. I wonder if it just didn't get updated. I assume it's going to be 1.2.0-rc2 plus a few commits related to the release process. I can download it. Make sure you refresh the page, maybe, so that it shows the 1.2.0 download as an option. I believe spark-yarn does not exist from 1.2 onwards. Have a look at spark-network-yarn for where some of that went, I believe. I just tried the exact same command and do not see any error. Maybe you can make sure you're starting from a clean extraction of the distro, and check your environment. I'm on OSX, Maven 3.2, Java 8 but I don't know that any of those would be relevant. It was not intended to be a public API but there is a request to keep publishing it as a developer API: https://issues.apache.org/jira/browse/SPARK-4923 It means a test failed but you have not shown the test failure. This would have been logged earlier. You would need to say how you ran tests too. The tests for 1.2.0 pass for me on several common permutations. Yeah, I hit this too. IntelliJ picks this up from the build but then it can't run its own scalac with this plugin added. clear the "Additional compiler options" field. It will work then although the option will come back when the project reimports. Right now I don't know of a better fix. There's another recent open question about updating IntelliJ docs: https://issues.apache.org/jira/browse/SPARK-5136  Should this stuff go in the site docs, or wiki? I vote for wiki I suppose and make the site docs point to the wiki. I'd be happy to make wiki edits if I can get permission, or propose this text along with other new text on the JIRA. I remember seeing this too, but it seemed to be transient. Try compiling again. In my case I recall that IJ was still reimporting some modules when I tried to build. I don't see this error in general. Guava is shaded, although one class is left in its original package. This shouldn't have anything to do with Spark's package or namespace though. What are you saying is in com/google/guava? You can un-skip the install plugin with -Dmaven.install.skip=false Yes I've seen that error in the past too, and was just talking to Imran the other day about it. I thought it only occurred when the hive module was enabled, which I don't enable. The problem is that the plugin that causes an error in IntelliJ for scalac is what parses these values.* I think he got it to work with this change: http://stackoverflow.com/questions/26788367/quasiquotes-in-intellij-14/26908554#26908554 If that works for you let's put it on the wiki. * probably an ignorant question but is this feature important enough to warrant the extra scala compiler plugin? the quasiquotes syntax I mean. Agree, I think this can / should be fixed with a slightly more conservative version of https://github.com/apache/spark/pull/3938 related to SPARK-5108. My concern would mostly be maintenance. It adds to an already very complex build. It only assists developers who are a small audience. What does this provide, concretely? If the goal is a reproducible test environment then I think that is what Jenkins is. Granted you can only ask it for a test. But presumably you get the same result if you start from the same VM image as Jenkins and run the same steps. I bet it is not hard to set up and maintain. I bet it is easier than a VM. But unless Jenkins is using it aren't we just making another different standard build env in an effort to standardize? If it is not the same then it loses value as being exactly the same as the reference build env. Has a problem come up that this solves? If the goal is just easing developer set up then what does a Docker image do - what does it set up for me? I don't know of stuff I need set up on OS X for me beyond the IDE. Sure, can Jenkins use this new image too? If not then it doesn't help with reproducing a Jenkins failure, most of which even Jenkins can't reproduce. But if it does and it can be used for builds then that does seem like it is reducing rather than increasing environment configurations which is good. That's different from developer setup. Surely that is a large number of permutations to maintain? Win, Linux, OS X at least. Whereas I have not needed nor probably would want a whole second tool chain on my machine for Spark... for me it doesn't solve a problem. So just wondering how many people this will help as devs versus some apparent big maintenance overhead. Although if this could replace the scripts that try to fetch sbt and mvn et al that alone could save enough complexity to make it worthwhile. Would it do that? Did you use spark.files.userClassPathFirst = true? it's exactly for this kind of problem. You certainly do not need yo build Spark as root. It might clumsily overcome a permissions problem in your local env but probably causes other problems. I think there are several signing / hash issues that should be fixed before this release. Hashes: http://issues.apache.org/jira/browse/SPARK-5308 https://github.com/apache/spark/pull/4161 The hashes here are correct, but have two issues: As noted in the JIRA, the format of the hash file is "nonstandard" -- at least, doesn't match what Maven outputs, and apparently which tools like Leiningen expect, which is just the hash with no file name or spaces. There are two ways to fix that: different command-line tools (see PR), or, just ask Maven to generate these hashes (a different, easy PR). However, is the script I modified above used to generate these hashes? It's generating SHA1 sums, but the output in this release candidate has (correct) SHA512 sums. This may be more than a nuisance, since last time for some reason Maven Central did not register the project hashes. http://search.maven.org/#artifactdetails%7Corg.apache.spark%7Cspark-core_2.10%7C1.2.0%7Cjar does not show them but they exist: http://www.us.apache.org/dist/spark/spark-1.2.0/ It may add up to a problem worth rooting out before this release. Signing: As noted in https://issues.apache.org/jira/browse/SPARK-5299 there are two signing keys in https://people.apache.org/keys/committer/pwendell.asc (9E4FE3AF, 00799F7E) but only one is in http://www.apache.org/dist/spark/KEYS However, these artifacts seem to be signed by FC8ED089 which isn't in either. Details details, but I'd say non-binding -1 at the moment. Got it. Ignore the SHA512 issue since these aren't somehow expected by a policy or Maven to be in a certain format. Just wondered if the difference was intended. The Maven way of generated the SHA1 hashes is to set this on the install plugin, AFAIK, although I'm not sure if the intent was to hash files that Maven didn't create: As for the key issue, I think it's just a matter of uploading the new key in both places. We should all of course test the release anyway. +1 (nonbinding). I verified that all the hash / signing items I mentioned before are resolved. The source package compiles on Ubuntu / Java 8. I ran tests and the passed. Well, actually I see the same failure I've seeing locally on OS X and on Ubuntu for a while, but I think nobody else has seen this? MQTTStreamSuite: - mqtt input stream *** FAILED *** org.eclipse.paho.client.mqttv3.MqttException: Too many publishes in progress at org.eclipse.paho.client.mqttv3.internal.ClientState.send(ClientState.java:423) Doesn't happen on Jenkins. If nobody else is seeing this, I suspect it is something perhaps related to my env that I haven't figured out yet, so should not be considered a blocker. I don't see how this would relate to the problem in the OP? the assemblies build fine already as far as I can tell. Your new error may be introduced by your change. How do you mean you run LogQuery? you would run these using the run-example script rather than in IntelliJ. +1 The signatures are still fine. Building for Hadoop 2.6 with YARN works; tests pass, except that MQTTStreamSuite, which we established is a test problem and already fixed in master. When you are describing an error, you should say what the error is. Here I'm pretty sure it says there is no such member of Vector, right? You explicitly made the type of sv2 Vector and not SparseVector, and the trait does not have any indices member. No it's not a problem, and I think the compiler tells you what's happening in this case. Despite its name, stderr is frequently used as the destination for anything that's not the output of the program, which includes log messages. That way, for example, you can redirect the output of such a program to capture its result without also capturing log or error messages, which will still just print to the console. Thanks all, I appreciate the vote of trust. I'll do my best to help keep JIRA and commits moving along, and am ramping up carefully this week. Now get back to work reviewing things! One thing Marcelo pointed out to me is that the // style does not interfere with commenting out blocks of code with /* */, which is a small good thing. I am also accustomed to // style for multiline, and reserve /** */ for javadoc / scaladoc. Meaning, seeing the /* */ style inline always looks a little funny to me. I've wasted no time in wielding the commit bit to complete a number of small, uncontroversial changes. I wouldn't commit anything that didn't already appear to have review, consensus and little risk, but please let me know if anything looked a little too bold, so I can calibrate. Anyway, I'd like to continue some small house-cleaning by improving the state of JIRA's metadata, in order to let it give us a little clearer view on what's happening in the project: a. Add Component to every (open) issue that's missing one b. Review all Critical / Blocker issues to de-escalate ones that seem obviously neither c. Correct open issues that list a Fix version that has already been released d. Close all issues Resolved for a release that has already been released The problem with doing so is that it will create a tremendous amount of email to the list, like, several hundred. It's possible to make bulk changes and suppress e-mail though, which could be done for all but b. Better to suppress the emails when making such changes? or just not bother on some of these? --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org This is a straw poll to assess whether there is support to keep and fix, or remove, the Debian packaging-related config in Spark. I see several oldish outstanding JIRAs relating to problems in the packaging: https://issues.apache.org/jira/browse/SPARK-1799 https://issues.apache.org/jira/browse/SPARK-2614 https://issues.apache.org/jira/browse/SPARK-3624 https://issues.apache.org/jira/browse/SPARK-4436 (and a similar idea about making RPMs) https://issues.apache.org/jira/browse/SPARK-665 The original motivation seems related to Chef: https://issues.apache.org/jira/browse/SPARK-2614?focusedCommentId=14070908&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14070908 Mark's recent comments cast some doubt on whether it is essential: https://github.com/apache/spark/pull/4277#issuecomment-72114226 and in recent conversations I didn't hear dissent to the idea of removing this. Is this still useful enough to fix up? All else equal I'd like to start to walk back some of the complexity of the build, but I don't know how all-else-equal it is. Certainly, it sounds like nobody intends these to be used to actually deploy Spark. I don't doubt it's useful to someone, but can they maintain the packaging logic elsewhere? --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Old releases can't be changed, but new ones can. This was merged into the 1.3 branch for the upcoming 1.3.0 release. If you really had to, you could do some surgery on existing distributions to swap in/out Jackson. What about this straw man proposal: deprecate in 1.3 with some kind of message in the build, and remove for 1.4? And add a pointer to any third-party packaging that might provide similar functionality? Seems to work OK for me on OS X. I ran ./sbin/start-all.sh from the root. Both processes say they started successfully. Patrick and I were chatting about how to handle several issues which clearly need a fix, and are easy, but can't be implemented until a next major release like Spark 2.x since it would change APIs. Examples: https://issues.apache.org/jira/browse/SPARK-3266 https://issues.apache.org/jira/browse/SPARK-3369 https://issues.apache.org/jira/browse/SPARK-4819 We could simply make version 2.0.0 in JIRA. Although straightforward, it might imply that release planning has begun for 2.0.0. The version could be called "2+" for now to better indicate its status. There is also a "Later" JIRA resolution. Although resolving the above seems a little wrong, it might be reasonable if we're sure to revisit "Later", well, at some well defined later. The three issues above risk getting lost in the shuffle. We also wondered whether using "Later" is good or bad. It takes items off the radar that aren't going to be acted on anytime soon -- and there are lots of those right now. It might send a message that these will be revisited when they are even less likely to if resolved. Any opinions? --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Let me start with a version "2+" tag and at least write in the description that it's only for issues that are clearly to be fixed, but must wait until 2.x. There's no particular reason you have to remove the embedded Jetty server, right? it doesn't prevent you from using it inside another app that happens to run in Tomcat. You won't be able to switch it out without rewriting a fair bit of code, no, but you don't need to. I do not think it makes sense to make the web server configurable. Mostly because there's no real problem in running an HTTP service internally based on Netty while you run your own HTTP service based on something else like Tomcat. What's the problem? On OS X and Ubuntu I see the following test failure in the source release for 1.3.0-RC1: UISeleniumSuite: *** RUN ABORTED *** java.lang.NoClassDefFoundError: org/w3c/dom/ElementTraversal ... Patrick this link gives a 404: https://people.apache.org/keys/committer/pwendell.asc Finally, I already realized I failed to get the fix for https://issues.apache.org/jira/browse/SPARK-5669 correct, and that has to be correct for the release. I'll patch that up straight away, sorry. I believe the result of the intended fix is still as I described in SPARK-5669, so there is no bad news there. A local test seems to confirm it and I'm waiting on Jenkins. If it's all good I'll merge that fix. So, that much will need a new release, I apologize. Please keep testing anyway! Otherwise, I verified the signatures are correct, licenses are correct, compiles on OS X and Ubuntu 14. Sure, but you are not using Netty at all. It's invisible to you. It's not as if you have to set up and maintain a Jetty container. I don't think your single platform for your apps is relevant. You can turn off the UI, but as Reynold said, the HTTP servers are also part of the core data transport functionality and you can't turn that off. It's not merely unsupported to swap this out with an arbitrary container, it's not clear it would work with Tomcat without re-integrating with its behavior and tuning. But it also shouldn't matter to anyone. It sounds like your computation just isn't CPU bound, right? or maybe that only some stages are. It's not clear what work you are doing beyond the core LR. Stages don't wait on each other unless one depends on the other. You'd have to clarify what you mean by running stages in parallel, like what are the interdependencies. Yes that makes sense, but it doesn't make the jobs CPU-bound. What is the bottleneck? the model building or other stages? I would think you can get the model building to be CPU bound, unless you have chopped it up into really small partitions. I think it's best to look further into what stages are slow, and what it seems to be spending time on -- GC? I/O? No, you usually run Spark apps via the spark-submit script, and the Spark machinery is already deployed on a cluster. Although it's possible to embed the driver and get it working that way, it's not supported. Yes my understanding from Patrick's comment is that this RC will not be released, but, to keep testing. There's an implicit -1 out of the gates there, I believe, and so the vote won't pass, so perhaps that's why there weren't further binding votes. I'm sure that will be formalized shortly. FWIW here are 10 issues still listed as blockers for 1.3.0: SPARK-5910 DataFrame.selectExpr("col as newName") does not work SPARK-5904 SPARK-5166 DataFrame methods with varargs do not work in Java SPARK-5873 Can't see partially analyzed plans SPARK-5546 Improve path to Kafka assembly when trying Kafka Python API SPARK-5517 SPARK-5166 Add input types for Java UDFs SPARK-5463 Fix Parquet filter push-down SPARK-5310 SPARK-5166 Update SQL programming guide for 1.3 SPARK-5183 SPARK-5180 Document data source API SPARK-3650 Triangle Count handles reverse edges incorrectly SPARK-3511 Create a RELEASE-NOTES.txt file in the repo So you mean that the script is checking for this error, and takes it as a sign that you compiled with java 6. Your command seems to confirm that reading the assembly jar does fail on your system though. What version does the jar command show? are you sure you don't have JRE 7 but JDK 6 installed? I agree with that. My anecdotal impression is that Hadoop 1.x usage out there is maybe a couple percent, and so we should shift towards 2.x at least as defaults. I think we will have to fix https://issues.apache.org/jira/browse/SPARK-5143 as well before the final 1.3.x. But yes everything else checks out for me, including sigs and hashes and building the source release. I have been following JIRA closely and am not aware of other blockers besides the ones already identified. This has some disadvantage for Java, I think. You can't switch on an object defined like this, but you can with an enum. And although the scala compiler understands that the set of values is fixed because of 'sealed' and so can warn about missing cases, the JVM won't know this, and can't do the same. There are still three JIRAs marked as blockers for 1.3.0: SPARK-5310 Update SQL programming guide for 1.3 SPARK-5183 Document data source API SPARK-6128 Update Spark Streaming Guide for Spark 1.3 As a matter of hygiene, let's either mark them resolved if they're resolved, or push them / deprioritize them. Signatures look good, source compiles with a Hadoop-2.6 + YARN + Hive-flavored build, for me. On OS X and Ubuntu, I still observe the same test failure as in the first RC, but agree this isn't a blocker: UISeleniumSuite: *** RUN ABORTED *** java.lang.NoClassDefFoundError: org/w3c/dom/ElementTraversal ... On both, I also see a few Hive tests fail, like the following: - udf_std *** FAILED *** Results do not match for udf_std: DESCRIBE FUNCTION EXTENDED std == Parsed Logical Plan == HiveNativeCommand DESCRIBE FUNCTION EXTENDED std == Analyzed Logical Plan == HiveNativeCommand DESCRIBE FUNCTION EXTENDED std == Optimized Logical Plan == HiveNativeCommand DESCRIBE FUNCTION EXTENDED std == Physical Plan == ExecutedCommand (HiveNativeCommand DESCRIBE FUNCTION EXTENDED std) Code Generation: false == RDD == result !== HIVE - 2 row(s) ==                                         == CATALYST - 2 row(s) == std(x) - Returns the standard deviation of a set of numbers std(x) - Returns the standard deviation of a set of numbers !Synonyms: stddev_pop, stddev Synonyms: stddev, stddev_pop (HiveComparisonTest.scala:384) Before I give a +1 I wanted to see if anyone sees these test failures too, and/or believes they're ignorable for some reason. I also want to resolve the open blocker JIRAs. Given the title and tagging, it sounds like there could be some must-have doc changes to go with what is being released as 1.3. It can be finished later, and published later, but then the docs source shipped with the release doesn't match the site, and until then, 1.3 is released without some "must-have" docs for 1.3 on the site. The real question to me is: are there any further, absolutely essential doc changes that need to accompany 1.3 or not? If not, just resolve these. If there are, then it seems like the release has to block on them. If there are some docs that should have gone in for 1.3, but didn't, but aren't essential, well I suppose it bears thinking about how to not slip as much work, but it doesn't block. I think Documentation issues certainly can be a blocker and shouldn't be specially ignored. BTW the UISeleniumSuite issue is a real failure, but I do not think it is serious: http://issues.apache.org/jira/browse/SPARK-6205  It isn't a regression from 1.2.x, but only affects tests, and only affects a subset of build profiles. Although the problem is small, especially if indeed the essential docs changes are following just a couple days behind the final release, I mean, why the rush if they're essential? wait a couple days, finish them, make the release. Answer is, I think these changes aren't actually essential given the comment from tdas, so: just mark these Critical? (although ... they do say they're changes for the 1.3 release, so kind of funny to get to them for 1.3.x or 1.4, but that's not important now.) I thought that Blocker really meant Blocker in this project, as I've been encouraged to use it to mean "don't release without this." I think we should use it that way. Just thinking of it as "extra Critical" doesn't add anything. I don't think Documentation should be special-cased as less important, and I don't think there's confusion if Blocker means what it says, so I'd 'fix' that way. If nobody sees the Hive failure I observed, and if we can just zap those "Blockers" one way or the other, +1 Yeah, interesting question of what is the better default for the single set of artifacts published to Maven. I think there's an argument for Hadoop 2 and perhaps Hive for the 2.10 build too. Pros and cons discussed more at https://issues.apache.org/jira/browse/SPARK-5134 https://github.com/apache/spark/pull/3917 Ah. I misunderstood that Matei was referring to the Scala 2.11 tarball at http://people.apache.org/~pwendell/spark-1.3.0-rc3/ and not the Maven artifacts. Patrick I see you just commented on SPARK-5134 and will follow up there. Sounds like this may accidentally not be a problem. On binary tarball releases, I wonder if anyone has an opinion on my opinion that these shouldn't be distributed for specific Hadoop *distributions* to begin with. (Won't repeat the argument here yet.) That resolves this n x m explosion too. Vendors already provide their own distribution, yes, that's their job. Yeah it's not much overhead, but here's an example of where it causes a little issue. I like that reasoning. However, the released builds don't track the later versions of Hadoop that vendors would be distributing -- there's no Hadoop 2.6 build for example. CDH4 is here, but not the far-more-used CDH5. HDP isn't present at all. The CDH4 build doesn't actually work with many CDH4 versions. I agree with the goal of maximizing the reach of Spark, but I don't know how much these builds advance that goal. Anyone can roll-their-own exactly-right build, and the docs and build have been set up to make that as simple as can be expected. So these aren't *required* to let me use latest Spark on distribution X. I had thought these existed to sorta support 'legacy' distributions, like CDH4, and that build was justified as a quasi-Hadoop-2.0.x-flavored build. But then I don't understand what the MapR profiles are for. I think it's too much work to correctly, in parallel, maintain any customizations necessary for any major distro, and it might be best to do not at all than to do it incompletely. You could say it's also an enabler for distros to vary in ways that require special customization. Maybe there's a concern that, if lots of people consume Spark on Hadoop, and most people consume Hadoop through distros, and distros alone manage Spark distributions, then you de facto 'have to' go through a distro instead of get bits from Spark? Different conversation but I think this sort of effect does not end up being a negative. Well anyway, I like the idea of seeing how far Hadoop-provided releases can help. It might kill several birds with one stone. It's explained at https://spark.apache.org/docs/latest/programming-guide.html and it's configuration at https://spark.apache.org/docs/latest/configuration.html  Have a read over all the docs first. Yes, you should always find working bits at Apache no matter what -- though 'no matter what' really means 'as long as you use Hadoop distro compatible with upstream Hadoop'. Even distros have a strong interest in that, since the market, the 'pie', is made large by this kind of freedom at the core. If tso, then no vendor-specific builds are needed, only some Hadoop-release-specific ones. So a Hadoop 2.6-specific build could be good (although I'm not yet clear if there's something about 2.5 or 2.6 that needs a different build.) I take it that we already believe that, say, the "Hadoop 2.4" build works with CDH5, so no CDH5-specific build is provided by Spark. If a distro doesn't work with stock Spark, then it's either something Spark should fix (e.g. use of a private YARN API or something), or it's something the distro should really fix because it's incompatible. Could we maybe rename the "CDH4" build then, as it doesn't really work with all CDH4, to be a "Hadoop 2.0.x build"? That's been floated before. And can we remove the MapR builds -- or else can someone explain why these exist separately from a Hadoop 2.3 build? I hope it is not *because* they are somehow non-standard. And shall we first run down why Spark doesn't fully work on HDP and see if it's something that Spark or HDP needs to tweak, rather than contemplate another binary? or, if so, can it simply be called a "Hadoop 2.7 + YARN whatever" build and not made specific to a vendor, even if the project has to field another tarball combo for a vendor? Maybe we are saying almost the same thing. I'm +1 as I have not heard of any one else seeing the Hive test failure, which is likely a test issue rather than code issue anyway, and not a blocker. (I have been able to push over the last few hours and see the commits in github) Is the release certain enough that we can resume merging into branch-1.3 at this point? I have a number of back-ports queued up and didn't want to merge in case another last RC was needed. I see a few commits to the branch though. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Yeah, I'm guessing that is all happening quite literally as we speak. The Apache git tag is the one of reference: https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4aaf48d46d13129f0f9bdafd771dd80fe568a7dc Open season on 1.3 branch then... Here's the sentence: As part of stabilizing the Spark SQL API, the SchemaRDD class has been extended renamed to DataFrame. Yes, I can remove the word 'extended'Yeah the fully realized #4, which gets back the ability to use it in switch statements (? in Scala but not Java?) does end up being kind of huge. I confess I'm swayed a bit back to Java enums, seeing what it involves. The hashCode() issue can be 'solved' with the hash of the String representation. So far, my rule of thumb has been: - Don't back-port new features or improvements in general, only bug fixes - Don't back-port minor bug fixes - Back-port bug fixes that seem important enough to not wait for the next minor release - Back-port site doc changes to the release most likely to go out next, to make it a part of the next site publish But, how far should back-ports go, in general? If the last minor release was 1.N, then to branch 1.N surely. Farther back is a question of expectation for support of past minor releases. Given the pace of change and time available, I assume there's not much support for continuing to use release 1.(N-1) and very little for 1.(N-2). Concretely: does anyone expect a 1.1.2 release ever? a 1.2.2 release? It'd be good to hear the received wisdom explicitly. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org It's just the standard Apache JIRA, nothing separate. I'd say JIRA is used to track issues, bugs, features, but Github is where the concrete changes to implement those things are discussed and merged. So for a non-trivial issue, you'd want to describe the issue in general in JIRA, and then open a PR with the JIRA name in the title to propose the code change, rather than submit a patch or something. The license issue is with libgfortran, rather than OpenBLAS. (FWIW I am going through the motions to get OpenBLAS set up by default on CDH in the near future, and the hard part is just handling libgfortran.) I built from the head of branch-1.2 and spark-core compiled correctly with your exact command. You have something wrong with how you are building. For example, you're not trying to run this from the core subdirectory are you? I assume because map() could have side effects? Even if that's not generally a good idea. The expectation or contract is that it is still invoked. In this program the caller could also call count() on the parent. This is not a compile error, but an error from the scalac compiler. That is, the code and build are fine, but scalac is not compiling it. Usually when this happens, a clean build fixes it. No, I'm not saying side effects change the count. But not executing the map() function at all certainly has an effect on the side effects of that function: the side effects which should take place never do. I am not sure that is something to be 'fixed'; it's a legitimate question. You can persist an RDD if you do not want to compute it twice. Given that's it's an internal error from scalac, I think it may be something to take up with the Scala folks to really fix. We can just look for workarounds. Try blowing away your .m2 and .ivy cache for example. FWIW I was running on Linux with Java 8u31, latest scala 2.11 AFAIK. You can have diamonds but not cycles in the dependency graph. But what you are describing really sounds like simple iteration, since presumably you mean that the state of each element in the 'cycle'changes each time, and so isn't really the same element each time, and eventually you decide to stop. That is quite possible. Signatures and hashes are good. LICENSE, NOTICE still check out. Compiles for a Hadoop 2.6 + YARN + Hive profile. I still see the UISeleniumSuite test failure observed in 1.3.0, which is minor and already fixed. I don't know why I didn't back-port it: https://issues.apache.org/jira/browse/SPARK-6205 If we roll another, let's get this easy fix in, but it is only an issue with tests. On JIRA, I checked open issues with Fix Version = 1.3.0 or 1.3.1 and all look legitimate (e.g. reopened or in progress) There is 1 open Blocker for 1.3.1 per Andrew: https://issues.apache.org/jira/browse/SPARK-6673 spark-shell.cmd can't start even when spark was built in Windows I believe this can be resolved quickly but as a matter of hygiene should be fixed or demoted before release. FYI there are 16 Critical issues marked for 1.3.0 / 1.3.1; worth examining before release to see how critical they are: SPARK-6701,Flaky test: o.a.s.deploy.yarn.YarnClusterSuite Python application,,Open,4/3/15 SPARK-6484,"Ganglia metrics xml reporter doesn't escape correctly",Josh Rosen,Open,3/24/15 SPARK-6270,Standalone Master hangs when streaming job completes,,Open,3/11/15 SPARK-6209,ExecutorClassLoader can leak connections after failing to load classes from the REPL class server,Josh Rosen,In Progress,4/2/15 SPARK-5113,Audit and document use of hostnames and IP addresses in Spark,,Open,3/24/15 SPARK-5098,Number of running tasks become negative after tasks lost,,Open,1/14/15 SPARK-4925,Publish Spark SQL hive-thriftserver maven artifact,Patrick Wendell,Reopened,3/23/15 SPARK-4922,Support dynamic allocation for coarse-grained Mesos,,Open,3/31/15 SPARK-4888,"Spark EC2 doesn't mount local disks for i2.8xlarge instances",,Open,1/27/15 SPARK-4879,Missing output partitions after job completes with speculative execution,Josh Rosen,Open,3/5/15 SPARK-4751,Support dynamic allocation for standalone mode,Andrew Or,Open,12/22/14 SPARK-4454,Race condition in DAGScheduler,Josh Rosen,Reopened,2/18/15 SPARK-4452,Shuffle data structures can starve others on the same thread for memory,Tianshuo Deng,Open,1/24/15 SPARK-4352,Incorporate locality preferences in dynamic allocation requests,,Open,1/26/15 SPARK-4227,Document external shuffle service,,Open,3/23/15 SPARK-3650,Triangle Count handles reverse edges incorrectly,,Open,2/23/15 I noticed recent pull request build results weren't posting results of MiMa checks, etc. I think it's due to Github auth issues: Attempting to post to Github... "message": "Bad credentials", "documentation_url": "https://developer.github.com/v3"} I've heard another colleague say they're having trouble with credentials today. Anyone else? I don't know if it's transient or what, but for today, just be aware you'll have to look at the end of the Jenkins output to see if these other checks passed. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org See now: https://issues.apache.org/jira/browse/SPARK-6710 SPARK-6673 is not, in the end, relevant for 1.3.x I believe; we just resolved it for 1.4 anyway. False alarm there. I back-ported SPARK-6205 into the 1.3 branch for next time. We'll pick it up if there's another RC, but by itself is not something that needs a new RC. (I will give the same treatment to branch 1.2 if needed in light of the 1.2.2 release.) I applied the simple change in SPARK-6205 in order to continue executing tests and all was well. I still see a few failures in Hive tests: - show_create_table_serde *** FAILED *** - show_tblproperties *** FAILED *** - udf_std *** FAILED *** - udf_stddev *** FAILED *** with ... mvn -Phadoop-2.4 -Pyarn -Phive -Phive-0.13.1 -Dhadoop.version=2.6.0 -DskipTests clean package; mvn -Phadoop-2.4 -Pyarn -Phive -Phive-0.13.1 -Dhadoop.version=2.6.0 test ... but these are not regressions from 1.3.0. +1 from me at this point on the current artifacts. I don't think it's required. This looks like zinc is running (it seems to find the process on port 3030), but, something is wrong with zinc then. If you aren't running your own zinc, then it's the copy downloaded by Spark. Maybe try deleting that and shutting down the zinc process, and trying a clean build? I think that's close enough for a +1: Signatures and hashes are good. LICENSE, NOTICE still check out. Compiles for a Hadoop 2.6 + YARN + Hive profile. JIRAs with target version = 1.2.x look legitimate; no blockers. I still observe several Hive test failures with: mvn -Phadoop-2.4 -Pyarn -Phive -Phive-0.13.1 -Dhadoop.version=2.6.0 -DskipTests clean package; mvn -Phadoop-2.4 -Pyarn -Phive -Phive-0.13.1 -Dhadoop.version=2.6.0 test .. though again I think these are not regressions but known issues in older branches. FYI there are 16 Critical issues still open for 1.2.x: SPARK-6209,ExecutorClassLoader can leak connections after failing to load classes from the REPL class server,Josh Rosen,In Progress,4/5/15 SPARK-5098,Number of running tasks become negative after tasks lost,,Open,1/14/15 SPARK-4888,"Spark EC2 doesn't mount local disks for i2.8xlarge instances",,Open,1/27/15 SPARK-4879,Missing output partitions after job completes with speculative execution,Josh Rosen,Open,3/5/15 SPARK-4568,Publish release candidates under $VERSION-RCX instead of $VERSION,Patrick Wendell,Open,11/24/14 SPARK-4520,SparkSQL exception when reading certain columns from a parquet file,sadhan sood,Open,1/21/15 SPARK-4514,SparkContext localProperties does not inherit property updates across thread reuse,Josh Rosen,Open,3/31/15 SPARK-4454,Race condition in DAGScheduler,Josh Rosen,Reopened,2/18/15 SPARK-4452,Shuffle data structures can starve others on the same thread for memory,Tianshuo Deng,Open,1/24/15 SPARK-4356,Test Scala 2.11 on Jenkins,Patrick Wendell,Open,11/12/14 SPARK-4258,NPE with new Parquet Filters,Cheng Lian,Reopened,4/3/15 SPARK-4194,Exceptions thrown during SparkContext or SparkEnv construction might lead to resource leaks or corrupted global state,,In Progress,4/2/15 SPARK-4159,"Maven build doesn't run JUnit test suites",Sean Owen,Open,1/11/15 SPARK-4106,Shuffle write and spill to disk metrics are incorrect,,Open,10/28/14 SPARK-3492,Clean up Yarn integration code,Andrew Or,Open,9/12/14 SPARK-3461,Support external groupByKey using repartitionAndSortWithinPartitions,Sandy Ryza,Open,11/10/14 SPARK-2984,FileNotFoundException on _temporary directory,,Open,12/11/14 SPARK-2532,Fix issues with consolidated shuffle,,Open,3/26/15 SPARK-1312,Batch should read based on the batch interval provided in the StreamingContext,Tathagata Das,Open,12/24/14 Er, click the link? It is indeed a redirector HTML page. This is how all Apache releases are served. Yeah, this is why this pops up when you open a PR: https://github.com/apache/spark/blob/master/CONTRIBUTING.md Mostly, I want to take all reasonable steps to ensure that when somebody offers a code contribution, that they are fine with the ways in which it actually used (redistributed under the terms of the AL2), whether or not they understand the intricacies. In good faith, I'm all but sure that all contributors either think they're giving the contribution to the project anyway, or at least, do understand it to be their own work licensed under the same terms as all of the project contributions are. IANAL, but in stricter legal terms, the project license is plain and clear, and the intricacies are signposted and easy to read when you contribute. You would have a very hard time arguing that you made a contribution, didn't state anything about the license, but did not intend somehow that the work could be licensed as the rest of the project is. For reference Apache projects do not in general require a CLA. Utils.startServiceOnPort? Still a +1 from me; same result (except that now of course the UISeleniumSuite test does not fail) What you have there is how to do it although you want to use sc.hadoopConfiguration IIRC. +1 same result as last time. Pardon, I wanted to call attention to a JIRA I just created... https://issues.apache.org/jira/browse/SPARK-6889 ... in which I propose what I hope are some changes to the contribution process wiki that could help a bit with the flood of reviews and PRs. I'd be grateful for your thoughts and comments there, as it's my current pet issue. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Bringing a discussion to dev@. I think the general questions on the table are: - Should more changes be rejected? What are the pros/cons of that? - If no, how do you think about the very large backlog of PRs and JIRAs? - What should be rejected and why? - How much support is there for proactively cleaning house now? What would you close and why? - What steps can be taken to prevent people from wasting time on JIRAs / PRs that will be rejected? - What if anything does this tell us about the patterns of project planning to date and what can we learn? This overlaps with other discussion on SPARK-6889 but per Nicholas wanted to surface this I'm not sure I understand what you are suggesting is wrong. It prints the result of the last command. In the second case that is the whole pasted block so you see 19. No, look at the Spark UI. You can see all three were executed. No, of course Jenkins runs tests. The way some of the tests work, they need the build artifacts to have been created first. So it runs "mvn ... -DskipTests package" then "mvn ... test"Sree that doesn't show any error, so it doesn't help. I built with the same flags when I tested and it succeeded. There are N chat options out there, and of course there's no need or way to stop people from using them. If 1 is blessed as 'best', it excludes others who prefer a different one. Tomorrow there will be a New Best Chat App. If a bunch are blessed, the conversation fractures. There's also a principle that important-ish discussions should take place on official project discussion forums, i.e., the mailing lists. Chat is just chat but sometimes discussions appropriate to the list happen there. For this reason I've always thought it best to punt on official-ish chat and let it happen organically as it will, with a request to port any important discussions to the list. This is why spark.hadoop.validateOutputSpecs exists, really: https://spark.apache.org/docs/latest/configuration.html Anecdotally, there are a number of people asking to set the Assignee field. This is currently restricted to Committers in JIRA. I know the logic was to prevent people from Assigning a JIRA and then leaving it; it also matters a bit for questions of "credit". Still I wonder if it's best to just let people go ahead and set it, as the lesser "evil". People can already do a lot like resolve JIRAs and set shepherd and critical priority and all that. I think the intent was to let "Developers" set this, but maybe due to an error, that's not how the current JIRA permission is implemented. I ask because I'm about to ping INFRA to update our scheme. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I can get behind that point of view too. That's what I've told people who expect Assignee is a necessary part of workflow. The existence of a PR link is a signal someone's working on it. In that case we need not do anything. I think you misread the thread, since that's the opposite of what Patrick suggested. He's suggesting that *nobody ever waits* to be assigned a JIRA to work on it; that anyone may work on a JIRA without waiting for it to be assigned. The point is: assigning JIRAs discourages others from doing work and we don't want to do that. So the pattern so far has been to not use it (except retroactively to credit the major contributor to the resolution.) The cost of this policy is -- oops, maybe you work on something that's already being worked on. That isn't a problem in practice. We already have a way to signal that you're working on a patch: you open a PR. It automatically links to JIRA. Or you can just comment. I suppose you could also use Assignee as a strong signal that your'e working on it, and some people want to do that, and so I was floating the idea of just letting people use it as they like. But I also back the idea of not having a notion of "owner" of working on a JIRA. I think we discussed this a while ago (?) and the problem was the overhead of even verifying the sorted state took too long. Following several discussions about how to improve the contribution process in Spark, I've overhauled the guide to contributing. Anyone who is going to contribute needs to read it, as it has more formal guidance about the process: https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark We may push back harder now on pull requests and JIRAs that don't follow this guidance. It will help everyone spend less time to get changes in, and spend less time on duplicated effort, or changes that won't. A summary of key points is found in CONTRIBUTING.md, a prompt presented before opening pull requests (https://github.com/apache/spark/blob/master/CONTRIBUTING.md): - Is the change important and ready enough to ask the community to spend time reviewing? - Have you searched for existing, related JIRAs and pull requests? - Is this a new feature that can stand alone as a package on http://spark-packages.org ? - Is the change being proposed clearly explained and motivated? --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Only against master; it can be cherry-picked to other branches. Those are different RDDs that DecisionTree persists, though. It's not redundant. Following my comment earlier that "I think we set Assignee for Fixed JIRAs consistently", I found there are actually 880 counter examples. Lots of them are old, and I'll try to fix as many that are recent (for the 1.4.0 release credits) as I can stand to click through. Let's set Assignee after resolving consistently though. In various ways I've heard that people do really like the bit of credit, and I don't think anybody disputed setting Assignee *after* it was resolved as a way of giving credit. People who know they're missing a credit are welcome to ping me directly to get it fixed. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I think that your own tutorials and such should live on your blog. The goal isn't to pull in a bunch of external docs to the site. That would require giving wiki access to everyone or manually adding people any time they make a doc. I don't see how this helps though. They're still docs on the internet and they're still linked from the central project JIRA, which is what you should follow. On Apr 24, 2015 8:14 AM, "Punyashloka Biswal"  Dear Spark devs, I think it's OK to have design discussions on github, as emails go to ASF lists. After all, loads of PR discussions happen there. It's easy for anyone to follow. I also would rather just discuss on Github, except for all that noise. It's not great to put discussions in something like Google Docs actually; the resulting doc needs to be pasted back to JIRA promptly if so. I suppose it's still better than a private conversation or not talking at all, but the principle is that one should be able to access any substantive decision or conversation by being tuned in to only the project systems of record -- mailing list, JIRA. Only catch there is it requires commit access to the repo. We need a way for people who aren't committers to write and collaborate (for point #1) I know I recently used Google Docs from a JIRA, so am guilty as charged. I don't think there are a lot of design docs in general, but the ones I've seen have simply pushed docs to a JIRA. (I did the same, mirroring PDFs of the Google Doc.) I don't think this is hard to follow. I think you can do what you like: make a JIRA and attach files. Make a WIP PR and attach your notes. Make a Google Doc if you're feeling transgressive. I don't see much of a problem to solve here. In practice there are plenty of workable options, all of which are mainstream, and so I do not see an argument that somehow this is solved by letting people make wikis. I know Jenkins hasn't been responding to requests to test today. I don't know if it's related, but I also noted that test results aren't quite posting: ... Archiving unit tests logs... Attempting to post to Github... "message": "Problems parsing JSON", "documentation_url": "https://developer.github.com/v3"} finished](https://amplab.cs.berkeley.edu/jenkins/job/NewSparkPullRequestBuilder/701/consoleFull) for   PR 5672 at commit [`0f1abd0`](https://github.com/apache/spark/commit/0f1abd01287cd33aa73a0b5574f95369b8d42910).\n * This patch **passes all tests**.\n * This patch merges cleanly.\n * This patch adds no public classes.\nYour branch is ahead of 'origin/master' by 200 commits. Your branch is ahead of 'origin/master' by 200 commits. * This patch **adds the following new dependencies:**\n   * `tachyon-0.6.4.jar`\n   * `tachyon-client-0.6.4.jar`\n\n * This patch **removes the following dependencies:**\n   * `tachyon-0.5.0.jar`\n * `tachyon-client-0.5.0.jar`\n"} Recording test results Finished: SUCCESS --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I'm firmly in favor of this. It would also fix https://issues.apache.org/jira/browse/SPARK-7009 and avoid any more of the long-standing 64K file limit thing that's still a problem for PySpark. As a point of reference, CDH5 has never supported Java 6, and it was released over a year ago. Should be, but isn't what Jenkins does. https://issues.apache.org/jira/browse/SPARK-1437 At this point it might be simpler to just decide that 1.5 will require Java 7 and then the Jenkins setup is correct. (NB: you can also solve this by setting bootclasspath to JDK 6 libs even when using javac 7+ but I think this is overly complicated.) I'd like to preemptively post the current list of 35 Blockers for release 1.4.0. (There are 53 Critical too, and a total of 273 JIRAs targeted for 1.4.0. Clearly most of that isn't accurate, so would be good to un-target most of that.) As a matter of process and hygiene, it would be best to either decide they're not Blockers at this point and reprioritize, or focus on addressing them, as we're now in the run up to release. I suggest that we shouldn't release with any Blockers outstanding, by definition. SPARK-7298 Web UI Harmonize style of new UI visualizations Patrick Wendell SPARK-7297 Web UI Make timeline more discoverable Patrick Wendell SPARK-7284 "Documentation Streaming"Update streaming documentation for Spark 1.4.0 release Tathagata Das SPARK-7228 SparkR SparkR public API for 1.4 release Shivaram Venkataraman SPARK-7158 SQL collect and take return different results SPARK-7139 Streaming Allow received block metadata to be saved to WAL and recovered on driver failure Tathagata Das SPARK-7111 Streaming Exposing of input data rates of non-receiver streams like Kafka Direct stream Saisai Shao SPARK-6941 SQL Provide a better error message to explain that tables created from RDDs are immutable SPARK-6923 SQL Spark SQL CLI does not read Data Source schema correctly SPARK-6906 SQL Refactor Connection to Hive Metastore Michael Armbrust SPARK-6831 "Documentation PySpark SparkR SQL"Document how to use external data sources SPARK-6824 SparkR Fill the docs for DataFrame API in SparkR SPARK-6812 SparkR filter() on DataFrame does not work as expected SPARK-6811 SparkR Building binary R packages for SparkR SPARK-6806 "Documentation SparkR"SparkR examples in programming guide Davies Liu SPARK-6784 SQL Clean up all the inbound/outbound conversions for DateType Yin Huai SPARK-6702 "Streaming Web UI"Update the Streaming Tab in Spark UI to show more batch information Tathagata Das SPARK-6654 Streaming Update Kinesis Streaming impls (both KCL-based and Direct) to use latest aws-java-sdk and kinesis-client-library SPARK-5960 Streaming Allow AWS credentials to be passed to KinesisUtils.createStream() Chris Fregly SPARK-5948 SQL Support writing to partitioned table for the Parquet data source SPARK-5947 SQL First class partitioning support in data sources API SPARK-5920 Shuffle Use a BufferedInputStream to read local shuffle data Kay Ousterhout SPARK-5707 SQL Enabling spark.sql.codegen throws ClassNotFound exception SPARK-5517 SQL Add input types for Java UDFs SPARK-5463 SQL Fix Parquet filter push-down SPARK-5456 SQL Decimal Type comparison issue SPARK-5182 SQL Partitioning support for tables created by the data source API Cheng Lian SPARK-5180 SQL Data source API improvement SPARK-4867 SQL UDF clean up SPARK-2973 SQL "Use LocalRelation for all ExecutedCommands avoid job for take/collect()"Cheng Lian SPARK-2883 "Input/Output SQL"Spark Support for ORCFile format SPARK-2873 SQL Support disk spilling in Spark SQL aggregation Yin Huai SPARK-1517 "Build Project Infra""Publish nightly snapshots of documentation maven artifacts and binary builds"Nicholas Chammas SPARK-1442 SQL Add Window function support --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org OK to file a JIRA to scrape out a few Java 6-specific things in the code? and/or close issues about working with Java 6 if they're not going to be resolved for 1.4? I suppose this means the master builds and PR builder in Jenkins should simply continue to use Java 7 then. Ah! of course. That one is my fault. Thank you Andrew for fixing that up. I tend to find that any large project has a lot of walking dead JIRAs, and pretending they are simply Open causes problems. Any state is better for these, so I favor this. The possible objection is that this will squash or hide useful issues, but in practice we have the opposite problem. Resolved issues are still searchable by default, and, people aren't shy about opening duplicates anyway. At least the semantics Later do not discourage a diligent searcher from considering commenting on and reopening such an archived JIRA. Patrick this could piggy back on INFRA-9513. As a corollary I would welcome deciding that Target Version should be used more narrowly to mean 'I really mean to help resolve this for the indicated version'. Setting it to a future version just to mean Later should instead turn into resolving the JIRA. Last: if JIRAs are regularly ice-boxed this way, I think it should trigger some reflection. Why are these JIRAs going nowhere? For completely normal reasons or does it mean too many TODOs are filed and forgotten? That's no comment on the current state, just something to watch. So: yes I like the idea. This change will be merged shortly for Spark 1.4, and has a minor implication for those creating their own Spark builds: https://issues.apache.org/jira/browse/SPARK-7249 https://github.com/apache/spark/pull/5786 The default Hadoop dependency has actually been Hadoop 2.2 for some time, but the defaults weren't fully consistent as a Hadoop 2.2 build. That is what this resolves. The discussion highlights that it's actually not great to rely on the Hadoop defaults, if you care at all about the Hadoop binding, and that it's good practice to set some -Phadoop-x.y profile in any build. The net changes are: If you don't care about Hadoop at all, you could ignore this. You will get a consistent Hadoop 2.2 binding by default now. Still, you may wish to set a Hadoop profile. If you build for Hadoop 1, you need to set -Phadoop-1 now. If you build for Hadoop 2.2, you should still set -Phadoop-2.2 even though this is the default and is a no-op profile now. You can continue to set other Hadoop profiles and override hadoop.version; these are unaffected. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org You all are looking only at the pull request builder. It just does one build to sanity-check a pull request, since that already takes 2 hours and would be prohibitive to build all configurations for every push. There is a different set of Jenkins jobs that periodically tests master against a lot more configurations, including Hadoop 2.4. No, I didn't yet. I was hoping to change the default version and make a few obvious changes to take advantage of it all at once. Go ahead with a JIRA. I can look into it this evening. We have just a little actual Java code so the new language features might be nice to use there but won't have a big impact. However we might do well to replace some Guava usages with standard JDK equivalents. I'd have to see just how much disruption it would cause. Before I vote, I wanted to point out there are still 9 Blockers for 1.4.0. I'd like to use this status to really mean "must happen before the release". Many of these may be already fixed, or aren't really blockers -- can just be updated accordingly. I bet at least one will require further work if it's really meant for 1.4, so all this means is there is likely to be another RC. We should still kick the tires on RC1. (I also assume we should be extra conservative about what is merged into 1.4 at this point.) SPARK-6784 SQL Clean up all the inbound/outbound conversions for DateType Adrian Wang SPARK-6811 SparkR Building binary R packages for SparkR Shivaram Venkataraman SPARK-6941 SQL Provide a better error message to explain that tables created from RDDs are immutable SPARK-7158 SQL collect and take return different results SPARK-7478 SQL Add a SQLContext.getOrCreate to maintain a singleton instance of SQLContext Tathagata Das SPARK-7616 SQL Overwriting a partitioned parquet table corrupt data Cheng Lian SPARK-7654 SQL DataFrameReader and DataFrameWriter for input/output API Reynold Xin SPARK-7662 SQL Exception of multi-attribute generator anlysis in projection SPARK-7713 SQL Use shared broadcast hadoop conf for partitioned table scan. Yin Huai (Marcelo you might have some insight on this one) Warning: this may just be because I'm doing something non-standard -- trying embed Spark in a Java app and feed it all the classpath it needs manually. But this was surprising enough I wanted to ask. I have an app that includes among other things SLF4J. I have set spark.{driver,executor}.userClassPathFirst to true. If I run it and let it start a Spark job, it quickly fails with: 2015-05-20 04:35:01,747 WARN  TaskSetManager:71 Lost task 0.0 in stage 0.0 (TID 0, x.cloudera.com): java.lang.LinkageError: loader constraint violation: loader (instance of org/apache/spark/util/ChildFirstURLClassLoader) previously initiated loading for a different type with name "org/slf4j/Logger"at java.lang.ClassLoader.defineClass1(Native Method) at java.lang.ClassLoader.defineClass(ClassLoader.java:800) at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142) at java.net.URLClassLoader.defineClass(URLClassLoader.java:449) at java.net.URLClassLoader.access$100(URLClassLoader.java:71) at java.net.URLClassLoader$1.run(URLClassLoader.java:361) at java.net.URLClassLoader$1.run(URLClassLoader.java:355) at java.security.AccessController.doPrivileged(Native Method) at java.net.URLClassLoader.findClass(URLClassLoader.java:354) at java.lang.ClassLoader.loadClass(ClassLoader.java:425) at org.apache.spark.util.ChildFirstURLClassLoader.liftedTree1$1(MutableURLClassLoader.scala:74) at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(MutableURLClassLoader.scala:73) at java.lang.ClassLoader.loadClass(ClassLoader.java:358) at org.apache.spark.streaming.kafka.KafkaRDD.compute(KafkaRDD.scala:89) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277) ... I can see that this class was loaded from my app JAR: [Loaded org.slf4j.Logger from file:/home/sowen/oryx-batch-2.0.0-SNAPSHOT.jar] I'm assuming it's also loaded in some Spark classloader. Tracing the code, I don't see that it ever gets to consulting any other classloader; this happens during its own child-first attempt to load the class. This didn't happen in 1.2, FWIW, when the implementation was different, but that's only to say it was different, not correct. Anyone have thoughts on what this indicates? something to be expected or surprising? I think that disabling userClassPathFirst gets rid of this of course, although that may cause other issues later. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Signature, hashes, LICENSE/NOTICE, source tarball looks OK. I built for Hadoop 2.6 (-Pyarn -Phive -Phadoop-2.6) on Ubuntu from source and tests pass. The release looks OK except that I'd like to resolve the Blockers before giving a +1. I'm seeing some test failures, and wanted to cross-check with others. They're all in Hive. Some I think are due to Java 8 differences and are just test issues; they expect an exact output from a query plan and some HashSet ordering differences make it trivially different. If so, I've seen this in the past and we could ignore it for now, but would be good to get a second set of eyes. The trace is big so it's at the end. When rerunning with Java 7 I get a different error due to Hive version support: - success sanity check *** FAILED *** java.lang.RuntimeException: [download failed: org.jboss.netty#netty;3.2.2.Final!netty.jar(bundle), download failed: commons-net#commons-net;3.1!commons-net.jar] at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:972) at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anonfun$3.apply(IsolatedClientLoader.scala:62) ... Hive / possible Java 8 test issue: - windowing.q -- 20. testSTATs *** FAILED *** Results do not match for windowing.q -- 20. testSTATs: == Parsed Logical Plan == PRECEDING AND 2 FOLLOWING) 'Project ['p_mfgr,'p_name,'p_size,UnresolvedWindowExpression WindowSpecReference(w1) UnresolvedWindowFunction stddev UnresolvedAttribute [p_retailprice] AS sdev#159481,UnresolvedWindowExpression WindowSpecReference(w1) UnresolvedWindowFunction stddev_pop UnresolvedAttribute [p_retailprice] AS sdev_pop#159482,UnresolvedWindowExpression WindowSpecReference(w1) UnresolvedWindowFunction collect_set UnresolvedAttribute [p_size] AS uniq_size#159483,UnresolvedWindowExpression WindowSpecReference(w1) UnresolvedWindowFunction variance UnresolvedAttribute [p_retailprice] AS var#159484,UnresolvedWindowExpression WindowSpecReference(w1) UnresolvedWindowFunction corr UnresolvedAttribute [p_size] UnresolvedAttribute [p_retailprice] AS cor#159485,UnresolvedWindowExpression WindowSpecReference(w1) UnresolvedWindowFunction covar_pop UnresolvedAttribute [p_size] UnresolvedAttribute [p_retailprice] AS covarp#159486] 'UnresolvedRelation [part], None == Analyzed Logical Plan == p_mfgr: string, p_name: string, p_size: int, sdev: double, sdev_pop: double, uniq_size: array, var: double, cor: double, covarp: double Project [p_mfgr#159489,p_name#159488,p_size#159492,sdev#159481,sdev_pop#159482,uniq_size#159483,var#159484,cor#159485,covarp#159486] Window [p_mfgr#159489,p_name#159488,p_size#159492,p_retailprice#159494], [HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStd(p_retailprice#159494) WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS sdev#159481,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStd(p_retailprice#159494) WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS sdev_pop#159482,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCollectSet(p_size#159492) WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS uniq_size#159483,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance(p_retailprice#159494) WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS var#159484,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCorrelation(p_size#159492,p_retailprice#159494) WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS cor#159485,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovariance(p_size#159492,p_retailprice#159494) WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS covarp#159486], WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING Project [p_mfgr#159489,p_name#159488,p_size#159492,p_retailprice#159494] MetastoreRelation default, part, None == Optimized Logical Plan == Project [p_mfgr#159489,p_name#159488,p_size#159492,sdev#159481,sdev_pop#159482,uniq_size#159483,var#159484,cor#159485,covarp#159486] Window [p_mfgr#159489,p_name#159488,p_size#159492,p_retailprice#159494], [HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStd(p_retailprice#159494) WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS sdev#159481,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStd(p_retailprice#159494) WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS sdev_pop#159482,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCollectSet(p_size#159492) WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS uniq_size#159483,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance(p_retailprice#159494) WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS var#159484,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCorrelation(p_size#159492,p_retailprice#159494) WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS cor#159485,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovariance(p_size#159492,p_retailprice#159494) WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS covarp#159486], WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING Project [p_mfgr#159489,p_name#159488,p_size#159492,p_retailprice#159494] MetastoreRelation default, part, None == Physical Plan == Project [p_mfgr#159489,p_name#159488,p_size#159492,sdev#159481,sdev_pop#159482,uniq_size#159483,var#159484,cor#159485,covarp#159486] Window [p_mfgr#159489,p_name#159488,p_size#159492,p_retailprice#159494], [HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStd(p_retailprice#159494) WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS sdev#159481,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFStd(p_retailprice#159494) WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS sdev_pop#159482,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCollectSet(p_size#159492) WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS uniq_size#159483,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFVariance(p_retailprice#159494) WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS var#159484,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCorrelation(p_size#159492,p_retailprice#159494) WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS cor#159485,HiveWindowFunction#org.apache.hadoop.hive.ql.udf.generic.GenericUDAFCovariance(p_size#159492,p_retailprice#159494) WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING AS covarp#159486], WindowSpecDefinition ROWS BETWEEN 2 PRECEDING AND 2 FOLLOWING Sort [p_mfgr#159489 ASC,p_mfgr#159489 ASC,p_name#159488 ASC], false Exchange (HashPartitioning 2) HiveTableScan [p_mfgr#159489,p_name#159488,p_size#159492,p_retailprice#159494], (MetastoreRelation default, part, None), None ... This is expected for example if your RDD is the result of random sampling, or if the underlying source is not consistent. You haven't shown any code. I dont believe we are talking about adding things to the Apache project, but incidentally LGPL is not OK in Apache projects either. Wait, isn't the error message just saying you can't set 8mb buffers? So it is correctly parsing the args. I don't understand why this has to do with parsing the value. That much works. Ah right I misread this. I get it but I dont think the PR fixes this. Let me comment there. We still have 1 blocker for 1.4: SPARK-6784 Make sure values of partitioning columns are correctly converted based on their data types CC Davies Liu / Adrian Wang to check on the status of this. There are still 50 Critical issues tagged for 1.4, and 183 issues targeted for 1.4 in general. Obviously almost all of those won't be in 1.4. How do people want to deal with those? The field can be cleared, but do people want to take a pass at bumping a few to 1.4.1 that really truly are supposed to go into 1.4.1? No 1.4.0 Blockers at this point, which is great. Forking this thread to discuss something else. There are 92 issues targeted for 1.4.0, 28 of which are marked Critical. Many are procedural issues like "update docs for 1.4" or "check X for 1.4". Are these resolved? They sound like things that are definitely supposed to have finished by now. Certainly, almost all of these are not going to be resolved for 1.4.0. Is this something we should be concerned about? because they are predominantly filed by or assigned to committers, not inexperienced contributors. I'm concerned that Target Version loses meaning if this happens frequently, and this number is ~10% of all JIRAs for 1.4. It's tempting to say X is important and someone will do X before 1.4, and then forget about it since it has been safely noted for later. Meanwhile other issues that grab more immediate attention and get worked on. This constitutes a form of project management, but de facto it's ad-hoc and reactive. Look at how many new issues and changes have still been coming in since the first release candidate of 1.4.0, compared to those "targeted" for the release. In an ideal world,  Target Version really is what's going to go in as far as anyone knows and when new stuff comes up, we all have to figure out what gets dropped to fit by the release date. Boring, standard software project management practice. I don't know how realistic that is, but, I'm wondering how people feel about this, who have filed these JIRAs? Concretely, should non-Critical issues for 1.4.0 be un-Targeted? should they all be un-Targeted after the release? I get a bunch of failures in VersionSuite with build/test params "-Pyarn -Phive -Phadoop-2.6": - success sanity check *** FAILED *** java.lang.RuntimeException: [download failed: org.jboss.netty#netty;3.2.2.Final!netty.jar(bundle), download failed: commons-net#commons-net;3.1!commons-net.jar] at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:978) ... but maybe I missed the memo about how to build for Hive? do I still need another Hive profile? Other tests, signatures, etc look good. I've definitely seen the "dependency path must be relative" problem, and fixed it by deleting the ivy cache, but I don't know more than this. https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark ... which contains ... https://issues.apache.org/jira/browse/SPARK-7993?jql=project%20%3D%20SPARK%20AND%20labels%20%3D%20Starter%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened) Everything checks out again, and the tests pass for me on Ubuntu + Java 7 with '-Pyarn -Phadoop-2.6', except that I always get SparkSubmitSuite errors like ... - success sanity check *** FAILED *** java.lang.RuntimeException: [download failed: org.jboss.netty#netty;3.2.2.Final!netty.jar(bundle), download failed: commons-net#commons-net;3.1!commons-net.jar] at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:978) at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anonfun$3.apply(IsolatedClientLoader.scala:62) ... I also can't get hive tests to pass. Is anyone else seeing anything like this? if not I'll assume this is something specific to the env -- or that I don't have the build invocation just right. It's puzzling since it's so consistent, but I presume others' tests pass and Jenkins does. How does the idea of removing support for Hadoop 1.x for Spark 1.5 strike everyone? Really, I mean, Hadoop < 2.2, as 2.2 seems to me more consistent with the modern 2.x line than 2.1 or 2.0. The arguments against are simply, well, someone out there might be using these versions. The arguments for are just simplification -- fewer gotchas in trying to keep supporting older Hadoop, of which we've seen several lately. We get to chop out a little bit of shim code and update to use some non-deprecated APIs. Along with removing support for Java 6, it might be a reasonable time to also draw a line under older Hadoop too. I'm just gauging feeling now: for, against, indifferent? I favor it, but would not push hard on it if there are objections. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I don't imagine that can be guaranteed to be supported anyway... the 0.x branch has never necessarily worked with Spark, even if it might happen to. Is this really something you would veto for everyone because of your deployment? Question: what would happen if I cleared Target Version for everything still marked Target Version = 1.4.0? There are 76 right now, and clearly that's not correct. 56 were opened by committers, including issues like "Do X for 1.4". I'd like to understand whether these are resolved but just weren't closed, or else why so many issues are being filed as a todo and not resolved? Slipping things here or there is OK, but these weren't even slipped, just forgotten. I also like using Target Version meaningfully. It might be a little much to require no Target Version = X before starting an RC. I do think it's reasonable to not start the RC with Blockers open. And here we started the RC with almost 100 TODOs for 1.4.0, most of which did not get done. Not the end of the world, but, clearly some other decisions were made in the past based on the notion that most of those would get done. The 'targeting' is too optimistic. Given fixed time, adding more TODOs generally means other stuff has to be taken out for the release. If not, then it happens de facto anyway, which is worse than managing it on purpose. Anyway, thanks all for the attention to some cleanup. I'll wait a short while and then fix up the rest of them as intelligently as I can. Maybe I can push on this a little the next time we have a release cycle to see how we're doing with use of Target Version. Not sure if many of you use JIRA Client (http://almworks.com/jiraclient/overview.html) to keep tabs on JIRA -- definitely worth it -- but if you're on OS X, I wonder if you too have suddenly been experiencing some type of SSL / keypair error on syncing? It's something to do with a JIRA server update and the fact that this app only knows how to run on Apple's Java 6, and it has some lack of support for bigger keys. Anyway ... if so, and you have Java 7 / 8 available locally as 'java', this mostly works: cd /Applications/JIRA\ Client.app/Contents/Resources/Java/lib java -jar ../jiraclient.jar --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Quick point of reference for 1.5.0: 226 issues are Fixed for 1.5.0, and 388 are Targeted for 1.5.0. So maybe 36% of things to be done for 1.5.0 are complete, and we're in theory 3 of 8 weeks into the merge window, or 37.5%. That's nicely on track! assuming, of course, that nothing else is targeted for 1.5.0. History suggests that a lot more will be, since a minor release has more usually had 1000+ JIRAs. However lots of forward-looking JIRAs have been filed, so it may be that most planned work is on the books already this time around. I think it would be fantastic if this work was burned down before adding big new chunks of work. The stat is worth keeping an eye on. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org There are some committers who are active on JIRA and sometimes need to do things that require JIRA admin access -- in particular thinking of adding a new person as "Contributor" in order to assign them a JIRA. We can't change what roles can do what (think that INFRA ticket is dead) but can add to the Admin role. Would anyone object to making a few more committers JIRA Admins for this purpose? --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org There are 44 issues still targeted for 1.4.1. None are Blockers; 12 are Critical. ~80% were opened and/or set by committers. Compare with 90 issues resolved for 1.4.1. I'm concerned that committers are targeting lots more for a release even in the short term than realistically can go in. On its face, it suggests that an RC is premature. Why is 1.4.1 being put forth for release now? It seems like people are saying they want a fair bit more time to work on 1.4.1. I suspect that in fact people would rather untarget / slip (again) these JIRAs, but it calls into question again how the targeting is consistently off by this much. What unresolved JIRAs targeted for 1.4.1 are *really* still open for 1.4.1? like, what would go badly if all 32 non-Critical JIRAs were untargeted now? is the reality that there are a handful of items to get in before the final release, and those are hopefully the ~12 critical ones? How about some review of that before we ask people to seriously test these bits? They are different classes even. Your problem isn't class-not-found though. You're also comparing different builds really. You should not be including Spark code in your app. -dev +user That all sounds fine except are you packaging Spark classes with your app? that's the bit I'm wondering about. You would mark it as a 'provided' dependency in Maven. That makes sense to me -- there's an urgent fix to get out. I missed that part. Not that it really matters but was that expressed elsewhere? I know we tend to start the RC process even when a few more changes are still in progress, to get a first wave or two of testing done early, knowing that the RC won't be the final one. It makes sense for some issues for X to be open when an RC is cut, if they are actually truly intended for X. 44 seems like a lot, and I don't think it's good practice just because that's how it's happened before. It looks like half of them weren't actually important for 1.4.x as we're now down to 21. I don't disagree with the idea that only "most" of the issues targeted for version X will be in version X; the target expresses a "stretch goal". Given the fast pace of change that's probably the only practical view. I think we're just missing a step then: before RC of X, ask people to review and update the target of JIRAs for X? In this case, it was a good point to untarget stuff from 1.4.x entirely; I suspect everything else should then be targeted at 1.4.2 by default with the exception of a handful that people really do intend to work in for 1.4.1 before its final release. I know it sounds like pencil-pushing, but it's a cheap way to bring some additional focus to release planning. RC time has felt like a last-call to *begin* changes ad-hoc when it would go faster if it were more intentional and constrained. Meaning faster RCs, meaning getting back to a 3-month release cycle or less, and meaning less rush to push stuff into a .0 release and less frequent need for a maintenance .1 version. So what happens if all 1.4.1-targeted JIRAs are targeted to 1.4.2? would that miss something that is definitely being worked on for 1.4.1? Lots of spam like this popping up: https://github.com/apache/spark/pull/6972#issuecomment-115130412 I've already reported this to Github to get it stopped and tried to contact the user, FYI. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Yes spark-submit adds all this for you. You don't bring Spark classes in your app Try putting your same Mesos assembly on the classpath of your client then, to emulate what spark-submit does. I don't think you merely also want to put it on the classpath but make sure nothing else from Spark is coming from your app. In 1.4 there is the 'launcher' API which makes programmatic access a lot more feasible but still kinda needs you to get Spark code to your driver program, and if it's not the same as on your cluster you'd still risk some incompatibilities. Ultimately I think its Github that has to act to ban the user. I've already asked for him to be blocked from the apache org on Github. In case you've tried and failed to add a person to a role in JIRA... https://issues.apache.org/jira/browse/INFRA-9891 --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org +1 sigs, license, etc check out. All tests pass for me in the Hadoop 2.6 + Hive configuration on Ubuntu. (I still get those pesky cosmetic UDF test failures in Java 8, but they are clearly just test issues.) I'll follow up on retargeting 1.4.1 issues afterwards as needed, but again feel free to move those you're sure won't be in this release. I wanted to flag a potential blocker here, but pardon me if this is still after all this time just my misunderstanding of the POM/build theory -- So this is the final candiate release POM right? https://repository.apache.org/content/repositories/orgapachespark-1118/org/apache/spark/spark-core_2.10/1.4.1/spark-core_2.10-1.4.1.pom Compare to for example: https://repo1.maven.org/maven2/org/apache/spark/spark-core_2.10/1.4.0/spark-core_2.10-1.4.0.pom and see: https://issues.apache.org/jira/browse/SPARK-8781 For instance, in 1.4.0 it had but now that's: JIRA suggests it had to do with adding: Am I missing something or is that indeed not going to work as a release POM? Great, thanks for the fix. Anything marked as fixed for 1.4.2 should now be marked as fixed for 1.4.1 right? I saw you were already updating many of those; OK to finish that? pretty safe. A few things are kind of minor behavior changes like https://issues.apache.org/jira/browse/SPARK-8630  Still probably not wrong to include. Ideally these would not be merged in branch 1.4 while the RC process is in progress, but then, that bottlenecks things. Once the RC for 1.4.1 is cut, should be mark everything else merged into branch 1.4 as fixed for 1.4.2? and then if a new RC is cut, mark them as fixed for 1.4.1 instead? that's a nice easy convention. Anything that might be mergeable for a later 1.4.x release but shouldn't go into 1.4.1 could be left out of branch 1.4 and marked as backport-needed. That should be a rare occasion for late in the RC process. Obviously, the faster the RC process goes the smaller this issue is - the smaller and more disciplined the list of target issues is when the process is, the better. Sorry to say same happens on 3.3.3. I tried Shade 2.4 too. It is indeed MSHADE-148 that Andrew was trying to fix in the first place. I'm also trying to think of workarounds here. PS the resolution on this is just that we've hit a JIRA limit, since the Contributor role is so big now. We have a currently-unused Developer role that barely has different permissions. I propose to move people that I recognize as regular Contributors into the Developer group to make room. Practically speaking, there's no difference. Just a heads up in case you see changes here. But for reference, new contributors should go to Contributors by default. Yeah, I've just realized a problem, that the permission for Developer are not the same as Contributor. It includes the ability to Assign, but doesn't seem to include other more basic permission. I cleared room in Contributor the meantime (no point in having Committers there; Committer permission is a superset), and I think we can actually fix this long-term by just removing barely-active people from Contributor since it won't matter (they only need to be in the group to be Assigned usually). I've also pinged the ticket to get more control over JIRA permissions so we can rectify more of this. The POM issue is resolved and the build succeeds. The license and sigs still work. The tests pass for me with "-Pyarn -Phadoop-2.6", with the following two exceptions. Is anyone else seeing these? this is consistent on Ubuntu 14 with Java 7/8: DataFrameStatSuite: ... - special crosstab elements (., '', null, ``) *** FAILED *** java.lang.NullPointerException: at org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$4.apply(StatFunctions.scala:131) at org.apache.spark.sql.execution.stat.StatFunctions$$anonfun$4.apply(StatFunctions.scala:121) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244) at scala.collection.immutable.Map$Map4.foreach(Map.scala:181) at scala.collection.TraversableLike$class.map(TraversableLike.scala:244) at scala.collection.AbstractTraversable.map(Traversable.scala:105) at org.apache.spark.sql.execution.stat.StatFunctions$.crossTabulate(StatFunctions.scala:121) at org.apache.spark.sql.DataFrameStatFunctions.crosstab(DataFrameStatFunctions.scala:94) at org.apache.spark.sql.DataFrameStatSuite$$anonfun$5.apply$mcV$sp(DataFrameStatSuite.scala:97) ... HiveSparkSubmitSuite: - SPARK-8368: includes jars passed in through --jars *** FAILED *** Process returned with exit code 1. See the log4j logs for more detail. (HiveSparkSubmitSuite.scala:92) - SPARK-8020: set sql conf in spark conf *** FAILED *** Process returned with exit code 1. See the log4j logs for more detail. (HiveSparkSubmitSuite.scala:92) - SPARK-8489: MissingRequirementError during reflection *** FAILED *** Process returned with exit code 1. See the log4j logs for more detail. (HiveSparkSubmitSuite.scala:92) Although that should be fixed if it's incorrect, it's not something that would nearly block a release. The question here is whether this artifact can be released as 1.4.1, or whether it has a blocking regression from 1.4.0. I see, but shouldn't this test not be run when Hive isn't in the build? +1 nonbinding. All previous RC issues appear resolved. All tests pass with the "-Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver" invocation. Signatures et al are OK. I suggest we move this info to the developer wiki, to keep it out from the place all and users look for downloads. What do you think about that Sean B? nightly builds should be hidden from non-developer end users. In an age of Github, what on earth is the problem with distributing the content of master? However I do understand why this exists. To the extent the ASF provides any value, it is at least a legal framework for defining what it means for you and I to give software to a bunch of other people. Software artifacts released according to an ASF process becomes something the ASF can take responsibility for as an entity. Nightly builds are not. It might matter to the committers if, say, somebody commits a serious data loss bug. You don't want to be on the hook individually for putting that into end-user hands. More practically, I think this exists to prevent some projects from lazily depending on unofficial nightly builds as pseudo-releases for long periods of time. End users may come to perceive them as official sanctioned releases when they aren't. That's not the case here of course. I think nightlies aren't for end-users anyway, and I think developers who care would know how to get nightlies anyway. There's little cost to moving this info to the wiki, so I'd do it. I agree with these points. The ec2 support is substantially a separate project, and would likely be better managed as one. People can much more rapidly iterate on it and release it. I suggest: 1. Pick a new repo location. amplab/spark-ec2 ? spark-ec2/spark-ec2 ? 2. Add interested parties as owners/contributors 3. Reassemble a working clone of the current code from spark/ec2 and mesos/spark-ec2 and check it in 4. Announce the new location on user@, dev@ 5. Triage open JIRAs to the new repo's issue tracker and close them elsewhere 6. Remove the old copies of the code and leave a pointer to the new location in their place I'd also like to hear a few more nods before pulling the trigger though. (This sounds pretty good to me. Mark it developers-only, not formally tested by the community, etc.) You shouldn't get dependencies you need from Spark, right? you declare direct dependencies. Are we talking about re-scoping or excluding this dep from Hadoop transitively? Why does Spark need to depend on it? I'm missing that bit. If an openstack artifact is needed for openstack, shouldn't openstack add it? otherwise everybody gets it in their build. To move this forward, I think one of two things needs to happen: 1. Move this guidance to the wiki. Seems that people gathered here believe that resolves the issue. Done. 2. Put disclaimers on the current downloads page. This may resolve the issue, but then we bring it up on the right mailing list for discussion. It may end up at #1, or may end in a tweak to the policy. I can drive either one. Votes on how to proceed? +1 to removing them. Sometimes there are 50+ commits because people have been merging from master into their branch rather than rebasing. This is done, and yes I believe that resolves the issue as far all here know. http://spark.apache.org/downloads.html https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-NightlyBuilds (Related, not important comment: it would also be nice to separate out the Tachyon dependency from core, as it's conceptually pluggable but is still hard-coded into several places in the code, and a lot of the comments/docs in the code.) I agree it's worth informing Mesos devs and checking that there are no big objections. I presume Shivaram is plugged in enough to Mesos that there won't be any surprises there, and that the project would also agree with moving this Spark-specific bit out. they may also want to leave a pointer to the new location in the mesos repo of course. I don't think it is something that requires a formal vote. It's not a question of ownership -- neither Apache nor the project PMC owns the code. I don't think it's different from retiring or removing any other code. It looks like you have a number of review comments on the PR that you have not replied to. The PR does not merge at the moment either. confidence = 1 + alpha * |rating| here (so, c1 means confidence - 1), so alpha = 1 doesn't specially mean high confidence. The loss function is computed over the whole input matrix, including all missing "0"entries. These have a minimal confidence of 1 according to this formula. alpha controls how much more confident you are in what the entries that do exist in the input mean. So alpha = 1 is low-ish and means you don't think the existence of ratings means a lot more than their absence. I think the explicit case is similar, but not identical -- here. The cost function for the explicit case is not the same, which is the more substantial difference between the two. There, ratings aren't inputs to a confidence value that becomes a weight in the loss function, during this factorization of a 0/1 matrix. Instead the rating matrix is the thing being factorized directly. It sounds like you're describing the explicit case, or any matrix decomposition. Are you sure that's best for count-like data? "It depends," but my experience is that the implicit formulation is better. In a way, the difference between 10,000 and 1,000 count is less significant than the difference between 1 and 10. However if your loss function penalizes the square of the error, then the former case not only matters more for the same relative error, it matters 10x more than the latter. It's very heavily skewed to pay attention to the high-count instances. You can tune alpha like any other hyperparam, and measuring whatever metric makes most sense -- AUC, etc. I don't think there's a general guidelines that's more specific than that. I also have not applied this to document retrieval / recommendation before I don't think you need to modify counts or ratings, and shouldn't, since the formulation is already trying to take care of translating counts into weights as 1 + alpha * r. I took the liberty of adding this to the wiki, where it can change further if needed. https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-PolicyonBackportingBugFixes I do not see any problem like that on master. The syntax looks valid. Do you have an old version bash? I'm on 4.3.39, though that is probably newer than what comes with Macs in general as I use brew to get newer versions of lots of things (this may be a good option for you in general if you're a developer). What OS X are you on -- is it also old? or is this likely to be a more widespread problem? You only need to rebase if your branch/PR now conflicts with master. you don't need to squash since the merge script will do that in the end for you. You can squash commits and force-push if you think it would help clean up your intent, but, often it's clearer to leave the review and commit history of your branch since the review comments go along with it. Right now, 603 issues have been resolved for 1.5.0. 424 are still targeted for 1.5.0, of which 33 are marked Blocker and 60 Critical. This count is not supposed to be 0 at this point, but must conceptually get to 0 at the time of 1.5.0's release. Most will simply be un-targeted or pushed down the road. If the plan is to begin meaningful testing on Aug 1 (great) and release by Aug 15, this seems to be far too large. Yes, it just means some prioritization has to happen. Target Version and Priority still seem like the right tools to communicate this. Let me put up a straw-man: untarget any JIRA targeted to 1.5.0 that isn't Blocker or Critical on Aug 1. (JIRAs can be explicitly retargeted in the following week.) This still leaves 93 issues, which seems unrealistic to address in 2 weeks. What are additional or alternative steps to handle this? - Untarget a lot of the remaining 93? - Push out 1.5 by X weeks to address more items? - Argue there's another way to manage this? PS is this still in progress? it feels like something that would be good to do before 1.5.0, if it's going to happen soon. If you use build/mvn or are already using Maven 3.3.3 locally (i.e. via brew on OS X), then this won't affect you, but I wanted to call attention to https://github.com/apache/spark/pull/7852 which makes Maven 3.3.3 the minimum required to build Spark. This heads off problems from some behavior differences that Patrick and I observed between 3.3 and 3.2 last week, on top of the "dependency reduced POM"glitch from the 1.4.1 release window. Again all you need to do is use build/mvn if you don't already have the latest Maven installed and all will be well. Sean --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Are these about the right rules of engagement for now until the release candidate? - Don't merge new features or improvements into 1.5 unless they're Important and Have Been Discussed - Docs and tests are OK to merge into 1.5 - Bug fixes can be merged into 1.5, with increasing conservativeness as the release candidate approaches FWIW there are now 331 JIRAs targeted at 1.5.0. Would it be reasonable to start un-targeting non-bug non-blocker issues? like, would anyone yell if I started doing that? that would leave ~100 JIRAs, which still seems like more than can actually go into the release. And anyone can re-target as desired. I'm interested with using this to communicate about release planning so we can actually see how things are moving along and decide if 1.5 has to be pushed back or not; otherwise it seems pretty unpredictable what's coming, going in, and when the process stops and outputs a release. Using ./build/mvn should always be fine. Your local mvn is fine too if it's 3.3.3 or later (3.3.3 is the latest). That's what any brew users on OS X out there will have, by the way. That statement is true for Spark 1.4.x. But you've reminded me that I failed to update this doc for 1.5, to say Maven 3.3.3 is required. Patch coming up. take only brings n elements to the driver, which is probably still a win if n is small. I'm not sure what you mean by only taking a count argument -- what else would be an arg to take? Signatures, license, etc. look good. I'm getting some fairly consistent failures using Java 7 + Ubuntu 15 + "-Pyarn -Phive -Phive-thriftserver -Phadoop-2.6" -- does anyone else see these? they are likely just test problems, but worth asking. Stack traces are at the end. There are currently 79 issues targeted for 1.5.0, of which 19 are bugs, of which 1 is a blocker. (1032 have been resolved for 1.5.0.) That's significantly better than at the last release. I presume a lot of what's still targeted is not critical and can now be untargeted/retargeted. It occurs to me that the flurry of planning that took place at the start of the 1.5 QA cycle a few weeks ago was quite helpful, and is the kind of thing that would be even more useful at the start of a release cycle. So would be great to do this for 1.6 in a few weeks. Indeed there are already 267 issues targeted for 1.6.0 -- a decent roadmap already. Test failures: Core - Unpersisting TorrentBroadcast on executors and driver in distributed mode *** FAILED *** java.util.concurrent.TimeoutException: Can't find 2 executors before 10000 milliseconds elapsed at org.apache.spark.ui.jobs.JobProgressListener.waitUntilExecutorsUp(JobProgressListener.scala:561) at org.apache.spark.broadcast.BroadcastSuite.testUnpersistBroadcast(BroadcastSuite.scala:313) at org.apache.spark.broadcast.BroadcastSuite.org$apache$spark$broadcast$BroadcastSuite$$testUnpersistTorrentBroadcast(BroadcastSuite.scala:287) at org.apache.spark.broadcast.BroadcastSuite$$anonfun$16.apply$mcV$sp(BroadcastSuite.scala:165) at org.apache.spark.broadcast.BroadcastSuite$$anonfun$16.apply(BroadcastSuite.scala:165) at org.apache.spark.broadcast.BroadcastSuite$$anonfun$16.apply(BroadcastSuite.scala:165) at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22) at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) ... Streaming - stop slow receiver gracefully *** FAILED *** 0 was not greater than 0 (StreamingContextSuite.scala:324) Kafka - offset recovery *** FAILED *** The code passed to eventually never returned normally. Attempted 191 times over 10.043196973 seconds. Last failure message: strings.forall({}) was false. (DirectKafkaStreamSuite.scala:249) I think this is a symptom of not running "mvn ... package" and then "mvn ... test"? PS Shixiong Zhu is correct that this one has to be fixed: https://issues.apache.org/jira/browse/SPARK-10168 For example you can see assemblies like this are nearly empty: https://repository.apache.org/content/repositories/orgapachespark-1137/org/apache/spark/spark-streaming-flume-assembly_2.10/1.5.0-rc1/ Just a publishing glitch but worth a few more eyes on. My quick take: no blockers at this point, except for one potential issue. Still some 'critical' bugs worth a look. The release seems to pass tests but i get a lot of spurious failures; it took about 16 hours of running tests to get everything to pass at least once. Current score: 56 issues targeted at 1.5.0, of which 14 bugs, of which no blockers and 8 critical. This one might be a blocker as it seems to mean that SBT + Scala 2.11 does not compile: https://issues.apache.org/jira/browse/SPARK-10227 pretty simple issue, but weigh in on the PR: https://github.com/apache/spark/pull/8433 For reference here are the Critical ones: Key Component Summary Assignee SPARK-6484 Spark Core Ganglia metrics xml reporter doesn't escape correctly Josh Rosen SPARK-6701 Tests, YARN Flaky test: o.a.s.deploy.yarn.YarnClusterSuite Python application SPARK-7420 Tests Flaky test: o.a.s.streaming.JobGeneratorSuite "Do not clear received block data too soon" Tathagata Das SPARK-8119 Spark Core HeartbeatReceiver should not adjust application executor resources Andrew Or SPARK-8414 Spark Core Ensure ContextCleaner actually triggers clean ups Andrew Or SPARK-8447 Shuffle Test external shuffle service with all shuffle managers SPARK-10224 Streaming BlockGenerator may lost data in the last block SPARK-10287 SQL After processing a query using JSON data, Spark SQL continuously refreshes metadata of the table Total: 8 issues I'm seeing the following tests fail intermittently, with "-Phive -Phive-thriftserver -Phadoop-2.6" on Ubuntu 15 / Java 7: - security mismatch password *** FAILED *** Expected exception java.io.IOException to be thrown, but java.nio.channels.CancelledKeyException was thrown. (ConnectionManagerSuite.scala:123) DAGSchedulerSuite: ... - misbehaved resultHandler should not crash DAGScheduler and SparkContext *** FAILED *** java.lang.UnsupportedOperationException: taskSucceeded() called on a finished JobWaiter was not instance of org.apache.spark.scheduler.DAGSchedulerSuiteDummyException (DAGSchedulerSuite.scala:861) HeartbeatReceiverSuite: ... - normal heartbeat *** FAILED *** 3 did not equal 2 (HeartbeatReceiverSuite.scala:104) - Unpersisting HttpBroadcast on executors only in distributed mode *** FAILED *** ... - Unpersisting HttpBroadcast on executors and driver in distributed mode *** FAILED *** ... - Unpersisting TorrentBroadcast on executors only in distributed mode *** FAILED *** ... - Unpersisting TorrentBroadcast on executors and driver in distributed mode *** FAILED *** StreamingContextSuite: ... - stop gracefully *** FAILED *** 1749735 did not equal 1190429 Received records = 1749735, processed records = 1190428 (StreamingContextSuite.scala:279) DirectKafkaStreamSuite: - offset recovery *** FAILED *** The code passed to eventually never returned normally. Attempted 193 times over 10.010808486 seconds. Last failure message: strings.forall({}) was false. (DirectKafkaStreamSuite.scala:249) - As usual the license and signatures are OK - No blockers, check - 9 "Critical" bugs for 1.5.0 are listed below just for everyone's reference (48 total issues still targeted for 1.5.0) - Under Java 7 + Ubuntu 15, I only had one consistent test failure, but obviously it's not failing in Jenkins - I saw more test failures with Java 8 but the seemed like flaky tests, so am pretending I didn't see that Test failure DirectKafkaStreamSuite: - offset recovery *** FAILED *** The code passed to eventually never returned normally. Attempted 197 times over 10.012973046 seconds. Last failure message: strings.forall({}) was false. (DirectKafkaStreamSuite.scala:249) 1.5.0 Critical Bugs SPARK-6484 Spark Core Ganglia metrics xml reporter doesn't escape correctly Josh Rosen SPARK-6701 Tests, YARN Flaky test: o.a.s.deploy.yarn.YarnClusterSuite Python application SPARK-7420 Tests Flaky test: o.a.s.streaming.JobGeneratorSuite "Do not clear received block data too soon" Tathagata Das SPARK-8119 Spark Core HeartbeatReceiver should not adjust application executor resources Andrew Or SPARK-8414 Spark Core Ensure ContextCleaner actually triggers clean ups Andrew Or SPARK-8447 Shuffle Test external shuffle service with all shuffle managers SPARK-10224 Streaming BlockGenerator may lost data in the last block SPARK-10310 SQL [Spark SQL] All result records will be popluated into ONE line during the script transform due to missing the correct line/filed delimeter SPARK-10337 SQL Views are broken That's how it's intended to work; if it's a problem, you probably need to re-design your computation to not use groupByKey. Usually you can do so. I think groupByKey is intended for cases where you do want the values in memory; for one-pass use cases, it's more efficient to use reduceByKey, or aggregateByKey if lower-level operations are needed. For your case, you probably want to do you reduceByKey, then perform the expensive per-key lookups once per key. You also probably want to do this in foreachPartition, not foreach, in order to pay DB connection costs just once per partition. I saw the end of the RC3 vote: https://mail-archives.apache.org/mod_mbox/spark-dev/201509.mbox/%3CCAPh_B%3DbQWf_vVuPs_eRpvnNSj8fbULX4kULnbs6MCAA10ZQ9eQ%40mail.gmail.com%3E but there are no artifacts for it in Maven? http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22org.apache.spark%22%20AND%20a%3A%22spark-parent_2.10%22 and I don't see any announcement at dev@ https://mail-archives.apache.org/mod_mbox/spark-dev/201509.mbox/browser But it was announced here just now: https://databricks.com/blog/2015/09/09/announcing-spark-1-5.html Did I miss something? --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I feel like I knew the answer to this but have forgotten. Reynold do you know about this file? looks like you added it. This is probably  true as the scala plugin actually compiles both .scala and .java files. Still it seems like the wrong place just as a matter of style. Can we try moving it and verify it's still OK? There are actually 33 instances of a Java file in src/main/scala -- I opened https://issues.apache.org/jira/browse/SPARK-10576 to track a discussion and decision. It sounds a lot like you have some local Hadoop config pointing to a cluster, and you're picking that up when you run the shell. Look for HADOOP_* env variables and clear them, and use --master local[*] +1 non-binding. This is the first time I've seen all tests pass the first time with Java 8 + Ubuntu + "-Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver". Clearly the test improvement efforts are paying off. As usual the license, sigs, etc are OK. Hi Richard, those are messages reproduced from other projects' NOTICE files, not created by Spark. They need to be reproduced in Spark's NOTICE file to comply with the license, but their text may or may not apply to Spark's distribution. The intent is that users would track this back to the source project if interested to investigate what the upstream notice is about. Requirements vary by license, but I do not believe there is additional requirement to reproduce these other files. Their license information is already indicated in accordance with the license terms. What licenses are you looking for in LICENSE that you believe should be there? Getting all this right is both difficult and important. I've made some efforts over time to strictly comply with the Apache take on licensing, which is at http://www.apache.org/legal/resolved.html  It's entirely possible there's still a mistake somewhere in here (possibly a new dependency, etc). Please point it out if you see such a thing. But so far what you describe is "working as intended", as far as I know, according to Apache. Have a look at http://www.apache.org/dev/licensing-howto.html#mod-notice though, which makes a good point about limiting what goes into NOTICE to what is required. That's what makes me think we shouldn't do this. Yes, the issue of where 3rd-party license information goes is different, and varies by license. I think the BSD/MIT licenses are all already listed in LICENSE accordingly. Let me know if you spy an omission. Yes, but the ASF's reading seems to be clear: http://www.apache.org/dev/licensing-howto.html#permissive-deps "In LICENSE, add a pointer to the dependency's license within the source tree and a short note summarizing its licensing:"I'd be concerned if you get a different interpretation from the ASF. I suppose it's OK to ask the question again, but for the moment I don't see a reason to believe there's a problem. Update: I *think* the conclusion was indeed that nothing needs to happen with NOTICE. However, along the way in https://issues.apache.org/jira/browse/LEGAL-226 it emerged that the BSD/MIT licenses should be inlined into LICENSE (or copied in the distro somewhere). I can get on that -- just some grunt work to copy and paste it all. Work underway at ... https://issues.apache.org/jira/browse/SPARK-10833 https://github.com/apache/spark/pull/8919 It's on Maven Central already. These various updates have to happen in some order, and you'll probably see an inconsistent state for a day or so while things get slowly updated. Consider it released when there's an announcement, I suppose. Why change the number of partitions of RDDs? especially since you can't generally do that without a shuffle. If you just mean to ramp up and down resource usage, dynamic allocation (of executors) already does that. My guess is that the 1.6 merge window should close at the end of November (2 months from now)? I can update it but wanted to check if anyone else has a preferred tentative plan. The Spark releases include a source distribution and several binary distributions. This is pretty normal for Apache projects. What are you referring to here? Of course, but what's making you think this was a binary-only distribution? The downloads page points you directly to the source distro: http://spark.apache.org/downloads.html Look for the last vote, and you'll find it was of course a vote on source (and binary) artifacts: http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-5-1-RC1-tt14310.html#none http://people.apache.org/~pwendell/spark-releases/spark-1.5.1-rc1-bin/ Still confused. Why are you saying we didn't vote on an archive? refer to the email I linked, which includes both the git tag and a link to all generated artifacts (also in my email). So, there are two things at play here: First, I am not sure what you mean that a source distro can't have binary files. It's supposed to have the source code of Spark, and shouldn't contain binary Spark. Nothing you listed are Spark binaries. However, a distribution might have a lot of things in it that support the source build, like copies of tools, test files, etc.  That explains I think the first couple lines that you identified. Still, I am curious why you are saying that would invalidate a source release? I have never heard anything like that. Second, I do think there are some binaries in here that aren't supposed to be there, like the build/ directory stuff. IIRC these were included accidentally and won't be in the next release. At least, I don't see why they need to be bundled. These are just local copies of third party tools though, and don't really matter. As it happens, the licenses that get distributed with the source distro even cover all of this stuff. I think that's not supposed to be there, but, also don't see it's 'invalid' as a result. Daniel: we did not vote on a tag. Please again read the VOTE email I linked to you: http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-5-1-RC1-tt14310.html#none among other things, it contains a link to the concrete source (and binary) distribution under vote: http://people.apache.org/~pwendell/spark-releases/spark-1.5.1-rc1-bin/ You can still examine it, sure. Dependencies are *not* bundled in the source release. You're again misunderstanding what you are seeing. Read my email again. I am still pretty confused about what the problem is. This is entirely business as usual for ASF projects. I'll follow up with you offline if you have any more doubts. Agree, but we are talking about the build/ bit right? I don't agree that it invalidates the release, which is probably the more important idea. As a point of process, you would not want to modify and republish the artifact that was already released after being voted on - unless it was invalid in which case we spin up 1.5.1.1 or something. But that build/ directory should go in future releases. I think he is talking about more than this though and the other jars look like they are part of tests, and still nothing to do with Spark binaries. Those can and should stay. No we are voting on the artifacts being released (too) in principle. Although of course the artifacts should be a deterministic function of the source at a certain point in time. I think the concern is about putting Spark binaries or its dependencies into a source release. That should not happen, but it is not what has happened here. I don't see that you ever opened a pull request. You just linked to commits in your branch. Please have a look at https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark Someone asked, is "ML pipelines" stable? I said, no, most of the key classes are still marked @Experimental, which matches my expression that things may still be subject to change. But then, I see that MLlib classes, which are de facto not seeing much further work and no API change, are also mostly marked @Experimental. If, generally, no more significant work is going into MLlib classes, is it time to remove most or all of those labels, to keep it meaningful? Sean You're welcome to open a little pull request to fix that. I believe you still need to "clean package" and then "test"separately. Or did the change to make that unnecessary go in to 1.5? FWIW I do not see this (my results coming soon) The signatures and licenses are fine. I continue to get failures in these tests though, with "-Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver" on Ubuntu 15 / Java 7. - Unpersisting HttpBroadcast on executors and driver in distributed mode *** FAILED *** java.util.concurrent.TimeoutException: Can't find 2 executors before 10000 milliseconds elapsed at org.apache.spark.ui.jobs.JobProgressListener.waitUntilExecutorsUp(JobProgressListener.scala:561) at org.apache.spark.broadcast.BroadcastSuite.testUnpersistBroadcast(BroadcastSuite.scala:313) at org.apache.spark.broadcast.BroadcastSuite.org$apache$spark$broadcast$BroadcastSuite$$testUnpersistHttpBroadcast(BroadcastSuite.scala:238) at org.apache.spark.broadcast.BroadcastSuite$$anonfun$12.apply$mcV$sp(BroadcastSuite.scala:149) at org.apache.spark.broadcast.BroadcastSuite$$anonfun$12.apply(BroadcastSuite.scala:149) at org.apache.spark.broadcast.BroadcastSuite$$anonfun$12.apply(BroadcastSuite.scala:149) at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22) at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) ... - Unpersisting TorrentBroadcast on executors only in local mode - Unpersisting TorrentBroadcast on executors and driver in local mode - Unpersisting TorrentBroadcast on executors only in distributed mode - Unpersisting TorrentBroadcast on executors and driver in distributed mode *** FAILED *** java.util.concurrent.TimeoutException: Can't find 2 executors before 10000 milliseconds elapsed at org.apache.spark.ui.jobs.JobProgressListener.waitUntilExecutorsUp(JobProgressListener.scala:561) at org.apache.spark.broadcast.BroadcastSuite.testUnpersistBroadcast(BroadcastSuite.scala:313) at org.apache.spark.broadcast.BroadcastSuite.org$apache$spark$broadcast$BroadcastSuite$$testUnpersistTorrentBroadcast(BroadcastSuite.scala:287) at org.apache.spark.broadcast.BroadcastSuite$$anonfun$16.apply$mcV$sp(BroadcastSuite.scala:165) at org.apache.spark.broadcast.BroadcastSuite$$anonfun$16.apply(BroadcastSuite.scala:165) at org.apache.spark.broadcast.BroadcastSuite$$anonfun$16.apply(BroadcastSuite.scala:165) at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22) at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) ... -dev +user How are you measuring network traffic? It's not in general true that there will be zero network traffic, since not all executors are local to all data. That can be the situation in many cases but not always. Ah, good point. I also see it still reads 1.5.1. I imagine we just need another sweep to update all the version strings. I like the idea, but I think there's already a lot of triage backlog. Can we more concretely address this now and during the next two weeks? 1.6.0 stats from JIRA: 344 issues targeted at 1.6.0, of which 253 are from committers, of which 215 are improvements/other, of which 5 are blockers 38 are bugs, of which 4 are blockers 11 are critical Tip: It's really easy to manage saved queries for this and other things with the free JIRA Client (http://almworks.com/jiraclient/overview.html) that now works with Java 8. It still looks like a lot for a point where 1.6.0 is supposed to be being tested in theory. Lots of (most?) things that were said to be done for 1.6.0 for several months aren't going to be, and that still surprises me as a software development practice. Well, life is busy and chaotic out here in OSS land. I'd still like to push even more on lightweight triage and release planning, centering around Target Version, if only to make visible what's happening with intention and reality: 1. Any JIRAs that seem to have been targeted at 1.6.0 by a non-committer are untargeted, as they shouldn't be to begin with 2. This week, maintainers and interested parties review all JIRAs targeted at 1.6.0 and untarget/retarget accordingly 3. Start of next week (the final days before an RC), non-Blocker non-bugs untargeted, or in a few cases pushed to 1.6.1 or beyond 4. After next week, non-Blocker and non-Critical bugs are pushed, as the RC is then late. 5. No release candidate until no Blockers are open. 6. (Repeat 1 and 2 more regularly through the development period for 1.7 instead of at the end.) I'm pretty sure that attribute is required. I am not sure what PMML version the code has been written for but would assume 4.2.1. Feel free to open a PR to add this version to all the output. As usual the signatures and licenses and so on look fine. I continue to get the same test failures on Ubuntu in Java 7/8: - Unpersisting HttpBroadcast on executors only in distributed mode *** FAILED *** But I continue to assume that's specific to tests and/or Ubuntu and/or the build profile, since I don't see any evidence of this in other builds on Jenkins. It's not a change from previous behavior, though it doesn't always happen either. Maven isn't 'legacy', or supported for the benefit of third parties. SBT had some behaviors / problems that Maven didn't relative to what Spark needs. SBT is a development-time alternative only, and partly generated from the Maven build. Since branch-1.6 is cut, I was going to make version 1.7.0 in JIRA. However I've had a few side conversations recently about Spark 2.0, and I know I and others have a number of ideas about it already. I'll go ahead and make 1.7.0, but thought I'd ask, how much other interest is there in starting to plan Spark 2.0? is that even on the table as the next release after 1.6? Sean Hm, if I read that right, looks like --num-executors doesn't work at all on YARN unless dynamic allocation is on? the fix is easy, but sounds like it could be a Blocker. I'm aware that IntelliJ has (at least in the past) made licenses available to committers in bona fide open source projects, and I recall they did the same for Spark. I believe I'm using that license now, but it seems to have expired? If anyone knows the status of that (or of any renewals to the license), I wonder if you could share that with me, offline of course. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Thanks, yes I've seen this, though I recall from another project that at some point they said, wait, we already gave your project a license! and I had to track down who had it. I think Josh might be the keeper? Not a big deal, just making sure I didn't miss an update there. Moving this to dev -- That's good, that'll help. Technically there's still a Blocker bug: https://issues.apache.org/jira/browse/SPARK-12000 The 'race' doesn't matter that much, but release planning remains the real bug-bear here. There are still, for instance, 52 issues targeted at 1.6.0, 42 of which were raised and targeted by committers. For example, I count 6 'umbrella' JIRAs in ML alone that are still open. The release is theoretically several weeks behind plan on what's intended to be a fixed release cycle too. This is why I'm not sure why today it's suddenly potentially ready for release. I'm just curious, am I the only one that thinks this isn't roughly normal, or do other people manage releases this way? I know the real world is messy and this is better than in the past, but I still get surprised by how each 1.x release actually comes about. Yeah I can see the PMC list as it happens; technically there are committers that aren't PMC / ASF members though, yeah. Josh did update the list when the last one expired, and the current license hasn't expired yet, though it was no longer working for me. It turns out to not be valid for the very newest IJ. Josh actually gave me a better solution: just use the free IntelliJ. Actually, none of the features we're likely to need are omitted from the community edition. Sure, it's cool if the existence of an update can be disseminated on dev@ and I'm guessing it would certainly be when it came up. My problem was actually slightly different and should have clarified I was curious if anyone else had license problems 'early' and if I'd missed an update, but I hadn't. Pardon for tacking on one more message to this thread, but I'm reminded of one more issue when building the RC today: Scala 2.10 does not in general try to work with Java 8, and indeed I can never fully compile it with Java 8 on Ubuntu or OS X, due to scalac assertion errors. 2.11 is the first that's supposed to work with Java 8. This may be a good reason to drop 2.10 by the time this comes up. Licenses and signature are all fine. Docker integration tests consistently fail for me with Java 7 / Ubuntu and "-Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver"*** RUN ABORTED *** java.lang.NoSuchMethodError: org.apache.http.impl.client.HttpClientBuilder.setConnectionManagerShared(Z)Lorg/apache/http/impl/client/HttpClientBuilder; at org.glassfish.jersey.apache.connector.ApacheConnector.(ApacheConnector.java:240) at org.glassfish.jersey.apache.connector.ApacheConnectorProvider.getConnector(ApacheConnectorProvider.java:115) at org.glassfish.jersey.client.ClientConfig$State.initRuntime(ClientConfig.java:418) at org.glassfish.jersey.client.ClientConfig$State.access$000(ClientConfig.java:88) at org.glassfish.jersey.client.ClientConfig$State$3.get(ClientConfig.java:120) at org.glassfish.jersey.client.ClientConfig$State$3.get(ClientConfig.java:117) at org.glassfish.jersey.internal.util.collection.Values$LazyValueImpl.get(Values.java:340) at org.glassfish.jersey.client.ClientConfig.getRuntime(ClientConfig.java:726) at org.glassfish.jersey.client.ClientRequest.getConfiguration(ClientRequest.java:285) at org.glassfish.jersey.client.JerseyInvocation.validateHttpMethodAndEntity(JerseyInvocation.java:126) I also get this failure consistently: DirectKafkaStreamSuite - offset recovery *** FAILED *** recoveredOffsetRanges.forall(((or: (org.apache.spark.streaming.Time, earlierOffsetRangesAsSets.contains(scala.Tuple2.apply[org.apache.spark.streaming.Time, scala.collection.immutable.Set[org.apache.spark.streaming.kafka.OffsetRange]](or._1, scala.this.Predef.refArrayOps[org.apache.spark.streaming.kafka.OffsetRange](or._2).toSet[org.apache.spark.streaming.kafka.OffsetRange])))) was false Recovered ranges are not the same as the ones generated (DirectKafkaStreamSuite.scala:301) To be clear-er, I don't think it's clear yet whether a 1.7 release should exist or not. I could see both making sense. It's also not really necessary to decide now, well before a 1.6 is even out in the field. Deleting the version lost information, and I would not have done that given my reply. Reynold maybe I can take this up with you offline. I've heard this argument before, but don't quite get it. Documentation is part of a release, and I believe is something we're voting on here too, and therefore needs to 'work' as documentation. We could not release this HTML to the Apache site, so I think that does actually mean the artifacts including docs don't work as a release. Yes, I can see that the non-code artifacts can be released a little bit after the code artifacts with last minute fixes. But, the whole release can just happen later too. Why wouldn't this be a valid reason to block the release? (I can't -1 this.) I do agree that docs have been treated as if separate from releases in the past. With more maturity in the release process, I'm questioning that now, as I don't think it's normal. It would be a reason to release or not release this particular tarball, so a vote thread is the right place to discuss it. I'm surprised you're suggesting there's not a coupling between a release's code and the docs for that release. If a release happens and some time later docs come out, that has some effect on people's usage. Surely, the ideal is for docs for x.y to come from the bits for x.y, and thus are available at the same time. Reality is something else, and your argument is practical, that the release is again behind and so shouldn't we overlook this minor problem to get it out? This particular problem has to get fixed, soon, we agree. It's minor by virtue of being hopefully temporary. But if it can/will be fixed quickly, what's the hurry? I get it, people want a releases sooner than later all else equal, but this is always true. It'd be nice to talk about what behaviors have led to being behind schedule and this perceived rush to finish now, since this same thing has happened in 1.5, 1.4. I'd rather at least collect some opinions on it than invalidate the question. I'll send my result of testing the RC separately With Java 7 / Ubuntu 15, and "-Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver", I still see the Docker tests fail every time. Is anyone else seeing them fail (or running them)? The Hive CliSuite also fails (stack trace at the bottom). Same deal -- if people are running this test and it's not failing, this is probably just flakiness of some form. There's the aforementioned doc generation issue too. Other than that it compiled and ran all tests for me. JIRA score: 28 issues, of which 11 bugs, of which 5 critical (listed below), of which 0 blockers. OK there. Critical bugs: SPARK-8447 Test external shuffle service with all shuffle managers SPARK-10680 Flaky test: network.RequestTimeoutIntegrationSuite.timeoutInactiveRequests SPARK-11224 Flaky test: o.a.s.ExternalShuffleServiceSuite SPARK-11266 Peak memory tests swallow failures SPARK-11293 Spillable collections leak shuffle memory - Simple commands *** FAILED *** ======================= CliSuite failure output ======================= Spark SQL CLI command line: ../../bin/spark-sql --master local --driver-java-options -Dderby.system.durability=test --conf spark.ui.enabled=false --hiveconf javax.jdo.option.ConnectionURL=jdbc:derby:;databaseName=/home/srowen/spark-1.6.0/sql/hive-thriftserver/target/tmp/spark-240e9e22-8fe8-408b-a116-2a894b3cbf1f;create=true --hiveconf hive.metastore.warehouse.dir=/home/srowen/spark-1.6.0/sql/hive-thriftserver/target/tmp/spark-c336bc67-8e51-4284-b574-e8b79d0d4fce --hiveconf hive.exec.scratchdir=/home/srowen/spark-1.6.0/sql/hive-thriftserver/target/tmp/spark-3a4f9564-d9f1-467f-8016-d4c95389e568 Exception: java.util.concurrent.TimeoutException: Futures timed out after [3 minutes] Executed query 0 "CREATE TABLE hive_test(key INT, val STRING);", But failed to capture expected output "OK" within 3 minutes. SLF4J bindings. [jar:file:/home/srowen/spark-1.6.0/assembly/target/scala-2.10/spark-assembly-1.6.0-hadoop2.6.0.jar!/org/slf4j/impl/StaticLoggerBinder.class] [jar:file:/home/srowen/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class] http://www.slf4j.org/codes.html#multiple_bindings for an explanation. [org.slf4j.impl.Log4jLoggerFactory] =========================== End CliSuite failure output =========================== (CliSuite.scala:151) For me, mostly the same as before: tests are mostly passing, but I can never get the docker tests to pass. If anyone knows a special profile or package that needs to be enabled, I can try that and/or fix/document it. Just wondering if it's me. I'm on Java 7 + Ubuntu 15.10, with -Pyarn -Phive -Phive-thriftserver -Phadoop-2.6 Yes that's what I mean. If they're not quite working, let's disable them, but first, we have to rule out that I'm not just missing some requirement. Functionally, it's not worth blocking the release. It seems like bad form to release with tests that always fail for a non-trivial number of users, but we have to establish that. If it's something with an easy fix (or needs disabling) and another RC needs to be baked, might be worth including. Logs coming offline Docker integration tests still fail for Mark and I, and should probably be disabled: https://issues.apache.org/jira/browse/SPARK-12426 ... but if anyone else successfully runs these (and I assume Jenkins does) then not a blocker. I'm having intermittent trouble with other tests passing, but nothing unusual. Sigs and hashes are OK. We have 30 issues fixed for 1.6.1. All but those resolved in the last 24 hours or so should be fixed for 1.6.0 right? I can touch that up. There's a script that can be run manually which closes PRs that have been 'requested' to be closed. I'm not sure of the exact words it looks for but "Do you mind closing this PR?" seems to work. However it does seem to mean that PRs will occasionally get closed as a false positive, so maybe that happened here. You can use your judgment about whether to reopen, but I tend to think PRs are not meant to be long-lived. They don't go away even when closed, so can always stand as a record of a proposed change or be reopened. But there shouldn't be such a thing as a PR open for months. (In practice, you can see a huge number of dead, stale PRs are left open by people out there anyway) Yes, I mean "open" for a long time, but I do mean PRs aren't intended to be open for long periods. Of course, they actually stick around forever on github. I think Reynold did manually close yours, but I was noting for the record that there's also an automated process that does this in response to a request. That has also surprised people in the past. Generally, we have way more problem with people abandoning or failing to follow through on PRs, or simply proposing things that aren't going to be merged. I agree in general with reflecting the reality by closing lots more JIRAs and PRs -- mostly because these are not permanent operations at all, and the intent is that in the occasional case where the owner disagrees, it can simply be reopened. This serves as a reminder that we need to drive all of these things to a conclusion. +juliet for an additional opinion, but FWIW I think it's safe to say that future CDH will have a more consistent Python story and that story will support 2.7 rather than 2.6. Here are two interesting issues (with PRs) concerning Java APIs for Spark 2.x. Details and discussion inside, and comments requested. https://issues.apache.org/jira/browse/SPARK-3369 This concerns fixing Iterator/Iterable problems in some Java API methods, to make them consistent with Scala. This allows real streaming operation in, say, flatMap, but it's an API change that's definitely going to make people change their code. https://issues.apache.org/jira/browse/SPARK-4819 This concerns the fate of Guava Optional in the Java API, and what to replace it with. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Chiming in late, but my take on this line of argument is: these companies are welcome to keep using Spark 1.x. If anything the argument here is about how long to maintain 1.x, and indeed, it's going to go dormant quite soon. But using RHEL 6 (or any old-er version of any platform) and not wanting to update already means you prefer stability more than change. I don't receive an expectation that major releases of major things support older major releases of other things. Conversely: supporting something in Spark 2.x means making sure nothing breaks compatibility with it for a couple years. This is effort than can be spent elsewhere; this has to be weighed. (For similar reasons I personally don't favor supporting Java 7 or Scala 2.10 in Spark 2.x.) ... I forget who can give access -- is it INFRA at Apache or one of us? I can apply any edit you need in the meantime. Shane may be able to fill you in on how the Jenkins build is set up. I personally support this. I had suggest drawing the line at Hadoop 2.6, but that's minor. More info: Hadoop 2.7: April 2015 Hadoop 2.6: Nov 2014 Hadoop 2.5: Aug 2014 Hadoop 2.4: April 2014 Hadoop 2.3: Feb 2014 Hadoop 2.2: Oct 2013 CDH 5.0/5.1 = Hadoop 2.3 + backports CDH 5.2/5.3 = Hadoop 2.5 + backports CDH 5.4+ = Hadoop 2.6 + chunks of 2.7 + backports. I can only imagine that CDH6 this year will be based on something later still like 2.8 (no idea about the 3.0 schedule). In the sense that 5.2 was released about a year and half ago, yes, this vendor has moved on from 2.3 a while ago. These releases will also never contain a different minor Spark release. For example 5.7 will have Spark 1.6, I believe, and not 2.0. Here, I listed some additional things we could clean up in Spark if Hadoop 2.6 was assumed. By itself, not a lot: https://github.com/apache/spark/pull/10446#issuecomment-167971026 Yes, we also get less Jenkins complexity. Mostly, the jar-hell that's biting now gets a little more feasible to fix. And we get Hadoop fixes as well as new APIs, which helps mostly for YARN. My general position is that backwards-compatibility and supporting older platforms needs to be a low priority in a major release; it's a decision about what to support for users in the next couple years, not the preceding couple years. Users on older technologies simply stay on the older Spark until ready to update; they are in no sense suddenly left behind otherwise. That's not a Spark problem. Your compiler was not available. I think it will come significantly later -- or else we'd be at code freeze for 2.x in a few days. I haven't heard anyone discuss this officially but had batted around May or so instead informally in conversation. Does anyone have a particularly strong opinion on that? That's basically an extra 3 month period. https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage Many of these changes are not needed - please see how the profiles work. Please also open 1 PR for one logical change and not 4. Typically YARN is there because you're mediating resource requests from things besides Spark, so yeah using every bit of the cluster is a little bit of a corner case. There's not a good answer if all your nodes are the same size. I think you can let YARN over-commit RAM though, and allocate more memory than it actually has. It may be beneficial to let them all think they have an extra GB, and let one node running the AM technically be overcommitted, a state which won't hurt at all unless you're really really tight on memory, in which case something might get killed. If it's too small to run an executor, I'd think it would be chosen for the AM as the only way to satisfy the request. I think that difference in the code is just an oversight. They actually do the same thing. Why do you say this property can only be set in a file? Yes you said it is only set in a props file, but why do you say that? because the resolution of your first question is that this is not differently handled. Here he's referring to a line of code that calls SparkConf.getenv vs System.getenv, but the former calls the latter. In neither case does it read from a props file. I know Patrick told us at some point, but I can't find the email or wiki that describes how to run the script that auto-closes PRs with "do you mind closing this PR". Does anyone know? I think it's been a long time since it was run. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org That's what I'm talking about, yes, but I'm looking for the actual script. I'm sure there was a discussion about where it was and how to run it somewhere. Really just looking to have it run again. Good catch, though probably very slightly simpler to write math.min(requiredSamples.toDouble ... Make sure you're logged in to JIRA maybe. If you have any trouble I'll open it for you. You can file it as a minor bug against ML. This is how you open a PR and everything else https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark Yes, though more broadly, should this just be removed for 2.x? I had this sense Tachyon was going away, or at least being put into a corner of the project. There's probalby at least no need for special builds for it. @Yin Yang see https://issues.apache.org/jira/browse/SPARK-12426 Docker has to be running locally for these tests to pass. I think it's a little surprising. However I still get a docker error, below. For me, +0 I guess. The signatures and hashes are all fine, but as usual I'm getting test failures. I suspect they may just be environment related but would like others to confirm they're *not* seeing the same. The Docker bits are still giving me trouble even with docker runing. On Ubuntu 15.10, with -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver: Core: - spilling with compression *** FAILED *** java.lang.Exception: Test failed with compression using codec org.apache.spark.io.LZ4CompressionCodec: assertion failed: expected groupByKey to spill, but did not Docker Integration Tests: *** RUN ABORTED *** com.spotify.docker.client.DockerException: java.util.concurrent.ExecutionException: com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.io.IOException: Permission denied at com.spotify.docker.client.DefaultDockerClient.propagate(DefaultDockerClient.java:1141) Streaming Kafka - offset recovery *** FAILED *** The code passed to eventually never returned normally. Attempted 188 times over 10.036713564 seconds. Last failure message: strings.forall({}) was false. (DirectKafkaStreamSuite.scala:249) FWIW I was running this with OpenJDK 1.8.0_66 https://scan.coverity.com/projects/apache-spark-2f9d080d-401d-47bc-9dd1-7956c411fbb4?tab=overview This has to be run manually, and is Java-only, but the inspection results are pretty good. Anyone should be able to browse them, and let me know if anyone would like more access. Most are false-positives, but it's found some reasonable little bugs. When my stack of things to do clears I'll try to address them, but I bring it up as an FYI for anyone interested in static analysis. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Yeah, it's not going to help with Scala, but it can at least find stuff in the Java code. I'm not suggesting anyone run it regularly, but one run to catch some bugs is useful. I've already triaged ~70 issues there just in the Java code, of which a handful are important. Hi Ted, I've already marked them. You should be able to see the ones marked "Fix Required" if you click through to the defects. Most are just bad form and probably have no impact. The few that looked reasonably important were: - using platform char encoding, not UTF-8 - Incorrect notify/wait - volatile count with non-atomic update - bad equals/hashCode No. Those are all in Java examples, and while we should show stopping the context, it has no big impact. It's worth touching up. I'm concerned about the ones with a potential correctness implication. They are easy to fix and already identified; why wouldn't we fix them? we take PRs to fix typos in comments. These files are effectively an internal representation, and I would not expect them to have such an extension. For example, you're not really guaranteed that the way the data breaks up leaves each file a valid JSON doc. Oh yeah I already added it after your earlier message, have a look. Code can be removed from an ASF project. That code can live on elsewhere (in accordance with the license) It can't be presented as part of the official ASF project, like any other 3rd party project The package name certainly must change from org.apache.spark I don't know of a protocol, but common sense dictates a good-faith effort to offer equivalent access to the code (e.g. interested committers should probably be repo owners too.) This differs from "any other code deletion" in that there's an intent to keep working on the code but outside the project. More discussion -- like this one -- would have been useful beforehand but nothing's undoable Backwards-compatibility is not a good reason for things, because we're talking about Spark 2.x, and we're already talking about distributing the code differently. Is the reason for this change decoupling releases? or changing governance? Seems like the former, but we don't actually need the latter to achieve that. There's an argument for a new repo, but this is not an argument for moving X out of the project per se I'm sure doing this in the ASF is more overhead, but if changing governance is a non-goal, there's no choice. Convenience can't trump that. Kafka integration is clearly more important than the others. It seems to need to stay within the project. However this still leaves a packaging problem to solve, that might need a new repo. This is orthgonal. Here's what I think: 1. Leave the moved modules outside the project entirely (why not Kinesis though? that one was not made clear) 2. Change package names and make sure it's clearly presented as external 3. Add any committers that want to be repo owners as owners 4. Keep Kafka within the project 5. Add some subproject within the current project as needed to accomplish distribution goals Added to https://cwiki.apache.org/confluence/display/SPARK/Supplemental+Spark+Projects I'm not clear what our criteria are for being added as an org, vs project, vs posting projects on spark-packages.org. Should this page actually go away in favor of spark-packages.org? The wiki doesn't do harm, just is a bit duplicative, needs updating, and might be viewed as selectively emphasizing a small number of projects. I generally favor this for the simplification. I didn't realize there were actually some performance wins and important bug fixes. I've had lots of trouble with scalac 2.10 + Java 8. I don't know if it's still a problem since 2.11 + 8 seems OK, but for a long time the sql/ modules would never compile in this config. If it's actually required for 2.12, makes sense. As ever my general stance is that nobody has to make a major-version upgrade; Spark 1.6 does not stop working for those that need Java 7. I also think it's reasonable for anyone to expect that major-version upgrades require major-version dependency updates. Also remember that not removing Java 7 support means committing to it here for a couple more years. It's not just about the situation on release day. Maybe so; I think we have a ticket open to update to 2.10.6, which maybe fixes it. It brings up a different point: supporting multiple Scala versions is much more painful than Java versions because of mutual incompatibility. Right now I get the sense there's an intent to keep supporting 2.10, and 2.11, and 2.12 later in Spark 2. This seems like relatively way more trouble. In the same breath -- why not remove 2.10 support anyway? It's also EOL, 2.11 also brought big improvements, etc. This has been resolved; see the JIRA and related PRs but also http://apache-spark-developers-list.1001551.n3.nabble.com/SPARK-13843-Next-steps-td16783.html This is not a scenario where a [VOTE] needs to take place, and code changes don't proceed through PMC votes. From the project perspective, code was deleted/retired for lack of interest, and this is controlled by the normal lazy consensus protocol which wasn't vetoed. The subsequent discussion was in part about whether other modules should go, or whether one should come back, which it did. The latter suggests that change could have been left open for some discussion longer. Ideally, you would have commented before the initial change happened, but it sounds like several people would have liked more time. I don't think I'd call that "improper conduct" though, no. It was reversed via the same normal code management process. The rest of the question concerned what becomes of the code that was removed. It was revived outside the project for anyone who cares to continue collaborating. There seemed to be no disagreement about that, mostly because the code in question was of minimal interest. PMC doesn't need to rule on anything. There may still be some loose ends there like namespace changes. I'll add to the other thread about this. Looks like this is done; docs have been moved, flume is back in, etc. For the moment Kafka streaming is still in the project and I know there's still discussion about how to manage multiple versions within the project. One other thing we need to finish up is stuff like the namespace of the code that was moved out. I believe it'll have to move out of the org.apache namespace as well as change its artifact group. At least, David indicated Sonatype wouldn't let someone non-ASF push an artifact from that group anyway. Also might be worth adding a description at https://github.com/spark-packages explaining that these are just some unofficial Spark-related packages. I tend to agree. If it's going to present a significant technical hurdle and the software is clearly non ASF like via a different artifact, there's a decent argument the namespace should stay. The artifact has to change though and that is what David was referring to in his other message. (This should fork as its own thread, though it began during discussion of whether to continue Java 7 support in Spark 2.x.) Simply: would like to more clearly take the temperature of all interested parties about whether to support Scala 2.10 in the Spark 2.x lifecycle. Some of the arguments appear to be: Pro - Some third party dependencies do not support Scala 2.11+ yet and so would not be usable in a Spark app Con - Lower maintenance overhead -- no separate 2.10 build, cross-building, tests to check, esp considering support of 2.12 will be needed - Can use 2.11+ features freely - 2.10 was EOL in late 2014 and Spark 2.x lifecycle is years to come I would like to not support 2.10 for Spark 2.x, myself. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Yeah it is not crazy to drop support for something foundational like this in a feature release but is something ideally coupled to a major release. You could at least say it is probably a decision to keep supporting through the end of the year given how releases are likely to go. Given the availability of the 'right' time to do it in the near future, does the value of passing that up to extend support for 3-6 more months outweigh the negatives ? I guess I think 2.10 is already about as droppable as it will get so it doesn't buy much. It is an option. The change there was just to mark the methods non-experimental. The logic was that they'd been around for many releases without change, and are unlikely to be changed now that they've been in the wild so long, so already acted as if they're part of the normal stable API. Are they important? I personally consider the approximate count methods useful. There was some recent talk of deprecating the approximate sum, mean methods. But they're no longer experimental and not going away soon so I suppose they're worth supporting. Following https://github.com/apache/spark/pull/12165#issuecomment-205791222 I'd like to make a point about process and then answer points below. We have this funny system where anyone can propose a change, and any of a few people can veto a change unilaterally. The latter rarely comes up. 9 changes out of 10 nobody disagrees on; sometimes a committer will say 'no' to a change and nobody else with that bit disagrees. Sometimes it matters and here I see, what, 4 out of 5 people including committers supporting a particular change. A veto to oppose that is pretty drastic. It's not something to use because you or customers prefer a certain outcome. This reads like you're informing people you've changed your mind and that's the decision, when it can't work that way. I saw this happen to a lesser extent in the thread about Scala 2.10. It doesn't mean majority rules here either, but can I suggest you instead counter-propose an outcome that the people here voting in favor of what you're vetoing would probably also buy into? I bet everyone's willing to give wide accommodation to your concerns. It's probably not hard, like: let's plan to not support Java 7 in Spark 2.1.0. (Then we can debate the logic of that.) Answering for myself: I assume everyone is following http://semver.org/ semantic versioning. If not, would be good to hear an alternative theory. For semver, strictly speaking, minor releases should be backwards-compatible for callers. Are things like stopping support for Java 8 or Scala 2.10 backwards-incompatible? In the end, yes, non-trivially, on both counts. This is why it seems like these changes must go with 2.0 or wait until 3.0. Rules are made to be broken and few software projects do this right. I hear the legitimate concern that getting the latest features and fixes into users hands is really important, and it is. Really, that argues that it's important to keep maintaining the 1.x branch with features and fixes. However, it seems obvious there will never be a 1.7, and probably not 1.6.2, for lack of bandwidth. And indeed it's way too much work given the scope of this project compared to hands to do the work. But this is what's forcing conflation of backwards-compatibility concerns onto a version boundary that doesn't inherently have one. It's a reason I personally root for breaking as many promises and encumbrances that make sense to break all at once at this boundary. Anyway, hope that explains my general logic. I've heard several people refer to a code freeze for 2.0. Unless I missed it, nobody has discussed a particular date for this: https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage I'd like to start with a review of JIRAs before anyone decides a freeze is appropriate. There are hundreds of issues, some blockers, still targeted for 2.0. Probably best for everyone to review and retarget non essentials and then see where we are at? Why would this need to be an ASF project of its own? I don't think it's possible to have a yet another separate "Spark Extras" TLP (?) There is already a project to manage these bits of code on Github. How about all of the interested parties manage the code there, under the same process, under the same license, etc? I'm not against calling it Spark Extras myself but I wonder if that needlessly confuses the situation. They aren't part of the Spark TLP on purpose, so trying to give it some special middle-ground status might just be confusing. The thing that comes to mind immediately is "Connectors for Apache Spark", spark-connectors, etc. I think this meant to be understood as a community site, and as a directory listing pointers to third-party projects. It's not a project of its own, and not part of Spark itself, with no special status. At least, I think that's how it should be presented and pretty much seems to come across that way. FWIW, here's what I do to look at JIRA's answer to this: 1) Go download http://almworks.com/jiraclient/overview.html 2) Set up a query for "target = 2.0.0 and status = Open, In Progress, Reopened"3) Set up sub-queries for bugs vs non-bugs, and for critical, blocker and other Right now there are 172 issues open for 2.0.0. 40 are bugs, 4 of which are critical and 1 of which is a blocker. 9 non-bugs are blockers, 5 critical. JIRA info is inevitably noisy, but now is a good time to make this info meaningful so we have some shared reference about the short-term plan. What I suggest we do now is ... a) un-target anything that wasn't targeted to 2.0.0 by a committer b) committers un-target or re-target anything they know isn't that important for 2.0.0 (thanks jkbradley) d) see where we are next week, repeat I suggest we simply have "no blockers" as an exit criteria, with a strong pref for "no critical bugs either". It's a major release, so taking a little extra time to get it all done comfortably is both possible and unusually important. A couple weeks indeed might be realistic for an RC, but it really depends on burndown more than anything. We already have SparkException, indeed. The ID is an interesting idea; simple to implement and might help disambiguate. Does it solve a lot of problems of this form? if something is squelching Exception or SparkException the result will be the same. #2 is something we can sniff out with static analysis pretty easily, but not as much #1. Ideally we'd just fix blocks like this but I bet there are lots of them. I like the idea but for a different reason, and that's that it's probably best to control exceptions that propagate from the public API, since in some cases they're a meaningful part of the API (see https://issues.apache.org/jira/browse/SPARK-8393 which I'm hoping to fix now) And the catch there is -- throwing checked exceptions from Scala code in a way that Java code can catch requires annotating lots of methods. I support this. We used to do this, right? Anecdotally, from watching the stream most days, most stale PRs are, in descending order of frequency: 1. Probably not a good change, and not looked at (as a result) 2. Abandoned by submitter at some stage 3. Not an important change, not so bad, not really reviewed 4. A good change that needs review Whether your PR is #1 or #4 is a matter of perspective. But, I disagree with the tacit assumption that we're mostly talking about good PRs being closed because nobody could be bothered; #4 is, I think, well under 10%. So generating reports and warnings etc don't seem to address that. Closing merely means "at the moment there's not a reason to expect this to proceed, but that could change". Unlike JIRA we don't have more nuanced resolutions like "WontFix" vs "Later". Welcome the submitter to reopen if they really think it should be kept alive in good faith. As for always stating a close reason: well, a lot of PRs are simply not very good code, or features that just don't look that useful relative to their cost. Is it more polite to soft-close or honestly say "I don't think this is worth adding"? There is a carrying cost to not doing this. Right now being "Open" is fairly meaningless, and I've long since stopped bothering reviewing the backlog of open PRs since it's noise, instead sifting for good new ones to fast-track. I agree with comments here that suggest more can be pushed back on the contributors. We've started with a campaign to get people to read https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark first, which would solve a lot of the "mystery pull request" problems if followed: no real description, no test, no real problem statement. Put another way, any contribution that is clearly explained, cleanly implemented, and makes a good case why its pros outweigh its cons, is pretty consistently reviewed and quickly. Pushing on contributors to do these things won't harm good contributions, which already do these things; it'll make it harder for bad contributions to distract from them. And I think the effect of a change like this is, in the main, to push back mostly on less good contributions. Except for the last one I think they're closeable. We can't close any PR directly. It's possible to push an empty commit with comments like "Closes #xxxx" to make the ASF processes close them. I'd swear we have a script for this but it's not in the Spark project, and it used to be run regularly. It would generate messages like "Automated closing of PRs" and was triggered by text in the PRs like "do you mind closing this PR". I can't find where it is though. That would be ideal to run again. @pwendell am I crazy .. did you not make this script and even email about it? I've lost it somehow. Skimmed these as a one-off exercise, and I suggest ... Leave open for now 7739 mgrover 10701 nraychaudhuri 9920 jliwork 9936 Lewuathe 10052 dereksabryfb 10125 kevinyu98 10466 hhbyyh 10995 tedyu 10887 blbradley 11005 huaxingao 11129 AtkinsChang 11610 iyounus 11729 mbaddar1 11980 koertkuipers 12075 zhuoliu 10572 navis 10945 dmarcous Close 9354 jacek-lewandowski 9451 vidma 10507 JerryLead 10486 wilson888888888 10460 huaxingao 10967 kevinyu98 10681 nikit-os 11766 s4weng 9907 Lewuathe 10209 nongli 10379 yanakad 10403 naveenminchu 10842 rajeshbalamohan 11036 mbautin Hm, this may be related to updating to Jersey 2, which happened 4 days ago: https://issues.apache.org/jira/browse/SPARK-12154 That is a Jersey 1 class that's missing. How are you building and running Spark? I think the theory was that Jersey 1 would still be supplied at runtime. We may have to revise the exclusions. The reason I ask how you're running is that YARN itself should have the classes that YARN needs, and should be on your classpath. That's why it's not in Spark. There still could be a subtler problem. The best way to trouble shoot, if you want to act on it now, is to have a look at the exclusions for Jersey 1 artifacts put in place in that PR, and try removing ones related to YARN, and see if that remedies it. If so we may need to undo that part. I think it's a good idea. Although releases have been preceded before by release candidates for developers, it would be good to get a formal preview/beta release ratified for public consumption ahead of a new major release. Better to have a little more testing in the wild to identify problems before 2.0.0 is finalized. +1 to the release. License, sigs, etc check out. On Ubuntu 16 + Java 8, compilation and tests succeed for "-Pyarn -Phive -Phive-thriftserver -Phadoop-2.6". +1 (binding) No, because then none of the Java 8 support can build. Marcelo has a JIRA for handling that the right way with bootstrap class path config. Ideally it can be rolled into Jenkins though there are possibly historical reasons it was not enabled before. Best to fix those if possible but if not I'd rather have some automated checking than none. Checking lint is reasonably important. I don't think the project would bless anything but the standard release artifacts since only those are voted on. People are free to maintain whatever they like and even share it, as long as it's clear it's not from the Apache project. I don't think we generally use labels at all except "starter". I sometimes remove labels when I'm editing a JIRA otherwise, perhaps to make that point. I don't recall doing this recently. However I'd say they should not be used to tag JIRAs for your internal purposes. Have you looked at things like JIRA Client from Almworks? It's free and I highly recommend it, and IIRC it lets you manage some private labels locally. Yeah I think using labels is fine -- just not if they're for someone's internal purpose. I don't have a problem with using meaningful labels if they're meaningful to everyone. In fact, I'd rather be using labels rather than "umbrella" JIRAs. Labels I have removed as unuseful are ones like "patch" or "important"or "bug". "big-endian" sounds useful. The only downside is that, inevitably, a label won't be consistently applied. But such is life. I still don't see any artifacts in maven -- did it publish? http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.apache.spark%22 Just checked and they are still not published this week. Can these be published ASAP to complete the 2.0.0-preview release? --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org An RC is something that gets voted on, and the final one is turned into a blessed release. I agree that RCs don't get published to Maven Central, but releases do of course. This was certainly to be an official release, right? A beta or alpha can still be an official, published release. The proximate motivation was to solve a problem of advertising "Apache Spark 2.0.0 preview" in a product, when no such release existed from the ASF. Hence the point was to produce a full regular release, and I think that needs to include the usual Maven artifacts. I'd think we want less effort, not more, to let people test it? for example, right now I can't easily try my product build against 2.0.0-preview. I'll be more specific about the issue that I think trumps all this, which I realize maybe not everyone was aware of. There was a long and contentious discussion on the PMC about, among other things, advertising a "Spark 2.0 preview" from Databricks, such as at https://databricks.com/blog/2016/05/11/apache-spark-2-0-technical-preview-easier-faster-and-smarter.html That post has already been updated/fixed from an earlier version, but part of the resolution was to make a full "2.0.0 preview" release in order to continue to be able to advertise it as such. Without it, I believe the PMC's conclusion remains that this blog post / product announcement is not allowed by ASF policy. Hence, either the product announcements need to be taken down and a bunch of wording changed in the Databricks product, or, this needs to be a normal release. Obviously, it seems far easier to just finish the release per usual. I actually didn't realize this had not been offered for download at http://spark.apache.org/downloads.html either. It needs to be accessible there too. We can get back in the weeds about what a "preview" release means, but, normal voted releases can and even should be alpha/beta (http://www.apache.org/dev/release.html) The culture is, in theory, to release early and often. I don't buy an argument that it's too old, at 2 weeks, when the alternative is having nothing at all to test against. I think they're deprecated, not necessarily entirely unused. I personally might leave it, but don't feel strongly about it. Artifacts that are not for public consumption shouldn't be in a public release; this is instead what nightlies are for. However, this was a normal public release. I am not even sure why it's viewed as particularly unsafe, but, unsafe alpha and beta releases are just releases, and their name and documentation clarify their status for those who care. These are regularly released by other projects. That is, the question is not, is this a beta? Everyone agrees it probably is, and is documented as such. The question is, can you just not fully release it? I don't think so, even as a matter of process, and don't see a good reason not to. To Reynold's quote, I think that's suggesting that not all projects will release to a repo at all (e.g. OpenOffice?). I don't think it means you're free to not release some things to Maven, if that's appropriate and common for the type of project. Regarding risk, remember that the audience for Maven artifacts are developers, not admins or end users. I understand that developers can temporarily change their build to use a different resolver if they care, but, why? (and, where would someone figure this out?) Regardless: the 2.0.0-preview docs aren't published to go along with the source/binary releases. Those need be released to the project site, though probably under a different /preview/ path or something. If they are, is it weird that someone wouldn't find the release in the usual place in Maven then? Given that the driver of this was concern over wide access to 2.0.0-preview, I think it's best to err on the side openness vs some theoretical problem. I still don't know where this "severely compromised builds of limited usefulness" thing comes from? what's so bad? You didn't veto its release, after all. And rightly so: a release doesn't mean "definitely works"; it means it was created the right way. It's OK to say it's buggy alpha software; this isn't an argument to not really release it. But aside from that: if it should be used by someone, then who did you have in mind? It would be coherent at least to decide not to make alpha-like release, but, we agreed to, which is why this argument sort of surprises me. I share some concerns about piling on Databricks. Nothing here is by nature about an organization. However, this release really began in response to a thread (which not everyone here can see) about Databricks releasing a "2.0.0 preview" option in their product before it existed. I presume employees of that company sort of endorse this, which has put this same release into the hands of not just developers or admins but end users -- even with caveats and warnings. (And I think that's right!) While I'd like to see your reasons before I'd agree with you Mark, yours is a feasible position; I'm not as sure how people who work for Databricks can argue at the same time however that this should be carefully guarded as an ASF release -- even with caveats and warnings. We don't need to assume bad faith -- I don't. The appearance alone is enough to act to make this consistent. But, I think the resolution is simple: it's not 'dangerous' to release this and I don't think people who say they think this really do. So just finish this release normally, and we're done. Even if you think there's an argument against it, weigh vs the problems above. Artifacts can't be removed from Maven in any normal circumstance, but, it's no problem. The argument that people might keep using it goes for any older release. Why would anyone use 1.6.0 when 1.6.1 exists? yet we keep 1.6.0 just for the record and to not break builds. It may be that Foobar 3.0-beta depends on 2.0.0-preview and 3.0 will shortly depend on 2.0.0, but, killing the -preview artifact breaks that other historical release/branch. I agree that "-alpha-1" would have been better. But we're talking about working around pretty bone-headed behavior, to not notice what version of Spark they build against, or not understand what 2.0.0-preview vs 2.0.0 means in a world of semver. BTW Maven sorts 2.0.0-preview before 2.0.0, so 2.0.0 would show up as the latest, when released, in tools like mvn versions:display-dependency-updates. You could exclude the preview release by requiring version [2.0.0,). It's there (refresh maybe?). See the end of the downloads dropdown. For the moment you can see the docs in the nightly docs build: https://home.apache.org/~pwendell/spark-nightly/spark-branch-2.0-docs/latest/ I don't know, what's the best way to put this into the main site? under a /preview root? I am not sure how that process works. As a stop-gap, I can edit that page to have a small section about preview releases and point to the nightly docs. Not sure who has the power to push 2.0.0-preview to site/docs, but, if that's done then we can symlink "preview" in that dir to it and be done, and update this section about preview docs accordingly. OK, this is done: http://spark.apache.org/documentation.html http://spark.apache.org/docs/2.0.0-preview/ http://spark.apache.org/docs/preview/ Available but mostly as JIRA output: https://spark.apache.org/news/spark-2.0.0-preview.html Yeah it does the same thing anyway. It's fine to consistently use the method. I think there's an instance in ClientSuite that can use it. 1.6.2 RC seems fine to me; I don't know of outstanding issues. Clearly we need to keep the 1.x line going for a bit, so a bug fix release sounds good, Although we've got some work to do before 2.0.0 it does look like it's within reach. Especially if declaring an RC creates more focus on resolving the most important blocker issues -- and if we do burn those down before 2.0.0 -- this sounds like a good step IMHO. I think that's OK to change, yes. I don't see why it's necessary to init log_ the way it is now. initializeLogIfNecessary() has a purpose though. If you have a clean test case demonstrating the desired behavior, and a change which makes it work that way, yes make a JIRA and PR. I consistently get this error in the launcher suite on Ubuntu 16. I don't know if it's a known issue. ------------------------------------------------------- T E S T S ------------------------------------------------------- OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=512m; support was removed in 8.0 Running org.apache.spark.launcher.SparkSubmitOptionParserSuite Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.333 sec - in org.apache.spark.launcher.SparkSubmitOptionParserSuite Running org.apache.spark.launcher.LauncherServerSuite Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.376 sec <<< FAILURE! - in org.apache.spark.launcher.LauncherServerSuite testCommunication(org.apache.spark.launcher.LauncherServerSuite)  Time elapsed: 0.078 sec  <<< FAILURE! java.lang.AssertionError: null at org.junit.Assert.fail(Assert.java:86) at org.junit.Assert.assertTrue(Assert.java:41) at org.junit.Assert.assertNotNull(Assert.java:621) at org.junit.Assert.assertNotNull(Assert.java:631) at org.apache.spark.launcher.LauncherServerSuite.testCommunication(LauncherServerSuite.java:98) This is only my opinion, but, I really do not expect a 1.7.0. I can imagine a 1.6.3 bug fix release in a few months, but kind of doubt it would continue much past that. I'm getting some errors building on Ubuntu 16 + Java 7. First is one that may just be down to a Scala bug: [ERROR] bad symbolic reference. A signature in WebUI.class refers to term eclipse in package org which is not available. It may be completely missing from the current classpath, or the version on the classpath might be incompatible with the version used when compiling WebUI.class. [ERROR] bad symbolic reference. A signature in WebUI.class refers to term jetty in value org.eclipse which is not available. It may be completely missing from the current classpath, or the version on the classpath might be incompatible with the version used when compiling WebUI.class. But I'm seeing some consistent timezone-related failures, from core: UIUtilsSuite: - formatBatchTime *** FAILED *** "2015/05/14 [14]:04:40" did not equal "2015/05/14 [21]:04:40"(UIUtilsSuite.scala:73) and several from Spark SQL, like: - udf_unix_timestamp *** FAILED *** Results do not match for udf_unix_timestamp: == Parsed Logical Plan == 'Project [unresolvedalias(2009-03-20 11:30:01),unresolvedalias('unix_timestamp(2009-03-20 11:30:01))] +- 'UnresolvedRelation `oneline`, None == Analyzed Logical Plan == _c0: string, _c1: bigint Project [2009-03-20 11:30:01 AS _c0#122914,unixtimestamp(2009-03-20 11:30:01,yyyy-MM-dd HH:mm:ss) AS _c1#122915L] +- MetastoreRelation default, oneline, None == Optimized Logical Plan == Project [2009-03-20 11:30:01 AS _c0#122914,1237548601 AS _c1#122915L] +- MetastoreRelation default, oneline, None == Physical Plan == Project [2009-03-20 11:30:01 AS _c0#122914,1237548601 AS _c1#122915L] +- HiveTableScan MetastoreRelation default, oneline, None _c0 _c1 !== HIVE - 1 row(s) ==            == CATALYST - 1 row(s) == !2009-03-20 11:30:01 1237573801   2009-03-20 11:30:01 1237548601 (HiveComparisonTest.scala:458) I'll start looking into them. It could be real, if possibly minor, bugs because I presume most of the testing happens on machines in a PDT timezone instead of UTC? that's at least the timezone of the machine I'm testing on. Nice one, yeah indeed I was doing an incremental build. Not a blocker. I'll have a look into the others, though I suspect they're problems with tests rather than production code. While I'd officially -1 this while there are still many blockers, this should certainly be tested as usual, because they're mostly doc and "audit" type issues. I'm fairly convinced this error and others that appear timestamp related are an environment problem. This test and method have been present for several Spark versions, without change. I reviewed the logic and it seems sound, explicitly setting the time zone correctly. I am not sure why it behaves differently on this machine. I'd give a +1 to this release if nobody else is seeing errors like this. The sigs, hashes, other tests pass for me. Oops, one more in the "does anybody else see this" department: - offset recovery *** FAILED *** recoveredOffsetRanges.forall(((or: (org.apache.spark.streaming.Time, earlierOffsetRangesAsSets.contains(scala.Tuple2.apply[org.apache.spark.streaming.Time, scala.collection.immutable.Set[org.apache.spark.streaming.kafka.OffsetRange]](or._1, scala.this.Predef.refArrayOps[org.apache.spark.streaming.kafka.OffsetRange](or._2).toSet[org.apache.spark.streaming.kafka.OffsetRange])))) was false Recovered ranges are not the same as the ones generated (DirectKafkaStreamSuite.scala:301) This actually fails consistently for me too in the Kafka integration code. Not timezone related, I think. Good call, probably worth back-porting, I'll try to do that. I don't think it blocks a release, but would be good to get into a next RC if any. I profess ignorance again though I really should know by now, but, what's opposing that? I personally thought this was going to be in 2.0 and didn't kind of notice it wasn't ... Hm, I thought that was to be added for 2.0. Imran I know you may have been working alongside Mark on it; what do you think? TD / Reynold would you object to it for 2.0? First pass of feedback on the RC: all the sigs, hashes, etc are fine. Licensing is up to date to the best of my knowledge. I'm hitting test failures, some of which may be spurious. Just putting them out there to see if they ring bells. This is Java 8 on Ubuntu 16. - spilling with compression *** FAILED *** java.lang.Exception: Test failed with compression using codec org.apache.spark.io.SnappyCompressionCodec: assertion failed: expected cogroup to spill, but did not at scala.Predef$.assert(Predef.scala:170) at org.apache.spark.TestUtils$.assertSpilled(TestUtils.scala:170) at org.apache.spark.util.collection.ExternalAppendOnlyMapSuite.org$apache$spark$util$collection$ExternalAppendOnlyMapSuite$$testSimpleSpilling(ExternalAppendOnlyMapSuite.scala:263) ... I feel like I've seen this before, and see some possibly relevant fixes, but they're in 2.0.0 already: https://github.com/apache/spark/pull/10990 Is this something where a native library needs to be installed or something? - to UTC timestamp *** FAILED *** "2016-03-13 [02]:00:00.0" did not equal "2016-03-13 [10]:00:00.0"(DateTimeUtilsSuite.scala:506) I know, we talked about this for the 1.6.2 RC, but I reproduced this locally too. I will investigate, could still be spurious. StateStoreSuite: - maintenance *** FAILED *** The code passed to eventually never returned normally. Attempted 627 times over 10.000180116 seconds. Last failure message: StateStoreSuite.this.fileExists(provider, 1L, false) was true earliest file not deleted. (StateStoreSuite.scala:395) No idea. - offset recovery *** FAILED *** The code passed to eventually never returned normally. Attempted 197 times over 10.040864806 seconds. Last failure message: strings.forall({}) was false. (DirectKafkaStreamSuite.scala:250) Also something that was possibly fixed already for 2.0.0 and that I just back-ported into 1.6. Could be just a very similar failure. That seems OK. If it introduces another module dependency we'd have to think about it. I assume these constants should really be used consistently everywhere if possible, just because it otherwise means duplicating the defaults and possibly incorrectly. I think you could have a look at that more broadly too. TD has literally just merged the fix. Yeah, interesting question about whether it should be 2.0.1-SNAPSHOT at this stage because 2.0.0 is not yet released. But I'm not sure we publish snapshots anyway? So, on the one hand I think branch-2.0 should really still be on 2.0.0-SNAPSHOT but is on 2.0.1-SNAPSHOT, and while master should technically be on 2.1.0-SNAPSHOT but we can't quite because of MiMa right now, I do see that both snapshots are being produced still: https://repository.apache.org/content/groups/snapshots/org/apache/spark/spark-core_2.11/ 2.0.0-SNAPSHOT is actually from master, kinda confusingly. Not sure if that helps. Because a 2.0.0 release candidate is out. If for some reason the release candidate becomes the 2.0.0 release, then anything merged to branch-2.0 after it is necessarily fixed in 2.0.1 at best. At this stage we know the RC1 will not be 2.0.0, so really that vote should be formally cancelled. Then we just mark anything fixed for 2.0.1 as fixed for 2.0.0 and make another RC. master is not what will be released as 2.0.0. branch-2.0 is what will contain that release. I am not sure any other process makes sense. What are you suggesting should happen? It's not that you're starting 2.1 per se, but, that you're committing things that are not in 2.0. Releases are never made from master in moderately complex projects. It has nothing to do with pace of release. Do you not mean ds.foreachPartition(_.foreach(println)) or similar? A DStream is a sequence of RDDs, not of elements. I don't think I'd expect to express an operation on a DStream as if it were elements. Right, should have noticed that in your second mail. But foreach already does what you want, right? it would be identical here. How these two methods do conceptually different things on different arguments. I don't think I'd expect them to accept the same functions. Yeah we still have some blockers; I agree SPARK-16379 is a blocker which came up yesterday. We also have 5 existing blockers, all doc related: SPARK-14808 Spark MLlib, GraphX, SparkR 2.0 QA umbrella SPARK-14812 ML, Graph 2.0 QA: API: Experimental, DeveloperApi, final, sealed audit SPARK-14816 Update MLlib, GraphX, SparkR websites for 2.0 SPARK-14817 ML, Graph, R 2.0 QA: Programming guide update and migration guide SPARK-15124 R 2.0 QA: New R APIs and API docs While we'll almost surely need another RC, this one is well worth testing. It's much closer than even the last one. The sigs/hashes check out, and I successfully built with Ubuntu 16 / Java 8 with -Pyarn -Phadoop-2.7 -Phive. Tests pass except for: DirectKafkaStreamSuite: - offset recovery *** FAILED *** The code passed to eventually never returned normally. Attempted 196 times over 10.028979855 seconds. Last failure message: strings.forall({}) was false. (DirectKafkaStreamSuite.scala:250) - Direct Kafka stream report input information I know we've seen this before and tried to fix it but it may need another look. -user Reynold made the comment that he thinks this was resolved by another change; maybe he can comment. I don't agree that every change needs a JIRA, myself. Really, we didn't choose to have this system split across JIRA and Github PRs. It's necessitated by how the ASF works (and with some good reasons). But while we have this dual system, I figure, let's try to make some sense of it. I think it makes sense to make a JIRA for any non-trivial change. What's non-trivial? where the "how" is different from the "what". That is, if the JIRA is not just a repeat of the pull request, they should probably be separate. But, if the change is so simple that describing it amounts to dictating how it's implemented -- well, seems like a JIRA is just overhead. ONe problem that I think happened above was: pretty non-trivial things were being merged without a JIRA. The evidence? they were reverted. That means their effect was not quite obvious. They probably deserved more discussion. Anything that needs some discussion probably deserves a JIRA. Also: we have some hot-fixes here that aren't connected to JIRAs. Either they belong with an existing JIRA and aren't tagged correctly, or, again, are patching changes that weren't really trivial enough to skip a JIRA to begin with. You are right, this is messed up a bit now. branch-2.0 should really still be 2.0.0-SNAPSHOT, technically. I think that was accidentally updated in the RC release. It won't matter a whole lot except for people who consume snapshots, but, you can always roll your own. After 2.0.0 it should be 2.0.1-SNAPSHOT anyway. Master isn't done yet because of a hiccup in the API checking component, MiMa. It should really be on 2.1.0-SNAPSHOT. At the latest it will be so after 2.0.0 is released but it sorta looks like Reynold maybe has an answer as of a few hours ago? Sean I agree -- Wenchen/Reynold do you know what's the theory there? TBH I think that there has not been a 'real' release candidate yet. It's not that big a deal if these first two have been speculative RCs to get more feedback earlier for a major release, and that in fact people want to let this bake somewhat longer that the RC would imply. As long as it's converging towards fewer, more critical changes. Excepting these merges I think that had been generally happening. It's *mostly* critical stuff now. But yeah this won't actually get released until blockers are resolved and merges slow down to what belongs in a maintenance branch. Eh, to anyone else who's ever pushed to the SVN-hosted spark.apache.org site: are you able to commit anything right now? This error is brand-new and has stumped me: svn: E195023: Changing file '/Users/srowen/Documents/asf-spark-site/downloads.md' is forbidden by the server svn: E175013: Access to '/repos/asf/!svn/txr/1752209-12gpm/spark/downloads.md' forbidden Maybe my perms got messed up, so, first checking to see if it affects anyone else. FWIW this is all I'm trying to change; anyone is welcome to commit this: Index: downloads.md IDEA additional info: Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP UTF-8 =================================================================== --- downloads.md (revision 1752185) +++ downloads.md (revision ) @@ -31,7 +31,7 @@ +5. Verify this release using the  and [project release KEYS](https://www.apache.org/dist/spark/KEYS). _Note: Scala 2.11 users should download the Spark source package and build [with Scala 2.11 support](http://spark.apache.org/docs/latest/building-spark.html#building-for-scala-211)._ Index: site/downloads.html IDEA additional info: Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP UTF-8 =================================================================== --- site/downloads.html (revision 1752185) +++ site/downloads.html (revision ) @@ -213,7 +213,7 @@ -    Verify this release using the +    Verify this release using the  and project release KEYS Aha, that's landed. OK I'll figure it out tomorrow and push my update to verify it all works. PS I've already opened a test PR for the new apache/spark-website repo: https://github.com/apache/spark-website/pull/1 I guess we'll follow the same process for reviewing there. Next: see if the main merge script works for this repo! It's activated by a profile called 'yarn', like several other modules. Signatures and hashes are OK. I built and ran tests successfully on Ubuntu 16 + Java 8 with "-Phive -Phadoop-2.7 -Pyarn". Although I encountered a few tests failures, none were repeatable. Regarding other issues brought up so far: SPARK-16522 Does not seem quite enough to be a blocker if it's just an error at shutdown that does not affect the result. If there's another RC, worth fixing. SPARK-15899 Not a blocker. Only affects Windows and possibly even only affects tests. Not a regression. SPARK-16515 Not sure but Cheng please mark it a Blocker if you're pretty confident it must be fixed. Davies marked SPARK-16011 a Blocker, though should confirm that it's for 2.0.0. That's the only one officially open now. So I suppose that's provisionally a -1 from me as it's not clear there aren't blocking issues. It's close, and this should be tested by everyone. Remaining Critical issues are below. I'm still uncomfortable with documentation issues for 2.0 not being done before 2.0. If anyone's intent is to release and then finish the docs a few days later, I'd vote against that. There's just no rush that makes that make sense. However it's entirely possible that the remaining work is not essential for 2.0; I don't know. These should be retitled then. But to make this make sense, one or the other needs to happen. "Audit" JIRAs are similar, especially before a major release. SPARK-13393 Column mismatch issue in left_outer join using Spark DataFrame SPARK-13753 Column nullable is derived incorrectly SPARK-13959 Audit MiMa excludes added in SPARK-13948 to make sure none are unintended incompatibilities SPARK-14808 Spark MLlib, GraphX, SparkR 2.0 QA umbrella SPARK-14816 Update MLlib, GraphX, SparkR websites for 2.0 SPARK-14817 ML, Graph, R 2.0 QA: Programming guide update and migration guide SPARK-14823 Fix all references to HiveContext in comments and docs SPARK-15340 Limit the size of the map used to cache JobConfs to void OOM SPARK-15393 Writing empty Dataframes doesn't save any _metadata files SPARK-15703 Spark UI doesn't show all tasks as completed when it should SPARK-15944 Make spark.ml package backward compatible with spark.mllib vectors SPARK-16032 Audit semantics of various insertion operations related to partitioned tables SPARK-16090 Improve method grouping in SparkR generated docs SPARK-16301 Analyzer rule for resolving using joins should respect case sensitivity setting I think unfortunately at least this one is gonna block: https://issues.apache.org/jira/browse/SPARK-16620 Good news is that just about anything else that's at all a blocker has been resolved and there are only about 6 issues of any kind at all targeted for 2.0. It seems very close. If the change is just to replace "sbt assembly/assembly" with "sbt package", done. LMK if there are more edits. +1 at last. Sigs and hashes check out, and compiles and passes tests with "-Pyarn -Phadoop-2.7 -Phive" on Ubuntu 16 + Java 8. There are actually only 2 issues still targeted for 2.0.0, which is great: SPARK-16633 lag/lead does not return the default value when the offset row does not exist SPARK-16648 LAST_VALUE(FALSE) OVER () throws IndexOutOfBoundsException These are not marked blocker, though one is critical. I will assume these don't block. The only other JIRA that seems to be "for 2.0" and not resolved is... https://issues.apache.org/jira/browse/SPARK-16486 ... which I suspect is actually just something to be renamed and pushed out. I did encounter two test failures that weren't reproducible, just FYI: ExecutorAllocationManagerSuite: - basic functionality *** FAILED *** The code passed to eventually never returned normally. Attempted 613 times over 10.015362111999998 seconds. Last failure message: Wanted but not invoked: executorAllocationClient.killExecutor("2"); Actually, there were zero interactions with this mock. . (ExecutorAllocationManagerSuite.scala:61) StateStoreSuite: - maintenance *** FAILED *** The code passed to eventually never returned normally. Attempted 611 times over 10.007739936 seconds. Last failure message: StateStoreSuite.this.fileExists(provider, 1L, false) was true earliest file not deleted. (StateStoreSuite.scala:395) I had favored this for 2.0 even, so favor it sooner than later. The general shape of the argument is: - supporting Java 7 is starting to pinch a little because of extra builds and the inevitable gap between what the PR builder (7) tests and what the later Java 8 tests runs show - requiring Java 8 would allow a few minor cleanups in the code - Java 7 is already EOL (no, I don't count paying Oracle for support) but mostly importantly, - anyone can keep using 2.0 if they want to use Java 7. This does *not* require people to use Java 8. It requires them to use Java 8 to use the very latest version. The release vote has already closed and passed. Derby is only used in tests AFAIK, so I don't think this is even critical let alone a blocker. Updating is fine though, open a PR. Yeah I tried to zap lots of those before the release, but there are still many of them, mostly from the accumulator change (really, that should have been fixed as part of the original change? not so nice to merge a change that adds 200 build warnings). Some deprecated code must be called from tests to still test the deprecated code but it ought to be possible to make the non-test code avoid it entirely. This is https://issues.apache.org/jira/browse/SPARK-15899 Good catch, yes, it's the scaladoc of addShutdownHook that is wrong. It says lower priority executes firs.t The implementation seems to do the opposite. It uses a min queue of shutdown hooks, but inverts the notion of ordering to execute higher priority values first. The constants and comments in ShutdownHookManager are consistent with executing higher priority values first. So I think you can fix the scaladoc. Other usages of priority seem consistent with current behavior. https://issues.apache.org/jira/browse/SPARK-16216 https://github.com/apache/spark/pull/14279 This concerns default representation of times and dates in CSV and JSON. CSV has UNIX timestamps; JSON has formatted strings but unfortunately they lack timezones. The question here is which to change to do what; it was a significant enough potential change that I wanted to draw attention to it for more comment. Sean --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org There was a recent message about deprecating many Maven, ant and JDK combos for ASF Jenkins machines, and I was just triple-checking we're only making use of the Amplab ones. --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Not a big deal but 'he' is commenting on a lot of ancient PRs for some reason, like https://github.com/apache/spark/pull/51 and it generates mails to the list. I assume this is a misconfiguration somewhere. --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org I believe Chris was being a bit facetious. The ASF guidance is right, that it's important people don't consume non-blessed snapshot builds as like other releases. The intended audience is developers and so the easiest default policy is to only advertise the snapshots where only developers are likely to be looking. That said, they're not secret or confidential, and while this probably should go to dev@, it's not a sin to mention the name of snapshots on user@, as long as these disclaimers are clear too. I'd rather a user understand the full picture, than find the snapshots and not understand any of the context. Byte code, no. It's sufficient to store the information that the RDD represents, which can include serialized function closures, but that's not quite storing byte code. This is also what "classifiers" are for in Maven, to have variations on one artifact and version. https://maven.apache.org/pom.html It has been used to ship code for Hadoop 1 vs 2 APIs. In a way it's the same idea as Scala's "_2.xx" naming convention, with a less unfortunate implementation. If you're just varying versions (or things that can be controlled by a profile, which is most everything including dependencies), you don't need and probably don't want multiple POM files. Even that wouldn't mean you can't use classifiers. I have seen it used for HBase, core Hadoop. I am not sure I've seen it used for Spark 2 vs 1 but no reason it couldn't be. Frequently projects would instead declare that as of some version, Spark 2 is required, rather than support both. Or shim over an API difference with reflection if that's all there was to it. Spark does both of those sorts of things itself to avoid having to publish multiple variants at all. (Well, except for Scala 2.10 vs 2.11!) Yes you want to actively unpersist() or destroy() broadcast variables when they're no longer needed. They can eventually be removed when the reference on the driver is garbage collected, but you usually would not want to rely on that. Yes, although there's a difference between unpersist and destroy, you'll hit the same type of question either way. You do indeed have to reason about when you know the broadcast variable is no longer needed in the face of lazy evaluation, and that's hard. Sometimes it's obvious and you can take advantage of this to proactively free resources. You may have to consider restructuring the computation to allow for more resources to be freed, if this is important to scale. Keep in mind that things that are computed and cached may be lost and recomputed even after their parent RDDs were definitely already computed and don't seem to be needed. This is why unpersist is often the better thing to call because it allows for variables to be rebroadcast if needed in this case. Destroy permanently closes the broadcast. Yeah, after destroy, accessing the broadcast variable results in an error. Accessing it after it's unpersisted (on an executor) causes it to be rebroadcast. I'm not sure it's a UI bug; it really does record two different stages, the second of which executes quickly. I am not sure why that would happen off the top of my head. I don't see anything that failed here. Digging into those two stages and what they executed might give a clue to what's really going on there. I have the heady power to modify Jenkins jobs now, so I will carefully take a look at them and see if any of the config needs -Pmesos. But yeah I thought this should be baked into the script. Ah, I helped miss that. We don't enable -Pyarn for YARN because it's already always set? I wonder if it makes sense to make that optional in order to speed up builds, or, maybe I'm missing a reason it's always essential. I think it's not setting -Pmesos indeed because no Mesos code was changed but I think that script change is necessary as a follow up yes. Yeah, nothing is in the Jenkins config itself. Hm, what do you mean? k-means|| init is certainly slower because it's making passes over the data in order to pick better initial centroids. The idea is that you might then spend fewer iterations converging later, and converge to a better clustering. Your problem doesn't seem to be related to scale. You aren't even running out of memory it seems. Your memory settings are causing YARN to kill the executors for using more memory than they advertise. That could mean it never proceeds if this happens a lot. I don't have any problems with it. Yes it works fine, though each iteration of the parallel init step is slow indeed -- about 5 minutes on my cluster. Given your question I think you are actually 'hanging' because resources are being killed. I think this init may need some love and optimization. For example, I think treeAggregate might work better. An Array[Float] may be just fine and cut down memory usage, etc. Eh... more specifically, since Spark 2.0 the "runs" parameter in the KMeans mllib implementation has been ignored and is always 1. This means a lot of code that wraps this stuff up in arrays could be simplified quite a lot. I'll take a shot at optimizing this code and see if I can measure an effect. I opened https://issues.apache.org/jira/browse/SPARK-17389 to track some improvements, but by far the big one is that the init steps defaults to 5, when the paper says that 2 is pretty much optimal here. It's much faster with that setting. It's worth calling attention to: https://issues.apache.org/jira/browse/SPARK-17418 https://issues.apache.org/jira/browse/SPARK-17422 It looks like we need to at least not publish the kinesis *assembly* Maven artifact because it contains Amazon Software Licensed-code directly. However there's a reasonably strong reason to believe that we'd have to remove the non-assembly Kinesis artifact too, as well as the Ganglia one. This doesn't mean it goes away from the project, just means it would no longer be published as a Maven artifact. (These have never been bundled in the main Spark artifacts.) I wanted to give a heads up to see if anyone a) believes this conclusion is wrong or b) wants to take it up with legal@? I'm inclined to believe we have to remove them given the interpretation Luciano has put forth. Sean --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org (Credit to Luciano for pointing it out) Yes it's clear why the assembly can't be published but I had the same question about the non-assembly Kinesis (and ganglia) artifact, because the published artifact has no code from Kinesis. See the related discussion at https://issues.apache.org/jira/browse/LEGAL-198 ; the point I took from there is that the Spark Kinesis artifact is optional with respect to Spark, but still something published by Spark, and it requires the Amazon-licensed code non-optionally. I'll just ask that question to confirm or deny. (It also has some background on why the Amazon License is considered "Category X" in ASF policy due to field of use restrictions. I myself take that as read rather than know the details of that decision.) Agree, I've asked the question on that thread and will follow it up. I'd prefer not to pull these unless it's fairly clear it's going to be against policy. I think the @_root_ version is redundant because @scala.annotation.varargs is redundant. Actually wouldn't we just import varargs and write @varargs? Oh I get it now. I was necessary in the past. Sure, seems like it could be standardized now. There are almost no cases in which you'd want a zero-partition RDD. The only one I can think of is an empty RDD, where the number of partitions is irrelevant. Still, I would not be surprised if other parts of the code assume at least 1 partition. Maybe this check could be tightened. It would be interesting to see if the tests catch any scenario where a 0-partition RDD is created, and why. "null" is a valid value in an RDD, so it has to be partition-able. There are a few blockers for 2.0.1, but just two. For example https://issues.apache.org/jira/browse/SPARK-17418 must be resolved before another release. No, these are different major versions of these components, each of which gets used by something in the transitive dependency graph. They are not redundant because they're not actually presenting roughly the same component in the same namespace. However the parquet-hadoop bit looks wrong, in that it should be harmonized to one 1.x version. It's not that Spark uses inconsistent versions but that transitive deps do. We can still harmonize them in the build if it causes problems. Boy it's a long story, but I think the short answer is that it's only worth manually fixing the mismatches that are clearly going to cause a problem. Dependency version mismatch is inevitable, and Maven will always settle on one version of a particular group/artifact using a nearest-wins rule (SBT uses latest-wins). However it's possible that you get different versions of closely-related artifacts. Often it doesn't matter; sometimes it does. It's always possible to force a version of a group/artifact with . The drawback is that, as dependencies evolve, you may be silently forcing that to an older version than other dependencies want. It builds up its own quiet legacy problem. There can be just one published version of the Spark artifacts and they have to depend on something, though in truth they'd be binary-compatible with anything 2.2+. So you merely manage the dependency versions up to the desired version in your . FWIW it worked for me, but I may not be executing the same thing. I was running the commands given in R/DOCUMENTATION.md It succeeded for me in creating the vignette, on branch-2.0. Maybe it's a version or library issue? what R do you have installed, and are you up to date with packages like devtools and roxygen2? +1 Signatures and hashes check out. I checked that the Kinesis assembly artifacts are not present. I compiled and tested on Java 8 / Ubuntu 16 with -Pyarn -Phive -Phive-thriftserver -Phadoop-2.7 -Psparkr and only saw one test problem. This test never completed. If nobody else sees it, +1, assuming it's a bad test or env issue. - should clone and clean line object in ClosureCleaner *** FAILED *** isContain was true Interpreter output contained 'Exception': Welcome to ____              __ / __/__  ___ _____/ /__ _\ \/ _ \/ _ `/ __/  '_/ /___/ .__/\_,_/_/ /_/\_\   version 2.0.1 /_/ Using Scala version 2.11.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_91) Type in expressions to have them evaluated. Type :help for more information. // Exiting paste mode, now interpreting. org.apache.spark.SparkException: Job 0 cancelled because SparkContext was shut down at org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:818) ... The binary artifact that's published here is built with -Pmesos. The 'real' artifact from a process standpoint is the source release, however. That's why we do (should) test the source release foremost. I suppose individuals are invited to test with a configuration that is of interest to them, and so I enable just the flags I would care to test I suppose. It was added to the master branch, and this is a release from the 2.0.x branch. It's a change to the structure of the project, and probably not appropriate for a maintenance release. 2.0.1 core would then no longer contain Mesos code while 2.0.0 did. Master is implicitly 2.1.x right now. When branch-2.1 is cut, master becomes the de facto 2.2.x branch. It's not true that the next release is 2.0.2. You can see the master version: https://github.com/apache/spark/blob/master/pom.xml#L29 +1 binding. Same as for RC2 -- it all checks out, from license to sigs to compile and test. We have no issues of any kind targeted for 2.0.1. I do have one test failure that I have seen with some regularity in Ubuntu but not on any Jenkins machines. One of the YARN tests will just kill all my ssh sessions (!) YarnClusterSuite: ... - run Spark in yarn-cluster mode with additional jar - run Spark in yarn-cluster mode unsuccessfully Connection to ... closed by remote host. Every time. I wonder if anyone's seen that while testing? But it's no blocker. (Renaming thread to keep it separate from RC vote) If you're asking why there's a version 2.0.2 in JIRA, it's because we have to have that entity in order to target anything to version 2.0.2. 2.2.0 exists as well as version labels in JIRA. None are marked as 'released' because there is no such release, but it makes perfect sense to have a noun to talk about in JIRA. There may never be a 2.0.2 software release. But anything committed to 2.0.x after this point will be released in 2.0.2 _if it does get released_. If 2.0.2 happens it may happen before or after 2.1.0; that's normal. I suspect it would happen after, if ever, and I expect the next actual software release chronologically will be 2.1.0. I think this is all standard software procedure; what's the confusion? There is no formal plan for when which releases happen, so I don't think anyone can answer that definitively. I happens when it happens by loose consensus. The -Pmesos change is not in branch-2.0 and therefore would not be in any 2.0.x release. +1 -- I think the minor releases were taking more like 4 months than 3 months anyway, and it was good for the reasons you give. This reflects reality and is a good thing. All the better if we then can more comfortably really follow the timeline. (Process-wise there's no problem with that. The vote is open for at least 3 days and ends when the RM says it ends. So it's valid anyway as the vote is still open.) My guess is that Kryo specially handles Maps generically or relies on some mechanism that does, and it happens to iterate over all key/values as part of that and of course there aren't actually any key/values in the map. The Java serialization is a much more literal (expensive) field-by-field serialization which works here because there's no special treatment. I think you could register a custom serializer that handles this case. Or work around it in your client code. I know there have been other issues with Kryo and Map because, for example, sometimes a Map in an application is actually some non-serializable wrapper view. No, I think that's what dependencyManagent (or equivalent) is definitely for. +1 from me too, same result as my RC3 vote/testing. https://github.com/apache/spark/releases/tag/v2.0.1 ? Yes this come up once in a while. There's no need or way to stop people forming groups to chat, though blessing a new channel as 'official' is tough because it means, in theory, everyone has to follow another channel to see 100% of the discussion. I think that's why the couple of mailing lists, which can be controlled and archived by the ASF, will stay the official channels. But, naturally there's no problem with people forming unofficial communities. I believe Reynold mentioned he already did that. For anyone following: https://issues.apache.org/jira/browse/INFRA-12717 dev@ is for the project's own development discussions, so not the right place. user@ is better, but job postings are discouraged in general on ASF lists. I think people get away with the occasional legitimate, targeted message prefixed with [JOBS], but I hesitate to open the flood gates, because we also have no real way of banning the inevitable spam. Suggestion actions way at the bottom. That flood of emails means several people (Xiao, Holden mostly AFAICT) have been updating the status of old JIRAs. Thank you, I think that really does help. I have a suggested set of conventions I've been using, just to bring some order to the resolutions. It helps because JIRA functions as a huge archive of decisions and the more accurately we can record that the better. What do people think of this? - Resolve as Fixed if there's a change you can point to that resolved the issue - If the issue is a proper subset of another issue, mark it a Duplicate of that issue (rather than the other way around) - If it's probably resolved, but not obvious what fixed it or when, then Cannot Reproduce or Not a Problem - Obsolete issue? Not a Problem - If it's a coherent issue but does not seem like there is support or interest in acting on it, then Won't Fix - If the issue doesn't make sense (non-Spark issue, etc) then Invalid - I tend to mark Umbrellas as "Done" when done if they're just containers - Try to set Fix version - Try to set Assignee to the person who most contributed to the resolution. Usually the person who opened the PR. Strong preference for ties going to the more 'junior' contributor The only ones I think are sort of important are getting the Duplicate pointers right, and possibly making sure that Fixed issues have a clear path to finding what change fixed it and when. The rest doesn't matter much. I know what you mean Ted, and I think actually what's happening here is fine, but I'll explain my theory. There are a range of actions in the project where someone makes a decision, from answering an email to creating a release. We already accept that only some of these require a formal status; anyone can answer emails, but only the PMC can bless a release, for example. The reason commits and releases require a certain status is not _entirely_ to block most people from participating in these activities. It's in part because things the ASF's liability protections for releases depend on the existence of well-defined governance models, that wouldn't quite be compatible with anyone adding software at will. Issue management isn't in this category, and, of course, we let anyone make JIRAs and PRs. This causes problems occasionally but is on the whole powerfully good. So, it seems reasonable to let people close JIRAs if, in good faith, they have clear reason to do so. These things are reversible, too. I also think there's a cost to not getting this triage work done, just as there would be a cost to blocking people from creating issues. I've reviewed the cleanup in the past 24 hours and agree with virtually every action, so I have confidence that in practice this is a big positive. That said, I have suggested in the past that perhaps only committers should set "Blocker" and "Target Version", because this communicates something specific about what will be committed and in what release, and acting on those requires commit access. Although by the theory above we should let anyone set these -- and actually, we do -- I have found it usually set incorrectly, and so, argue that these fields should be treated differently as a matter of convention. Sean I added a variant on this text to https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-ContributingtoJIRAMaintenance Yes, it's really CONTRIBUTING.md that's more relevant, because github displays a link to it when opening pull requests. https://github.com/apache/spark/blob/master/CONTRIBUTING.md  There is also the pull request template: https://github.com/apache/spark/blob/master/.github/PULL_REQUEST_TEMPLATE I wouldn't want to duplicate info too much, but more pointers to a single source of information seems OK. Although I don't know if it will help much, sure, pointers from README.md are OK. I am pretty sure that's https://issues.apache.org/jira/browse/SPARK-17707 I have also 'embedded' a Spark driver without much trouble. It isn't that it can't work. The Launcher API is ptobably the recommended way to do that though. spark-submit is the way to go for non programmatic access. If you're not doing one of those things and it is not working, yeah I think people would tell you you're on your own. I think that's consistent with all the JIRA discussions I have seen over time. I'm not sure this is applied consistently across Spark, but I'm dealing with another change now where an unqualified path is assumed to be a local file. The method Utils.resolvePath implements this logic and is used several places. Therefore I think this is probably intended behavior and you can write hdfs:///tmp if you mean to reference /tmp on HDFS. IIRC this was all about shading of dependencies, not changes to the source. (I don't think 2.0.2 will be released for a while if at all but that's not what you're asking I think) It's a fairly safe change, but also isn't exactly a fix in my opinion. Because there are some other changes to make it all work for SPARC, I think it's more realistic to look to the 2.1.0 release anyway, which is likely to come first. I'm OK with that. The upside to the wiki is that it can be edited directly outside of a release cycle. However, in practice I find that the wiki is rarely changed. To me it also serves as a place for information that isn't exactly project documentation like "powered by" listings. In a way I'd like to get rid of the wiki to have one less place for docs, that doesn't have the same accessibility (I don't know who can give edit access), and doesn't have a review process. For now I'd settle for bringing over a few key docs like the one you mention. I spent a little time a while ago removing some duplication across the wiki and project docs and think there's a bit more than could be done. Yeah I see that too. I'll work on back-porting it. The release otherwise looks good to me, but let's keep testing please to identify anything else in the meantime. BTW I wrote up a straw-man proposal for migrating the wiki content: https://issues.apache.org/jira/browse/SPARK-18073 Well, it's more of a reference to the fallacy than anything. Writing down a proposed action implicitly claims it's what others are arguing for. It's self-deprecating to call it a "straw man", suggesting that it may not at all be what others are arguing for, and is done to openly invite criticism and feedback. The logical fallacy is "attacking a straw man", and that's not what was written here. Really, the important thing is that we understand each other, and I'm guessing you did. Although I think the usage here is fine, casually, avoiding idioms is best, where plain language suffices, especially given we have people from lots of language backgrounds here. I don't believe emails about the spark-website repo are forwarded to the project mailing lists. If you want to watch for them, go star/watch the repo to be sure. I just opened a PR, for example. https://github.com/apache/spark-website I'd like to gauge where people stand on the issue of dropping support for a few things that were considered for 2.0. First: Scala 2.10. We've seen a number of build breakages this week because the PR builder only tests 2.11. No big deal at this stage, but, it did cause me to wonder whether it's time to plan to drop 2.10 support, especially with 2.12 coming soon. Next, Java 7. It's reasonably old and out of public updates at this stage. It's not that painful to keep supporting, to be honest. It would simplify some bits of code, some scripts, some testing. Hadoop versions: I think the the general argument is that most anyone would be using, at the least, 2.6, and it would simplify some code that has to reflect to use not-even-that-new APIs. It would remove some moderate complexity in the build. "When" is a tricky question. Although it's a little aggressive for minor releases, I think these will all happen before 3.x regardless. 2.1.0 is not out of the question, though coming soon. What about ... 2.2.0? Although I tend to favor dropping support, I'm mostly asking for current opinions. The general forces are that new versions of things to support emerge, and are valuable to support, but have some cost to support in addition to old versions. And the old versions become less used and therefore less valuable to support, and at some point it tips to being more cost than value. It's hard to judge these costs and benefits. Scala is perhaps the trickiest one because of the general mutual incompatibilities across minor versions. The cost of supporting multiple versions is high, and a third version is about to arrive. That's probably the most pressing question. It's actually biting with some regularity now, with compile errors on 2.10. (Python I confess I don't have an informed opinion about.) Java, Hadoop are not as urgent because they're more backwards-compatible. Anecdotally, I'd be surprised if anyone today would "upgrade" to Java 7 or an old Hadoop version. And I think that's really the question. Even if one decided to drop support for all this in 2.1.0, it would not mean people can't use Spark with these things. It merely means they can't necessarily use Spark 2.1.x. This is why we have maintenance branches for 1.6.x, 2.0.x. Tying Scala 2.11/12 support to Java 8 might make sense. In fact, I think that's part of the reason that an update in master, perhaps 2.1.x, could be overdue, because it actually is just the beginning of the end of the support burden. If you want to stop dealing with these in ~6 months they need to stop being supported in minor branches by right about now. Seems OK by me. How about Hadoop < 2.6, Python 2.6? Those seem more removeable. I'd like to add that to a list of things that will begin to be unsupported 6 months from now. The burden may be a little more apparent when dealing with the day to day merging and fixing of breaks. The upside is maybe the more compelling argument though. For example, lambda-fying all the Java code, supporting java.time, and taking advantage of some newer Hadoop/YARN APIs is a moderate win for users too, and there's also a cost to not doing that. I must say I don't see a risk of fragmentation as nearly the problem it's made out to be here. We are, after all, here discussing _beginning_ to remove support _in 6 months_, for long since non-current versions of things. An org's decision to not, say, use Java 8 is a decision to not use the new version of lots of things. It's not clear this is a constituency that is either large or one to reasonably serve indefinitely. In the end, the Scala issue may be decisive. Supporting 2.10 - 2.12 simultaneously is a bridge too far, and if 2.12 requires Java 8, it's a good reason to for Spark to require Java 8. And Steve suggests that means a minimum of Hadoop 2.6 too. (I still profess ignorance of the Python part of the issue.) Put another way I am not sure what the criteria is, if not the above? I support deprecating all of these things, at the least, in 2.1.0. Although it's a separate question, I believe it's going to be necessary to remove support in ~6 months in 2.2.0. If the subtext is vendors, then I'd have a look at what recent distros look like. I'll write about CDH as a representative example, but I think other distros are naturally similar. CDH has been on Java 8, Hadoop 2.6, Python 2.7 for almost two years (CDH 5.3 / Dec 2014). Granted, this depends on installing on an OS with that Java / Python version. But Java 8 / Python 2.7 is available for all of the supported OSes. The population that isn't on CDH 4, because that supported was dropped a long time ago in Spark, and who is on a version released 2-2.5 years ago, and won't update, is a couple percent of the installed base. They do not in general want anything to change at all. I assure everyone that vendors too are aligned in wanting to cater to the crowd that wants the most recent version of everything. For example, CDH offers both Spark 2.0.1 and 1.6 at the same time. I wouldn't dismiss support for these supporting components as a relevant proxy for whether they are worth supporting in Spark. Java 7 is long since EOL (no, I don't count paying Oracle for support). No vendor is supporting Hadoop < 2.6. Scala 2.10 was EOL at the end of 2014. Is there a criteria here that reaches a different conclusion about these things just for Spark? This was roughly the same conversation that happened 6 months ago. I imagine we're going to find that in about 6 months it'll make more sense all around to remove these. If we can just give a heads up with deprecation and then kick the can down the road a bit more, that sounds like enough for now. It's required because the tags module uses it to define annotations for tests. I don't see it in compile scope for anything but the tags module, which is then in test scope for other modules. What are you seeing that makes you say it's in compile scope? Yes, but scalatest doesn't end up in compile scope, says Maven? ... [INFO] +- org.apache.spark:spark-tags_2.11:jar:2.1.0-SNAPSHOT:compile [INFO] |  +- (org.scalatest:scalatest_2.11:jar:2.2.6:test - scope managed from compile; omitted for duplicate) [INFO] |  \- (org.spark-project.spark:unused:jar:1.0.0:compile - omitted for duplicate) [INFO] +- org.apache.commons:commons-crypto:jar:1.0.0:compile [INFO] +- org.spark-project.spark:unused:jar:1.0.0:compile [INFO] +- org.scalatest:scalatest_2.11:jar:2.2.6:test ... Declare your scalatest dependency as test scope (which is correct anyway). That would override it I think as desired? SBT and Maven resolution rules do differ. I thought SBT was generally latest-first though, which should make 3.0 take priority. Maven is more like closest-first, which means you can pretty much always override things in your own build. An exclusion is the right way to go in this case because the deployed POM does look like it says what we all want it to say, at least. Yes this came up from a different direction: https://issues.apache.org/jira/browse/SPARK-18140 I think it's fine to pursue an upgrade to fix these several issues. The question is just how well it will play with other components, so bears some testing and evaluation of the changes from 1.8, but yes this would be good. I couldn't figure out why I was missing a lot of dev@ announcements, and have just realized hundreds of messages to dev@ over the past month or so have been marked as spam for me by Gmail. I have no idea why but it's usually messages from Michael and Reynold, but not all of them. I'll see replies to the messages but not the original. Who knows. I can make a filter. I just wanted to give a heads up in case anyone else has been silently missing a lot of messages. Sigs, license, etc are OK. There are no Blockers for 2.0.2, though here are the 4 issues still open: SPARK-14387 Enable Hive-1.x ORC compatibility with spark.sql.hive.convertMetastoreOrc SPARK-17957 Calling outer join and na.fill(0) and then inner join will miss rows SPARK-17981 Incorrectly Set Nullability to False in FilterExec SPARK-18160 spark.files & spark.jars should not be passed to driver in yarn mode Running with Java 8, -Pyarn -Phive -Phive-thriftserver -Phadoop-2.7 on Ubuntu 16, I am seeing consistent failures in this test below. I think we very recently changed this so it could be legitimate. But does anyone else see something like this? I have seen other failures in this test due to OOM but my MAVEN_OPTS allows 6g of heap, which ought to be plenty. - SPARK-18189: Fix serialization issue in KeyValueGroupedDataset *** FAILED *** isContain was true Interpreter output contained 'Exception': Welcome to ____              __ / __/__  ___ _____/ /__ _\ \/ _ \/ _ `/ __/  '_/ /___/ .__/\_,_/_/ /_/\_\   version 2.0.2 /_/ Using Scala version 2.11.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_102) Type in expressions to have them evaluated. Type :help for more information. org.apache.spark.sql.KeyValueGroupedDataset[Int,(Int, Int)] = org.apache.spark.sql.KeyValueGroupedDataset@70c30f72 _2: int] Broadcast(0) Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): com.google.common.util.concurrent.ExecutionError: java.lang.ClassCircularityError: io/netty/util/internal/__matchers__/org/apache/spark/network/protocol/MessageMatcher at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2261) at com.google.common.cache.LocalCache.get(LocalCache.java:4000) at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004) at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874) at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.compile(CodeGenerator.scala:841) at org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:188) at org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection$.create(GenerateSafeProjection.scala:36) at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:825) at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:822) at org.apache.spark.sql.execution.ObjectOperator$.deserializeRowToObject(objects.scala:137) at org.apache.spark.sql.execution.AppendColumnsExec$$anonfun$9.apply(objects.scala:251) at org.apache.spark.sql.execution.AppendColumnsExec$$anonfun$9.apply(objects.scala:250) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803) at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319) at org.apache.spark.rdd.RDD.iterator(RDD.scala:283) at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319) at org.apache.spark.rdd.RDD.iterator(RDD.scala:283) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47) at org.apache.spark.scheduler.Task.run(Task.scala:86) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Caused by: java.lang.ClassCircularityError: io/netty/util/internal/__matchers__/org/apache/spark/network/protocol/MessageMatcher at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348) at io.netty.util.internal.JavassistTypeParameterMatcherGenerator.generate(JavassistTypeParameterMatcherGenerator.java:62) at io.netty.util.internal.JavassistTypeParameterMatcherGenerator.generate(JavassistTypeParameterMatcherGenerator.java:54) at io.netty.util.internal.TypeParameterMatcher.get(TypeParameterMatcher.java:42) at io.netty.util.internal.TypeParameterMatcher.find(TypeParameterMatcher.java:78) at io.netty.handler.codec.MessageToMessageEncoder.(MessageToMessageEncoder.java:60) at org.apache.spark.network.protocol.MessageEncoder.(MessageEncoder.java:34) at org.apache.spark.network.TransportContext.(TransportContext.java:78) at org.apache.spark.rpc.netty.NettyRpcEnv.downloadClient(NettyRpcEnv.scala:354) at org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:324) at org.apache.spark.repl.ExecutorClassLoader.org $apache$spark$repl$ExecutorClassLoader$$getClassFileInputStreamFromSparkRPC(ExecutorClassLoader.scala:90) at org.apache.spark.repl.ExecutorClassLoader$$anonfun$1.apply(ExecutorClassLoader.scala:57) at org.apache.spark.repl.ExecutorClassLoader$$anonfun$1.apply(ExecutorClassLoader.scala:57) at org.apache.spark.repl.ExecutorClassLoader.findClassLocally(ExecutorClassLoader.scala:161) at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:80) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348) at io.netty.util.internal.JavassistTypeParameterMatcherGenerator.generate(JavassistTypeParameterMatcherGenerator.java:62) at io.netty.util.internal.JavassistTypeParameterMatcherGenerator.generate(JavassistTypeParameterMatcherGenerator.java:54) at io.netty.util.internal.TypeParameterMatcher.get(TypeParameterMatcher.java:42) at io.netty.util.internal.TypeParameterMatcher.find(TypeParameterMatcher.java:78) at io.netty.handler.codec.MessageToMessageEncoder.(MessageToMessageEncoder.java:60) at org.apache.spark.network.protocol.MessageEncoder.(MessageEncoder.java:34) at org.apache.spark.network.TransportContext.(TransportContext.java:78) at org.apache.spark.rpc.netty.NettyRpcEnv.downloadClient(NettyRpcEnv.scala:354) at org.apache.spark.rpc.netty.NettyRpcEnv.openChannel(NettyRpcEnv.scala:324) at org.apache.spark.repl.ExecutorClassLoader.org $apache$spark$repl$ExecutorClassLoader$$getClassFileInputStreamFromSparkRPC(ExecutorClassLoader.scala:90) at org.apache.spark.repl.ExecutorClassLoader$$anonfun$1.apply(ExecutorClassLoader.scala:57) at org.apache.spark.repl.ExecutorClassLoader$$anonfun$1.apply(ExecutorClassLoader.scala:57) at org.apache.spark.repl.ExecutorClassLoader.findClassLocally(ExecutorClassLoader.scala:161) at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:80) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at java.lang.ClassLoader.loadClass(ClassLoader.java:411) at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348) at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:78) at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:254) at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6893) at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5331) at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5207) at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:5188) at org.codehaus.janino.UnitCompiler.access$12600(UnitCompiler.java:185) at org.codehaus.janino.UnitCompiler$16.visitReferenceType(UnitCompiler.java:5119) at org.codehaus.janino.Java$ReferenceType.accept(Java.java:2880) at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:5159) at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:5414) at org.codehaus.janino.UnitCompiler.access$12400(UnitCompiler.java:185) at org.codehaus.janino.UnitCompiler$16.visitArrayType(UnitCompiler.java:5117) at org.codehaus.janino.Java$ArrayType.accept(Java.java:2954) at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:5159) at org.codehaus.janino.UnitCompiler.access$16700(UnitCompiler.java:185) at org.codehaus.janino.UnitCompiler$31.getParameterTypes2(UnitCompiler.java:8533) at org.codehaus.janino.IClass$IInvocable.getParameterTypes(IClass.java:835) at org.codehaus.janino.IClass$IMethod.getDescriptor2(IClass.java:1063) at org.codehaus.janino.IClass$IInvocable.getDescriptor(IClass.java:849) at org.codehaus.janino.IClass.getIMethods(IClass.java:211) at org.codehaus.janino.IClass.getIMethods(IClass.java:199) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:409) at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393) at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185) at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347) at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139) at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354) at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322) at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383) at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315) at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233) at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192) at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84) at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:887) at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:950) at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:947) at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599) at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379) at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342) at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257) ... 26 more I guess it's worth explicitly stating that I think we need another RC one way or the other because this test seems to consistently fail. It was a (surprising) last-minute regression. I think I'd have to say -1 only for this. Reverting https://github.com/apache/spark/pull/15706 for branch-2.0 would unblock this. There's also some discussion about an alternative resolution for the test problem. +1 binding (See comments on last vote; same results, except, the regression we identified is fixed now.) Yes they really correspond to, if anything, the categories at spark-prs.appspot.com . They aren't that consistently used however and there isn't really a definite list. It is really mostly of use for the fact that it tags emails in a way people can filter semi-effectively. So I think we have left it at an informal mechanism rather than make yet another list of categories to maintain. Once you get to needing this level of fine-grained control, should you not consider using the programmatic API in part, to let you control individual jobs? I updated the wiki to point to the /community.html page. (We're going to migrate the wiki real soon now anyway) I updated the /community.html page per this thread too. PR: https://github.com/apache/spark-website/pull/16 I have seen the same issue from time to time where I couldn't see a person's alias in the popup. Happened yesterday for me with @joshrosen. No idea why. I was also missing emails for a while to spam, but it seemed like a Gmail problem. It said I had marked messages from about 6 different people as spam, which doesn't seem like something i could have done with a mis-click. So, worth looking out for if you're missing emails and using gmail. Also, I noted that if you just add an "Approve" in the new review system that does not generate an email. Which is maybe a good thing. I do still see a big problem with Gmail and these messages not being threaded. One PR's messages will typically split somewhat arbitrarily over 5-6 threads, which makes managing discussions really quite annoying. I don't know if it's somehow related. It's a degenerate case of course. 0, 0.5 and 1 all make about as much sense. Is there a strong convention elsewhere to use 0? Min/max scaling is the wrong thing to do for a data set like this anyway. What you probably intend to do is scale each image so that its max intensity is 1 and min intensity is 0, but that's different. Scaling each pixel across all images doesn't make as much sense. Maybe I missed it, but did anyone declare a QA period? In the past I've not seen this, and just seen people start talking retrospectively about how "we're in QA now" until it stops. We have https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage saying it is already over, but clearly we're not doing RCs. We should make this more formal and predictable. We probably need a clearer definition of what changes in QA. I'm moving the wiki to spark.apache.org now and could try to put up some words around this when I move this page above today. Thanks, this was another message that went to spam for me: http://apache-spark-developers-list.1001551.n3.nabble.com/ANNOUNCE-Apache-Spark-branch-2-1-td19688.html Looks great -- cutting branch = in RC period. I completed the migration. You can see the results live right now at http://spark.apache.org, and https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage A summary of the changes: https://issues.apache.org/jira/browse/SPARK-18073 The substance of the changes: https://github.com/apache/spark-website/pull/19 https://github.com/apache/spark-website/pull/25 No information has been lost. Old wikis still either exist as they were with an "end of life" notice, or, point to the new location of the information. I ported the content basically as-is, only making minor changes to fix obviously out of date content. I did alter the menu structure, most significantly to add a "Developer"menu. Of course, we can change it further. Please comment if you see any errors, don't like some of the choices, etc. Note that all the release docs are now also updated according in branch 2.1. SparkPi is just an example, so its performance doesn't really matter. Simpler is better. Kryo could be an issue but that would be a change in Kryo. See https://github.com/apache/spark/pull/15499#discussion_r89008564 in particular. Hyukjin / Xiao do we need to undo part of this change? Yes, feel free to open a [MINOR] PR to fix that. Yeah, there's no official position on this. BTW see the new home of what info is published on this topic: http://spark.apache.org/versioning-policy.html The answer is indeed that minor releases have a target cadence, but maintenance releases are as-needed, as defined by the release manager's judgment. From https://issues.apache.org/jira/browse/SPARK/?selectedTab=com.atlassian.jira.jira-projects-plugin:versions-panel you can see that maintenance releases for a minor release seem to continue for 3-6 months in general, with 1.6.x going on for a longer period as the last 1.x minor release. I wouldn't mind putting down some text to set a non-binding expectation around this, that minor releases might be 'supported' for 3-6 months? until 2 more minor releases have succeeded it? Because in practice that's very much how back-ports behave. We also don't say anything about major releases but I think that may also be too rare to put even informal statements around. Every couple years? We still have several blockers for 2.1, so I imagine at least one will mean this won't be the final RC: SPARK-18318 ML, Graph 2.1 QA: API: New Scala APIs, docs SPARK-18319 ML, Graph 2.1 QA: API: Experimental, DeveloperApi, final, sealed audit SPARK-18326 SparkR 2.1 QA: New R APIs and API docs SPARK-18516 Separate instantaneous state from progress performance statistics SPARK-18538 Concurrent Fetching DataFrameReader JDBC APIs Do Not Work SPARK-18553 Executor loss may cause TaskSetManager to be leaked However I understand the purpose here is of course to get started testing early and we should all do so. BTW here are the Critical issues still open: SPARK-12347 Write script to run all MLlib examples for testing SPARK-16032 Audit semantics of various insertion operations related to partitioned tables SPARK-17861 Store data source partitions in metastore and push partition pruning into metastore SPARK-18091 Deep if expressions cause Generated SpecificUnsafeProjection code to exceed JVM code size limit SPARK-18274 Memory leak in PySpark StringIndexer SPARK-18316 Spark MLlib, GraphX 2.1 QA umbrella SPARK-18322 ML, Graph 2.1 QA: Update user guide for new features & APIs SPARK-18323 Update MLlib, GraphX websites for 2.1 SPARK-18324 ML, Graph 2.1 QA: Programming guide update and migration guide SPARK-18329 Spark R 2.1 QA umbrella SPARK-18330 SparkR 2.1 QA: Update user guide for new features & APIs SPARK-18331 Update SparkR website for 2.1 SPARK-18332 SparkR 2.1 QA: Programming guide, migration guide, vignettes updates SPARK-18468 Flaky test: org.apache.spark.sql.hive.HiveSparkSubmitSuite.SPARK-9757 Persist Parquet relation with decimal column SPARK-18549 Failed to Uncache a View that References a Dropped Table. SPARK-18560 Receiver data can not be dataSerialized properly. FWIW I am seeing several test failures, each more than once, but, none are necessarily repeatable. These are likely just flaky tests but I thought I'd flag these unless anyone else sees similar failures: - SELECT a.i, b.i FROM oneToTen a JOIN oneToTen b ON a.i = b.i + 1 *** FAILED *** org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 9.0 failed 1 times, most recent failure: Lost task 1.0 in stage 9.0 (TID 19, localhost, executor driver): java.lang.NullPointerException at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.(Unknown Source) at org.apache.spark.sql.catalyst.expressions.GeneratedClass.generate(Unknown Source) ... udf3Test(test.org.apache.spark.sql.JavaUDFSuite)  Time elapsed: 0.302 sec <<< ERROR! java.lang.NoSuchMethodError: org.apache.spark.sql.catalyst.JavaTypeInference$.inferDataType(Lcom/google/common/reflect/TypeToken;)Lscala/Tuple2; at test.org.apache.spark.sql.JavaUDFSuite.udf3Test(JavaUDFSuite.java:107) - SPARK-18360: default table path of tables in default database should depend on the location of default database *** FAILED *** Timeout of './bin/spark-submit' '--class''org.apache.spark.sql.hive.SPARK_18360' '--name' 'SPARK-18360' '--master''local-cluster[2,1,1024]' '--conf' 'spark.ui.enabled=false' '--conf''spark.master.rest.enabled=false' '--driver-java-options''-Dderby.system.durability=test''file:/home/srowen/spark-2.1.0/sql/hive/target/tmp/spark-dc9f43f2-ded4-4bcf-947e-d5af6f0e1561/testJar-1480440084611.jar'See the log4j logs for more detail. ... - should clone and clean line object in ClosureCleaner *** FAILED *** isContain was true Interpreter output contained 'Exception': java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext. This stopped SparkContext was created at: I read it again and that looks like it implements mean precision@k as I would expect. What is the issue? As I understand, this might best be called "mean precision@k", not "mean average precision, up to k". As usual, the sigs / hashes are fine and licenses look fine. I am still seeing some test failures. A few I've seen over time and aren't repeatable, but a few seem persistent. ANyone else observed these? I'm on Ubuntu 16 / Java 8 building for -Pyarn -Phadoop-2.7 -Phive If anyone can confirm I'll investigate the cause if I can. I'd hesitate to support the release yet unless the build is definitely passing for others. udf3Test(test.org.apache.spark.sql.JavaUDFSuite)  Time elapsed: 0.281 sec <<< ERROR! java.lang.NoSuchMethodError: org.apache.spark.sql.catalyst.JavaTypeInference$.inferDataType(Lcom/google/common/reflect/TypeToken;)Lscala/Tuple2; at test.org.apache.spark.sql.JavaUDFSuite.udf3Test(JavaUDFSuite.java:107) - caching on disk *** FAILED *** java.util.concurrent.TimeoutException: Can't find 2 executors before 30000 milliseconds elapsed at org.apache.spark.ui.jobs.JobProgressListener.waitUntilExecutorsUp(JobProgressListener.scala:584) at org.apache.spark.DistributedSuite.org $apache$spark$DistributedSuite$$testCaching(DistributedSuite.scala:154) at org.apache.spark.DistributedSuite$$anonfun$32$$anonfun$apply$1.apply$mcV$sp(DistributedSuite.scala:191) at org.apache.spark.DistributedSuite$$anonfun$32$$anonfun$apply$1.apply(DistributedSuite.scala:191) at org.apache.spark.DistributedSuite$$anonfun$32$$anonfun$apply$1.apply(DistributedSuite.scala:191) at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22) at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85) at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) at org.scalatest.Transformer.apply(Transformer.scala:22) at org.scalatest.Transformer.apply(Transformer.scala:20) ... - stress test for failOnDataLoss=false *** FAILED *** org.apache.spark.sql.streaming.StreamingQueryException: Query [id = 3b191b78-7f30-46d3-93f8-5fbeecce94a2, runId = 0cab93b6-19d8-47a7-88ad-d296bea72405] terminated with exception: null at org.apache.spark.sql.execution.streaming.StreamExecution.org $apache$spark$sql$execution$streaming$StreamExecution$$runBatches(StreamExecution.scala:262) at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:160) ... Cause: java.lang.NullPointerException: ... Sure, it's only an issue insofar as it may be a flaky test. If it's fixable or disable-able for a possible next RC that could be helpful. One of many things that gets lost in the shuffle -- it looks pretty straightforward so I will review today. (If you have a template for these emails, maybe update it to use https links. They work for apache.org domains. After all we are asking people to verify the integrity of release artifacts, so it might as well be secure.) (Also the new archives use .tar.gz instead of .tgz like the others. No big deal, my OCD eye just noticed it.) I don't see an Apache license / notice for the Pyspark or SparkR artifacts. It would be good practice to include this in a convenience binary. I'm not sure if it's strictly mandatory, but something to adjust in any event. I think that's all there is to do for SparkR. For Pyspark, which packages a bunch of dependencies, it does include the licenses (good) but I think it should include the NOTICE file. This is the first time I recall getting 0 test failures off the bat! I'm using Java 8 / Ubuntu 16 and yarn/hive/hadoop-2.7 profiles. I think I'd +1 this therefore unless someone knows that the license issue above is real and a blocker. PS, here are the open issues for 2.1.0. Forgot this one. No Blockers, but one "Critical": SPARK-16845 org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificOrdering"grows beyond 64 KB SPARK-18669 Update Apache docs regard watermarking in Structured Streaming SPARK-18894 Event time watermark delay threshold specified in months or years gives incorrect results SPARK-18899 append data to a bucketed table with mismatched bucketing should fail SPARK-18909 The error message in `ExpressionEncoder.toRow` and `fromRow` is too verbose SPARK-18912 append to a non-file-based data source table should detect columns number mismatch SPARK-18913 append to a table with special column names should work SPARK-18921 check database existence with Hive.databaseExists instead of getDatabase It is breeze, but, what's the option? It can't be excluded. I think this falls in the category of things an app would need to shade in this situation. Don't do that. Union them all at once with SparkContext.union This could be true if you knew you were just going to scale the input to StandardScaler and nothing else. It's probably more typical you'd scale some other data. The current behavior is therefore the sensible default, because the input is a sample of some unknown larger population. I think it doesn't matter much except for toy problems, because at any scale, the difference between 1/n and 1/(n-1) is negligible, and for most purposes for which the scaler is used, it won't matter anyway (faster convergence of an optimizer for example). I'm neutral on whether it's worth complicating the API to do both, therefore. Looks like the JIRA maintenance left a bunch of duplicate JIRAs, from SPARK-19191 to SPARK-19202. For some reason, I can't resolve these issues, but I can resolve others. Does anyone else see the same? I know SPARK-19190 was similarly borked but closed by its owner. Do you see a button to resolve other issues? you may not be able to resolve any of them. I am a JIRA admin though, like most other devs, so should be able to resolve anything. Yes, I certainly know how resolving issues works but it's suddenly today only working for a subset of issues, and I bet it's related to the JIRA problems this week. Yes, I'm asking about a specific range: 19191 - 19202. These seem to be the ones created during the downtime. Most are duplicates or incomplete. A-ha. I'll try to clean them up and ask for new JIRAs to be created in some cases. That is usually so the result comes out in one file, not partitioned over n files. You're referring to code that serializes models, which are quite small. For example a PCA model consists of a few principal component vector. It's a Dataset of just one element being saved here. It's re-using the code path normally used to save big data sets, to output 1 file with 1 thing as Parquet. Yes, certainly debatable for word2vec. You have a good point that this could overrun the 2GB limit if the model is one big datum, for large but not crazy models. This model could probably easily be serialized as individual vectors in this case. It would introduce a backwards-compatibility issue but it's possible to read old and new formats, I believe. It doesn't strike me as something that's problematic to use. There are a thousand things in the API that, maybe in hindsight, could have been done differently, but unless something is bad practice or superseded by another superior mechanism, it's probably not worth the bother for maintainers or users. I don't see much problem with this, and it's actually used by Spark. As you mentioned, it's called in ForeachSink. I don't know that the scaladoc is wrong. You're saying something else, that there's no such thing as local execution. I confess I don't know if that's true? but the doc isn't wrong in that case, really. More broadly, I just don't think this type of thing is worth this small amount of attention. WontFix or Later is fine. There's not really any practical distinction. I figure that if something times out and is closed, it's very unlikely to be looked at again. Therefore marking it as something to do 'later' seemed less accurate. It still doesn't pass tests -- I'd usually not look until that point. Hm. Unless I am also totally missing or forgetting something, I think you're right. The equivalent in PairRDDFunctions.scala operations on a function from T to TraversableOnce[U] and a TraversableOnce is most like java.util.Iterator. You can work around it by wrapping it in a faked IteratorIterable. I think this is fixable in the API by deprecating this method and adding a new one that takes a FlatMapFunction. We'd have to triple-check in a test that this doesn't cause an API compatibility problem with respect to Java 8 lambdas, but if that's settled, I think this could be fixed without breaking the API. Yes, confirmed that fixing it unfortunately causes trouble in Java 8. See https://issues.apache.org/jira/browse/SPARK-19287 for further discussion. My $0.02, which shouldn't be weighted too much. I believe the mission as of Spark ML has been to provide the framework, and then implementation of 'the basics' only. It should have the tools that cover ~80% of use cases, out of the box, in a pretty well-supported and tested way. It's not a goal to support an arbitrarily large collection of algorithms because each one adds marginally less value, and IMHO, is proportionally bigger baggage, because the contributors tend to skew academic, produce worse code, and don't stick around to maintain it. The project is already generally quite overloaded; I don't know if there's bandwidth to even cover the current scope. While 'the basics' is a subjective label, de facto, I think we'd have to define it as essentially "what we already have in place" for the foreseeable future. That the bits on spark-packages.org aren't so hot is not a problem but a symptom. Would these really be better in the core project? And, or: I entirely agree with Joseph's take. Certainly a typo -- feel free to make a PR for the spark-website repo. (Might search for other instances of 'cyclic' too) Last year we discussed removing support for things like Hadoop 2.5 and earlier. It was deprecated in Spark 2.1.0. I'd like to go ahead with this, so am checking whether anyone has strong feelings about it. The original rationale for separate Hadoop profile was bridging the significant difference between Hadoop 1 and 2, and the moderate differences between 2.0 alpha, 2.1 beta, and 2.2 final. 2.2 is really the "stable"Hadoop 2, and releases from there to current are comparatively very similar from Spark's perspective. We nevertheless continued to make a separate build profile for every minor release, which isn't serving much purpose. The argument here is mostly that it will simplify code a little bit (less reflection, fewer profiles), simplify the build -- we now have 6 profiles x 2 build systems x 4 major branches in Jenkins, whereas master could go down to 2 profiles. Realistically, I don't know how much we'd do to support Hadoop before 2.6 anyway. Any distro user is long since on 2.6+. Would this cause anyone significant pain? if so, let's talk about when it would be realistic to remove this, when does that change. I don't think anyone's tried it. I think we'd first have to agree to drop Java 7 support before that could be seriously considered. The 8-9 difference is a bit more of a breaking change. I believe that if we ran the Jenkins builds with Java 8 we would catch these? this doesn't require dropping Java 7 support or anything. @joshrosen I know we are just now talking about modifying the Jenkins jobs to remove old Hadoop configs. Is it possible to change the master jobs to use Java 8? can't hurt really in any event. Or maybe I'm mistaken and they already run Java 8 and it doesn't catch this until Java 8 is the target. Yeah this is going to keep breaking as javadoc 8 is pretty strict. Thanks Hyukjin. It has forced us to clean up a lot of sloppy bits of doc though. As you have seen, there's a WIP PR to implement removal of Java 7 support: https://github.com/apache/spark/pull/16871 I have heard several +1s at https://issues.apache.org/jira/browse/SPARK-19493 but am asking for concerns too, now that there's a concrete change to review. If this goes in for 2.2 it can be followed by more extensive update of the Java code to take advantage of Java 8; this is more or less the baseline change. We also just removed Hadoop 2.5 support. I know there was talk about removing Python 2.6. I have no opinion on that myself, but, might be time to revive that conversation too. As usual I think maintenance release branches are created ad-hoc when there seems to be some demand. I personally would guess there will be at least one more 2.0.x and 2.1.x maintenance release. In that sense, yeah it's not really even the end of actively supporting a Java 7-compatible release. Yes, that's a key concern about the Java dependency, that its update is a function of the OS packages and those who control them, which is often not the end user. I think that's why this has been delayed a while. My general position is that, of course, someone in that boat can use Spark 2.1.x. It's likely going to see maintenance releases through the end of the year, even. On the flip side, no (non-paid) support has been available for Java 7 for a while. It wouldn't surprise me if some people are yet still stuck on Java 7; it would surprise me if they expect to use the latest of any package at this stage. Taking your CDH example, yes it's been a couple years since people have been able to deploy it on Java 8. Spark 2 isn't supported before 5.7 anyway. The default is Java 8. Scala 2.10 is a good point that we are dealing with now. It's not really a question of whether it will run -- it's all libraries and bytecode to the JVM and it will happily deal with a mix of 7 and 8 bytecode. It's a question of whether the build for 2.10 will succeed. I believe it's 'yes'but am following up on some tests there. I just saw https://www.youtube.com/user/TheApacheSpark and wondered who 'owns' it? if it's a quasi-official channel, can we list it on http://spark.apache.org/community.html but then, how does one add videos? If it's the Spark Summit video account, as it seems to be at the moment, it shouldn't be called "Apache Spark" right? The text seems fine to me. Really, this is not describing a fundamentally new process, which is good. We've always had JIRAs, we've always been able to call a VOTE for a big question. This just writes down a sensible set of guidelines for putting those two together when a major change is proposed. I look forward to turning some big JIRAs into a request for a SPIP. My only hesitation is that this seems to be perceived by some as a new or different thing, that is supposed to solve some problems that aren't otherwise solvable. I see mentioned problems like: clear process for managing work, public communication, more committers, some sort of binding outcome and deadline. If SPIP is supposed to be a way to make people design in public and a way to force attention to a particular change, then, this doesn't do that by itself. Therefore I don't want to let a detailed discussion of SPIP detract from the discussion about doing what SPIP implies. It's just a process document. Still, a fine step IMHO. I'm not sure what you're specifically suggesting. Of course flaky tests are bad and they should be fixed, and people do. Yes, some are pretty hard to fix because they are rarely reproducible if at all. If you want to fix, fix; there's nothing more to it. I don't perceive flaky tests to be a significant problem. It has gone from bad to occasional over the past year in my anecdotal experience. I saw this too yesterday but not today. It may have been fixed by some recent commits. This job is using Java 7 still. Josh are you the person to ask? I think it needs to set JAVA_HOME to Java 8. To me, no new process is being invented here, on purpose, and so we should just rely on whatever governs any large JIRA or vote, because SPIPs are really just guidance for making a big JIRA. http://apache.org/foundation/voting.html suggests that PMC members have the binding votes in general, and for code-modification votes in particular, which is what this is. Absent a strong reason to diverge from that, I'd go with that. (PS: On reading this, I didn't realize that the guidance was that releases are blessed just by majority vote. Oh well, not that it has mattered.) I also don't see a need to require a shepherd, because JIRAs don't have such a process, though I also can't see a situation where nobody with a vote cares to endorse the SPIP ever, but three people vote for it and nobody objects? Perhaps downgrade this to "strongly suggested, so that you don't waste your time."Or, implicitly, that proposing a SPIP calls a vote that lasts for, dunno, a month. If fewer than 3 PMC vote for it, it doesn't pass anyway. If at least 1 does, OK, they're the shepherd(s). No new process. Another call for comments on removal of Scala 2.10 support, if you haven't already. See https://github.com/apache/spark/pull/17150 http://issues.apache.org/jira/browse/SPARK-19810 I've heard several votes in support and no specific objections at this point, but wanted to make another call to check for any doubts before I go ahead for Spark 2.2. (2.10 was already deprecated for 2.1, so that's done actually.) Personally I'm fine with leaving in 2.10 support for 2.2. (FWIW CDH is Scala 2.11-only for Spark 2.) If there were no voices in support of keeping it, might be worth moving on right now, but if there's any substantive argument against, I'd also punt it another release. It's not really driven by cleanup, though that's nice, but 2.12 support. I don't think 2.10 and 2.12 support can coexist, and soon, 2.12 support will be important. How about tagging this for 2.3.0, as well as targeting 2.12 support for 2.3.0? Do we need a VOTE? heck I think anyone can call one, anyway. Pre-flight vote check: anyone have objections to the text as-is? See https://docs.google.com/document/d/1-Zdi_W-wtuxS9hTK0P9qb2x-nRanvXmnZ7SUi4qMljg/edit# If so let's hash out specific suggest changes. If not, then I think the next step is to probably update the github.com/apache/spark-website repo with the text here. That's a code/doc change we can just review and merge as usual. I think a VOTE is over-thinking it, and is rarely used, but, can't hurt. Nah, anyone can call a vote. This really isn't that formal. We just want to declare and document consensus. I think SPIP is just a remix of existing process anyway, and don't think it will actually do much anyway, which is why I am sanguine about the whole thing. To bring this to a conclusion, I will just put the contents of the doc in an email tomorrow for a VOTE. Raise any objections now. Alrighty, if nobody is objecting, and nobody calls for a VOTE, then, let's say this document is the SPIP 1.0 process. I think the next step is just to translate the text to some suitable location. I suggest adding it to https://github.com/apache/spark-website/blob/asf-site/contributing.md http://spark.apache.org/improvement-proposals.html (Thanks Cody!) We should use this process where appropriate now, and we can refine it further if needed. I'm skeptical.  Serving synchronous queries from a model at scale is a fundamentally different activity. As you note, it doesn't logically involve Spark. If it has to happen in milliseconds it's going to be in-core. Scoring even 10qps with a Spark job per request is probably a non-starter; think of the thousands of tasks per second and the overhead of just tracking them. When you say the RDDs support point prediction, I think you mean that those older models expose a method to score a Vector. They are not somehow exposing distributed point prediction. You could add this to the newer models, but it raises the question of how to make the Row to feed it? the .mllib punts on this and assumes you can construct the Vector. I think this sweeps a lot under the rug in assuming that there can just be a "local" version of every Transformer -- but, even if there could be, consider how much extra implementation that is. Lots of them probably could be but I'm not sure that all can. The bigger problem in my experience is the Pipelines don't generally encapsulate the entire pipeline from source data to score. They encapsulate the part after computing underlying features. That is, if one of your features is "total clicks from this user", that's the product of a DataFrame operation that precedes a Pipeline. This can't be turned into a non-distributed, non-Spark local version. Solving subsets of this problem could still be useful, and you've highlighted some external projects that try. I'd also highlight PMML as an established interchange format for just the model part, and for cases that don't involve much or any pipeline, it's a better fit paired with a library that can score from PMML. I think this is one of those things that could live outside the project, because it's more not-Spark than Spark. Remember too that building a solution into the project blesses one at the expense of others. This ended up proceeding as a normal doc change, instead of precipitating a meta-vote. However, the text that's on the web site now can certainly be further amended if anyone wants to propose a change from here. It's not a new process, in that it doesn't entail anything not already in http://apache.org/foundation/voting.html . We're just deciding to call a VOTE for this type of code modification. To your point -- yes, it's been around a long time with no further comment, and I called several times for more input. That's pretty strong lazy consensus of the form we use every day. Responding to your request for a vote, I meant that this isn't required per se and the consensus here was not to vote on it. Hence the jokes about meta-voting protocol. In that sense nothing new happened process-wise, nothing against ASF norms, if that's your concern. I think it's just an agreed convention now, that we will VOTE, as normal, on particular types of changes that we call SPIPs. I mean it's no new process in the ASF sense because VOTEs are an existing mechanic. I personally view it as, simply, additional guidance about how to manage huge JIRAs in a way that makes them stand a chance of moving forward. I suppose we could VOTE about any JIRA if we wanted. They all proceed via lazy consensus at the moment. Practically -- I heard support for codifying this process and no objections to the final form. This was bouncing around in process purgatory, when no particular new process was called for. It takes effect immediately, implicitly, like anything else I guess, like amendments to code style guidelines. Please uses SPIPs to propose big changes from here. As to finding it hard to pick out of the noise, sure, I sympathize. Many big things happen without a VOTE tag though. It does take a time investment to triage these email lists. I don't know that this by itself means a VOTE should have happened. It seems reasonable to me, in that other x.y.1 releases have followed ~2 months after the x.y.0 release and it's been about 3 months since 2.1.0. Related: creating releases is tough work, so I feel kind of bad voting for someone else to do that much work. Would it make sense to deputize another release manager to help get out just the maintenance releases? this may in turn mean maintenance branches last longer. Experienced hands can continue to manage new minor and major releases as they require more coordination. I know most of the release process is written down; I know it's also still going to be work to make it 100% documented. Eventually it'll be necessary to make sure it's entirely codified anyway. Not pushing for it myself, just noting I had heard this brought up in side conversations before.