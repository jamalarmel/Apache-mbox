Replied on https://issues.apache.org/jira/browse/INFRA-6418 -- overall it looks good.

Matei

Yeah, similarly to the users list, we asked for a "commits" one on the incubator proposal, where we'll hopefully set up Git to email commit messages. Do we have to request these specifically with JIRAs on INFRA?

Matei

Ah, got it. I'd prefer not to do this actually because we already had separate dev and issues lists before we moved to Apache. It could be annoying for people to add new filters that deal with them as one list.

Matei

Hey Chris,

I definitely understand that we need to move to Apache lists, and I think that's fine, but maybe one question -- is it possible to transform the *@spark.incubator.apache.org lists into *@spark.apache.org more seamlessly when we graduate? It's weird to ask users to move twice. If these could somehow become aliases that would be ideal.

In any case though, I think we'll have a gradual transition. I'd first move the dev list over, then the user one, and for a while I'd keep both and keep a notice on the Google group. I'm not sure how Google groups allow that but there must be a way.

Matei

Hey Chris,

I definitely understand that we need to move to Apache lists, and I think that's fine, but maybe one question -- is it possible to transform the *@spark.incubator.apache.org lists into *@spark.apache.org more seamlessly when we graduate? It's weird to ask users to move twice. If these could somehow become aliases that would be ideal.

In any case though, I think we'll have a gradual transition. I'd first move the dev list over, then the user one, and for a while I'd keep both and keep a notice on the Google group. I'm not sure how Google groups allow that but there must be a way.

Matei


Folks, it seems from the incubator@ list like the deadline for this is actually July 10th or so, but I'll work on getting this done tomorrow or Thursday. I'll put it on the incubator wiki and send it out when done.

Matei


Hey Chris, given that we'll do this, how do we request a user@spark.incubator.apache.org list?

Matei

Hey Chris, unfortunately I don't have permission to edit the wiki page (on my MateiZaharia account), but here is the update in plain text:

------------

Spark

Spark is an open source system for fast and flexible large-scale data analysis. Spark provides a general purpose runtime that supports low-latency execution in several forms.

Spark has been incubating since 2013-06-19.

Three most important issues to address in the move towards graduation:

  1. Finish bringing up infrastructure on Apache (JIRA, "user" mailing list, SVN repo for website)
  2. Migrate mailing lists and development to Apache
  3. Make a Spark 0.8 under the Apache Incubator

Any issues that the Incubator PMC (IPMC) or ASF Board wish/need to be aware of?

While most of our infrastructure is now up, it has taken a while to get a JIRA, a SVN repo for our website (so we can use the CMS), and a user@spark.incubator.apache.org mailing list (so we can move our existing user list, which is large).

How has the community developed since the last report?

We only entered the Apache Incubator at the end of June, but in the existing developer community keeps expanding and we are seeing many new features from new contributors.

How has the project developed since the last report?

In terms of the Apache incubation process, we filed our IP papers and got a decent part of the infrastructure set up (Git, dev list, wiki, Jenkins group).

Date of last release: 

Please check this [ ] when you have filled in the report for Spark.

Signed-off-by: 

Chris Mattmann: [ ](spark)
Paul Ramirez: [ ](spark)
Andrew Hart: [ ](spark)
Thomas Dudziak: [ ](spark)
Suresh Marru: [ ](spark)
Henry Saputra: [ ](spark)
Roman Shaposhnik: [ ](spark)

Shepherd notes:

------------


My account name is MateiZaharia. I'm pretty sure I had access at some point in the past, but I guess I don't. Maybe they added the access controls after I joined.

Thanks for signing this off! I also saw you set up a SVN for the CMS, which is great. The other thing I'm curious about is the user mailing list -- is the thing to set those up working? Once that's up I want to announce the transition on our Google groups (I can also do it before if needed but I thought it would be nice to do both lists at once).

Matei

Hey Chris, do people other than you have commit access to this yet? I might be missing something but I tried to svn commit and I get this:

svn: Commit failed (details follow):
svn: access to '/repos/asf/!svn/act/a4f3b6e0-0f36-485f-8d12-9e68583ba0b5' forbidden

My svn info says:

Path: .
URL: http://svn.apache.org/repos/asf/incubator/spark/site
Repository Root: http://svn.apache.org/repos/asf
Repository UUID: 13f79535-47bb-0310-9956-ffa450edef68
Revision: 1502606
Node Kind: directory
Schedule: normal
Last Changed Author: mattmann
Last Changed Rev: 1500725
Last Changed Date: 2013-07-08 06:42:05 -0700 (Mon, 08 Jul 2013)

Hi Karthik,

Are you trying to import Spark into an IDE, or just write a project that depends on it?

I've found IDEA works pretty well for importing Spark. You do sbt/sbt gen-idea in the Spark folder, then make sure you have the Scala plugin in IDEA, and import it. I saw one weird warning about "same path for test and production" in the root-build project that I fixed by going to the project options and giving a different path for test.

Matei

Sure, let me know if you have any problems.

Hi Henry,

I actually want to do this soon but haven't had the time yet. Hopefully we'll announce it in the next few days. I just want to test that the GitHub pull request workflow works with this, which I believe it should, but it's slightly more complicated because the GitHub repo will be a mirror.

Matei

Hi all,

I wanted to bring up a topic that there isn't a 100% perfect solution for, but that's been bothering the team at Berkeley for a while: consolidating Spark's build system. Right now we have two build systems, Maven and SBT, that need to be maintained together on each change. We added Maven a while back to try it as an alternative to SBT and to get some better publishing options, like Debian packages and classifiers, but we've found that 1) SBT has actually been fairly stable since then (unlike the rapid release cycle before) and 2) classifiers don't actually seem to work for publishing versions of Spark with different dependencies (you need to give them different artifact names). More importantly though, because maintaining two systems is confusing, it would be good to converge to just one soon, or to find a better way of maintaining the builds.

In terms of which system to go for, neither is perfect, but I think many of us are leaning toward SBT, because it's noticeably faster and it has less code to maintain. If we do this, however, I'd really like to understand the use cases for Maven, and make sure that either we can support them in SBT or we can do them externally. Can people say a bit about that? The ones I've thought of are the following:

- Debian packaging -- this is certainly nice, but there are some plugins for SBT too so may be possible to migrate.
- BigTop integration; I'm not sure how much this relies on Maven but Cos has been using it.
- Classifiers for hadoop1 and hadoop2 -- as far as I can tell, these don't really work if you want to publish to Maven Central; you still need two artifact names because the artifacts have different dependencies. However, more importantly, we'd like to make Spark work with all Hadoop versions by using hadoop-client and a bit of reflection, similar to how projects like Parquet handle this.

Are there other things I'm missing here, or other ways to handle this problem that I'm missing? For example, one possibility would be to keep the Maven build scripts in a separate repo managed by the people who want to use them, or to have some dedicated maintainers for them. But because this is often an issue, I do think it would be simpler for the project to have one build system in the long term. In either case though, we will keep the project structure compatible with Maven, so people who want to use it internally should be fine; I think that we've done this well and, if anything, we've simplified the Maven build process lately by removing Twirl.

Anyway, as I said, I don't think any solution is perfect here, but I'm curious to hear your input.

Matei
Cos, do you have any experience with Gradle by any chance? Is it something you'd recommend trying? I do agree that SBT's dependency management, being based on Ivy, is not ideal, but I'm not sure how common Gradle is and whether it will really work well with Scala.

Matei

Henry, our hope is to avoid having to create two different Hadoop profiles altogether by using the hadoop-client package and reflection. This is what projects like Parquet (https://github.com/Parquet) are doing. If this works out, you get one artifact that can link to any Hadoop version that includes hadoop-client (which I believe means 1.2 onward).

Matei

Unfortunately, we'll probably have to have different branches of Spark for different Scala versions, because there are also other libraries we depend on (e.g. Akka) that have separate versions for Scala 2.10. You can actually find a Scala 2.10 port of Spark in the scala-2.10 branch on GitHub.

Matei


Thanks for the feedback. It looks like there are more advantages to Maven than I was originally thinking of -- specifically, the better dependency resolution and assembly construction. (SBT assembly just takes all the JARs in lib_managed and packages them together unfortunately, which means you sometimes get multiple versions of the same artifact if you aren't very careful with exclusion rules). I think what we'll do is to wait until we see whether we can have a single Spark artifact that works with any Hadoop version, and go back to the build system issue then.

Matei

Hey Mark,

The motivation was to separate internal DAGScheduler data structures, such as Stage, from the interface we'll present to SparkListener, which will be a semi-public API. (Semi-public in that it might still change if we make drastic changes to the scheduler, but we want people to be able to use it for monitoring with as little pain as possible). We aren't following this consistently in all the SparkListener events yet but the goal is to do so.

Matei

Hey Nick,

Thanks for your interest in this stuff! I'm going to let the MLbase team answer this in more detail, but just some quick answers on the first part of your email:

- From my point of view, the ML library in Spark is meant to be just a library of "kernel" functions you can call, not a complete ETL and data format system like Mahout. The goal was to have good implementations of common algorithms that different higher-level systems (e.g. MLbase, Shark, PySpark) can call into.

- We wanted to try keeping this in Spark initially to make it a kind of "standard library". This is something that can help ensure it becomes high-quality over time and keep it supported by the project. If you think about it, projects like R and Matlab are very strong primarily because they have great standard libraries. This was also one of the things we thought would differentiate us from Hadoop and Mahout. However, we will of course see how things go and separate it out if it needs a faster dev cycle.

- I haven't worried much about compatibility with Mahout because I'm not sure Mahout is too widely used and I'm not sure its abstractions are best. Mahout is very tied to HDFS, SequenceFiles, etc. We will of course try to interoperate well with data from Mahout, but at least as far as I was concerned, I wanted an API that makes sense for Spark users.

- Something that's maybe not clear about the MLlib API is that we also want it to be used easily from Java and Python. So we've explicitly avoided having very high-level types or using Scala-specific features, in order to get something that will be simple to call from these languages. This does leave room for wrappers that provide higher-level interfaces.

In any case, if you like this "kernel" design for MLlib, it would be great to get more people contributing to it, or to get it used in other projects. I'll let the MLbase folks talk about higher-level interfaces -- this is definitely something they want to do, but they might be able to use help. In any case though, sharing the low-level kernels across Spark projects would make a lot of sense.

Matei

BTW I should also add about Mahout that it might also make sense for Mahout to call MLlib internally. I just haven't looked into it enough to decide whether we'd want to provide more than input/output wrappers. But it would certainly be great to have Mahout people help out with MLlib in some way.

Matei

I fully agree that we need to be clearer with the timelines in AMP Lab. One thing is that many of these are still research projects, so it's hard to predict when they will be ready for prime-time. Usually with all the things we officially announce (e.g. MLlib, GraphX), and especially the things we put in the Spark codebase, the team behind them really wants to make them widely available and has committed to spend the engineering to make them usable in real applications (as opposed to prototyping and moving on). But even then it can take some time to get the first release out. Hopefully we'll improve our communication about this through more careful tracking in JIRA.

Matei

Hey Michael,

Glad to hear you're interested in helping. Are there specific things you'd like to work on? Certainly we will need help with various Apache packaging, etc so it's good to have more people with experience at Apache.

Matei

Hey Michael,

Depending on your background, there are quite a few things to do.

One general area that we might use more help for, if you have experience there, is the Python API. Part of it can be just to add more examples in Python, e.g., to show how one can use NumPy or SciPy with it. Another thing that would be super useful if you also have access to Windows is this: https://spark-project.atlassian.net/browse/SPARK-649. We want to make Spark very broadly accessible for science work and it sounds like your background at JPL is good for that.

Alternatively, if you prefer to work on the Java VM, there are a bunch of internal things to do there too -- I can give an overview of what I'd consider easy to jump into there.

Matei

Let's at the very least make it configurable, but an even better thing will be to make sbt assembly not include it. I think the only thing that depends on HBase is the examples project, but unfortunately SBT puts all its JARs in the lib_managed folder and just stupidly creates an assembly by grouping those. The Maven build, for example, should not do that.

Matei

Cool! The way I'd start is perhaps by adding a new Python example job. For example, a good one to implement would be PageRank -- you can look at these slides for a Scala version of it: http://ampcamp.berkeley.edu/wp-content/uploads/2012/06/matei-zaharia-part-2-amp-camp-2012-standalone-programs.pdf. Another possibility is linear regression. But feel free to also come up with your own.

There are also a number of Python issues open relating to adding some missing API features, but these require a more thorough understanding of how PySpark work and possibly some hacking around in pickled data: https://spark-project.atlassian.net/browse/SPARK-791?jql=component%20%3D%20PySpark%20AND%20status%20%3D%20Open . The easiest one to start with is probably SPARK-838.

Matei

Yeah, and maybe we will want to change to Maven as the recommended tool for assembly building. I want to look into this more for the 0.8 release.

Matei

Yeah, that is true. But the assembly shouldn't include the examples project at all IMO -- if it does now, we should remove it.

Matei

Basically the way to think of the assembly is that it should have libraries that users' client programs need to run. These are core, repl (needed if they use the shell), and likely bagel and streaming and mllib, though originally we'd opted to leave those out. We are still deciding on that -- could go either way.

Matei

Hey folks, FYI, I've added a "Starter" tag in JIRA to identify such issues. Here's the page listing them: https://spark-project.atlassian.net/browse/SPARK-865?jql=project%20%3D%2010000%20AND%20labels%20%3D%20Starter

Matei

I'll work on it later today and send it out. Thanks for the reminder!

Matei

Alright, I've written it up: https://wiki.apache.org/incubator/August2013. Let me know if you have any comments.

Matei

Ah, good idea. I've added that in.

Matei

Hi folks,

In order to make the 0.8 release soon, I've created a new branch for it, on which we'll merge only bug fixes and a few of the new deployment features we still want there (e.g. updated EC2 scripts and some work to build one artifact that works with all Hadoop versions). If you continue sending pull requests against master, we will cherry-pick the ones with bug fixes into branch-0.8. Let me know if there are any other things you believe should be in 0.8 as well.

Matei
Yes, that's what I count under "bug fixes".

Matei

I think a change to Optional is good, so let's do that.

@Evan -- documentation should definitely be added; feel free to send a PR adding a page on this to the docs. I also want to take a look at the binary distribution stuff once we've figured out the SBT build a little bit better.

Matei

Thanks for forwarding this! We had indeed looked at SciKit-learn for MLLib.

Matei

Hi Evan,

We plan to do it after Spark 0.8 comes out. You can already find a fairly complete work-in-progress version on branch scala-2.10.

Matei

Hi Henry,

I'd be happy to do this as long as Apache is okay with it! It will save a lot of hassle for our users. Does it basically just require subscribing user@spark.i.a.o to the Google Group?

Matei

The other point, in case it wasn't clear, is that we do want to have an archive on the Apache mailing lists -- all messages sent to Google would also appear on user@spark.i.a.o.

Matei

I understand this Cos, but Jey's patch actually removes the idea of "hadoop2". You only set SPARK_HADOOP_VERSION (which can be 1.0.x, 2.0.0-cdh4, 2.0.5-alpha, etc) and possibly SPARK_YARN_MODE if you want to run on YARN.

Matei

Thanks Chris! We're still working on it and hopefully we'll have some improvements over the old one soon too.

Matei

Hi Mike,

I think the Eclipse project is just broken. The SBT-Eclipse plugin isn't great. I recommend using IntelliJ IDEA (community edition, which is free) -- this is what most of us use. You can do sbt/sbt gen-idea to create an IDEA project.

Matei

Yes, as posted there, we uploaded some files for INFRA to try, and haven't heard back. Is this a thing you could personally help with?

Matei

I sent an email about a week ago saying we're doing a feature freeze and only finishing up some of the current tasks (e.g. better build setup for linking to Hadoop). I think many of those are now in. The other big scary thing left is to change the package name to org.apache.spark.. will do that this weekend. When it's done I plan to post a release candidate.

Matei

Guys, FYI, I'm going to try adding user@spark.i.a.o as a member of the Google group, but before that I'll do it temporarily for the dev group for testing. (I agree that we should move the dev group permanently to Apache). Cross your fingers.

Matei

 Testing cross-posting to Apache.

About this group:

Developer list for the Spark cluster computing framework: www.spark-project.
org.

----------------------- Google Groups Information ----------------------

The owner of the group has set your subscription type as "Email", meaning that
you'll receive a copy of every message posted to the group as they are posted.
Visit this group:

http://groups.google.com/group/spark-developers?hl=en

You can unsubscribe from this group using the following URL:

http://groups.google.com/group/spark-developers/unsub?u=qDIzWBQAAAA3hagNqQDQBiFNMFu2FqbBtx20f395-tTXprg9jvQSzw&hl=en

If you feel that this message is abuse, please inform the Google Groups staff 
by using the URL below. 
http://groups.google.com/groups/abuse?direct=YQAAAB264nDzAAAA0h766AoAAACbjS9RI7DltbsBUGhHhNwnqX8Z_fM&hl=en

We're going to switch from GitHub to the Apache git repo [3] soon. We've been periodically importing stuff there, but I want to have new pull requests happen against https://github.com/apache/incubator-spark directly once we release 0.8. The Apache SVN is only needed for the website, and that's why it has commits.

Matei

Alright, here's a new backup: http://cs.berkeley.edu/~matei/JIRA-backup-20130831.tgz. We only need to import the "SPARK" project from in there. Thanks for looking into it.

Matei

Hi everyone,

In preparation for the 0.8 release, I've put together a first release candidate mostly to help mentors or more experienced Apache people check whether the licensing, POMs, etc are right: http://www.cs.berkeley.edu/~matei/spark-0.8.0-incubating-RC1.tgz. I've tried to write the LICENSE, NOTICE, and POMs as required by Apache. Please take a look if you have a chance -- the ReadMe contains instructions on how to build it and launch the shell.

This first release candidate is still missing some doc pages, so it won't be the final release, but it should have in place all the packaging stuff we'd like for 0.8, and it should be able to run out of the box on Linux, Windows and Mac OS X. Feel free to also test it for functionality; we may merge in a couple more bug fixes but most of the functionality will be there.

Matei
By the way, this tar file also corresponds to commit a106ed8b97e707b36818c11d1d7211fa28636178 in our Apache git repo.

Matei

Hi guys,

> So are you planning to release 0.8 from the master branch (which is at
> a106ed8... now) or from branch-0.8?

Right now the branches are the same in terms of content (though I might not have merged the latest changes into 0.8). If we add stuff into master that we won't want in 0.8 we'll break that.

> My recommendation is that we start to use the Incubator release doc/guide:
> 
> http://incubator.apache.org/guides/releasemanagement.html

Cool, thanks for the pointer. I'll try to follow the steps there about signing.

> Are we "locking" pull requests to github repo by tomorrow?
> Meaning no more push to GitHub repo for Spark.
> 
> From your email seems like there will be more potential pull requests for
> github repo to be merged back to ASF Git repo.

We'll probably use the GitHub repo for the last few changes in this release and then switch. The reason is that there's a bit of work to do pull requests against the Apache one.

Matei
Okay, thanks for asking them about it. I agree that it would be awesome to move JIRA over as soon as possible.

Matei

Yup, the plan is as follows:

- Make pull request against the mirror
- Code review on GitHub as usual
- Whoever merges it will simply merge it into the main Apache repo; when this propagates, the PR will be marked as merged

I found at least one other Apache project that did this: http://wiki.apache.org/cordova/ContributorWorkflow.

Matei

Yes, please do the PRs against the GitHub repo for now.

Matei

As far as I understood, we will have to manually merge those PRs into the Apache repo. However, GitHub will notice that they're "merged" as soon as it sees those commits in the repo, and will automatically close them. At least this is my experience merging other peoples' code (sometimes I just check out their branch from their repo and merge it manually).

Matei

BTW, what docs are you planning to write? Something on make-distribution.sh would be nice.

Matei

Cool, thanks for trying it out!

Regarding the pull request model, let's see how it goes. I'd prefer not to have to support two different ways to send patches, but let's see what people ask for -- maybe existing Apache contributors will be more familiar with the JIRA way. I do want to document the model much better on our "how to contribute" page before our last release candidate of 0.8 though.

Matei

> "The plan"?  Can we do that now?  If I'm putting together a pull request 
> for post-0.8, is there any reason to wait to fork 
> apache/incubator-spark?

Nope, go for it. It would be great if people started doing this now.

Matei


Yup, use a) for pre-0.8 stuff.

Matei

Hi guys,

I've written a draft update at https://wiki.apache.org/incubator/September2013#preview. Let me know how it looks.

Matei

Let me try to put together a doc on this. I wanted to wait for 0.8 to be out to simplify the process and immediately have a new "how to contribute" doc on the website, but I guess I can host this elsewhere if the release takes a while.

Matei

Henry, the problem is that we already have a website: http://spark.incubator.apache.org/docs/latest/contributing-to-spark.html. That's what most people read, so we need to update that.

Matei

Cos, the examples are being built into a separate assembly. That should also be true in the Maven assembly project as far as I can tell.

Matei

The examples do ship with their dependencies though AFAIK. I added a Maven shade plugin entry in the examples POM. Anyway, as long as you can run with that JAR, it means it works!

Matei

Yup, expect to see a pull request soon.

Matei

Hi Henry,

> For example some in repl directory has this:
> 
> /* NSC -- new Scala compiler
> * Copyright 2005-2011 LAMP/EPFL
> * @author Paul Phillips
> */
> 
> Not sure if we need to ASF header to it since we are technically put
> in under apache package.

Regarding this, it's a third-party library that we included and modified under a different license. It's listed in LICENSE.

Thanks for the feedback otherwise! I'm also working on a guide on how to contribute via the Apache repo but I've been traveling the past few days and am still away today.

Matei


> 
> Scala source files under mllib are missing ASF headers.
> 
> 
> 5. Add public key of RE to
> http://people.apache.org/keys/group/spark.asc (@Chris do we still need
> to create KEYS file in the Spark git repo?)
> 
> Once these are completed then the RE will ask dev@ list to verify the
> proposed RC build/artifacts through VOTE thread. If we got no -1 then
> we could bring it to general@ncubator.a.o list for VOTE from IPMCs.
> 
> Other mentors please do definitely chime in about it.
> 
> 
> Thanks,
> 
> Henry
> 
> 
> On Thu, Sep 5, 2013 at 8:08 PM, Patrick Wendell  wrote:
>> Hey All,
>> 
>> Matei asked me to pick this up because he's travelling this week. I
>> cut a second release candidate from the head of the 0.8 branch (on
>> mesos/spark gitub) to address the following issues:
>> 
>> - RC is now hosted in an apache web space
>> - RC now includes signature
>> - RC now includes MD5 and SHA512 digests
>> 
>> [tgz] http://people.apache.org/~pwendell/spark-rc/spark-0.8.0-src-incubating-RC2.tgz
>> [all files] http://people.apache.org/~pwendell/spark-rc/
>> 
>> It would be great to get feedback on the release structure. I also
>> changed the name to include "src" since we will be releasing both
>> source and binary releases.
>> 
>> I was a bit confused about how to attach my GPG key to the spark.asc
>> file. I took the following steps.
>> 
>> 1. Greated a GPG key locally
>> 2. Distributed the key to public key servers (gpg --send-key)
>> 3. Add exported key to my apache web space:
>> http://people.apache.org/~pwendell/9E4FE3AF.asc
>> 4. Added the key fingerprint at id.apage.org
>> 5. Create an apache FOAF file with the key signature
>> 
>> However, this doesn't seem sufficient to get my key on this page (at
>> least, not yet):
>> http://people.apache.org/keys/group/spark.asc
>> 
>> Chris - are there other steps I missed? Is there a manual way to
>> augment this file?
>> 
>> - Patrick


I don't think we're allowed to just slap on a license header since it's mostly their code, but I'm not sure. Maybe you can ask someone who's done this before. I though it was best to use and report it under the terms of their license. Which package name it's in doesn't matter -- it's still substantially the same code.

Matei

Yeah, unfortunately I think this way of doing mirroring won't work. Give me a few days and I'll start closing the Google groups and getting people here (starting with the dev list first).

Matei


Cool, thanks for posting this! We'll probably merge it after the 0.8.0 release since it's a big change.

Matei

Please use 1 for now, because Apache Infra hasn't finished importing our issues into 2.

Matei

Hmm, good point -- might be worth it to fix this for 0.8.0.

Matei

We could also manually write an overview of the high-level changes (similar to a set of release notes) if desired.

Matei

I'm not sure where to ask this, but here goes: since we'll receive pull requests on https://github.com/apache/incubator-spark, is there any way to subscribe the dev@ list (or some other list) to the GitHub emails from those discussions? I think that would be useful, and similar to subscribing it to JIRA. My main question is who manages the mirroring (i.e. the github.com/apache account), since they'd have to configure this.

Matei
Awesome, thanks.

Matei

Patrick's emails to this are not making it, so I just want to try sending one.

Matei
Yes, unfortunately this is a side-effect of how we change the Scala REPL's code generation. Before, the REPL would create a singleton object for each line:

object Line1 {
  class Foo
}

object Line2 {
  import Line1.Foo
  class Bar(val x: Int) extends Foo
}

And so on. We change this to:

class Line1 {
  class Foo
}
object Line1 { val INSTANCE = new Line1 }

class Line2 {
  import Line1.INSTANCE.Foo
  class Bar(val x: Int) extends Foo
}
object Line1 { val INSTANCE = new Line2 }

Now, Foo becomes an inner class of Line1 instead of a top-level static class, so it's bound to the Line1 instance that it belongs to, and the compiler can't prove that the Foos you pass to meow are necessarily subclasses of Line1.INSTANCE.Bar (as opposed to Bar in general). 

Unfortunately this is kind of tricky to fix without more hacking of the Scala interpreter. The reason we switched to these classes is to allow variable definitions to work correctly. For example, if you do

var x = Console.readLine()

The original Scala interpreter would do

object Line1 {
  var x = Console.readLine()
}

This is bad because it would mean that on *every* worker node that uses x, we'd be trying to call Console.readLine.

Matei

Hey Patrick, unfortunately, I noticed some bugs in the documentation on Mesos that I thought I had pushed a fix for, but I didn't. The fix is:
- In docs/_config.yml, change MESOS_VERSION to 0.12.1

In addition, if we do this, we might as well change it to 0.13.0 throughout. That is in docs/_config.yml, project/SparkBuild.scala, and pom.xml.

I also noticed that there's no CHANGES.txt in this release.

Otherwise, the files and code look fine. I built it and ran through the tests on Mac OS X.

Matei

FWIW, I tested it otherwise and it seems good modulo this issue.

Matei

Hi Shane,

I agree with all these points. Improving the configuration system is one of the main things I'd like to have in the next release.

> 1) Usually the application developers/users and platform administrators
> belongs to two teams. So it's better to separate the scripts used by
> administrators and application users, e.g. put them in sbin and bin folders
> respectively

Yup, right now we don't have any attempt to install on standard system paths.

> 3) If there are multiple ways to specify an option, an overriding rule
> should be present and should not be error-prone.

Yes, I think this should always be Configuration class in code > system properties > env vars. Over time we will deprecate the env vars and maybe even system properties.

> 4) Currently the options are set and get using System property. It's hard
> to manage and inconvenient for users. It's good to gather the options into
> one file using format like xml or json.

I think this is the main thing to do first -- pick one configuration class and change the code to use this.

> Our rough proposal:
> 
>   - Scripts
> 
>   1. make an "sbin" folder containing all the scripts for administrators,
>   specifically,
>      - all service administration scripts, i.e. start-*, stop-*,
>      slaves.sh, *-daemons, *-daemon scripts
>      - low-level or internally used utility scripts, i.e.
>      compute-classpath, spark-config, spark-class, spark-executor
>   2. make a "bin" folder containing all the scripts for application
>   developers/users, specifically,
>      - user level app  running scripts, i.e. pyspark, spark-shell, and we
>      propose to add a script "spark" for users to run applications (very much
>      like spark-class but may add some more control or convenient utilities)
>      - scripts for status checking, e.g. spark and hadoop version
>      checking, running applications checking, etc. We can make this a separate
>      script or add functionality to "spark" script.
>   3. No wandering scripts outside the sbin and bin folders

Makes sense.

>   -  Configurations/Options and overriding rule
> 
>   1. Define a Configuration class which contains all the options available
>   for Spark application. A Configuration instance can be de-/serialized
>   from/to a json formatted file.
>   2. Each application (SparkContext) has one Configuration instance and it
>   is initialized by the application which creates it (either read from file
>   or passed from command line options or env SPARK_JAVA_OPTS).
>   3. When launching an Executor on a node, the Configuration is firstly
>   initialized using the node-local configuration file as default. The
>   Configuration passed from application driver context will override any
>   options specified in default.

This sounds great to me! The one thing I'll add is that we might want to prevent applications from overriding certain settings on each node, such as work directories. The best way is to probably just ignore the app's version of those settings in the Executor.

If you guys would like, feel free to write up this design on SPARK-544 and start working on it. I think it looks good.

Matei
+1

Tried new staging repo to make sure the issue with RC5 is fixed.

Matei

In Maven, mvn package should also create the assembly, but the non-obvious thing is that it needs to happen for all projects before mvn test for core works. Unfortunately I don't know any easy way around that.

Matei

Good catch, we should probably just load it from the checkpoint in the latter case.

Please send pull requests to apache/incubator-spark instead -- there are some new docs on it here: https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

Matei

Maybe we can replace the link to "official Apache download site" in the release notes to point to the mirrors? Do the mirrors all have signatures on them too?

Matei

If the mirrors don't have the signatures, then we should probably link to the mirrors and the signatures separately. It's definitely important to have a link to the mirrors so people can get this through ASF infrastructure without hitting only the main server.

It's true that they don't seem to have them, even for other projects -- for example check out http://mirror.tcpdiag.net/apache/hadoop/common/hadoop-2.0.5-alpha/.

Matei

Hi Evan,

I think this is an old property that isn't used anymore, so it would be good to clean it up and get rid of it.

Matei

The main reason I wanted them separate was that people might use Akka in their own application for other things. As such, the Spark Akka properties only affect Akka instances started by Spark. I think we should keep them separate to avoid messing with users' applications.

Matei

Sure, that makes sense.

Matei

Yeah, that's too bad. I've now pushed the scala-2.10 branch from the old repo to Apache. It should show up on the Apache GitHub mirror soon.

Matei


Hey folks, sorry for the delay, but I've now written a report at https://wiki.apache.org/incubator/October2013. I saw Dave Fisher, who was assigned to oversee the project, already looked at the page earlier today. Dave, please take another look.

Matei

For most development, you might not need to do assembly. You can run most of the unit tests if you just do sbt compile -- only the ones that spawn processes, like DistributedSuite, won't work. That said, we are looking to optimize assembly by maybe having it only package the dependencies rather than Spark itself -- there were some messages on this earlier. For now I'd just recommend doing it in a RAMFS if possible (symlink the assembly/target directory to be a RAMFS).

Matei

Hey Shane, I don't know if you saw my message on GitHub, but I did review this a few days ago: https://github.com/apache/incubator-spark/pull/21. Make sure you're allowing emails from GitHub to get comments. It looks good overall but I had some suggestions in there.

Matei

Hey folks, FYI, the talk submission deadline for this is October 25th. We've gotten a lot of great submissions already. If you'd like to submit one, go to http://www.spark-summit.org/submit/. It can be about anything -- projects you're doing with Spark, open source development within the project, tips / tutorials, etc. Demos are also welcome.

Matei

This is a good idea, though you may also be able to do it in your KryoRegistrator by just calling kryo.setRegistrationOptional there. (You get full access to the Kryo object.)

Omitting to register a class is not an error by the way, it just leads to bigger data.

Matei

Hi folks,

Just as a heads-up, I think we're getting close to the major features to make a Spark 0.8.1 release, and we'd like to merge the Scala 2.10 branch into master to facilitate work on Spark 0.9. I'm thinking of doing this in the next week. This won't mean that development on Scala 2.9 will stop -- that will keep going on in the spark-0.8 branch, where we've been cherry-picking nearly every change from master. But it will make it easier to do the next phase of development (configuration system, changes to run scripts, etc) for Scala 2.10 and Spark 0.9. Let me know if you have any concerns about this.

By the way, there's been a lot of stuff contributed to 0.8.1 in the month since we released 0.8! Here are some of the things either merged or close to merging:

- Standalone master fault tolerance
- Shuffle file consolidation (improves performance of big shuffles)
- Better P2P broadcast (improves speed and stability of big broadcasts)
- Optimized hashtable classes
- Sending of large task results through block manager (improves performance)
- Sort() function in PySpark
- New ALS variant for implicit feedback
- Task and job killing

Matei
If this could be compiled to run on a Spark-like engine, it would be interesting, from a quick look this doesn't seem easy because this model has arbitrary graphs of fine-grained tasks (more similar to something like Cilk) rather than parallel operations on collections. I think it would require a different engine with different properties about fault tolerance, locality, etc.

Matei

Just to be clear, Spark actually *does* support general task graphs, similar to Dryad (though a bit simpler in that there's a notion of "stages" and a fixed set of connection patterns between them). However, MBrace goes a step beyond that, in that the graphs can be modified dynamically based on user code. It's also not clear what the granularity of task spawns in MBrace is -- can you spawn stuff that runs for 1 millisecond, or 1 second, or 1 hour? The choice there greatly affects system design.

Matei

Yeah, very cool, thanks for writing these up.

Hi Nick,

This would definitely be interesting to explore, especially if the Julia folks are open to supporting other parallel compute engines. In terms of technical work, the toughest part will likely be capturing Julia functions and shipping them across the network, as you said. It all depends on how easy that is within that language. Beyond that, you may want to ask them for JVM bindings. There is lots of software that uses the JVM so it might not be a bad idea to add it. I would avoid going through Python if possible unless you specifically think mixing those libraries is important (but even then it might be possible to do that in a different way, e.g. call Python from Julia).

Matei

So far we've maintained backwards compatibility for point releases in everything except internal or alpha APIs. One place where it changed, for example, is when Twitter removed its username/password authentication method, which required us to change the Spark Streaming API (but that was actually alpha). Which APIs did you see broken?

The problem is likely that Shark calls into on some internal APIs, which we should fix in the future to put it on more equal footing with other clients. But you can expect that Shark releases from our team will work with the corresponding Spark release, and user programs written for Spark 0.8.0 will definitely work for 0.8.1.

Matei

Yeah, true. It would be good to add it back.

Matei

Hi everyone,

We're glad to announce the agenda of the Spark Summit, which will happen on December 2nd and 3rd in San Francisco. We have 5 keynotes and 24 talks lined up, from 18 different companies. Check out the agenda here: http://spark-summit.org/agenda/.

This will be the biggest Spark event yet, with some very cool use case talks, so we hope to see you there! Sign up now to still get access to the early-bird registration rate.

Matei


No worries Chris! Apart from the JIRA thing, we also plan another release or two soon.

Matei

Looks cool! Josh, if you replace CloudPickle with this, make sure to also update the LICENSE file, which is supposed to contain third-party licenses.

Matei

+1

Built and tested it on Mac OS X.

Matei


Thanks Patrick for coordinating this release!

Matei

Not that I know of. This would be very useful to add, especially if we can make SBT automatically check the code style (or we can somehow plug this into Jenkins).

Matei

+1 for me as well.

I built and tested this on Mac OS X, and looked through the new docs.

Matei

This is definitely an important issue to fix. Instead of renaming properties, one solution would be to replace Typesafe Config with just reading Java system properties, and disable config files for this release. I kind of like that over renaming.

Matei

Yeah, this is exactly my reasoning as well.

Matei

+1

Re-tested on Mac.

Matei

+1 looked at changes since last RC and tested on mac.

Matei

Just sending a test email from another email address to check for bounces..

+1

Congrats Michael and all for getting this so far. Spark SQL and Catalyst will make it much easier to use structured data in Spark, and open the door for some very cool extensions later.

Matei

+1 tested on Mac OS X.

Matei

Shh, maybe I really wanted people to fix that one issue.

Your key needs to implement hashCode in addition to equals.

Matei

We do actually have replicated StorageLevels in Spark. You can use MEMORY_AND_DISK_2 or construct your own StorageLevel with your own custom replication factor.

BTW you guys should probably have this discussion on the JIRA rather than the dev list; I think the replies somehow ended up on the dev list.

Matei

+1

Tested it on both Windows and Mac OS X, with both Scala and Python. Confirmed that the issues in the previous RC were fixed.

Matei

+1

Tested on Mac OS X and Windows.

Matei


Madhu, can you send me your Wiki username? (Sending it just to me is fine.) I can add you to the list to edit it.

Matei

Done. Looks like this was lost in the JIRA import.

Matei

You can modify project/SparkBuild.scala and build Spark with sbt instead of Maven.


+1

Tested it out on Mac OS X and Windows, looked through docs.

Matei

+1

Tested on Mac OS X.

Matei

Unless you can diagnose the problem quickly, Gary, I think we need to go ahead with this release as is. This release didn't touch the Mesos support as far as I know, so the problem might be a nondeterministic issue with your application. But on the other hand the release does fix some critical bugs that affect all users. We can always do 1.0.2 later if we discover a problem.

Matei

I haven't seen issues using the JVM's own tools (jstack, jmap, hprof and such), so maybe there's a problem in YourKit or in your release of the JVM. Otherwise I'd suggest increasing the heap size of the unit tests a bit (you can do this in the SBT build file). Maybe they are very close to full and profiling pushes them over the edge.

Matei

Yeah, I'd just add a spark-util that has these things.

Matei

You can actually turn off shuffle compression by setting spark.shuffle.compress to false. Try that out, there will still be some buffers for the various OutputStreams, but they should be smaller.

Matei

Yeah, that seems like something we can inline :).

Yeah, that seems like something we can inline :).

Hey Reynold, just to clarify, users will still have to manually broadcast objects that they want to use *across* operations (e.g. in multiple iterations of an algorithm, or multiple map functions, or stuff like that). But they won't have to broadcast something they only use once.

Matei

+1

Tested on Mac, verified CHANGES.txt is good, verified several of the bug fixes.

Matei

For this particular issue, it would be good to know if Hadoop provides an API to determine the Hadoop version. If not, maybe that can be added to Hadoop in its next release, and we can check for it with reflection. We recently added a SparkContext.version() method in Spark to let you tell the version.

Matei

+1

Tested this on Mac OS X.

Matei

We could also do this, though it would be great if the Hadoop project provided this version number as at least a baseline. It's up to distributors to decide which version they report but I imagine they won't remove stuff that's in the reported version number.

Matei

I agree as well. FWIW sometimes I've seen this happen due to language barriers, i.e. contributors whose primary language is not English, but we need more motivation for each change.

Hah, weird. "log" should be protected actually (look at trait Logging). Is your class extending SparkContext or somehow being placed in the org.apache.spark package? Or maybe the Scala compiler looks at it anyway.. in that case we can rename it. Please open a JIRA for it if that's the case.

Hi everyone,

The PMC recently voted to add two new committers and PMC members: Joey Gonzalez and Andrew Or. Both have been huge contributors in the past year -- Joey on much of GraphX as well as quite a bit of the initial work in MLlib, and Andrew on Spark Core. Join me in welcoming them as committers!

Matei




---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Just as a note, when you're developing stuff, you can use "test-only" in sbt, or the equivalent feature in Maven, to run just some of the tests. This is what I do, I don't wait for Jenkins to run things. 90% of the time if it passes the tests that I know could break stuff, it will pass all of Jenkins.

Jenkins should always be doing all the integration tests, so I don't think it will become *that* much shorter in the long run, though it can certainly be improved.

Matei

Just as a note on this paper, apart from implementing the algorithms in naive Python, they also run it in a fairly inefficient way. In particular their implementations send the model out with every task closure, which is really expensive for a large model, and bring it back with collectAsMap(). It would be much more efficient to send it e.g. with SparkContext.broadcast() or keep it distributed on the cluster throughout the computation, instead of making the drive node a bottleneck for communication.

Implementing ML algorithms well by hand is unfortunately difficult, and this is why we have MLlib. The hope is that you either get your desired algorithm out of the box or get a higher-level primitive (e.g. stochastic gradient descent) that you can plug some functions into, without worrying about the communication.

Matei

Hey Gary, just as a workaround, note that you can use Mesos in coarse-grained mode by setting spark.mesos.coarse=true. Then it will hold onto CPUs for the duration of the job.

Matei

BTW it seems to me that even without that patch, you should be getting tasks launched as long as you leave at least 32 MB of memory free on each machine (that is, the sum of the executor memory sizes is not exactly the same as the total size of the machine). Then Mesos will be able to re-offer that machine whenever CPUs free up.

Matei

Was the original issue with Spark 1.1 (i.e. master branch) or an earlier release?

One possibility is that your S3 bucket is in a remote Amazon region, which would make it very slow. In my experience though saveAsTextFile has worked even for pretty large datasets in that situation, so maybe there's something else in your job causing a problem. Have you tried other operations on the data, like count(), or saving synthetic datasets (e.g. sc.parallelize(1 to 100*1000*1000, 20).saveAsTextFile(...)?

Matei

My problem is that I'm not sure this workaround would solve things, given the issue described here (where there was a lot of memory free but it didn't get re-offered). If you think it does, it would be good to explain why it behaves like that.

Matei

Got it. Another thing that would help is if you spot any exceptions or failed tasks in the web UI (http://:4040).

Matei

Hey Nicholas,

In general we've been looking at these periodically (at least I have) and asking people to close out of date ones, but it's true that the list has gotten fairly large. We should probably have an expiry time of a few months and close them automatically. I agree that it's daunting to see so many open PRs.

Matei

Personally I'd actually consider putting CDH4 back if there are still users on it. It's always better to be inclusive, and the convenience of a one-click download is high. Do we have a sense on what % of CDH users still use CDH4?

Matei

+1

Matei

Hi Tom,

HDFS and Spark don't actually have a minimum block size -- so in that first dataset, the files won't each be costing you 64 MB. However, the main reason for difference in performance here is probably the number of RDD partitions. In the first case, Spark will create an RDD with 10000 partitions, one per file, while in the second case it will likely have only 1-2 of them. The number of partitions affects the level of parallelism of operations like reduceByKey (by default, reduceByKey uses as many partitions as the parent RDD it runs on), and in this case, I think it's causing reduceByKey to spill to disk within tasks in the second case and not in the first case, because each task has more input data.

You can see the number of partitions in each stage of your computation on the application web UI at http://:4040 if you want to confirm this. Also, for both programs, you can manually set the number of partitions for the reduce by passing a second argument to reduceByKey (e.g. reduceByKey(myFunc, 100)). In general it's best to choose this so that the input data for each reduce task fits in memory to avoid spilling.

Matei

PySpark doesn't attempt to support Jython at present. IMO while it might be a bit faster, it would lose a lot of the benefits of Python, which are the very strong data processing libraries (NumPy, SciPy, Pandas, etc). So I'm not sure it's worth supporting unless someone demonstrates a really major performance benefit.

There was actually a recent patch to add PyPy support (https://github.com/apache/spark/pull/2144), which is worth a try if you want Python applications to run faster. It might actually be faster overall than Jython.

Matei

Hi folks,

I interrupt your regularly scheduled user / dev list to bring you some pretty cool news for the project, which is that we've been able to use Spark to break MapReduce's 100 TB and 1 PB sort records, sorting data 3x faster on 10x fewer nodes. There's a detailed writeup at http://databricks.com/blog/2014/10/10/spark-breaks-previous-large-scale-sort-record.html. Summary: while Hadoop MapReduce held last year's 100 TB world record by sorting 100 TB in 72 minutes on 2100 nodes, we sorted it in 23 minutes on 206 nodes; and we also scaled up to sort 1 PB in 234 minutes.

I want to thank Reynold Xin for leading this effort over the past few weeks, along with Parviz Deyhim, Xiangrui Meng, Aaron Davidson and Ali Ghodsi. In addition, we'd really like to thank Amazon's EC2 team for providing the machines to make this possible. Finally, this result would of course not be possible without the many many other contributions, testing and feature requests from throughout the community.

For an engine to scale from these multi-hour petabyte batch jobs down to 100-millisecond streaming and interactive queries is quite uncommon, and it's thanks to all of you folks that we are able to make this happen.

Matei
---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Hi Michael,

I've been working on this in my repo: https://github.com/mateiz/spark/tree/decimal. I'll make some pull requests with these features soon, but meanwhile you can try this branch. See https://github.com/mateiz/spark/compare/decimal for the individual commits that went into it. It has exactly the precision stuff you need, plus some optimizations for working on decimals.

Matei

I'm also against these huge reformattings. They slow down development and backporting for trivial reasons. Let's not do that at this point, the style of the current code is quite consistent and we have plenty of other things to worry about. Instead, what you can do is as you edit a file when you're working on a feature, fix up style issues you see. Or, as Josh suggested, some way to make this apply only to new files would help.

Matei

The fixed-length binary type can hold fewer bytes than an int64, though many encodings of int64 can probably do the right thing. We can look into supporting multiple ways to do this -- the spec does say that you should at least be able to read int32s and int64s.

Matei

The biggest scaling issue was supporting a large number of reduce tasks efficiently, which the JIRAs in that post handle. In particular, our current default shuffle (the hash-based one) has each map task open a separate file output stream for each reduce task, which wastes a lot of memory (since each stream has its own buffer).

A second thing that helped efficiency tremendously was Reynold's new network module (https://issues.apache.org/jira/browse/SPARK-2468). Doing I/O on 32 cores, 10 Gbps Ethernet and 8+ disks efficiently is not easy, as can be seen when you try to scale up other software.

Finally, with 30,000 tasks even sending info about every map's output size to each reducer was a problem, so Reynold has a patch that avoids that if the number of tasks is large.

Matei

I'd also wait a bit until these are gone. Jetty is unfortunately a much hairier topic by the way, because the Hadoop libraries also depend on Jetty. I think it will be hard to update. However, a patch that shades Jetty might be nice to have, if that doesn't require shading a lot of other stuff.

Matei

> On Oct 18, 2014, at 4:37 PM, Koert Kuipers  wrote:
> 
> my experience is that there are still a lot of java 6 clusters out there.
> also distros that bundle spark still support java 6
> On Oct 17, 2014 8:01 PM, "Andrew Ash"  wrote:
> 
>> Hi Spark devs,
>> 
>> I've heard a few times that keeping support for Java 6 is a priority for
>> Apache Spark.  Given that Java 6 has been publicly EOL'd since Feb 2013
>>  and the last
>> public update was Apr 2013
>> , why
>> are we still maintaing support for 6?  The only people using it now must be
>> paying for the extended support to continue receiving security fixes.
>> 
>> Bumping the lower bound of Java versions up to Java 7 would allow us to
>> upgrade from Jetty 8 to 9, which is currently a conflict with the
>> Dropwizard framework and a personal pain point.
>> 
>> Java 6 vs 7 for Spark links:
>> Try with resources
>>  for
>> SparkContext et al
>> Upgrade to Jetty 9
>>  Warn when not compiling with Java6
>>  
>> 
>> Who are the people out there that still need Java 6 support?
>> 
>> Thanks!
>> Andrew
>> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


So from my point of view, I'd do it maybe 1-2 years after all the major Hadoop vendors have stopped supporting Java 6. We're not there yet, but we will be soon. The reason is that the cost of staying on Java 6 is much smaller to us (as developers) than the cost of fragmenting the Spark community by having a big chunk of users unable to upgrade past a certain version of Spark (or requiring them to use a modified third-party version, which is a similar thing). There's very little stuff in Java 7 or 8 that would make the project much better if we dropped support for 6 -- this Jetty issue might be one, but we can certainly work around it.

We've done a lot of stuff to reach the broadest set of users, including pushing back the versions of Python and NumPy we support, and in my experience it's been well worth it. In surveys I've seen the majority of users (something like 75%) updating to each new Spark release within a few months of it coming out, which is awesome for keeping the community healthy.

Matei


> On Oct 19, 2014, at 9:48 AM, Corey Nolet  wrote:
> 
> A concrete plan and a definite version upon which the upgrade would be applied sounds like it would benefit the community. If you plan far enough out (as Hadoop has done) and give the community enough of a notice, I can't see it being a problem as they would have ample time upgrade. 
> 
> 
> 
> On Sat, Oct 18, 2014 at 9:20 PM, Marcelo Vanzin <vanzin@cloudera.com  wrote:
> Hadoop, for better or worse, depends on an ancient version of Jetty
> (6), that is even on a different package. So Spark (or anyone trying
> to use a newer Jetty) is lucky on that front...
> 
> IIRC Hadoop is planning to move to Java 7-only starting with 2.7. Java
> 7 is also supposed to be EOL some time next year, so a plan to move to
> Java 7 and, eventually, Java 8 would be nice.
> 
> On Sat, Oct 18, 2014 at 5:44 PM, Matei Zaharia <matei.zaharia@gmail.com  wrote:
> > I'd also wait a bit until these are gone. Jetty is unfortunately a much hairier topic by the way, because the Hadoop libraries also depend on Jetty. I think it will be hard to update. However, a patch that shades Jetty might be nice to have, if that doesn't require shading a lot of other stuff.
> >
> > Matei
> >
> >> On Oct 18, 2014, at 4:37 PM, Koert Kuipers <koert@tresata.com  wrote:
> >>
> >> my experience is that there are still a lot of java 6 clusters out there.
> >> also distros that bundle spark still support java 6
> >> On Oct 17, 2014 8:01 PM, "Andrew Ash" <andrew@andrewash.com  wrote:
> >>
> >>> Hi Spark devs,
> >>>
> >>> I've heard a few times that keeping support for Java 6 is a priority for
> >>> Apache Spark.  Given that Java 6 has been publicly EOL'd since Feb 2013
> >>> <http://www.oracle.com/technetwork/java/eol-135779.html  and the last
> >>> public update was Apr 2013
> >>> <https://en.wikipedia.org/wiki/Java_version_history#Java_6_updates , why
> >>> are we still maintaing support for 6?  The only people using it now must be
> >>> paying for the extended support to continue receiving security fixes.
> >>>
> >>> Bumping the lower bound of Java versions up to Java 7 would allow us to
> >>> upgrade from Jetty 8 to 9, which is currently a conflict with the
> >>> Dropwizard framework and a personal pain point.
> >>>
> >>> Java 6 vs 7 for Spark links:
> >>> Try with resources
> >>> <https://github.com/apache/spark/pull/2575/files#r18152125  for
> >>> SparkContext et al
> >>> Upgrade to Jetty 9
> >>> <https://github.com/apache/spark/pull/167#issuecomment-54544494  >>> Warn when not compiling with Java6
> >>> <https://github.com/apache/spark/pull/859  >>>
> >>>
> >>> Who are the people out there that still need Java 6 support?
> >>>
> >>> Thanks!
> >>> Andrew
> >>>
> >
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org  > For additional commands, e-mail: dev-help@spark.apache.org  >
> 
> 
> 
> --
> Marcelo
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org  For additional commands, e-mail: dev-help@spark.apache.org  
> 


Hi Stephen,

How did you generate your Maven workspace? You need to make sure the Hive profile is enabled for it. For example sbt/sbt -Phive gen-idea.

Matei

> On Oct 28, 2014, at 7:42 PM, Stephen Boesch  wrote:
> 
> I have run on the command line via maven and it is fine:
> 
> mvn   -Dscalastyle.failOnViolation=false -DskipTests -Pyarn -Phadoop-2.3
> compile package install
> 
> 
> But with the latest code Intellij builds do not work. Following is one of
> 26 similar errors:
> 
> 
> Error:(173, 38) not found: value HiveShim
>          Option(tableParameters.get(HiveShim.getStatsSetupConstTotalSize))
>                                     ^


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


In Spark 1.1, the sort-based shuffle (spark.shuffle.manager=sort) will have better performance while creating fewer files. So I'd suggest trying that too.

Matei

> On Nov 3, 2014, at 6:12 PM, Andrew Or  wrote:
> 
> Hey Matt,
> 
> There's some prior work that compares consolidation performance on some
> medium-scale workload:
> http://www.cs.berkeley.edu/~kubitron/courses/cs262a-F13/projects/reports/project16_report.pdf
> 
> There we noticed about 2x performance degradation in the reduce phase on
> ext3. I am not aware of any other concrete numbers. Maybe others have more
> experiences to add.
> 
> -Andrew
> 
> 2014-11-03 17:26 GMT-08:00 Matt Cheah  
>> Hi everyone,
>> 
>> I'm running into more and more cases where too many files are opened when
>> spark.shuffle.consolidateFiles is turned off.
>> 
>> I was wondering if this is a common scenario among the rest of the
>> community, and if so, if it is worth considering the setting to be turned
>> on by default. From the documentation, it seems like the performance could
>> be hurt on ext3 file systems. However, what are the concrete numbers of
>> performance degradation that is seen typically? A 2x slowdown in the
>> average job? 3x? Also, what cause the performance degradation on ext3 file
>> systems specifically?
>> 
>> Thanks,
>> 
>> -Matt Cheah
>> 
>> 
>> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


(BTW this had a bug with negative hash codes in 1.1.0 so you should try branch-1.1 for it).

Matei

> On Nov 3, 2014, at 6:28 PM, Matei Zaharia  wrote:
> 
> In Spark 1.1, the sort-based shuffle (spark.shuffle.manager=sort) will have better performance while creating fewer files. So I'd suggest trying that too.
> 
> Matei
> 
>> On Nov 3, 2014, at 6:12 PM, Andrew Or  wrote:
>> 
>> Hey Matt,
>> 
>> There's some prior work that compares consolidation performance on some
>> medium-scale workload:
>> http://www.cs.berkeley.edu/~kubitron/courses/cs262a-F13/projects/reports/project16_report.pdf
>> 
>> There we noticed about 2x performance degradation in the reduce phase on
>> ext3. I am not aware of any other concrete numbers. Maybe others have more
>> experiences to add.
>> 
>> -Andrew
>> 
>> 2014-11-03 17:26 GMT-08:00 Matt Cheah  
>>> Hi everyone,
>>> 
>>> I'm running into more and more cases where too many files are opened when
>>> spark.shuffle.consolidateFiles is turned off.
>>> 
>>> I was wondering if this is a common scenario among the rest of the
>>> community, and if so, if it is worth considering the setting to be turned
>>> on by default. From the documentation, it seems like the performance could
>>> be hurt on ext3 file systems. However, what are the concrete numbers of
>>> performance degradation that is seen typically? A 2x slowdown in the
>>> average job? 3x? Also, what cause the performance degradation on ext3 file
>>> systems specifically?
>>> 
>>> Thanks,
>>> 
>>> -Matt Cheah
>>> 
>>> 
>>> 
> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Congrats to everyone who helped make this happen. And if anyone has even more machines they'd like us to run on next year, let us know :).

Matei

> On Nov 5, 2014, at 3:11 PM, Reynold Xin  wrote:
> 
> Hi all,
> 
> We are excited to announce that the benchmark entry has been reviewed by
> the Sort Benchmark committee and Spark has officially won the Daytona
> GraySort contest in sorting 100TB of data.
> 
> Our entry tied with a UCSD research team building high performance systems
> and we jointly set a new world record. This is an important milestone for
> the project, as it validates the amount of engineering work put into Spark
> by the community.
> 
> As Matei said, "For an engine to scale from these multi-hour petabyte batch
> jobs down to 100-millisecond streaming and interactive queries is quite
> uncommon, and it's thanks to all of you folks that we are able to make this
> happen."
> 
> Updated blog post:
> http://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html
> 
> 
> 
> 
> On Fri, Oct 10, 2014 at 7:54 AM, Matei Zaharia  wrote:
> 
>> Hi folks,
>> 
>> I interrupt your regularly scheduled user / dev list to bring you some
>> pretty cool news for the project, which is that we've been able to use
>> Spark to break MapReduce's 100 TB and 1 PB sort records, sorting data 3x
>> faster on 10x fewer nodes. There's a detailed writeup at
>> http://databricks.com/blog/2014/10/10/spark-breaks-previous-large-scale-sort-record.html.
>> Summary: while Hadoop MapReduce held last year's 100 TB world record by
>> sorting 100 TB in 72 minutes on 2100 nodes, we sorted it in 23 minutes on
>> 206 nodes; and we also scaled up to sort 1 PB in 234 minutes.
>> 
>> I want to thank Reynold Xin for leading this effort over the past few
>> weeks, along with Parviz Deyhim, Xiangrui Meng, Aaron Davidson and Ali
>> Ghodsi. In addition, we'd really like to thank Amazon's EC2 team for
>> providing the machines to make this possible. Finally, this result would of
>> course not be possible without the many many other contributions, testing
>> and feature requests from throughout the community.
>> 
>> For an engine to scale from these multi-hour petabyte batch jobs down to
>> 100-millisecond streaming and interactive queries is quite uncommon, and
>> it's thanks to all of you folks that we are able to make this happen.
>> 
>> Matei
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>> For additional commands, e-mail: user-help@spark.apache.org
>> 
>> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Several people asked about having maintainers review the PR queue for their modules regularly, and I like that idea. We have a new tool now to help with that in https://spark-prs.appspot.com.

In terms of the set of open PRs itself, it is large but note that there are also 2800 *closed* PRs, which means we close the majority of PRs (and I don't know the exact stats but I'd guess that 90% of those are accepted and merged). I think one problem is that with GitHub, people often develop something as a PR and have a lot of discussion on there (including whether we even want the feature). I recently updated our "how to contribute" page to encourage opening a JIRA and having discussions on the dev list first, but I do think we need to be faster with closing ones that we don't have a plan to merge. Note that Hadoop, Hive, HBase, etc also have about 300 issues each in the "patch available" state, so this is some kind of universal constant :P.

Matei


> On Nov 5, 2014, at 10:46 PM, Sean Owen  wrote:
> 
> Naturally, this sounds great. FWIW my only but significant worry about
> Spark is scaling up to meet unprecedented demand in the form of
> questions and contribution. Clarifying responsibility and ownership
> helps more than it hurts by adding process.
> 
> This is related but different topic, but, I wonder out loud what this
> can do to help clear the backlog -- ~*1200* open JIRAs and ~300 open
> PRs, most of which have de facto already fallen between some cracks.
> This harms the usefulness of these tools and processes.
> 
> I'd love to see this translate into triage / closing of most of it by
> maintainers, and new actions and strategies for increasing
> 'throughput' in review and/or helping people make better contributions
> in the first place.
> 
> On Thu, Nov 6, 2014 at 1:31 AM, Matei Zaharia  wrote:
>> Hi all,
>> 
>> I wanted to share a discussion we've been having on the PMC list, as well as call for an official vote on it on a public list. Basically, as the Spark project scales up, we need to define a model to make sure there is still great oversight of key components (in particular internal architecture and public APIs), and to this end I've proposed implementing a maintainer model for some of these components, similar to other large projects.


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


So I don't understand, Greg, are the partial committers committers, or are they not? Spark also has a PMC, but our PMC currently consists of all committers (we decided not to have a differentiation when we left the incubator). I see the Subversion partial committers listed as "committers" on https://people.apache.org/committers-by-project.html#subversion, so I assume they are committers. As far as I can see, CloudStack is similar.

Matei

> On Nov 6, 2014, at 4:43 PM, Greg Stein  wrote:
> 
> Partial committers are people invited to work on a particular area, and they do not require sign-off to work on that area. They can get a sign-off and commit outside that area. That approach doesn't compare to this proposal.
> 
> Full committers are PMC members. As each PMC member is responsible for *every* line of code, then every PMC member should have complete rights to every line of code. Creating disparity flies in the face of a PMC member's responsibility. If I am a Spark PMC member, then I have responsibility for GraphX code, whether my name is Ankur, Joey, Reynold, or Greg. And interposing a barrier inhibits my responsibility to ensure GraphX is designed, maintained, and delivered to the Public.
> 
> Cheers,
> -g
> 
> (and yes, I'm aware of COMMITTERS; I've been changing that file for the past 12 years :-) )
> 
> On Thu, Nov 6, 2014 at 6:28 PM, Patrick Wendell <pwendell@gmail.com  wrote:
> In fact, if you look at the subversion commiter list, the majority of
> people here have commit access only for particular areas of the
> project:
> 
> http://svn.apache.org/repos/asf/subversion/trunk/COMMITTERS  
> On Thu, Nov 6, 2014 at 4:26 PM, Patrick Wendell <pwendell@gmail.com  wrote:
> > Hey Greg,
> >
> > Regarding subversion - I think the reference is to partial vs full
> > committers here:
> > https://subversion.apache.org/docs/community-guide/roles.html  >
> > - Patrick
> >
> > On Thu, Nov 6, 2014 at 4:18 PM, Greg Stein <gstein@gmail.com  wrote:
> >> -1 (non-binding)
> >>
> >> This is an idea that runs COMPLETELY counter to the Apache Way, and is
> >> to be severely frowned up. This creates *unequal* ownership of the
> >> codebase.
> >>
> >> Each Member of the PMC should have *equal* rights to all areas of the
> >> codebase until their purview. It should not be subjected to others'
> >> "ownership" except throught the standard mechanisms of reviews and
> >> if/when absolutely necessary, to vetos.
> >>
> >> Apache does not want "leads", "benevolent dictators" or "assigned
> >> maintainers", no matter how you may dress it up with multiple
> >> maintainers per component. The fact is that this creates an unequal
> >> level of ownership and responsibility. The Board has shut down
> >> projects that attempted or allowed for "Leads". Just a few months ago,
> >> there was a problem with somebody calling themself a "Lead".
> >>
> >> I don't know why you suggest that Apache Subversion does this. We
> >> absolutely do not. Never have. Never will. The Subversion codebase is
> >> owned by all of us, and we all care for every line of it. Some people
> >> know more than others, of course. But any one of us, can change any
> >> part, without being subjected to a "maintainer". Of course, we ask
> >> people with more knowledge of the component when we feel
> >> uncomfortable, but we also know when it is safe or not to make a
> >> specific change. And *always*, our fellow committers can review our
> >> work and let us know when we've done something wrong.
> >>
> >> Equal ownership reduces fiefdoms, enhances a feeling of community and
> >> project ownership, and creates a more open and inviting project.
> >>
> >> So again: -1 on this entire concept. Not good, to be polite.
> >>
> >> Regards,
> >> Greg Stein
> >> Director, Vice Chairman
> >> Apache Software Foundation
> >>
> >> On Wed, Nov 05, 2014 at 05:31:58PM -0800, Matei Zaharia wrote:
> >>> Hi all,
> >>>
> >>> I wanted to share a discussion we've been having on the PMC list, as well as call for an official vote on it on a public list. Basically, as the Spark project scales up, we need to define a model to make sure there is still great oversight of key components (in particular internal architecture and public APIs), and to this end I've proposed implementing a maintainer model for some of these components, similar to other large projects.
> >>>
> >>> As background on this, Spark has grown a lot since joining Apache. We've had over 80 contributors/month for the past 3 months, which I believe makes us the most active project in contributors/month at Apache, as well as over 500 patches/month. The codebase has also grown significantly, with new libraries for SQL, ML, graphs and more.
> >>>
> >>> In this kind of large project, one common way to scale development is to assign "maintainers" to oversee key components, where each patch to that component needs to get sign-off from at least one of its maintainers. Most existing large projects do this -- at Apache, some large ones with this model are CloudStack (the second-most active project overall), Subversion, and Kafka, and other examples include Linux and Python. This is also by-and-large how Spark operates today -- most components have a de-facto maintainer.
> >>>
> >>> IMO, adopting this model would have two benefits:
> >>>
> >>> 1) Consistent oversight of design for that component, especially regarding architecture and API. This process would ensure that the component's maintainers see all proposed changes and consider them to fit together in a good way.
> >>>
> >>> 2) More structure for new contributors and committers -- in particular, it would be easy to look up who's responsible for each module and ask them for reviews, etc, rather than having patches slip between the cracks.
> >>>
> >>> We'd like to start with in a light-weight manner, where the model only applies to certain key components (e.g. scheduler, shuffle) and user-facing APIs (MLlib, GraphX, etc). Over time, as the project grows, we can expand it if we deem it useful. The specific mechanics would be as follows:
> >>>
> >>> - Some components in Spark will have maintainers assigned to them, where one of the maintainers needs to sign off on each patch to the component.
> >>> - Each component with maintainers will have at least 2 maintainers.
> >>> - Maintainers will be assigned from the most active and knowledgeable committers on that component by the PMC. The PMC can vote to add / remove maintainers, and maintained components, through consensus.
> >>> - Maintainers are expected to be active in responding to patches for their components, though they do not need to be the main reviewers for them (e.g. they might just sign off on architecture / API). To prevent inactive maintainers from blocking the project, if a maintainer isn't responding in a reasonable time period (say 2 weeks), other committers can merge the patch, and the PMC will want to discuss adding another maintainer.
> >>>
> >>> If you'd like to see examples for this model, check out the following projects:
> >>> - CloudStack: https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide  <https://cwiki.apache.org/confluence/display/CLOUDSTACK/CloudStack+Maintainers+Guide  >>> - Subversion: https://subversion.apache.org/docs/community-guide/roles.html  <https://subversion.apache.org/docs/community-guide/roles.html  >>>
> >>> Finally, I wanted to list our current proposal for initial components and maintainers. It would be good to get feedback on other components we might add, but please note that personnel discussions (e.g. "I don't think Matei should maintain *that* component) should only happen on the private list. The initial components were chosen to include all public APIs and the main core components, and the maintainers were chosen from the most active contributors to those modules.
> >>>
> >>> - Spark core public API: Matei, Patrick, Reynold
> >>> - Job scheduler: Matei, Kay, Patrick
> >>> - Shuffle and network: Reynold, Aaron, Matei
> >>> - Block manager: Reynold, Aaron
> >>> - YARN: Tom, Andrew Or
> >>> - Python: Josh, Matei
> >>> - MLlib: Xiangrui, Matei
> >>> - SQL: Michael, Reynold
> >>> - Streaming: TD, Matei
> >>> - GraphX: Ankur, Joey, Reynold
> >>>
> >>> I'd like to formally call a [VOTE] on this model, to last 72 hours. The [VOTE] will end on Nov 8, 2014 at 6 PM PST.
> >>>
> >>> Matei
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org  >> For additional commands, e-mail: dev-help@spark.apache.org  >>
> 


Alright, Greg, I think I understand how Subversion's model is different, which is that the PMC members are all full committers. However, I still think that the model proposed here is purely organizational (how the PMC and committers organize themselves), and in no way changes peoples' ownership or rights. Certainly the reason I proposed it was organizational, to make sure patches get seen by the right people. I believe that every PMC member still has the same responsibility for two reasons:

1) The PMC is actually what selects the maintainers, so basically this mechanism is a way for the PMC to make sure certain people review each patch.

2) Code changes are all still made by consensus, where any individual has veto power over the code. The maintainer model mentioned here is only meant to make sure that the "experts" in an area get to see each patch *before* it is merged, and choose whether to exercise their veto power.

Let me give a simple example, which is a patch to the Spark core public API. Say I'm a maintainer in this API. Without the maintainer model, the decision on the patch would be made as follows:

- Any committer could review the patch and merge it
- At any point during this process, I (as the main expert on this) could come in and -1 it, or give feedback
- In addition, any other committer beyond me is allowed to -1 this patch

With the maintainer model, the process is as follows:

- Any committer could review the patch and merge it, but they would need to forward it to me (or another core API maintainer) to make sure we also approve
- At any point during this process, I could come in and -1 it, or give feedback
- In addition, any other committer beyond me is still allowed to -1 this patch

The only change in this model is that committers are responsible to forward patches in these areas to certain other committers. If every committer had perfect oversight of the project, they could have also seen every patch to their component on their own, but this list ensures that they see it even if they somehow overlooked it.

It's true that technically this model might "gate" development in the sense of adding some latency, but it doesn't "gate" it any more than consensus as a whole does, where any committer (not even PMC member) can -1 any code change. In fact I believe this will speed development by motivating the maintainers to be active in reviewing their areas and by reducing the chance that mistakes happen that require a revert.

I apologize if this wasn't clear in any way, but I do think it's pretty clear in the original wording of the proposal. The sign-off by a maintainer is simply an extra step in the merge process, it does *not* mean that other committers can't -1 a patch, or that the maintainers get to review all patches, or that they somehow have more "ownership" of the component (since they already had the ability to -1). I also wanted to clarify another thing -- it seems there is a misunderstanding that only PMC members can be maintainers, but this was not the point; the PMC *assigns* maintainers but they can do it out of the whole committer pool (and if we move to separating the PMC from the committers, I fully expect some non-PMC committers to be made maintainers).

I hope this clarifies where we're coming from, and why we believe that this still conforms fully with the spirit of Apache (collaborative, open development that anyone can participate in, and meritocracy for project governance). There were some comments made about the maintainers being only some kind of list of people without a requirement to review stuff, but as you can see it's the requirement to review that is the main reason I'm proposing this, to ensure we have an automated process for patches to certain components to be seen. If it helps we may be able to change the wording to something like "it is every committer's responsibility to forward patches for a maintained component to that component's maintainer", or something like that, instead of using "sign off". If we don't do this, I'd actually be against any measure that lists some component "maintainers" without them having a specific responsibility. Apache is not a place for people to gain kudos by having fancier titles given on a website, it's a place for building great communities and software.

Matei



> On Nov 6, 2014, at 9:27 PM, Greg Stein  wrote:
> 
> On Thu, Nov 6, 2014 at 7:28 PM, Sandy Ryza  wrote:
> 
>> It looks like the difference between the proposed Spark model and the
>> CloudStack / SVN model is:
>> * In the former, maintainers / partial committers are a way of
>> centralizing oversight over particular components among committers
>> * In the latter, maintainers / partial committers are a way of giving
>> non-committers some power to make changes
>> 
> 
> I can't speak for CloudStack, but for Subversion: yes, you're exactly
> right, Sandy.
> 
> We use the "partial committer" role as a way to bring in new committers.
> "Great idea, go work >there<, and have fun". Any PMC member can give a
> single +1, and that new (partial) committer gets and account/access, and is
> off and running. We don't even ask for a PMC vote (though, we almost always
> have a brief discussion).
> 
> The "svnrdump" tool was written by a *Git* Google Summer of Code student.
> He wanted a quick way to get a Subversion dumpfile from a remote
> repository, in order to drop that into Git. We gave him commit access
> directly into trunk/svnrdump, and he wrote the tool. Technically, he could
> commit anywhere in our tree, but we just asked him not to, without a +1
> from a PMC member.
> 
> Partial committers are a way to *include* people into the [coding]
> community. And hopefully, over time, they grow into something more.
> 
> "Maintainers" are a way (IMO) to *exclude* people from certain commit
> activity. (or more precisely: limit/restrict, rather than exclude)
> 
> You can see why it concerns me :-)
> 
> Cheers,
> -g


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Yup, the JIRA for this was https://issues.apache.org/jira/browse/SPARK-540 (one of our older JIRAs). I think it would be interesting to explore this further. Basically the way to add it into the API would be to add a version of persist() that takes another class than StorageLevel, say StorageStrategy, which allows specifying a custom serializer or perhaps even a transformation to turn each partition into another representation before saving it. It would also be interesting if this could work directly on an InputStream or ByteBuffer to deal with off-heap data.

One issue we've found with our current Serializer interface by the way is that a lot of type information is lost when you pass data to it, so the serializers spend a fair bit of time figuring out what class each object written is. With this model, it would be possible for a serializer to know that all its data is of one type, which is pretty cool, but we might also consider ways of expanding the current Serializer interface to take more info.

Matei

> On Nov 7, 2014, at 1:09 AM, Reynold Xin  wrote:
> 
> Technically you can already do custom serializer for each shuffle operation
> (it is part of the ShuffledRDD). I've seen Matei suggesting on jira issues
> (or github) in the past a "storage policy" in which you can specify how
> data should be stored. I think that would be a great API to have in the
> long run. Designing it won't be trivial though.
> 
> 
> On Fri, Nov 7, 2014 at 1:05 AM, Sandy Ryza  wrote:
> 
>> Hey all,
>> 
>> Was messing around with Spark and Google FlatBuffers for fun, and it got me
>> thinking about Spark and serialization.  I know there's been work / talk
>> about in-memory columnar formats Spark SQL, so maybe there are ways to
>> provide this flexibility already that I've missed?  Either way, my
>> thoughts:
>> 
>> Java and Kryo serialization are really nice in that they require almost no
>> extra work on the part of the user.  They can also represent complex object
>> graphs with cycles etc.
>> 
>> There are situations where other serialization frameworks are more
>> efficient:
>> * A Hadoop Writable style format that delineates key-value boundaries and
>> allows for raw comparisons can greatly speed up some shuffle operations by
>> entirely avoiding deserialization until the object hits user code.
>> Writables also probably ser / deser faster than Kryo.
>> * "No-deserialization" formats like FlatBuffers and Cap'n Proto address the
>> tradeoff between (1) Java objects that offer fast access but take lots of
>> space and stress GC and (2) Kryo-serialized buffers that are more compact
>> but take time to deserialize.
>> 
>> The drawbacks of these frameworks are that they require more work from the
>> user to define types.  And that they're more restrictive in the reference
>> graphs they can represent.
>> 
>> In large applications, there are probably a few points where a
>> "specialized" serialization format is useful. But requiring Writables
>> everywhere because they're needed in a particularly intense shuffle is
>> cumbersome.
>> 
>> In light of that, would it make sense to enable varying Serializers within
>> an app? It could make sense to choose a serialization framework both based
>> on the objects being serialized and what they're being serialized for
>> (caching vs. shuffle).  It might be possible to implement this underneath
>> the Serializer interface with some sort of multiplexing serializer that
>> chooses between subserializers.
>> 
>> Nothing urgent here, but curious to hear other's opinions.
>> 
>> -Sandy
>> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Thanks everyone for voting on this. With all of the PMC votes being for, the vote passes, but there were some concerns that I wanted to address for everyone who brought them up, as well as in the wording we will use for this policy.

First, like every Apache project, Spark follows the Apache voting process (http://www.apache.org/foundation/voting.html), wherein all code changes are done by consensus. This means that any PMC member can block a code change on technical grounds, and thus that there is consensus when something goes in. It's absolutely true that every PMC member is responsible for the whole codebase, as Greg said (not least due to legal reasons, e.g. making sure it complies to licensing rules), and this idea will not change that. To make this clear, I will include that in the wording on the project page, to make sure new committers and other community members are all aware of it.

What the maintainer model does, instead, is to change the review process, by having a required review from some people on some types of code changes (assuming those people respond in time). Projects can have their own diverse review processes (e.g. some do commit-then-review and others do review-then-commit, some point people to specific reviewers, etc). This kind of process seems useful to try (and to refine) as the project grows. We will of course evaluate how it goes and respond to any problems.

So to summarize,

- Every committer is responsible for, and more than welcome to review and vote on, every code change. In fact all community members are welcome to do this, and lots are doing it.
- Everyone has the same voting rights on these code changes (namely consensus as described at http://www.apache.org/foundation/voting.html)
- Committers will be asked to run patches that are making architectural and API changes by the maintainers before merging.

In practice, none of this matters too much because we are not exactly a hot-well of discord ;), and even in the case of discord, the point of the ASF voting process is to create consensus. The goal is just to have a better structure for reviewing and minimize the chance of errors.

Here is a tally of the votes:

Binding votes (from PMC): 17 +1, no 0 or -1

Matei Zaharia
Michael Armbrust
Reynold Xin
Patrick Wendell
Andrew Or
Prashant Sharma
Mark Hamstra
Xiangrui Meng
Ankur Dave
Imran Rashid
Jason Dai
Tom Graves
Sean McNamara
Nick Pentreath
Josh Rosen
Kay Ousterhout
Tathagata Das

Non-binding votes: 18 +1, one +0, one -1

+1:
Nan Zhu
Nicholas Chammas
Denny Lee
Cheng Lian
Timothy Chen
Jeremy Freeman
Cheng Hao
Jackylk Likun
Kousuke Saruta
Reza Zadeh
Xuefeng Wu
Witgo
Manoj Babu
Ravindra Pesala
Liquan Pei
Kushal Datta
Davies Liu
Vaquar Khan

+0: Corey Nolet

-1: Greg Stein

I'll send another email when I have a more detailed writeup of this on the website.

Matei
---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Hey Andrew, your JIRA search link seems wrong, it's probably supposed to be this:

https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20resolution%20%3D%20Fixed%20AND%20fixVersion%20%3D%201.1.1%20ORDER%20BY%20priority%20DESC

Matei

> On Nov 10, 2014, at 2:18 PM, Andrew Or  wrote:
> 
> (Tonight at midnight being in PST 12am on 11/11)
> 
> 2014-11-10 14:17 GMT-08:00 Andrew Or  
>> Hi everyone,
>> 
>> I am the release manager for 1.1.1, and I am preparing to cut a release
>> tonight at midnight. 1.1.1 is a maintenance release which will ship several
>> important bug fixes to users of Spark 1.1. Many users are waiting for
>> these fixes so I would like to release it as soon as possible.
>> 
>> At this point, I believe we have already back ported all critical fixes
>> from master other than a few known ones. Below is a list of issues that
>> have been back ported into 1.1.1. If there are other critical fixes in the
>> master branch that are not in this list, please let me know and I will take
>> a look.
>> 
>> 
>> https://issues.apache.org/jira/browse/SPARK-3653?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%201.1.1%20AND%20fixVersion%20%3D%201.2.0
>> 
>> Best,
>> - Andrew
>> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


+1

Tested on Mac OS X, and verified that sort-based shuffle bug is fixed.

Matei

> On Nov 14, 2014, at 10:45 AM, Andrew Or  wrote:
> 
> Hi all, since the vote ends on a Sunday, please let me know if you would
> like to extend the deadline to allow more time for testing.
> 
> 2014-11-13 12:10 GMT-08:00 Sean Owen  
>> Ah right. This is because I'm running Java 8. This was fixed in
>> SPARK-3329 (
>> https://github.com/apache/spark/commit/2b7ab814f9bde65ebc57ebd04386e56c97f06f4a#diff-7bfd8d7c8cbb02aa0023e4c3497ee832
>> ).
>> Consider back-porting it if other reasons arise, but this is specific
>> to tests and to Java 8.
>> 
>> On Thu, Nov 13, 2014 at 8:01 PM, Andrew Or  wrote:
>>> Yeah, this seems to be somewhat environment specific too. The same test
>> has
>>> been passing here for a while:
>>> 
>> https://amplab.cs.berkeley.edu/jenkins/job/Spark-1.1-Maven-pre-YARN/hadoop.version=1.0.4,label=centos/lastBuild/consoleFull
>>> 
>>> 2014-11-13 11:26 GMT-08:00 Michael Armbrust  
>>>> Hey Sean,
>>>> 
>>>> Thanks for pointing this out.  Looks like a bad test where we should be
>>>> doing Set comparison instead of Array.
>>>> 
>>>> Michael
>>>> 
>>>> On Thu, Nov 13, 2014 at 2:05 AM, Sean Owen  wrote:
>>>>> 
>>>>> LICENSE and NOTICE are fine. Signature and checksum is fine. I
>>>>> unzipped and built the plain source distribution, which built.
>>>>> 
>>>>> However I am seeing a consistent test failure with "mvn -DskipTests
>>>>> clean package; mvn test". In the Hive module:
>>>>> 
>>>>> - SET commands semantics for a HiveContext *** FAILED ***
>>>>>  Expected Array("spark.sql.key.usedfortestonly=test.val.0",
>>>>> 
>>>>> 
>> "spark.sql.key.usedfortestonlyspark.sql.key.usedfortestonly=test.val.0test.val.0"),
>>>>> but got
>>>>> 
>> Array("spark.sql.key.usedfortestonlyspark.sql.key.usedfortestonly=test.val.0test.val.0",
>>>>> "spark.sql.key.usedfortestonly=test.val.0") (HiveQuerySuite.scala:544)
>>>>> 
>>>>> Anyone else seeing this?
>>>>> 
>>>>> 
>>>>> On Thu, Nov 13, 2014 at 8:18 AM, Krishna Sankar  wrote:
>>>>>> +1
>>>>>> 1. Compiled OSX 10.10 (Yosemite) mvn -Pyarn -Phadoop-2.4
>>>>>> -Dhadoop.version=2.4.0 -DskipTests clean package 10:49 min
>>>>>> 2. Tested pyspark, mlib
>>>>>> 2.1. statistics OK
>>>>>> 2.2. Linear/Ridge/Laso Regression OK
>>>>>> 2.3. Decision Tree, Naive Bayes OK
>>>>>> 2.4. KMeans OK
>>>>>> 2.5. rdd operations OK
>>>>>> 2.6. recommendation OK
>>>>>> 2.7. Good work ! In 1.1.0, there was an error and my program used to
>>>>>> hang
>>>>>> (over memory allocation) consistently running validation using
>>>>>> itertools,
>>>>>> compute optimum rank, lambda,numofiterations/rmse; data - movielens
>>>>>> medium
>>>>>> dataset (1 million records) . It works well in 1.1.1 !
>>>>>> Cheers
>>>>>>  P.S: Missed Reply all, first time
>>>>>> 
>>>>>> On Wed, Nov 12, 2014 at 8:35 PM, Andrew Or  wrote:
>>>>>> 
>>>>>>> I will start the vote with a +1
>>>>>>> 
>>>>>>> 2014-11-12 20:34 GMT-08:00 Andrew Or  
>>>>>>>> Please vote on releasing the following candidate as Apache Spark
>>>>>>>> version
>>>>>>> 1
>>>>>>>> .1.1.
>>>>>>>> 
>>>>>>>> This release fixes a number of bugs in Spark 1.1.0. Some of the
>>>>>>>> notable
>>>>>>>> ones are
>>>>>>>> - [SPARK-3426] Sort-based shuffle compression settings are
>>>>>>>> incompatible
>>>>>>>> - [SPARK-3948] Stream corruption issues in sort-based shuffle
>>>>>>>> - [SPARK-4107] Incorrect handling of Channel.read() led to data
>>>>>>> truncation
>>>>>>>> The full list is at http://s.apache.org/z9h and in the
>> CHANGES.txt
>>>>>>>> attached.
>>>>>>>> 
>>>>>>>> The tag to be voted on is v1.1.1-rc1 (commit 72a4fdbe):
>>>>>>>> http://s.apache.org/cZC
>>>>>>>> 
>>>>>>>> The release files, including signatures, digests, etc can be found
>>>>>>>> at:
>>>>>>>> http://people.apache.org/~andrewor14/spark-1.1.1-rc1/
>>>>>>>> 
>>>>>>>> Release artifacts are signed with the following key:
>>>>>>>> https://people.apache.org/keys/committer/andrewor14.asc
>>>>>>>> 
>>>>>>>> The staging repository for this release can be found at:
>>>>>>>> 
>>>>>>>> 
>> https://repository.apache.org/content/repositories/orgapachespark-1034/
>>>>>>>> 
>>>>>>>> The documentation corresponding to this release can be found at:
>>>>>>>> http://people.apache.org/~andrewor14/spark-1.1.1-rc1-docs/
>>>>>>>> 
>>>>>>>> Please vote on releasing this package as Apache Spark 1.1.1!
>>>>>>>> 
>>>>>>>> The vote is open until Sunday, November 16, at 04:30 UTC and
>> passes
>>>>>>>> if
>>>>>>>> a majority of at least 3 +1 PMC votes are cast.
>>>>>>>> [ ] +1 Release this package as Apache Spark 1.1.1
>>>>>>>> [ ] -1 Do not release this package because ...
>>>>>>>> 
>>>>>>>> To learn more about Apache Spark, please see
>>>>>>>> http://spark.apache.org/
>>>>>>>> 
>>>>>>>> Cheers,
>>>>>>>> Andrew
>>>>>>>> 
>>>>>>> 
>>>>> 
>>>>> ---------------------------------------------------------------------
>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>> 
>>>> 
>>> 
>> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


+1

Tested on Mac OS X, checked that bugs with too many small files being spilled are fixed.

Matei

> On Nov 19, 2014, at 7:44 PM, Krishna Sankar  wrote:
> 
> +1
> 1. Compiled OSX 10.10 (Yosemite) mvn -Pyarn -Phadoop-2.4
> -Dhadoop.version=2.4.0 -DskipTests clean package 10:49 min
> 2. Tested pyspark, mlib
> 2.1. statistics OK
> 2.2. Linear/Ridge/Laso Regression OK
> 2.3. Decision Tree, Naive Bayes OK
> 2.4. KMeans OK
> 2.5. rdd operations OK
> 2.6. recommendation OK
> 2.7. Good work ! In 1.1.0, there was an error and my program used to hang
> (over memory allocation) consistently running validation using itertools,
> compute optimum rank, lambda,numofiterations/rmse; data - movielens medium
> dataset (1 million records) . It works well in 1.1.1 !
> 
> Cheers
>  
> On Wed, Nov 19, 2014 at 6:00 PM, Xiangrui Meng  wrote:
> 
>> +1. Checked version numbers and doc. Tested a few ML examples with
>> Java 6 and verified some recently merged bug fixes. -Xiangrui
>> 
>> On Wed, Nov 19, 2014 at 2:51 PM, Andrew Or  wrote:
>>> I will start with a +1
>>> 
>>> 2014-11-19 14:51 GMT-08:00 Andrew Or  
>>>> Please vote on releasing the following candidate as Apache Spark
>> version 1
>>>> .1.1.
>>>> 
>>>> This release fixes a number of bugs in Spark 1.1.0. Some of the notable
>>>> ones are
>>>> - [SPARK-3426] Sort-based shuffle compression settings are incompatible
>>>> - [SPARK-3948] Stream corruption issues in sort-based shuffle
>>>> - [SPARK-4107] Incorrect handling of Channel.read() led to data
>> truncation
>>>> The full list is at http://s.apache.org/z9h and in the CHANGES.txt
>>>> attached.
>>>> 
>>>> Additionally, this candidate fixes two blockers from the previous RC:
>>>> - [SPARK-4434] Cluster mode jar URLs are broken
>>>> - [SPARK-4480][SPARK-4467] Too many open files exception from shuffle
>>>> spills
>>>> 
>>>> The tag to be voted on is v1.1.1-rc2 (commit 3693ae5d):
>>>> http://s.apache.org/p8
>>>> 
>>>> The release files, including signatures, digests, etc can be found at:
>>>> http://people.apache.org/~andrewor14/spark-1.1.1-rc2/
>>>> 
>>>> Release artifacts are signed with the following key:
>>>> https://people.apache.org/keys/committer/andrewor14.asc
>>>> 
>>>> The staging repository for this release can be found at:
>>>> https://repository.apache.org/content/repositories/orgapachespark-1043/
>>>> 
>>>> The documentation corresponding to this release can be found at:
>>>> http://people.apache.org/~andrewor14/spark-1.1.1-rc2-docs/
>>>> 
>>>> Please vote on releasing this package as Apache Spark 1.1.1!
>>>> 
>>>> The vote is open until Saturday, November 22, at 23:00 UTC and passes if
>>>> a majority of at least 3 +1 PMC votes are cast.
>>>> [ ] +1 Release this package as Apache Spark 1.1.1
>>>> [ ] -1 Do not release this package because ...
>>>> 
>>>> To learn more about Apache Spark, please see
>>>> http://spark.apache.org/
>>>> 
>>>> Cheers,
>>>> Andrew
>>>> 
>> 
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>> 
>> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


You can still send patches for docs until the release goes out -- please do if you see stuff.

Matei

> On Nov 20, 2014, at 6:39 AM, Madhu  wrote:
> 
> Thanks Patrick.
> 
> I've been testing some 1.2 features, looks good so far.
> I have some example code that I think will be helpful for certain MR-style
> use cases (secondary sort).
> Can I still add that to the 1.2 documentation, or is that frozen at this
> point?
> 
> 
> 
> -----
> --
> Madhu
> https://www.linkedin.com/in/msiddalingaiah
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/ANNOUNCE-Spark-1-2-0-Release-Preview-Posted-tp9400p9449.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Hector, is this a comment on 1.1.1 or on the 1.2 preview?

Matei

> On Nov 20, 2014, at 11:39 AM, Hector Yee  wrote:
> 
> I think it is a race condition caused by netty deactivating a channel while
> it is active.
> Switched to nio and it works fine
> --conf spark.shuffle.blockTransferService=nio
> 
> On Thu, Nov 20, 2014 at 10:44 AM, Hector Yee  wrote:
> 
>> I'm still seeing the fetch failed error and updated
>> https://issues.apache.org/jira/browse/SPARK-3633
>> 
>> On Thu, Nov 20, 2014 at 10:21 AM, Marcelo Vanzin  wrote:
>> 
>>> +1 (non-binding)
>>> 
>>> . ran simple things on spark-shell
>>> . ran jobs in yarn client & cluster modes, and standalone cluster mode
>>> 
>>> On Wed, Nov 19, 2014 at 2:51 PM, Andrew Or  wrote:
>>>> Please vote on releasing the following candidate as Apache Spark version
>>>> 1.1.1.
>>>> 
>>>> This release fixes a number of bugs in Spark 1.1.0. Some of the notable
>>> ones
>>>> are
>>>> - [SPARK-3426] Sort-based shuffle compression settings are incompatible
>>>> - [SPARK-3948] Stream corruption issues in sort-based shuffle
>>>> - [SPARK-4107] Incorrect handling of Channel.read() led to data
>>> truncation
>>>> The full list is at http://s.apache.org/z9h and in the CHANGES.txt
>>> attached.
>>>> 
>>>> Additionally, this candidate fixes two blockers from the previous RC:
>>>> - [SPARK-4434] Cluster mode jar URLs are broken
>>>> - [SPARK-4480][SPARK-4467] Too many open files exception from shuffle
>>> spills
>>>> 
>>>> The tag to be voted on is v1.1.1-rc2 (commit 3693ae5d):
>>>> http://s.apache.org/p8
>>>> 
>>>> The release files, including signatures, digests, etc can be found at:
>>>> http://people.apache.org/~andrewor14/spark-1.1.1-rc2/
>>>> 
>>>> Release artifacts are signed with the following key:
>>>> https://people.apache.org/keys/committer/andrewor14.asc
>>>> 
>>>> The staging repository for this release can be found at:
>>>> https://repository.apache.org/content/repositories/orgapachespark-1043/
>>>> 
>>>> The documentation corresponding to this release can be found at:
>>>> http://people.apache.org/~andrewor14/spark-1.1.1-rc2-docs/
>>>> 
>>>> Please vote on releasing this package as Apache Spark 1.1.1!
>>>> 
>>>> The vote is open until Saturday, November 22, at 23:00 UTC and passes if
>>>> a majority of at least 3 +1 PMC votes are cast.
>>>> [ ] +1 Release this package as Apache Spark 1.1.1
>>>> [ ] -1 Do not release this package because ...
>>>> 
>>>> To learn more about Apache Spark, please see
>>>> http://spark.apache.org/
>>>> 
>>>> Cheers,
>>>> Andrew
>>>> 
>>>> 
>>>> ---------------------------------------------------------------------
>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>> 
>>> 
>>> 
>>> --
>>> Marcelo
>>> 
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>> 
>>> 
>> 
>> 
>> --
>> Yee Yang Li Hector  *google.com/+HectorYee  
> 
> 
> 
> -- 
> Yee Yang Li Hector  *google.com/+HectorYee *


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Ah, I see. But the spark.shuffle.blockTransferService property doesn't exist in 1.1 (AFAIK) -- what exactly are you doing to get this problem?

Matei

> On Nov 20, 2014, at 11:50 AM, Hector Yee  wrote:
> 
> This is whatever was in http://people.apache.org/~andrewor14/spark-1.1.1-rc2/  
> On Thu, Nov 20, 2014 at 11:48 AM, Matei Zaharia <matei.zaharia@gmail.com  wrote:
> Hector, is this a comment on 1.1.1 or on the 1.2 preview?
> 
> Matei
> 
> > On Nov 20, 2014, at 11:39 AM, Hector Yee <hector.yee@gmail.com  wrote:
> >
> > I think it is a race condition caused by netty deactivating a channel while
> > it is active.
> > Switched to nio and it works fine
> > --conf spark.shuffle.blockTransferService=nio
> >
> > On Thu, Nov 20, 2014 at 10:44 AM, Hector Yee <hector.yee@gmail.com  wrote:
> >
> >> I'm still seeing the fetch failed error and updated
> >> https://issues.apache.org/jira/browse/SPARK-3633  >>
> >> On Thu, Nov 20, 2014 at 10:21 AM, Marcelo Vanzin <vanzin@cloudera.com  >> wrote:
> >>
> >>> +1 (non-binding)
> >>>
> >>> . ran simple things on spark-shell
> >>> . ran jobs in yarn client & cluster modes, and standalone cluster mode
> >>>
> >>> On Wed, Nov 19, 2014 at 2:51 PM, Andrew Or <andrew@databricks.com  wrote:
> >>>> Please vote on releasing the following candidate as Apache Spark version
> >>>> 1.1.1.
> >>>>
> >>>> This release fixes a number of bugs in Spark 1.1.0. Some of the notable
> >>> ones
> >>>> are
> >>>> - [SPARK-3426] Sort-based shuffle compression settings are incompatible
> >>>> - [SPARK-3948] Stream corruption issues in sort-based shuffle
> >>>> - [SPARK-4107] Incorrect handling of Channel.read() led to data
> >>> truncation
> >>>> The full list is at http://s.apache.org/z9h  and in the CHANGES.txt
> >>> attached.
> >>>>
> >>>> Additionally, this candidate fixes two blockers from the previous RC:
> >>>> - [SPARK-4434] Cluster mode jar URLs are broken
> >>>> - [SPARK-4480][SPARK-4467] Too many open files exception from shuffle
> >>> spills
> >>>>
> >>>> The tag to be voted on is v1.1.1-rc2 (commit 3693ae5d):
> >>>> http://s.apache.org/p8  >>>>
> >>>> The release files, including signatures, digests, etc can be found at:
> >>>> http://people.apache.org/~andrewor14/spark-1.1.1-rc2/  >>>>
> >>>> Release artifacts are signed with the following key:
> >>>> https://people.apache.org/keys/committer/andrewor14.asc  >>>>
> >>>> The staging repository for this release can be found at:
> >>>> https://repository.apache.org/content/repositories/orgapachespark-1043/  >>>>
> >>>> The documentation corresponding to this release can be found at:
> >>>> http://people.apache.org/~andrewor14/spark-1.1.1-rc2-docs/  >>>>
> >>>> Please vote on releasing this package as Apache Spark 1.1.1!
> >>>>
> >>>> The vote is open until Saturday, November 22, at 23:00 UTC and passes if
> >>>> a majority of at least 3 +1 PMC votes are cast.
> >>>> [ ] +1 Release this package as Apache Spark 1.1.1
> >>>> [ ] -1 Do not release this package because ...
> >>>>
> >>>> To learn more about Apache Spark, please see
> >>>> http://spark.apache.org/  >>>>
> >>>> Cheers,
> >>>> Andrew
> >>>>
> >>>>
> >>>> ---------------------------------------------------------------------
> >>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org  >>>> For additional commands, e-mail: dev-help@spark.apache.org  >>>
> >>>
> >>>
> >>> --
> >>> Marcelo
> >>>
> >>> ---------------------------------------------------------------------
> >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org  >>> For additional commands, e-mail: dev-help@spark.apache.org  >>>
> >>>
> >>
> >>
> >> --
> >> Yee Yang Li Hector <http://google.com/+HectorYee  >> *google.com/+HectorYee  <http://google.com/+HectorYee  >>
> >
> >
> >
> > --
> > Yee Yang Li Hector <http://google.com/+HectorYee  > *google.com/+HectorYee  <http://google.com/+HectorYee  
> 
> 
> 
> -- 
> Yee Yang Li Hector  google.com/+HectorYee 


Interesting, perhaps we could publish each one with two IDs, of which the rc one is unofficial. The problem is indeed that you have to vote on a hash for a potentially final artifact.

Matei

> On Nov 23, 2014, at 7:54 PM, Stephen Haberman  wrote:
> 
> Hi,
> 
> I wanted to try 1.1.1-rc2 because we're running into SPARK-3633, but
> the"rc" releases not being tagged with "-rcX" means the pre-built artifacts
> are basically useless to me.
> 
> (Pedantically, to test a release, I have to upload it into our internal
> repo, to compile jobs, start clusters, etc. Invariably when an rcX artifact
> ends up not being final, then I'm screwed, because I would have to clear
> the local cache of any of our machines, dev/Jenkins/etc., that ever
> downloaded the "formerly known as 1.1.1 but not really" rc artifacts.)
> 
> What's frustrating is that I know other Apache projects do rc releases, and
> even get them into Maven central, e.g.:
> 
> http://search.maven.org/#search%7Cgav%7C1%7Cg%3A%22org.apache.tapestry%22%20AND%20a%3A%22tapestry-ioc%22
> 
> So, I apologize for the distraction from getting real work done, but
> perhaps you guys could find a creative way to work around the
> well-intentioned mandate on artifact voting?
> 
> (E.g. perhaps have multiple votes, one for each successive rc (with -rcX
> suffix), then, once blessed, another one on the actually-final/no-rcX
> artifact (built from the last rc's tag); or publish no-rcX artifacts for
> official voting, as today, but then, at the same time, add -rcX artifacts
> to Maven central for non-binding/3rd party testing, etc.)
> 
> Thanks,
> Stephen


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Hey Patrick, unfortunately you got some of the text here wrong, saying 1.1.0 instead of 1.2.0. Not sure it will matter since there can well be another RC after testing, but we should be careful.

Matei

> On Nov 28, 2014, at 9:16 PM, Patrick Wendell  wrote:
> 
> Please vote on releasing the following candidate as Apache Spark version 1.2.0!
> 
> The tag to be voted on is v1.2.0-rc1 (commit 1056e9ec1):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=1056e9ec13203d0c51564265e94d77a054498fdb
> 
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.2.0-rc1/
> 
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
> 
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1048/
> 
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.2.0-rc1-docs/
> 
> Please vote on releasing this package as Apache Spark 1.2.0!
> 
> The vote is open until Tuesday, December 02, at 05:15 UTC and passes
> if a majority of at least 3 +1 PMC votes are cast.
> 
> [ ] +1 Release this package as Apache Spark 1.1.0
> [ ] -1 Do not release this package because ...
> 
> To learn more about Apache Spark, please see
> http://spark.apache.org/
> 
> == What justifies a -1 vote for this release? ==
> This vote is happening very late into the QA period compared with
> previous votes, so -1 votes should only occur for significant
> regressions from 1.0.2. Bugs already present in 1.1.X, minor
> regressions, or bugs related to new features will not block this
> release.
> 
> == What default changes should I be aware of? ==
> 1. The default value of "spark.shuffle.blockTransferService" has been
> changed to "netty"
> --> Old behavior can be restored by switching to "nio"
> 
> 2. The default value of "spark.shuffle.manager" has been changed to "sort".
> --> Old behavior can be restored by setting "spark.shuffle.manager" to "hash".
> 
> == Other notes ==
> Because this vote is occurring over a weekend, I will likely extend
> the vote if this RC survives until the end of the vote period.
> 
> - Patrick
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Hi Ryan,

As a tip (and maybe this isn't documented well), I normally use SBT for development to avoid the slow build process, and use its interactive console to run only specific tests. The nice advantage is that SBT can keep the Scala compiler loaded and JITed across builds, making it faster to iterate. To use it, you can do the following:

- Start the SBT interactive console with sbt/sbt
- Build your assembly by running the "assembly" target in the assembly project: assembly/assembly
- Run all the tests in one module: core/test
- Run a specific suite: core/test-only org.apache.spark.rdd.RDDSuite (this also supports tab completion)

Running all the tests does take a while, and I usually just rely on Jenkins for that once I've run the tests for the things I believed my patch could break. But this is because some of them are integration tests (e.g. DistributedSuite, which creates multi-process mini-clusters). Many of the individual suites run fast without requiring this, however, so you can pick the ones you want. Perhaps we should find a way to tag them so people  can do a "quick-test" that skips the integration ones.

The assembly builds are annoying but they only take about a minute for me on a MacBook Pro with SBT warmed up. The assembly is actually only required for some of the "integration" tests (which launch new processes), but I'd recommend doing it all the time anyway since it would be very confusing to run those with an old assembly. The Scala compiler crash issue can also be a problem, but I don't see it very often with SBT. If it happens, I exit SBT and do sbt clean.

Anyway, this is useful feedback and I think we should try to improve some of these suites, but hopefully you can also try the faster SBT process. At the end of the day, if we want integration tests, the whole test process will take an hour, but most of the developers I know leave that to Jenkins and only run individual tests locally before submitting a patch.

Matei


> On Nov 30, 2014, at 2:39 PM, Ryan Williams  wrote:
> 
> In the course of trying to make contributions to Spark, I have had a lot of
> trouble running Spark's tests successfully. The main pain points I've
> experienced are:
> 
>    1) frequent, spurious test failures
>    2) high latency of running tests
>    3) difficulty running specific tests in an iterative fashion
> 
> Here is an example series of failures that I encountered this weekend
> (along with footnote links to the console output from each and
> approximately how long each took):
> 
> - `./dev/run-tests` [1]: failure in BroadcastSuite that I've not seen
> before.
> - `mvn '-Dsuites=*BroadcastSuite*' test` [2]: same failure.
> - `mvn '-Dsuites=*BroadcastSuite* Unpersisting' test` [3]: BroadcastSuite
> passed, but scala compiler crashed on the "catalyst" project.
> - `mvn clean`: some attempts to run earlier commands (that previously
> didn't crash the compiler) all result in the same compiler crash. Previous
> discussion on this list implies this can only be solved by a `mvn clean`
> [4].
> - `mvn '-Dsuites=*BroadcastSuite*' test` [5]: immediately post-clean,
> BroadcastSuite can't run because assembly is not built.
> - `./dev/run-tests` again [6]: pyspark tests fail, some messages about
> version mismatches and python 2.6. The machine this ran on has python 2.7,
> so I don't know what that's about.
> - `./dev/run-tests` again [7]: "too many open files" errors in several
> tests. `ulimit -a` shows a maximum of 4864 open files. Apparently this is
> not enough, but only some of the time? I increased it to 8192 and tried
> again.
> - `./dev/run-tests` again [8]: same pyspark errors as before. This seems to
> be the issue from SPARK-3867 [9], which was supposedly fixed on October 14;
> not sure how I'm seeing it now. In any case, switched to Python 2.6 and
> installed unittest2, and python/run-tests seems to be unblocked.
> - `./dev/run-tests` again [10]: finally passes!
> 
> This was on a spark checkout at ceb6281 (ToT Friday), with a few trivial
> changes added on (that I wanted to test before sending out a PR), on a
> macbook running OSX Yosemite (10.10.1), java 1.8 and mvn 3.2.3 [11].
> 
> Meanwhile, on a linux 2.6.32 / CentOS 6.4 machine, I tried similar commands
> from the same repo state:
> 
> - `./dev/run-tests` [12]: YarnClusterSuite failure.
> - `./dev/run-tests` [13]: same YarnClusterSuite failure. I know I've seen
> this one before on this machine and am guessing it actually occurs every
> time.
> - `./dev/run-tests` [14]: to be sure, I reverted my changes, ran one more
> time from ceb6281, and saw the same failure.
> 
> This was with java 1.7 and maven 3.2.3 [15]. In one final attempt to narrow
> down the linux YarnClusterSuite failure, I ran `./dev/run-tests` on my mac,
> from ceb6281, with java 1.7 (instead of 1.8, which the previous runs used),
> and it passed [16], so the failure seems specific to my linux machine/arch.
> 
> At this point I believe that my changes don't break any tests (the
> YarnClusterSuite failure on my linux presumably not being... "real"), and I
> am ready to send out a PR. Whew!
> 
> However, reflecting on the 5 or 6 distinct failure-modes represented above:
> 
> - One of them (too many files open), is something I can (and did,
> hopefully) fix once and for all. It cost me an ~hour this time (approximate
> time of running ./dev/run-tests) and a few hours other times when I didn't
> fully understand/fix it. It doesn't happen deterministically (why?), but
> does happen somewhat frequently to people, having been discussed on the
> user list multiple times [17] and on SO [18]. Maybe some note in the
> documentation advising people to check their ulimit makes sense?
> - One of them (unittest2 must be installed for python 2.6) was supposedly
> fixed upstream of the commits I tested here; I don't know why I'm still
> running into it. This cost me a few hours of running `./dev/run-tests`
> multiple times to see if it was transient, plus some time researching and
> working around it.
> - The original BroadcastSuite failure cost me a few hours and went away
> before I'd even run `mvn clean`.
> - A new incarnation of the sbt-compiler-crash phenomenon cost me a few
> hours of running `./dev/run-tests` in different ways before deciding that,
> as usual, there was no way around it and that I'd need to run `mvn clean`
> and start running tests from scratch.
> - The YarnClusterSuite failures on my linux box have cost me hours of
> trying to figure out whether they're my fault. I've seen them many times
> over the past weeks/months, plus or minus other failures that have come and
> gone, and was especially befuddled by them when I was seeing a disjoint set
> of reproducible failures on my mac [19] (the triaging of which involved
> dozens of runs of `./dev/run-tests`).
> 
> While I'm interested in digging into each of these issues, I also want to
> discuss the frequency with which I've run into issues like these. This is
> unfortunately not the first time in recent months that I've spent days
> playing spurious-test-failure whack-a-mole with a 60-90min dev/run-tests
> iteration time, which is no fun! So I am wondering/thinking:
> 
> - Do other people experience this level of flakiness from spark tests?
> - Do other people bother running dev/run-tests locally, or just let Jenkins
> do it during the CR process?
> - Needing to run a full assembly post-clean just to continue running one
> specific test case feels especially wasteful, and the failure output when
> naively attempting to run a specific test without having built an assembly
> jar is not always clear about what the issue is or how to fix it; even the
> fact that certain tests require "building the world" is not something I
> would have expected, and has cost me hours of confusion.
>    - Should a person running spark tests assume that they must build an
> assembly JAR before running anything?
>    - Are there some proper "unit" tests that are actually self-contained /
> able to be run without building an assembly jar?
>    - Can we better document/demarcate which tests have which dependencies?
>    - Is there something finer-grained than building an assembly JAR that
> is sufficient in some cases?
>        - If so, can we document that?
>        - If not, can we move to a world of finer-grained dependencies for
> some of these?
> - Leaving all of these spurious failures aside, the process of assembling
> and testing a new JAR is not a quick one (40 and 60 mins for me typically,
> respectively). I would guess that there are dozens (hundreds?) of people
> who build a Spark assembly from various ToTs on any given day, and who all
> wait on the exact same compilation / assembly steps to occur. Expanding on
> the recent work to publish nightly snapshots [20], can we do a better job
> caching/sharing compilation artifacts at a more granular level (pre-built
> assembly JARs at each SHA? pre-built JARs per-maven-module, per-SHA? more
> granular maven modules, plus the previous two?), or otherwise save some of
> the considerable amount of redundant compilation work that I had to do over
> the course of my odyssey this weekend?
> 
> Ramping up on most projects involves some amount of supplementing the
> documentation with trial and error to figure out what to run, which
> "errors" are real errors and which can be ignored, etc., but navigating
> that minefield on Spark has proved especially challenging and
> time-consuming for me. Some of that comes directly from scala's relatively
> slow compilation times and immature build-tooling ecosystem, but that is
> the world we live in and it would be nice if Spark took the alleviation of
> the resulting pain more seriously, as one of the more interesting and
> well-known large scala projects around right now. The official
> documentation around how to build different subsets of the codebase is
> somewhat sparse [21], and there have been many mixed [22] accounts [23] on
> this mailing list about preferred ways to build on mvn vs. sbt (none of
> which has made it into official documentation, as far as I've seen).
> Expecting new contributors to piece together all of this received
> folk-wisdom about how to build/test in a sane way by trawling mailing list
> archives seems suboptimal.
> 
> Thanks for reading, looking forward to hearing your ideas!
> 
> -Ryan
> 
> P.S. Is "best practice" for emailing this list to not incorporate any HTML
> in the body? It seems like all of the archives I've seen strip it out, but
> other people have used it and gmail displays it.
> 
> 
> [1]
> https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw/484c2fb8bc0efa0e39d142087eefa9c3d5292ea3/dev%20run-tests:%20fail
> (57 mins)
> [2]
> https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw/ce264e469be3641f061eabd10beb1d71ac243991/mvn%20test:%20fail
> (6 mins)
> [3]
> https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw/6bc76c67aeef9c57ddd9fb2ba260fb4189dbb927/mvn%20test%20case:%20pass%20test,%20fail%20subsequent%20compile
> (4 mins)
> [4]
> https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=0CCUQFjAB&url=http%3A%2F%2Fapache-spark-user-list.1001560.n3.nabble.com%2Fscalac-crash-when-compiling-DataTypeConversions-scala-td17083.html&ei=aRF6VJrpNKr-iAKDgYGYBQ&usg=AFQjCNHjM9m__Hrumh-ecOsSE00-JkjKBQ&sig2=zDeSqOgs02AXJXj78w5I9g&bvm=bv.80642063,d.cGE&cad=rja
> [5]
> https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw/4ab0bd6e76d9fc5745eb4b45cdf13195d10efaa2/mvn%20test,%20post%20clean,%20need%20dependencies%20built
> [6]
> https://gist.githubusercontent.com/ryan-williams/8a162367c4dc157d2479/raw/f4c7e6fc8c301f869b00598c7b541dac243fb51e/dev%20run-tests,%20post%20clean
> (50 mins)
> [7]
> https://gist.github.com/ryan-williams/57f8bfc9328447fc5b97#file-dev-run-tests-failure-too-many-files-open-then-hang-L5260
> (1hr)
> [8] https://gist.github.com/ryan-williams/d0164194ad5de03f6e3f (1hr)
> [9] https://issues.apache.org/jira/browse/SPARK-3867
> [10] https://gist.github.com/ryan-williams/735adf543124c99647cc
> [11] https://gist.github.com/ryan-williams/8d149bbcd0c6689ad564
> [12]
> https://gist.github.com/ryan-williams/07df5c583c9481fe1c14#file-gistfile1-txt-L853
> (~90 mins)
> [13]
> https://gist.github.com/ryan-williams/718f6324af358819b496#file-gistfile1-txt-L852
> (91 mins)
> [14]
> https://gist.github.com/ryan-williams/c06c1f4aa0b16f160965#file-gistfile1-txt-L854
> [15] https://gist.github.com/ryan-williams/f8d410b5b9f082039c73
> [16] https://gist.github.com/ryan-williams/2e94f55c9287938cf745
> [17]
> http://apache-spark-user-list.1001560.n3.nabble.com/quot-Too-many-open-files-quot-exception-on-reduceByKey-td2462.html
> [18]
> http://stackoverflow.com/questions/25707629/why-does-spark-job-fail-with-too-many-open-files
> [19] https://issues.apache.org/jira/browse/SPARK-4002
> [20] https://issues.apache.org/jira/browse/SPARK-4542
> [21]
> https://spark.apache.org/docs/latest/building-with-maven.html#spark-tests-in-maven
> [22] https://www.mail-archive.com/dev@spark.apache.org/msg06443.html
> [23]
> http://mail-archives.apache.org/mod_mbox/spark-dev/201410.mbox/%3CCAOhmDzeUNhuCr41B7KRPTEwMn4cga_2TNpZrWqQB8REekokxzg@mail.gmail.com%3E


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


An update on this: After adding the initial maintainer list, we got feedback to add more maintainers for some components, so we added four others (Josh Rosen for core API, Mark Hamstra for scheduler, Shivaram Venkataraman for MLlib and Xiangrui Meng for Python). We also decided to lower the "timeout" for waiting for a maintainer to a week. Hopefully this will provide more options for reviewing in these components.

The complete list is available at https://cwiki.apache.org/confluence/display/SPARK/Committers.

Matei

> On Nov 8, 2014, at 7:28 PM, Matei Zaharia  wrote:
> 
> Thanks everyone for voting on this. With all of the PMC votes being for, the vote passes, but there were some concerns that I wanted to address for everyone who brought them up, as well as in the wording we will use for this policy.
> 
> First, like every Apache project, Spark follows the Apache voting process (http://www.apache.org/foundation/voting.html), wherein all code changes are done by consensus. This means that any PMC member can block a code change on technical grounds, and thus that there is consensus when something goes in. It's absolutely true that every PMC member is responsible for the whole codebase, as Greg said (not least due to legal reasons, e.g. making sure it complies to licensing rules), and this idea will not change that. To make this clear, I will include that in the wording on the project page, to make sure new committers and other community members are all aware of it.
> 
> What the maintainer model does, instead, is to change the review process, by having a required review from some people on some types of code changes (assuming those people respond in time). Projects can have their own diverse review processes (e.g. some do commit-then-review and others do review-then-commit, some point people to specific reviewers, etc). This kind of process seems useful to try (and to refine) as the project grows. We will of course evaluate how it goes and respond to any problems.
> 
> So to summarize,
> 
> - Every committer is responsible for, and more than welcome to review and vote on, every code change. In fact all community members are welcome to do this, and lots are doing it.
> - Everyone has the same voting rights on these code changes (namely consensus as described at http://www.apache.org/foundation/voting.html)
> - Committers will be asked to run patches that are making architectural and API changes by the maintainers before merging.
> 
> In practice, none of this matters too much because we are not exactly a hot-well of discord ;), and even in the case of discord, the point of the ASF voting process is to create consensus. The goal is just to have a better structure for reviewing and minimize the chance of errors.
> 
> Here is a tally of the votes:
> 
> Binding votes (from PMC): 17 +1, no 0 or -1
> 
> Matei Zaharia
> Michael Armbrust
> Reynold Xin
> Patrick Wendell
> Andrew Or
> Prashant Sharma
> Mark Hamstra
> Xiangrui Meng
> Ankur Dave
> Imran Rashid
> Jason Dai
> Tom Graves
> Sean McNamara
> Nick Pentreath
> Josh Rosen
> Kay Ousterhout
> Tathagata Das
> 
> Non-binding votes: 18 +1, one +0, one -1
> 
> +1:
> Nan Zhu
> Nicholas Chammas
> Denny Lee
> Cheng Lian
> Timothy Chen
> Jeremy Freeman
> Cheng Hao
> Jackylk Likun
> Kousuke Saruta
> Reza Zadeh
> Xuefeng Wu
> Witgo
> Manoj Babu
> Ravindra Pesala
> Liquan Pei
> Kushal Datta
> Davies Liu
> Vaquar Khan
> 
> +0: Corey Nolet
> 
> -1: Greg Stein
> 
> I'll send another email when I have a more detailed writeup of this on the website.
> 
> Matei


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


+1

Tested on Mac OS X.

Matei

> On Dec 10, 2014, at 1:08 PM, Patrick Wendell  wrote:
> 
> Please vote on releasing the following candidate as Apache Spark version 1.2.0!
> 
> The tag to be voted on is v1.2.0-rc2 (commit a428c446e2):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=a428c446e23e628b746e0626cc02b7b3cadf588e
> 
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.2.0-rc2/
> 
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
> 
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1055/
> 
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.2.0-rc2-docs/
> 
> Please vote on releasing this package as Apache Spark 1.2.0!
> 
> The vote is open until Saturday, December 13, at 21:00 UTC and passes
> if a majority of at least 3 +1 PMC votes are cast.
> 
> [ ] +1 Release this package as Apache Spark 1.2.0
> [ ] -1 Do not release this package because ...
> 
> To learn more about Apache Spark, please see
> http://spark.apache.org/
> 
> == What justifies a -1 vote for this release? ==
> This vote is happening relatively late into the QA period, so
> -1 votes should only occur for significant regressions from
> 1.0.2. Bugs already present in 1.1.X, minor
> regressions, or bugs related to new features will not block this
> release.
> 
> == What default changes should I be aware of? ==
> 1. The default value of "spark.shuffle.blockTransferService" has been
> changed to "netty"
> --> Old behavior can be restored by switching to "nio"
> 
> 2. The default value of "spark.shuffle.manager" has been changed to "sort".
> --> Old behavior can be restored by setting "spark.shuffle.manager" to "hash".
> 
> == How does this differ from RC1 ==
> This has fixes for a handful of issues identified - some of the
> notable fixes are:
> 
> [Core]
> SPARK-4498: Standalone Master can fail to recognize completed/failed
> applications
> 
> [SQL]
> SPARK-4552: Query for empty parquet table in spark sql hive get
> IllegalArgumentException
> SPARK-4753: Parquet2 does not prune based on OR filters on partition columns
> SPARK-4761: With JDBC server, set Kryo as default serializer and
> disable reference tracking
> SPARK-4785: When called with arguments referring column fields, PMOD throws NPE
> 
> - Patrick
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


It's just Bootstrap checked into SVN and built using Jekyll. You can check out the raw source files from SVN from https://svn.apache.org/repos/asf/spark. IMO it's fine if you guys use the layout, but just make sure it doesn't look exactly the same because otherwise both sites will look like they're copied from some standard template. We used a slightly customized Bootstrap theme for it.

Matei

> On Dec 15, 2014, at 8:26 AM, Pei Chen  wrote:
> 
> Hi Spark Dev,
> The cTAKES community was looking at revamping their web site and really
> liked the clean look of the Spark one.  Is it just using Bootstrap and
> static html pages checked into SVN?  Or are you using the Apache CMS
> somehow?  Mind if we borrow the layout?
> 
> --Pei


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Please ask someone else to assign them for now, and just comment on them that you're working on them. Over time if you contribute a bunch we'll add you to that list. The problem is that in the past, people would assign issues to themselves and never actually work on them, making it confusing for others.

Matei

> On Dec 29, 2014, at 7:59 AM, Jakub Dubovsky  wrote:
> 
> Hi devs,
> 
>   I'd like to ask what are the procedures/conditions for being assigned a 
> role of a developer on spark jira? My motivation is to be able to assign 
> issues to myself. Only related resource I have found is jira permission 
> scheme [1].
> 
>   regards
>   Jakub
> 
>  [1] https://cwiki.apache.org/confluence/display/SPARK/Jira+Permissions+
> Scheme


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


FYI, ApacheCon North America call for papers is up.

Matei

> Begin forwarded message:
> 
> Date: January 5, 2015 at 9:40:41 AM PST
> From: Rich Bowen  Reply-To: dev  To: dev  Subject: ApacheCon North America 2015 Call For Papers
> 
> Fellow ASF enthusiasts,
> 
> We now have less than a month remaining in the Call For Papers for ApacheCon North America 2015, and so far the submissions are on the paltry side. Please consider submitting papers for consideration for this event.
> 
> Details about the event are available at http://events.linuxfoundation.org/events/apachecon-north-america
> 
> The call for papers is at http://events.linuxfoundation.org//events/apachecon-north-america/program/cfp
> 
> Please help us out by getting this message out to your user@ and dev@ community on the projects that you're involved in, so that these projects can be represented in Austin.
> 
> If you are interested in chairing a content track, and taking on the task of wrangling your community together to create a compelling story about your technology space, please join the comdev mailing list - dev-subscribe@community.apache.org - and speak up there.
> 
> (Message is Bcc'ed committers@, and Reply-to set to dev@community, if you want to discuss this topic further there.)
> 
> Thanks!
> 
> -- 
> Rich Bowen - rbowen@rcbowen.com - @rbowen
> http://apachecon.com/ - @apachecon


+1 on this.

> On Jan 17, 2015, at 6:16 PM, Reza Zadeh  wrote:
> 
> LGTM
> 
> On Sat, Jan 17, 2015 at 5:40 PM, Patrick Wendell  wrote:
> 
>> Hey All,
>> 
>> Just wanted to ping about a minor issue - but one that ends up having
>> consequence given Spark's volume of reviews and commits. As much as
>> possible, I think that we should try and gear towards "Google Style"
>> LGTM on reviews. What I mean by this is that LGTM has the following
>> semantics:
>> 
>> "I know this code well, or I've looked at it close enough to feel
>> confident it should be merged. If there are issues/bugs with this code
>> later on, I feel confident I can help with them."
>> 
>> Here is an alternative semantic:
>> 
>> "Based on what I know about this part of the code, I don't see any
>> show-stopper problems with this patch".
>> 
>> The issue with the latter is that it ultimately erodes the
>> significance of LGTM, since subsequent reviewers need to reason about
>> what the person meant by saying LGTM. In contrast, having strong
>> semantics around LGTM can help streamline reviews a lot, especially as
>> reviewers get more experienced and gain trust from the comittership.
>> 
>> There are several easy ways to give a more limited endorsement of a patch:
>> - "I'm not familiar with this code, but style, etc look good" (general
>> endorsement)
>> - "The build changes in this code LGTM, but I haven't reviewed the
>> rest" (limited LGTM)
>> 
>> If people are okay with this, I might add a short note on the wiki.
>> I'm sending this e-mail first, though, to see whether anyone wants to
>> express agreement or disagreement with this approach.
>> 
>> - Patrick
>> 
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>> 
>> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


It's hard to tell without more details, but the start-up latency in Hive can sometimes be high, especially if you are running Hive on MapReduce. MR just takes 20-30 seconds per job to spin up even if the job is doing nothing.

For real use of Spark SQL for short queries by the way, I'd recommend using the JDBC server so that you can have a long-running Spark process. It gets quite a bit faster after the first few queries.

Matei

> On Jan 22, 2015, at 10:22 PM, Saumitra Shahapure (Vizury)  wrote:
> 
> Hello,
> 
> We were comparing performance of some of our production hive queries
> between Hive and Spark. We compared Hive(0.13)+hadoop (1.2.1) against both
> Spark 0.9 and 1.1. We could see that the performance gains have been good
> in Spark.
> 
> We tried a very simple query,
> select count(*) from T where col3=123
> in both sparkSQL and Hive (with hive.map.aggr=true) and found that Spark
> performance had been 2x better than Hive (120sec vs 60sec). Table T is
> stored in S3 and contains 600MB single GZIP file.
> 
> My question is, why Spark is faster than Hive here? In both of the cases,
> the file will be downloaded, uncompressed and lines will be counted by a
> single process. For Hive case, reducer will be identity function
> since hive.map.aggr is true.
> 
> Note that disk spills and network I/O are very less for Hive's case as well,
> --
> Regards,
> Saumitra Shahapure


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


(Actually when we designed Spark SQL we thought of giving it another name, like Spark Schema, but we decided to stick with SQL since that was the most obvious use case to many users.)

Matei

> On Jan 26, 2015, at 5:31 PM, Matei Zaharia  wrote:
> 
> While it might be possible to move this concept to Spark Core long-term, supporting structured data efficiently does require quite a bit of the infrastructure in Spark SQL, such as query planning and columnar storage. The intent of Spark SQL though is to be more than a SQL server -- it's meant to be a library for manipulating structured data. Since this is possible to build over the core API, it's pretty natural to organize it that way, same as Spark Streaming is a library.
> 
> Matei
> 
>> On Jan 26, 2015, at 4:26 PM, Koert Kuipers  wrote:
>> 
>> "The context is that SchemaRDD is becoming a common data format used for
>> bringing data into Spark from external systems, and used for various
>> components of Spark, e.g. MLlib's new pipeline API."
>> 
>> i agree. this to me also implies it belongs in spark core, not sql
>> 
>> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak  michaelmalak@yahoo.com.invalid> wrote:
>> 
>>> And in the off chance that anyone hasn't seen it yet, the Jan. 13 Bay Area
>>> Spark Meetup YouTube contained a wealth of background information on this
>>> idea (mostly from Patrick and Reynold :-).
>>> 
>>> https://www.youtube.com/watch?v=YWppYPWznSQ
>>> 
>>> ________________________________
>>> From: Patrick Wendell  To: Reynold Xin  Cc: "dev@spark.apache.org"  Sent: Monday, January 26, 2015 4:01 PM
>>> Subject: Re: renaming SchemaRDD -> DataFrame
>>> 
>>> 
>>> One thing potentially not clear from this e-mail, there will be a 1:1
>>> correspondence where you can get an RDD to/from a DataFrame.
>>> 
>>> 
>>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin  wrote:
>>>> Hi,
>>>> 
>>>> We are considering renaming SchemaRDD -> DataFrame in 1.3, and wanted to
>>>> get the community's opinion.
>>>> 
>>>> The context is that SchemaRDD is becoming a common data format used for
>>>> bringing data into Spark from external systems, and used for various
>>>> components of Spark, e.g. MLlib's new pipeline API. We also expect more
>>> and
>>>> more users to be programming directly against SchemaRDD API rather than
>>> the
>>>> core RDD API. SchemaRDD, through its less commonly used DSL originally
>>>> designed for writing test cases, always has the data-frame like API. In
>>>> 1.3, we are redesigning the API to make the API usable for end users.
>>>> 
>>>> 
>>>> There are two motivations for the renaming:
>>>> 
>>>> 1. DataFrame seems to be a more self-evident name than SchemaRDD.
>>>> 
>>>> 2. SchemaRDD/DataFrame is actually not going to be an RDD anymore (even
>>>> though it would contain some RDD functions like map, flatMap, etc), and
>>>> calling it Schema*RDD* while it is not an RDD is highly confusing.
>>> Instead.
>>>> DataFrame.rdd will return the underlying RDD for all RDD methods.
>>>> 
>>>> 
>>>> My understanding is that very few users program directly against the
>>>> SchemaRDD API at the moment, because they are not well documented.
>>> However,
>>>> oo maintain backward compatibility, we can create a type alias DataFrame
>>>> that is still named SchemaRDD. This will maintain source compatibility
>>> for
>>>> Scala. That said, we will have to update all existing materials to use
>>>> DataFrame rather than SchemaRDD.
>>> 
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>> 
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>> 
>>> 
> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


While it might be possible to move this concept to Spark Core long-term, supporting structured data efficiently does require quite a bit of the infrastructure in Spark SQL, such as query planning and columnar storage. The intent of Spark SQL though is to be more than a SQL server -- it's meant to be a library for manipulating structured data. Since this is possible to build over the core API, it's pretty natural to organize it that way, same as Spark Streaming is a library.

Matei

> On Jan 26, 2015, at 4:26 PM, Koert Kuipers  wrote:
> 
> "The context is that SchemaRDD is becoming a common data format used for
> bringing data into Spark from external systems, and used for various
> components of Spark, e.g. MLlib's new pipeline API."
> 
> i agree. this to me also implies it belongs in spark core, not sql
> 
> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak  michaelmalak@yahoo.com.invalid> wrote:
> 
>> And in the off chance that anyone hasn't seen it yet, the Jan. 13 Bay Area
>> Spark Meetup YouTube contained a wealth of background information on this
>> idea (mostly from Patrick and Reynold :-).
>> 
>> https://www.youtube.com/watch?v=YWppYPWznSQ
>> 
>> ________________________________
>> From: Patrick Wendell  To: Reynold Xin  Cc: "dev@spark.apache.org"  Sent: Monday, January 26, 2015 4:01 PM
>> Subject: Re: renaming SchemaRDD -> DataFrame
>> 
>> 
>> One thing potentially not clear from this e-mail, there will be a 1:1
>> correspondence where you can get an RDD to/from a DataFrame.
>> 
>> 
>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin  wrote:
>>> Hi,
>>> 
>>> We are considering renaming SchemaRDD -> DataFrame in 1.3, and wanted to
>>> get the community's opinion.
>>> 
>>> The context is that SchemaRDD is becoming a common data format used for
>>> bringing data into Spark from external systems, and used for various
>>> components of Spark, e.g. MLlib's new pipeline API. We also expect more
>> and
>>> more users to be programming directly against SchemaRDD API rather than
>> the
>>> core RDD API. SchemaRDD, through its less commonly used DSL originally
>>> designed for writing test cases, always has the data-frame like API. In
>>> 1.3, we are redesigning the API to make the API usable for end users.
>>> 
>>> 
>>> There are two motivations for the renaming:
>>> 
>>> 1. DataFrame seems to be a more self-evident name than SchemaRDD.
>>> 
>>> 2. SchemaRDD/DataFrame is actually not going to be an RDD anymore (even
>>> though it would contain some RDD functions like map, flatMap, etc), and
>>> calling it Schema*RDD* while it is not an RDD is highly confusing.
>> Instead.
>>> DataFrame.rdd will return the underlying RDD for all RDD methods.
>>> 
>>> 
>>> My understanding is that very few users program directly against the
>>> SchemaRDD API at the moment, because they are not well documented.
>> However,
>>> oo maintain backward compatibility, we can create a type alias DataFrame
>>> that is still named SchemaRDD. This will maintain source compatibility
>> for
>>> Scala. That said, we will have to update all existing materials to use
>>> DataFrame rather than SchemaRDD.
>> 
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>> 
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>> 
>> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


The type alias means your methods can specify either type and they will work. It's just another name for the same type. But Scaladocs and such will show DataFrame as the type.

Matei

> On Jan 27, 2015, at 12:10 PM, Dirceu Semighini Filho  wrote:
> 
> Reynold,
> But with type alias we will have the same problem, right?
> If the methods doesn't receive schemardd anymore, we will have to change
> our code to migrade from schema to dataframe. Unless we have an implicit
> conversion between DataFrame and SchemaRDD
> 
> 
> 
> 2015-01-27 17:18 GMT-02:00 Reynold Xin  
>> Dirceu,
>> 
>> That is not possible because one cannot overload return types.
>> 
>> SQLContext.parquetFile (and many other methods) needs to return some type,
>> and that type cannot be both SchemaRDD and DataFrame.
>> 
>> In 1.3, we will create a type alias for DataFrame called SchemaRDD to not
>> break source compatibility for Scala.
>> 
>> 
>> On Tue, Jan 27, 2015 at 6:28 AM, Dirceu Semighini Filho  dirceu.semighini@gmail.com> wrote:
>> 
>>> Can't the SchemaRDD remain the same, but deprecated, and be removed in the
>>> release 1.5(+/- 1)  for example, and the new code been added to DataFrame?
>>> With this, we don't impact in existing code for the next few releases.
>>> 
>>> 
>>> 
>>> 2015-01-27 0:02 GMT-02:00 Kushal Datta  
>>>> I want to address the issue that Matei raised about the heavy lifting
>>>> required for a full SQL support. It is amazing that even after 30 years
>>> of
>>>> research there is not a single good open source columnar database like
>>>> Vertica. There is a column store option in MySQL, but it is not nearly
>>> as
>>>> sophisticated as Vertica or MonetDB. But there's a true need for such a
>>>> system. I wonder why so and it's high time to change that.
>>>> On Jan 26, 2015 5:47 PM, "Sandy Ryza"  wrote:
>>>> 
>>>>> Both SchemaRDD and DataFrame sound fine to me, though I like the
>>> former
>>>>> slightly better because it's more descriptive.
>>>>> 
>>>>> Even if SchemaRDD's needs to rely on Spark SQL under the covers, it
>>> would
>>>>> be more clear from a user-facing perspective to at least choose a
>>> package
>>>>> name for it that omits "sql".
>>>>> 
>>>>> I would also be in favor of adding a separate Spark Schema module for
>>>> Spark
>>>>> SQL to rely on, but I imagine that might be too large a change at this
>>>>> point?
>>>>> 
>>>>> -Sandy
>>>>> 
>>>>> On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia  matei.zaharia@gmail.com>
>>>>> wrote:
>>>>> 
>>>>>> (Actually when we designed Spark SQL we thought of giving it another
>>>>> name,
>>>>>> like Spark Schema, but we decided to stick with SQL since that was
>>> the
>>>>> most
>>>>>> obvious use case to many users.)
>>>>>> 
>>>>>> Matei
>>>>>> 
>>>>>>> On Jan 26, 2015, at 5:31 PM, Matei Zaharia  matei.zaharia@gmail.com>
>>>>>> wrote:
>>>>>>> 
>>>>>>> While it might be possible to move this concept to Spark Core
>>>>> long-term,
>>>>>> supporting structured data efficiently does require quite a bit of
>>> the
>>>>>> infrastructure in Spark SQL, such as query planning and columnar
>>>> storage.
>>>>>> The intent of Spark SQL though is to be more than a SQL server --
>>> it's
>>>>>> meant to be a library for manipulating structured data. Since this
>>> is
>>>>>> possible to build over the core API, it's pretty natural to
>>> organize it
>>>>>> that way, same as Spark Streaming is a library.
>>>>>>> 
>>>>>>> Matei
>>>>>>> 
>>>>>>>> On Jan 26, 2015, at 4:26 PM, Koert Kuipers  wrote:
>>>>>>>> 
>>>>>>>> "The context is that SchemaRDD is becoming a common data format
>>> used
>>>>> for
>>>>>>>> bringing data into Spark from external systems, and used for
>>> various
>>>>>>>> components of Spark, e.g. MLlib's new pipeline API."
>>>>>>>> 
>>>>>>>> i agree. this to me also implies it belongs in spark core, not
>>> sql
>>>>>>>> 
>>>>>>>> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak  michaelmalak@yahoo.com.invalid> wrote:
>>>>>>>> 
>>>>>>>>> And in the off chance that anyone hasn't seen it yet, the Jan.
>>> 13
>>>> Bay
>>>>>> Area
>>>>>>>>> Spark Meetup YouTube contained a wealth of background
>>> information
>>>> on
>>>>>> this
>>>>>>>>> idea (mostly from Patrick and Reynold :-).
>>>>>>>>> 
>>>>>>>>> https://www.youtube.com/watch?v=YWppYPWznSQ
>>>>>>>>> 
>>>>>>>>> ________________________________
>>>>>>>>> From: Patrick Wendell  To: Reynold Xin  Cc: "dev@spark.apache.org"  Sent: Monday, January 26, 2015 4:01 PM
>>>>>>>>> Subject: Re: renaming SchemaRDD -> DataFrame
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> One thing potentially not clear from this e-mail, there will be
>>> a
>>>> 1:1
>>>>>>>>> correspondence where you can get an RDD to/from a DataFrame.
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin  rxin@databricks.com>
>>>>>> wrote:
>>>>>>>>>> Hi,
>>>>>>>>>> 
>>>>>>>>>> We are considering renaming SchemaRDD -> DataFrame in 1.3, and
>>>>> wanted
>>>>>> to
>>>>>>>>>> get the community's opinion.
>>>>>>>>>> 
>>>>>>>>>> The context is that SchemaRDD is becoming a common data format
>>>> used
>>>>>> for
>>>>>>>>>> bringing data into Spark from external systems, and used for
>>>> various
>>>>>>>>>> components of Spark, e.g. MLlib's new pipeline API. We also
>>> expect
>>>>>> more
>>>>>>>>> and
>>>>>>>>>> more users to be programming directly against SchemaRDD API
>>> rather
>>>>>> than
>>>>>>>>> the
>>>>>>>>>> core RDD API. SchemaRDD, through its less commonly used DSL
>>>>> originally
>>>>>>>>>> designed for writing test cases, always has the data-frame like
>>>> API.
>>>>>> In
>>>>>>>>>> 1.3, we are redesigning the API to make the API usable for end
>>>>> users.
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> There are two motivations for the renaming:
>>>>>>>>>> 
>>>>>>>>>> 1. DataFrame seems to be a more self-evident name than
>>> SchemaRDD.
>>>>>>>>>> 
>>>>>>>>>> 2. SchemaRDD/DataFrame is actually not going to be an RDD
>>> anymore
>>>>>> (even
>>>>>>>>>> though it would contain some RDD functions like map, flatMap,
>>>> etc),
>>>>>> and
>>>>>>>>>> calling it Schema*RDD* while it is not an RDD is highly
>>> confusing.
>>>>>>>>> Instead.
>>>>>>>>>> DataFrame.rdd will return the underlying RDD for all RDD
>>> methods.
>>>>>>>>>> 
>>>>>>>>>> 
>>>>>>>>>> My understanding is that very few users program directly
>>> against
>>>> the
>>>>>>>>>> SchemaRDD API at the moment, because they are not well
>>> documented.
>>>>>>>>> However,
>>>>>>>>>> oo maintain backward compatibility, we can create a type alias
>>>>>> DataFrame
>>>>>>>>>> that is still named SchemaRDD. This will maintain source
>>>>> compatibility
>>>>>>>>> for
>>>>>>>>>> Scala. That said, we will have to update all existing
>>> materials to
>>>>> use
>>>>>>>>>> DataFrame rather than SchemaRDD.
>>>>>>>>> 
>>>>>>>>> 
>>>> ---------------------------------------------------------------------
>>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>>>>>> 
>>>>>>>>> 
>>>> ---------------------------------------------------------------------
>>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>>>>>> 
>>>>>>>>> 
>>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> 
>>> ---------------------------------------------------------------------
>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>>> 
>>>>>> 
>>>>> 
>>>> 
>>> 
>> 
>> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


This looks like a pretty serious problem, thanks! Glad people are testing on Windows.

Matei

> On Jan 31, 2015, at 11:57 AM, MartinWeindel  wrote:
> 
> FYI: Spark 1.2.1rc2 does not work on Windows!
> 
> On creating a Spark context you get following log output on my Windows
> machine:
> INFO  org.apache.spark.SparkEnv:59 - Registering BlockManagerMaster
> ERROR org.apache.spark.util.Utils:75 - Failed to create local root dir in
> C:\Users\mweindel\AppData\Local\Temp\. Ignoring this directory.
> ERROR org.apache.spark.storage.DiskBlockManager:75 - Failed to create any
> local dir.
> 
> I have already located the cause. A newly added function chmod700() in
> org.apache.util.Utils uses functionality which only works on a Unix file
> system.
> 
> See also pull request [https://github.com/apache/spark/pull/4299] for my
> suggestion how to resolve the issue.
> 
> Best regards,
> 
> Martin Weindel
> 
> 
> 
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-2-1-RC2-tp10317p10370.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Hi all,

The PMC recently voted to add three new committers: Cheng Lian, Joseph Bradley and Sean Owen. All three have been major contributors to Spark in the past year: Cheng on Spark SQL, Joseph on MLlib, and Sean on ML and many pieces throughout Spark Core. Join me in welcoming them as committers!

Matei
---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


+1

Tested on Mac OS X.

Matei


> On Feb 2, 2015, at 8:57 PM, Patrick Wendell  wrote:
> 
> Please vote on releasing the following candidate as Apache Spark version 1.2.1!
> 
> The tag to be voted on is v1.2.1-rc3 (commit b6eaf77):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=b6eaf77d4332bfb0a698849b1f5f917d20d70e97
> 
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.2.1-rc3/
> 
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
> 
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1065/
> 
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.2.1-rc3-docs/
> 
> Changes from rc2:
> A single patch fixing a windows issue.
> 
> Please vote on releasing this package as Apache Spark 1.2.1!
> 
> The vote is open until Friday, February 06, at 05:00 UTC and passes
> if a majority of at least 3 +1 PMC votes are cast.
> 
> [ ] +1 Release this package as Apache Spark 1.2.1
> [ ] -1 Do not release this package because ...
> 
> For a list of fixes in this release, see http://s.apache.org/Mpn.
> 
> To learn more about Apache Spark, please see
> http://spark.apache.org/
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Thanks Denny; added you.

Matei

> On Feb 9, 2015, at 10:11 PM, Denny Lee  wrote:
> 
> Forgot to add Concur to the "Powered by Spark" wiki:
> 
> Concur
> https://www.concur.com
> Spark SQL, MLLib
> Using Spark for travel and expenses analytics and personalization
> 
> Thanks!
> Denny


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


You're not really supposed to subclass DataFrame, instead you can make it from an RDD of Rows and a schema (e.g. with SQLContext.applySchema). Actually the Spark SQL data source API supports that too (org.apache.spark.sql.sources). Think of DataFrame as a container for structured data, not as a class that all data sources will have to implement. If you want to do something fancy like compute the Rows dynamically, your RDD can implement its own compute() method to do that.

Matei

> On Feb 10, 2015, at 11:47 AM, Koert Kuipers  wrote:
> 
> so i understand the success or spark.sql. besides the fact that anything
> with the words SQL in its name will have thousands of developers running
> towards it because of the familiarity, there is also a genuine need for a
> generic RDD that holds record-like objects, with field names and runtime
> types. after all that is a successfull generic abstraction used in many
> structured data tools.
> 
> but to me that abstraction is as simple as:
> 
> trait SchemaRDD extends RDD[Row] {
>  def schema: StructType
> }
> 
> and perhaps another abstraction to indicate it intends to be column
> oriented (with a few methods to efficiently extract a subset of columns).
> so that could be DataFrame.
> 
> such simple contracts would allow many people to write loaders for this
> (say from csv) and whatnot.
> 
> what i do not understand why it has to be much more complex than this. but
> if i look at DataFrame it has so much additional stuff, that has (in my
> eyes) nothing to do with generic structured data analysis.
> 
> for example to implement DataFrame i need to implement about 40 additional
> methods!? and for some the SQLness is obviously leaking into the
> abstraction. for example why would i care about:
>  def registerTempTable(tableName: String): Unit
> 
> 
> best, koert
> 
> On Sun, Feb 1, 2015 at 3:31 AM, Evan Chan  wrote:
> 
>> It is true that you can persist SchemaRdds / DataFrames to disk via
>> Parquet, but a lot of time and inefficiencies is lost.   The in-memory
>> columnar cached representation is completely different from the
>> Parquet file format, and I believe there has to be a translation into
>> a Row (because ultimately Spark SQL traverses Row's -- even the
>> InMemoryColumnarTableScan has to then convert the columns into Rows
>> for row-based processing).   On the other hand, traditional data
>> frames process in a columnar fashion.   Columnar storage is good, but
>> nowhere near as good as columnar processing.
>> 
>> Another issue, which I don't know if it is solved yet, but it is
>> difficult for Tachyon to efficiently cache Parquet files without
>> understanding the file format itself.
>> 
>> I gave a talk at last year's Spark Summit on this topic.
>> 
>> I'm working on efforts to change this, however.  Shoot me an email at
>> velvia at gmail if you're interested in joining forces.
>> 
>> On Thu, Jan 29, 2015 at 1:59 PM, Cheng Lian  wrote:
>>> Yes, when a DataFrame is cached in memory, it's stored in an efficient
>>> columnar format. And you can also easily persist it on disk using
>> Parquet,
>>> which is also columnar.
>>> 
>>> Cheng
>>> 
>>> 
>>> On 1/29/15 1:24 PM, Koert Kuipers wrote:
>>>> 
>>>> to me the word DataFrame does come with certain expectations. one of
>> them
>>>> is that the data is stored columnar. in R data.frame internally uses a
>>>> list
>>>> of sequences i think, but since lists can have labels its more like a
>>>> SortedMap[String, Array[_]]. this makes certain operations very cheap
>>>> (such
>>>> as adding a column).
>>>> 
>>>> in Spark the closest thing would be a data structure where per Partition
>>>> the data is also stored columnar. does spark SQL already use something
>>>> like
>>>> that? Evan mentioned "Spark SQL columnar compression", which sounds like
>>>> it. where can i find that?
>>>> 
>>>> thanks
>>>> 
>>>> On Thu, Jan 29, 2015 at 2:32 PM, Evan Chan  wrote:
>>>> 
>>>>> +1.... having proper NA support is much cleaner than using null, at
>>>>> least the Java null.
>>>>> 
>>>>> On Wed, Jan 28, 2015 at 6:10 PM, Evan R. Sparks  
>>>>> wrote:
>>>>>> 
>>>>>> You've got to be a little bit careful here. "NA" in systems like R or
>>>>> 
>>>>> pandas
>>>>>> 
>>>>>> may have special meaning that is distinct from "null".
>>>>>> 
>>>>>> See, e.g. http://www.r-bloggers.com/r-na-vs-null/
>>>>>> 
>>>>>> 
>>>>>> 
>>>>>> On Wed, Jan 28, 2015 at 4:42 PM, Reynold Xin  
>>>>> wrote:
>>>>>>> 
>>>>>>> Isn't that just "null" in SQL?
>>>>>>> 
>>>>>>> On Wed, Jan 28, 2015 at 4:41 PM, Evan Chan  wrote:
>>>>>>> 
>>>>>>>> I believe that most DataFrame implementations out there, like
>> Pandas,
>>>>>>>> supports the idea of missing values / NA, and some support the idea
>> of
>>>>>>>> Not Meaningful as well.
>>>>>>>> 
>>>>>>>> Does Row support anything like that?  That is important for certain
>>>>>>>> applications.  I thought that Row worked by being a mutable object,
>>>>>>>> but haven't looked into the details in a while.
>>>>>>>> 
>>>>>>>> -Evan
>>>>>>>> 
>>>>>>>> On Wed, Jan 28, 2015 at 4:23 PM, Reynold Xin  wrote:
>>>>>>>>> 
>>>>>>>>> It shouldn't change the data source api at all because data sources
>>>>>>>> 
>>>>>>>> create
>>>>>>>>> 
>>>>>>>>> RDD[Row], and that gets converted into a DataFrame automatically
>>>>>>>> 
>>>>>>>> (previously
>>>>>>>>> 
>>>>>>>>> to SchemaRDD).
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>> 
>>>>> 
>>>>> 
>> https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala
>>>>>>>>> 
>>>>>>>>> One thing that will break the data source API in 1.3 is the
>> location
>>>>>>>>> of
>>>>>>>>> types. Types were previously defined in sql.catalyst.types, and now
>>>>>>>> 
>>>>>>>> moved to
>>>>>>>>> 
>>>>>>>>> sql.types. After 1.3, sql.catalyst is hidden from users, and all
>>>>>>>>> public
>>>>>>>> 
>>>>>>>> APIs
>>>>>>>>> 
>>>>>>>>> have first class classes/objects defined in sql directly.
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> On Wed, Jan 28, 2015 at 4:20 PM, Evan Chan  velvia.github@gmail.com
>>>>>>>> 
>>>>>>>> wrote:
>>>>>>>>>> 
>>>>>>>>>> Hey guys,
>>>>>>>>>> 
>>>>>>>>>> How does this impact the data sources API?  I was planning on
>> using
>>>>>>>>>> this for a project.
>>>>>>>>>> 
>>>>>>>>>> +1 that many things from spark-sql / DataFrame is universally
>>>>>>>>>> desirable and useful.
>>>>>>>>>> 
>>>>>>>>>> By the way, one thing that prevents the columnar compression stuff
>>>>> 
>>>>> in
>>>>>>>>>> 
>>>>>>>>>> Spark SQL from being more useful is, at least from previous talks
>>>>>>>>>> with
>>>>>>>>>> Reynold and Michael et al., that the format was not designed for
>>>>>>>>>> persistence.
>>>>>>>>>> 
>>>>>>>>>> I have a new project that aims to change that.  It is a
>>>>>>>>>> zero-serialisation, high performance binary vector library,
>>>>> 
>>>>> designed
>>>>>>>>>> 
>>>>>>>>>> from the outset to be a persistent storage friendly.  May be one
>>>>> 
>>>>> day
>>>>>>>>>> 
>>>>>>>>>> it can replace the Spark SQL columnar compression.
>>>>>>>>>> 
>>>>>>>>>> Michael told me this would be a lot of work, and recreates parts
>> of
>>>>>>>>>> Parquet, but I think it's worth it.  LMK if you'd like more
>>>>> 
>>>>> details.
>>>>>>>>>> 
>>>>>>>>>> -Evan
>>>>>>>>>> 
>>>>>>>>>> On Tue, Jan 27, 2015 at 4:35 PM, Reynold Xin  
>>>>>>>> 
>>>>>>>> wrote:
>>>>>>>>>>> 
>>>>>>>>>>> Alright I have merged the patch (
>>>>>>>>>>> https://github.com/apache/spark/pull/4173
>>>>>>>>>>> ) since I don't see any strong opinions against it (as a matter
>>>>> 
>>>>> of
>>>>>>>> 
>>>>>>>> fact
>>>>>>>>>>> 
>>>>>>>>>>> most were for it). We can still change it if somebody lays out a
>>>>>>>> 
>>>>>>>> strong
>>>>>>>>>>> 
>>>>>>>>>>> argument.
>>>>>>>>>>> 
>>>>>>>>>>> On Tue, Jan 27, 2015 at 12:25 PM, Matei Zaharia
>>>>>>>>>>>  wrote:
>>>>>>>>>>> 
>>>>>>>>>>>> The type alias means your methods can specify either type and
>>>>> 
>>>>> they
>>>>>>>> 
>>>>>>>> will
>>>>>>>>>>>> 
>>>>>>>>>>>> work. It's just another name for the same type. But Scaladocs
>>>>> 
>>>>> and
>>>>>>>> 
>>>>>>>> such
>>>>>>>>>>>> 
>>>>>>>>>>>> will
>>>>>>>>>>>> show DataFrame as the type.
>>>>>>>>>>>> 
>>>>>>>>>>>> Matei
>>>>>>>>>>>> 
>>>>>>>>>>>>> On Jan 27, 2015, at 12:10 PM, Dirceu Semighini Filho  
>>>>>>>>>>>> dirceu.semighini@gmail.com> wrote:
>>>>>>>>>>>>> 
>>>>>>>>>>>>> Reynold,
>>>>>>>>>>>>> But with type alias we will have the same problem, right?
>>>>>>>>>>>>> If the methods doesn't receive schemardd anymore, we will have
>>>>>>>>>>>>> to
>>>>>>>>>>>>> change
>>>>>>>>>>>>> our code to migrade from schema to dataframe. Unless we have
>>>>> 
>>>>> an
>>>>>>>>>>>>> 
>>>>>>>>>>>>> implicit
>>>>>>>>>>>>> conversion between DataFrame and SchemaRDD
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 
>>>>>>>>>>>>> 2015-01-27 17:18 GMT-02:00 Reynold Xin  
>>>>>>>>>>>>>> Dirceu,
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> That is not possible because one cannot overload return
>>>>> 
>>>>> types.
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> SQLContext.parquetFile (and many other methods) needs to
>>>>> 
>>>>> return
>>>>>>>> 
>>>>>>>> some
>>>>>>>>>>>> 
>>>>>>>>>>>> type,
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> and that type cannot be both SchemaRDD and DataFrame.
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> In 1.3, we will create a type alias for DataFrame called
>>>>>>>>>>>>>> SchemaRDD
>>>>>>>>>>>>>> to
>>>>>>>>>>>> 
>>>>>>>>>>>> not
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> break source compatibility for Scala.
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> On Tue, Jan 27, 2015 at 6:28 AM, Dirceu Semighini Filho  dirceu.semighini@gmail.com> wrote:
>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> Can't the SchemaRDD remain the same, but deprecated, and be
>>>>>>>> 
>>>>>>>> removed
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> in
>>>>>>>>>>>> 
>>>>>>>>>>>> the
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> release 1.5(+/- 1)  for example, and the new code been added
>>>>>>>>>>>>>>> to
>>>>>>>>>>>> 
>>>>>>>>>>>> DataFrame?
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> With this, we don't impact in existing code for the next few
>>>>>>>>>>>>>>> releases.
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> 2015-01-27 0:02 GMT-02:00 Kushal Datta
>>>>>>>>>>>>>>>  
>>>>>>>>>>>>>>>> I want to address the issue that Matei raised about the
>>>>> 
>>>>> heavy
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> lifting
>>>>>>>>>>>>>>>> required for a full SQL support. It is amazing that even
>>>>>>>>>>>>>>>> after
>>>>>>>> 
>>>>>>>> 30
>>>>>>>>>>>> 
>>>>>>>>>>>> years
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> of
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> research there is not a single good open source columnar
>>>>>>>> 
>>>>>>>> database
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> like
>>>>>>>>>>>>>>>> Vertica. There is a column store option in MySQL, but it is
>>>>>>>>>>>>>>>> not
>>>>>>>>>>>>>>>> nearly
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> as
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> sophisticated as Vertica or MonetDB. But there's a true
>>>>> 
>>>>> need
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> for
>>>>>>>>>>>>>>>> such
>>>>>>>>>>>> 
>>>>>>>>>>>> a
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> system. I wonder why so and it's high time to change that.
>>>>>>>>>>>>>>>> On Jan 26, 2015 5:47 PM, "Sandy Ryza"
>>>>>>>>>>>>>>>>  
>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> Both SchemaRDD and DataFrame sound fine to me, though I
>>>>> 
>>>>> like
>>>>>>>> 
>>>>>>>> the
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> former
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> slightly better because it's more descriptive.
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> Even if SchemaRDD's needs to rely on Spark SQL under the
>>>>>>>> 
>>>>>>>> covers,
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> it
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> would
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> be more clear from a user-facing perspective to at least
>>>>>>>> 
>>>>>>>> choose a
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> package
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> name for it that omits "sql".
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> I would also be in favor of adding a separate Spark Schema
>>>>>>>> 
>>>>>>>> module
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> for
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> Spark
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> SQL to rely on, but I imagine that might be too large a
>>>>>>>>>>>>>>>>> change
>>>>>>>> 
>>>>>>>> at
>>>>>>>>>>>> 
>>>>>>>>>>>> this
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> point?
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> -Sandy
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> On Mon, Jan 26, 2015 at 5:32 PM, Matei Zaharia  
>>>>>>>>>>>>>>> matei.zaharia@gmail.com>
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> (Actually when we designed Spark SQL we thought of giving
>>>>>>>>>>>>>>>>>> it
>>>>>>>>>>>>>>>>>> another
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> name,
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> like Spark Schema, but we decided to stick with SQL since
>>>>>>>>>>>>>>>>>> that
>>>>>>>>>>>>>>>>>> was
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> the
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> most
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> obvious use case to many users.)
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> Matei
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> On Jan 26, 2015, at 5:31 PM, Matei Zaharia  
>>>>>>>>>>>>>>> matei.zaharia@gmail.com>
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> While it might be possible to move this concept to Spark
>>>>>>>>>>>>>>>>>>> Core
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> long-term,
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> supporting structured data efficiently does require
>>>>> 
>>>>> quite a
>>>>>>>> 
>>>>>>>> bit
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> of
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> the
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> infrastructure in Spark SQL, such as query planning and
>>>>>>>> 
>>>>>>>> columnar
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> storage.
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> The intent of Spark SQL though is to be more than a SQL
>>>>>>>>>>>>>>>>>> server
>>>>>>>>>>>>>>>>>> --
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> it's
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> meant to be a library for manipulating structured data.
>>>>>>>>>>>>>>>>>> Since
>>>>>>>>>>>>>>>>>> this
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> is
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> possible to build over the core API, it's pretty natural
>>>>> 
>>>>> to
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> organize it
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> that way, same as Spark Streaming is a library.
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>> Matei
>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>> On Jan 26, 2015, at 4:26 PM, Koert Kuipers  
>>>>>>>> koert@tresata.com>
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>> "The context is that SchemaRDD is becoming a common
>>>>> 
>>>>> data
>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>> format
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> used
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> for
>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>> bringing data into Spark from external systems, and
>>>>> 
>>>>> used
>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>> for
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> various
>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>> components of Spark, e.g. MLlib's new pipeline API."
>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>> i agree. this to me also implies it belongs in spark
>>>>>>>>>>>>>>>>>>>> core,
>>>>>>>> 
>>>>>>>> not
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> sql
>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>> On Mon, Jan 26, 2015 at 6:11 PM, Michael Malak  michaelmalak@yahoo.com.invalid> wrote:
>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> And in the off chance that anyone hasn't seen it yet,
>>>>>>>>>>>>>>>>>>>>> the
>>>>>>>>>>>>>>>>>>>>> Jan.
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> 13
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> Bay
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> Area
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> Spark Meetup YouTube contained a wealth of background
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> information
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> on
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> this
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> idea (mostly from Patrick and Reynold :-).
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> https://www.youtube.com/watch?v=YWppYPWznSQ
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> ________________________________
>>>>>>>>>>>>>>>>>>>>> From: Patrick Wendell  To: Reynold Xin  Cc: "dev@spark.apache.org"  Sent: Monday, January 26, 2015 4:01 PM
>>>>>>>>>>>>>>>>>>>>> Subject: Re: renaming SchemaRDD -> DataFrame
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> One thing potentially not clear from this e-mail,
>>>>> 
>>>>> there
>>>>>>>> 
>>>>>>>> will
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> be
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> a
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 1:1
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> correspondence where you can get an RDD to/from a
>>>>>>>> 
>>>>>>>> DataFrame.
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> On Mon, Jan 26, 2015 at 2:18 PM, Reynold Xin  
>>>>>>>>>>>>>>> rxin@databricks.com>
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> wrote:
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> Hi,
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> We are considering renaming SchemaRDD -> DataFrame in
>>>>>>>>>>>>>>>>>>>>>> 1.3,
>>>>>>>>>>>>>>>>>>>>>> and
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> wanted
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> to
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> get the community's opinion.
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> The context is that SchemaRDD is becoming a common
>>>>> 
>>>>> data
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> format
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> used
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> for
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> bringing data into Spark from external systems, and
>>>>>>>>>>>>>>>>>>>>>> used
>>>>>>>> 
>>>>>>>> for
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> various
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> components of Spark, e.g. MLlib's new pipeline API.
>>>>> 
>>>>> We
>>>>>>>> 
>>>>>>>> also
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> expect
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> more
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> and
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> more users to be programming directly against
>>>>> 
>>>>> SchemaRDD
>>>>>>>> 
>>>>>>>> API
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> rather
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> than
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> the
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> core RDD API. SchemaRDD, through its less commonly
>>>>> 
>>>>> used
>>>>>>>> 
>>>>>>>> DSL
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> originally
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> designed for writing test cases, always has the
>>>>>>>>>>>>>>>>>>>>>> data-frame
>>>>>>>>>>>>>>>>>>>>>> like
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> API.
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> In
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 1.3, we are redesigning the API to make the API
>>>>> 
>>>>> usable
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> for
>>>>>>>>>>>>>>>>>>>>>> end
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> users.
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> There are two motivations for the renaming:
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 1. DataFrame seems to be a more self-evident name
>>>>> 
>>>>> than
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> SchemaRDD.
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 2. SchemaRDD/DataFrame is actually not going to be an
>>>>>>>>>>>>>>>>>>>>>> RDD
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> anymore
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> (even
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> though it would contain some RDD functions like map,
>>>>>>>>>>>>>>>>>>>>>> flatMap,
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> etc),
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> and
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> calling it Schema*RDD* while it is not an RDD is
>>>>> 
>>>>> highly
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> confusing.
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> Instead.
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> DataFrame.rdd will return the underlying RDD for all
>>>>>>>>>>>>>>>>>>>>>> RDD
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> methods.
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> My understanding is that very few users program
>>>>>>>>>>>>>>>>>>>>>> directly
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> against
>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> the
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> SchemaRDD API at the moment, because they are not
>>>>> 
>>>>> well
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> documented.
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> However,
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> oo maintain backward compatibility, we can create a
>>>>>>>>>>>>>>>>>>>>>> type
>>>>>>>>>>>>>>>>>>>>>> alias
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> DataFrame
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> that is still named SchemaRDD. This will maintain
>>>>>>>>>>>>>>>>>>>>>> source
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> compatibility
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> for
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> Scala. That said, we will have to update all existing
>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> materials to
>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>> use
>>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>>> DataFrame rather than SchemaRDD.
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>> 
>> ---------------------------------------------------------------------
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> To unsubscribe, e-mail:
>>>>> 
>>>>> dev-unsubscribe@spark.apache.org
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> For additional commands, e-mail:
>>>>>>>>>>>>>>>>>>>>> dev-help@spark.apache.org
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>> 
>>>>>>>> 
>> ---------------------------------------------------------------------
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> To unsubscribe, e-mail:
>>>>> 
>>>>> dev-unsubscribe@spark.apache.org
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> For additional commands, e-mail:
>>>>>>>>>>>>>>>>>>>>> dev-help@spark.apache.org
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>> 
>>>>>>>> 
>> ---------------------------------------------------------------------
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>>>>>>>>>>>>>>> For additional commands, e-mail:
>>>>> 
>>>>> dev-help@spark.apache.org
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>>>>>> 
>>>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>> ---------------------------------------------------------------------
>>>>>>>>>>>> 
>>>>>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>>>>>>>>> 
>>>>>>>>>>>> 
>>>>>>>>> 
>>>>>> 
>>>>> ---------------------------------------------------------------------
>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>> 
>>>>> 
>>> 
>>> 
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>> 
>> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


+1

Tested it on Mac OS X.

One small issue I noticed is that the Scala 2.11 build is using Hadoop 1 without Hive, which is kind of weird because people will more likely want Hadoop 2 with Hive. So it would be good to publish a build for that configuration instead. We can do it if we do a new RC, or it might be that binary builds may not need to be voted on (I forgot the details there).

Matei

> On Mar 5, 2015, at 9:52 PM, Patrick Wendell  wrote:
> 
> Please vote on releasing the following candidate as Apache Spark version 1.3.0!
> 
> The tag to be voted on is v1.3.0-rc2 (commit 4aaf48d4):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4aaf48d46d13129f0f9bdafd771dd80fe568a7dc
> 
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc3/
> 
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
> 
> Staging repositories for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1078
> 
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc3-docs/
> 
> Please vote on releasing this package as Apache Spark 1.3.0!
> 
> The vote is open until Monday, March 09, at 02:52 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
> 
> [ ] +1 Release this package as Apache Spark 1.3.0
> [ ] -1 Do not release this package because ...
> 
> To learn more about Apache Spark, please see
> http://spark.apache.org/
> 
> == How does this compare to RC2 ==
> This release includes the following bug fixes:
> 
> https://issues.apache.org/jira/browse/SPARK-6144
> https://issues.apache.org/jira/browse/SPARK-6171
> https://issues.apache.org/jira/browse/SPARK-5143
> https://issues.apache.org/jira/browse/SPARK-6182
> https://issues.apache.org/jira/browse/SPARK-6175
> 
> == How can I help test this release? ==
> If you are a Spark user, you can help us test this release by
> taking a Spark 1.2 workload and running on this release candidate,
> then reporting any regressions.
> 
> If you are happy with this release based on your own testing, give a +1 vote.
> 
> == What justifies a -1 vote for this release? ==
> This vote is happening towards the end of the 1.3 QA period,
> so -1 votes should only occur for significant regressions from 1.2.1.
> Bugs already present in 1.2.X, minor regressions, or bugs related
> to new features will not block this release.
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Our goal is to let people use the latest Apache release even if vendors fall behind or don't want to package everything, so that's why we put out releases for vendors' versions. It's fairly low overhead.

Matei

> On Mar 8, 2015, at 5:56 PM, Sean Owen  wrote:
> 
> Ah. I misunderstood that Matei was referring to the Scala 2.11 tarball
> at http://people.apache.org/~pwendell/spark-1.3.0-rc3/ and not the
> Maven artifacts.
> 
> Patrick I see you just commented on SPARK-5134 and will follow up
> there. Sounds like this may accidentally not be a problem.
> 
> On binary tarball releases, I wonder if anyone has an opinion on my
> opinion that these shouldn't be distributed for specific Hadoop
> *distributions* to begin with. (Won't repeat the argument here yet.)
> That resolves this n x m explosion too.
> 
> Vendors already provide their own distribution, yes, that's their job.
> 
> 
> On Sun, Mar 8, 2015 at 9:42 PM, Krishna Sankar  wrote:
>> Yep, otherwise this will become an N^2 problem - Scala versions X Hadoop
>> Distributions X ...
>> 
>> May be one option is to have a minimum basic set (which I know is what we
>> are discussing) and move the rest to spark-packages.org. There the vendors
>> can add the latest downloads - for example when 1.4 is released, HDP can
>> build a release of HDP Spark 1.4 bundle.
>> 
>> Cheers
>>  
>> On Sun, Mar 8, 2015 at 2:11 PM, Patrick Wendell  wrote:
>>> 
>>> We probably want to revisit the way we do binaries in general for
>>> 1.4+. IMO, something worth forking a separate thread for.
>>> 
>>> I've been hesitating to add new binaries because people
>>> (understandably) complain if you ever stop packaging older ones, but
>>> on the other hand the ASF has complained that we have too many
>>> binaries already and that we need to pare it down because of the large
>>> volume of files. Doubling the number of binaries we produce for Scala
>>> 2.11 seemed like it would be too much.
>>> 
>>> One solution potentially is to actually package "Hadoop provided"
>>> binaries and encourage users to use these by simply setting
>>> HADOOP_HOME, or have instructions for specific distros. I've heard
>>> that our existing packages don't work well on HDP for instance, since
>>> there are some configuration quirks that differ from the upstream
>>> Hadoop.
>>> 
>>> If we cut down on the cross building for Hadoop versions, then it is
>>> more tenable to cross build for Scala versions without exploding the
>>> number of binaries.
>>> 
>>> - Patrick
>>> 
>>> On Sun, Mar 8, 2015 at 12:46 PM, Sean Owen  wrote:
>>>> Yeah, interesting question of what is the better default for the
>>>> single set of artifacts published to Maven. I think there's an
>>>> argument for Hadoop 2 and perhaps Hive for the 2.10 build too. Pros
>>>> and cons discussed more at
>>>> 
>>>> https://issues.apache.org/jira/browse/SPARK-5134
>>>> https://github.com/apache/spark/pull/3917
>>>> 
>>>> On Sun, Mar 8, 2015 at 7:42 PM, Matei Zaharia  wrote:
>>>>> +1
>>>>> 
>>>>> Tested it on Mac OS X.
>>>>> 
>>>>> One small issue I noticed is that the Scala 2.11 build is using Hadoop
>>>>> 1 without Hive, which is kind of weird because people will more likely want
>>>>> Hadoop 2 with Hive. So it would be good to publish a build for that
>>>>> configuration instead. We can do it if we do a new RC, or it might be that
>>>>> binary builds may not need to be voted on (I forgot the details there).
>>>>> 
>>>>> Matei
>>> 
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>> 
>> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Yeah, my concern is that people should get Apache Spark from *Apache*, not from a vendor. It helps everyone use the latest features no matter where they are. In the Hadoop distro case, Hadoop made all this effort to have standard APIs (e.g. YARN), so it should be easy. But it is a problem if we're not packaging for the newest versions of some distros; I think we just fell behind at Hadoop 2.4.

Matei

> On Mar 8, 2015, at 8:02 PM, Sean Owen  wrote:
> 
> Yeah it's not much overhead, but here's an example of where it causes
> a little issue.
> 
> I like that reasoning. However, the released builds don't track the
> later versions of Hadoop that vendors would be distributing -- there's
> no Hadoop 2.6 build for example. CDH4 is here, but not the
> far-more-used CDH5. HDP isn't present at all. The CDH4 build doesn't
> actually work with many CDH4 versions.
> 
> I agree with the goal of maximizing the reach of Spark, but I don't
> know how much these builds advance that goal.
> 
> Anyone can roll-their-own exactly-right build, and the docs and build
> have been set up to make that as simple as can be expected. So these
> aren't *required* to let me use latest Spark on distribution X.
> 
> I had thought these existed to sorta support 'legacy' distributions,
> like CDH4, and that build was justified as a
> quasi-Hadoop-2.0.x-flavored build. But then I don't understand what
> the MapR profiles are for.
> 
> I think it's too much work to correctly, in parallel, maintain any
> customizations necessary for any major distro, and it might be best to
> do not at all than to do it incompletely. You could say it's also an
> enabler for distros to vary in ways that require special
> customization.
> 
> Maybe there's a concern that, if lots of people consume Spark on
> Hadoop, and most people consume Hadoop through distros, and distros
> alone manage Spark distributions, then you de facto 'have to' go
> through a distro instead of get bits from Spark? Different
> conversation but I think this sort of effect does not end up being a
> negative.
> 
> Well anyway, I like the idea of seeing how far Hadoop-provided
> releases can help. It might kill several birds with one stone.
> 
> On Sun, Mar 8, 2015 at 11:07 PM, Matei Zaharia  wrote:
>> Our goal is to let people use the latest Apache release even if vendors fall behind or don't want to package everything, so that's why we put out releases for vendors' versions. It's fairly low overhead.
>> 
>> Matei
>> 
>>> On Mar 8, 2015, at 5:56 PM, Sean Owen  wrote:
>>> 
>>> Ah. I misunderstood that Matei was referring to the Scala 2.11 tarball
>>> at http://people.apache.org/~pwendell/spark-1.3.0-rc3/ and not the
>>> Maven artifacts.
>>> 
>>> Patrick I see you just commented on SPARK-5134 and will follow up
>>> there. Sounds like this may accidentally not be a problem.
>>> 
>>> On binary tarball releases, I wonder if anyone has an opinion on my
>>> opinion that these shouldn't be distributed for specific Hadoop
>>> *distributions* to begin with. (Won't repeat the argument here yet.)
>>> That resolves this n x m explosion too.
>>> 
>>> Vendors already provide their own distribution, yes, that's their job.
>>> 
>>> 
>>> On Sun, Mar 8, 2015 at 9:42 PM, Krishna Sankar  wrote:
>>>> Yep, otherwise this will become an N^2 problem - Scala versions X Hadoop
>>>> Distributions X ...
>>>> 
>>>> May be one option is to have a minimum basic set (which I know is what we
>>>> are discussing) and move the rest to spark-packages.org. There the vendors
>>>> can add the latest downloads - for example when 1.4 is released, HDP can
>>>> build a release of HDP Spark 1.4 bundle.
>>>> 
>>>> Cheers
>>>>  
>>>> On Sun, Mar 8, 2015 at 2:11 PM, Patrick Wendell  wrote:
>>>>> 
>>>>> We probably want to revisit the way we do binaries in general for
>>>>> 1.4+. IMO, something worth forking a separate thread for.
>>>>> 
>>>>> I've been hesitating to add new binaries because people
>>>>> (understandably) complain if you ever stop packaging older ones, but
>>>>> on the other hand the ASF has complained that we have too many
>>>>> binaries already and that we need to pare it down because of the large
>>>>> volume of files. Doubling the number of binaries we produce for Scala
>>>>> 2.11 seemed like it would be too much.
>>>>> 
>>>>> One solution potentially is to actually package "Hadoop provided"
>>>>> binaries and encourage users to use these by simply setting
>>>>> HADOOP_HOME, or have instructions for specific distros. I've heard
>>>>> that our existing packages don't work well on HDP for instance, since
>>>>> there are some configuration quirks that differ from the upstream
>>>>> Hadoop.
>>>>> 
>>>>> If we cut down on the cross building for Hadoop versions, then it is
>>>>> more tenable to cross build for Scala versions without exploding the
>>>>> number of binaries.
>>>>> 
>>>>> - Patrick
>>>>> 
>>>>> On Sun, Mar 8, 2015 at 12:46 PM, Sean Owen  wrote:
>>>>>> Yeah, interesting question of what is the better default for the
>>>>>> single set of artifacts published to Maven. I think there's an
>>>>>> argument for Hadoop 2 and perhaps Hive for the 2.10 build too. Pros
>>>>>> and cons discussed more at
>>>>>> 
>>>>>> https://issues.apache.org/jira/browse/SPARK-5134
>>>>>> https://github.com/apache/spark/pull/3917
>>>>>> 
>>>>>> On Sun, Mar 8, 2015 at 7:42 PM, Matei Zaharia  wrote:
>>>>>>> +1
>>>>>>> 
>>>>>>> Tested it on Mac OS X.
>>>>>>> 
>>>>>>> One small issue I noticed is that the Scala 2.11 build is using Hadoop
>>>>>>> 1 without Hive, which is kind of weird because people will more likely want
>>>>>>> Hadoop 2 with Hive. So it would be good to publish a build for that
>>>>>>> configuration instead. We can do it if we do a new RC, or it might be that
>>>>>>> binary builds may not need to be voted on (I forgot the details there).
>>>>>>> 
>>>>>>> Matei
>>>>> 
>>>>> ---------------------------------------------------------------------
>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>> 
>>>> 
>> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Just a note, one challenge with the BYOH version might be that users who download that can't run in local mode without also having Hadoop. But if we describe it correctly then hopefully it's okay.

Matei

> On Mar 24, 2015, at 3:05 PM, Patrick Wendell  wrote:
> 
> Hey All,
> 
> For a while we've published binary packages with different Hadoop
> client's pre-bundled. We currently have three interfaces to a Hadoop
> cluster (a) the HDFS client (b) the YARN client (c) the Hive client.
> 
> Because (a) and (b) are supposed to be backwards compatible
> interfaces. My working assumption was that for the most part (modulo
> Hive) our packages work with *newer* Hadoop versions. For instance,
> our Hadoop 2.4 package should work with HDFS 2.6 and YARN 2.6.
> However, I have heard murmurings that these are not compatible in
> practice.
> 
> So I have three questions I'd like to put out to the community:
> 
> 1. Have people had difficulty using 2.4 packages with newer Hadoop
> versions? If so, what specific incompatibilities have you hit?
> 2. Have people had issues using our binary Hadoop packages in general
> with commercial or Apache Hadoop distro's, such that you have to build
> from source?
> 3. How would people feel about publishing a "bring your own Hadoop"
> binary, where you are required to point us to a local Hadoop
> distribution by setting HADOOP_HOME? This might be better for ensuring
> full compatibility:
> https://issues.apache.org/jira/browse/SPARK-6511
> 
> - Patrick
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


You do actually sign a CLA when you become a committer, and in general, we should ask for CLAs from anyone who contributes a large piece of code. This is the individual CLA: https://www.apache.org/licenses/icla.txt. Some people have sent them proactively because their employer asks them too.

Matei

> On Apr 7, 2015, at 10:19 PM, Nicholas Chammas  wrote:
> 
> SGTM.
> 
> On Tue, Apr 7, 2015 at 9:11 PM Sean Owen  wrote:
> 
>> Yeah, this is why this pops up when you open a PR:
>> https://github.com/apache/spark/blob/master/CONTRIBUTING.md
>> 
>> Mostly, I want to take all reasonable steps to ensure that when
>> somebody offers a code contribution, that they are fine with the ways
>> in which it actually used (redistributed under the terms of the AL2),
>> whether or not they understand the intricacies. In good faith, I'm all
>> but sure that all contributors either think they're giving the
>> contribution to the project anyway, or at least, do understand it to
>> be their own work licensed under the same terms as all of the project
>> contributions are.
>> 
>> IANAL, but in stricter legal terms, the project license is plain and
>> clear, and the intricacies are signposted and easy to read when you
>> contribute. You would have a very hard time arguing that you made a
>> contribution, didn't state anything about the license, but did not
>> intend somehow that the work could be licensed as the rest of the
>> project is. For reference Apache projects do not in general require a
>> CLA.
>> 
>> On Tue, Apr 7, 2015 at 8:59 PM, Nicholas Chammas
>>  wrote:
>>> I've seen many other OSS projects ask contributors to sign CLAs. I've
>> never
>>> seen us do that.
>>> 
>>> I assume it's not an issue, since people opening PRs generally understand
>>> what it means. But legally I'm sure there's some danger in taking an
>>> implied vs. explicit license to do something.
>>> 
>>> So: Do we need to make people sign contributor CLAs?
>>> 
>>> I'm betting Sean Owen knows something about this... :)
>>> 
>>> Nick
>> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


+1. Tested on Mac OS X and verified that some of the bugs were fixed.

Matei

> On Apr 8, 2015, at 7:13 AM, Sean Owen  wrote:
> 
> Still a +1 from me; same result (except that now of course the
> UISeleniumSuite test does not fail)
> 
> On Wed, Apr 8, 2015 at 1:46 AM, Patrick Wendell  wrote:
>> Please vote on releasing the following candidate as Apache Spark version 1.3.1!
>> 
>> The tag to be voted on is v1.3.1-rc2 (commit 7c4473a):
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7c4473aa5a7f5de0323394aaedeefbf9738e8eb5
>> 
>> The list of fixes present in this release can be found at:
>> http://bit.ly/1C2nVPY
>> 
>> The release files, including signatures, digests, etc. can be found at:
>> http://people.apache.org/~pwendell/spark-1.3.1-rc2/
>> 
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/pwendell.asc
>> 
>> The staging repository for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1083/
>> 
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~pwendell/spark-1.3.1-rc2-docs/
>> 
>> The patches on top of RC1 are:
>> 
>> [SPARK-6737] Fix memory leak in OutputCommitCoordinator
>> https://github.com/apache/spark/pull/5397
>> 
>> [SPARK-6636] Use public DNS hostname everywhere in spark_ec2.py
>> https://github.com/apache/spark/pull/5302
>> 
>> [SPARK-6205] [CORE] UISeleniumSuite fails for Hadoop 2.x test with
>> NoClassDefFoundError
>> https://github.com/apache/spark/pull/4933
>> 
>> Please vote on releasing this package as Apache Spark 1.3.1!
>> 
>> The vote is open until Saturday, April 11, at 07:00 UTC and passes
>> if a majority of at least 3 +1 PMC votes are cast.
>> 
>> [ ] +1 Release this package as Apache Spark 1.3.1
>> [ ] -1 Do not release this package because ...
>> 
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>> 
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>> 
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


It could also be that your hash function is expensive. What is the key class you have for the reduceByKey / groupByKey?

Matei

> On May 12, 2015, at 10:08 AM, Night Wolf  wrote:
> 
> I'm seeing a similar thing with a slightly different stack trace. Ideas?
> 
> org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:150)
> org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)
> org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:205)
> org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:56)
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
> org.apache.spark.scheduler.Task.run(Task.scala:64)
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> java.lang.Thread.run(Thread.java:745)
> 
> On Tue, May 12, 2015 at 5:55 AM, Reynold Xin <rxin@databricks.com  wrote:
> Looks like it is spending a lot of time doing hash probing. It could be a
> number of the following:
> 
> 1. hash probing itself is inherently expensive compared with rest of your
> workload
> 
> 2. murmur3 doesn't work well with this key distribution
> 
> 3. quadratic probing (triangular sequence) with a power-of-2 hash table
> works really badly for this workload.
> 
> One way to test this is to instrument changeValue function to store the
> number of probes in total, and then log it. We added this probing
> capability to the new Bytes2Bytes hash map we built. We should consider
> just having it being reported as some built-in metrics to facilitate
> debugging.
> 
> https://github.com/apache/spark/blob/b83091ae4589feea78b056827bc3b7659d271e41/unsafe/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java#L214  
> 
> 
> 
> 
> 
> On Mon, May 11, 2015 at 4:21 AM, Michal Haris <michal.haris@visualdna.com  wrote:
> 
> > This is the stack trace of the worker thread:
> >
> >
> > org.apache.spark.util.collection.AppendOnlyMap.changeValue(AppendOnlyMap.scala:150)
> >
> > org.apache.spark.util.collection.SizeTrackingAppendOnlyMap.changeValue(SizeTrackingAppendOnlyMap.scala:32)
> >
> > org.apache.spark.util.collection.ExternalAppendOnlyMap.insertAll(ExternalAppendOnlyMap.scala:130)
> > org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:60)
> >
> > org.apache.spark.shuffle.hash.HashShuffleReader.read(HashShuffleReader.scala:46)
> > org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:92)
> > org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
> > org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
> > org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
> > org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
> > org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
> > org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
> > org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
> > org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
> > org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
> > org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
> > org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:70)
> > org.apache.spark.rdd.RDD.iterator(RDD.scala:242)
> > org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
> > org.apache.spark.scheduler.Task.run(Task.scala:64)
> > org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
> >
> > java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> >
> > java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> > java.lang.Thread.run(Thread.java:745)
> >
> > On 8 May 2015 at 22:12, Josh Rosen <rosenville@gmail.com  wrote:
> >
> >> Do you have any more specific profiling data that you can share?  I'm
> >> curious to know where AppendOnlyMap.changeValue is being called from.
> >>
> >> On Fri, May 8, 2015 at 1:26 PM, Michal Haris <michal.haris@visualdna.com  >> wrote:
> >>
> >>> +dev
> >>> On 6 May 2015 10:45, "Michal Haris" <michal.haris@visualdna.com  wrote:
> >>>
> >>> > Just wanted to check if somebody has seen similar behaviour or knows
> >>> what
> >>> > we might be doing wrong. We have a relatively complex spark application
> >>> > which processes half a terabyte of data at various stages. We have
> >>> profiled
> >>> > it in several ways and everything seems to point to one place where
> >>> 90% of
> >>> > the time is spent:  AppendOnlyMap.changeValue. The job scales and is
> >>> > relatively faster than its map-reduce alternative but it still feels
> >>> slower
> >>> > than it should be. I am suspecting too much spill but I haven't seen
> >>> any
> >>> > improvement by increasing number of partitions to 10k. Any idea would
> >>> be
> >>> > appreciated.
> >>> >
> >>> > --
> >>> > Michal Haris
> >>> > Technical Architect
> >>> > direct line: +44 (0) 207 749 0229  >>> > www.visualdna.com  | t: +44 (0) 207 734 7033  >>> >
> >>>
> >>
> >>
> >
> >
> > --
> > Michal Haris
> > Technical Architect
> > direct line: +44 (0) 207 749 0229  > www.visualdna.com  | t: +44 (0) 207 734 7033  >
> 


Your best bet might be to use a map in SQL and make the keys be longer paths (e.g. params_param1 and params_param2). I don't think you can have a map in some of them but not in others.

Matei

> On May 28, 2015, at 3:48 PM, Jeremy Lucas  wrote:
> 
> Hey Reynold,
> 
> Thanks for the suggestion. Maybe a better definition of what I mean by a "recursive" data structure is rather what might resemble (in Scala) the type Map[String, Any]. With a type like this, the keys are well-defined as strings (as this is JSON) but the values can be basically any arbitrary value, including another Map[String, Any].
> 
> For example, in the below "stream" of JSON records:
> 
> {
>   "timestamp": "2015-01-01T00:00:00Z",
>   "data": {
>     "event": "click",
>     "url": "http://mywebsite.com    }
> }
> ...
> {
>   "timestamp": "2015-01-01T08:00:00Z",
>   "data": {
>     "event": "purchase",
>     "sku": "123456789",
>     "quantity": 1,
>     "params": {
>       "arbitrary-param-1": "blah",
>       "arbitrary-param-2": 123456
>   }
> }
> 
> I am trying to figure out a way to run SparkSQL over the above JSON records. My inclination would be to define the "timestamp" field as a well-defined DateType, but the "data" field is way more free-form.
> 
> Also, any pointers on where to look for how data types are evaluated and serialized/deserialized would be super helpful as well.
> 
> Thanks
> 
> 
> 
> On Thu, May 28, 2015 at 12:30 AM Reynold Xin <rxin@databricks.com  wrote:
> I think it is fairly hard to support recursive data types. What I've seen in one other proprietary system in the past is to let the user define the depth of the nested data types, and then just expand the struct/map/list definition to the maximum level of depth.
> 
> Would this solve your problem?
> 
> 
> 
> 
> On Wed, May 20, 2015 at 6:07 PM, Jeremy Lucas <jeremyalucas@gmail.com  wrote:
> Hey Rakesh,
> 
> To clarify, what I was referring to is when doing something like this:
> 
> sqlContext.applySchema(rdd, mySchema)
> 
> mySchema must be a well-defined StructType, which presently does not allow for a recursive type.
> 
> 
> On Wed, May 20, 2015 at 5:39 PM Rakesh Chalasani <vnit.rakesh@gmail.com  wrote:
> Hi Jeremy:
> 
> Row is a collect of 'Any'. So, you can be used as a recursive data type. Is this what you were looking for?
> 
> Example:
> val x = sc.parallelize(Array.range(0,10)).map(x => Row(Row(x), Row(x.toString)))
> 
> Rakesh
> 
> 
> 
> On Wed, May 20, 2015 at 7:23 PM Jeremy Lucas <jeremyalucas@gmail.com  wrote:
> Spark SQL has proven to be quite useful in applying a partial schema to large JSON logs and being able to write plain SQL to perform a wide variety of operations over this data. However, one small thing that keeps coming back to haunt me is the lack of support for recursive data types, whereby a member of a complex/struct value can be of the same type as the complex/struct value itself.
> I am hoping someone may be able to point me in the right direction of where to start to build out such capabilities, as I'd be happy to contribute, but am very new to this particular component of the Spark project.
> 


+1 

Tested on Mac OS X

> On Jun 4, 2015, at 1:09 PM, Patrick Wendell  wrote:
> 
> I will give +1 as well.
> 
> On Wed, Jun 3, 2015 at 11:59 PM, Reynold Xin  wrote:
>> Let me give you the 1st
>> 
>> +1
>> 
>> 
>> 
>> On Tue, Jun 2, 2015 at 10:47 PM, Patrick Wendell  wrote:
>>> 
>>> He all - a tiny nit from the last e-mail. The tag is v1.4.0-rc4. The
>>> exact commit and all other information is correct. (thanks Shivaram
>>> who pointed this out).
>>> 
>>> On Tue, Jun 2, 2015 at 8:53 PM, Patrick Wendell  wrote:
>>>> Please vote on releasing the following candidate as Apache Spark version
>>>> 1.4.0!
>>>> 
>>>> The tag to be voted on is v1.4.0-rc3 (commit 22596c5):
>>>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=
>>>> 22596c534a38cfdda91aef18aa9037ab101e4251
>>>> 
>>>> The release files, including signatures, digests, etc. can be found at:
>>>> http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc4-bin/
>>>> 
>>>> Release artifacts are signed with the following key:
>>>> https://people.apache.org/keys/committer/pwendell.asc
>>>> 
>>>> The staging repository for this release can be found at:
>>>> [published as version: 1.4.0]
>>>> https://repository.apache.org/content/repositories/orgapachespark-1111/
>>>> [published as version: 1.4.0-rc4]
>>>> https://repository.apache.org/content/repositories/orgapachespark-1112/
>>>> 
>>>> The documentation corresponding to this release can be found at:
>>>> http://people.apache.org/~pwendell/spark-releases/spark-1.4.0-rc4-docs/
>>>> 
>>>> Please vote on releasing this package as Apache Spark 1.4.0!
>>>> 
>>>> The vote is open until Saturday, June 06, at 05:00 UTC and passes
>>>> if a majority of at least 3 +1 PMC votes are cast.
>>>> 
>>>> [ ] +1 Release this package as Apache Spark 1.4.0
>>>> [ ] -1 Do not release this package because ...
>>>> 
>>>> To learn more about Apache Spark, please see
>>>> http://spark.apache.org/
>>>> 
>>>> == What has changed since RC3 ==
>>>> In addition to may smaller fixes, three blocker issues were fixed:
>>>> 4940630 [SPARK-8020] [SQL] Spark SQL conf in spark-defaults.conf make
>>>> metadataHive get constructed too early
>>>> 6b0f615 [SPARK-8038] [SQL] [PYSPARK] fix Column.when() and otherwise()
>>>> 78a6723 [SPARK-7978] [SQL] [PYSPARK] DecimalType should not be singleton
>>>> 
>>>> == How can I help test this release? ==
>>>> If you are a Spark user, you can help us test this release by
>>>> taking a Spark 1.3 workload and running on this release candidate,
>>>> then reporting any regressions.
>>>> 
>>>> == What justifies a -1 vote for this release? ==
>>>> This vote is happening towards the end of the 1.4 QA period,
>>>> so -1 votes should only occur for significant regressions from 1.3.1.
>>>> Bugs already present in 1.3.X, minor regressions, or bugs related
>>>> to new features will not block this release.
>>> 
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>> 
>> 
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


I don't like the idea of removing Hadoop 1 unless it becomes a significant maintenance burden, which I don't think it is. You'll always be surprised how many people use old software, even though various companies may no longer support them.

With Hadoop 2 in particular, I may be misremembering, but I believe that the experience on Windows is considerably worse because it requires these shell scripts to set permissions that it won't find if you just download Spark. That would be one reason to keep Hadoop 1 in the default build. But I could be wrong, it's been a while since I tried Windows.

Matei


> On Jun 12, 2015, at 11:21 AM, Sean Owen  wrote:
> 
> I don't imagine that can be guaranteed to be supported anyway... the
> 0.x branch has never necessarily worked with Spark, even if it might
> happen to. Is this really something you would veto for everyone
> because of your deployment?
> 
> On Fri, Jun 12, 2015 at 7:18 PM, Thomas Dudziak  wrote:
>> -1 to this, we use it with an old Hadoop version (well, a fork of an old
>> version, 0.23). That being said, if there were a nice developer api that
>> separates Spark from Hadoop (or rather, two APIs, one for scheduling and one
>> for HDFS), then we'd be happy to maintain our own plugins for those.
>> 
>> cheers,
>> Tom
>> 
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Hey all,

Over the past 1.5 months we added a number of new committers to the project, and I wanted to welcome them now that all of their respective forms, accounts, etc are in. Join me in welcoming the following new committers:

- Davies Liu
- DB Tsai
- Kousuke Saruta
- Sandy Ryza
- Yin Huai

Looking forward to more great contributions from all of these folks.

Matei
---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Just FYI, it would be easiest to follow SparkR's example and add the DataFrame API first. Other APIs will be designed to work on DataFrames (most notably machine learning pipelines), and the surface of this API is much smaller than of the RDD API. This API will also give you great performance as we continue to optimize Spark SQL.

Matei

> On Jun 23, 2015, at 1:46 PM, Shivaram Venkataraman  wrote:
> 
> Every language has its own quirks / features -- so I don't think there exists a document on how to go about doing this for a new language. The most related write up I know of is the wiki page on PySpark internals https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals  written by Josh Rosen -- It covers some of the issues like closure capture, serialization, JVM communication that you'll need to handle for a new language. 
> 
> Thanks
> Shivaram
> 
> On Tue, Jun 23, 2015 at 1:35 PM, Vasili I. Galchin <vigalchin@gmail.com  wrote:
> Hello,
> 
>       I want to add language support for another language(other than Scala, Java et. al.). Where is documentation that explains to provide support for a new language?
> 
> Thank you,
> 
> Vasili
> 


I agree with this -- basically, to build on Reynold's point, you should be able to get almost the same performance by implementing either the Hadoop FileSystem API or the Spark Data Source API over Ignite in the right way. This would let people save data persistently in Ignite in addition to using it for caching, and it would provide a global namespace, optionally a schema, etc. You can still provide data locality, short-circuit reads, etc with these APIs.

Matei

> On Jul 20, 2015, at 9:40 PM, Reynold Xin  wrote:
> 
> I sent it prematurely.
> 
> They are already pluggable, or at least in the process to be more pluggable. In 1.4, instead of calling the external system's API directly, we added an API for that.  There is a patch to add support for HDFS in-memory cache. 
> 
> Somewhat orthogonal to this, longer term, I am not sure whether it makes sense to have the current off heap API, because there is no namespacing and the benefit to end users is actually not very substantial (at least I can think of much simpler ways to achieve exactly the same gains), and yet it introduces quite a bit of complexity to the codebase.
> 
> 
> 
> 
> On Mon, Jul 20, 2015 at 9:34 PM, Reynold Xin <rxin@databricks.com  wrote:
> They are already pluggable.
> 
> 
> On Mon, Jul 20, 2015 at 9:32 PM, Prashant Sharma <scrapcodes@gmail.com  wrote:
> +1 Looks like a nice idea(I do not see any harm). Would you like to work on the patch to support it ?
> 
> Prashant Sharma
> 
> 
> 
> On Tue, Jul 21, 2015 at 2:46 AM, Alexey Goncharuk <alexey.goncharuk@gmail.com  wrote:
> Hello Spark community,
> 
> I was looking through the code in order to understand better how RDD is persisted to Tachyon off-heap filesystem. It looks like that the Tachyon filesystem is hard-coded and there is no way to switch to another in-memory filesystem. I think it would be great if the implementation of the BlockManager and BlockStore would be able to plug in another filesystem.
> 
> For example, Apache Ignite also has an implementation of in-memory filesystem which can store data in on-heap and off-heap formats. It would be great if it could integrate with Spark.
> 
> I have filed a ticket in Jira: 
> https://issues.apache.org/jira/browse/SPARK-9203  
> If it makes sense, I will be happy to contribute to it.
> 
> Thoughts?
> 
> -Alexey (Apache Ignite PMC)
> 
> 
> 


I like the idea of popping out Tachyon to an optional component too to reduce the number of dependencies. In the future, it might even be useful to do this for Hadoop, but it requires too many API changes to be worth doing now.

Regarding Scala 2.12, we should definitely support it eventually, but I don't think we need to block 2.0 on that because it can be added later too. Has anyone investigated what it would take to run on there? I imagine we don't need many code changes, just maybe some REPL stuff.

Needless to say, but I'm all for the idea of making "major" releases as undisruptive as possible in the model Reynold proposed. Keeping everyone working with the same set of releases is super important.

Matei

> On Nov 11, 2015, at 4:58 AM, Sean Owen  wrote:
> 
> On Wed, Nov 11, 2015 at 12:10 AM, Reynold Xin  wrote:
>> to the Spark community. A major release should not be very different from a
>> minor release and should not be gated based on new features. The main
>> purpose of a major release is an opportunity to fix things that are broken
>> in the current API and remove certain deprecated APIs (examples follow).
> 
> Agree with this stance. Generally, a major release might also be a
> time to replace some big old API or implementation with a new one, but
> I don't see obvious candidates.
> 
> I wouldn't mind turning attention to 2.x sooner than later, unless
> there's a fairly good reason to continue adding features in 1.x to a
> 1.7 release. The scope as of 1.6 is already pretty darned big.
> 
> 
>> 1. Scala 2.11 as the default build. We should still support Scala 2.10, but
>> it has been end-of-life.
> 
> By the time 2.x rolls around, 2.12 will be the main version, 2.11 will
> be quite stable, and 2.10 will have been EOL for a while. I'd propose
> dropping 2.10. Otherwise it's supported for 2 more years.
> 
> 
>> 2. Remove Hadoop 1 support.
> 
> I'd go further to drop support for <2.2 for sure (2.0 and 2.1 were
> sort of 'alpha' and 'beta' releases) and even  
> I'm sure we'll think of a number of other small things -- shading a
> bunch of stuff? reviewing and updating dependencies in light of
> simpler, more recent dependencies to support from Hadoop etc?
> 
> Farming out Tachyon to a module? (I felt like someone proposed this?)
> Pop out any Docker stuff to another repo?
> Continue that same effort for EC2?
> Farming out some of the "external" integrations to another repo (?
> controversial)
> 
> See also anything marked version "2+" in JIRA.
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Hi all,

The PMC has recently added two new Spark committers -- Herman van Hovell and Wenchen Fan. Both have been heavily involved in Spark SQL and Tungsten, adding new features, optimizations and APIs. Please join me in welcoming Herman and Wenchen.

Matei
---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


I agree that putting it in 2.0 doesn't mean keeping Scala 2.10 for the entire 2.x line. My vote is to keep Scala 2.10 in Spark 2.0, because it's the default version we built with in 1.x. We want to make the transition from 1.x to 2.0 as easy as possible. In 2.0, we'll have the default downloads be for Scala 2.11, so people will more easily move, but we shouldn't create obstacles that lead to fragmenting the community and slowing down Spark 2.0's adoption. I've seen companies that stayed on an old Scala version for multiple years because switching it, or mixing versions, would affect the company's entire codebase.

Matei

> On Mar 30, 2016, at 12:08 PM, Koert Kuipers  wrote:
> 
> oh wow, had no idea it got ripped out
> 
> On Wed, Mar 30, 2016 at 11:50 AM, Mark Hamstra <mark@clearstorydata.com  wrote:
> No, with 2.0 Spark really doesn't use Akka: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkConf.scala#L744  
> On Wed, Mar 30, 2016 at 9:10 AM, Koert Kuipers <koert@tresata.com  wrote:
> Spark still runs on akka. So if you want the benefits of the latest akka (not saying we do, was just an example) then you need to drop scala 2.10
> 
> On Mar 30, 2016 10:44 AM, "Cody Koeninger" <cody@koeninger.org  wrote:
> I agree with Mark in that I don't see how supporting scala 2.10 for
> spark 2.0 implies supporting it for all of spark 2.x
> 
> Regarding Koert's comment on akka, I thought all akka dependencies
> have been removed from spark after SPARK-7997 and the recent removal
> of external/akka
> 
> On Wed, Mar 30, 2016 at 9:36 AM, Mark Hamstra <mark@clearstorydata.com  wrote:
> > Dropping Scala 2.10 support has to happen at some point, so I'm not
> > fundamentally opposed to the idea; but I've got questions about how we go
> > about making the change and what degree of negative consequences we are
> > willing to accept.  Until now, we have been saying that 2.10 support will be
> > continued in Spark 2.0.0.  Switching to 2.11 will be non-trivial for some
> > Spark users, so abruptly dropping 2.10 support is very likely to delay
> > migration to Spark 2.0 for those users.
> >
> > What about continuing 2.10 support in 2.0.x, but repeatedly making an
> > obvious announcement in multiple places that such support is deprecated,
> > that we are not committed to maintaining it throughout 2.x, and that it is,
> > in fact, scheduled to be removed in 2.1.0?
> >
> > On Wed, Mar 30, 2016 at 7:45 AM, Sean Owen <sowen@cloudera.com  wrote:
> >>
> >> (This should fork as its own thread, though it began during discussion
> >> of whether to continue Java 7 support in Spark 2.x.)
> >>
> >> Simply: would like to more clearly take the temperature of all
> >> interested parties about whether to support Scala 2.10 in the Spark
> >> 2.x lifecycle. Some of the arguments appear to be:
> >>
> >> Pro
> >> - Some third party dependencies do not support Scala 2.11+ yet and so
> >> would not be usable in a Spark app
> >>
> >> Con
> >> - Lower maintenance overhead -- no separate 2.10 build,
> >> cross-building, tests to check, esp considering support of 2.12 will
> >> be needed
> >> - Can use 2.11+ features freely
> >> - 2.10 was EOL in late 2014 and Spark 2.x lifecycle is years to come
> >>
> >> I would like to not support 2.10 for Spark 2.x, myself.
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org  >> For additional commands, e-mail: dev-help@spark.apache.org  >>
> >
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org  For additional commands, e-mail: dev-help@spark.apache.org  
> 
> 


Hi folks,

Around 1.5 years ago, Spark added a maintainer process for reviewing API and architectural changes (https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-ReviewProcessandMaintainers) to make sure these are seen by people who spent a lot of time on that component. At the time, the worry was that changes might go unnoticed as the project grows, but there were also concerns that this approach makes the project harder to contribute to and less welcoming. Since implementing the model, I think that a good number of developers concluded it doesn't make a huge difference, so because of these concerns, it may be useful to remove it. I've also heard that we should try to keep some other instructions for contributors to find the "right" reviewers, so it would be great to see suggestions on that. For my part, I'd personally prefer something "automatic", such as easily tracking who reviewed each patch and having people look at the commit history of the module they want to work on, instead of a list that needs to be maintained separately.

Matei
---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


It looks like the discussion thread on this has only had positive replies, so I'm going to call a VOTE. The proposal is to remove the maintainer process in https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-ReviewProcessandMaintainers  given that it doesn't seem to have had a huge impact on the project, and it can unnecessarily create friction in contributing. We already have +1s from Mridul, Tom, Andrew Or and Imran on that thread.

I'll leave the VOTE open for 48 hours, until 9 PM EST on May 24, 2016.

Matei
---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Correction, let's run this for 72 hours, so until 9 PM EST May 25th.

> On May 22, 2016, at 8:34 PM, Matei Zaharia  wrote:
> 
> It looks like the discussion thread on this has only had positive replies, so I'm going to call a VOTE. The proposal is to remove the maintainer process in https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-ReviewProcessandMaintainers  given that it doesn't seem to have had a huge impact on the project, and it can unnecessarily create friction in contributing. We already have +1s from Mridul, Tom, Andrew Or and Imran on that thread.
> 
> I'll leave the VOTE open for 48 hours, until 9 PM EST on May 24, 2016.
> 
> Matei


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Just wondering, what is the main use case for the Docker images -- to develop apps locally or to deploy a cluster? If the image is really just a script to download a certain package name from a mirror, it may be okay to create an official one, though it does seem tricky to make it properly use the right mirror.

Matei

> On May 25, 2016, at 6:05 PM, Luciano Resende  wrote:
> 
> 
> 
> On Wed, May 25, 2016 at 2:34 PM, Sean Owen <sowen@cloudera.com  wrote:
> I don't think the project would bless anything but the standard
> release artifacts since only those are voted on. People are free to
> maintain whatever they like and even share it, as long as it's clear
> it's not from the Apache project.
> 
> 
> +1
> 
> 
> -- 
> Luciano Resende
> http://twitter.com/lresende1975  http://lresende.blogspot.com/ 

Thanks everyone for voting. With only +1 votes, the vote passes, so I'll update the contributor wiki appropriately.

+1 votes:

Matei Zaharia (binding)
Mridul Muralidharan (binding)
Andrew Or (binding)
Sean Owen (binding)
Nick Pentreath (binding)
Tom Graves (binding)
Imran Rashid (binding)
Holden Karau
Owen O'Malley

No 0 or -1 votes.

Matei


> On May 24, 2016, at 12:27 PM, Owen O'Malley  wrote:
> 
> +1 (non-binding)
> 
> I think this is an important step to improve Spark as an Apache project.
> 
> .. Owen
> 
> On Mon, May 23, 2016 at 11:18 AM, Holden Karau <holden@pigscanfly.ca  wrote:
> +1 non-binding (as a contributor anything which speed things up is worth a try, and git blame is a good enough substitute for the list when figuring out who to ping on a PR).
> 
> 
> On Monday, May 23, 2016, Imran Rashid <irashid@cloudera.com  wrote:
> +1 (binding)
> 
> On Mon, May 23, 2016 at 8:13 AM, Tom Graves <tgraves_cs@yahoo.com.invalid  wrote:
> +1 (binding)
> 
> Tom
> 
> 
> On Sunday, May 22, 2016 7:34 PM, Matei Zaharia <matei.zaharia@gmail.com  wrote:
> 
> 
> It looks like the discussion thread on this has only had positive replies, so I'm going to call a VOTE. The proposal is to remove the maintainer process in https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-ReviewProcessandMaintainers  <https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-ReviewProcessandMaintainers  given that it doesn't seem to have had a huge impact on the project, and it can unnecessarily create friction in contributing. We already have +1s from Mridul, Tom, Andrew Or and Imran on that thread.
> 
> I'll leave the VOTE open for 48 hours, until 9 PM EST on May 24, 2016.
> 
> Matei
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org  For additional commands, e-mail: dev-help@spark.apache.org  
> 
> 
> 
> 
> -- 
> Cell : 425-233-8271  Twitter: https://twitter.com/holdenkarau  


Hi all,

The PMC recently voted to add Yanbo Liang as a committer. Yanbo has been a super active contributor in many areas of MLlib. Please join me in welcoming Yanbo!

Matei
---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Personally I'd just put them on the staging repo and link to that on the downloads page. It will create less confusion for people browsing Maven Central later and wondering which releases are safe to use.

Matei

> On Jun 3, 2016, at 8:22 AM, Mark Hamstra  wrote:
> 
> It's not a question of whether the preview artifacts can be made available on Maven central, but rather whether they must be or should be.  I've got no problems leaving these unstable, transitory artifacts out of the more permanent, canonical repository.
> 
> On Fri, Jun 3, 2016 at 1:53 AM, Steve Loughran <stevel@hortonworks.com  wrote:
> 
> It's been voted on by the project, so can go up on central
> 
> There's already some JIRAs being filed against it, this is a metric of success as pre-beta of the artifacts.
> 
> The risk of exercising the m2 central option is that people may get expectations that they can point their code at the 2.0.0-preview and then, when a release comes out, simply
> update their dependency; this may/may not be the case. But is it harmful if people do start building and testing against the preview? If it finds problems early, it can only be a good thing
> 
> 
> > On 1 Jun 2016, at 23:10, Sean Owen <sowen@cloudera.com  wrote:
> >
> > I'll be more specific about the issue that I think trumps all this,
> > which I realize maybe not everyone was aware of.
> >
> > There was a long and contentious discussion on the PMC about, among
> > other things, advertising a "Spark 2.0 preview" from Databricks, such
> > as at https://databricks.com/blog/2016/05/11/apache-spark-2-0-technical-preview-easier-faster-and-smarter.html  >
> > That post has already been updated/fixed from an earlier version, but
> > part of the resolution was to make a full "2.0.0 preview" release in
> > order to continue to be able to advertise it as such. Without it, I
> > believe the PMC's conclusion remains that this blog post / product
> > announcement is not allowed by ASF policy. Hence, either the product
> > announcements need to be taken down and a bunch of wording changed in
> > the Databricks product, or, this needs to be a normal release.
> >
> > Obviously, it seems far easier to just finish the release per usual. I
> > actually didn't realize this had not been offered for download at
> > http://spark.apache.org/downloads.html  either. It needs to be
> > accessible there too.
> >
> >
> > We can get back in the weeds about what a "preview" release means,
> > but, normal voted releases can and even should be alpha/beta
> > (http://www.apache.org/dev/release.html ) The culture is, in theory, to
> > release early and often. I don't buy an argument that it's too old, at
> > 2 weeks, when the alternative is having nothing at all to test
> > against.
> >
> > On Wed, Jun 1, 2016 at 5:02 PM, Michael Armbrust <michael@databricks.com  wrote:
> >>> I'd think we want less effort, not more, to let people test it? for
> >>> example, right now I can't easily try my product build against
> >>> 2.0.0-preview.
> >>
> >>
> >> I don't feel super strongly one way or the other, so if we need to publish
> >> it permanently we can.
> >>
> >> However, either way you can still test against this release.  You just need
> >> to add a resolver as well (which is how I have always tested packages
> >> against RCs).  One concern with making it permeant is this preview release
> >> is already fairly far behind branch-2.0, so many of the issues that people
> >> might report have already been fixed and that might continue even after the
> >> release is made.  I'd rather be able to force upgrades eventually when we
> >> vote on the final 2.0 release.
> >>
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org  > For additional commands, e-mail: dev-help@spark.apache.org  >
> >
> 
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org  For additional commands, e-mail: dev-help@spark.apache.org  
> 


Is there any way to remove artifacts from Maven Central? Maybe that would
help clean these things up long-term, though it would create problems for
users who for some reason decide to rely on these previews.

In any case, if people are *really* concerned about this, we should just
put it there. My thought was that it's better for users to do something
special to link to this release (e.g. add a reference to the staging repo)
so that they are more likely to know that it's a special, unstable thing.
Same thing they do to use snapshots.

Matei

BTW, same goes with docs -- Sean, if you want to add a /docs/2.0-preview on
the website and link to it, go for it!

Matei

Hi all, FYI, we've recently updated the Spark logo at https://spark.apache.org/ to say "Apache Spark" instead of just "Spark". Many ASF projects have been doing this recently to make it clearer that they are associated with the ASF, and indeed the ASF's branding guidelines generally require that projects be referred to as "Apache X" in various settings, especially in related commercial or open source products (https://www.apache.org/foundation/marks/). If you have any kind of site or product that uses Spark logo, it would be great to update to this full one.

There are EPS versions of the logo available at https://spark.apache.org/images/spark-logo.eps and https://spark.apache.org/images/spark-logo-reverse.eps; before using these also check https://www.apache.org/foundation/marks/.

Matei
---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


Hi Jacek,

This applies to all schedulers actually -- it just tells Spark to re-check the available nodes and possibly launch tasks on them, because a new stage was submitted. Then when any node is available, the scheduler will call the TaskSetManager with an "offer" for the node.

Matei

> On Jun 19, 2016, at 11:54 PM, Jacek Laskowski  wrote:
> 
> Hi,
> 
> Whenever I see `backend.reviveOffers()` I'm struggling myself with
> properly explaining what it does. My understanding is that it requests
> a SchedulerBackend (that's responsible for talking to a cluster
> manager) to...that's the moment I'm not sure about.
> 
> How would you explain `backend.reviveOffers()`?
> 
> p.s. I understand that it's somehow related to how Mesos manages
> resources where it offers resources, but can't find anything related
> to `reviving offers` in Mesos docs :(
> 
> Please guide. Thanks!
> 
> Pozdrawiam,
> Jacek Laskowski
> ----
> https://medium.com/@jaceklaskowski/
> Mastering Apache Spark http://bit.ly/mastering-apache-spark
> Follow me at https://twitter.com/jaceklaskowski
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
> 


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


+1

Tested on Mac.

Matei

> On Jul 22, 2016, at 11:18 AM, Joseph Bradley  wrote:
> 
> +1
> 
> Mainly tested ML/Graph/R.  Perf tests from Tim Hunter showed minor speedups from 1.6 for common ML algorithms.
> 
> On Thu, Jul 21, 2016 at 9:41 AM, Ricardo Almeida <ricardo.almeida@actnowib.com  wrote:
> +1 (non binding)
> 
> Tested PySpark Core, DataFrame/SQL, MLlib and Streaming on a standalone cluster
> 
> On 21 July 2016 at 05:24, Reynold Xin <rxin@databricks.com  wrote:
> +1
> 
> 
> On Wednesday, July 20, 2016, Krishna Sankar <ksankar42@gmail.com  wrote:
> +1 (non-binding, of course)
> 
> 1. Compiled OS X 10.11.5 (El Capitan) OK Total time: 24:07 min
>      mvn clean package -Pyarn -Phadoop-2.7 -DskipTests
> 2. Tested pyspark, mllib (iPython 4.0)
> 2.0 Spark version is 2.0.0 
> 2.1. statistics (min,max,mean,Pearson,Spearman) OK
> 2.2. Linear/Ridge/Lasso Regression OK 
> 2.3. Classification : Decision Tree, Naive Bayes OK
> 2.4. Clustering : KMeans OK
>        Center And Scale OK
> 2.5. RDD operations OK
>       State of the Union Texts - MapReduce, Filter,sortByKey (word count)
> 2.6. Recommendation (Movielens medium dataset ~1 M ratings) OK
>        Model evaluation/optimization (rank, numIter, lambda) with itertools OK
> 3. Scala - MLlib
> 3.1. statistics (min,max,mean,Pearson,Spearman) OK
> 3.2. LinearRegressionWithSGD OK
> 3.3. Decision Tree OK
> 3.4. KMeans OK
> 3.5. Recommendation (Movielens medium dataset ~1 M ratings) OK
> 3.6. saveAsParquetFile OK
> 3.7. Read and verify the 3.6 save(above) - sqlContext.parquetFile, registerTempTable, sql OK
> 3.8. result = sqlContext.sql("SELECT OrderDetails.OrderID,ShipCountry,UnitPrice,Qty,Discount FROM Orders INNER JOIN OrderDetails ON Orders.OrderID = OrderDetails.OrderID") OK
> 4.0. Spark SQL from Python OK
> 4.1. result = sqlContext.sql("SELECT * from people WHERE State = 'WA'") OK
> 5.0. Packages
> 5.1. com.databricks.spark.csv - read/write OK (--packages com.databricks:spark-csv_2.10:1.4.0)
> 6.0. DataFrames 
> 6.1. cast,dtypes OK
> 6.2. groupBy,avg,crosstab,corr,isNull,na.drop OK
> 6.3. All joins,sql,set operations,udf OK
> [Dataframe Operations very fast from 11 secs to 3 secs, to 1.8 secs, to 1.5 secs! Good work !!!]
> 7.0. GraphX/Scala
> 7.1. Create Graph (small and bigger dataset) OK
> 7.2. Structure APIs - OK
> 7.3. Social Network/Community APIs - OK
> 7.4. Algorithms : PageRank of 2 datasets, aggregateMessages() - OK
> 
> Cheers
>  
> On Tue, Jul 19, 2016 at 7:35 PM, Reynold Xin <rxin@databricks.com  wrote:
> Please vote on releasing the following candidate as Apache Spark version 2.0.0. The vote is open until Friday, July 22, 2016 at 20:00 PDT and passes if a majority of at least 3 +1 PMC votes are cast.
> 
> [ ] +1 Release this package as Apache Spark 2.0.0
> [ ] -1 Do not release this package because ...
> 
> 
> The tag to be voted on is v2.0.0-rc5 (13650fc58e1fcf2cf2a26ba11c819185ae1acc1f).
> 
> This release candidate resolves ~2500 issues: https://s.apache.org/spark-2.0.0-jira  
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc5-bin/  
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc  
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1195/  
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc5-docs/  
> 
> =================================
> How can I help test this release?
> =================================
> If you are a Spark user, you can help us test this release by taking an existing Spark workload and running on this release candidate, then reporting any regressions from 1.x.
> 
> ==========================================
> What justifies a -1 vote for this release?
> ==========================================
> Critical bugs impacting major functionalities.
> 
> Bugs already present in 1.x, missing features, or bugs related to new features will not necessarily block this release. Note that historically Spark documentation has been published on the website separately from the main release so we do not need to block the release due to documentation errors either.
> 
> 
> 
> 


Hi all,

The PMC recently voted to add Felix Cheung as a committer. Felix has been a major contributor to SparkR and we're excited to have him join officially. Congrats and welcome, Felix!

Matei
---------------------------------------------------------------------
To unsubscribe e-mail: dev-unsubscribe@spark.apache.org


I think you should ask legal about how to have some Maven artifacts for these. Both Ganglia and Kinesis are very widely used, so it's weird to ask users to build them from source. Maybe the Maven artifacts can be marked as being under a different license?

In the initial discussion for LEGAL-198, we were told the following:

"If the component that uses this dependency is not required for the rest of Spark to function then you can have a subproject to build the component. See http://www.apache.org/legal/resolved.html#optional. This means you will have to provide instructions for users to enable the optional component (which IMO should provide pointers to the licensing)."

It's not clear whether "enable the optional component" means "every user must build it from source", or whether we could tell users "here's a Maven coordinate you can add to your project if you're okay with the licensing".

Matei

> On Sep 7, 2016, at 11:35 AM, Sean Owen  wrote:
> 
> (Credit to Luciano for pointing it out)
> 
> Yes it's clear why the assembly can't be published but I had the same
> question about the non-assembly Kinesis (and ganglia) artifact,
> because the published artifact has no code from Kinesis.
> 
> See the related discussion at
> https://issues.apache.org/jira/browse/LEGAL-198 ; the point I took
> from there is that the Spark Kinesis artifact is optional with respect
> to Spark, but still something published by Spark, and it requires the
> Amazon-licensed code non-optionally.
> 
> I'll just ask that question to confirm or deny.
> 
> (It also has some background on why the Amazon License is considered
> "Category X" in ASF policy due to field of use restrictions. I myself
> take that as read rather than know the details of that decision.)
> 
> On Wed, Sep 7, 2016 at 6:44 PM, Cody Koeninger  wrote:
>> I don't see a reason to remove the non-assembly artifact, why would
>> you?  You're not distributing copies of Amazon licensed code, and the
>> Amazon license goes out of its way not to over-reach regarding
>> derivative works.
>> 
>> This seems pretty clearly to fall in the spirit of
>> 
>> http://www.apache.org/legal/resolved.html#optional
>> 
>> I certainly think the majority of Spark users will still want to use
>> Spark without adding Kinesis
>> 
>> On Wed, Sep 7, 2016 at 3:29 AM, Sean Owen  wrote:
>>> It's worth calling attention to:
>>> 
>>> https://issues.apache.org/jira/browse/SPARK-17418
>>> https://issues.apache.org/jira/browse/SPARK-17422
>>> 
>>> It looks like we need to at least not publish the kinesis *assembly*
>>> Maven artifact because it contains Amazon Software Licensed-code
>>> directly.
>>> 
>>> However there's a reasonably strong reason to believe that we'd have
>>> to remove the non-assembly Kinesis artifact too, as well as the
>>> Ganglia one. This doesn't mean it goes away from the project, just
>>> means it would no longer be published as a Maven artifact. (These have
>>> never been bundled in the main Spark artifacts.)
>>> 
>>> I wanted to give a heads up to see if anyone a) believes this
>>> conclusion is wrong or b) wants to take it up with legal@? I'm
>>> inclined to believe we have to remove them given the interpretation
>>> Luciano has put forth.
>>> 
>>> Sean
>>> 
>>> ---------------------------------------------------------------------
>>> To unsubscribe e-mail: dev-unsubscribe@spark.apache.org
>>> 
> 
> ---------------------------------------------------------------------
> To unsubscribe e-mail: dev-unsubscribe@spark.apache.org
> 


---------------------------------------------------------------------
To unsubscribe e-mail: dev-unsubscribe@spark.apache.org


The question is just whether the metadata and instructions involving these Maven packages counts as sufficient to tell the user that they have different licensing terms. For example, our Ganglia package was called spark-ganglia-lgpl (so you'd notice it's a different license even from its name), and our Kinesis one was called spark-streaming-kinesis-asl, and our docs both mentioned these were under different licensing terms. But is that enough? That's the question.

Matei

> On Sep 7, 2016, at 2:05 PM, Cody Koeninger  wrote:
> 
> To be clear, "safe" has very little to do with this.
> 
> It's pretty clear that there's very little risk of the spark module
> for kinesis being considered a derivative work, much less all of
> spark.
> 
> The use limitation in 3.3 that caused the amazon license to be put on
> the apache X list also doesn't have anything to do with a legal safety
> risk here.  Really, what are you going to use a kinesis connector for,
> except for connecting to kinesis?
> 
> 
> On Wed, Sep 7, 2016 at 2:41 PM, Luciano Resende  wrote:
>> 
>> 
>> On Wed, Sep 7, 2016 at 12:20 PM, Mridul Muralidharan  wrote:
>>> 
>>> 
>>> It is good to get clarification, but the way I read it, the issue is
>>> whether we publish it as official Apache artifacts (in maven, etc).
>>> 
>>> Users can of course build it directly (and we can make it easy to do so) -
>>> as they are explicitly agreeing to additional licenses.
>>> 
>>> Regards
>>> Mridul
>>> 
>> 
>> +1, by providing instructions on how the user would build, and attaching the
>> license details on the instructions, we are then safe on the legal aspects
>> of it.
>> 
>> 
>> 
>> --
>> Luciano Resende
>> http://twitter.com/lresende1975
>> http://lresende.blogspot.com/


---------------------------------------------------------------------
To unsubscribe e-mail: dev-unsubscribe@spark.apache.org


This source is meant to be used for a shared file system such as HDFS or NFS, where both the driver and the workers can see the same folders. There's no support in Spark for just working with local files on different workers.

Matei

> On Sep 8, 2016, at 2:23 AM, Jacek Laskowski  wrote:
> 
> Hi Steve,
> 
> Thank you for more source-oriented answer. Helped but didn't explain
> the reason for such eagerness. The file(s) might not be on the driver
> but on executors only where the Spark job(s) run. I don't see why
> Spark should check the file(s) regardless of glob pattern being used.
> 
> You see my way of thinking?
> 
> Pozdrawiam,
> Jacek Laskowski
> ----
> https://medium.com/@jaceklaskowski/
> Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
> Follow me at https://twitter.com/jaceklaskowski
> 
> 
> On Thu, Sep 8, 2016 at 11:20 AM, Steve Loughran  wrote:
>> failfast generally means that you find problems sooner rather than later, and here, potentially, that your code runs but simply returns empty data without any obvious cue as to what is wrong.
>> 
>> As is always good in OSS, follow those stack trace links to see what they say:
>> 
>>        // Check whether the path exists if it is not a glob pattern.
>>        // For glob pattern, we do not check it because the glob pattern might only make sense
>>        // once the streaming job starts and some upstream source starts dropping data.
>> 
>> If you specify a glob pattern, you'll get the late check at the expense of the risk of that empty data source if the pattern is wrong. Something like "/var/log\s" would suffice, as the presence of the backslash is enough for SparkHadoopUtil.isGlobPath() to conclude that its something for the globber.
>> 
>> 
>>> On 8 Sep 2016, at 07:33, Jacek Laskowski  wrote:
>>> 
>>> Hi,
>>> 
>>> I'm wondering what's the rationale for checking the path option
>>> eagerly in FileStreamSource? My thinking is that until start is called
>>> there's no processing going on that is supposed to happen on executors
>>> (not the driver) with the path available.
>>> 
>>> I could (and perhaps should) use dfs but IMHO that just hides the real
>>> question of the text source eagerness.
>>> 
>>> Please help me understand the rationale of the choice. Thanks!
>>> 
>>> scala> spark.version
>>> res0: String = 2.1.0-SNAPSHOT
>>> 
>>> scala> spark.readStream.format("text").load("/var/logs")
>>> org.apache.spark.sql.AnalysisException: Path does not exist: /var/logs;
>>> at org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:229)
>>> at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:81)
>>> at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:81)
>>> at org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:30)
>>> at org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:142)
>>> at org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:153)
>>> ... 48 elided
>>> 
>>> Pozdrawiam,
>>> Jacek Laskowski
>>> ----
>>> https://medium.com/@jaceklaskowski/
>>> Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
>>> Follow me at https://twitter.com/jaceklaskowski
>>> 
>>> ---------------------------------------------------------------------
>>> To unsubscribe e-mail: dev-unsubscribe@spark.apache.org
>>> 
>>> 
>> 
> 
> ---------------------------------------------------------------------
> To unsubscribe e-mail: dev-unsubscribe@spark.apache.org
> 


---------------------------------------------------------------------
To unsubscribe e-mail: dev-unsubscribe@spark.apache.org


+1

Matei

> On Sep 25, 2016, at 1:25 PM, Josh Rosen  wrote:
> 
> +1
> 
> On Sun, Sep 25, 2016 at 1:16 PM Yin Huai <yhuai@databricks.com  wrote:
> +1
> 
> On Sun, Sep 25, 2016 at 11:40 AM, Dongjoon Hyun <dongjoon@apache.org  wrote:
> +1 (non binding)
> 
> RC3 is compiled and tested on the following two systems, too. All tests passed.
> 
> * CentOS 7.2 / Oracle JDK 1.8.0_77 / R 3.3.1
>    with -Pyarn -Phadoop-2.7 -Pkinesis-asl -Phive -Phive-thriftserver -Dsparkr
> * CentOS 7.2 / Open JDK 1.8.0_102
>    with -Pyarn -Phadoop-2.7 -Pkinesis-asl -Phive -Phive-thriftserver
> 
> Cheers,
> Dongjoon
> 
> 
> 
> On Saturday, September 24, 2016, Reynold Xin <rxin@databricks.com  wrote:
> Please vote on releasing the following candidate as Apache Spark version 2.0.1. The vote is open until Tue, Sep 27, 2016 at 15:30 PDT and passes if a majority of at least 3+1 PMC votes are cast.
> 
> [ ] +1 Release this package as Apache Spark 2.0.1
> [ ] -1 Do not release this package because ...
> 
> 
> The tag to be voted on is v2.0.1-rc3 (9d28cc10357a8afcfb2fa2e6eecb5c2cc2730d17)
> 
> This release candidate resolves 290 issues: https://s.apache.org/spark-2.0.1-jira  
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc3-bin/  
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc  
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1201/  
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-releases/spark-2.0.1-rc3-docs/  
> 
> Q: How can I help test this release?
> A: If you are a Spark user, you can help us test this release by taking an existing Spark workload and running on this release candidate, then reporting any regressions from 2.0.0.
> 
> Q: What justifies a -1 vote for this release?
> A: This is a maintenance release in the 2.0.x series.  Bugs already present in 2.0.0, missing features, or bugs related to new features will not necessarily block this release.
> 
> Q: What fix version should I use for patches merging into branch-2.0 from now on?
> A: Please mark the fix version as 2.0.2, rather than 2.0.1. If a new RC (i.e. RC4) is cut, I will change the fix version of those patches to 2.0.1.
> 
> 
> 


Hey Cody,

Thanks for bringing these things up. You're talking about quite a few different things here, but let me get to them each in turn.

1) About technical / design discussion -- I fully agree that everything big should go through a lot of review, and I like the idea of a more formal way to propose and comment on larger features. So far, all of this has been done through JIRA, but as a start, maybe marking JIRAs as large (we often use Umbrella for this) and also opening a thread on the list about each such JIRA would help. For Structured Streaming in particular, FWIW, there was a pretty complete doc on the proposed semantics at https://issues.apache.org/jira/browse/SPARK-8360 since March. But it's true that other things such as the Kafka source for it didn't have as much design on JIRA. Nonetheless, this component is still early on and there's still a lot of time to change it, which is happening.

2) About what people say at Reactive Summit -- there will always be trolls, but just ignore them and build a great project. Those of us involved in the project for a while have long seen similar stuff, e.g. a prominent company saying Spark doesn't scale past 100 nodes when there were many documented instances to the contrary, and the best answer is to just make the project better. This same company, if you read their website now, recommends Apache Spark for most anything. For streaming in particular, there is a lot of confusion because many of the concepts aren't well-defined (e.g. what is "at least once", etc), and it's also a crowded space. But Spark Streaming prioritizes a few things that it does very well: correctness (you can easily tell what the app will do, and it does the same thing despite failures), ease of programming (which also requires correctness), and scalability. We should of course both explain what it does in more places and work on improving it where needed (e.g. adding a higher level API with Structured Streaming and built-in primitives for external timestamps).

3) About number and diversity of committers -- the PMC is always working to expand these, and you should email people on the PMC (or even the whole list) if you have people you'd like to propose. In general I think nearly all committers added in the past year were from organizations that haven't long been involved in Spark, and the number of committers continues to grow pretty fast.

4) Finally, about better organizing JIRA, marking dead issues, etc, this would be great and I think we just need a concrete proposal for how to do it. It would be best to point to an existing process that someone else has used here BTW so that we can see it in action.

Matei

> On Oct 6, 2016, at 7:51 PM, Cody Koeninger  wrote:
> 
> I love Spark.  3 or 4 years ago it was the first distributed computing
> environment that felt usable, and the community was welcoming.
> 
> But I just got back from the Reactive Summit, and this is what I observed:
> 
> - Industry leaders on stage making fun of Spark's streaming model
> - Open source project leaders saying they looked at Spark's governance
> as a model to avoid
> - Users saying they chose Flink because it was technically superior
> and they couldn't get any answers on the Spark mailing lists
> 
> Whether you agree with the substance of any of this, when this stuff
> gets repeated enough people will believe it.
> 
> Right now Spark is suffering from its own success, and I think
> something needs to change.
> 
> - We need a clear process for planning significant changes to the codebase.
> I'm not saying you need to adopt Kafka Improvement Proposals exactly,
> but you need a documented process with a clear outcome (e.g. a vote).
> Passing around google docs after an implementation has largely been
> decided on doesn't cut it.
> 
> - All technical communication needs to be public.
> Things getting decided in private chat, or when 1/3 of the committers
> work for the same company and can just talk to each other...
> Yes, it's convenient, but it's ultimately detrimental to the health of
> the project.
> The way structured streaming has played out has shown that there are
> significant technical blind spots (myself included).
> One way to address that is to get the people who have domain knowledge
> involved, and listen to them.
> 
> - We need more committers, and more committer diversity.
> Per committer there are, what, more than 20 contributors and 10 new
> jira tickets a month?  It's too much.
> There are people (I am _not_ referring to myself) who have been around
> for years, contributed thousands of lines of code, helped educate the
> public around Spark... and yet are never going to be voted in.
> 
> - We need a clear process for managing volunteer work.
> Too many tickets sit around unowned, unclosed, uncertain.
> If someone proposed something and it isn't up to snuff, tell them and
> close it.  It may be blunt, but it's clearer than "silent no".
> If someone wants to work on something, let them own the ticket and set
> a deadline. If they don't meet it, close it or reassign it.
> 
> This is not me putting on an Apache Bureaucracy hat.  This is me
> saying, as a fellow hacker and loyal dissenter, something is wrong
> with the culture and process.
> 
> Please, let's change it.
> 
> ---------------------------------------------------------------------
> To unsubscribe e-mail: dev-unsubscribe@spark.apache.org
> 


---------------------------------------------------------------------
To unsubscribe e-mail: dev-unsubscribe@spark.apache.org


For the improvement proposals, I think one major point was to make them really visible to users who are not contributors, so we should do more than sending stuff to dev@. One very lightweight idea is to have a new type of JIRA called a SIP and have a link to a filter that shows all such JIRAs from http://spark.apache.org. I also like the idea of SIP and design doc templates (in fact many projects have them).

Matei

> On Oct 7, 2016, at 10:38 AM, Reynold Xin  wrote:
> 
> I called Cody last night and talked about some of the topics in his email. It became clear to me Cody genuinely cares about the project.
> 
> Some of the frustrations come from the success of the project itself becoming very "hot", and it is difficult to get clarity from people who don't dedicate all their time to Spark. In fact, it is in some ways similar to scaling an engineering team in a successful startup: old processes that worked well might not work so well when it gets to a certain size, cultures can get diluted, building culture vs building process, etc.
> 
> I also really like to have a more visible process for larger changes, especially major user facing API changes. Historically we upload design docs for major changes, but it is not always consistent and difficult to quality of the docs, due to the volunteering nature of the organization.
> 
> Some of the more concrete ideas we discussed focus on building a culture to improve clarity:
> 
> - Process: Large changes should have design docs posted on JIRA. One thing Cody and I didn't discuss but an idea that just came to me is we should create a design doc template for the project and ask everybody to follow. The design doc template should also explicitly list goals and non-goals, to make design doc more consistent.
> 
> - Process: Email dev@ to solicit feedback. We have some this with some changes, but again very inconsistent. Just posting something on JIRA isn't sufficient, because there are simply too many JIRAs and the signal get lost in the noise. While this is generally impossible to enforce because we can't force all volunteers to conform to a process (or they might not even be aware of this),  those who are more familiar with the project can help by emailing the dev@ when they see something that hasn't been.
> 
> - Culture: The design doc author(s) should be open to feedback. A design doc should serve as the base for discussion and is by no means the final design. Of course, this does not mean the author has to accept every feedback. They should also be comfortable accepting / rejecting ideas on technical grounds.
> 
> - Process / Culture: For major ongoing projects, it can be useful to have some monthly Google hangouts that are open to the world. I am actually not sure how well this will work, because of the volunteering nature and we need to adjust for timezones for people across the globe, but it seems worth trying.
> 
> - Culture: Contributors (including committers) should be more direct in setting expectations, including whether they are working on a specific issue, whether they will be working on a specific issue, and whether an issue or pr or jira should be rejected. Most people I know in this community are nice and don't enjoy telling other people no, but it is often more annoying to a contributor to not know anything than getting a no.
> 
> 
> On Fri, Oct 7, 2016 at 10:03 AM, Matei Zaharia <matei.zaharia@gmail.com  wrote:
> 
> Love the idea of a more visible "Spark Improvement Proposal" process that solicits user input on new APIs. For what it's worth, I don't think committers are trying to minimize their own work -- every committer cares about making the software useful for users. However, it is always hard to get user input and so it helps to have this kind of process. I've certainly looked at the *IPs a lot in other software I use just to see the biggest things on the roadmap.
> 
> When you're talking about "changing interfaces", are you talking about public or internal APIs? I do think many people hate changing public APIs and I actually think that's for the best of the project. That's a technical debate, but basically, the worst thing when you're using a piece of software is that the developers constantly ask you to rewrite your app to update to a new version (and thus benefit from bug fixes, etc). Cue anyone who's used Protobuf, or Guava. The "let's get everyone to change their code this release" model works well within a single large company, but doesn't work well for a community, which is why nearly all *very* widely used programming interfaces (I'm talking things like Java standard library, Windows API, etc) almost *never* break backwards compatibility. All this is done within reason though, e.g. we do change things in major releases (2.x, 3.x, etc).
> 
> 


This makes a lot of sense; just to comment on a few things:

> - More committers
> Just looking at the ratio of committers to open tickets, or committers
> to contributors, I don't think you have enough human power.
> I realize this is a touchy issue.  I don't have dog in this fight,
> because I'm not on either coast nor in a big company that views
> committership as a political thing.  I just think you need more people
> to do the work, and more diversity of viewpoint.
> It's unfortunate that the Apache governance process involves giving
> someone all the keys or none of the keys, but until someone really
> starts screwing up, I think it's better to err on the side of
> accepting hard-working people.

This is something the PMC is actively discussing. Historically, we've added committers when people contributed a new module or feature, basically to the point where other developers are asking them to review changes in that area (https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-BecomingaCommitter). For example, we added the original authors of GraphX when we merged in GraphX, the authors of new ML algorithms, etc. However, there's a good argument that some areas are simply not covered well now and we should add people there. Also, as the project has grown, there are also more people who focus on smaller fixes and are nonetheless contributing a lot.

> - Each major area of the code needs at least one person who cares
> about it that is empowered with a vote, otherwise decisions get made
> that don't make technical sense.
> I don't know if anyone with a vote is shepherding GraphX (or maybe
> it's just dead), the Mesos relationship has always been weird, no one
> with a vote really groks Kafka.
> marmbrus and zsxwing are getting there quickly on the Kafka side, and
> I appreciate it, but it's been bad for a while.
> Because I don't have any political power, my response to seeing things
> that I know are technically dangerous has been to yell really loud
> until someone listens, which sucks for everyone involved.
> I already apologized to Michael privately; Ryan, I'm sorry, it's not about you.
> This seems pretty straightforward to fix, if politically awkward:
> those people exist, just give them a vote.
> Failing that, listen the first or second time they say something not
> the third or fourth, and if it doesn't make sense, ask.

Just as a note here -- it's true that some areas are not super well covered, but I also hope to avoid a situation where people have to yell to be listened to. I can't say anything about *all* technical discussions we've ever had, but historically, people have been able to comment on the design of many things without yelling. This is actually important because a culture of having to yell can drive away contributors. So it's awesome that you yelled about the Kafka source stuff, but at the same time, hopefully we make these types of things work without yelling. This would be a problem even if there were committers with more expertise in each area -- what if someone disagrees with the committers?

Matei


---------------------------------------------------------------------
To unsubscribe e-mail: dev-unsubscribe@spark.apache.org


Sounds good. Just to comment on the compatibility part:

> I meant changing public user interfaces.  I think the first design is
> unlikely to be right, because it's done at a time when you have the
> least information.  As a user, I find it considerably more frustrating
> to be unable to use a tool to get my job done, than I do having to
> make minor changes to my code in order to take advantage of features.
> I've seen committers be seriously reluctant to allow changes to
> @experimental code that are needed in order for it to really work
> right.  You need to be able to iterate, and if people on both sides of
> the fence aren't going to respect that some newer apis are subject to
> change, then why even mark them as such?
> 
> Ideally a finished SIP should give me a checklist of things that an
> implementation must do, and things that it doesn't need to do.
> Contributors/committers should be seriously discouraged from putting
> out a version 0.1 that doesn't have at least a prototype
> implementation of all those things, especially if they're then going
> to argue against interface changes necessary to get the the rest of
> the things done in the 0.2 version.

Experimental APIs and alpha components are indeed supposed to be changeable (https://cwiki.apache.org/confluence/display/SPARK/Spark+Versioning+Policy). Maybe people are being too conservative in some cases, but I do want to note that regardless of what precise policy we try to write down, this type of issue will ultimately be a judgment call. Is it worth making a small cosmetic change in an API that's marked experimental, but has been used widely for a year? Perhaps not. Is it worth making it in something one month old, or even in an older API as we move to 2.0? Maybe yes. I think we should just discuss each one (start an email thread if resolving it on JIRA is too complex) and perhaps be more religious about making things non-experimental when we think they're done.

Matei


> 
> 
> On Fri, Oct 7, 2016 at 2:18 PM, Reynold Xin  wrote:
>> I like the lightweight proposal to add a SIP label.
>> 
>> During Spark 2.0 development, Tom (Graves) and I suggested using wiki to
>> track the list of major changes, but that never really materialized due to
>> the overhead. Adding a SIP label on major JIRAs and then link to them
>> prominently on the Spark website makes a lot of sense.
>> 
>> 
>> On Fri, Oct 7, 2016 at 10:50 AM, Matei Zaharia  wrote:
>>> 
>>> For the improvement proposals, I think one major point was to make them
>>> really visible to users who are not contributors, so we should do more than
>>> sending stuff to dev@. One very lightweight idea is to have a new type of
>>> JIRA called a SIP and have a link to a filter that shows all such JIRAs from
>>> http://spark.apache.org. I also like the idea of SIP and design doc
>>> templates (in fact many projects have them).
>>> 
>>> Matei
>>> 
>>> On Oct 7, 2016, at 10:38 AM, Reynold Xin  wrote:
>>> 
>>> I called Cody last night and talked about some of the topics in his email.
>>> It became clear to me Cody genuinely cares about the project.
>>> 
>>> Some of the frustrations come from the success of the project itself
>>> becoming very "hot", and it is difficult to get clarity from people who
>>> don't dedicate all their time to Spark. In fact, it is in some ways similar
>>> to scaling an engineering team in a successful startup: old processes that
>>> worked well might not work so well when it gets to a certain size, cultures
>>> can get diluted, building culture vs building process, etc.
>>> 
>>> I also really like to have a more visible process for larger changes,
>>> especially major user facing API changes. Historically we upload design docs
>>> for major changes, but it is not always consistent and difficult to quality
>>> of the docs, due to the volunteering nature of the organization.
>>> 
>>> Some of the more concrete ideas we discussed focus on building a culture
>>> to improve clarity:
>>> 
>>> - Process: Large changes should have design docs posted on JIRA. One thing
>>> Cody and I didn't discuss but an idea that just came to me is we should
>>> create a design doc template for the project and ask everybody to follow.
>>> The design doc template should also explicitly list goals and non-goals, to
>>> make design doc more consistent.
>>> 
>>> - Process: Email dev@ to solicit feedback. We have some this with some
>>> changes, but again very inconsistent. Just posting something on JIRA isn't
>>> sufficient, because there are simply too many JIRAs and the signal get lost
>>> in the noise. While this is generally impossible to enforce because we can't
>>> force all volunteers to conform to a process (or they might not even be
>>> aware of this),  those who are more familiar with the project can help by
>>> emailing the dev@ when they see something that hasn't been.
>>> 
>>> - Culture: The design doc author(s) should be open to feedback. A design
>>> doc should serve as the base for discussion and is by no means the final
>>> design. Of course, this does not mean the author has to accept every
>>> feedback. They should also be comfortable accepting / rejecting ideas on
>>> technical grounds.
>>> 
>>> - Process / Culture: For major ongoing projects, it can be useful to have
>>> some monthly Google hangouts that are open to the world. I am actually not
>>> sure how well this will work, because of the volunteering nature and we need
>>> to adjust for timezones for people across the globe, but it seems worth
>>> trying.
>>> 
>>> - Culture: Contributors (including committers) should be more direct in
>>> setting expectations, including whether they are working on a specific
>>> issue, whether they will be working on a specific issue, and whether an
>>> issue or pr or jira should be rejected. Most people I know in this community
>>> are nice and don't enjoy telling other people no, but it is often more
>>> annoying to a contributor to not know anything than getting a no.
>>> 
>>> 
>>> On Fri, Oct 7, 2016 at 10:03 AM, Matei Zaharia  wrote:
>>>> 
>>>> 
>>>> Love the idea of a more visible "Spark Improvement Proposal" process that
>>>> solicits user input on new APIs. For what it's worth, I don't think
>>>> committers are trying to minimize their own work -- every committer cares
>>>> about making the software useful for users. However, it is always hard to
>>>> get user input and so it helps to have this kind of process. I've certainly
>>>> looked at the *IPs a lot in other software I use just to see the biggest
>>>> things on the roadmap.
>>>> 
>>>> When you're talking about "changing interfaces", are you talking about
>>>> public or internal APIs? I do think many people hate changing public APIs
>>>> and I actually think that's for the best of the project. That's a technical
>>>> debate, but basically, the worst thing when you're using a piece of software
>>>> is that the developers constantly ask you to rewrite your app to update to a
>>>> new version (and thus benefit from bug fixes, etc). Cue anyone who's used
>>>> Protobuf, or Guava. The "let's get everyone to change their code this
>>>> release" model works well within a single large company, but doesn't work
>>>> well for a community, which is why nearly all *very* widely used programming
>>>> interfaces (I'm talking things like Java standard library, Windows API, etc)
>>>> almost *never* break backwards compatibility. All this is done within reason
>>>> though, e.g. we do change things in major releases (2.x, 3.x, etc).
>>> 
>>> 
>>> 
>>> 
>> 


---------------------------------------------------------------------
To unsubscribe e-mail: dev-unsubscribe@spark.apache.org


Yup, this is the stuff that I found unclear. Thanks for clarifying here, but we should also clarify it in the writeup. In particular:

- Goals needs to be about user-facing behavior ("people" is broad)

- I'd rename Rejected Goals to Non-Goals. Otherwise someone will dig up one of these and say "Spark's developers have officially rejected X, which our awesome system has".

- For user-facing stuff, I think you need a section on API. Virtually all other *IPs I've seen have that.

- I'm still not sure why the strategy section is needed if the purpose is to define user-facing behavior -- unless this is the strategy for setting the goals or for defining the API. That sounds squarely like a design doc issue. In some sense, who cares whether the proposal is technically feasible right now? If it's infeasible, that will be discovered later during design and implementation. Same thing with rejected strategies -- listing some of those is definitely useful sometimes, but if you make this a *required* section, people are just going to fill it in with bogus stuff (I've seen this happen before).

Matei

> On Oct 9, 2016, at 2:14 PM, Cody Koeninger  wrote:
> 
> So to focus the discussion on the specific strategy I'm suggesting,
> documented at
> 
> https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md
> 
> "Goals: What must this allow people to do, that they can't currently?"
> 
> Is it unclear that this is focusing specifically on people-visible behavior?
> 
> Rejected goals -  are important because otherwise people keep trying
> to argue about scope.  Of course you can change things later with a
> different SIP and different vote, the point is to focus.
> 
> Use cases - are something that people are going to bring up in
> discussion.  If they aren't clearly documented as a goal ("This must
> allow me to connect using SSL"), they should be added.
> 
> Internal architecture - if the people who need specific behavior are
> implementers of other parts of the system, that's fine.
> 
> Rejected strategies - If you have none of these, you have no evidence
> that the proponent didn't just go with the first thing they had in
> mind (or have already implemented), which is a big problem currently.
> Approval isn't binding as to specifics of implementation, so these
> aren't handcuffs.  The goals are the contract, the strategy is
> evidence that contract can actually be met.
> 
> Design docs - I'm not touching design docs.  The markdown file I
> linked specifically says of the strategy section "This is not a full
> design document."  Is this unclear?  Design docs can be worked on
> obviously, but that's not what I'm concerned with here.
> 
> 
> 
> 
> On Sun, Oct 9, 2016 at 2:34 PM, Matei Zaharia  wrote:
>> Hi Cody,
>> 
>> I think this would be a lot more concrete if we had a more detailed template
>> for SIPs. Right now, it's not super clear what's in scope -- e.g. are  they
>> a way to solicit feedback on the user-facing behavior or on the internals?
>> "Goals" can cover both things. I've been thinking of SIPs more as Product
>> Requirements Docs (PRDs), which focus on *what* a code change should do as
>> opposed to how.
>> 
>> In particular, here are some things that you may or may not consider in
>> scope for SIPs:
>> 
>> - Goals and non-goals: This is definitely in scope, and IMO should focus on
>> user-visible behavior (e.g. "system supports SQL window functions" or
>> "system continues working if one node fails"). BTW I wouldn't say "rejected
>> goals" because some of them might become goals later, so we're not
>> definitively rejecting them.
>> 
>> - Public API: Probably should be included in most SIPs unless it's too large
>> to fully specify then (e.g. "let's add an ML library").
>> 
>> - Use cases: I usually find this very useful in PRDs to better communicate
>> the goals.
>> 
>> - Internal architecture: This is usually *not* a thing users can easily
>> comment on and it sounds more like a design doc item. Of course it's
>> important to show that the SIP is feasible to implement. One exception,
>> however, is that I think we'll have some SIPs primarily on internals (e.g.
>> if somebody wants to refactor Spark's query optimizer or something).
>> 
>> - Rejected strategies: I personally wouldn't put this, because what's the
>> point of voting to reject a strategy before you've really begun designing
>> and implementing something? What if you discover that the strategy is
>> actually better when you start doing stuff?
>> 
>> At a super high level, it depends on whether you want the SIPs to be PRDs
>> for getting some quick feedback on the goals of a feature before it is
>> designed, or something more like full-fledged design docs (just a more
>> visible design doc for bigger changes). I looked at Kafka's KIPs, and they
>> actually seem to be more like design docs. This can work too but it does
>> require more work from the proposer and it can lead to the same problems you
>> mentioned with people already having a design and implementation in mind.
>> 
>> Basically, the question is, are you trying to iterate faster on design by
>> adding a step for user feedback earlier? Or are you just trying to make
>> design docs for key features more visible (and their approval more formal)?
>> 
>> BTW note that in either case, I'd like to have a template for design docs
>> too, which should also include goals. I think that would've avoided some of
>> the issues you brought up.
>> 
>> Matei
>> 
>> On Oct 9, 2016, at 10:40 AM, Cody Koeninger  wrote:
>> 
>> Here's my specific proposal (meta-proposal?)
>> 
>> Spark Improvement Proposals (SIP)
>> 
>> 
>> Background:
>> 
>> The current problem is that design and implementation of large features are
>> often done in private, before soliciting user feedback.
>> 
>> When feedback is solicited, it is often as to detailed design specifics, not
>> focused on goals.
>> 
>> When implementation does take place after design, there is often
>> disagreement as to what goals are or are not in scope.
>> 
>> This results in commits that don't fully meet user needs.
>> 
>> 
>> Goals:
>> 
>> - Ensure user, contributor, and committer goals are clearly identified and
>> agreed upon, before implementation takes place.
>> 
>> - Ensure that a technically feasible strategy is chosen that is likely to
>> meet the goals.
>> 
>> 
>> Rejected Goals:
>> 
>> - SIPs are not for detailed design.  Design by committee doesn't work.
>> 
>> - SIPs are not for every change.  We dont need that much process.
>> 
>> 
>> Strategy:
>> 
>> My suggestion is outlined as a Spark Improvement Proposal process documented
>> at
>> 
>> https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md
>> 
>> Specifics of Jira manipulation are an implementation detail we can figure
>> out.
>> 
>> I'm suggesting voting; the need here is for a _clear_ outcome.
>> 
>> 
>> Rejected Strategies:
>> 
>> Having someone who understands the problem implement it first works, but
>> only if significant iteration after user feedback is allowed.
>> 
>> Historically this has been problematic due to pressure to limit public api
>> changes.
>> 
>> 
>> On Fri, Oct 7, 2016 at 5:16 PM, Reynold Xin  wrote:
>>> 
>>> Alright looks like there are quite a bit of support. We should wait to
>>> hear from more people too.
>>> 
>>> To push this forward, Cody and I will be working together in the next
>>> couple of weeks to come up with a concrete, detailed proposal on what this
>>> entails, and then we can discuss this the specific proposal as well.
>>> 
>>> 
>>> On Fri, Oct 7, 2016 at 2:29 PM, Cody Koeninger  wrote:
>>>> 
>>>> Yeah, in case it wasn't clear, I was talking about SIPs for major
>>>> user-facing or cross-cutting changes, not minor feature adds.
>>>> 
>>>> On Fri, Oct 7, 2016 at 3:58 PM, Stavros Kontopoulos
>>>>  wrote:
>>>>> 
>>>>> +1 to the SIP label as long as it does not slow down things and it
>>>>> targets optimizing efforts, coordination etc. For example really small
>>>>> features should not need to go through this process (assuming they dont
>>>>> touch public interfaces)  or re-factorings and hope it will be kept this
>>>>> way. So as a guideline doc should be provided, like in the KIP case.
>>>>> 
>>>>> IMHO so far aside from tagging things and linking them elsewhere simply
>>>>> having design docs and prototypes implementations in PRs is not something
>>>>> that has not worked so far. What is really a pain in many projects out there
>>>>> is discontinuity in progress of PRs, missing features, slow reviews which is
>>>>> understandable to some extent... it is not only about Spark but things can
>>>>> be improved for sure for this project in particular as already stated.
>>>>> 
>>>>> On Fri, Oct 7, 2016 at 11:14 PM, Cody Koeninger  wrote:
>>>>>> 
>>>>>> +1 to adding an SIP label and linking it from the website.  I think it
>>>>>> needs
>>>>>> 
>>>>>> - template that focuses it towards soliciting user goals / non goals
>>>>>> - clear resolution as to which strategy was chosen to pursue.  I'd
>>>>>> recommend a vote.
>>>>>> 
>>>>>> Matei asked me to clarify what I meant by changing interfaces, I think
>>>>>> it's directly relevant to the SIP idea so I'll clarify here, and split
>>>>>> a thread for the other discussion per Nicholas' request.
>>>>>> 
>>>>>> I meant changing public user interfaces.  I think the first design is
>>>>>> unlikely to be right, because it's done at a time when you have the
>>>>>> least information.  As a user, I find it considerably more frustrating
>>>>>> to be unable to use a tool to get my job done, than I do having to
>>>>>> make minor changes to my code in order to take advantage of features.
>>>>>> I've seen committers be seriously reluctant to allow changes to
>>>>>> @experimental code that are needed in order for it to really work
>>>>>> right.  You need to be able to iterate, and if people on both sides of
>>>>>> the fence aren't going to respect that some newer apis are subject to
>>>>>> change, then why even mark them as such?
>>>>>> 
>>>>>> Ideally a finished SIP should give me a checklist of things that an
>>>>>> implementation must do, and things that it doesn't need to do.
>>>>>> Contributors/committers should be seriously discouraged from putting
>>>>>> out a version 0.1 that doesn't have at least a prototype
>>>>>> implementation of all those things, especially if they're then going
>>>>>> to argue against interface changes necessary to get the the rest of
>>>>>> the things done in the 0.2 version.
>>>>>> 
>>>>>> 
>>>>>> On Fri, Oct 7, 2016 at 2:18 PM, Reynold Xin  wrote:
>>>>>>> I like the lightweight proposal to add a SIP label.
>>>>>>> 
>>>>>>> During Spark 2.0 development, Tom (Graves) and I suggested using wiki
>>>>>>> to
>>>>>>> track the list of major changes, but that never really materialized
>>>>>>> due to
>>>>>>> the overhead. Adding a SIP label on major JIRAs and then link to them
>>>>>>> prominently on the Spark website makes a lot of sense.
>>>>>>> 
>>>>>>> 
>>>>>>> On Fri, Oct 7, 2016 at 10:50 AM, Matei Zaharia
>>>>>>>  wrote:
>>>>>>>> 
>>>>>>>> For the improvement proposals, I think one major point was to make
>>>>>>>> them
>>>>>>>> really visible to users who are not contributors, so we should do
>>>>>>>> more than
>>>>>>>> sending stuff to dev@. One very lightweight idea is to have a new
>>>>>>>> type of
>>>>>>>> JIRA called a SIP and have a link to a filter that shows all such
>>>>>>>> JIRAs from
>>>>>>>> http://spark.apache.org. I also like the idea of SIP and design doc
>>>>>>>> templates (in fact many projects have them).
>>>>>>>> 
>>>>>>>> Matei
>>>>>>>> 
>>>>>>>> On Oct 7, 2016, at 10:38 AM, Reynold Xin  wrote:
>>>>>>>> 
>>>>>>>> I called Cody last night and talked about some of the topics in his
>>>>>>>> email.
>>>>>>>> It became clear to me Cody genuinely cares about the project.
>>>>>>>> 
>>>>>>>> Some of the frustrations come from the success of the project itself
>>>>>>>> becoming very "hot", and it is difficult to get clarity from people
>>>>>>>> who
>>>>>>>> don't dedicate all their time to Spark. In fact, it is in some ways
>>>>>>>> similar
>>>>>>>> to scaling an engineering team in a successful startup: old
>>>>>>>> processes that
>>>>>>>> worked well might not work so well when it gets to a certain size,
>>>>>>>> cultures
>>>>>>>> can get diluted, building culture vs building process, etc.
>>>>>>>> 
>>>>>>>> I also really like to have a more visible process for larger
>>>>>>>> changes,
>>>>>>>> especially major user facing API changes. Historically we upload
>>>>>>>> design docs
>>>>>>>> for major changes, but it is not always consistent and difficult to
>>>>>>>> quality
>>>>>>>> of the docs, due to the volunteering nature of the organization.
>>>>>>>> 
>>>>>>>> Some of the more concrete ideas we discussed focus on building a
>>>>>>>> culture
>>>>>>>> to improve clarity:
>>>>>>>> 
>>>>>>>> - Process: Large changes should have design docs posted on JIRA. One
>>>>>>>> thing
>>>>>>>> Cody and I didn't discuss but an idea that just came to me is we
>>>>>>>> should
>>>>>>>> create a design doc template for the project and ask everybody to
>>>>>>>> follow.
>>>>>>>> The design doc template should also explicitly list goals and
>>>>>>>> non-goals, to
>>>>>>>> make design doc more consistent.
>>>>>>>> 
>>>>>>>> - Process: Email dev@ to solicit feedback. We have some this with
>>>>>>>> some
>>>>>>>> changes, but again very inconsistent. Just posting something on JIRA
>>>>>>>> isn't
>>>>>>>> sufficient, because there are simply too many JIRAs and the signal
>>>>>>>> get lost
>>>>>>>> in the noise. While this is generally impossible to enforce because
>>>>>>>> we can't
>>>>>>>> force all volunteers to conform to a process (or they might not even
>>>>>>>> be
>>>>>>>> aware of this),  those who are more familiar with the project can
>>>>>>>> help by
>>>>>>>> emailing the dev@ when they see something that hasn't been.
>>>>>>>> 
>>>>>>>> - Culture: The design doc author(s) should be open to feedback. A
>>>>>>>> design
>>>>>>>> doc should serve as the base for discussion and is by no means the
>>>>>>>> final
>>>>>>>> design. Of course, this does not mean the author has to accept every
>>>>>>>> feedback. They should also be comfortable accepting / rejecting
>>>>>>>> ideas on
>>>>>>>> technical grounds.
>>>>>>>> 
>>>>>>>> - Process / Culture: For major ongoing projects, it can be useful to
>>>>>>>> have
>>>>>>>> some monthly Google hangouts that are open to the world. I am
>>>>>>>> actually not
>>>>>>>> sure how well this will work, because of the volunteering nature and
>>>>>>>> we need
>>>>>>>> to adjust for timezones for people across the globe, but it seems
>>>>>>>> worth
>>>>>>>> trying.
>>>>>>>> 
>>>>>>>> - Culture: Contributors (including committers) should be more direct
>>>>>>>> in
>>>>>>>> setting expectations, including whether they are working on a
>>>>>>>> specific
>>>>>>>> issue, whether they will be working on a specific issue, and whether
>>>>>>>> an
>>>>>>>> issue or pr or jira should be rejected. Most people I know in this
>>>>>>>> community
>>>>>>>> are nice and don't enjoy telling other people no, but it is often
>>>>>>>> more
>>>>>>>> annoying to a contributor to not know anything than getting a no.
>>>>>>>> 
>>>>>>>> 
>>>>>>>> On Fri, Oct 7, 2016 at 10:03 AM, Matei Zaharia
>>>>>>>>  wrote:
>>>>>>>>> 
>>>>>>>>> 
>>>>>>>>> Love the idea of a more visible "Spark Improvement Proposal"
>>>>>>>>> process that
>>>>>>>>> solicits user input on new APIs. For what it's worth, I don't think
>>>>>>>>> committers are trying to minimize their own work -- every committer
>>>>>>>>> cares
>>>>>>>>> about making the software useful for users. However, it is always
>>>>>>>>> hard to
>>>>>>>>> get user input and so it helps to have this kind of process. I've
>>>>>>>>> certainly
>>>>>>>>> looked at the *IPs a lot in other software I use just to see the
>>>>>>>>> biggest
>>>>>>>>> things on the roadmap.
>>>>>>>>> 
>>>>>>>>> When you're talking about "changing interfaces", are you talking
>>>>>>>>> about
>>>>>>>>> public or internal APIs? I do think many people hate changing
>>>>>>>>> public APIs
>>>>>>>>> and I actually think that's for the best of the project. That's a
>>>>>>>>> technical
>>>>>>>>> debate, but basically, the worst thing when you're using a piece of
>>>>>>>>> software
>>>>>>>>> is that the developers constantly ask you to rewrite your app to
>>>>>>>>> update to a
>>>>>>>>> new version (and thus benefit from bug fixes, etc). Cue anyone
>>>>>>>>> who's used
>>>>>>>>> Protobuf, or Guava. The "let's get everyone to change their code
>>>>>>>>> this
>>>>>>>>> release" model works well within a single large company, but
>>>>>>>>> doesn't work
>>>>>>>>> well for a community, which is why nearly all *very* widely used
>>>>>>>>> programming
>>>>>>>>> interfaces (I'm talking things like Java standard library, Windows
>>>>>>>>> API, etc)
>>>>>>>>> almost *never* break backwards compatibility. All this is done
>>>>>>>>> within reason
>>>>>>>>> though, e.g. we do change things in major releases (2.x, 3.x, etc).
>>>>>>>> 
>>>>>>>> 
>>>>>>>> 
>>>>>>>> 
>>>>>>> 
>>>>>> 
>>>>>> ---------------------------------------------------------------------
>>>>>> To unsubscribe e-mail: dev-unsubscribe@spark.apache.org
>>>>>> 
>>>>> 
>>>>> 
>>>>> 
>>>>> --
>>>>> Stavros Kontopoulos
>>>>> Senior Software Engineer
>>>>> Lightbend, Inc.
>>>>> p:  +30 6977967274
>>>>> e: stavros.kontopoulos@lightbend.com
>>>>> 
>>>>> 
>>>> 
>>> 
>> 
>> 


---------------------------------------------------------------------
To unsubscribe e-mail: dev-unsubscribe@spark.apache.org


Well, I think there are a few things here that don't make sense. First, why should only committers submit SIPs? Development in the project should be open to all contributors, whether they're committers or not. Second, I think unrealistic goals can be found just by inspecting the goals, and I'm not super worried that we'll accept a lot of SIPs that are then infeasible -- we can then submit new ones. But this depends on whether you want this process to be a "design doc lite", where people also agree on implementation strategy, or just a way to agree on goals. This is what I asked earlier about PRDs vs design docs (and I'm open to either one but I'd just like clarity). Finally, both as a user and designer of software, I always want to give feedback on APIs, so I'd really like a culture of having those early. People don't argue about prettiness when they discuss APIs, they argue about the core concepts to expose in order to meet various goals, and then they're stuck maintaining those for a long time.

Matei

> On Oct 9, 2016, at 3:10 PM, Cody Koeninger  wrote:
> 
> Users instead of people, sure.  Commiters and contributors are (or at least should be) a subset of users.
> 
> Non goals, sure. I don't care what the name is, but we need to clearly say e.g. 'no we are not maintaining compatibility with XYZ right now'.
> 
> API, what I care most about is whether it allows me to accomplish the goals. Arguing about how ugly or pretty it is can be saved for design/ implementation imho.
> 
> Strategy, this is necessary because otherwise goals can be out of line with reality.  Don't propose goals you don't have at least some idea of how to implement.
> 
> Rejected strategies, given that commiters are the only ones I'm saying should formally submit SPARKLIs or SIPs, if they put junk in a required section then slap them down for it and tell them to fix it.
> 
> 
> On Oct 9, 2016 4:36 PM, "Matei Zaharia" <matei.zaharia@gmail.com  wrote:
> Yup, this is the stuff that I found unclear. Thanks for clarifying here, but we should also clarify it in the writeup. In particular:
> 
> - Goals needs to be about user-facing behavior ("people" is broad)
> 
> - I'd rename Rejected Goals to Non-Goals. Otherwise someone will dig up one of these and say "Spark's developers have officially rejected X, which our awesome system has".
> 
> - For user-facing stuff, I think you need a section on API. Virtually all other *IPs I've seen have that.
> 
> - I'm still not sure why the strategy section is needed if the purpose is to define user-facing behavior -- unless this is the strategy for setting the goals or for defining the API. That sounds squarely like a design doc issue. In some sense, who cares whether the proposal is technically feasible right now? If it's infeasible, that will be discovered later during design and implementation. Same thing with rejected strategies -- listing some of those is definitely useful sometimes, but if you make this a *required* section, people are just going to fill it in with bogus stuff (I've seen this happen before).
> 
> Matei
> 
> > On Oct 9, 2016, at 2:14 PM, Cody Koeninger <cody@koeninger.org  wrote:
> >
> > So to focus the discussion on the specific strategy I'm suggesting,
> > documented at
> >
> > https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md  >
> > "Goals: What must this allow people to do, that they can't currently?"
> >
> > Is it unclear that this is focusing specifically on people-visible behavior?
> >
> > Rejected goals -  are important because otherwise people keep trying
> > to argue about scope.  Of course you can change things later with a
> > different SIP and different vote, the point is to focus.
> >
> > Use cases - are something that people are going to bring up in
> > discussion.  If they aren't clearly documented as a goal ("This must
> > allow me to connect using SSL"), they should be added.
> >
> > Internal architecture - if the people who need specific behavior are
> > implementers of other parts of the system, that's fine.
> >
> > Rejected strategies - If you have none of these, you have no evidence
> > that the proponent didn't just go with the first thing they had in
> > mind (or have already implemented), which is a big problem currently.
> > Approval isn't binding as to specifics of implementation, so these
> > aren't handcuffs.  The goals are the contract, the strategy is
> > evidence that contract can actually be met.
> >
> > Design docs - I'm not touching design docs.  The markdown file I
> > linked specifically says of the strategy section "This is not a full
> > design document."  Is this unclear?  Design docs can be worked on
> > obviously, but that's not what I'm concerned with here.
> >
> >
> >
> >
> > On Sun, Oct 9, 2016 at 2:34 PM, Matei Zaharia <matei.zaharia@gmail.com  wrote:
> >> Hi Cody,
> >>
> >> I think this would be a lot more concrete if we had a more detailed template
> >> for SIPs. Right now, it's not super clear what's in scope -- e.g. are  they
> >> a way to solicit feedback on the user-facing behavior or on the internals?
> >> "Goals" can cover both things. I've been thinking of SIPs more as Product
> >> Requirements Docs (PRDs), which focus on *what* a code change should do as
> >> opposed to how.
> >>
> >> In particular, here are some things that you may or may not consider in
> >> scope for SIPs:
> >>
> >> - Goals and non-goals: This is definitely in scope, and IMO should focus on
> >> user-visible behavior (e.g. "system supports SQL window functions" or
> >> "system continues working if one node fails"). BTW I wouldn't say "rejected
> >> goals" because some of them might become goals later, so we're not
> >> definitively rejecting them.
> >>
> >> - Public API: Probably should be included in most SIPs unless it's too large
> >> to fully specify then (e.g. "let's add an ML library").
> >>
> >> - Use cases: I usually find this very useful in PRDs to better communicate
> >> the goals.
> >>
> >> - Internal architecture: This is usually *not* a thing users can easily
> >> comment on and it sounds more like a design doc item. Of course it's
> >> important to show that the SIP is feasible to implement. One exception,
> >> however, is that I think we'll have some SIPs primarily on internals (e.g.
> >> if somebody wants to refactor Spark's query optimizer or something).
> >>
> >> - Rejected strategies: I personally wouldn't put this, because what's the
> >> point of voting to reject a strategy before you've really begun designing
> >> and implementing something? What if you discover that the strategy is
> >> actually better when you start doing stuff?
> >>
> >> At a super high level, it depends on whether you want the SIPs to be PRDs
> >> for getting some quick feedback on the goals of a feature before it is
> >> designed, or something more like full-fledged design docs (just a more
> >> visible design doc for bigger changes). I looked at Kafka's KIPs, and they
> >> actually seem to be more like design docs. This can work too but it does
> >> require more work from the proposer and it can lead to the same problems you
> >> mentioned with people already having a design and implementation in mind.
> >>
> >> Basically, the question is, are you trying to iterate faster on design by
> >> adding a step for user feedback earlier? Or are you just trying to make
> >> design docs for key features more visible (and their approval more formal)?
> >>
> >> BTW note that in either case, I'd like to have a template for design docs
> >> too, which should also include goals. I think that would've avoided some of
> >> the issues you brought up.
> >>
> >> Matei
> >>
> >> On Oct 9, 2016, at 10:40 AM, Cody Koeninger <cody@koeninger.org  wrote:
> >>
> >> Here's my specific proposal (meta-proposal?)
> >>
> >> Spark Improvement Proposals (SIP)
> >>
> >>
> >> Background:
> >>
> >> The current problem is that design and implementation of large features are
> >> often done in private, before soliciting user feedback.
> >>
> >> When feedback is solicited, it is often as to detailed design specifics, not
> >> focused on goals.
> >>
> >> When implementation does take place after design, there is often
> >> disagreement as to what goals are or are not in scope.
> >>
> >> This results in commits that don't fully meet user needs.
> >>
> >>
> >> Goals:
> >>
> >> - Ensure user, contributor, and committer goals are clearly identified and
> >> agreed upon, before implementation takes place.
> >>
> >> - Ensure that a technically feasible strategy is chosen that is likely to
> >> meet the goals.
> >>
> >>
> >> Rejected Goals:
> >>
> >> - SIPs are not for detailed design.  Design by committee doesn't work.
> >>
> >> - SIPs are not for every change.  We dont need that much process.
> >>
> >>
> >> Strategy:
> >>
> >> My suggestion is outlined as a Spark Improvement Proposal process documented
> >> at
> >>
> >> https://github.com/koeninger/spark-1/blob/SIP-0/docs/spark-improvement-proposals.md  >>
> >> Specifics of Jira manipulation are an implementation detail we can figure
> >> out.
> >>
> >> I'm suggesting voting; the need here is for a _clear_ outcome.
> >>
> >>
> >> Rejected Strategies:
> >>
> >> Having someone who understands the problem implement it first works, but
> >> only if significant iteration after user feedback is allowed.
> >>
> >> Historically this has been problematic due to pressure to limit public api
> >> changes.
> >>
> >>
> >> On Fri, Oct 7, 2016 at 5:16 PM, Reynold Xin <rxin@databricks.com  wrote:
> >>>
> >>> Alright looks like there are quite a bit of support. We should wait to
> >>> hear from more people too.
> >>>
> >>> To push this forward, Cody and I will be working together in the next
> >>> couple of weeks to come up with a concrete, detailed proposal on what this
> >>> entails, and then we can discuss this the specific proposal as well.
> >>>
> >>>
> >>> On Fri, Oct 7, 2016 at 2:29 PM, Cody Koeninger <cody@koeninger.org  wrote:
> >>>>
> >>>> Yeah, in case it wasn't clear, I was talking about SIPs for major
> >>>> user-facing or cross-cutting changes, not minor feature adds.
> >>>>
> >>>> On Fri, Oct 7, 2016 at 3:58 PM, Stavros Kontopoulos
> >>>> <stavros.kontopoulos@lightbend.com  wrote:
> >>>>>
> >>>>> +1 to the SIP label as long as it does not slow down things and it
> >>>>> targets optimizing efforts, coordination etc. For example really small
> >>>>> features should not need to go through this process (assuming they dont
> >>>>> touch public interfaces)  or re-factorings and hope it will be kept this
> >>>>> way. So as a guideline doc should be provided, like in the KIP case.
> >>>>>
> >>>>> IMHO so far aside from tagging things and linking them elsewhere simply
> >>>>> having design docs and prototypes implementations in PRs is not something
> >>>>> that has not worked so far. What is really a pain in many projects out there
> >>>>> is discontinuity in progress of PRs, missing features, slow reviews which is
> >>>>> understandable to some extent... it is not only about Spark but things can
> >>>>> be improved for sure for this project in particular as already stated.
> >>>>>
> >>>>> On Fri, Oct 7, 2016 at 11:14 PM, Cody Koeninger <cody@koeninger.org  >>>>> wrote:
> >>>>>>
> >>>>>> +1 to adding an SIP label and linking it from the website.  I think it
> >>>>>> needs
> >>>>>>
> >>>>>> - template that focuses it towards soliciting user goals / non goals
> >>>>>> - clear resolution as to which strategy was chosen to pursue.  I'd
> >>>>>> recommend a vote.
> >>>>>>
> >>>>>> Matei asked me to clarify what I meant by changing interfaces, I think
> >>>>>> it's directly relevant to the SIP idea so I'll clarify here, and split
> >>>>>> a thread for the other discussion per Nicholas' request.
> >>>>>>
> >>>>>> I meant changing public user interfaces.  I think the first design is
> >>>>>> unlikely to be right, because it's done at a time when you have the
> >>>>>> least information.  As a user, I find it considerably more frustrating
> >>>>>> to be unable to use a tool to get my job done, than I do having to
> >>>>>> make minor changes to my code in order to take advantage of features.
> >>>>>> I've seen committers be seriously reluctant to allow changes to
> >>>>>> @experimental code that are needed in order for it to really work
> >>>>>> right.  You need to be able to iterate, and if people on both sides of
> >>>>>> the fence aren't going to respect that some newer apis are subject to
> >>>>>> change, then why even mark them as such?
> >>>>>>
> >>>>>> Ideally a finished SIP should give me a checklist of things that an
> >>>>>> implementation must do, and things that it doesn't need to do.
> >>>>>> Contributors/committers should be seriously discouraged from putting
> >>>>>> out a version 0.1 that doesn't have at least a prototype
> >>>>>> implementation of all those things, especially if they're then going
> >>>>>> to argue against interface changes necessary to get the the rest of
> >>>>>> the things done in the 0.2 version.
> >>>>>>
> >>>>>>
> >>>>>> On Fri, Oct 7, 2016 at 2:18 PM, Reynold Xin <rxin@databricks.com  >>>>>> wrote:
> >>>>>>> I like the lightweight proposal to add a SIP label.
> >>>>>>>
> >>>>>>> During Spark 2.0 development, Tom (Graves) and I suggested using wiki
> >>>>>>> to
> >>>>>>> track the list of major changes, but that never really materialized
> >>>>>>> due to
> >>>>>>> the overhead. Adding a SIP label on major JIRAs and then link to them
> >>>>>>> prominently on the Spark website makes a lot of sense.
> >>>>>>>
> >>>>>>>
> >>>>>>> On Fri, Oct 7, 2016 at 10:50 AM, Matei Zaharia
> >>>>>>> <matei.zaharia@gmail.com  >>>>>>> wrote:
> >>>>>>>>
> >>>>>>>> For the improvement proposals, I think one major point was to make
> >>>>>>>> them
> >>>>>>>> really visible to users who are not contributors, so we should do
> >>>>>>>> more than
> >>>>>>>> sending stuff to dev@. One very lightweight idea is to have a new
> >>>>>>>> type of
> >>>>>>>> JIRA called a SIP and have a link to a filter that shows all such
> >>>>>>>> JIRAs from
> >>>>>>>> http://spark.apache.org . I also like the idea of SIP and design doc
> >>>>>>>> templates (in fact many projects have them).
> >>>>>>>>
> >>>>>>>> Matei
> >>>>>>>>
> >>>>>>>> On Oct 7, 2016, at 10:38 AM, Reynold Xin <rxin@databricks.com  >>>>>>>> wrote:
> >>>>>>>>
> >>>>>>>> I called Cody last night and talked about some of the topics in his
> >>>>>>>> email.
> >>>>>>>> It became clear to me Cody genuinely cares about the project.
> >>>>>>>>
> >>>>>>>> Some of the frustrations come from the success of the project itself
> >>>>>>>> becoming very "hot", and it is difficult to get clarity from people
> >>>>>>>> who
> >>>>>>>> don't dedicate all their time to Spark. In fact, it is in some ways
> >>>>>>>> similar
> >>>>>>>> to scaling an engineering team in a successful startup: old
> >>>>>>>> processes that
> >>>>>>>> worked well might not work so well when it gets to a certain size,
> >>>>>>>> cultures
> >>>>>>>> can get diluted, building culture vs building process, etc.
> >>>>>>>>
> >>>>>>>> I also really like to have a more visible process for larger
> >>>>>>>> changes,
> >>>>>>>> especially major user facing API changes. Historically we upload
> >>>>>>>> design docs
> >>>>>>>> for major changes, but it is not always consistent and difficult to
> >>>>>>>> quality
> >>>>>>>> of the docs, due to the volunteering nature of the organization.
> >>>>>>>>
> >>>>>>>> Some of the more concrete ideas we discussed focus on building a
> >>>>>>>> culture
> >>>>>>>> to improve clarity:
> >>>>>>>>
> >>>>>>>> - Process: Large changes should have design docs posted on JIRA. One
> >>>>>>>> thing
> >>>>>>>> Cody and I didn't discuss but an idea that just came to me is we
> >>>>>>>> should
> >>>>>>>> create a design doc template for the project and ask everybody to
> >>>>>>>> follow.
> >>>>>>>> The design doc template should also explicitly list goals and
> >>>>>>>> non-goals, to
> >>>>>>>> make design doc more consistent.
> >>>>>>>>
> >>>>>>>> - Process: Email dev@ to solicit feedback. We have some this with
> >>>>>>>> some
> >>>>>>>> changes, but again very inconsistent. Just posting something on JIRA
> >>>>>>>> isn't
> >>>>>>>> sufficient, because there are simply too many JIRAs and the signal
> >>>>>>>> get lost
> >>>>>>>> in the noise. While this is generally impossible to enforce because
> >>>>>>>> we can't
> >>>>>>>> force all volunteers to conform to a process (or they might not even
> >>>>>>>> be
> >>>>>>>> aware of this),  those who are more familiar with the project can
> >>>>>>>> help by
> >>>>>>>> emailing the dev@ when they see something that hasn't been.
> >>>>>>>>
> >>>>>>>> - Culture: The design doc author(s) should be open to feedback. A
> >>>>>>>> design
> >>>>>>>> doc should serve as the base for discussion and is by no means the
> >>>>>>>> final
> >>>>>>>> design. Of course, this does not mean the author has to accept every
> >>>>>>>> feedback. They should also be comfortable accepting / rejecting
> >>>>>>>> ideas on
> >>>>>>>> technical grounds.
> >>>>>>>>
> >>>>>>>> - Process / Culture: For major ongoing projects, it can be useful to
> >>>>>>>> have
> >>>>>>>> some monthly Google hangouts that are open to the world. I am
> >>>>>>>> actually not
> >>>>>>>> sure how well this will work, because of the volunteering nature and
> >>>>>>>> we need
> >>>>>>>> to adjust for timezones for people across the globe, but it seems
> >>>>>>>> worth
> >>>>>>>> trying.
> >>>>>>>>
> >>>>>>>> - Culture: Contributors (including committers) should be more direct
> >>>>>>>> in
> >>>>>>>> setting expectations, including whether they are working on a
> >>>>>>>> specific
> >>>>>>>> issue, whether they will be working on a specific issue, and whether
> >>>>>>>> an
> >>>>>>>> issue or pr or jira should be rejected. Most people I know in this
> >>>>>>>> community
> >>>>>>>> are nice and don't enjoy telling other people no, but it is often
> >>>>>>>> more
> >>>>>>>> annoying to a contributor to not know anything than getting a no.
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> On Fri, Oct 7, 2016 at 10:03 AM, Matei Zaharia
> >>>>>>>> <matei.zaharia@gmail.com  >>>>>>>> wrote:
> >>>>>>>>>
> >>>>>>>>>
> >>>>>>>>> Love the idea of a more visible "Spark Improvement Proposal"
> >>>>>>>>> process that
> >>>>>>>>> solicits user input on new APIs. For what it's worth, I don't think
> >>>>>>>>> committers are trying to minimize their own work -- every committer
> >>>>>>>>> cares
> >>>>>>>>> about making the software useful for users. However, it is always
> >>>>>>>>> hard to
> >>>>>>>>> get user input and so it helps to have this kind of process. I've
> >>>>>>>>> certainly
> >>>>>>>>> looked at the *IPs a lot in other software I use just to see the
> >>>>>>>>> biggest
> >>>>>>>>> things on the roadmap.
> >>>>>>>>>
> >>>>>>>>> When you're talking about "changing interfaces", are you talking
> >>>>>>>>> about
> >>>>>>>>> public or internal APIs? I do think many people hate changing
> >>>>>>>>> public APIs
> >>>>>>>>> and I actually think that's for the best of the project. That's a
> >>>>>>>>> technical
> >>>>>>>>> debate, but basically, the worst thing when you're using a piece of
> >>>>>>>>> software
> >>>>>>>>> is that the developers constantly ask you to rewrite your app to
> >>>>>>>>> update to a
> >>>>>>>>> new version (and thus benefit from bug fixes, etc). Cue anyone
> >>>>>>>>> who's used
> >>>>>>>>> Protobuf, or Guava. The "let's get everyone to change their code
> >>>>>>>>> this
> >>>>>>>>> release" model works well within a single large company, but
> >>>>>>>>> doesn't work
> >>>>>>>>> well for a community, which is why nearly all *very* widely used
> >>>>>>>>> programming
> >>>>>>>>> interfaces (I'm talking things like Java standard library, Windows
> >>>>>>>>> API, etc)
> >>>>>>>>> almost *never* break backwards compatibility. All this is done
> >>>>>>>>> within reason
> >>>>>>>>> though, e.g. we do change things in major releases (2.x, 3.x, etc).
> >>>>>>>>
> >>>>>>>>
> >>>>>>>>
> >>>>>>>>
> >>>>>>>
> >>>>>>
> >>>>>> ---------------------------------------------------------------------
> >>>>>> To unsubscribe e-mail: dev-unsubscribe@spark.apache.org  >>>>>>
> >>>>>
> >>>>>
> >>>>>
> >>>>> --
> >>>>> Stavros Kontopoulos
> >>>>> Senior Software Engineer
> >>>>> Lightbend, Inc.
> >>>>> p:  +30 6977967274  >>>>> e: stavros.kontopoulos@lightbend.com  >>>>>
> >>>>>
> >>>>
> >>>
> >>
> >>
> 


What are the main use cases you've seen for this? Maybe we can add a page to the docs about how to launch Spark as an embedded library.

Matei

> On Oct 10, 2016, at 10:21 AM, Russell Spitzer  wrote:
> 
> I actually had not seen SparkLauncher before, that looks pretty great :)
> 
> On Mon, Oct 10, 2016 at 10:17 AM Russell Spitzer <russell.spitzer@gmail.com  wrote:
> I'm definitely only talking about non-embedded uses here as I also use embedded Spark (cassandra, and kafka) to run tests. This is almost always safe since everything is in the same JVM. It's only once we get to launching against a real distributed env do we end up with issues.
> 
> Since Pyspark uses spark submit in the java gateway i'm not sure if that matters :)
> 
> The cases I see are usually usually going through main directly, adding jars programatically. 
> 
> Usually ends up with classpath errors (Spark not on the CP, their jar not on the CP, dependencies not on the cp), 
> conf errors (executors have the incorrect environment, executor classpath broken, not understanding spark-defaults won't do anything),
> Jar version mismatches
> Etc ...
> 
> On Mon, Oct 10, 2016 at 10:05 AM Sean Owen <sowen@cloudera.com  wrote:
> I have also 'embedded' a Spark driver without much trouble. It isn't that it can't work. 
> 
> The Launcher API is ptobably the recommended way to do that though. spark-submit is the way to go for non programmatic access. 
> 
> If you're not doing one of those things and it is not working, yeah I think people would tell you you're on your own. I think that's consistent with all the JIRA discussions I have seen over time. 
> 
> 
> On Mon, Oct 10, 2016, 17:33 Russell Spitzer <russell.spitzer@gmail.com  wrote:
> I've seen a variety of users attempting to work around using Spark Submit with at best middling levels of success. I think it would be helpful if the project had a clear statement that submitting an application without using Spark Submit is truly for experts only or is unsupported entirely.
> 
> I know this is a pretty strong stance and other people have had different experiences than me so please let me know what you think :)


Is there any way to tie wiki accounts with JIRA accounts? I found it weird that they're not tied at the ASF.

Otherwise, moving this into the docs might make sense.

Matei

> On Oct 18, 2016, at 6:19 AM, Cody Koeninger  wrote:
> 
> +1 to putting docs in one clear place.
> 
> On Oct 18, 2016 6:40 AM, "Sean Owen" <sowen@cloudera.com  wrote:
> I'm OK with that. The upside to the wiki is that it can be edited directly outside of a release cycle. However, in practice I find that the wiki is rarely changed. To me it also serves as a place for information that isn't exactly project documentation like "powered by" listings.
> 
> In a way I'd like to get rid of the wiki to have one less place for docs, that doesn't have the same accessibility (I don't know who can give edit access), and doesn't have a review process.
> 
> For now I'd settle for bringing over a few key docs like the one you mention. I spent a little time a while ago removing some duplication across the wiki and project docs and think there's a bit more than could be done.
> 
> 
> On Tue, Oct 18, 2016 at 12:25 PM Holden Karau <holden@pigscanfly.ca  wrote:
> Right now the wiki isn't particularly accessible to updates by external contributors. We've already got a contributing to spark page which just links to the wiki - how about if we just move the wiki contents over? This way contributors can contribute to our documentation about how to contribute probably helping clear up points of confusion for new contributors which the rest of us may be blind to.
> 
> If we do this we would probably want to update the wiki page to point to the documentation generated from markdown. It would also mean that the results of any update to the contributing guide take a full release cycle to be visible. Another alternative would be opening up the wiki to a broader set of people.
> 
> I know a lot of people are probably getting ready for Spark Summit EU (and I hope to catch up with some of y'all there) but I figured this a relatively minor proposal.
> -- 
> Cell : 425-233-8271  Twitter: https://twitter.com/holdenkarau 

I'm also curious whether there are concerns other than latency with the way stuff executes in Structured Streaming (now that the time steps don't have to act as triggers), as well as what latency people want for various apps.

The stateful operator designs for streaming systems aren't inherently "better" than micro-batching -- they lose a lot of stuff that is possible in Spark, such as load balancing work dynamically across nodes, speculative execution for stragglers, scaling clusters up and down elastically, etc. Moreover, Spark itself could execute the current model with much lower latency. The question is just what combinations of latency, throughput, fault recovery, etc to target.

Matei

> On Oct 19, 2016, at 2:18 PM, Amit Sela  wrote:
> 
> 
> 
> On Thu, Oct 20, 2016 at 12:07 AM Shivaram Venkataraman <shivaram@eecs.berkeley.edu  wrote:
> At the AMPLab we've been working on a research project that looks at
> just the scheduling latencies and on techniques to get lower
> scheduling latency. It moves away from the micro-batch model, but
> reuses the fault tolerance etc. in Spark. However we haven't yet
> figure out all the parts in integrating this with the rest of
> structured streaming. I'll try to post a design doc / SIP about this
> soon.
> 
> On a related note - are there other problems users face with
> micro-batch other than latency ?
> I think that the fact that they serve as an output trigger is a problem, but Structured Streaming seems to resolve this now.  
> 
> Thanks
> Shivaram
> 
> On Wed, Oct 19, 2016 at 1:29 PM, Michael Armbrust
> <michael@databricks.com  wrote:
> > I know people are seriously thinking about latency.  So far that has not
> > been the limiting factor in the users I've been working with.
> >
> > On Wed, Oct 19, 2016 at 1:11 PM, Cody Koeninger <cody@koeninger.org  wrote:
> >>
> >> Is anyone seriously thinking about alternatives to microbatches?
> >>
> >> On Wed, Oct 19, 2016 at 2:45 PM, Michael Armbrust
> >> <michael@databricks.com  wrote:
> >> > Anything that is actively being designed should be in JIRA, and it seems
> >> > like you found most of it.  In general, release windows can be found on
> >> > the
> >> > wiki.
> >> >
> >> > 2.1 has a lot of stability fixes as well as the kafka support you
> >> > mentioned.
> >> > It may also include some of the following.
> >> >
> >> > The items I'd like to start thinking about next are:
> >> >  - Evicting state from the store based on event time watermarks
> >> >  - Sessionization (grouping together related events by key / eventTime)
> >> >  - Improvements to the query planner (remove some of the restrictions on
> >> > what queries can be run).
> >> >
> >> > This is roughly in order based on what I've been hearing users hit the
> >> > most.
> >> > Would love more feedback on what is blocking real use cases.
> >> >
> >> > On Tue, Oct 18, 2016 at 1:51 AM, Ofir Manor <ofir.manor@equalum.io  >> > wrote:
> >> >>
> >> >> Hi,
> >> >> I hope it is the right forum.
> >> >> I am looking for some information of what to expect from
> >> >> StructuredStreaming in its next releases to help me choose when / where
> >> >> to
> >> >> start using it more seriously (or where to invest in workarounds and
> >> >> where
> >> >> to wait). I couldn't find a good place where such planning discussed
> >> >> for 2.1
> >> >> (like, for example ML and SPARK-15581).
> >> >> I'm aware of the 2.0 documented limits
> >> >>
> >> >> (http://spark.apache.org/docs/2.0.1/structured-streaming-programming-guide.html#unsupported-operations  >> >> like no support for multiple aggregations levels, joins are strictly to
> >> >> a
> >> >> static dataset (no SCD or stream-stream) etc, limited sources / sinks
> >> >> (like
> >> >> no sink for interactive queries) etc etc
> >> >> I'm also aware of some changes that have landed in master, like the new
> >> >> Kafka 0.10 source (and its on-going improvements) in SPARK-15406, the
> >> >> metrics in SPARK-17731, and some improvements for the file source.
> >> >> If I remember correctly, the discussion on Spark release cadence
> >> >> concluded
> >> >> with a preference to a four-month cycles, with likely code freeze
> >> >> pretty
> >> >> soon (end of October). So I believe the scope for 2.1 should likely
> >> >> quite
> >> >> clear to some, and that 2.2 planning should likely be starting about
> >> >> now.
> >> >> Any visibility / sharing will be highly appreciated!
> >> >> thanks in advance,
> >> >>
> >> >> Ofir Manor
> >> >>
> >> >> Co-Founder & CTO | Equalum
> >> >>
> >> >> Mobile: +972-54-7801286  | Email: ofir.manor@equalum.io  >> >
> >> >
> >
> >
> 
> ---------------------------------------------------------------------
> To unsubscribe e-mail: dev-unsubscribe@spark.apache.org  


Yeah, as Shivaram pointed out, there have been research projects that looked at it. Also, Structured Streaming was explicitly designed to not make microbatching part of the API or part of the output behavior (tying triggers to it). However, when people begin working on that is a function of demand relative to other features. I don't think we can commit to one plan before exploring more options, but basically there is Shivaram's project, which adds a few new concepts to the scheduler, and there's the option to reduce control plane latency in the current system, which hasn't been heavily optimized yet but should be doable (lots of systems can handle 10,000s of RPCs per second).

Matei

> On Oct 19, 2016, at 9:20 PM, Cody Koeninger  wrote:
> 
> I don't think it's just about what to target - if you could target 1ms batches, without harming 1 second or 1 minute batches.... why wouldn't you?
> I think it's about having a clear strategy and dedicating resources to it. If  scheduling batches at an order of magnitude or two lower latency is the strategy, and that's actually feasible, that's great. But I haven't seen that clear direction, and this is by no means a recent issue.
> 
> 
> On Oct 19, 2016 7:36 PM, "Matei Zaharia" <matei.zaharia@gmail.com  wrote:
> I'm also curious whether there are concerns other than latency with the way stuff executes in Structured Streaming (now that the time steps don't have to act as triggers), as well as what latency people want for various apps.
> 
> The stateful operator designs for streaming systems aren't inherently "better" than micro-batching -- they lose a lot of stuff that is possible in Spark, such as load balancing work dynamically across nodes, speculative execution for stragglers, scaling clusters up and down elastically, etc. Moreover, Spark itself could execute the current model with much lower latency. The question is just what combinations of latency, throughput, fault recovery, etc to target.
> 
> Matei
> 
>> On Oct 19, 2016, at 2:18 PM, Amit Sela <amitsela33@gmail.com  wrote:
>> 
>> 
>> 
>> On Thu, Oct 20, 2016 at 12:07 AM Shivaram Venkataraman <shivaram@eecs.berkeley.edu  wrote:
>> At the AMPLab we've been working on a research project that looks at
>> just the scheduling latencies and on techniques to get lower
>> scheduling latency. It moves away from the micro-batch model, but
>> reuses the fault tolerance etc. in Spark. However we haven't yet
>> figure out all the parts in integrating this with the rest of
>> structured streaming. I'll try to post a design doc / SIP about this
>> soon.
>> 
>> On a related note - are there other problems users face with
>> micro-batch other than latency ?
>> I think that the fact that they serve as an output trigger is a problem, but Structured Streaming seems to resolve this now.  
>> 
>> Thanks
>> Shivaram
>> 
>> On Wed, Oct 19, 2016 at 1:29 PM, Michael Armbrust
>> <michael@databricks.com  wrote:
>> > I know people are seriously thinking about latency.  So far that has not
>> > been the limiting factor in the users I've been working with.
>> >
>> > On Wed, Oct 19, 2016 at 1:11 PM, Cody Koeninger <cody@koeninger.org  wrote:
>> >>
>> >> Is anyone seriously thinking about alternatives to microbatches?
>> >>
>> >> On Wed, Oct 19, 2016 at 2:45 PM, Michael Armbrust
>> >> <michael@databricks.com  wrote:
>> >> > Anything that is actively being designed should be in JIRA, and it seems
>> >> > like you found most of it.  In general, release windows can be found on
>> >> > the
>> >> > wiki.
>> >> >
>> >> > 2.1 has a lot of stability fixes as well as the kafka support you
>> >> > mentioned.
>> >> > It may also include some of the following.
>> >> >
>> >> > The items I'd like to start thinking about next are:
>> >> >  - Evicting state from the store based on event time watermarks
>> >> >  - Sessionization (grouping together related events by key / eventTime)
>> >> >  - Improvements to the query planner (remove some of the restrictions on
>> >> > what queries can be run).
>> >> >
>> >> > This is roughly in order based on what I've been hearing users hit the
>> >> > most.
>> >> > Would love more feedback on what is blocking real use cases.
>> >> >
>> >> > On Tue, Oct 18, 2016 at 1:51 AM, Ofir Manor <ofir.manor@equalum.io  >> > wrote:
>> >> >>
>> >> >> Hi,
>> >> >> I hope it is the right forum.
>> >> >> I am looking for some information of what to expect from
>> >> >> StructuredStreaming in its next releases to help me choose when / where
>> >> >> to
>> >> >> start using it more seriously (or where to invest in workarounds and
>> >> >> where
>> >> >> to wait). I couldn't find a good place where such planning discussed
>> >> >> for 2.1
>> >> >> (like, for example ML and SPARK-15581).
>> >> >> I'm aware of the 2.0 documented limits
>> >> >>
>> >> >> (http://spark.apache.org/docs/2.0.1/structured-streaming-programming-guide.html#unsupported-operations  >> >> like no support for multiple aggregations levels, joins are strictly to
>> >> >> a
>> >> >> static dataset (no SCD or stream-stream) etc, limited sources / sinks
>> >> >> (like
>> >> >> no sink for interactive queries) etc etc
>> >> >> I'm also aware of some changes that have landed in master, like the new
>> >> >> Kafka 0.10 source (and its on-going improvements) in SPARK-15406, the
>> >> >> metrics in SPARK-17731, and some improvements for the file source.
>> >> >> If I remember correctly, the discussion on Spark release cadence
>> >> >> concluded
>> >> >> with a preference to a four-month cycles, with likely code freeze
>> >> >> pretty
>> >> >> soon (end of October). So I believe the scope for 2.1 should likely
>> >> >> quite
>> >> >> clear to some, and that 2.2 planning should likely be starting about
>> >> >> now.
>> >> >> Any visibility / sharing will be highly appreciated!
>> >> >> thanks in advance,
>> >> >>
>> >> >> Ofir Manor
>> >> >>
>> >> >> Co-Founder & CTO | Equalum
>> >> >>
>> >> >> Mobile: +972-54-7801286  | Email: ofir.manor@equalum.io  >> >
>> >> >
>> >
>> >
>> 
>> ---------------------------------------------------------------------
>> To unsubscribe e-mail: dev-unsubscribe@spark.apache.org  
> 


Both Spark Streaming and Structured Streaming preserve locality for operator state actually. They only reshuffle state if a cluster node fails or if the load becomes heavily imbalanced and it's better to launch a task on another node and load the state remotely.

Matei

> On Oct 19, 2016, at 9:38 PM, Abhishek R. Singh  wrote:
> 
> Its not so much about latency actually. The bigger rub for me is that the state has to be reshuffled every micro/mini-batch (unless I am not understanding it right - spark 2.0 state model i.e.).
> 
> Operator model avoids it by preserving state locality. Event time processing and state purging are the other essentials (which are thankfully getting addressed).
> 
> Any guidance on (timelines for) expected exit from alpha state would also be greatly appreciated.
> 
> -Abhishek-
> 
>> On Oct 19, 2016, at 5:36 PM, Matei Zaharia <matei.zaharia@gmail.com  wrote:
>> 
>> I'm also curious whether there are concerns other than latency with the way stuff executes in Structured Streaming (now that the time steps don't have to act as triggers), as well as what latency people want for various apps.
>> 
>> The stateful operator designs for streaming systems aren't inherently "better" than micro-batching -- they lose a lot of stuff that is possible in Spark, such as load balancing work dynamically across nodes, speculative execution for stragglers, scaling clusters up and down elastically, etc. Moreover, Spark itself could execute the current model with much lower latency. The question is just what combinations of latency, throughput, fault recovery, etc to target.
>> 
>> Matei
>> 
>>> On Oct 19, 2016, at 2:18 PM, Amit Sela <amitsela33@gmail.com  wrote:
>>> 
>>> 
>>> 
>>> On Thu, Oct 20, 2016 at 12:07 AM Shivaram Venkataraman <shivaram@eecs.berkeley.edu  wrote:
>>> At the AMPLab we've been working on a research project that looks at
>>> just the scheduling latencies and on techniques to get lower
>>> scheduling latency. It moves away from the micro-batch model, but
>>> reuses the fault tolerance etc. in Spark. However we haven't yet
>>> figure out all the parts in integrating this with the rest of
>>> structured streaming. I'll try to post a design doc / SIP about this
>>> soon.
>>> 
>>> On a related note - are there other problems users face with
>>> micro-batch other than latency ?
>>> I think that the fact that they serve as an output trigger is a problem, but Structured Streaming seems to resolve this now.  
>>> 
>>> Thanks
>>> Shivaram
>>> 
>>> On Wed, Oct 19, 2016 at 1:29 PM, Michael Armbrust
>>> <michael@databricks.com  wrote:
>>> > I know people are seriously thinking about latency.  So far that has not
>>> > been the limiting factor in the users I've been working with.
>>> >
>>> > On Wed, Oct 19, 2016 at 1:11 PM, Cody Koeninger <cody@koeninger.org  wrote:
>>> >>
>>> >> Is anyone seriously thinking about alternatives to microbatches?
>>> >>
>>> >> On Wed, Oct 19, 2016 at 2:45 PM, Michael Armbrust
>>> >> <michael@databricks.com  wrote:
>>> >> > Anything that is actively being designed should be in JIRA, and it seems
>>> >> > like you found most of it.  In general, release windows can be found on
>>> >> > the
>>> >> > wiki.
>>> >> >
>>> >> > 2.1 has a lot of stability fixes as well as the kafka support you
>>> >> > mentioned.
>>> >> > It may also include some of the following.
>>> >> >
>>> >> > The items I'd like to start thinking about next are:
>>> >> >  - Evicting state from the store based on event time watermarks
>>> >> >  - Sessionization (grouping together related events by key / eventTime)
>>> >> >  - Improvements to the query planner (remove some of the restrictions on
>>> >> > what queries can be run).
>>> >> >
>>> >> > This is roughly in order based on what I've been hearing users hit the
>>> >> > most.
>>> >> > Would love more feedback on what is blocking real use cases.
>>> >> >
>>> >> > On Tue, Oct 18, 2016 at 1:51 AM, Ofir Manor <ofir.manor@equalum.io  >> > wrote:
>>> >> >>
>>> >> >> Hi,
>>> >> >> I hope it is the right forum.
>>> >> >> I am looking for some information of what to expect from
>>> >> >> StructuredStreaming in its next releases to help me choose when / where
>>> >> >> to
>>> >> >> start using it more seriously (or where to invest in workarounds and
>>> >> >> where
>>> >> >> to wait). I couldn't find a good place where such planning discussed
>>> >> >> for 2.1
>>> >> >> (like, for example ML and SPARK-15581).
>>> >> >> I'm aware of the 2.0 documented limits
>>> >> >>
>>> >> >> (http://spark.apache.org/docs/2.0.1/structured-streaming-programming-guide.html#unsupported-operations  >> >> like no support for multiple aggregations levels, joins are strictly to
>>> >> >> a
>>> >> >> static dataset (no SCD or stream-stream) etc, limited sources / sinks
>>> >> >> (like
>>> >> >> no sink for interactive queries) etc etc
>>> >> >> I'm also aware of some changes that have landed in master, like the new
>>> >> >> Kafka 0.10 source (and its on-going improvements) in SPARK-15406, the
>>> >> >> metrics in SPARK-17731, and some improvements for the file source.
>>> >> >> If I remember correctly, the discussion on Spark release cadence
>>> >> >> concluded
>>> >> >> with a preference to a four-month cycles, with likely code freeze
>>> >> >> pretty
>>> >> >> soon (end of October). So I believe the scope for 2.1 should likely
>>> >> >> quite
>>> >> >> clear to some, and that 2.2 planning should likely be starting about
>>> >> >> now.
>>> >> >> Any visibility / sharing will be highly appreciated!
>>> >> >> thanks in advance,
>>> >> >>
>>> >> >> Ofir Manor
>>> >> >>
>>> >> >> Co-Founder & CTO | Equalum
>>> >> >>
>>> >> >> Mobile: +972-54-7801286  | Email: ofir.manor@equalum.io  >> >
>>> >> >
>>> >
>>> >
>>> 
>>> ---------------------------------------------------------------------
>>> To unsubscribe e-mail: dev-unsubscribe@spark.apache.org  
>> 
> 


Just to comment on this, I'm generally against removing these types of things unless they create a substantial burden on project contributors. It doesn't sound like Python 2.6 and Java 7 do that yet -- Scala 2.10 might, but then of course we need to wait for 2.12 to be out and stable.

In general, this type of stuff only hurts users, and doesn't have a huge impact on Spark contributors' productivity (sure, it's a bit unpleasant, but that's life). If we break compatibility this way too quickly, we fragment the user community, and then either people have a crappy experience with Spark because their corporate IT doesn't yet have an environment that can run the latest version, or worse, they create more maintenance burden for us because they ask for more patches to be backported to old Spark versions (1.6.x, 2.0.x, etc). Python in particular is pretty fundamental to many Linux distros.

In the future, rather than just looking at when some software came out, it may be good to have some criteria for when to drop support for something. For example, if there are really nice libraries in Python 2.7 or Java 8 that we're missing out on, that may be a good reason. The maintenance burden for multiple Scala versions is definitely painful but I also think we should always support the latest two Scala releases.

Matei

> On Oct 27, 2016, at 12:15 PM, Reynold Xin  wrote:
> 
> I created a JIRA ticket to track this: https://issues.apache.org/jira/browse/SPARK-18138  
> 
> 
> On Thu, Oct 27, 2016 at 10:19 AM, Steve Loughran <stevel@hortonworks.com  wrote:
> 
>> On 27 Oct 2016, at 10:03, Sean Owen <sowen@cloudera.com  wrote:
>> 
>> Seems OK by me.
>> How about Hadoop < 2.6, Python 2.6? Those seem more removeable. I'd like to add that to a list of things that will begin to be unsupported 6 months from now.
>> 
> 
> If you go to java 8 only, then hadoop 2.6+ is mandatory. 
> 
> 
>> On Wed, Oct 26, 2016 at 8:49 PM Koert Kuipers <koert@tresata.com  wrote:
>> that sounds good to me
>> 
>> On Wed, Oct 26, 2016 at 2:26 PM, Reynold Xin <rxin@databricks.com  wrote:
>> We can do the following concrete proposal:
>> 
>> 1. Plan to remove support for Java 7 / Scala 2.10 in Spark 2.2.0 (Mar/Apr 2017).
>> 
>> 2. In Spark 2.1.0 release, aggressively and explicitly announce the deprecation of Java 7 / Scala 2.10 support.
>> 
>> (a) It should appear in release notes, documentations that mention how to build Spark
>> 
>> (b) and a warning should be shown every time SparkContext is started using Scala 2.10 or Java 7.
>> 
> 
> 


Deprecating them is fine (and I know they're already deprecated), the question is just whether to remove them. For example, what exactly is the downside of having Python 2.6 or Java 7 right now? If it's high, then we can remove them, but I just haven't seen a ton of details. It also sounded like fairly recent versions of CDH, HDP, RHEL, etc still have old versions of these.

Just talking with users, I've seen many of people who say "we have a Hadoop cluster from $VENDOR, but we just download Spark from Apache and run newer versions of that". That's great for Spark IMO, and we need to stay compatible even with somewhat older Hadoop installs because they are time-consuming to update. Having the whole community on a small set of versions leads to a better experience for everyone and also to more of a "network effect": more people can battle-test new versions, answer questions about them online, write libraries that easily reach the majority of Spark users, etc.

Matei

> On Oct 27, 2016, at 11:51 PM, Ofir Manor  wrote:
> 
> I totally agree with Sean, just a small correction:
> Java 7 and Python 2.6 are already deprecated since Spark 2.0 (after a lengthy discussion), so there is no need to discuss whether they should become deprecated in 2.1
>   http://spark.apache.org/releases/spark-release-2-0-0.html#deprecations  The discussion is whether Scala 2.10 should also be marked as deprecated (no one is objecting that), and more importantly, when to actually move from deprecation to actually dropping support for any combination of JDK / Scala / Hadoop / Python.
> 
> Ofir Manor
> 
> Co-Founder & CTO | Equalum
> 
> 
> Mobile: +972-54-7801286  | Email: ofir.manor@equalum.io  On Fri, Oct 28, 2016 at 12:13 AM, Sean Owen <sowen@cloudera.com  wrote:
> The burden may be a little more apparent when dealing with the day to day merging and fixing of breaks. The upside is maybe the more compelling argument though. For example, lambda-fying all the Java code, supporting java.time, and taking advantage of some newer Hadoop/YARN APIs is a moderate win for users too, and there's also a cost to not doing that.
> 
> I must say I don't see a risk of fragmentation as nearly the problem it's made out to be here. We are, after all, here discussing _beginning_ to remove support _in 6 months_, for long since non-current versions of things. An org's decision to not, say, use Java 8 is a decision to not use the new version of lots of things. It's not clear this is a constituency that is either large or one to reasonably serve indefinitely.
> 
> In the end, the Scala issue may be decisive. Supporting 2.10 - 2.12 simultaneously is a bridge too far, and if 2.12 requires Java 8, it's a good reason to for Spark to require Java 8. And Steve suggests that means a minimum of Hadoop 2.6 too. (I still profess ignorance of the Python part of the issue.)
> 
> Put another way I am not sure what the criteria is, if not the above?
> 
> I support deprecating all of these things, at the least, in 2.1.0. Although it's a separate question, I believe it's going to be necessary to remove support in ~6 months in 2.2.0.
> 
> 
> On Thu, Oct 27, 2016 at 4:36 PM Matei Zaharia <matei.zaharia@gmail.com  wrote:
> Just to comment on this, I'm generally against removing these types of things unless they create a substantial burden on project contributors. It doesn't sound like Python 2.6 and Java 7 do that yet -- Scala 2.10 might, but then of course we need to wait for 2.12 to be out and stable.
> 
> In general, this type of stuff only hurts users, and doesn't have a huge impact on Spark contributors' productivity (sure, it's a bit unpleasant, but that's life). If we break compatibility this way too quickly, we fragment the user community, and then either people have a crappy experience with Spark because their corporate IT doesn't yet have an environment that can run the latest version, or worse, they create more maintenance burden for us because they ask for more patches to be backported to old Spark versions (1.6.x, 2.0.x, etc). Python in particular is pretty fundamental to many Linux distros.
> 
> In the future, rather than just looking at when some software came out, it may be good to have some criteria for when to drop support for something. For example, if there are really nice libraries in Python 2.7 or Java 8 that we're missing out on, that may be a good reason. The maintenance burden for multiple Scala versions is definitely painful but I also think we should always support the latest two Scala releases.
> 
> Matei
> 
>> On Oct 27, 2016, at 12:15 PM, Reynold Xin <rxin@databricks.com  wrote:
>> 
>> I created a JIRA ticket to track this: https://issues.apache.org/jira/browse/SPARK-18138  
>> 
>> 
>> On Thu, Oct 27, 2016 at 10:19 AM, Steve Loughran <stevel@hortonworks.com  wrote:
>> 
>>> On 27 Oct 2016, at 10:03, Sean Owen <sowen@cloudera.com  wrote:
>>> 
>>> Seems OK by me.
>>> How about Hadoop < 2.6, Python 2.6? Those seem more removeable. I'd like to add that to a list of things that will begin to be unsupported 6 months from now.
>>> 
>> 
>> If you go to java 8 only, then hadoop 2.6+ is mandatory. 
>> 
>> 
>>> On Wed, Oct 26, 2016 at 8:49 PM Koert Kuipers <koert@tresata.com  wrote:
>>> that sounds good to me
>>> 
>>> On Wed, Oct 26, 2016 at 2:26 PM, Reynold Xin <rxin@databricks.com  wrote:
>>> We can do the following concrete proposal:
>>> 
>>> 1. Plan to remove support for Java 7 / Scala 2.10 in Spark 2.2.0 (Mar/Apr 2017).
>>> 
>>> 2. In Spark 2.1.0 release, aggressively and explicitly announce the deprecation of Java 7 / Scala 2.10 support.
>>> 
>>> (a) It should appear in release notes, documentations that mention how to build Spark
>>> 
>>> (b) and a warning should be shown every time SparkContext is started using Scala 2.10 or Java 7.
>>> 
>> 
>> 
> 
> 


BTW maybe one key point that isn't obvious is that with YARN and Mesos, the version of Spark used can be solely up to the developer who writes an app, not to the cluster administrator. So even in very conservative orgs, developers can download a new version of Spark, run it, and demonstrate value, which is good both for them and for the project. On the other hand, if they were stuck with, say, Spark 1.3, they'd have a much worse experience and perhaps get a worse impression of the project.

Matei

> On Oct 28, 2016, at 9:58 AM, Matei Zaharia  wrote:
> 
> Deprecating them is fine (and I know they're already deprecated), the question is just whether to remove them. For example, what exactly is the downside of having Python 2.6 or Java 7 right now? If it's high, then we can remove them, but I just haven't seen a ton of details. It also sounded like fairly recent versions of CDH, HDP, RHEL, etc still have old versions of these.
> 
> Just talking with users, I've seen many of people who say "we have a Hadoop cluster from $VENDOR, but we just download Spark from Apache and run newer versions of that". That's great for Spark IMO, and we need to stay compatible even with somewhat older Hadoop installs because they are time-consuming to update. Having the whole community on a small set of versions leads to a better experience for everyone and also to more of a "network effect": more people can battle-test new versions, answer questions about them online, write libraries that easily reach the majority of Spark users, etc.
> 
> Matei
> 
>> On Oct 27, 2016, at 11:51 PM, Ofir Manor <ofir.manor@equalum.io  wrote:
>> 
>> I totally agree with Sean, just a small correction:
>> Java 7 and Python 2.6 are already deprecated since Spark 2.0 (after a lengthy discussion), so there is no need to discuss whether they should become deprecated in 2.1
>>   http://spark.apache.org/releases/spark-release-2-0-0.html#deprecations  The discussion is whether Scala 2.10 should also be marked as deprecated (no one is objecting that), and more importantly, when to actually move from deprecation to actually dropping support for any combination of JDK / Scala / Hadoop / Python.
>> 
>> Ofir Manor
>> 
>> Co-Founder & CTO | Equalum
>> 
>> 
>> Mobile: +972-54-7801286  | Email: ofir.manor@equalum.io  On Fri, Oct 28, 2016 at 12:13 AM, Sean Owen <sowen@cloudera.com  wrote:
>> The burden may be a little more apparent when dealing with the day to day merging and fixing of breaks. The upside is maybe the more compelling argument though. For example, lambda-fying all the Java code, supporting java.time, and taking advantage of some newer Hadoop/YARN APIs is a moderate win for users too, and there's also a cost to not doing that.
>> 
>> I must say I don't see a risk of fragmentation as nearly the problem it's made out to be here. We are, after all, here discussing _beginning_ to remove support _in 6 months_, for long since non-current versions of things. An org's decision to not, say, use Java 8 is a decision to not use the new version of lots of things. It's not clear this is a constituency that is either large or one to reasonably serve indefinitely.
>> 
>> In the end, the Scala issue may be decisive. Supporting 2.10 - 2.12 simultaneously is a bridge too far, and if 2.12 requires Java 8, it's a good reason to for Spark to require Java 8. And Steve suggests that means a minimum of Hadoop 2.6 too. (I still profess ignorance of the Python part of the issue.)
>> 
>> Put another way I am not sure what the criteria is, if not the above?
>> 
>> I support deprecating all of these things, at the least, in 2.1.0. Although it's a separate question, I believe it's going to be necessary to remove support in ~6 months in 2.2.0.
>> 
>> 
>> On Thu, Oct 27, 2016 at 4:36 PM Matei Zaharia <matei.zaharia@gmail.com  wrote:
>> Just to comment on this, I'm generally against removing these types of things unless they create a substantial burden on project contributors. It doesn't sound like Python 2.6 and Java 7 do that yet -- Scala 2.10 might, but then of course we need to wait for 2.12 to be out and stable.
>> 
>> In general, this type of stuff only hurts users, and doesn't have a huge impact on Spark contributors' productivity (sure, it's a bit unpleasant, but that's life). If we break compatibility this way too quickly, we fragment the user community, and then either people have a crappy experience with Spark because their corporate IT doesn't yet have an environment that can run the latest version, or worse, they create more maintenance burden for us because they ask for more patches to be backported to old Spark versions (1.6.x, 2.0.x, etc). Python in particular is pretty fundamental to many Linux distros.
>> 
>> In the future, rather than just looking at when some software came out, it may be good to have some criteria for when to drop support for something. For example, if there are really nice libraries in Python 2.7 or Java 8 that we're missing out on, that may be a good reason. The maintenance burden for multiple Scala versions is definitely painful but I also think we should always support the latest two Scala releases.
>> 
>> Matei
>> 
>>> On Oct 27, 2016, at 12:15 PM, Reynold Xin <rxin@databricks.com  wrote:
>>> 
>>> I created a JIRA ticket to track this: https://issues.apache.org/jira/browse/SPARK-18138  
>>> 
>>> 
>>> On Thu, Oct 27, 2016 at 10:19 AM, Steve Loughran <stevel@hortonworks.com  wrote:
>>> 
>>>> On 27 Oct 2016, at 10:03, Sean Owen <sowen@cloudera.com  wrote:
>>>> 
>>>> Seems OK by me.
>>>> How about Hadoop < 2.6, Python 2.6? Those seem more removeable. I'd like to add that to a list of things that will begin to be unsupported 6 months from now.
>>>> 
>>> 
>>> If you go to java 8 only, then hadoop 2.6+ is mandatory. 
>>> 
>>> 
>>>> On Wed, Oct 26, 2016 at 8:49 PM Koert Kuipers <koert@tresata.com  wrote:
>>>> that sounds good to me
>>>> 
>>>> On Wed, Oct 26, 2016 at 2:26 PM, Reynold Xin <rxin@databricks.com  wrote:
>>>> We can do the following concrete proposal:
>>>> 
>>>> 1. Plan to remove support for Java 7 / Scala 2.10 in Spark 2.2.0 (Mar/Apr 2017).
>>>> 
>>>> 2. In Spark 2.1.0 release, aggressively and explicitly announce the deprecation of Java 7 / Scala 2.10 support.
>>>> 
>>>> (a) It should appear in release notes, documentations that mention how to build Spark
>>>> 
>>>> (b) and a warning should be shown every time SparkContext is started using Scala 2.10 or Java 7.
>>>> 
>>> 
>>> 
>> 
>> 
> 


It might be useful to ask Apache Infra whether they have any information on these (e.g. what do their own spam metrics say, do they get any feedback from Google, etc). Unfortunately mailing lists seem to be less and less well supported by most email providers.

Matei

> On Nov 2, 2016, at 6:48 AM, Pete Robbins  wrote:
> 
> I have gmail filters to add labels and skip inbox for anything sent to dev@spark user@spark etc but still get the occasional message marked as spam
> 
> 
> On Wed, 2 Nov 2016 at 08:18 Sean Owen <sowen@cloudera.com  wrote:
> I couldn't figure out why I was missing a lot of dev@ announcements, and have just realized hundreds of messages to dev@ over the past month or so have been marked as spam for me by Gmail. I have no idea why but it's usually messages from Michael and Reynold, but not all of them. I'll see replies to the messages but not the original. Who knows. I can make a filter. I just wanted to give a heads up in case anyone else has been silently missing a lot of messages.


The Kafka source will only appear in 2.0.2 -- see this thread for the current release candidate: https://lists.apache.org/thread.html/597d630135e9eb3ede54bb0cc0b61a2b57b189588f269a64b58c9243@%3Cdev.spark.apache.org%3E . You can try that right now if you want from the staging Maven repo shown there. The vote looks likely to pass so an actual release should hopefully also be out soon.

Matei

> On Nov 6, 2016, at 5:25 PM, shyla deshpande  wrote:
> 
> Hi Jaya!
> 
> Thanks for the reply. Structured streaming works fine for me with socket text stream . I think structured streaming with kafka source not yet supported.
> 
> Please if anyone has got it working with kafka source, please provide me some sample code or direction.
> 
> Thanks
> 
> 
> On Sun, Nov 6, 2016 at 5:17 PM, Jayaradha Natarajan <jayaradhaa@gmail.com  wrote:
> Shyla!
> 
> Check
> https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html  
> Thanks,
> Jayaradha
> 
> On Sun, Nov 6, 2016 at 5:13 PM, shyla <deshpandeshyla@gmail.com  wrote:
> I am trying to do Structured Streaming with Kafka Source. Please let me know
> where I can find some sample code for this. Thanks
> 
> 
> 
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Structured-Streaming-with-Kafka-Source-does-it-work-tp19748.html  Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
> 
> ---------------------------------------------------------------------
> To unsubscribe e-mail: dev-unsubscribe@spark.apache.org  
> 
> 


