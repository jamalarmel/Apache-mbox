We experienced similar problem when implementing LDA on Spark.  Now we call
RDD.checkpoint every 10 iterations to cut the lineage DAG.  Notice that
checkpointing hurts performance since it submits a job to write HDFS.
On Tue, Jan 28, 2014 at 5:15 PM, Qiuzhuang Lian  I see this error thrown from Executor.scala:
+1
We can maintain a contrib component just like what Akka and many other projects do.  With the help of Scala implicits, extensions to the RDD API can be done in a non-intrusive way and leave spark-core untouched.
On Feb 23, 2014, at 2:06 PM, Mridul Muralidharan  wrote:
Hi all,
Due to a bug  of Ivy, SBT
tries to download .orbit instead of .jar files and causing problems. This
bug has been fixed in Ivy 2.3.0, but SBT 0.13.1 still uses Ivy 2.0. Aaron
had kindly provided a workaround in PR
#183,
but I'm afraid only explicitly depend on javax.servlet only is not enough.
I'm not pretty sure about this because I'm facing both this issue and
ridiculously unstable network environment, which makes reproducing the bug
extremely time consuming (sbt gen-idea costs at least half an hour to
complete, and the generated result is broken. Most of the time was spent in
dependency resolution).
At last, I worked around this issue by updating my local SBT to 0.13.2-RC1.
If any of you are experiencing similar problem, I suggest you upgrade your
local SBT version. Since SBT 0.13.2-RC1 is not an official release, we have
to build it from scratch:
$ git clone git@github.com:sbt/sbt.git 
$ cd 
$ git checkout v0.13.2-RC1
Now ensure you have SBT 0.13.1 installed as the latest stable version is
required for bootstrapping:
$ sbt publishLocal
$ mv ~/.sbt/boot /tmp
$ cd /bin
$ mv sbt-launch.jar sbt-launch-0.13.1.jar
$ ln -sf /target/sbt-launch-0.13.2-RC1.jar
sbt-launch.jar
Now you should be able to build Spark without worrying .orbit files. Hope
it helps.
Cheng
AFAIK, according a recent talk, Hulu team in China has built Spark SQL
against Hive 0.13 (or 0.13.1?) successfully. Basically they also
re-packaged Hive 0.13 as what the Spark team did. The slides of the talk
hasn't been released yet though.
On Tue, Jul 29, 2014 at 1:01 AM, Ted Yu  wrote:
Transifex (https://www.transifex.com/) may be helpful if you're considering
online crowdsourcing. At least you may easily maintain a unified glossary
with it among all translators.
On Thu, Jul 31, 2014 at 3:40 PM, Yu Ishikawa  Hi Nick,
Transifex (https://www.transifex.com/) may be helpful if you're considering
online crowdsourcing. At least you may easily maintain a unified glossary
with it among all translators.
On Thu, Jul 31, 2014 at 3:40 PM, Yu Ishikawa  Hi Nick,
Just opened a PR based on the branch Patrick mentioned for this issue
https://github.com/apache/spark/pull/1864
On Sat, Aug 9, 2014 at 6:48 AM, Patrick Wendell  wrote:
You can just start the work :)
On Thu, Aug 28, 2014 at 3:52 PM, Bill Bejeck  wrote:
+1. Tested Spark SQL Thrift server and CLI against a single node standalone
cluster.
On Thu, Aug 28, 2014 at 9:27 PM, Timothy Chen  wrote:
Yea, SSD + SPARK_PREPEND_CLASSES totally changed my life :)
Maybe we should add a "developer notes" page to document all these useful
black magic.
On Tue, Sep 2, 2014 at 10:54 AM, Reynold Xin  wrote:
Cool, didn't notice that, thanks Josh!
On Tue, Sep 2, 2014 at 11:55 AM, Josh Rosen  wrote:
+1.
Tested locally on OSX 10.9, built with Hadoop 2.4.1
- Checked Datanucleus jar files
- Tested Spark SQL Thrift server and CLI under local mode and standalone
cluster against MySQL backed metastore
On Wed, Sep 3, 2014 at 11:25 AM, Josh Rosen  wrote:
Since we can easily catch the list of all changed files in a PR, I think we can start with adding the no trailing space check for newly changed files only?
The foreign data source API PR also matters here https://www.github.com/apache/spark/pull/2475
Foreign data source like ORC can be added more easily and systematically after this PR is merged.
Hi Marcelo, yes this is a known Spark SQL bug and we've got PRs to fix it
(2887 & 2967). Not merged yet because newly merged Hive 0.13.1 support
causes some conflicts. Thanks for reporting this :)
On Tue, Oct 28, 2014 at 6:41 AM, Marcelo Vanzin  wrote:
My two cents for Mac Vim/Emacs users. Fixed a Scala ctags Mac compatibility
bug months ago, and you may want to use the most recent version here
https://github.com/scala/scala-dist/blob/master/tool-support/src/emacs/contrib/dot-ctags
On Tue, Oct 28, 2014 at 4:26 PM, Duy Huynh  wrote:
Hao Cheng had just written such a "from scratch" guide for building Spark SQL in IDEA. Although it's written in Chinese, I think the illustrations are already descriptive enough.
http://www.cnblogs.com/cccchhhh/articles/4058371.html
Hm, the shim source folder could be automatically recognized some time before, although at a wrong directory level (sql/hive/v0.12.0/src instead of sql/hive/v0.12.0/src/main/scala), it compiles.
Just tried against a fresh checkout, indeed need to add shim source folder manually. Sorry for the confusion.
Cheng
I often see this when I first build the whole Spark project with SBT, then
modify some code and tries to build and debug within IDEA, or vice versa. A
clean rebuild can always solve this.
On Mon, Nov 3, 2014 at 11:28 AM, Patrick Wendell  wrote:
Hey Sadhan,
I really don't think this is Spark log... Unlike Shark, Spark SQL doesn't even provide a Hive mode to let you execute queries against Hive. Would you please check whether there is an existing HiveServer2 running there? Spark SQL HiveThriftServer2 is just a Spark port of HiveServer2, and they share the same default listening port. I guess the Thrift server didn't start successfully because the HiveServer2 occupied the port, and your Beeline session was probably linked against HiveServer2.
Cheng
Hey Sadhan,
Sorry for my previous abrupt reply. Submitting a MR job is definitely wrong here, I'm investigating. Would you mind to provide the Spark/Hive/Hadoop versions you are using? If you're using most recent master branch, a concrete commit sha1 would be very helpful.
Thanks!
Cheng
+1
Tested HiveThriftServer2 against Hive 0.12.0 on Mac OS X. Known issues are fixed. Hive version inspection works as expected.
Talked with Yi offline, personally I think this feature is pretty useful, and the design makes sense, and he's already got a running prototype.
Yi, would you mind to open a PR for this? Thanks!
Cheng
Hi Aniket,
In general the schema of all rows in a single table must be same. This is a basic assumption made by Spark SQL. Schema union does make sense, and we're planning to support this for Parquet. But as you've mentioned, it doesn't help if types of different versions of a column differ from each other. Also, you need to reload the data source table after schema changes happen.
Cheng
Yes, when a DataFrame is cached in memory, it's stored in an efficient columnar format. And you can also easily persist it on disk using Parquet, which is also columnar.
Cheng
It's already fixed in the master branch. Sorry that we forgot to update this before releasing 1.2.0 and caused you trouble...
Cheng
How about persisting the computed result table first before caching it? So that you only need to cache the result table after restarting your service without recomputing it. Somewhat like checkpointing.
Cheng
My bad, had once fixed all Hive 12 test failures in PR #4107, but didn't got time to get it merged.
Considering the release is close, I can cherry-pick those Hive 12 fixes from #4107 and open a more surgical PR soon.
Cheng
Hi Masaki,
I guess what you saw is the partition number of the last stage, which must be 1 to perform the global phase of LIMIT. To tune partition number of normal shuffles like joins, you may resort to spark.sql.shuffle.partitions.
Cheng
Hey Andrew,
Would you please create a JIRA ticket for this? To preserve compatibility with existing Hive JDBC/ODBC drivers, Spark SQL's HiveThriftServer intercepts some HiveServer2 components and injects Spark stuff into it. This makes the implementation details are somewhat hacky (e.g. a bunch of reflection tricks were used). We haven't include KRB tests in Spark unit/integration test suites, and it's possible that HiveThriftServer2 somehow breaks Hive's KRB feature.
Cheng
It's still marked as 1.2.1 here http://spark.apache.org/docs/latest/
But this page is updated (1.3.0) http://spark.apache.org/docs/latest/index.html
Cheng
---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
Patrick, Ted - My bad, yeah, it's because of browser cache.
Hey Gil,
ParquetRelation2 is based on the external data sources API, which is a more modular and non-intrusive way to add external data sources to Spark SQL. We are planning to replace ParquetRelation with ParquetRelation2 entirely after the latter is more mature and stable. That's why you see two separate sets of Parquet code in the code base, and currently they also share part of the code.
In Spark 1.3, the new Parquet data source (ParquetRelation2) is enabled by default. So you can find entries of projection and filter push-down code in newParquet.scala.
Cheng
We're planning to replace the current Hive version profiles and shim layer with an adaption layer in Spark SQL in 1.4. This adaption layer allows Spark SQL to connect to arbitrary Hive version greater than or equal to 0.12.0 (or maybe 0.13.1, not decided yet).
However, it's not a promise yet, since this requires major refactoring of the current Spark SQL Hive support.
Cheng
Hi Pat,
I don't understand what "lazy casting" mean here. Why do you think current Catalyst casting is "eager"? Casting happens at runtime, and doesn't disable column pruning.
Cheng
I found in general it's a pain to build/run Spark inside IntelliJ IDEA. I guess most people resort to this approach so that they can leverage the integrated debugger to debug and/or learn Spark internals. A more convenient way I'm using recently is resorting to the remote debugging feature. In this way, by adding driver/executor Java options, you may build and start the Spark applications/tests/daemons in the normal way and attach the debugger to it. I was using this to debug the HiveThriftServer2, and it worked perfectly.
Steps to enable remote debugging:
1. Menu "Run / Edit configurations..."
2. Click the "+" button, choose "Remote"
3. Choose "Attach" or "Listen" in "Debugger mode" according to your actual needs
4. Copy, edit, and add Java options suggested in the dialog to `--driver-java-options` or `--executor-java-options`
5. If you're using attaching mode, first start your Spark program, then start remote debugging in IDEA
6. If you're using listening mode, first start remote debugging in IDEA, and then start your Spark program.
Hope this can be helpful.
Cheng
Thanks for reporting this! Would you mind to open JIRA tickets for both Spark and Parquet?
I'm not sure whether Parquet declares somewhere the user mustn't reuse byte arrays when using binary type. If it does, then it's a Spark bug. Anyway, this should be fixed.
Cheng
Yeah, SQL is the right component. Thanks!
Cheng
Would you mind to open a JIRA for this?
I think your suspicion makes sense. Will have a look at this tomorrow. Thanks for reporting!
Cheng
Hi all,
The unreleased version 1.6.0 has was removed from JIRA due to my misoperation. I've added it back, but JIRA tickets that once targeted to 1.6.0 now have empty target version/s. If you found tickets that should have targeted to 1.6.0, please help marking the target version/s field back to 1.6.0.
Thanks in advance and sorry for all the trouble!
Best,
Cheng
---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
Hi Gil,
Sorry for the late reply and thanks for raising this question. The file listing logic in HadoopFsRelation is intentionally made different from Hadoop FileInputFormat. Here are the reasons:
1. Efficiency: when computing RDD partitions, FileInputFormat.listStatus() is called on the driver side in a sequential manner, and can be slow for S3 directories with lots of sub-directories, e.g. partitioned tables with thousands or even more partitions. This is partly because file metadata operation can be very slow on S3. HadoopFsRelation relies on this file listing action to do partition discovery, and we've made a distributed parallel version in Spark 1.5: we first list input paths on driver side in a sequential breadth-first manner, and once we find the number of directories to be listed exceeds a threshold (32 by default), we launch a Spark job to do file listing. With this mechanism, we've observed 2 orders of magnitude performance boost when reading partitioned table with thousands of distinct partitions located on S3.
2. Semantics difference: the default hiddenFileFilter doesn't apply in every cases. For example, Parquet summary files _metadata and _common_metadata plays crucial roles in schema discovery and schema merging, and we don't want to exclude them when listing the files. But they are removed when reading the actual data. However, we probably should allow users to pass in user defined path filters.
Cheng
Yeah, two of the reasons why the built-in in-memory columnar storage doesn't achieve comparable compression ratio as Parquet are:
1. The in-memory columnar representation doesn't handle nested types. So array/map/struct values are not compressed.
2. Parquet may use more than one kind of compression methods to compress a single column. For example, dictionary  + RLE.
Cheng
Hey Hyukjin,
Sorry that I missed the JIRA ticket. Thanks for bring this issue up here, your detailed investigation.
From my side, I think this is a bug of Parquet. Parquet was designed to support schema evolution. When scanning a Parquet, if a column exists in the requested schema but missing in the file schema, that column is filled with null. This should also hold for pushed-down predicate filters. For example, if filter "a = 1" is pushed down but column "a" doesn't exist in the Parquet file being scanned, it's safe to assume "a" is null in all records and drop all of them. On the contrary, if "a IS NULL" is pushed down, all records should be preserved.
Apparently, before this issue is properly fixed on Parquet side, we need to workaround this issue from Spark side. Please see my comments of all 3 of your solutions inlined below. In short, I'd like to have approach 1 for branch-1.5 and approach 2 for master.
Cheng
Hi Shane,
I found that Jenkins has been in the status of "Jenkins is going to shut
down" for at least 4 hours (from ~23:30 Dec 9 to 3:45 Dec 10, PDT). Not
sure whether this is part of the schedule or related?
Cheng
On Thu, Dec 10, 2015 at 3:56 AM, shane knapp  wrote:
+1
Awesome! Congrats and welcome!!
Awesome! Congrats and welcome!!
Cheng
On Tue, Feb 9, 2016 at 2:55 AM, Shixiong(Ryan) Zhu  Congrats!!! Herman and Wenchen!!!
Hey Pedro,
SQL programming guide is being updated. Here's the PR, but not merged yet: https://github.com/apache/spark/pull/13592
Cheng
As mentioned in the PR description, this is just an initial PR to bring existing contents up to date, so that people can add more contents incrementally.
We should definitely cover more about Dataset.
Cheng
Congratulations!!!
Cheng
On Tue, Oct 4, 2016 at 1:46 PM, Reynold Xin  wrote:
Hey Dongjoon,
Thanks for reporting. I'm looking into these OOM errors. Already reproduced them locally but haven't figured out the root cause yet. Gonna disable them temporarily for now.
Sorry for the inconvenience!
Cheng
JIRA: https://issues.apache.org/jira/browse/SPARK-18403
PR: https://github.com/apache/spark/pull/15845
Will merge it as soon as Jenkins passes.
Cheng
Finished reviewing the list and it LGTM now (left comments in the spreadsheet and Ryan already made corresponding changes).
Ryan - Thanks a lot for pushing this and making it happen!
Cheng
This one seems to be relevant, but it's already fixed in 2.1.0.
One way to debug is to turn on trace log and check how the analyzer/optimizer behaves.
