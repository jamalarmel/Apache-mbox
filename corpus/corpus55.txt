When enabling mergedSchema and predicate filter, this fails since Parquet filters are pushed down regardless of each schema of the splits (or rather files). Dominic Ricard reported this issue (https://issues.apache.org/jira/browse/SPARK-11103) Even though this would work okay by setting spark.sql.parquet.filterPushdown to false, the default value of this is true. So this looks an issue. My questions are, is this clearly an issue? and if so, which way would this be handled? I thought this is an issue and I made three rough patches for this and tested them and this looks fine though. The first approach looks simpler and appropriate as I presume from the previous approaches such as https://issues.apache.org/jira/browse/SPARK-11153 However, in terms of safety and performances, I also want to ensure which one would be a proper approach before trying to open a PR. 1. Simply set false to spark.sql.parquet.filterPushdown when using mergeSchema 2. If spark.sql.parquet.filterPushdown is true, retrieve all the schema of every part-files (and also merged one) and check if each can accept the given schema and then, apply the filter only when they all can accept, which I think it's a bit over-implemented. 3. If spark.sql.parquet.filterPushdown is true, retrieve all the schema of every part-files (and also merged one) and apply the filter to each split (rather file) that can accept the filter which (I think it's hacky) ends up different configurations for each task in a job. Hi all, I am writing this email to both user-group and dev-group since this is applicable to both. I am now working on Spark XML datasource (https://github.com/databricks/spark-xml). This uses a InputFormat implementation which I downgraded to Hadoop 1.x for version compatibility. However, I found all the internal JSON datasource and others in Databricks use Hadoop 2.x API dealing with TaskAttemptContextImpl by reflecting the method for this because TaskAttemptContext is a class in Hadoop 1.x and an interface in Hadoop 2.x. So, I looked through the codes for some advantages for Hadoop 2.x API but I couldn't. I wonder if there are some advantages for using Hadoop 2.x API. I understand that it is still preferable to use Hadoop 2.x APIs at least for future differences but somehow I feel like it might not have to use Hadoop 2.x by reflecting a method. I would appreciate that if you leave a comment here https://github.com/databricks/spark-xml/pull/14 as well as sending back a reply if there is a good explanation Thanks! Hi all, I usually have been working with Spark in IntelliJ. Before this PR, https://github.com/apache/spark/commit/7cd7f2202547224593517b392f56e49e4c94cabc for `[SPARK-12575][SQL] Grammar parity with existing SQL parser`. I was able to just open the project and then run some tests with IntelliJ Run button. However, it looks that PR adds some ANTLR files for parsing and I cannot run the tests as I did. So, I ended up with doing this by mvn compile first and then running some tests with IntelliJ. I can still run some tests with sbt or maven in comment line but this is a bit inconvenient. I just want to run some tests as I did in IntelliJ. I followed this https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools several times but it still emits some exceptions such as Error:(779, 34) not found: value SparkSqlParser ^ and I still should run mvn compile or mvn test first for them. Is there any good way to run some Spark tests within IntelliJ as I did before? Thanks! Hi all, I am planning to submit a PR for https://issues.apache.org/jira/browse/SPARK-8000. Currently, file format is not detected by the file extension unlike compression codecs are being detected. I am thinking of introducing another interface (a function) at DataSourceRegister just like shortName() at in order to specify possible file exceptions so that we can detect datasources by file extensions just like Hadoop does for compression codecs. Since adding an interface should be carefully done, I want to first ask if this approach looks appropriate. Could you please give me some feedback for this? Thanks! Hi all, While I am testing some codes in PySpark, I met a weird issue. This works fine at Spark 1.6.0 but it looks it does not for Spark 2.0.0. When I simply run *logData = sc.textFile(path).coalesce(1) *with some big files in stand-alone local mode without HDFS, this simply throws the exception, *_fill_function() takes exactly 4 arguments (5 given)* I firstly wanted to open a Jira for this but feel like it is a too primitive functionality and I felt like I might be doing this wrong. The full error message is below: 16/03/07 11:12:44 INFO rdd.HadoopRDD: Input split: file:/Users/hyukjinkwon/Desktop/workspace/local/spark-local-ade/spark/data/00_REF/20160119000000-20160215235900-TROI_STAT_ADE_0.DAT:2415919104+33554432 *16/03/07 11:12:44 INFO rdd.HadoopRDD: Input split: file:/Users/hyukjinkwon/Desktop/workspace/local/spark-local-ade/spark/data/00_REF/20160119000000-20160215235900-TROI_STAT_ADE_0.DAT:805306368+33554432* *16/03/07 11:12:44 INFO rdd.HadoopRDD: Input split: file:/Users/hyukjinkwon/Desktop/workspace/local/spark-local-ade/spark/data/00_REF/20160119000000-20160215235900-TROI_STAT_ADE_0.DAT:0+33554432* *16/03/07 11:12:44 INFO rdd.HadoopRDD: Input split: file:/Users/hyukjinkwon/Desktop/workspace/local/spark-local-ade/spark/data/00_REF/20160119000000-20160215235900-TROI_STAT_ADE_0.DAT:1610612736+33554432* *16/03/07 11:12:44 ERROR executor.Executor: Exception in task 2.0 in stage 0.0 (TID 2)* *org.apache.spark.api.python.PythonException: Traceback (most recent call last):* *  File "./python/pyspark/worker.py", line 98, in main* *    command = pickleSer._read_with_length(infile)* *  File "./python/pyspark/serializers.py", line 164, in _read_with_length* *    return self.loads(obj)* *  File "./python/pyspark/serializers.py", line 422, in loads* *    return pickle.loads(obj)* *TypeError: ('_fill_function() takes exactly 4 arguments (5 given)', * at org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:168)* * at org.apache.spark.api.python.PythonRunner$$anon$1.(PythonRDD.scala:209)* * at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:127)* * at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:62)* * at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)* * at org.apache.spark.rdd.RDD.iterator(RDD.scala:277)* * at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:349)* * at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)* * at org.apache.spark.rdd.RDD.iterator(RDD.scala:277)* * at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:77)* * at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:45)* * at org.apache.spark.scheduler.Task.run(Task.scala:82)* * at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)* * at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)* * at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)* * at java.lang.Thread.run(Thread.java:745)* *16/03/07 11:12:44 ERROR executor.Executor: Exception in task 3.0 in stage 0.0 (TID 3)* *org.apache.spark.api.python.PythonException: Traceback (most recent call last):* *  File "./python/pyspark/worker.py", line 98, in main* *    command = pickleSer._read_with_length(infile)* *  File "./python/pyspark/serializers.py", line 164, in _read_with_length* *    return self.loads(obj)* *  File "./python/pyspark/serializers.py", line 422, in loads* *    return pickle.loads(obj)* *TypeError: ('_fill_function() takes exactly 4 arguments (5 given)', Thanks! Just in case, My python version is 2.7.10. 2016-03-07 11:19 GMT+09:00 Hyukjin Kwon  Hi all, Hi all, Currently, the output from CSV, TEXT and JSON data sources does not have file extensions such as .csv, .txt and .json (except for compression extensions such as .gz, .deflate and .bz4). In addition, it looks Parquet has the extensions such as .gz.parquet or .snappy.parquet according to compression codecs whereas ORC does not have such extensions but it is just .orc. I tried to search some JIRAs related with this but I could not find yet but I did not open a JIRA directly because I feel like this is already concerned Maybe could I open a JIRA for this inconsistent file extensions? It would be thankful if you give me some feedback Thanks! This discussion is going to the Jira. Please refer the Jira if anyone is interested in this. On 9 Mar 2016 6:31 p.m., "Sean Owen"  wrote: Hi all, I recently noticed that actually there are some usages of functional transformations (eg. map, foreach and etc.) with extra anonymous closure. For example, ... }) which can be just simply as below: ... } I wrote a regex to find all of them and corrected them for a PR (I did not submit yet). However, I feel a bit hesitating because only reasons I can think for this are, firstly, Spark coding guides in both https://github.com/databricks/scala-style-guide and https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide are not using the examples as above secondly, I feel like extra anonymous closure can harm performance but I am too sure, which I think are not persuasive enough. To cut it short, my questions are, 1. Would this be a proper change for a PR? 2. Would there be more explicit reasons to remove extra closure not only for coding style? Thanks! Yea I agree with you all. Just let you know, this was anyway fixed in https://github.com/apache/spark/commit/6fc3dc8839eaed673c64ec87af6dfe24f8cebe0c On 14 Apr 2016 5:13 p.m., "Takeshi Yamamuro"  wrote: your two examples the same way that you do.  Looking at a few similar cases, I've only found the bytecode produced to be the same regardless of which style is used. transformations (eg. map, foreach and etc.) with extra anonymous closure. not submit yet). this are, https://github.com/databricks/scala-style-guide and https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide are not using the examples as above but I am too sure, only for coding style? Hi all, First of all, I am sorry that this is relatively trivial and too minor but I just want to be clear on this and careful for the more PRs in the future. Recently, I have submitted a PR (https://github.com/apache/spark/pull/12413) about Scala style and this was merged. In this PR, I changed 1. from ... }) to ... } 2. from to 3. from function(x) } to .map(function(_)) My question is, I think it looks 2. and 3. are arguable (please see the discussion in the PR). I agree that I might not have to change those in the future but I just wonder if I should revert 2. and 3.. FYI, - The usage of 2. is pretty rare. - 3. is pretty a lot. but the PR corrects ones like above only when the val within closure looks obviously meaningless (such as x or a) and with only single line. I would appreciate that if you add some comments and opinions on this. Thanks! Hi Mark, I know but that could harm readability. AFAIK, for this reason, that is not (or rarely) used in Spark. 2016-04-17 15:54 GMT+09:00 Mark Hamstra  FWIW, 3 should work as just `.map(function)`. +1 Yea, I am facing this problem as well, https://github.com/apache/spark/pull/12452 I thought they are spurious because the tests are passed in my local. 2016-04-18 3:26 GMT+09:00 Kazuaki Ishizaki  I realized that recent Jenkins among different pull requests always fails I also think this might not have to be closed only because it is inactive. How about closing issues after 30 days when a committer's comment is added at the last without responses from the author? IMHO, If the committers are not sure whether the patch would be useful, then I think they should leave some comments why they are not sure, not just ignoring. Or, simply they could ask the author to prove that the patch is useful or safe with some references and tests. I think it might be nicer than that users are supposed to keep pinging. **Personally**, apparently, I am sometimes a bit worried if pinging multiple times can be a bit annoying. 2016-04-19 9:56 GMT+09:00 Saisai Shao  It would be better to have a specific technical reason why this PR should Hi all, This was similar with the proposal of closing PRs before I asked. I think the PRs suggested to be closed below are closable but not very sure of PRs apparently abandoned by its author at least for a month. I remember the discussion about auto-closing PR before. So, I included the PRs as below anyway. I looked though the open every PR at this time and could make a list as below: 1. Suggested to be closed. https://github.com/apache/spark/pull/7739  <- not sure https://github.com/apache/spark/pull/9354 https://github.com/apache/spark/pull/9451 https://github.com/apache/spark/pull/10507 https://github.com/apache/spark/pull/10486 https://github.com/apache/spark/pull/10460 https://github.com/apache/spark/pull/10967 https://github.com/apache/spark/pull/10945 https://github.com/apache/spark/pull/10701 https://github.com/apache/spark/pull/10681 https://github.com/apache/spark/pull/11766 2. Author not answering at least for a month. https://github.com/apache/spark/pull/9907 https://github.com/apache/spark/pull/9920 https://github.com/apache/spark/pull/9936 https://github.com/apache/spark/pull/10052 https://github.com/apache/spark/pull/10125 https://github.com/apache/spark/pull/10209 https://github.com/apache/spark/pull/10572 <- not sure https://github.com/apache/spark/pull/10326 https://github.com/apache/spark/pull/10379 https://github.com/apache/spark/pull/10403 https://github.com/apache/spark/pull/10466 https://github.com/apache/spark/pull/10572 <- not sure https://github.com/apache/spark/pull/10995 https://github.com/apache/spark/pull/10887 https://github.com/apache/spark/pull/10842 https://github.com/apache/spark/pull/11005 https://github.com/apache/spark/pull/11036 https://github.com/apache/spark/pull/11129 https://github.com/apache/spark/pull/11610 https://github.com/apache/spark/pull/11729 https://github.com/apache/spark/pull/11980 https://github.com/apache/spark/pull/12075 Thanks. Sorry, here are two more PRs I guess should be closed. 10052  dereksabryfb   - This one is suggested by Sean and Herman to close 10356  somideshmukh   - This has not been confirmed by a committer yet but I tested this and it seems not an issue. Thanks. 2016-05-07 2:47 GMT+09:00 Sean Owen  Skimmed these as a one-off exercise, and I suggest ... I happened to test SparkR in Windows 7 (32 bits) and it seems some tests are failed. Could this be a reason to downvote? For more details of the tests, please see https://github.com/apache/spark/pull/13165#issuecomment-220515182 2016-05-20 13:44 GMT+09:00 Takuya UESHIN  Hi all, +1 -  I wouldn't be bothered if a build becomes longer if I can write cleaner codes without manually running it. I have just looked though the related PRs and JIRAs and it looks generally okay and reasonable to me. 2016-05-23 18:54 GMT+09:00 Steve Loughran  On 23 May 2016, at 05:21, Dongjoon Hyun  wrote: Congratulations! 2016-06-04 11:48 GMT+09:00 Matei Zaharia  Hi all, For the question 1., It is possible but not supported yet. Please refer https://github.com/apache/spark/pull/13775 Thanks! 2016-07-25 19:01 GMT+09:00 Ovidiu-Cristian MARCU  Hi, Hi all, PR: https://github.com/apache/spark/pull/14118 JIRAs https://issues.apache.org/jira/browse/SPARK-17290 https://issues.apache.org/jira/browse/SPARK-16903 https://issues.apache.org/jira/browse/SPARK-16462 https://issues.apache.org/jira/browse/SPARK-16460 https://issues.apache.org/jira/browse/SPARK-15144 It seems this is not critical but the duplicated issues are being opened due to this. Also, it seems many users are affected by this. I hope this email make some interests for reviewers. Thanks. I was also actually wondering why it is being written like this. I actually took a look for this before and wanted to fix them but I found https://github.com/apache/spark/pull/12077/files#r58041468 So, I kind of persuaded myself that committers already know about it and there is a reason for this. I'd like to know the full details why we don't import but write full path though. On 9 Sep 2016 5:28 a.m., "Jakob Odersky"  wrote: Then, are we going to submit a PR and fix this maybe? On 9 Sep 2016 9:30 p.m., "Sean Owen"  wrote: Congratulations! 2016-10-04 15:51 GMT+09:00 Dilip Biswal  Hi Xiao, I am uncertain too. It'd be great if these are documented too. FWIW, in my case, I privately asked and told Sean first that I am going to look though the JIRAs and resolve some via the suggested conventions from Sean. (Definitely all blames should be on me if I have done something terribly wrong). 2016-10-08 22:37 GMT+09:00 Cody Koeninger  That makes sense, thanks. Hi all, I just noticed the README.md (https://github.com/apache/spark) does not describe the steps or links to follow for creating a PR or JIRA directly. I know probably it is sensible to search google about the contribution guides first before trying to make a PR/JIRA but I think it seems not enough when I see some inappropriate PRs/JIRAs time to time. I guess flooding JIRAs and PRs is problematic (assuming from the emails in dev mailing list) and I think we should explicitly mention and describe this in the README.md and pull request template[1]. (I know we have CONTBITUTING.md[2] and wiki[3] but it seems pretty true that we still have some PRs or JIRAs not following the documentation.) So, my suggestions are as below: - Create a section maybe "Contributing To Apache Spark" describing the Wiki and CONTRIBUTING.md[2] in the README.md. - Describe an explicit warning in pull request template[1], for example, "Please double check if your pull request is from a branch to a branch. In most cases, this change is not appropriate. Please ask to mailing list (http://spark.apache.org/community.html) if you are not sure."[1]https://github.com/apache/spark/blob/master/.github/PULL_REQUEST_TEMPLATE [2]https://github.com/apache/spark/blob/master/CONTRIBUTING.md [3]https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage Thank you all. Thanks for confirming this, Sean. I filed this in https://issues.apache.org/jira/browse/SPARK-17840 I would appreciate if anyone who has a better writing skills better than me tries to fix this. I don't want to let reviewers make an effort to correct the grammar. On 10 Oct 2016 1:34 a.m., "Sean Owen"  wrote: Just as one of those who subscribed to dev/user mailing list, I would like to avoid to recieve flooding emails about job recruiting. In my personal opinion, I think that might mean virtually allowing that this list is being used as the mean for some profits in an organisation. On 7 Oct 2016 5:05 p.m., "Sean Owen"  wrote: Hi, The reason is just simply JSON data source depends on Hadoop's LineRecordReader when we first try to read the files. There is a workaround for this here in this link, http://searchdatascience.com/spark-adventures-1-processing-multi-line-json-files/ I hope this is helpful. Thanks! 2016-10-16 11:20 GMT+09:00 WangJianfei  Hi devs: +1 if the docs can be exposed more. On 19 Oct 2016 2:04 a.m., "Shivaram Venkataraman"  wrote: Hi all, First of all, this might be minor but I just have been curious of different PR titles in particular component part. So, I looked through Spark wiki again and I found the description not quite the same with the PRs. It seems, it is said, Pull Request ... 1. The PR title should be of the form [SPARK-xxxx] [COMPONENT] Title, where SPARK-xxxx is the relevant JIRA number, COMPONENT is one of the PR categories shown at https://spark-prs.appspot.com/ and Title may be the JIRA's title or a more specific title describing the PR itself. ... So, It seems the component should be one of SQL, MLlib, Core, Python, Scheduler, Build, Docs, Streaming, Mesos, Web UI, YARN, GraphX, R. If the component refers the ones specified in the JIRA, then, it seems it should be one of Block Manager, Build, Deploy, Documentation, DStream, EC2, Examples, GraphX, Input/Output, Java API, Mesos, ML, MLlib, Optimizer, Project Infra, PySpark, Scheduler, Shuffle, Spark Core, Spark Shell, Spark Submit, SparkR, SQL, Structured Streaming, Tests, Web UI, Windows, YARN It seems they are a bit different with the PRs. I hope this is clarified in more details (including whether it should be all upper-cased or just the same with the component name maybe). Thanks. I see. I was just curious as I find myself hesitating when I open a PR time to time. Thank you both for echoing! On 14 Nov 2016 5:02 a.m., "Sean Owen"  wrote: Maybe it sounds like you are looking for from_json/to_json functions after en/decoding properly. On 16 Nov 2016 6:45 p.m., "kant kodali"  wrote: I believe https://github.com/apache/spark/pull/15975 fixes this regression. I am sorry for the trouble. 2016-11-25 22:23 GMT+09:00 Sean Owen  See https://github.com/apache/spark/pull/15499#discussion_r89008564 in Oh, Joseph, thanks. It is nice to inform this in dev mailing list. Let me please leave another PR to refer, https://github.com/apache/spark/pull/16013 and the JIRA you kindly opened, https://issues.apache.org/jira/browse/SPARK-18692 On 7 Feb 2017 9:13 a.m., "Joseph Bradley"  wrote: Public service announcement: Our doc build has worked with Java 8 for brief time periods, but new changes keep breaking the Java 8 unidoc build. Please be aware of this, and try to test doc changes with Java 8!  In general, it is stricter than Java 7 for docs. A shout out to @HyukjinKwon and others who have made many fixes for this! See these sample PRs for some issues causing failures (especially around links): https://github.com/apache/spark/pull/16741 https://github.com/apache/spark/pull/16604 Thanks, Joseph -- Joseph Bradley Software Engineer - Machine Learning Databricks, Inc. [image: http://databricks.com] (One more.. https://github.com/apache/spark/pull/15999 this describes some more cases that might easily be mistaken) On 7 Feb 2017 9:21 a.m., "Hyukjin Kwon"  wrote: Please take a look at https://issues.apache.org/jira/browse/SPARK-18359. 2017-02-23 21:53 GMT+09:00 Arkadiusz Bicz  Thank you Sam for answer, I have solved problem by loading all decimals I think we should ask to disable this within Web UI configuration. In this JIRA, https://issues.apache.org/jira/browse/INFRA-12590, Daniel said In the case of my accounts, I manually went to https://ci.appveyor.com/ notifications and configured them all as  "Do not send" and it does not send me any email. However, in case of AFS account, this turns out an assumption because I don't know how it is defined as I can't access. This might be defined in account - https://ci.appveyor.com/notifications or in project - https://ci.appveyor.com/project/ApacheSoftwareFoundation/ spark/settings I'd like to note that I disabled the notification in the appveyor.yml but it seems the configurations are merged in Web UI, according to the documentation (https://www.appveyor.com/ docs/notifications/#global-email-notifications). notifications defined in appveyor.yml. Should we maybe an INFRA JIRA to check and ask this? 2017-03-05 8:31 GMT+09:00 Shivaram Venkataraman  I'm not sure why the AppVeyor updates are coming to the dev list.  Hyukjin Oh BTW, I was asked about this by Reynold. Few month ago and I said the similar answer. I think I am not supposed to don't recieve the emails (not sure but I have not recieved) so I am not too sure if this has happened so far or occationally. On 5 Mar 2017 9:08 a.m., "Hyukjin Kwon"  wrote: I opened https://issues.apache.org/jira/browse/INFRA-13621. Thanks. 2017-03-05 10:07 GMT+09:00 Shivaram Venkataraman  Thanks for investigating. We should file an INFRA jira about this.