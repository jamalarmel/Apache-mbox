+1
Ran all catalyst tests against the RC using the staging repo.  Everything
looks good.
+1 to Shivaram's proposal.  I think we should try to avoid functions with
many args as much as possible so having a high vertical cost here isn't the
worst thing.  I also like the visual consistency.
FWIW, (based on a cursory inspection) in the scala compiler they don't seem
to ever orphan the return type from the closing parenthese.  It seems there
are two main accepted styles:
    def mkSlowPathDef(clazz: Symbol, lzyVal: Symbol, cond: Tree, syncBody:
List[Tree],
                      stats: List[Tree], retVal: Tree): Tree = {
and
    def tryToSetIfExists(
      cmd: String,
      args: List[String],
      setter: (Setting) => (List[String] => Option[List[String]])
    ): Option[List[String]] =
Sorry to revive an old thread, but I just ran into this issue myself.  It
is likely that you do not have the assembly jar built, or that you have
SPARK_HOME set incorrectly (it does not need to be set).
Michael
Hi Everyone,
I'm very excited about merging this new feature into Spark!  We have a lot
of cool things in the pipeline, including: porting Shark's in-memory
columnar format to Spark SQL, code-generation for expression evaluation and
improved support for complex types in parquet.
I would love to hear feedback on the interfaces, and what is missing.  In
particular, while we have pretty good test coverage for Hive, there has not
been a lot of testing with real Hive deployments and there is certainly a
lot more work to do.  So, please test it out and if there are any missing
features let me know!
Michael
There are examples in the Spark documentation.  Patrick posted and updated
copy here so people can see them before 1.0 is released:
http://people.apache.org/~pwendell/catalyst-docs/sql-programming-guide.html
Depending on how you use this, there is still a dependency on Hive (By
default this is not the case.  See the above documentation for more
details).  However, the dependency is on a stock version of Hive instead of
one modified by the AMPLab.  Furthermore, Spark SQL has its own optimizer,
instead of relying on the Hive optimizer.  Long term, this is going to give
us a lot more flexibility to optimize queries specifically for the Spark
execution engine.  We are actively porting over the best parts of shark
(specifically the in-memory columnar representation).
Shark still has some features that are missing in Spark SQL, including
SharkServer (and years of testing).  Once SparkSQL graduates from Alpha
status, it'll likely become the new backend for Shark.
Hey Everyone,
Here is a pretty major (but source compatible) change we are considering
making to the RDD API for 1.0.  Java and Python APIs would remain the same,
but users of Scala would likely need to use less casts.  This would be
especially true for libraries whose functions take RDDs as parameters.  Any
comments would be appreciated!
https://spark-project.atlassian.net/browse/SPARK-1296
Michael
This is exactly the kind of feedback I was hoping for!  Can you be any more
specific about the kinds of problems you ran into here?
Hi Pascal,
Thanks for the input.  I think we are going to be okay here since, as Koert
said, the current serializers use runtime type information.  We could also
keep at ClassTag around for the original type when the RDD was created.
 Good things to be aware of though.
Michael
Hi Evan,
Index support is definitely something we would like to add, and it is
possible that adding support for your custom indexing solution would not be
too difficult.
We already push predicates into hive table scan operators when the
predicates are over partition keys.  You can see an example of how we
collect filters and decide which can pushed into the scan using the
HiveTableScan
query planning strategy
.
I'd like to know more about your indexing solution.  Is this something
publicly available?  One concern here is that the query planning code is
not considered a public API and so is likely to change quite a bit as we
improve the optimizer.  Its not currently something that we plan to expose
for external components to modify.
Michael
Just a quick note to everyone that Patrick and I are playing around with
Travis CI on the Spark github repository.  For now, travis does not run all
of the test cases, so will only be turned on experimentally.  Long term it
looks like Travis might give better integration with github, so we are
going to see if it is feasible to get all of our tests running on it.
*Jenkins remains the reference CI and should be consulted before merging
pull requests, independent of what Travis says.*
If you have any questions or want to help out with the investigation, let
me know!
Michael
It is not finished and really at this point it is only something we are
considering, not something that will happen for sure.  We turned it on in
addition to Jenkins so that we could start finding issues exactly like the
ones you described below to determine if Travis is going to be a viable
option.
Basically it seems to me that the Travis environment is a little less
predictable (probably because of virtualization) and this is pointing out
some existing flakey-ness in the tests
If there are tests that are regularly flakey we should probably file JIRAs
so they can be fixed or switched off.  If you have seen a test fail 2-3
times and then pass with no changes, I'd say go ahead and file an issue for
it (others should feel free to chime in if we want some other process here)
A few more specific comments inline below.
Hmm, this is a little confusing.  Do you have a pointer to this one?  Was
there any other error?
Here I think the right thing to do is probably break the hive tests in two
and run them in parallel.  There is already machinery for doing this, we
just need to flip the options on in the travis.yml to make it happen.  This
is only going to get more critical as we whitelist more hive tests.  We
also talked about checking the PR and skipping the hive tests when there
have been no changes in catalyst/sql/hive.  I'm okay with this plan, just
need to find someone with time to implement it....
There is a JIRA for one of the flakey tests here:
https://issues.apache.org/jira/browse/SPARK-1409
I agree these should be disabled right away, and the JIRA can be used to
track fixing / turning them back on.
Hi Marcelo,
Thanks for bringing this up here, as this has been a topic of debate
recently.  Some thoughts below.
... all of the suffer from the fact that the log message needs to be built
This is not true of the current implementation (and this is actually why
Spark has a logging trait instead of just using a logger directly.)
If you look at the original function signatures:
protected def logDebug(msg: => String) ...
The => implies that we are passing the msg by name instead of by value.
Under the covers, scala is creating a closure that can be used to calculate
the log message, only if its actually required.  This does result is a
significant performance improvement, but still requires allocating an
object for the closure.  The bytecode is really something like this:
val logMessage = new Function0() { def call() =  "Log message" +
someExpensiveComputation() }
log.debug(logMessage)
In Catalyst and Spark SQL we are using the scala-logging package, which
uses macros to automatically rewrite all of your log statements.
You write: logger.debug(s"Log message $someExpensiveComputation")
You get:
if(logger.debugEnabled) {
  val logMsg = "Log message" + someExpensiveComputation()
  logger.debug(logMsg)
}
IMHO, this is the cleanest option (and is supported by Typesafe).  Based on
a micro-benchmark, it is also the fastest:
std logging: 19885.48ms
spark logging 914.408ms
scala logging 729.779ms
Once the dust settles from the 1.0 release, I'd be in favor of
standardizing on scala-logging.
Michael
BTW...
You can do calculations in string interpolation:
s"Time: ${timeMillis / 1000}s"
Or use format strings.
f"Float with two decimal places: $floatValue%.2f"
More info:
http://docs.scala-lang.org/overviews/core/string-interpolation.html
I believe you may need an assembly jar to run the ReplSuite. "sbt/sbt
assembly/assembly".
Michael
The REPL spins up an org.apache.spark.HttpServer, which provides classes
that are generated by the REPL as well as jars from addJar.
Michael
The REPL spins up an org.apache.spark.HttpServer, which provides classes
that are generated by the REPL as well as jars from addJar.
Michael
Yeah, I think that is correct.
Yeah, I think that is correct.
Yeah, you are right.  Thanks for pointing this out!
If you call .count() that is just the native Spark count, which is not
aware of the potential optimizations.  We could just override count() in a
schema RDD to be something like
"groupBy()(Count(Literal(1))).collect().head.getInt(0)"
Here is a JIRA: SPARK-1822 - SchemaRDD.count() should use the
optimizer.
Michael
-1
We found a regression in the way configuration is passed to executors.
https://issues.apache.org/jira/browse/SPARK-1864
https://github.com/apache/spark/pull/808
Michael
Thanks for reporting this!
https://issues.apache.org/jira/browse/SPARK-1964
https://github.com/apache/spark/pull/913
If you could test out that PR and see if it fixes your problems I'd really
appreciate it!
Michael
Thanks for reporting this!
https://issues.apache.org/jira/browse/SPARK-1964
https://github.com/apache/spark/pull/913
If you could test out that PR and see if it fixes your problems I'd really
appreciate it!
Michael
Yes, you'll need to download the code from that PR and reassemble Spark
(sbt/sbt assembly).
You should be able to get away with only doing it locally.  This bug is
happening during analysis which only occurs on the driver.
You should be able to get away with only doing it locally.  This bug is
happening during analysis which only occurs on the driver.
Awesome, thanks for testing!
Awesome, thanks for testing!
I assume you are adding tests?  because that is the only time you should
see that message.
That error could mean a couple of things:
 1) The query is invalid and hive threw an exception
 2) Your Hive setup is bad.
Regarding #2, you need to have the source for Hive 0.12.0 available and
built as well as a hadoop installation.  You also have to have the
environment vars set as specified here:
https://github.com/apache/spark/tree/master/sql
Michael
+1
I tested sql/hive functionality.
Hey Ian,
Thanks for bringing these up!  Responses in-line:
Just wondering if right now spark sql is expected to be thread safe on
You are probably hitting SPARK-2178
 which is caused by
SI-6240 .  We have a plan to
fix this by moving the schema introspection to compile time, using macros.
Sounds like SPARK-2102 .
 There is no reason AFAIK to not reuse the instance. A PR would be greatly
appreciated!
Thats just not an optimization that we had implemented yet... but I've just
done it here  and it'll be in
master soon :)
Yeah, thats true that we only look in the constructor at the moment, but I
don't think there is a really good reason for that (other than I guess we
will need to add code to make sure we skip builtin object methods).  If you
want to open a JIRA, we can try fixing this.
Michael
Yeah, sadly this dependency was introduced when someone consolidated the
logging infrastructure.  However, the dependency should be very small and
thus easy to remove, and I would like catalyst to be usable outside of
Spark.  A pull request to make this possible would be welcome.
Ideally, we'd create some sort of spark common package that has things like
logging.  That way catalyst could depend on that, without pulling in all of
Hadoop, etc.  Maybe others have opinions though, so I'm cc-ing the dev list.
I just wanted to send out a quick note about a change in the handling of
strings when loading / storing data using parquet and Spark SQL.  Before,
Spark SQL did not support binary data in Parquet, so all binary blobs were
implicitly treated as Strings.  9fe693
fixes
this limitation by adding support for binary data.
However, data written out with a prior version of Spark SQL will be missing
the annotation telling us to interpret a given column as a String, so old
string data will now be loaded as binary data.  If you would like to use
the data as a string, you will need to add a CAST to convert the datatype.
New string data written out after this change, will correctly be loaded in
as a string as now we will include an annotation about the desired type.
 Additionally, this should now interoperate correctly with other systems
that write Parquet data (hive, thrift, etc).
Michael
Thanks for reporting back.  I was pretty confused trying to reproduce the
error :)
That query is looking at "Fix Version" not "Target Version".  The fact that
the first one is still open is only because the bug is not resolved in
master.  It is fixed in 1.0.2.  The second one is partially fixed in 1.0.2,
but is not worth blocking the release for.
How recent is this? We've already reverted this patch once due to failing
tests.  It would be helpful to include a link to the failed build.  If its
failing again we'll have to revert again.
Could you make a PR as described here:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
It is not in 1.1 and there are not concrete plans for adding it at this
point.  Currently, there is more engineering investment going into caching
parquet data in Tachyon instead.  This approach is going to have much
better support for nested data, leverages other work being done on parquet,
and alleviates your concerns about wire format compatibility.
That said, if someone really wants to try and implement it, I don't think
it would be very hard.  The primary issue is going to be designing a clean
interface that is not too tied to this one implementation.
We aren't making any guarantees at the moment that it won't change.  Its
currently only intended for temporary caching of data.
- dev list
+ user list
You should be able to query Spark SQL using JDBC, starting with the 1.1
release.  There is some documentation is the repo
,
and we'll update the official docs once the release is out.
Caching parquet files in tachyon with saveAsParquetFile and then reading
them with parquetFile should already work. You can use SQL on these tables
by using registerTempTable.
Some of the general parquet work that we have been doing includes: #1935
, SPARK-2721
, SPARK-3036
, SPARK-3037
 and #1819
The reason I'm asking about the columnar compressed format is that
Can you elaborate?
It seems like there are two things here:
 - Co-locating blocks with the same keys to avoid network transfer.
 - Leveraging partitioning information to avoid a shuffle when data is
already partitioned correctly (even if those partitions aren't yet on the
same machine).
The former seems more complicated and probably requires the support from
Hadoop you linked to.  However, the latter might be easier as there is
already a framework for reasoning about partitioning and the need to
shuffle in the Spark SQL planner.
+1
Thanks!
I think usually people add these directories as multiple partitions of the
same table instead of union.  This actually allows us to efficiently prune
directories when reading in addition to standard column pruning.
What Patrick said is correct.  Two other points:
 - In the 1.2 release we are hoping to beef up the support for working with
partitioned parquet independent of the metastore.
 - You can actually do operations like INSERT INTO for parquet tables to
add data.  This creates new parquet files for each insertion.  This will
break if there are multiple concurrent writers to the same table.
Hey Cody,
Thanks for doing this!  Will look at your PR later today.
Michael
Yeah, thanks for implementing it!
Since Spark SQL is an alpha component and moving quickly the plan is to
backport all of master into the next point release in the 1.1 series.
Hi Cody,
There are currently no concrete plans for adding buckets to Spark SQL, but
thats mostly due to lack of resources / demand for this feature.  Adding
full support is probably a fair amount of work since you'd have to make
changes throughout parsing/optimization/execution.  That said, there are
probably some smaller tasks that could be easier (for example, you might be
able to avoid a shuffle when doing joins on tables that are already
bucketed by exposing more metastore information to the planner).
Michael
I actually submitted a patch to do this yesterday:
https://github.com/apache/spark/pull/2493
Can you tell us more about your configuration.  In particular how much
memory/cores do the executors have and what does the schema of your data
look like?
The hard part here is updating the existing code base... which is going to
create merge conflicts with like all of the open PRs...
Hi Cody,
Assuming you are talking about 'safe' changes to the schema (i.e. existing
column names are never reused with incompatible types), this is something
I'd love to support.  Perhaps you can describe more what sorts of changes
you are making, and if simple merging of the schemas would be sufficient.
If so, we can open a JIRA, though I'm not sure when we'll have resources to
dedicate to this.
In the near term, I'd suggest writing converters for each version of the
schema, that translate to some desired master schema.  You can then union
all of these together and avoid the cost of batch conversion.  It seems
like in most cases this should be pretty efficient, at least now that we
have good pushdown past union operators :)
Michael
Thanks for the input.  We purposefully made sure that the config option did
not make it into a release as it is not something that we are willing to
support long term.  That said we'll try and make this easier in the future
either through hints or better support for statistics.
In this particular case you can get what you want by registering the tables
as external tables and setting an flag.  Here's a helper function to do
what you need.
/**
 * Sugar for creating a Hive external table from a parquet path.
 */
def createParquetTable(name: String, file: String): Unit = {
  import org.apache.spark.sql.hive.HiveMetastoreTypes
  val rdd = parquetFile(file)
  val schema = rdd.schema.fields.map(f => s"${f.name}
${HiveMetastoreTypes.toMetastoreType(f.dataType)}").mkString(",\n")
  val ddl = s"""
    |CREATE EXTERNAL TABLE $name (
    |  $schema
    |)
    |ROW FORMAT SERDE 'parquet.hive.serde.ParquetHiveSerDe'
    |STORED AS INPUTFORMAT 'parquet.hive.DeprecatedParquetInputFormat'
    |OUTPUTFORMAT 'parquet.hive.DeprecatedParquetOutputFormat'
    |LOCATION '$file'""".stripMargin
  sql(ddl)
  setConf("spark.sql.hive.convertMetastoreParquet", "true")
}
You'll also need to run this to populate the statistics:
ANALYZE TABLE  tableName COMPUTE STATISTICS noscan;
Filed here: https://issues.apache.org/jira/browse/SPARK-3851
I was proposing you manually convert each different format into one unified
format  (by adding literal nulls and such for missing columns) and then
union these converted datasets.  It would be weird to have union all try
and do this automatically.
Yes, the foreign sources work is only about exposing a stable set of APIs
for external libraries to link against (to avoid the spark assembly
becoming a dependency mess).  The code path these APIs use will be the same
as that for datasources included in the core spark sql library.
Michael
You can't change parquet schema without reencoding the data as you need to
recalculate the footer index data.  You can manually do what SPARK-3851
 is going to do today
however.
Consider two schemas:
Old Schema: (a: Int, b: String)
New Schema, where I've dropped and added a column: (a: Int, c: Long)
parquetFile(old).registerTempTable("old")
parquetFile(new).registerTempTable("new")
sql("""
  SELECT a, b, CAST(null AS LONG) AS c  FROM old UNION ALL
  SELECT a, CAST(null AS STRING) AS b, c FROM new
""").registerTempTable("unifiedData")
Because of filter/column pushdown past UNIONs this should executed as
desired even if you write more complicated queries on top of
"unifiedData".  Its a little onerous but should work for now.  This can
also support things like column renaming which would be much harder to do
automatically.
dev to bcc.
Thanks for reaching out, Ozgun.  Let's discuss if there were any missing
optimizations off list.  We'll make sure to report back or add any findings
to the tuning guide.
I was thinking more about the average end-to-end latency for launching a
query that has 100s of partitions. Its also quite possible that SQLs task
launch overhead is higher since we have never profiled how much is getting
pulled into the closures.
Hey Sean,
Thanks for pointing this out.  Looks like a bad test where we should be
doing Set comparison instead of Array.
Michael
I'm going to have to disagree here.  If you are building a release
distribution or integrating with legacy systems then maven is probably the
correct choice.  However most of the core developers that I know use sbt,
and I think its a better choice for exploration and development overall.
That said, this probably falls into the category of a religious argument so
you might want to look at both options and decide for yourself.
In my experience the SBT build is significantly faster with less effort
(and I think sbt is still faster even if you go through the extra effort of
installing zinc) and easier to read.  The console mode of sbt (just run
sbt/sbt and then a long running console session is started that will accept
further commands) is great for building individual subprojects or running
single test suites.  In addition to being faster since its a long running
JVM, its got a lot of nice features like tab-completion for test case names.
For example, if I wanted to see what test cases are available in the SQL
subproject you can do the following:
[marmbrus@michaels-mbp spark (tpcds)]$ sbt/sbt
[info] Loading project definition from
/Users/marmbrus/workspace/spark/project/project
[info] Loading project definition from
/Users/marmbrus/.sbt/0.13/staging/ad8e8574a5bcb2d22d23/sbt-pom-reader/project
[info] Set current project to spark-parent (in build
file:/Users/marmbrus/workspace/spark/)
--
 org.apache.spark.sql.CachedTableSuite
org.apache.spark.sql.DataTypeSuite
 org.apache.spark.sql.DslQuerySuite
org.apache.spark.sql.InsertIntoSuite
...
Another very useful feature is the development console, which starts an
interactive REPL including the most recent version of the code and a lot of
useful imports for some subprojects.  For example in the hive subproject it
automatically sets up a temporary database with a bunch of test data
pre-loaded:
$ sbt/sbt hive/console
...
import org.apache.spark.sql.hive._
import org.apache.spark.sql.hive.test.TestHive._
import org.apache.spark.sql.parquet.ParquetTestData
Welcome to Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java
1.7.0_45).
Type in expressions to have them evaluated.
Type :help for more information.
scala> sql("SELECT * FROM src").take(2)
res0: Array[org.apache.spark.sql.Row] = Array([238,val_238], [86,val_86])
Michael
To be clear, I think that the PR builder actually uses sbt
 currently,
but there are master builds that make sure maven doesn't break (amongst
other things).
Yeah, this is a very good point.  I have used `sbt/sbt gen-idea` in the
past, but I'm currently using the maven integration of inteliJ since it
seems more stable.
The Spark SQL developer readme
 has a little bit of this,
but we really should have some documentation on using SBT as well.
 Integrating with those systems is generally easier if you are also working
Also a good point, though I've seen some pretty clever uses of sbt's
external project references to link spark into other projects.  I'll
certainly admit I have a bias towards new shiny things in general though,
so my definition of legacy is probably skewed :)
You probably don't need to create a new kind of SchemaRDD.  Instead I'd
suggest taking a look at the data sources API that we are adding in Spark
1.2.  There is not a ton of documentation, but the test cases show how to
implement the various interfaces
,
and there is an example library for reading Avro data
.
No, it should support any data source that has a schema and can produce
rows.
The command run fine for me on master.  Note that Hive does print an
exception in the logs, but that exception does not propogate to user code.
Thanks for reporting.  This looks like a regression related to:
https://github.com/apache/spark/pull/2570
I've filed it here: https://issues.apache.org/jira/browse/SPARK-4769
This is by hive's design.  From the Hive documentation:
The column change command will only modify Hive's metadata, and will not
This is merged now and should be fixed in the next 1.2 RC.
You might also try out the recently added support for views.
As the scala doc for applySchema says, "It is important to make sure that
the structure of every [[Row]] of the provided RDD matches the provided
schema. Otherwise, there will be runtime exceptions."  We don't check as
doing runtime reflection on all of the data would be very expensive.  You
will only get errors if you try to manipulate the data, but otherwise it
will pass it though.
I have written some debugging code (developer API, not guaranteed to be
stable) though that you can use.
import org.apache.spark.sql.execution.debug._
schemaRDD.typeCheck()
I'd suggest looking at the reference in the programming guide:
http://spark.apache.org/docs/latest/sql-programming-guide.html#spark-sql-datatype-reference
I agree and this is something that we have discussed in the past.
Essentially I think instead of creating a RelationProvider that returns a
single table, we'll have something like an external catalog that can return
multiple base relations.
I'd love to get both of these in.  There is some trickiness that I talk
about on the JIRA for timestamps since the SQL timestamp class can support
nano seconds and I don't think parquet has a type for this.  Other systems
(impala) seem to use INT96.  It would be great to maybe ask on the parquet
mailing list what the plan is there to make sure that whatever we do is
going to be compatible long term.
Michael
Yeah, I saw those.  The problem is that #3822 truncates timestamps that
include nanoseconds.
This was not intended, can you open a JIRA?
I will look at it this weekend.
+1 to adding such an optimization to parquet.  The bytes are tagged
specially as UTF8 in the parquet schema so it seem like it would be
possible to add this.
There was work being done at Berkeley on prototyping support for Succinct
in Spark SQL.  Rachit might have more information.
In particular the performance tricks are in SpecificMutableRow.
Its not completely transparent, but you can do something like the following
today:
CACHE TABLE hotData AS SELECT columns, I, care, about FROM fullTable
I'd suggest marking the HiveContext as @transient since its not valid to
use it on the slaves anyway.
I was suggesting you mark the variable that is holding the HiveContext
'@transient' since the scala compiler is not correctly propagating this
through the tuple extraction.  This is only a workaround.  We can also
remove the tuple extraction.
It is not.
2) if it is not honored, does it matter ? Hive introduced this feature to
It could matter for very skewed data, though I have not heard many
complaints.  We could consider adding it in the future if people are having
problems with skewed data.
We will write up a whole migration guide before the final release, but I
can quickly explain this one.  We made the implicit conversion
significantly less broad to avoid the chance of confusing conflicts.
However, now you have to call .toDF in order to force RDDs to become
DataFrames.
Thanks for reporting.  This was a result of a change to our DDL parser that
resulted in types becoming reserved words.  I've filled a JIRA and will
investigate if this is something we can fix.
https://issues.apache.org/jira/browse/SPARK-6250
FYI: https://issues.apache.org/jira/browse/INFRA-9259
We are looking at the issue and will likely fix it for Spark 1.3.1.
Here is the JIRA: https://issues.apache.org/jira/browse/SPARK-6315
I have not heard this reported yet, but your invocation looks correct to
me.  Can you open a JIRA?
Two other criteria that I use when deciding what to backport:
 - Is it a regression from a previous minor release?  I'm much more likely
to backport fixes in this case, as I'd love for most people to stay up to
date.
 - How scary is the change?  I think the primary goal is stability of the
maintenance branches.  When I am confident that something is isolated and
unlikely to break things (i.e. I'm fixing a confusing error message), then
i'm much more likely to backport it.
Regarding the length of time to continue backporting, I mostly don't
backport to N-1, but this is partially because SQL is changing too fast for
that to generally be useful.  These old branches usually only get attention
from me when there is an explicit request.
I'd love to hear more feedback from others.
Michael
-1 (binding)
We just were alerted to a pretty serious regression since 1.3.0 (
https://issues.apache.org/jira/browse/SPARK-6851).  Should have a fix
shortly.
Michael
Yeah, we don't currently push down predicates into the metastore.  Though,
we do prune partitions based on predicates (so we don't read the data).
We can try to add this as part of some hive refactoring we are doing for
1.4.  I've created a JIRA: https://issues.apache.org/jira/browse/SPARK-6910
HiveQL
Already done :)
https://github.com/apache/spark/commit/2e8c6ca47df14681c1110f0736234ce76a3eca9b
I am working on it.  Here is the (very rough) version:
https://github.com/apache/spark/compare/apache:master...marmbrus:multiHiveVersions
Hey Marcelo,
Thanks for the heads up!  I'm currently in the process of refactoring all
of this (to separate the metadata connection from the execution side) and
as part of this I'm making the initialization of the session not lazy.  It
would be great to hear if this also works for your internal integration
tests once the patch is up (hopefully this weekend).
Michael
What version of Spark are you using?  It appears that at least in master we
are doing the conversion correctly, but its possible older versions of
applySchema do not.  If you can reproduce the same bug in master, can you
open a JIRA?
Overall this seems like a reasonable proposal to me.  Here are a few
thoughts:
 - There is some debugging utility to the ruleName, so we would probably
want to at least make that an argument to the rule function.
 - We also have had rules that operate on SparkPlan, though since there is
only one ATM maybe we don't need sugar there.
 - I would not call the sugar for creating Strategies rule/seqrule, as I
think the one-to-one vs one-to-many distinction is useful.
 - I'm generally pro-refactoring to make the code nicer, especially when
its not official public API, but I do think its important to maintain
source compatibility (which I think you are) when possible as there are
other projects using catalyst.
 - Finally, we'll have to balance this with other code changes / conflicts.
You should probably open a JIRA and we can continue the discussion there.
Thanks for the feedback.  As you stated UDTs are explicitly not a public
api as we knew we were going to be making breaking changes to them.  We
hope to stabilize / open them up in future releases.  Regarding the Hive
issue, have you tried using TestHive instead.  This is what we use for
testing and it takes care of creating temporary directories for all
storage.  It also has a reset() function that you can call in-between
tests.  If this doesn't work for you, maybe open a JIRA and we can discuss
more there.
I think this is likely something that we'll want to do during the code
generation phase.  Though its probably not the lowest hanging fruit at this
point.
Through the DataFrame API, users should never see UTF8String.
Expression (and any class in the catalyst package) is considered internal
and so uses the internal representation of various types.  Which type we
use here is not stable across releases.
Is there a reason you aren't defining a UDF instead?
This is something I'd hoping to add in Spark 1.5
I'm super open to suggestions here.  Mind possibly opening a JIRA with a
proposed interface?
This was a change that was made to match a wrong answer coming from older
versions of Hive.  Unfortunately I think its too late to fix this in the
1.4 branch (as I'd like to avoid changing answers at all in point
releases), but in Spark 1.5 we revert to the correct behavior.
https://issues.apache.org/jira/browse/SPARK-8828
There is a lot of info here:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
In this particular case I'd start by looking at the JIRA (which already has
a pull request posted to it).
+1
Can you add your description of the problem as a comment to that ticket and
we'll make sure to test both cases and break it out if the root cause ends
up being different.
I'd suggest using org.apache.spark.sql.hive.test.TestHive as the context in
unit tests.  It takes care of creating separate directories for each
invocation automatically.
+1
I think the maintainers of the various components should take care of
this.  Reynold and I just did a pass over SQL and I think that by Friday
there should only be blocker bugs / documentation remaining.
I'd be okay skipping the HiveCompatibilitySuite for core-only changes.
They do often catch bugs in changes to catalyst or sql though.  Same for
HashJoinCompatibilitySuite/VersionsSuite.
HiveSparkSubmitSuite/CliSuite should probably stay, as they do test things
like addJar that have been broken by core in the past.
This isn't really answering the question, but for what it is worth, I
manage several different branches of Spark and publish custom named
versions regularly to an internal repository, and this is *much* easier
with SBT than with maven.  You can actually link the Spark SBT build into
an external SBT build and write commands that cross publish as needed.
For your case something as simple as build/sbt "set version in Global :=
'1.4.1-custom-string'" publish might do the trick.
+1 Ran TPC-DS and ported several jobs over to 1.5
+1 to reynolds suggestion.  This is probably the fastest way to iterate.
Another option for more ad-hoc debugging is `sbt/sbt sparkShell` which is
similar to bin/spark-shell but doesn't require you to rebuild the assembly
jar.
Thanks for pointing this out.
https://issues.apache.org/jira/browse/SPARK-10539
We will fix this for Spark 1.5.1.
Can you open a JIRA?
Are you using a SQLContext or a HiveContext?  The programming guide
suggests the latter, as the former is really only there because some
applications may have conflicts with Hive dependencies.  SQLContext is case
sensitive by default where as the HiveContext is not.  The parser in
HiveContext is also a lot better.
HiveQL uses `backticks` for quoted identifiers.
+1 - Ran TPCDS and some other micro benchmarks
We have to try and maintain binary compatibility here, so probably the
easiest thing to do here would be to add a method to the class.  Perhaps
something like:
def unhandledFilters(filters: Array[Filter]): Array[Filter] = filters
By default, this could return all filters so behavior would remain the
same, but specific implementations could override it.  There is still a
chance that this would conflict with existing methods, but hopefully that
would not be a problem in practice.
Thoughts?
Michael
I think this is the most up to date branch (used in Spark 1.5):
https://github.com/pwendell/hive/tree/release-1.2.1-spark
Please do.
-dev +user
1). Is that the reason why it's always slow in the first run? Or are there
You are probably seeing the effect of the JVMs JIT.  The first run is
executing in interpreted mode.  Once the JVM sees its a hot piece of code
it will compile it to native code.  This applies both to Spark / Spark SQL
itself and (as of Spark 1.5) the code that we dynamically generate for
doing expression evaluation.  Multiple runs with the same expressions will
used cached code that might have been JITed.
No, we do not use the map reduce engine for execution.  You can however
compile Spark to work with either version of hadoop for so you can access
HDFS, etc.
+1
It also does not seem that expensive to test only compilation for Scala
2.11 on PR builds.
It means that there is an invalid attribute reference (i.e. a #n where the
attribute is missing from the child operator).
Hey All,
Just a friendly reminder that today (October 31st) is the scheduled code
freeze for Spark 1.6.  Since a lot of developers were busy with the Spark
Summit last week I'm going to delay cutting the branch until Monday,
November 2nd.  After that point, we'll package a release for testing and
then go into the normal triage process where bugs are prioritized and some
smaller features are allowed in on a case by case basis (if they are very
low risk/additive/feature flagged/etc).
As a reminder, release window dates are always maintained on the wiki and
are updated after each release according to our 3 month release cadence:
https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage
Thanks!
Michael
In SBT:
build/sbt "mllib/test-only *ProbabilisticClassifierSuite"
We support both build systems.  We use maven to publish the canonical
distributions as it interoperates better with downstream consumers.  Most
of the developers that I know, however, use SBT for day to day development.
Sorry for the delay due to traveling...
The branch has been cut.  At this point anything that we want to go into
Spark 1.6 will need to be cherry-picked.  Please be cautious when doing so,
and contact me if you are uncertain.
Michael
+1
I'm noticing several problems with Jenkins since the upgrade.
PR comments say: "Build started sha1 is merged." instead of actually
printing the hash
Also:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/45246/console
GitHub pull request #9527 of commit
0e0959efada849a56430d30305eb4e9a88ecafad, no merge conflicts.
Setting status of 0e0959efada849a56430d30305eb4e9a88ecafad to PENDING
with url  and message: 'Build started sha1 is merged.'
FileNotFoundException means that the credentials Jenkins is using is
probably wrong. Or the user account does not have write access to the
repo.
java.io.FileNotFoundException: {"message":"Not
Found","documentation_url":"https://developer.github.com/v3"}
	at org.kohsuke.github.Requester.handleApiError(Requester.java:501)
	at org.kohsuke.github.Requester._to(Requester.java:248)
	at org.kohsuke.github.Requester.to(Requester.java:194)
	at org.kohsuke.github.GHRepository.createCommitStatus(GHRepository.java:829)
	at org.jenkinsci.plugins.ghprb.extensions.status.GhprbSimpleStatus.createCommitStatus(GhprbSimpleStatus.java:208)
	at org.jenkinsci.plugins.ghprb.extensions.status.GhprbSimpleStatus.onBuildStart(GhprbSimpleStatus.java:147)
	at org.jenkinsci.plugins.ghprb.GhprbBuilds.onStarted(GhprbBuilds.java:122)
	at org.jenkinsci.plugins.ghprb.GhprbBuildListener.onStarted(GhprbBuildListener.java:24)
	at org.jenkinsci.plugins.ghprb.GhprbBuildListener.onStarted(GhprbBuildListener.java:17)
	at hudson.model.listeners.RunListener.fireStarted(RunListener.java:215)
	at hudson.model.Run.execute(Run.java:1737)
	at hudson.model.FreeStyleBuild.run(FreeStyleBuild.java:43)
	at hudson.model.ResourceController.execute(ResourceController.java:98)
	at hudson.model.Executor.run(Executor.java:410)
Caused by: java.io.FileNotFoundException:
https://api.github.com/repos/apache/spark/statuses/0e0959efada849a56430d30305eb4e9a88ecafad
	at sun.reflect.GeneratedConstructorAccessor174.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at sun.net.www.protocol.http.HttpURLConnection$6.run(HttpURLConnection.java:1675)
	at sun.net.www.protocol.http.HttpURLConnection$6.run(HttpURLConnection.java:1673)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.net.www.protocol.http.HttpURLConnection.getChainedException(HttpURLConnection.java:1671)
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1244)
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getInputStream(HttpsURLConnectionImpl.java:254)
	at org.kohsuke.github.Requester.parse(Requester.java:458)
	at org.kohsuke.github.Requester._to(Requester.java:227)
	... 12 more
Caused by: java.io.FileNotFoundException:
https://api.github.com/repos/apache/spark/statuses/0e0959efada849a56430d30305eb4e9a88ecafad
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1624)
	at java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:468)
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:338)
	at org.kohsuke.github.Requester.parse(Requester.java:454)
	... 13 more
Its not included, it is downloaded on demand.
That said I think the fact that we can download the jar is a huge feature
of SBT, no installation needed, build the project as long as you have a JVM.
We do support hive style views, though all tables have to be visible to
Hive.  You can also turn on the experimental native view support (but it
does not canonicalize the query).
set spark.sql.nativeView = true
In order to facilitate community testing of Spark 1.6.0, I'm excited to
announce the availability of an early preview of the release. This is not a
release candidate, so there is no voting involved. However, it'd be awesome
if community members can start testing with this preview package and report
any problems they encounter.
This preview package contains all the commits to branch-1.6
 till commit
308381420f51b6da1007ea09a02d740613a226e0
.
The staging maven repository for this preview build can be found here:
https://repository.apache.org/content/repositories/orgapachespark-1162
Binaries for this preview build can be found here:
http://people.apache.org/~pwendell/spark-releases/spark-v1.6.0-preview2-bin/
A build of the docs can also be found here:
http://people.apache.org/~pwendell/spark-releases/spark-v1.6.0-preview2-docs/
The full change log for this release can be found on JIRA
.
*== How can you help? ==*
If you are a Spark user, you can help us test this release by taking a
Spark workload and running on this preview release, then reporting any
regressions.
*== Major Features ==*
When testing, we'd appreciate it if users could focus on areas that have
changed in this release.  Some notable new features include:
SPARK-11787  *Parquet
Performance* - Improve Parquet scan performance when using flat schemas.
SPARK-10810  *Session *
*Management* - Multiple users of the thrift (JDBC/ODBC) server now have
isolated sessions including their own default database (i.e USE mydb) even
on shared clusters.
SPARK-9999   *Dataset API* -
A new, experimental type-safe API (similar to RDDs) that performs many
operations on serialized binary data and code generation (i.e. Project
Tungsten)
SPARK-10000  *Unified
Memory Management* - Shared memory for execution and caching instead of
exclusive division of the regions.
SPARK-10978  *Datasource
API Avoid Double Filter* - When implementing a datasource with filter
pushdown, developers can now tell Spark SQL to avoid double evaluating a
pushed-down filter.
SPARK-2629   *New
improved state management* - trackStateByKey - a DStream transformation for
stateful stream processing, supersedes updateStateByKey in functionality
and performance.
Happy testing!
Michael
Please vote on releasing the following candidate as Apache Spark version
1.6.0!
The vote is open until Saturday, December 5, 2015 at 21:00 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 1.6.0
[ ] -1 Do not release this package because ...
To learn more about Apache Spark, please see http://spark.apache.org/
The tag to be voted on is *v1.6.0-rc1
(bf525845cef159d2d4c9f4d64e158f037179b5c4)
*
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-v1.6.0-rc1-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1165/
The test repository (versioned as v1.6.0-rc1) for this release can be found
at:
https://repository.apache.org/content/repositories/orgapachespark-1164/
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc1-docs/
=======================================
== How can I help test this release? ==
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.
================================================
== What justifies a -1 vote for this release? ==
================================================
This vote is happening towards the end of the 1.6 QA period, so -1 votes
should only occur for significant regressions from 1.5. Bugs already
present in 1.5, minor regressions, or bugs related to new features will not
block this release.
===============================================================
== What should happen to JIRA tickets still targeting 1.6.0? ==
===============================================================
1. It is OK for documentation patches to target 1.6.0 and still go into
branch-1.6, since documentations will be published separately from the
release.
2. New features for non-alpha-modules should target 1.7+.
3. Non-blocker bug fixes should target 1.6.1 or 1.7.0, or drop the target
version.
==================================================
== Major changes to help you focus your testing ==
==================================================
Spark SQL
   - SPARK-10810 
   Session Management - The ability to create multiple isolated SQL
   Contexts that have their own configuration and default database.  This is
   turned on by default in the thrift server.
   - SPARK-9999   Dataset
   API - A type-safe API (similar to RDDs) that performs many operations on
   serialized binary data and code generation (i.e. Project Tungsten).
   - SPARK-10000  Unified
   Memory Management - Shared memory for execution and caching instead of
   exclusive division of the regions.
   - SPARK-11197  SQL
   Queries on Files - Concise syntax for running SQL queries over files of
   any supported format without registering a table.
   - SPARK-11745  Reading
   non-standard JSON files - Added options to read non-standard JSON files
   (e.g. single-quotes, unquoted attributes)
   - SPARK-10412 
Per-operator
   Metics for SQL Execution - Display statistics on a per-operator basis
   for memory usage and spilled data size.
   - SPARK-11329  Star
   (*) expansion for StructTypes - Makes it easier to nest and unest
   arbitrary numbers of columns
   - SPARK-10917 ,
   SPARK-11149  In-memory
   Columnar Cache Performance - Significant (up to 14x) speed up when
   caching data that contains complex types in DataFrames or SQL.
   - SPARK-11111  Fast
   null-safe joins - Joins using null-safe equality () will now execute
   using SortMergeJoin instead of computing a cartisian product.
   - SPARK-11389  SQL
   Execution Using Off-Heap Memory - Support for configuring query
   execution to occur using off-heap memory to avoid GC overhead
   - SPARK-10978  Datasource
   API Avoid Double Filter - When implementing a datasource with filter
   pushdown, developers can now tell Spark SQL to avoid double evaluating a
   pushed-down filter.
   - SPARK-4849   Advanced
   Layout of Cached Data - storing partitioning and ordering schemes in
   In-memory table scan, and adding distributeBy and localSort to DF API
   - SPARK-9858   Adaptive
   query execution - Initial support for automatically selecting the number
   of reducers for joins and aggregations.
Spark Streaming
   - API Updates
      - SPARK-2629   New
      improved state management - trackStateByKey - a DStream
      transformation for stateful stream processing, supersedes
      updateStateByKey in functionality and performance.
      - SPARK-11198  Kinesis
      record deaggregation - Kinesis streams have been upgraded to use KCL
      1.4.0 and supports transparent deaggregation of KPL-aggregated records.
      - SPARK-10891  Kinesis
      message handler function - Allows arbitrary function to be applied to
      a Kinesis record in the Kinesis receiver before to customize what data is
      to be stored in memory.
      - SPARK-6328  
       Python Streaming Listener API - Get streaming statistics (scheduling
      delays, batch processing times, etc.) in streaming.
   - UI Improvements
      - Made failures visible in the streaming tab, in the timelines, batch
      list, and batch details page.
      - Made output operations visible in the streaming tab as progress bars
MLlibNew algorithms/models
   - SPARK-8518   Survival
   analysis - Log-linear model for survival analysis
   - SPARK-9834   Normal
   equation for least squares - Normal equation solver, providing R-like
   model summary statistics
   - SPARK-3147   Online
   hypothesis testing - A/B testing in the Spark Streaming framework
   - SPARK-9930   New
   feature transformers - ChiSqSelector, QuantileDiscretizer, SQL
   transformer
   - SPARK-6517   Bisecting
   K-Means clustering - Fast top-down clustering variant of K-Means
API improvements
   - ML Pipelines
      - SPARK-6725   Pipeline
      persistence - Save/load for ML Pipelines, with partial coverage of
      spark.ml algorithms
      - SPARK-5565   LDA
      in ML Pipelines - API for Latent Dirichlet Allocation in ML Pipelines
   - R API
      - SPARK-9836   R-like
      statistics for GLMs - (Partial) R-like stats for ordinary least
      squares via summary(model)
      - SPARK-9681   Feature
      interactions in R formula - Interaction operator ":" in R formula
   - Python API - Many improvements to Python API to approach feature parity
Misc improvements
   - SPARK-7685  ,
   SPARK-9642   Instance
   weights for GLMs - Logistic and Linear Regression can take instance
   weights
   - SPARK-10384 ,
   SPARK-10385  Univariate
   and bivariate statistics in DataFrames - Variance, stddev, correlations,
   etc.
   - SPARK-10117  LIBSVM
   data source - LIBSVM as a SQL data sourceDocumentation improvements
   - SPARK-7751   @since
   versions - Documentation includes initial version when classes and
   methods were added
   - SPARK-11337  Testable
   example code - Automated testing for code in user guide examples
Deprecations
   - In spark.mllib.clustering.KMeans, the "runs" parameter has been
   deprecated.
   - In spark.ml.classification.LogisticRegressionModel and
   spark.ml.regression.LinearRegressionModel, the "weights" field has been
   deprecated, in favor of the new name "coefficients." This helps
   disambiguate from instance (row) weights given to algorithms.
Changes of behavior
   - spark.mllib.tree.GradientBoostedTrees validationTol has changed
   semantics in 1.6. Previously, it was a threshold for absolute change in
   error. Now, it resembles the behavior of GradientDescent convergenceTol:
   For large errors, it uses relative error (relative to the previous error);
   for small errors (< 0.01), it uses absolute error.
   - spark.ml.feature.RegexTokenizer: Previously, it did not convert
   strings to lowercase before tokenizing. Now, it converts to lowercase by
   default, with an option not to. This matches the behavior of the simpler
   Tokenizer transformer.
Thanks for bringing this up Sean. I think we are all happy to adopt
concrete suggestions to make the release process more transparent,
including pinging the list before kicking off the release build.
Technically there's still a Blocker bug:
Sorry, I misprioritized this particular issue when I thought that it was
going to block the release by causing the doc build to fail. When I
realized the failure was non-deterministic and isolated to OSX (i.e. the
official release build on jenkins is not affected) I failed to update the
issue.  It doesn't show up on the dashboard that I've been using to track
the release
since
its labeled documentation.
This can be debated, but I explicitly ignored test and documentation
issues.  Since the docs are published separately and easy to update, I
don't think its worth further disturbing the release cadence for these
JIRAs.
Up until today various committers have told me that there were known issues
with branch-1.6 that would cause them to -1 the release.  Whenever this
happened, I asked them to ensure there was a properly targeted blocker JIRA
open so people could publicly track the status of the release.  As long as
such issues were open, I only published a preview since making an RC is
pretty high cost.
I'm sorry that it felt sudden to you, but as of last night all such known
issues were resolved and thus I cut a release as soon as this was the case.
I'm just curious, am I the only one that thinks this isn't roughly
I actually did spent quite a bit of time asking people to close various
umbrella issues, and I was pretty strict about watching JIRA throughout the
process.  Perhaps as an additional step, future preview releases or branch
cuts can include a link to an authoritative dashboard that we will use to
decide when we are ready to make an RC.  I'm also open to other suggestions.
Michael
There is a lot of tooling:
https://amplab.cs.berkeley.edu/jenkins/view/Spark-Packaging/
Still you have check JIRA, sync with people who have been working on known
issues, run the jenkins jobs (which take 1+ hours) and then write that
email which has a bunch of links in it.  Short of automating the creation
of the email (PRs welcome!) I'm not sure what else you would automate.
That said, this is all I have done since I came into work today.
I'm going to kick the voting off with a +1 (binding).  We ran TPC-DS and
most queries are faster than 1.5.  We've also ported several production
pipelines to 1.6.
An update: the vote fails due to the -1.   I'll post another RC as soon as
we've resolved these issues.  In the mean time I encourage people to
continue testing and post any problems they encounter here.
I think that it is generally good to have parity when the functionality is
useful.  However, in some cases various features are there just to maintain
compatibility with other system.  For example CACHE TABLE is eager because
Shark's cache table was.  df.cache() is lazy because Spark's cache is.
Does that mean that we need to add some eager caching mechanism to
dataframes to have parity?  Probably not, users can just call .count() if
they want to force materialization.
Regarding the differences between HiveQL and the SQLParser, I think we
should get rid of the SQL parser.  Its kind of a hack that I built just so
that there was some SQL story for people who didn't compile with Hive.
Moving forward, I'd like to see the distinction between the HiveContext and
SQLContext removed and we can standardize on a single parser.  For this
reason I'd be opposed to spending a lot of dev/reviewer time on adding
features there.
I don't plan to abandon HiveQL compatibility, but I'd like to see us move
towards something with more SQL compliance (perhaps just newer versions of
the HiveQL parser).  Exactly which parser will do that for us is under
investigation.
Yeah, I would like to address any actual gaps in functionality that are
present.
We are getting close to merging patches for SPARK-12155
 and SPARK-12253
.  I'll be cutting RC2
shortly after that.
Michael
Cutting RC2 now.
Trying again now that eec36607
is
merged.
Please vote on releasing the following candidate as Apache Spark version
1.6.0!
The vote is open until Tuesday, December 15, 2015 at 6:00 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 1.6.0
[ ] -1 Do not release this package because ...
To learn more about Apache Spark, please see http://spark.apache.org/
The tag to be voted on is *v1.6.0-rc2
(23f8dfd45187cb8f2216328ab907ddb5fbdffd0b)
*
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc2-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1169/
The test repository (versioned as v1.6.0-rc2) for this release can be found
at:
https://repository.apache.org/content/repositories/orgapachespark-1168/
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc2-docs/
=======================================
== How can I help test this release? ==
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.
================================================
== What justifies a -1 vote for this release? ==
================================================
This vote is happening towards the end of the 1.6 QA period, so -1 votes
should only occur for significant regressions from 1.5. Bugs already
present in 1.5, minor regressions, or bugs related to new features will not
block this release.
===============================================================
== What should happen to JIRA tickets still targeting 1.6.0? ==
===============================================================
1. It is OK for documentation patches to target 1.6.0 and still go into
branch-1.6, since documentations will be published separately from the
release.
2. New features for non-alpha-modules should target 1.7+.
3. Non-blocker bug fixes should target 1.6.1 or 1.7.0, or drop the target
version.
==================================================
== Major changes to help you focus your testing ==
==================================================
Spark 1.6.0 PreviewNotable changes since 1.6 RC1Spark Streaming
   - SPARK-2629  
   trackStateByKey has been renamed to mapWithState
Spark SQL
   - SPARK-12165 
   SPARK-12189  Fix bugs
   in eviction of storage memory by execution.
   - SPARK-12258  correct
   passing null into ScalaUDF
Notable Features Since 1.5Spark SQL
   - SPARK-11787  Parquet
   Performance - Improve Parquet scan performance when using flat schemas.
   - SPARK-10810 
   Session Management - Isolated devault database (i.e USE mydb) even on
   shared clusters.
   - SPARK-9999   Dataset
   API - A type-safe API (similar to RDDs) that performs many operations on
   serialized binary data and code generation (i.e. Project Tungsten).
   - SPARK-10000  Unified
   Memory Management - Shared memory for execution and caching instead of
   exclusive division of the regions.
   - SPARK-11197  SQL
   Queries on Files - Concise syntax for running SQL queries over files of
   any supported format without registering a table.
   - SPARK-11745  Reading
   non-standard JSON files - Added options to read non-standard JSON files
   (e.g. single-quotes, unquoted attributes)
   - SPARK-10412 
Per-operator
   Metrics for SQL Execution - Display statistics on a peroperator basis
   for memory usage and spilled data size.
   - SPARK-11329  Star
   (*) expansion for StructTypes - Makes it easier to nest and unest
   arbitrary numbers of columns
   - SPARK-10917 ,
   SPARK-11149  In-memory
   Columnar Cache Performance - Significant (up to 14x) speed up when
   caching data that contains complex types in DataFrames or SQL.
   - SPARK-11111  Fast
   null-safe joins - Joins using null-safe equality () will now execute
   using SortMergeJoin instead of computing a cartisian product.
   - SPARK-11389  SQL
   Execution Using Off-Heap Memory - Support for configuring query
   execution to occur using off-heap memory to avoid GC overhead
   - SPARK-10978  Datasource
   API Avoid Double Filter - When implemeting a datasource with filter
   pushdown, developers can now tell Spark SQL to avoid double evaluating a
   pushed-down filter.
   - SPARK-4849   Advanced
   Layout of Cached Data - storing partitioning and ordering schemes in
   In-memory table scan, and adding distributeBy and localSort to DF API
   - SPARK-9858   Adaptive
   query execution - Intial support for automatically selecting the number
   of reducers for joins and aggregations.
   - SPARK-9241   Improved
   query planner for queries having distinct aggregations - Query plans of
   distinct aggregations are more robust when distinct columns have high
   cardinality.
Spark Streaming
   - API Updates
      - SPARK-2629   New
      improved state management - mapWithState - a DStream transformation
      for stateful stream processing, supercedes updateStateByKey in
      functionality and performance.
      - SPARK-11198  Kinesis
      record deaggregation - Kinesis streams have been upgraded to use KCL
      1.4.0 and supports transparent deaggregation of KPL-aggregated records.
      - SPARK-10891  Kinesis
      message handler function - Allows arbitraray function to be applied
      to a Kinesis record in the Kinesis receiver before to customize what data
      is to be stored in memory.
      - SPARK-6328   Python
      Streamng Listener API - Get streaming statistics (scheduling delays,
      batch processing times, etc.) in streaming.
   - UI Improvements
      - Made failures visible in the streaming tab, in the timelines, batch
      list, and batch details page.
      - Made output operations visible in the streaming tab as progress
      bars.
MLlibNew algorithms/models
   - SPARK-8518   Survival
   analysis - Log-linear model for survival analysis
   - SPARK-9834   Normal
   equation for least squares - Normal equation solver, providing R-like
   model summary statistics
   - SPARK-3147   Online
   hypothesis testing - A/B testing in the Spark Streaming framework
   - SPARK-9930   New
   feature transformers - ChiSqSelector, QuantileDiscretizer, SQL
   transformer
   - SPARK-6517   Bisecting
   K-Means clustering - Fast top-down clustering variant of K-Means
API improvements
   - ML Pipelines
      - SPARK-6725   Pipeline
      persistence - Save/load for ML Pipelines, with partial coverage of
      spark.ml algorithms
      - SPARK-5565   LDA
      in ML Pipelines - API for Latent Dirichlet Allocation in ML Pipelines
   - R API
      - SPARK-9836   R-like
      statistics for GLMs - (Partial) R-like stats for ordinary least
      squares via summary(model)
      - SPARK-9681   Feature
      interactions in R formula - Interaction operator ":" in R formula
   - Python API - Many improvements to Python API to approach feature parity
Misc improvements
   - SPARK-7685  ,
   SPARK-9642   Instance
   weights for GLMs - Logistic and Linear Regression can take instance
   weights
   - SPARK-10384 ,
   SPARK-10385  Univariate
   and bivariate statistics in DataFrames - Variance, stddev, correlations,
   etc.
   - SPARK-10117  LIBSVM
   data source - LIBSVM as a SQL data sourceDocumentation improvements
   - SPARK-7751   @since
   versions - Documentation includes initial version when classes and
   methods were added
   - SPARK-11337  Testable
   example code - Automated testing for code in user guide examples
Deprecations
   - In spark.mllib.clustering.KMeans, the "runs" parameter has been
   deprecated.
   - In spark.ml.classification.LogisticRegressionModel and
   spark.ml.regression.LinearRegressionModel, the "weights" field has been
   deprecated, in favor of the new name "coefficients." This helps
   disambiguate from instance (row) weights given to algorithms.
Changes of behavior
   - spark.mllib.tree.GradientBoostedTrees validationTol has changed
   semantics in 1.6. Previously, it was a threshold for absolute change in
   error. Now, it resembles the behavior of GradientDescent convergenceTol:
   For large errors, it uses relative error (relative to the previous error);
   for small errors (< 0.01), it uses absolute error.
   - spark.ml.feature.RegexTokenizer: Previously, it did not convert
   strings to lowercase before tokenizing. Now, it converts to lowercase by
   default, with an option not to. This matches the behavior of the simpler
   Tokenizer transformer.
   - Spark SQL's partition discovery has been changed to only discover
   partition directories that are children of the given path. (i.e. if
   path="/my/data/x=1" then x=1 will no longer be considered a partition
   but only children of x=1.) This behavior can be overridden by manually
   specifying the basePath that partitioning discovery should start with (
   SPARK-11678 ).
   - When casting a value of an integral type to timestamp (e.g. casting a
   long value to timestamp), the value is treated as being in seconds instead
   of milliseconds (SPARK-11724
   ).
   - With the improved query planner for queries having distinct
   aggregations (SPARK-9241
   ), the plan of a query
   having a single distinct aggregation has been changed to a more robust
   version. To switch back to the plan generated by Spark 1.5's planner,
   please set spark.sql.specializeSingleDistinctAggPlanning to true (
   SPARK-12077 ).
I'll kick off the voting with a +1.
Thanks Ben, but as I said in the first email, docs are published separately
from the release, so this isn't a valid reason to down vote the RC.  We
just provide them to help with testing.
I'll ask the mllib guys to take a look at that patch though.
Sean, if you would like to -1 the release you are certainly entitled to,
but in the past we have never held a release for documentation only
issues.  If you'd like to change the policy of the project I'm not sure
that a voting thread is the right place to do it.
I think the right question here, is "How are users going to be affected by
this temporary issue?".  Given that I'm pretty certain that no users build
the documentation from the release themselves and instead consume it from
the published documentation, the docs contained in the release seem less
important as far as voting on the artifacts is concerned.
In contrast, there have been several threads on the users list asking when
the release is going to happen.  Should we make them wait longer for
something that isn't going to affect their usage of the release?  I would
vote no.  That doesn't mean that we shouldn't fix the documentation issue.
It just means we shouldn't add unnecessary coupling where it has no benefit.
I'm only suggesting that we shouldn't delay testing of the actual bits, or
wait to iterate on another RC.  Ideally docs should come out with the
actual release announcement (and I'll do everything in my power to make
this happen).  The should also be updated regularly as small issues are
found.
But if it can/will be fixed quickly, what's the hurry? I get it, people
I'm happy to debate concrete process suggestions on another thread.
Here are a fixed version of the docs for 1.6:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc2-docsfixed-docs
There still might be some minor rendering issues of the ML page, but people
are investigating.
This vote is canceled due to the issue with the incorrect version.  This
issue will be fixed by https://github.com/apache/spark/pull/10317
We can wait a little bit for a fix to
https://issues.apache.org/jira/browse/SPARK-12345.  However if it looks
like there is not an easy fix coming soon, I'm planning to move forward
with RC3.
Please vote on releasing the following candidate as Apache Spark version
1.6.0!
The vote is open until Saturday, December 19, 2015 at 18:00 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 1.6.0
[ ] -1 Do not release this package because ...
To learn more about Apache Spark, please see http://spark.apache.org/
The tag to be voted on is *v1.6.0-rc3
(168c89e07c51fa24b0bb88582c739cec0acb44d7)
*
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc3-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1174/
The test repository (versioned as v1.6.0-rc3) for this release can be found
at:
https://repository.apache.org/content/repositories/orgapachespark-1173/
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc3-docs/
=======================================
== How can I help test this release? ==
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.
================================================
== What justifies a -1 vote for this release? ==
================================================
This vote is happening towards the end of the 1.6 QA period, so -1 votes
should only occur for significant regressions from 1.5. Bugs already
present in 1.5, minor regressions, or bugs related to new features will not
block this release.
===============================================================
== What should happen to JIRA tickets still targeting 1.6.0? ==
===============================================================
1. It is OK for documentation patches to target 1.6.0 and still go into
branch-1.6, since documentations will be published separately from the
release.
2. New features for non-alpha-modules should target 1.7+.
3. Non-blocker bug fixes should target 1.6.1 or 1.7.0, or drop the target
version.
==================================================
== Major changes to help you focus your testing ==
==================================================
Notable changes since 1.6 RC2
- SPARK_VERSION has been set correctly
- SPARK-12199 ML Docs are publishing correctly
- SPARK-12345 Mesos cluster mode has been fixed
Notable changes since 1.6 RC1
Spark Streaming
   - SPARK-2629  
   trackStateByKey has been renamed to mapWithState
Spark SQL
   - SPARK-12165 
   SPARK-12189  Fix bugs
   in eviction of storage memory by execution.
   - SPARK-12258  correct
   passing null into ScalaUDF
Notable Features Since 1.5Spark SQL
   - SPARK-11787  Parquet
   Performance - Improve Parquet scan performance when using flat schemas.
   - SPARK-10810 
   Session Management - Isolated devault database (i.e USE mydb) even on
   shared clusters.
   - SPARK-9999   Dataset
   API - A type-safe API (similar to RDDs) that performs many operations on
   serialized binary data and code generation (i.e. Project Tungsten).
   - SPARK-10000  Unified
   Memory Management - Shared memory for execution and caching instead of
   exclusive division of the regions.
   - SPARK-11197  SQL
   Queries on Files - Concise syntax for running SQL queries over files of
   any supported format without registering a table.
   - SPARK-11745  Reading
   non-standard JSON files - Added options to read non-standard JSON files
   (e.g. single-quotes, unquoted attributes)
   - SPARK-10412 
Per-operator
   Metrics for SQL Execution - Display statistics on a peroperator basis
   for memory usage and spilled data size.
   - SPARK-11329  Star
   (*) expansion for StructTypes - Makes it easier to nest and unest
   arbitrary numbers of columns
   - SPARK-10917 ,
   SPARK-11149  In-memory
   Columnar Cache Performance - Significant (up to 14x) speed up when
   caching data that contains complex types in DataFrames or SQL.
   - SPARK-11111  Fast
   null-safe joins - Joins using null-safe equality () will now execute
   using SortMergeJoin instead of computing a cartisian product.
   - SPARK-11389  SQL
   Execution Using Off-Heap Memory - Support for configuring query
   execution to occur using off-heap memory to avoid GC overhead
   - SPARK-10978  Datasource
   API Avoid Double Filter - When implemeting a datasource with filter
   pushdown, developers can now tell Spark SQL to avoid double evaluating a
   pushed-down filter.
   - SPARK-4849   Advanced
   Layout of Cached Data - storing partitioning and ordering schemes in
   In-memory table scan, and adding distributeBy and localSort to DF API
   - SPARK-9858   Adaptive
   query execution - Intial support for automatically selecting the number
   of reducers for joins and aggregations.
   - SPARK-9241   Improved
   query planner for queries having distinct aggregations - Query plans of
   distinct aggregations are more robust when distinct columns have high
   cardinality.
Spark Streaming
   - API Updates
      - SPARK-2629   New
      improved state management - mapWithState - a DStream transformation
      for stateful stream processing, supercedes updateStateByKey in
      functionality and performance.
      - SPARK-11198  Kinesis
      record deaggregation - Kinesis streams have been upgraded to use KCL
      1.4.0 and supports transparent deaggregation of KPL-aggregated records.
      - SPARK-10891  Kinesis
      message handler function - Allows arbitraray function to be applied
      to a Kinesis record in the Kinesis receiver before to customize what data
      is to be stored in memory.
      - SPARK-6328   Python
      Streamng Listener API - Get streaming statistics (scheduling delays,
      batch processing times, etc.) in streaming.
   - UI Improvements
      - Made failures visible in the streaming tab, in the timelines, batch
      list, and batch details page.
      - Made output operations visible in the streaming tab as progress
      bars.
MLlibNew algorithms/models
   - SPARK-8518   Survival
   analysis - Log-linear model for survival analysis
   - SPARK-9834   Normal
   equation for least squares - Normal equation solver, providing R-like
   model summary statistics
   - SPARK-3147   Online
   hypothesis testing - A/B testing in the Spark Streaming framework
   - SPARK-9930   New
   feature transformers - ChiSqSelector, QuantileDiscretizer, SQL
   transformer
   - SPARK-6517   Bisecting
   K-Means clustering - Fast top-down clustering variant of K-Means
API improvements
   - ML Pipelines
      - SPARK-6725   Pipeline
      persistence - Save/load for ML Pipelines, with partial coverage of
      spark.mlalgorithms
      - SPARK-5565   LDA
      in ML Pipelines - API for Latent Dirichlet Allocation in ML Pipelines
   - R API
      - SPARK-9836   R-like
      statistics for GLMs - (Partial) R-like stats for ordinary least
      squares via summary(model)
      - SPARK-9681   Feature
      interactions in R formula - Interaction operator ":" in R formula
   - Python API - Many improvements to Python API to approach feature parity
Misc improvements
   - SPARK-7685  ,
   SPARK-9642   Instance
   weights for GLMs - Logistic and Linear Regression can take instance
   weights
   - SPARK-10384 ,
   SPARK-10385  Univariate
   and bivariate statistics in DataFrames - Variance, stddev, correlations,
   etc.
   - SPARK-10117  LIBSVM
   data source - LIBSVM as a SQL data sourceDocumentation improvements
   - SPARK-7751   @since
   versions - Documentation includes initial version when classes and
   methods were added
   - SPARK-11337  Testable
   example code - Automated testing for code in user guide examples
Deprecations
   - In spark.mllib.clustering.KMeans, the "runs" parameter has been
   deprecated.
   - In spark.ml.classification.LogisticRegressionModel and
   spark.ml.regression.LinearRegressionModel, the "weights" field has been
   deprecated, in favor of the new name "coefficients." This helps
   disambiguate from instance (row) weights given to algorithms.
Changes of behavior
   - spark.mllib.tree.GradientBoostedTrees validationTol has changed
   semantics in 1.6. Previously, it was a threshold for absolute change in
   error. Now, it resembles the behavior of GradientDescent convergenceTol:
   For large errors, it uses relative error (relative to the previous error);
   for small errors (< 0.01), it uses absolute error.
   - spark.ml.feature.RegexTokenizer: Previously, it did not convert
   strings to lowercase before tokenizing. Now, it converts to lowercase by
   default, with an option not to. This matches the behavior of the simpler
   Tokenizer transformer.
   - Spark SQL's partition discovery has been changed to only discover
   partition directories that are children of the given path. (i.e. if
   path="/my/data/x=1" then x=1 will no longer be considered a partition
   but only children of x=1.) This behavior can be overridden by manually
   specifying the basePath that partitioning discovery should start with (
   SPARK-11678 ).
   - When casting a value of an integral type to timestamp (e.g. casting a
   long value to timestamp), the value is treated as being in seconds instead
   of milliseconds (SPARK-11724
   ).
   - With the improved query planner for queries having distinct
   aggregations (SPARK-9241
   ), the plan of a query
   having a single distinct aggregation has been changed to a more robust
   version. To switch back to the plan generated by Spark 1.5's planner,
   please set spark.sql.specializeSingleDistinctAggPlanning to true (
   SPARK-12077 ).
I think this is a pretty common way to model things (glancing at postgres
it looks similar).  Expression and plans are pretty different concepts.  An
expression can be evaluated on a single input row and returns a single
value.  In contrast a query plan operates on a relation and has a schema
with many different atomic values.
These traits actually have different semantics for expressions vs. plans
(i.e. a UnaryExpression nullability is based on its child's nullability,
whereas this would not make sense for a UnaryNode which does not have a
concept of nullability).
It is not clear to me that you actually want these transformations to
happen seamlessly.  For example, the resolution rules for subqueries are
different than normal plans because you have to reason about correlation.
That said, it seems like you should be able to do some magic in
RuleExecutor to make sure that things like the optimizer descend seamlessly
into nested query plans.
It's come to my attention that there have been several bug fixes merged
since RC3:
  - SPARK-12404 - Fix serialization error for Datasets with
Timestamps/Arrays/Decimal
  - SPARK-12218 - Fix incorrect pushdown of filters to parquet
  - SPARK-12395 - Fix join columns of outer join for DataFrame using
  - SPARK-12413 - Fix mesos HA
Normally, these would probably not be sufficient to hold the release,
however with the holidays going on in the US this week, we don't have the
resources to finalize 1.6 until next Monday.  Given this delay anyway, I
propose that we cut one final RC with the above fixes and plan for the
actual release first thing next week.
I'll post RC4 shortly and cancel this vote if there are no objections.
Since this vote nearly passed with no major issues, I don't anticipate any
problems with RC4.
Michael
Please vote on releasing the following candidate as Apache Spark version
1.6.0!
The vote is open until Friday, December 25, 2015 at 18:00 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 1.6.0
[ ] -1 Do not release this package because ...
To learn more about Apache Spark, please see http://spark.apache.org/
The tag to be voted on is *v1.6.0-rc4
(4062cda3087ae42c6c3cb24508fc1d3a931accdf)
*
The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc4-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1176/
The test repository (versioned as v1.6.0-rc4) for this release can be found
at:
https://repository.apache.org/content/repositories/orgapachespark-1175/
The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc4-docs/
=======================================
== How can I help test this release? ==
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.
================================================
== What justifies a -1 vote for this release? ==
================================================
This vote is happening towards the end of the 1.6 QA period, so -1 votes
should only occur for significant regressions from 1.5. Bugs already
present in 1.5, minor regressions, or bugs related to new features will not
block this release.
===============================================================
== What should happen to JIRA tickets still targeting 1.6.0? ==
===============================================================
1. It is OK for documentation patches to target 1.6.0 and still go into
branch-1.6, since documentations will be published separately from the
release.
2. New features for non-alpha-modules should target 1.7+.
3. Non-blocker bug fixes should target 1.6.1 or 1.7.0, or drop the target
version.
==================================================
== Major changes to help you focus your testing ==
==================================================
Notable changes since 1.6 RC3
  - SPARK-12404 - Fix serialization error for Datasets with
Timestamps/Arrays/Decimal
  - SPARK-12218 - Fix incorrect pushdown of filters to parquet
  - SPARK-12395 - Fix join columns of outer join for DataFrame using
  - SPARK-12413 - Fix mesos HA
Notable changes since 1.6 RC2
- SPARK_VERSION has been set correctly
- SPARK-12199 ML Docs are publishing correctly
- SPARK-12345 Mesos cluster mode has been fixed
Notable changes since 1.6 RC1
Spark Streaming
   - SPARK-2629  
   trackStateByKey has been renamed to mapWithState
Spark SQL
   - SPARK-12165 
   SPARK-12189  Fix bugs
   in eviction of storage memory by execution.
   - SPARK-12258  correct
   passing null into ScalaUDF
Notable Features Since 1.5Spark SQL
   - SPARK-11787  Parquet
   Performance - Improve Parquet scan performance when using flat schemas.
   - SPARK-10810 
   Session Management - Isolated devault database (i.e USE mydb) even on
   shared clusters.
   - SPARK-9999   Dataset
   API - A type-safe API (similar to RDDs) that performs many operations on
   serialized binary data and code generation (i.e. Project Tungsten).
   - SPARK-10000  Unified
   Memory Management - Shared memory for execution and caching instead of
   exclusive division of the regions.
   - SPARK-11197  SQL
   Queries on Files - Concise syntax for running SQL queries over files of
   any supported format without registering a table.
   - SPARK-11745  Reading
   non-standard JSON files - Added options to read non-standard JSON files
   (e.g. single-quotes, unquoted attributes)
   - SPARK-10412 
Per-operator
   Metrics for SQL Execution - Display statistics on a peroperator basis
   for memory usage and spilled data size.
   - SPARK-11329  Star
   (*) expansion for StructTypes - Makes it easier to nest and unest
   arbitrary numbers of columns
   - SPARK-10917 ,
   SPARK-11149  In-memory
   Columnar Cache Performance - Significant (up to 14x) speed up when
   caching data that contains complex types in DataFrames or SQL.
   - SPARK-11111  Fast
   null-safe joins - Joins using null-safe equality () will now execute
   using SortMergeJoin instead of computing a cartisian product.
   - SPARK-11389  SQL
   Execution Using Off-Heap Memory - Support for configuring query
   execution to occur using off-heap memory to avoid GC overhead
   - SPARK-10978  Datasource
   API Avoid Double Filter - When implemeting a datasource with filter
   pushdown, developers can now tell Spark SQL to avoid double evaluating a
   pushed-down filter.
   - SPARK-4849   Advanced
   Layout of Cached Data - storing partitioning and ordering schemes in
   In-memory table scan, and adding distributeBy and localSort to DF API
   - SPARK-9858   Adaptive
   query execution - Intial support for automatically selecting the number
   of reducers for joins and aggregations.
   - SPARK-9241   Improved
   query planner for queries having distinct aggregations - Query plans of
   distinct aggregations are more robust when distinct columns have high
   cardinality.
Spark Streaming
   - API Updates
      - SPARK-2629   New
      improved state management - mapWithState - a DStream transformation
      for stateful stream processing, supercedes updateStateByKey in
      functionality and performance.
      - SPARK-11198  Kinesis
      record deaggregation - Kinesis streams have been upgraded to use KCL
      1.4.0 and supports transparent deaggregation of KPL-aggregated records.
      - SPARK-10891  Kinesis
      message handler function - Allows arbitraray function to be applied
      to a Kinesis record in the Kinesis receiver before to customize what data
      is to be stored in memory.
      - SPARK-6328   Python
      Streamng Listener API - Get streaming statistics (scheduling delays,
      batch processing times, etc.) in streaming.
   - UI Improvements
      - Made failures visible in the streaming tab, in the timelines, batch
      list, and batch details page.
      - Made output operations visible in the streaming tab as progress
      bars.
MLlibNew algorithms/models
   - SPARK-8518   Survival
   analysis - Log-linear model for survival analysis
   - SPARK-9834   Normal
   equation for least squares - Normal equation solver, providing R-like
   model summary statistics
   - SPARK-3147   Online
   hypothesis testing - A/B testing in the Spark Streaming framework
   - SPARK-9930   New
   feature transformers - ChiSqSelector, QuantileDiscretizer, SQL
   transformer
   - SPARK-6517   Bisecting
   K-Means clustering - Fast top-down clustering variant of K-Means
API improvements
   - ML Pipelines
      - SPARK-6725   Pipeline
      persistence - Save/load for ML Pipelines, with partial coverage of
      spark.mlalgorithms
      - SPARK-5565   LDA
      in ML Pipelines - API for Latent Dirichlet Allocation in ML Pipelines
   - R API
      - SPARK-9836   R-like
      statistics for GLMs - (Partial) R-like stats for ordinary least
      squares via summary(model)
      - SPARK-9681   Feature
      interactions in R formula - Interaction operator ":" in R formula
   - Python API - Many improvements to Python API to approach feature parity
Misc improvements
   - SPARK-7685  ,
   SPARK-9642   Instance
   weights for GLMs - Logistic and Linear Regression can take instance
   weights
   - SPARK-10384 ,
   SPARK-10385  Univariate
   and bivariate statistics in DataFrames - Variance, stddev, correlations,
   etc.
   - SPARK-10117  LIBSVM
   data source - LIBSVM as a SQL data sourceDocumentation improvements
   - SPARK-7751   @since
   versions - Documentation includes initial version when classes and
   methods were added
   - SPARK-11337  Testable
   example code - Automated testing for code in user guide examples
Deprecations
   - In spark.mllib.clustering.KMeans, the "runs" parameter has been
   deprecated.
   - In spark.ml.classification.LogisticRegressionModel and
   spark.ml.regression.LinearRegressionModel, the "weights" field has been
   deprecated, in favor of the new name "coefficients." This helps
   disambiguate from instance (row) weights given to algorithms.
Changes of behavior
   - spark.mllib.tree.GradientBoostedTrees validationTol has changed
   semantics in 1.6. Previously, it was a threshold for absolute change in
   error. Now, it resembles the behavior of GradientDescent convergenceTol:
   For large errors, it uses relative error (relative to the previous error);
   for small errors (< 0.01), it uses absolute error.
   - spark.ml.feature.RegexTokenizer: Previously, it did not convert
   strings to lowercase before tokenizing. Now, it converts to lowercase by
   default, with an option not to. This matches the behavior of the simpler
   Tokenizer transformer.
   - Spark SQL's partition discovery has been changed to only discover
   partition directories that are children of the given path. (i.e. if
   path="/my/data/x=1" then x=1 will no longer be considered a partition
   but only children of x=1.) This behavior can be overridden by manually
   specifying the basePath that partitioning discovery should start with (
   SPARK-11678 ).
   - When casting a value of an integral type to timestamp (e.g. casting a
   long value to timestamp), the value is treated as being in seconds instead
   of milliseconds (SPARK-11724
   ).
   - With the improved query planner for queries having distinct
   aggregations (SPARK-9241
   ), the plan of a query
   having a single distinct aggregation has been changed to a more robust
   version. To switch back to the plan generated by Spark 1.5's planner,
   please set spark.sql.specializeSingleDistinctAggPlanning to true (
   SPARK-12077 ).
I'll kick the voting off with a +1.
Which version of spark?
Looks like a bug.  Have you tried Spark 1.6 RC4?
http://people.apache.org/~pwendell/spark-releases/spark-1.6.0-rc4-bin/
Hi All,
Spark 1.6.0 is the seventh release on the 1.x line. This release includes
patches from 248+ contributors! To download Spark 1.6.0 visit the downloads
page.  (It may take a while for all mirrors to update.)
A huge thanks go to all of the individuals and organizations involved in
development and testing of this release.
Visit the release notes [1] to read about the new features, or download [2]
the release today.
For errata in the contributions or release notes, please e-mail me
*directly* (not on-list).
Thanks to everyone who helped work on this release!
[1] http://spark.apache.org/releases/spark-release-1-6-0.html
[2] http://spark.apache.org/downloads.html
Thanks for providing a great description.  I've opened
https://issues.apache.org/jira/browse/SPARK-12696
I'm actually getting a different error (running in notebooks though).
Something seems wrong either way.
We have tests for reordering
can
you provide a smaller reproduction of this problem?
Were you running in the REPL?
I think this should be fixed in both master and branch-1.6 now.  We'll look
at doing 1.6.1 sometime in the near future.  Please let me know if you can
reproduce any issues there.
Hey All,
While I'm not aware of any critical issues with 1.6.0, there are several
corner cases that users are hitting with the Dataset API that are fixed in
branch-1.6.  As such I'm considering a 1.6.1 release.
At the moment there are only two critical issues targeted for 1.6.1:
 - SPARK-12624 - When schema is specified, we should treat undeclared
fields as null (in Python)
 - SPARK-12783 - Dataset map serialization error
When these are resolved I'll likely begin the release process.  If there
are any other issues that we should wait for please contact me.
Michael
We do maintenance releases on demand when there is enough to justify doing
one.  I'm hoping to cut 1.6.1 soon, but have not had time yet.
Its already underway: https://github.com/apache/spark/pull/10608
I think this is fixed in branch-1.6 already.  If you can reproduce it there
can you please open a JIRA and ping me?
We typically do not allow changes to the classpath in maintenance releases.
I think we have enough issues queued up that I would not hold the release
for that, but if there is a patch we should try and review it.  We can
always do 1.6.2 when more issues have been resolved.  Is this an actual
issue that is affecting a production workload or are we concerned about an
edge case?
I'm waiting for a few last fixes to be merged.  Hoping to cut an RC in the
next few days.
RDD level partitioning information is not used to decide when to shuffle
for queries planned using Catalyst (since we have better information about
distribution from the query plan itself).  Instead you should be looking at
the logic in EnsureRequirements
.
We don't yet reason about equivalence classes for attributes when deciding
if a given partitioning is valid, but #10844
 is a start at building that
infrastructure.
That looks like a bug in toString for columns.  Can you open a JIRA?
I'm not going to be able to do anything until after the Spark Summit, but I
will kick off RC1 after that (end of week).  Get your patches in before
then!
I will cut the RC today.  Sorry for the delay!
I've kicked off the build.  Please be extra careful about merging into
branch-1.6 until after the release.
An update: people.apache.org has been shut down so the release scripts are
broken. Will try again after we fix them.
Unfortunately I don't think thats sufficient as they don't seem to support
sftp in the same way they did before.  We'll still need to update our
release scripts.
Please vote on releasing the following candidate as Apache Spark version
1.6.1!
The vote is open until Saturday, March 5, 2016 at 20:00 UTC and passes if a
majority of at least 3+1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 1.6.1
[ ] -1 Do not release this package because ...
To learn more about Apache Spark, please see http://spark.apache.org/
The tag to be voted on is *v1.6.1-rc1
(15de51c238a7340fa81cb0b80d029a05d97bfc5c)
*
The release files, including signatures, digests, etc. can be found at:
https://home.apache.org/~pwendell/spark-releases/spark-1.6.1-rc1-bin/
Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc
The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1180/
The test repository (versioned as v1.6.1-rc1) for this release can be found
at:
https://repository.apache.org/content/repositories/orgapachespark-1179/
The documentation corresponding to this release can be found at:
https://home.apache.org/~pwendell/spark-releases/spark-1.6.1-rc1-docs/
=======================================
== How can I help test this release? ==
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 1.6.0.
================================================
== What justifies a -1 vote for this release? ==
================================================
This is a maintenance release in the 1.6.x series.  Bugs already present in
1.6.0, missing features, or bugs related to new features will not
necessarily block this release.
===============================================================
== What should happen to JIRA tickets still targeting 1.6.0? ==
===============================================================
1. It is OK for documentation patches to target 1.6.1 and still go into
branch-1.6, since documentations will be published separately from the
release.
2. New features for non-alpha-modules should target 1.7+.
3. Non-blocker bug fixes should target 1.6.2 or 2.0.0, or drop the target
version.
That looks like a bug to me.  Open a JIRA?
This is in active development, so there is not much that can be done from
an end user perspective.  In particular the only sink that is available in
apache/master is a testing sink that just stores the data in memory.  We
are working on a parquet based file sink and will eventually support all
the of Data Source API file formats (text, json, csv, orc, parquet).
+1 - Ported all our internal jobs to run on 1.6.1 with no regressions.
This vote passes with nine +1s (five binding) and one binding +0!  Thanks
to everyone who tested/voted.  I'll start work on publishing the release
today.
+1:
Mark Hamstra*
Moshe Eshel
Egor Pahomov
Reynold Xin*
Yin Huai*
Andrew Or*
Burak Yavuz
Kousuke Saruta
Michael Armbrust*
0:
Sean Owen*
-1: (none)
*Binding
Spark 1.6.1 is a maintenance release containing stability fixes. This
release is based on the branch-1.6 maintenance branch of Spark. We
*strongly recommend* all 1.6.0 users to upgrade to this release.
Notable fixes include:
 - Workaround for OOM when writing large partitioned tables SPARK-12546
 - Several fixes to the experimental Dataset API - SPARK-12478
, SPARK-12696
, SPARK-13101
, SPARK-12932
The full list of bug fixes is here: http://s.apache.org/spark-1.6.1
http://spark.apache.org/releases/spark-release-1-6-1.html
(note: it can take a few hours for everything to be propagated, so you
might get 404 on some download links, but everything should be in maven
central already.  If you see any issues with the release notes or webpage
*please contact me directly, off-list*)
Trees are immutable, and TreeNode takes care of copying unchanged parts of
the tree when you are doing transformations.  As a result, even if you do
construct a DAG with the Dataset API, the first transformation will turn it
back into a tree.
The only exception to this rule is when we share the results of plans after
an Exchange operator.  This is the last step before execution and sometimes
turns the query into a DAG to avoid redundant computation.
+1 to Matei's reasoning.
Some answers and more questions inline
- UDFs can pretty much only take in Primitives, Seqs, Maps and Row objects
This is true today, but could be improved using the new encoder framework.
Out of curiosity, have you look at that?  If so, what is missing thats
leading you back to UDFs.
Is there any way to return a Row object in scala from a UDF and specify the
I think UDF1/2/3 are the only way to do this today.  Is the problem here
that you are only changing a subset of the nested data and you want to
preserve the structure.  What kind of changes are you doing?
2) Is Spark actually converting the returned case class object when the UDF
We use reflection to figure out the schema and extract the data into the
internal row format.  We actually runtime build bytecode for this in many
cases (though not all yet) so it can be pretty fast.
You can do this with Datasets:
df.as[CaseClass].map(o => do stuff)
There is an experimental preview of Datasets in Spark 1.6
Even if you give us a Row there's still a conversion into the binary format
of InternalRow
This was before we decided to unify the APIs for Scala and Java, so its
mostly historical.
It is called groupByKey now.  Similar to joinWith, the schema produced by
relational joins and aggregations is different than what you would expect
when working with objects.  So, when combining DataFrame+Dataset we renamed
these functions to make this distinction clearer.
This would also probably improve performance:
https://github.com/apache/spark/pull/9565
Spark SQL's query planner has always delayed building the RDD, so has never
needed to eagerly calculate the range boundaries (since Spark 1.0).
+1 to the general structure of Reynold's proposal.  I've found what we do
currently a little confusing.  In particular, it doesn't make much sense
that @DeveloperApi things are always labeled as possibly changing.  For
example the Data Source API should arguably be one of the most stable
interfaces since its very difficult for users to recompile libraries that
might break when there are changes.
For a similar reason, I don't really see the point of LimitedPrivate.  The
goal here should be communication of promises of stability or future
stability.
Regarding Developer vs. Public. I don't care too much about the naming, but
it does seem useful to differentiate APIs that we expect end users to
consume from those that are used to augment Spark. "Library" and
"Application" also seem reasonable.
Yeah, can you open a JIRA with that reproduction please?  You can ping me
on it.
+1, excited for 2.0!
We did turn on travis a few years ago, but ended up turning it off because
it was failing (I believe because of insufficient resources) which was
confusing for developers.  I wouldn't be opposed to turning it on if it
provides more/faster signal, but its not obvious to me that it would.  In
particular, do we know that given the rate PRs are created if we will hit
rate limits?
Really my main feedback is, if the java linter is important we should
probably have it as part of the canonical build process.  I worry about
having more than one set of CI infrastructure to maintain.
Another clarification: not databricks, but the Apache Spark PMC grants
access to the JIRA / wiki.  That said... I'm not actually sure how its done.
Yeah, we don't usually publish RCs to central, right?
I don't feel super strongly one way or the other, so if we need to publish
it permanently we can.
However, either way you can still test against this release.  You just need
to add a resolver as well (which is how I have always tested packages
against RCs).  One concern with making it permeant is this preview release
is already fairly far behind branch-2.0, so many of the issues that people
might report have already been fixed and that might continue even after the
release is made.  I'd rather be able to force upgrades eventually when we
vote on the final 2.0 release.
NoSuchMethodError always means that you are compiling against a different
classpath than is available at runtime, so it sounds like you are on the
right track.  The project is not abandoned, we're just busy with the
release.  It would be great if you could open a pull request.
+1 to both of these!
There is no public API for writing encoders at the moment, though we are
hoping to open this up in Spark 2.1.
What is not working about encoders for options?  Which version of Spark are
you running?  This is working as I would expect?
https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1023043053387187/1073771007111588/2840265927289860/latest.html
Another good signal is the "target version" (which by convention is only
set by committers).  When I set this for the upcoming version it means I
think its important enough that I will prioritize reviewing a patch for it.
Even equals(...) does not do what you want on the JVM:
scala> Array(1,2).equals(Array(1,2))
res1: Boolean = false
Pull requests for documentation welcome!
Internally for tests, we usually compare the string representation of the
Row.
+1
This is not too broadly worded, and in general I would caution that any
interface in org.apache.spark.sql.catalyst or
org.apache.spark.sql.execution is considered internal and likely to change
in between releases.  We do plan to open a stable source/sink API in a
future release.
The problem here is that the DataFrame is constructed using an
incrementalized physical query plan.  If you call any operations on the
Dataframe that change the logical plan, you will loose prior state and the
DataFrame will return an incorrect result.  Since this was discovered late
in the release process we decided it was better to document the current
behavior, rather than do a large refactoring.
Yeah, turning it into an RDD should preserve the incremental planning.
+ dev, reynold
Yeah, thats a good point.  I wonder if SparkSession.sqlContext should be
public/deprecated?
+1
I don't think this would be hard to implement.  The physical explode
operator supports it (for our HiveQL compatibility).
Perhaps comment on this JIRA?
https://issues.apache.org/jira/browse/SPARK-13721
It could probably just be another argument to explode()
Michael
Spark SQL has great support for reading text files that contain JSON data.
However, in many cases the JSON data is just one column amongst others.
This is particularly true when reading from sources such as Kafka. This PR
 adds a new functions
from_json that
converts a string column into a nested StructType with a user specified
schema, using the same internal logic as the json Data Source.
Would love to hear any comments / suggestions.
Michael
+1
Spark has a long history
 of maintaining
binary compatibility in its public APIs.  I strongly believe this is one of
the things that has made the project successful.  Exposing internals that
we know are going to change in the primary user facing API for creating
Streaming DataFrames seems directly counter to this goal.  I think the
argument that "you can do it anyway" fails to capture user expectations who
probably aren't closely following this discussion.
If advanced users want to dig though the code and experiment, great.  I
hope they report back on whats good and what can be improved.  However, if
you add the function suggested in the PR to DataStreamReader, you are
giving them a bad experience by leaking internals that don't even show up
in the published documentation.
We recently merged support for Kafak 0.10.0 in Structured Streaming, but
I've been hearing a few people tell me that they are stuck on an older
version of Kafka and cannot upgrade.  I'm considering revisiting SPARK-17344
, but it would be good
to have more information.
Could people please vote or comment on the above ticket if a lack of
support for older versions of kafka would block you from trying out
structured streaming?
Thanks!
Michael
Can you elaborate? Especially in the context of the interface provided by
structured streaming.
 This lines up with my testing.  Is there a page I'm missing that describes
this?  Like does a 0.9 client work with 0.8 broker?  Is it always old
clients can talk to new brokers but not vice versa?
How would the current "subscribe" break?
This is super helpful, thanks for writing it up!
Agree, this should be the major focus.
I've heard this one too, but don't know of anyone actively working on it.
Would be awesome to open a JIRA and start discussing what the APIs would
look like.
This sounds like two separate things to me.  High-level APIs (are streaming
DataFrames / Datasets missing anything?) and multi-query optimization for
streams.  I've been thinking about the latter.  I think we probably want to
crush latency/throughput/stability in the simpler case first, but after
that I think there is a lot of machinery already in SQL we can reuse (i.e.
the sameResult calculations used for caching).
There is a  lot
 of
 confusion
 around
 nullable
 in
 StructType
 and we should definitly
come up with a consistent story and make sure we have better
documentation.  This might even mean deprecating this field.  At a high
level, I think the key problem is that internally, nullable is used as an
optimization, not an enforcement mechanism.  This is a lot different than
NOT NULL in a traditional database. Specifically, we take "nulllable =
false" as a promise that that column will never be null and use that fact
to do optimizations like skipping null checks.  This means that if you lie
to us, you actually can get wrong answers (i.e. 0 instead of null).  This
is clearly confusing and not ideal.
A little bit of explanation for some of the specific cases you brought up:
the reason that we call asNullable on file sources is that even if that
column is never null in the data, there are cases that can still produce a
null result.  For example, when parsing JSON, we null out all columns other
than _corrupt_record when we fail to parse a line.  The fact that its
different in streaming is a bug.
Would you mind opening up a JIRA ticket, and we discuss the right path
forward there?
Yes please! This probably affects correctness.
Anything that is actively being designed should be in JIRA, and it seems
like you found most of it.  In general, release windows can be found on the
wiki .
2.1 has a lot of stability fixes as well as the kafka support you
mentioned.  It may also include some of the following.
The items I'd like to start thinking about next are:
 - Evicting state from the store based on event time watermarks
 - Sessionization (grouping together related events by key / eventTime)
 - Improvements to the query planner (remove some of the restrictions on
what queries can be run).
This is roughly in order based on what I've been hearing users hit the
most.  Would love more feedback on what is blocking real use cases.
I know people are seriously thinking about latency.  So far that has not
been the limiting factor in the users I've been working with.
I totally agree we should spend more time making sure the roadmap is clear
to everyone, but I disagree with this characterization.  There is a lot of
work happening in Structured Streaming. In this next release (2.1 as well
as 2.0.1 and 2.0.2) it has been more about stability and scalability rather
than user visible features.  We are running it for real on production jobs
and working to make it rock solid (Everyone can help here!). Just look at the
list of commits
.
Regarding the timeline to graduation, I think its instructive to look at
what happened with Spark SQL.
 - Spark 1.0 - added to Spark
 - Spark 1.1 - basic apis, and stability
 - Spark 1.2 - stabilization of Data Source APIs for plugging in external
sources
 - Spark 1.3 - GA
 - Spark 1.4-1.5 - Tungsten
 - Spark 1.6 - Fully-codegened / memory managed
 - Spark 2.0 - Whole stage codegen, experimental streaming support
We probably won't follow that exactly, and we clearly are not done yet.
However, I think the trajectory is good.
But Streaming Query sources
It certainly could be, but what Matei is saying is that user code should be
able to seamlessly upgrade.  A lot of early focus and thought was towards
this goal.  However, these kinds of concerns are exactly why I think it is
premature to expose these internal APIs to end users. Lets build several
Sources and Sinks internally, and figure out what works and what doesn't.
Spark SQL had JSON, Hive, Parquet, and RDDs before we opened up the APIs.
This experience allowed us keep the Data Source API stable into 2.x and
build a large library of connectors.
Hmm, that is unfortunate.  Maybe the best solution is to add support for
sets?  I don't think that would be super hard.
+1
Sorry, I realize that set is only one example here, but I don't think that
making the type of the implicit more narrow to include only ProductN or
something eliminates the issue.  Even with that change, we will fail to
generate an encoder with the same error if you, for example, have a field
of your case class that is an unsupported type.
Short of changing this to compile-time macros, I think we are stuck with
this class of errors at runtime.  The simplest solution seems to be to
expand the set of thing we can handle as much as possible and allow users
to turn on a kryo fallback for expression encoders.  I'd be hesitant to
make this the default though, as behavior would change with each release
that adds support for more types.  I would be very supportive of making
this fallback a built-in option though.
You use kryo encoder for the whole thing?  Or just the subtree that we
don't have specific encoders for?
Also, I'm saying I like the idea of having a kryo fallback.  I don't see
the point of narrowing the the definition of the implicit.
Awesome, this is a great idea.  I opened SPARK-18122
.
And the JIRA: https://issues.apache.org/jira/browse/SPARK-18124
+1
I'm planning to do a little maintenance on JIRA to hopefully improve the
visibility into the progress / gaps in Structured Streaming.  In
particular, while we share a lot of optimization / execution logic with
SQL, the set of desired features and bugs is fairly different.
Proposal:
  - Structured Streaming (new component, move existing tickets here)
  - Streaming -> DStreams
Thoughts, objections?
Michael
I did this
.
Please help me correct any issues I may have missed.
Yeah, agreed.  As mentioned here
, its near the top of my list.
I just opened SPARK-18234
 to track.
+1
+1
+1
I would definitly like to open up APIs for people to write their own
encoders.  The challenge thus far has been that Encoders use internal APIs
that have not been stable for translating the data into the tungsten
format.  We also make use of the analyzer to figure out the mapping from
columns to fields (also not a stable API)  This is the only "magic" that is
happening.
If someone wants to propose a stable / fast API here it would be great to
start the discussion.  Its an often requested feature.
Doing this generally is pretty hard.  We will likely support algebraic
aggregate eventually, but this is not currently slotted for 2.2.  Instead I
think we will add something like mapWithState that lets users compute
arbitrary stateful things.  What is your use case?
Unfortunately the FileFormat APIs are not stable yet, so if you are using
spark-avro, we are going to need to update it for this release.
Here is the JIRA for adding this feature:
https://issues.apache.org/jira/browse/SPARK-10816
Pull requests would be welcome for any major missing features in the guide:
https://github.com/apache/spark/blob/master/docs/sql-programming-guide.md
An encoder uses reflection
to generate expressions that can extract data out of an object (by calling
methods on the object) and encode its contents directly into the tungsten
binary row format (and vice versa).  We codegenerate bytecode that
evaluates these expression in the same way that we code generate code for
normal expression evaluation in query processing.  However, this reflection
only works for simple ATDs
.  Another key thing to
realize is that we do this reflection / code generation at runtime, so we
aren't constrained by binary compatibility across versions of spark.
UDTs let you write custom code that translates an object into into a
generic row, which we can then translate into Spark's internal format
(using a RowEncoder). Unlike expressions and tungsten binary encoding, the
Row type that you return here is a stable public API that hasn't changed
since Spark 1.3.
So to summarize, if encoders don't work for your specific types you can use
UDTs, but they probably won't be as efficient. I'd love to unify these code
paths more, but its actually a fair amount of work to come up with a good
stable public API that doesn't sacrifice performance.
I think we should add something similar to mapWithState in 2.2.  It would
be great if you could add the description of your problem to this ticket:
https://issues.apache.org/jira/browse/SPARK-19067
You might also be interested in this:
https://issues.apache.org/jira/browse/SPARK-19031
+1, we should just fix the error to explain why months aren't allowed and
suggest that you manually specify some number of days.
-dev
You can use withColumn to change the type after the data has been loaded
.
Here a JIRA: https://issues.apache.org/jira/browse/SPARK-19497
We should add this soon.
It is always possible that there will be extra jobs from failed batches.
However, for the file sink, only one set of files will make it into
_spark_metadata directory log.  This is how we get atomic commits even when
there are files in more than one directory.  When reading the files with
Spark, we'll detect this directory and use it instead of listStatus to find
the list of valid files.
Sorry, I think I was a little unclear.  There are two things at play here.
 - Exactly-once semantics with file output: spark writes out extra metadata
on which files are valid to ensure that failures don't cause us to "double
count" any of the input.  Spark 2.0+ detects this info automatically when
you use dataframe reader (spark.read...). There may be extra files, but
they will be ignored. If you are consuming the output with another system
you'll have to take this into account.
 - Retries: right now we always retry the last batch when restarting.  This
is safe/correct because of the above, but we could also optimize this away
by tracking more information about batch progress.
Read the JSON log of files that is in `/your/path/_spark_metadata` and only
read files that are present in that log (ignore anything else).
The JSON log is only used by the file sink (which it doesn't seem like you
are using).  Though, I'm not sure exactly what is going on inside of
setupGoogle or how tableReferenceSource is used.
Typically you would run df.writeStream.option("path", "/my/path")... and
then the transaction log would go into /my/path/_spark_metadata.
There is not requirement that a sink uses this kind of a update log.  This
is just how we get better transactional semantics than HDFS is providing.
If your sink supports transactions natively you should just use those
instead.  We pass a unique ID to the sink method addBatch so that you can
make sure you don't commit the same transaction more than once.
Function1 is specialized, but nullSafeEval is Any => Any, so that's still
going to box in the non-codegened execution path.
Thanks for your interest in Apache Spark Structured Streaming!
There is nothing secret in that demo, though I did make some configuration
changes in order to get the timing right (gotta have some dramatic effect
:) ).  Also I think the visualizations based on metrics output by the
StreamingQueryListener
are
still being rolled out, but should be available everywhere soon.
First, I set two options to make sure that files were read one at a time,
thus allowing us to see incremental results.
spark.readStream
  .option("maxFilesPerTrigger", "1")
  .option("latestFirst", "true")
...
There is more detail on how these options work in this post
.
Regarding continually updating result of a streaming query using display(df)for
streaming DataFrames (i.e. one created with spark.readStream), that has
worked in Databrick's since Spark 2.1.  The longer form example we
published requires you to rerun the count to see it change at the end of
the notebook because that is not a streaming query. Instead it is a batch
query over data that has been written out by another stream.  I'd like to
add the ability to run a streaming query from data that has been written
out by the FileSink (tracked here SPARK-19633
).
In the demo, I started two different streaming queries:
 - one that reads from json / kafka => writes to parquet
 - one that reads from json / kafka => writes to memory sink
/ pushes latest answer to the js running in a browser using the
StreamingQueryListener
.
This is packaged up nicely in display(), but there is nothing stopping you
from building something similar with vanilla Apache Spark.
Michael
