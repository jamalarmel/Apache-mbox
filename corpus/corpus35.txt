There is also an alternative called 'sbt-onejar" we can look at. On Thu, Jul 18, 2013 at 10:48 AM, Matei Zaharia  Thanks for the feedback. It looks like there are more advantages to Maven -- -- Evan Chan Staff Engineer ev@ooyala.com  | Hey Patrick, A while back I posted an SBT recipe allowing users to build Scala job assemblies that excluded Spark and its deps, which is what most people want I believe.  This allows you to include your own libraries and exclude Spark's for the smallest possible one. We don't use Spark's run script, instead we have SBT configured so that you can simply type "run" to run jobs.   I believe this gives maximum developer velocity.   We also have "sbt console" hooked up so that you can run spark shell from it (no need for ./spark-shell script). And, as you know, we are going to contribute back a job server.   We believe that for most organizations this will provide the easiest way for submitting and managing jobs -- IT/OPS sets up Spark as HTTP service (using job server), and users/developers can submit jobs to a managed service. We even have a giter8 template to make creating jobs for job server super simple.  The template has support for local run, spark shell, assembly, and testing. So anyways, I believe we'll have a lot to contribute to your guide -- both now and especially once the job server is contributed....  feel free to touch base offline. -Evan On Fri, Aug 2, 2013 at 9:50 PM, Patrick Wendell  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Matei, How about documentation updates, such as for the binary distribution? On Friday, August 9, 2013 10:42:15 AM UTC-7, Matei Zaharia wrote: When is the upgrade to 2.10 planned? thx, Evan Hey guys, What is the schedule for the 0.8 release? In general, will the dev community be notified of code freeze, testing deadlines, doc deadlines, etc.? I'm specifically looking to know when is the deadline for submitting doc pull requests.  :) thanks, Evan -- -- Evan Chan Staff Engineer ev@ooyala.com  | Cross post thank you. On Fri, Aug 30, 2013 at 3:44 PM, Evan Chan  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Sorry one more clarification. For doc pull requests for 0.8 release, should these be done against the existing mesos/spark repo, or against the mirror at apache/incubator-spark ? I'm hoping to clear up a couple things in the docs before the release this week. thanks, -Evan On Tue, Sep 3, 2013 at 2:25 PM, Mark Hamstra  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Matei, On Tue, Sep 3, 2013 at 7:09 PM, Matei Zaharia  wrote: Yes, I was planning to write something on make-distribution.sh, and reference it in the standalone and Mesos deploy guides. I was also going to document public methods in SparkContext that have not been documented before, such as getPersistentRdds, getExecutorStatus etc.   Some folks on my team don't realize that such methods existed as they were not in the doc. -Evan -- -- Evan Chan Staff Engineer ev@ooyala.com  | Sorry just to be super clear but "the GitHub repo" (ie PRs for 0.8 docs) refers to: a)  github.com/mesos/spark b)  github.com/apache/incubator-spark I'm assuming a) but don't want to be wrong.... thanks! On Tue, Sep 3, 2013 at 7:08 PM, Matei Zaharia  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Patrick, I'm planning to submit documentation PR's against mesos/spark, by tomorrow, is that OK?    We really should update the docs. thanks, Evan On Thu, Sep 5, 2013 at 9:20 PM, Patrick Wendell  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Are we ready to document the fair scheduler?    This section on the standalone docs seems out of date.... # Job Scheduling The standalone cluster mode currently only supports a simple FIFO scheduler across jobs. However, to allow multiple concurrent jobs, you can control the maximum number of resources each Spark job will acquire. By default, it will acquire *all* the cores in the cluster, which only makes sense if you run just a single job at a time. You can cap the number of cores using `System.setProperty("spark.cores.max", "10")` (for example). This value must be set *before* initializing your SparkContext. -- -- Evan Chan Staff Engineer ev@ooyala.com  | Shane, and others, Let's work together on the configuration thing.   I had proposed in a separate thread to use Typesafe Config to hold all configuration (essentially a configuration class, but which can read from both JSON files as well as -D java command line args). Typesafe Config works much much better than a simple config class, and also better than Hadoop configs.  It also has advantages over JSON (more readable, comments).   It would also be the easiest to transition from the current scheme, since the current java system properties can be seamlessly integrated. I would be happy to contribute this back soon because it is also a big pain point for us.  I also have extensive experience with both Typesafe Config and other config systems. I would definitely start with SparkContext and work our way out from there. In fact I can submit a patch for everyone to test out fairly quickly just for SparkContext. -Evan On Tue, Sep 24, 2013 at 10:26 PM, Shane Huang  I think it's good to have Bigtop to package Spark. But in this track we're -- -- Evan Chan Staff Engineer ev@ooyala.com  | Hi Shane, Junluan, Definitely, let's cooperate.  Should we chat offline? -Evan On Thu, Sep 26, 2013 at 1:24 AM, Xia, Junluan  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Hey guys, Does anyone see a reason to keep the "spark.hostPort" system property around? It is cleared by lots of tests, set in two places (SparkEnv, and StandaloneExecutorBackend), and only used as follows: Utils.localHostPort() reads it localHostPort is called from - BlockManager (which calls it, but _never_ uses the resulting output!) - MapOutputTracker (which passes it as a value, not sure to where) Just trying to clean house on properties. thanks, Evan -- -- Evan Chan Staff Engineer ev@ooyala.com  | On a related note, there are tons of spark.akka.* properties. Does anyone see why we should not just use the base akka.* properties defined by Akka itself?  Right now the code simply maps spark.akka.* to akka.* when starting Akka. The only reasons I see for keeping spark.akka is that we might not want people fiddling with the base Akka properties.  On the other hand, Akka's documentation is excellent, and using akka's own config properties means we no longer need to keep up with this shim layer. -Evan On Sat, Sep 28, 2013 at 3:44 PM, Matei Zaharia  Hi Evan, -- -- Evan Chan Staff Engineer ev@ooyala.com  | Got it.  How about we make it such that any property spark.akka.* will be forwarded to akka as akka.* (ie with the "spark." part stripped).   So there is no need to manually transcribe properties over. On Sat, Sep 28, 2013 at 4:53 PM, Matei Zaharia  The main reason I wanted them separate was that people might use Akka in -- -- Evan Chan Staff Engineer ev@ooyala.com  | Once you have compiled everything the first time using SBT (assembly will do that for you), successive runs of assembly are much faster.  I just did it on my MacBook Pro in about 36 seconds. Running builds using IntelliJ or an IDE is wasted time, because the compiled classes go to a different place than SBT.   Maybe there's some way to symlink them. -Evan On Tue, Oct 8, 2013 at 6:29 AM, Markus Losoi  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Matei, I wonder if we can further optimize / reduce the size of the assembly. One idea is to produce just a core assembly, and have the other projects produce their own assemblies which exclude the core dependencies. Also, DistributedSuite is pretty slow.  would it make sense to tag certain tests as the "core" tests and give it a separate build target?     The overall tests that include DistributedSuite can trigger assembly, but then it would be much faster to run the core tests. -Evan On Wed, Oct 9, 2013 at 12:54 AM, Matei Zaharia  For most development, you might not need to do assembly. You can run most -- -- Evan Chan Staff Engineer ev@ooyala.com  | +1 for Ruby, as Ruby already has a functional-ish collection API, so mapping the RDD functional transforms over would be pretty idiomatic. Would love to review this. On Tue, Oct 15, 2013 at 10:13 AM, Patrick Wendell  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | I'm at the latest commit f0e23a023ce1356bc0f04248605c48d4d08c2d05 Merge: aec9bf9 a197137 Author: Reynold Xin Date:   Tue Oct 29 01:41:44 2013 -0400 and seeing this when I do a "test-only FileServerSuite": 13/10/30 09:35:04.300 INFO DAGScheduler: Completed ResultTask(0, 0) 13/10/30 09:35:04.307 INFO LocalTaskSetManager: Loss was due to java.io.StreamCorruptedException java.io.StreamCorruptedException: invalid type code: AC at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353) at java.io.ObjectInputStream.readObject(ObjectInputStream.java:348) at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:39) at org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:101) at org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71) at scala.collection.Iterator$$anon$21.hasNext(Iterator.scala:440) at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:26) at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:27) at org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:53) at org.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKey$2.apply(PairRDDFunctions.scala:95) at org.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKey$2.apply(PairRDDFunctions.scala:94) at org.apache.spark.rdd.MapPartitionsWithContextRDD.compute(MapPartitionsWithContextRDD.scala:40) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237) at org.apache.spark.rdd.RDD.iterator(RDD.scala:226) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:107) at org.apache.spark.scheduler.Task.run(Task.scala:53) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:212) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918) at java.lang.Thread.run(Thread.java:680) Anybody else seen this yet? I have a really simple PR and this fails without my change, so I may go ahead and submit it anyways. -- -- Evan Chan Staff Engineer ev@ooyala.com  | Must be a local environment thing, because AmpLab Jenkins can't reproduce it..... :-p On Wed, Oct 30, 2013 at 11:10 AM, Josh Rosen  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Mark:  I'm using JDK 1.6. -- On Wed, Oct 30, 2013 at 1:44 PM, Mark Hamstra  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Oops, hit enter too soon. Mark: 1.6.0_51.   I'm hesitant to upgrade to JDK 1.7, as several folks reported problems on OSX.   Also, I have no problems building the assembly, and it only takes me about 2 minutes (I'm running on SSD's though  :) I might bite the bullet and upgrade to JDK 1.7 for other reasons though. Patrick:  I know the branch with my config overhaul, last merged from master Oct-10th, doesn't exhibit this problem.  (Note that I tend to set SPARK_LOCAL_IP to "localhost", which I don't think affects this, and it fails either with it set or not, i believe)  We could potentially run a git bisect starting roughly 2-3 weeks ago. On Sun, Nov 3, 2013 at 3:55 PM, Evan Chan  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | +1 for IteratorWithSizeEstimate. I believe today only HadoopRDDs are able to give fine grained progress;  with an enhanced iterator interface (which can still expose the base Iterator trait) we can extend the possibility of fine grained progress to all RDDs that implement the enhanced iterator. On Tue, Nov 12, 2013 at 11:07 AM, Stephen Haberman wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Hi Umar, It's fine to look into hooking into HadoopRDD, though I think we need a general purpose way to provide metrics and progress for non-Hadoop RDDs (ie RDDs that aren't based on an InputFormat).   Any ideas would be great.  :) -Evan On Wed, Nov 20, 2013 at 9:20 PM, Umar Javed  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | 1) zk:// URIs can no longer be passed to SparkContext as the first argument, as the MESOS_REGEX was changed to take only mesos:// URIs. This essentially renders Spark incompatible with real production Mesos setups. 2) Spinning up tasks on a Mesos cluster, all of them fail right away, with no log message visible.  This might possibly be me, but this was working with 0.8.0. I can submit a pull request for #1, but a better fix would be to have some kind of MesosSuite so that any kind of breakages can be caught systematically.   Maybe some kind of VM, but ideas are welcome.  What do you guys think? (I've cc'ed two folks from Mesosphere, esp on setting up a test suite) -Evan -- -- Evan Chan Staff Engineer ev@ooyala.com  | Mark, Thanks.  The FutureAction API looks awesome. On Mon, Dec 9, 2013 at 9:31 AM, Mark Hamstra  Spark has already supported async jobs for awhile now -- -- -- Evan Chan Staff Engineer ev@ooyala.com  | Nick, have you tried using the latest Scala plug-in, which features native SBT project imports?   ie you no longer need to run gen-idea. On Sat, Dec 7, 2013 at 4:15 AM, Nick Pentreath  Hi Spark Devs, -- -- Evan Chan Staff Engineer ev@ooyala.com  | Hi Reynold, The default, documented methods of starting Spark all use the assembly jar, and thus java, right? -Evan On Fri, Dec 20, 2013 at 11:36 PM, Reynold Xin  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | +1 for using more functional idioms in general. That's a pretty clever use of `fold`, but putting the default condition first there makes it not as intuitive.   What about the following, which are more readable? option.map { a => someFuncMakesB() } .getOrElse(b) option.map { a => someFuncMakesB() } .orElse { a => otherDefaultB() }.get On Thu, Dec 26, 2013 at 12:33 PM, Mark Hamstra  In code added to Spark over the past several months, I'm glad to see more -- -- Evan Chan Staff Engineer ev@ooyala.com  | http://www.scala-graph.org/ Have you guys seen the above site?  I wonder if this will ever be merged into the Scala standard library, but might be interesting to see if this fits into GraphX at all, or to add a Spark backend to it. -- -- Evan Chan Staff Engineer ev@ooyala.com  | +1 for JSON.  The job server could also save context history and state and make it queryable via REST APIs regardless of YARN / Mesos / standalone. On Tue, Jan 7, 2014 at 2:51 PM, Tom Graves  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | BTW, we also run Scalariform, but we don't turn it on automatically.  We find that for the most part it is good, but there are a few places where it reformats things and doesn't look good, and requires cleanup.  I think Scalariform requires some more rules to make it more generally useful. -Evan On Thu, Jan 9, 2014 at 12:23 PM, DB Tsai  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | By the way, is there any plan to make a pluggable backend for checkpointing?   We might be interested in writing a, for example, Cassandra backend. On Sat, Jan 25, 2014 at 9:49 PM, Xia, Junluan  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | I might have missed it earlier, but is anybody planning to present at ApacheCon?  I think it's in Denver this year, April 7-9. Thinking of submitting a talk about how we use Spark and Cassandra. -Evan -- -- Evan Chan Staff Engineer ev@ooyala.com  | +1 for 0.10.0. It would give more time to study things (such as the new SparkConf) and let the community decide if any breaking API changes are needed. Also, a +1 for minor revisions not breaking code compatibility, including Scala versions.   (I guess this would mean that 1.x would stay on Scala 2.10.x) On Thu, Feb 6, 2014 at 11:05 AM, Sandy Ryza  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | The other reason for waiting are things like stability. It would be great to have as a goal for 1.0.0 that under most heavy use scenarios, workers and executors don't just die, which is not true today. Also, there should be minimal "silent failures" which are difficult to debug. On Thu, Feb 6, 2014 at 11:54 AM, Evan Chan  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | By the way, I did a benchmark on JSON parsing performance recently. Based on that, spray-json was about 10x slower than the Jackson-based parsers.  I recommend json4s-jackson, because jackson is almost certainly already a dependency of Sparks (many other Java libraries use it), so the dependencies are very lightweight.   I didn't benchmark Argonaut or play-json, partly because play-json pulled in all the Play dependencies, tho as someone else commented in this thread, they plan to split it out. On Mon, Feb 10, 2014 at 12:22 AM, Pascal Voitot Dev wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | +1 to the proposal. On Mon, Feb 10, 2014 at 2:56 PM, Michael Armbrust wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Any interest in adding Fast Serialization (or possibly replacing the default of Java Serialization)? https://code.google.com/p/fast-serialization/ -- -- Evan Chan Staff Engineer ev@ooyala.com  | Pascal, Ah, I stand corrected, thanks. On Mon, Feb 10, 2014 at 11:49 PM, Pascal Voitot Dev wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | The correct way to exclude dependencies in SBT is actually to declare a dependency as "provided".   I'm not familiar with Maven or its dependencySet, but provided will mark the entire dependency tree as excluded.   It is also possible to exclude jar by jar, but this is pretty error prone and messy. On Tue, Feb 25, 2014 at 2:45 PM, Koert Kuipers  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | The problem is that plugins are not equivalent.  There is AFAIK no equivalent to the maven shader plugin for SBT. There is an SBT plugin which can apparently read POM XML files (sbt-pom-reader).   However, it can't possibly handle plugins, which is still problematic. On Tue, Feb 25, 2014 at 3:31 PM, yao  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Patrick -- not sure I understand your request, do you mean - somehow creating a shaded jar (eg with maven shader plugin) - then including it in the spark jar (which would then be an assembly)? On Tue, Feb 25, 2014 at 4:01 PM, Patrick Wendell  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Hi Patrick, If you include shaded dependencies inside of the main Spark jar, such that it would have combined classes from all dependencies, wouldn't you end up with a sub-assembly jar?  It would be dangerous in that since it is a single unit, it would break normal packaging assumptions that the jar only contains its own classes, and maven/sbt/ivy/etc is used to resolve the remaining deps.... but maybe I don't know what you mean. The shader plugin in maven is apparently used to 1) build uber jars  - this is the part that sbt-assembly also does 2) "shade" existing jars, ie rename the classes and rewrite bytecode depending on them such that it doesn't conflict with other jars having the same classes  -- this is something sbt-assembly doesn't do, which you point out is done manually. On Tue, Feb 25, 2014 at 4:09 PM, Patrick Wendell  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Sandy, I believe the sbt-pom-reader plugin might work very well for this exact use case.   Otherwise, the SBT build file is just Scala code, so it can easily read the pom XML directly if needed and parse stuff out. On Tue, Feb 25, 2014 at 4:36 PM, Sandy Ryza  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | I'd like to propose the following way to move forward, based on the comments I've seen: 1.  Aggressively clean up the giant dependency graph.   One ticket I might work on if I have time is SPARK-681 which might remove the giant fastutil dependency (~15MB by itself). 2.  Take an intermediate step by having only ONE source of truth w.r.t. dependencies and versions.  This means either: a)  Using a maven POM as the spec for dependencies, Hadoop version, etc.   Then, use sbt-pom-reader to import it. b)  Using the build.scala as the spec, and "sbt make-pom" to generate the pom.xml for the dependencies The idea is to remove the pain and errors associated with manual translation of dependency specs from one system to another, while still maintaining the things which are hard to translate (plugins). On Wed, Feb 26, 2014 at 7:17 AM, Koert Kuipers  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Mark, No, I haven't tried this myself yet  :-p   Also I would expect that sbt-pom-reader does not do assemblies at all .... because that is an SBT plugin, so we would still need code to include sbt-assembly. There is also the trick question of how to include the assembly stuff into sbt-pom-reader generated projects.  So, needs much more investigation..... My hunch is that it's easier to generate the pom from SBT (make-pom) than the other way around. On Wed, Feb 26, 2014 at 10:54 AM, Mark Hamstra  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Can't maven pom's include other ones?  So what if we remove the artifact specs from the main pom, have them generated by sbt make-pom, and include the generated file in the main pom.xml?    I guess, just trying to figure out how much this would help (it seems at least it would remove the issue of maintaining and translating dependencies and exclusions).   If the burden of maintaining the plugins turns out to be the heavier commitment then maybe it's not worth it. On Wed, Feb 26, 2014 at 11:55 AM, Mark Hamstra  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Hey guys, There is no plan to move the Spark JIRA from the current https://spark-project.atlassian.net/ right? -- -- Evan Chan Staff Engineer ev@ooyala.com  | Ok, will continue to work with the existing one for now. On Fri, Feb 28, 2014 at 11:36 AM, Josh Rosen  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Hey guys, FYI, I just filed a new ticket, https://spark-project.atlassian.net/browse/SPARK-1154, concerning that Spark leaves tons of app-* folders on disk, which can quickly fill up the disk. Feel free to comment. I believe it is better for Spark to clean itself up; ie users should not need a cron job to clean up old folders. thanks, -Evan -- -- Evan Chan Staff Engineer ev@ooyala.com  | http://engineering.ooyala.com/blog/using-parquet-and-scrooge-spark Enjoy! By the way, I was not able to subscribe to the user-digest list for some reason.   The help email claims Similar addresses exist for the digest list: But if you send a email to the digest-subscribe it bounces back with a help email. -- -- Evan Chan Staff Engineer ev@ooyala.com  | I would love to hear the answer to this as well. On Thu, Mar 6, 2014 at 4:09 AM, Manoj Awasthi  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | I would love to hear the answer to this as well. On Thu, Mar 6, 2014 at 4:09 AM, Manoj Awasthi  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Hey guys, This is a follow-up to this semi-recent thread: http://apache-spark-developers-list.1001551.n3.nabble.com/0-9-0-forces-log4j-usage-td532.html 0.9.0 final is causing issues for us as well because we use Logback as our backend and Spark requires Log4j now. I see Patrick has a PR #560 to incubator-spark, was that merged in or left out? Also I see references to a new PR that might fix this, but I can't seem to find it in the github open PR page.   Anybody have a link? As a last resort we can switch to Log4j, but would rather not have to do that if possible. thanks, Evan -- -- Evan Chan Staff Engineer ev@ooyala.com  | +1. Not just for Typesafe Config, but if we want to consider hierarchical configs like JSON rather than flat key mappings, it is necessary.  It is also clearer. On Wed, Mar 12, 2014 at 9:58 AM, Aaron Davidson  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | +1. Not just for Typesafe Config, but if we want to consider hierarchical configs like JSON rather than flat key mappings, it is necessary.  It is also clearer. On Wed, Mar 12, 2014 at 9:58 AM, Aaron Davidson  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Jsuereth - Thanks for jumping on this thread.  There are a couple things which would help SBT IMHO (w.r.t. issues mentioned here): - A way of generating a project-wide pom.xml  (I believe make-pom only generates it for each sub-project) - and probably better or more feature-complete POMs, but the Maven folks here can speak to that - Make the sbt-pom-reader plugin officially part of SBT, and make it work well (again the Maven folks need to jump in here, though plugins can't be directly translated) - Have the sbt-assembly plugin officially maintained by Typesafe, or part of SBT....  most Maven folks expect not to have to include a plugin to generate a fat jar, and it's a pretty essential plugin for just about every SBT project - Also there is no equivalent (AFAIK) to the maven shader plugin. I also wish that the dependency-graph plugin was included by default, but that's just me  :) -Evan On Fri, Mar 14, 2014 at 6:47 AM, jsuereth  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Jsuereth - Thanks for jumping on this thread.  There are a couple things which would help SBT IMHO (w.r.t. issues mentioned here): - A way of generating a project-wide pom.xml  (I believe make-pom only generates it for each sub-project) - and probably better or more feature-complete POMs, but the Maven folks here can speak to that - Make the sbt-pom-reader plugin officially part of SBT, and make it work well (again the Maven folks need to jump in here, though plugins can't be directly translated) - Have the sbt-assembly plugin officially maintained by Typesafe, or part of SBT....  most Maven folks expect not to have to include a plugin to generate a fat jar, and it's a pretty essential plugin for just about every SBT project - Also there is no equivalent (AFAIK) to the maven shader plugin. I also wish that the dependency-graph plugin was included by default, but that's just me  :) -Evan On Fri, Mar 14, 2014 at 6:47 AM, jsuereth  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Dear Spark developers, Ooyala is happy to announce that we have pushed our official, Spark 0.9.0 / Scala 2.10-compatible, job server as a github repo: https://github.com/ooyala/spark-jobserver Complete with unit tests, deploy scripts, and examples. The original PR (#222) on incubator-spark is now closed. Please have a look; pull requests are very welcome. -- -- Evan Chan Staff Engineer ev@ooyala.com  | Matei, Maybe it's time to explore the spark-contrib idea again?   Should I start a JIRA ticket? -Evan On Tue, Mar 18, 2014 at 4:04 PM, Matei Zaharia  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | https://spark-project.atlassian.net/browse/SPARK-1283 On Wed, Mar 19, 2014 at 10:59 AM, Gerard Maas  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | The alternative is for Spark to not explicitly include hadoop_client, perhaps only as "provided", and provide a facility to insert the hadoop client jars of your choice at packaging time.   Unfortunately, hadoop_client pulls in a ton of other deps, so it's not as simple as copying one extra jar into dist/jars. On Mon, Mar 17, 2014 at 10:58 AM, Patrick Wendell  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | I also have a really minor fix for SPARK-1057  (upgrading fastutil), could that also make it in? -Evan On Sun, Mar 23, 2014 at 11:01 PM, Shivaram Venkataraman wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Hi Michael, Congrats, this is really neat! What thoughts do you have regarding adding indexing support and predicate pushdown to this SQL framework?    Right now we have custom bitmap indexing to speed up queries, so we're really curious as far as the architectural direction. -Evan On Fri, Mar 21, 2014 at 11:09 AM, Michael Armbrust wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | @Tathagata,  the PR is here: https://github.com/apache/spark/pull/215 On Mon, Mar 24, 2014 at 12:02 AM, Tathagata Das wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Suhas, here is the update, which I posted to SPARK-818: An update: we have put up the final job server here: https://github.com/ooyala/spark-jobserver The plan is to have a spark-contrib repo/github account and this would be one of the first projects. See SPARK-1283 for the ticket to track spark-contrib. On Sat, Mar 22, 2014 at 6:15 PM, Suhas Satish  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Patrick, yes, that is indeed a risk. On Mon, Mar 24, 2014 at 12:30 AM, Tathagata Das wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Suhas, You're welcome.  We are planning to speak about the job server at the Spark Summit by the way. -Evan On Mon, Mar 24, 2014 at 9:38 AM, Suhas Satish  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Hey guys, I think SPARK-1138 should be resolved before releasing Spark 0.9.1. It's affecting multiple users ability to use Spark 0.9 with various versions of Hadoop. I have one fix but not sure if it works for others. -Evan On Mon, Mar 24, 2014 at 5:30 PM, Tathagata Das wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | HI Michael, It's not publicly available right now, though we can probably chat about it offline.   It's not a super novel concept or anything, in fact I had proposed it a long time ago on the mailing lists. -Evan On Mon, Mar 24, 2014 at 1:34 PM, Michael Armbrust wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | +1. I really like this idea.  I know having a shepherd would have been really helpful for a couple of changes. On Thu, Mar 27, 2014 at 8:18 AM, Andy Konwinski  I thought this email exchange from the Mesos dev list was worth sharing. -- -- Evan Chan Staff Engineer ev@ooyala.com  | +1. I really like this idea.  I know having a shepherd would have been really helpful for a couple of changes. On Thu, Mar 27, 2014 at 8:18 AM, Andy Konwinski  I thought this email exchange from the Mesos dev list was worth sharing. -- -- Evan Chan Staff Engineer ev@ooyala.com  | Hey folks, We are in the middle of creating a Chef recipe for Spark.   As part of that we want to create a Debian package for Spark. What do folks think of adding the sbt-package-bin plugin to allow easy creation of a Spark .deb file?  I believe it adds all dependency jars into a single lib/ folder, so in some ways it's even easier to manage than the assembly. Also I'm not sure if there's an equivalent plugin for Maven. thanks, Evan -- -- Evan Chan Staff Engineer ev@ooyala.com  | Also, I understand this is the last week / merge window for 1.0, so if folks are interested I'd like to get in a PR quickly. thanks, Evan On Tue, Apr 1, 2014 at 11:24 AM, Evan Chan  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Mark - sorry, would you mind expanding what the "...." is? Something like mvn -Pdeb package ? I get: [ERROR] Plugin org.apache.maven.plugins:maven-compiler-plugin:3.1 or one of its dependencies could not be resolved: Failed to read artifact descriptor for org.apache.maven.plugins:maven-compiler-plugin:jar:3.1: Could not find artifact org.apache:apache:pom:13 -> [Help 1] On Tue, Apr 1, 2014 at 11:36 AM, Patrick Wendell  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | Lee, sorry, I actually meant exactly that, sbt-native-packager. On Tue, Apr 1, 2014 at 8:14 PM, Lee Mighdoll  wrote: -- -- Evan Chan Staff Engineer ev@ooyala.com  | https://github.com/apache/spark/pull/288 It's for fixing SPARK-1154, which would help Spark be a better citizen for most deploys, and should be really small and easy to review. thanks, Evan -- -- Evan Chan Staff Engineer ev@ooyala.com  | I doubt Titan would be able to give you traversal of billions of nodes in real-time either.   In-memory traversal is typically much faster than Cassandra-based tree traversal, even including in-memory caching. On Tue, Apr 8, 2014 at 1:23 PM, Nick Pentreath  GraphX, like Spark, will not typically be "real-time" (where by "real-time"-- -- Evan Chan Staff Engineer ev@ooyala.com  | Dear community, Wow, I remember when we first open sourced the job server, at the first Spark Summit in December.  Since then, more and more of you have started using it and contributing to it.   It is awesome to see! If you are not familiar with the spark job server, it is a REST API for managing your Spark jobs and job history and status. In order to make sure the project can continue to move forward independently, new features developed and contributions merged, we are moving the project to a new github organization.  The new location is: https://github.com/spark-jobserver/spark-jobserver The git commit history is still there, but unfortunately the pull requests don't migrate over.   I'll be contacting each of you with open PRs to move them over to the new location. Happy Hacking! Evan (@velvia) Kelvin (@kelvinchu) Daniel (@dan-null) --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I'm hoping to get in some doc enhancements and small bug fixes for Spark SQL. Also possibly a small new API to list the tables in sqlContext. Oh, and to get the doc page I had talked about before, a list of community Spark projects. thanks, Evan --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hey guys, What is the plan for getting Tachyon/off-heap support for the columnar compressed store?  It's not in 1.1 is it? In particular: - being able to set TACHYON as the caching mode - loading of hot columns or all columns - write-through of columnar store data to HDFS or backing store - being able to start a context and query directly from Tachyon's cached columnar data I think most of this was in Shark 0.9.1. Also, how likely is the wire format for the columnar compressed data to change?  That would be a problem for write-through or persistence. thanks, Evan --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org What would be the timeline for the parquet caching work? The reason I'm asking about the columnar compressed format is that there are some problems for which Parquet is not practical. On Mon, Aug 25, 2014 at 1:13 PM, Michael Armbrust wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Sure. - Organization or co has no Hadoop, but significant investment in some other NoSQL store. - Need to efficiently add a new column to existing data - Need to mark some existing rows as deleted or replace small bits of existing data For these use cases, it would be much more efficient and practical if we didn't have to take the origin of the data from the datastore, convert it to Parquet first.  Doing so loses significant latency and causes Ops headaches in having to maintain HDFS.     It would be great to be able to load data directly into the columnar format, into the InMemoryColumnarCache. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org On Sun, Aug 31, 2014 at 8:27 PM, Ian O'Connell  wrote: Something like that. I'd like, 1)  An API to produce a schema RDD from an RDD of columns, not rows. However, an RDD[Column] would not make sense, since it would be spread out across partitions.  Perhaps what is needed is a Seq[RDD[ColumnSegment]].    The idea is that each RDD would hold the segments for one column.  The segments represent a range of rows. This would then read from something like Vertica or Cassandra. 2)  A variant of 1) where you could read this data from Tachyon. Tachyon is supposed to support a columnar representation of data, it did for Shark 0.9.x. The goal is basically to load columnar data from something like Cassandra into Tachyon, with the compression ratio of columnar storage, and the speed of InMemoryColumnarTableScan.   If data is appended into the Tachyon representation, be able to cache it back. The write back is not as high a priority though. A workaround would be to read data from Cassandra/Vertica/etc. and write back into Parquet, but this would take a long time and incur huge I/O overhead. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org James, Michael at the meetup last night said there was some development activity around ORCFiles. I'm curious though, what are the pros and cons of ORCFiles vs Parquet? On Wed, Oct 8, 2014 at 10:03 AM, James Yu  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Ashwin, I would say the strategies in general are: 1) Have each user submit separate Spark app (each its own Spark Context), with its own resource settings, and share data through HDFS or something like Tachyon for speed. 2) Share a single spark context amongst multiple users, using fair scheduler.  This is sort of like having a Hadoop resource pool.    It has some obvious HA/SPOF issues, namely that if the context dies then every user using it is also dead.   Also, sharing RDDs in cached memory has the same resiliency problems, namely that if any executor dies then Spark must recompute / rebuild the RDD (it tries to only rebuild the missing part, but sometimes it must rebuild everything). Job server can help with 1 or 2, 2 in particular.  If you have any questions about job server, feel free to ask at the spark-jobserver google group.   I am the maintainer. -Evan On Thu, Oct 23, 2014 at 1:06 PM, Marcelo Vanzin  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hey guys, How does this impact the data sources API?  I was planning on using this for a project. +1 that many things from spark-sql / DataFrame is universally desirable and useful. By the way, one thing that prevents the columnar compression stuff in Spark SQL from being more useful is, at least from previous talks with Reynold and Michael et al., that the format was not designed for persistence. I have a new project that aims to change that.  It is a zero-serialisation, high performance binary vector library, designed from the outset to be a persistent storage friendly.  May be one day it can replace the Spark SQL columnar compression. Michael told me this would be a lot of work, and recreates parts of Parquet, but I think it's worth it.  LMK if you'd like more details. -Evan On Tue, Jan 27, 2015 at 4:35 PM, Reynold Xin  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I believe that most DataFrame implementations out there, like Pandas, supports the idea of missing values / NA, and some support the idea of Not Meaningful as well. Does Row support anything like that?  That is important for certain applications.  I thought that Row worked by being a mutable object, but haven't looked into the details in a while. -Evan On Wed, Jan 28, 2015 at 4:23 PM, Reynold Xin  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Yeah, it's "null".   I was worried you couldn't represent it in Row because of primitive types like Int (unless you box the Int, which would be a performance hit).  Anyways, I'll take another look at the Row API again  :-p On Wed, Jan 28, 2015 at 4:42 PM, Reynold Xin  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org +1.... having proper NA support is much cleaner than using null, at least the Java null. On Wed, Jan 28, 2015 at 6:10 PM, Evan R. Sparks  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org It is true that you can persist SchemaRdds / DataFrames to disk via Parquet, but a lot of time and inefficiencies is lost.   The in-memory columnar cached representation is completely different from the Parquet file format, and I believe there has to be a translation into a Row (because ultimately Spark SQL traverses Row's -- even the InMemoryColumnarTableScan has to then convert the columns into Rows for row-based processing).   On the other hand, traditional data frames process in a columnar fashion.   Columnar storage is good, but nowhere near as good as columnar processing. Another issue, which I don't know if it is solved yet, but it is difficult for Tachyon to efficiently cache Parquet files without understanding the file format itself. I gave a talk at last year's Spark Summit on this topic. I'm working on efforts to change this, however.  Shoot me an email at velvia at gmail if you're interested in joining forces. On Thu, Jan 29, 2015 at 1:59 PM, Cheng Lian  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Congrats everyone!!! On Tue, Feb 3, 2015 at 3:17 PM, Timothy Chen  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Why not just use SLF4J? On Tue, Feb 3, 2015 at 2:22 PM, Reynold Xin  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hey guys, Is there any guidance on what the different tracks for Spark Summit West mean?  There are some new ones, like "Third Party Apps", which seems like it would be similar to the "Use Cases".   Any further guidance would be great. thanks, Evan --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hey folks, I'd like to use local-cluster mode in my Spark-related projects to test Spark functionality in an automated way in a simulated local cluster.    The idea is to test multi-process things in a much easier fashion than setting up a real cluster.   However, getting this up and running in a separate project (I'm using Scala 2.10 and ScalaTest) is nontrivial.   Does anyone have any suggestions to get up and running? This is what I've observed so far (I'm testing against 1.5.1, but suspect this would apply equally to 1.6.x): - One needs to have a real Spark distro and point to it using SPARK_HOME - SPARK_SCALA_VERSION needs to be set - One needs to manually inject jar paths, otherwise dependencies are missing.  For example, build an assembly jar of all your deps.  Java class directory hierarchies don't seem to work with the setJars(...). How does Spark's internal scripts make it possible to run local-cluster mode and set up all the class paths correctly?   And, is it possible to mimic this setup for external Spark projects? thanks, Evan --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org What I want to find out is how to run tests like Spark's with local-cluster, just like that suite, but in your own projects.   Has anyone done this? On Sun, Apr 17, 2016 at 5:37 AM, Takeshi Yamamuro  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Jon,  Thanks.   I think I've figured it out, actually.   It's really simple, one needs to simply set spark.executor.extraClassPath to the current value of the java class path (java.class.path system property).   Also, to not use HiveContext, which gives errors about initializing a Derby database multiple times. On Sun, Apr 17, 2016 at 9:51 AM, Jon Maurer  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org +1000. Especially if the UI can help correlate exceptions, and we can reduce some exceptions. There are some exceptions which are in practice very common, such as the nasty ClassNotFoundException, that most folks end up spending tons of time debugging. On Mon, Apr 18, 2016 at 12:16 PM, Reynold Xin  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org