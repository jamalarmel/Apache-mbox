Hi, I've read the spark code style guide for contributors here: https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide For scala code, do you have a scalariform configuration that you use to format your code to these specs? Cheers, Michael Hello, I'm interested in reading/writing parquet SchemaRDDs that support the Parquet Decimal converted type. The first thing I did was update the Spark parquet dependency to version 1.5.0, as this version introduced support for decimals in parquet. However, conversion between the catalyst decimal type and the parquet decimal type is complicated by the fact that the catalyst type does not specify a decimal precision and scale but the parquet type requires them. I'm wondering if perhaps we could add an optional precision and scale to the catalyst decimal type? The catalyst decimal type would have unspecified precision and scale by default for backwards compatibility, but users who want to serialize a SchemaRDD with decimal(s) to parquet would have to narrow their decimal type(s) by specifying a precision and scale. Thoughts? Michael --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi Matei, Thanks, I can see you've been hard at work on this! I examined your patch and do have a question. It appears you're limiting the precision of decimals written to parquet to those that will fit in a long, yet you're writing the values as a parquet binary type. Why not write them using the int64 parquet type instead? Cheers, Michael On Oct 12, 2014, at 3:32 PM, Matei Zaharia  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi Matei, Another thing occurred to me. Will the binary format you're writing sort the data in numeric order? Or would the decimals have to be decoded for comparison? Cheers, Michael --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I'm implementing a custom ReceiverInputDStream and I'm not sure how to initialize the Receiver with the storage level. The storage level is set on the DStream, but there doesn't seem to be a way to pass it to the Receiver. At the same time, setting the storage level separately on the Receiver seems to introduce potential confusion as the storage level of the DStream can be set separately. Is this desired behavior---to have distinct DStream and Receiver storage levels? Perhaps I'm missing something? Also, the storageLevel property of the Receiver[T] class is undocumented. Cheers, Michael --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I've examined the experimental support for ExternalSorter in Spark SQL, and it does not appear that the external sorted is ever stopped (ExternalSorter.stop). According to the API documentation, this suggests a resource leak. Before I file a bug report in Jira, can someone familiar with the codebase confirm this is indeed a bug? Thanks, Michael --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hi Reynold, Our build is based off of the 1.3 branch as of commit 6d3c4d8b04b2738a821dfcc3df55a5635b89e506. Unless your fix is on another branch it appears this problem still exists. LMK if there's anything else I can help with. Thanks, Michael Hello, Do any Spark SQL committers/experts have bandwidth to review a PR I submitted a week ago, https://github.com/apache/spark/pull/13818 ? The associated Jira ticket is https://issues.apache.org/jira/browse/SPARK-15968 . Thank you! Michael I should briefly mention what the PR is about... This is a patch to address a problem where non-empty partitioned Hive metastore tables are never returned in a cache lookup in HiveMetastoreCatalog.getCached. Thanks, Michael Hello, I'm no longer able to successfully run `sbt unidoc` in branch-2.0, and the problem seems to stem from the addition of Kafka 0.10 support. If I remove either the Kafka 0.8 or 0.10 projects from the build then unidoc works. If I keep both in I get two dozen inexplicable compilation errors as part of the unidoc task execution. Here's the first few: [error] /Users/msa/workspace/spark-2.0/external/kafka-0-10/src/main/scala/org/apache/spark/streaming/kafka010/CachedKafkaConsumer.scala:50: value assign is not a member of org.apache.kafka.clients.consumer.KafkaConsumer[K,V] [error]     c.assign(tps) [error]       ^ [error] /Users/msa/workspace/spark-2.0/external/kafka-0-10/src/main/scala/org/apache/spark/streaming/kafka010/CachedKafkaConsumer.scala:95: too many arguments for method seek: (x$1: java.util.Map[org.apache.kafka.common.TopicPartition,Long])Unit [error]     consumer.seek(topicPartition, offset) [error]                  ^ [error] /Users/msa/workspace/spark-2.0/external/kafka-0-10/src/main/scala/org/apache/spark/streaming/kafka010/CachedKafkaConsumer.scala:100: value records is not a member of java.util.Map[String,org.apache.kafka.clients.consumer.ConsumerRecords[K,V]] [error]     val r = p.records(topicPartition) Running `sbt compile` completes without error. Has anyone else seen this behavior? Any ideas? This seems to be an issue around dependency management, but I'm otherwise stumped. Cheers, Michael --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org These topics have been included in the documentation for recent builds of Spark 2.0. Michael FYI if you just want to look at the source code, there are source jars for those binary versions in maven central. I was just looking at the metastore source code last night. Michael Hi Adam, Do you have your spark confs and your spark-env.sh somewhere where we can see them? If not, can you make them available? Cheers, Michael Hello, I've seen a few messages on the mailing list regarding Spark performance concerns, especially regressions from previous versions. It got me thinking that perhaps an automated performance regression suite would be a worthwhile contribution? Is anyone working on this? Do we have a Jira issue for it? I cannot commit to taking charge of such a project. I just thought it would be a great contribution for someone who does have the time and the chops to build it. Cheers, Michael --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Hi Adam, From our experience we've found the default Spark 2.0 configuration to be highly suboptimal. I don't know if this affects your benchmarks, but I would consider running some tests with tuned and alternate configurations. Michael Here are some settings we use for some very large GraphX jobs. These are based on using EC2 c3.8xl workers: .set("spark.sql.shuffle.partitions", "1024") .set("spark.sql.tungsten.enabled", "true") .set("spark.executor.memory", "24g") .set("spark.kryoserializer.buffer.max","1g") .set("spark.sql.codegen.wholeStage", "true") .set("spark.memory.offHeap.enabled", "true") .set("spark.memory.offHeap.size", "25769803776") // 24 GB Some of these are in fact default configurations. Some are not. Michael Marcin, I'm not sure what you're referring to. Can you be more specific? Cheers, Michael FYI, I posted this to user@ and have followed up with a bug report: https://issues.apache.org/jira/browse/SPARK-17204  Begin forwarded message: I've replied on the issue's page, but in a word, "yes". See https://issues.apache.org/jira/browse/SPARK-17204  On Aug 23, 2016, at 11:55 AM, Reynold Xin  wrote: FYI, I've updated the issue's description to include a very simple program which reproduces the issue for me. Cheers, Michael Hi Dayne, Have a look at https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark . I think you'll find answers to most of your questions there. Cheers, Michael Hi All, Is anyone working on updating Spark's Parquet library dep to 1.9? If not, I can at least get started on it and publish a PR. Cheers, Michael --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org Sounds great. Regarding the min/max stats issue, is that an issue with the way the files are written or read? What's the Parquet project issue for that bug? What's the 1.9.1 release timeline look like? I will aim to have a PR in by the end of the week. I feel strongly that either this or https://github.com/apache/spark/pull/15538  needs to make it into 2.1. The logging output issue is really bad. I would probably call it a blocker. Michael Hello, I'm running into an issue with a Spark app I'm building, which depends on a library which depends on Jackson 2.8, which fails at runtime because Spark brings in Jackson 2.6. I'm looking for a solution. As a workaround, I've patched our build of Spark to use Jackson 2.8. That's working, however given all the trouble associated with attempting a Jackson upgrade in the past (see https://issues.apache.org/jira/browse/SPARK-14989  and https://github.com/apache/spark/pull/13417 ), I'm wondering if I should submit a PR for that. Is shading Spark's Jackson deps another option? Any other suggestions for an acceptable way to fix this incompatibility with apps using a newer version of Jackson? FWIW, Jackson claims to support backward compatibility within minor releases (https://github.com/FasterXML/jackson-docs#on-jackson-versioning ). So in theory, apps that depend on an upgraded Spark version should work even if they ask for an older version. Cheers, Michael Hello, When I try to read from a Hive table created by Spark 2.1 in Spark 2.0 or earlier, I get an error: java.lang.ClassNotFoundException: Failed to load class for data source: hive. Is there a way to get previous versions of Spark to read tables written with Spark 2.1? Cheers, Michael --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org This is not an issue with all tables created in Spark 2.1, though I'm not sure why some work and some do not. I have found that a table created as such sql("create table test stored as parquet as select 1") in Spark 2.1 cannot be read in previous versions of Spark. Michael --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org I believe https://github.com/apache/spark/pull/16122  needs to be included in Spark 2.1. It's a simple bug fix to some functionality that is introduced in 2.1. Unfortunately, it's been manually verified only. There's no unit test that covers it, and building one is far from trivial. Michael Hi Raju, I'm sorry this isn't working for you. I helped author this functionality and will try my best to help. First, I'm curious why you set spark.sql.hive.convertMetastoreParquet to false? Can you link specifically to the jira issue or spark pr you referred to? The first thing I would try is setting spark.sql.hive.convertMetastoreParquet to true. Setting that to false might also explain why you're getting parquet decode errors. If you're writing your table data with Spark's parquet file writer and reading with Hive's parquet file reader, there may be an incompatibility accounting for the decode errors you're seeing. Can you reply with your table's Hive metastore schema, including partition schema? Where are the table's files located? If you do a "show partitions " in the spark-sql shell, does it show the partitions you expect to see? If not, run "msck repair table  On Jan 17, 2017, at 12:02 AM, Raju Bairishetti  wrote: What is the physical query plan after you set spark.sql.hive.convertMetastoreParquet to true? Michael Can you paste the actual query plan here, please? What version of Spark are you running? I think I understand. Partition pruning for the case where spark.sql.hive.convertMetastoreParquet is true was not added to Spark until 2.1.0. I think that in previous versions it only worked when spark.sql.hive.convertMetastoreParquet is false. Unfortunately, that configuration gives you data decoding errors. If it's possible for you to write all of your data with Hive, then you should be able to read it without decoding errors and with partition pruning turned on. Another possibility is running your Spark app with a very large maximum heap configuration, like 8g or even 16g. However, loading all of that partition metadata can be quite slow for very large tables. I'm sorry I can't think of a better solution for you. Michael Based on what you've described, I think you should be able to use Spark's parquet reader plus partition pruning in 2.1. Personally I'd love to see some kind of pluggability, configurability in the JSON schema parsing, maybe as an option in the DataFrameReader. Perhaps you can propose an API? Hi Stan, What OS/version are you using? Michael That's understandable. Maybe I can help. :) What happens if you set `HIVE_TABLE_NAME = "default.employees"`? Also, does that table exist before you call `filtered_output_timestamp.write.mode("append").saveAsTable(HIVE_TABLE_NAME)`? Cheers, Michael Hi Sumit, Can you use http://spark.apache.org/docs/latest/api/python/pyspark.html?highlight=rdd#pyspark.RDD.mapPartitionsWithIndex  to solve your problem? Michael Hi Guys, Can someone help move https://github.com/apache/spark/pull/16499  along in the review process? This PR fixes replicated off-heap storage. Thanks! Michael Hi Yuhao, BigDL looks very promising and it's a framework we're considering using. It seems the general approach to high performance DL is via GPUs. Your project mentions performance on a Xeon comparable to that of a GPU, but where does this claim come from? Can you provide benchmarks? Thanks, Michael