Maybe we could try LZ4 [1], which has better performance and smaller footprint than LZF and Snappy. In fast scan mode, the performance is 1.5 - 2x higher than LZF[2], but memory used is 10x smaller than LZF (16k vs 190k). [1] https://github.com/jpountz/lz4-java [2] http://ning.github.io/jvm-compressor-benchmark/results/calgary/roundtrip-2013-06-06/index.html On Mon, Jul 14, 2014 at 12:01 AM, Reynold Xin  wrote: On Wed, Aug 13, 2014 at 2:16 PM, Ignacio Zendejas wrote: A quick comparison by word count on 4.3G text file (local mode), Spark:  40 seconds PySpark: 2 minutes and 16 seconds So PySpark is 3.4x slower than Spark. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org On Wed, Aug 13, 2014 at 2:31 PM, Davies Liu  wrote: I also tried DPark, which is a pure Python clone of Spark: DPark: 53 seconds so it's 2 times faster than PySpark, because of it does not have the over head of passing data between JVM and Python. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org -1 (not binding, +1 for maintainer, -1 for sign off) Agree with Greg and Vinod. In the beginning, everything is better (more efficient, more focus), but after some time, fighting begins. Code style is the most hot topic to fight (we already saw it in some PRs). If two committers (one of them is maintainer) have not got a agreement on code style, before this process, they will ask comments from other committers, but after this process, the maintainer have higher priority to -1, then maintainer will keep his/her personal preference, it's hard to make a agreement. Finally, different components will have different code style (or others). Right now, maintainers are kind of first contact or best contacts, the best person to review the PR in that component. We could announce it, then new contributors can easily find the right one to review. My 2 cents. Davies On Thu, Nov 6, 2014 at 11:43 PM, Vinod Kumar Vavilapalli wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Sorry for my last email, I misunderstood the proposal here, all the committer still have equal -1 to all the code changes. Also, as mentioned in the proposal, the sign off only happens to public API and architect, something like discussion about code style things are still the same. So, I'd revert my vote to +1. Sorry for this. Davies On Fri, Nov 7, 2014 at 3:18 PM, Davies Liu  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org This should be fixed in 1.2, could you try it? On Mon, Dec 29, 2014 at 8:04 PM, guoxu1231  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hey Meethu, The Java API accepts only Vector, so you should convert the numpy array into pyspark.mllib.linalg.DenseVector. BTW, which class are you using? the KMeansModel.predict() accept numpy.array, it will do the conversion for you. Davies On Fri, Jan 9, 2015 at 4:45 AM, Meethu Mathew  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Could you post a piece of code here? On Sun, Jan 11, 2015 at 9:28 PM, Meethu Mathew  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org On Sun, Jan 11, 2015 at 10:21 PM, Meethu Mathew wrote: What's the Java API looks like? all the arguments of findPredict should be converted into java objects, so what should `mu` be converted to? --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org On Mon, Jan 12, 2015 at 8:14 PM, Meethu Mathew  wrote: I see, we should remove the special case for list and tuple, pickle should work more reliably for them. I had tried to remove it, it did not break any tests. Could you do it in your PR or I create a PR for it separately? --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org It's not necessary, I will create a PR to remove them. For larger dict/list/tuple, the pickle approach may have less RPC calls, better performance. Davies On Tue, Jan 13, 2015 at 4:53 AM, Meethu Mathew  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hey, Without having Python as fast as Scala/Java, I think it's impossible to similar performance in PySpark as in Scala/Java. Jython is also much slower than Scala/Java. With Jython, we can avoid the cost of manage multiple process and RPC, we may still need to do the data conversion between Java and Python. Given that fact that Jython is not widely used in production, it may introduce more troubles than the performance gain. Spark jobs can be easily speed up by scaling out (by adding more resources). I think the most advantage of PySpark is that it let you do fast prototype. Once you got your ETL finalized, it's not that hard to translate your pure Python jobs into Scala to reduce the cost(it's optional). Now days, engineer time is much more expensive than CPU time, I think we should be more focus on the former. That's my 2 cents. Davies On Thu, Jan 29, 2015 at 12:45 PM, rtshadow wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org The CallbackServer is part of Py4j, it's only used in driver, not used in slaves or workers. On Wed, Feb 11, 2015 at 12:32 AM, Todd Gao wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Yes. On Wed, Feb 11, 2015 at 5:44 PM, Todd Gao  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org see https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools On Fri, Mar 27, 2015 at 10:02 AM, Stephen Boesch  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I usually just open a terminal to do `build/sbt ~compile`, coding in IntelliJ, then run python tests in another terminal once it compiled successfully. On Fri, Mar 27, 2015 at 10:11 AM, Reynold Xin  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org put these lines in your ~/.bash_profile export SPARK_PREPEND_CLASSES=true export SPARK_HOME=path_to_spark export PYTHONPATH="${SPARK_HOME}/python/lib/py4j-0.8.2.1-src.zip:${SPARK_HOME}/python:${PYTHONPATH}"$ source ~/.bash_profile $ build/sbt assembly $ build/sbt ~compile  # do not stop this Then in another terminal you could run python tests as $ cd python/pyspark/ $  python rdd.py cc to dev list On Fri, Mar 27, 2015 at 10:15 AM, Stephen Boesch  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org On Fri, Mar 27, 2015 at 4:16 PM, Stephen Boesch  wrote: No, iPython shell is statefull, it will have unexpected behavior when you reload the library. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org The PR for integrate SparkR into Spark may help: https://github.com/apache/spark/pull/5096 -- Davies Liu Sent with Sparrow (http://www.sparrowmailapp.com/?sig) On Wednesday, March 25, 2015 at 7:35 PM, danilo2 wrote: What's the format you have in json file? On Fri, Apr 10, 2015 at 6:57 PM, Suraj Shetiya  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hey Suraj, You should use "date" for DataType: df.withColumn(df.DateCol.cast("date")) Davies On Sat, Apr 11, 2015 at 10:57 PM, Suraj Shetiya  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Hey Shane, Have you updated all the jenkins slaves? There is a run with old configurations (no Python 3, with 130 minutes timeout), see https://amplab.cs.berkeley.edu/jenkins/job/NewSparkPullRequestBuilder/666/consoleFull Davies On Thu, Apr 9, 2015 at 10:18 AM, shane knapp  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org It does not work now, could you file a jira for it? On Wed, Apr 15, 2015 at 9:29 AM, Suraj Shetiya  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org toDF() is first introduced in Scala and Python (because createDataFrame is too long), is used in lots places, I think it's useful. On Fri, May 8, 2015 at 11:03 AM, Shivaram Venkataraman wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org There is a module called 'types' in python 3: davies@localhost:~/work/spark$ python3 Python 3.4.1 (v3.4.1:c0e311e010fc, May 18 2014, 00:54:21) [GCC 4.2.1 (Apple Inc. build 5666) (dot 3)] on darwin Type "help", "copyright", "credits" or "license" for more information. <module 'types' from Without renaming, our `types.py` will conflict with it when you run unittests in pyspark/sql/ . On Tue, May 26, 2015 at 11:57 AM, Justin Uang  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org When you run the test in python/pyspark/sql/ by bin/spark-submit python/pyspark/sql/dataframe.py the the current directory is the first item in sys.path, sql/types.py will have higher priority then python3.4/types.py, the tests will fail. On Tue, May 26, 2015 at 12:08 PM, Justin Uang  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I think relative imports can not help in this case. When you run scripts in pyspark/sql, it doesn't know anything about pyspark.sql, it just see types.py as a separate module. On Tue, May 26, 2015 at 12:44 PM, Punyashloka Biswal wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Thanks for looking into it, I'd like the idea of having ForkingIterator. If we have unlimited buffer in it, then will not have the problem of deadlock, I think. The writing thread will be blocked by Python process, so there will be not much rows be buffered(still be a reason to OOM). At least, this approach is better than current one. Could you create a JIRA and sending out the PR? On Tue, Jun 23, 2015 at 3:27 PM, Justin Uang  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Fare points, I also like simpler solutions. The overhead of Python task could be a few of milliseconds, which means we also should eval them as batches (one Python task per batch). Decreasing the batch size for UDF sounds reasonable to me, together with other tricks to reduce the data in socket/pipe buffer. BTW, what do your UDF looks like? How about to use Jython to run simple Python UDF (without some external libraries). On Tue, Jun 23, 2015 at 8:21 PM, Justin Uang  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org batch size as 1, right? On Wed, Jun 24, 2015 at 12:11 PM, Justin Uang  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I'm thinking that the batched synchronous version will be too slow (with small batch size) or easy to OOM with large (batch size). If it's not that hard, you can give it a try. On Wed, Jun 24, 2015 at 4:39 PM, Justin Uang  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org We finally fix this in 1.5 (next release), see https://github.com/apache/spark/pull/7301 On Sat, Jul 11, 2015 at 10:32 PM, Jerry Lam  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Thanks for reporting this, I'm working on it. It turned out that it's a bug in when run with Python3.4, will sending out a fix soon. On Sun, Jul 12, 2015 at 1:33 PM, Cheolsoo Park  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Will be fixed by https://github.com/apache/spark/pull/7363 On Sun, Jul 12, 2015 at 7:45 PM, Davies Liu  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org +1, built 1.5 from source and ran TPC-DS locally and clusters, ran performance benchmark for aggregation and join with difference scales, all worked well. On Thu, Sep 3, 2015 at 10:05 AM, Michael Armbrust wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org I tried with Python 2.7/3.4 and Spark 1.4.1/1.5-RC3, they all work as expected: ``` +-----+---------+ |label| featuers| +-----+---------+ |  1.0|    [1.0]| |  0.0|(1,[],[])| +-----+---------+ ['label', 'featuers'] ``` On Tue, Sep 8, 2015 at 1:45 AM, Prabeesh K.  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org On Tue, Sep 15, 2015 at 1:46 PM, Renyi Xiong  wrote: There are two kind of callback in pyspark streaming: 1) one operate on RDDs, it take an RDD and return an new RDD, uses py4j callback, because SparkContext and RDDs are not accessible in worker.py 2) operate on records of RDD, it take an record and return new records, uses worker.py Yes. --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Could you tell us a way to reproduce this failure? Reading from JSON or Parquet? On Mon, Oct 5, 2015 at 4:28 AM, Eugene Morozov wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Can you reproduce it on master? I can't reproduce it with the following code: ShuffledHashJoin [id#21], [id#19], BuildRight TungstenExchange hashpartitioning(id#21,200) TungstenProject [concat(A,cast(id#20L as string)) AS id#21] Scan PhysicalRDD[id#20L] TungstenExchange hashpartitioning(id#19,200) TungstenProject [concat(A,cast(id#18L as string)) AS id#19] Scan PhysicalRDD[id#18L] 10 On Mon, Oct 19, 2015 at 2:59 AM, gsvic  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org Does this https://github.com/apache/spark/pull/10134 is valid fix? (still worse than 1.5) On Thu, Dec 3, 2015 at 8:45 AM, mkhaitman  wrote: --------------------------------------------------------------------- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For additional commands, e-mail: dev-help@spark.apache.org +1 On Thu, Oct 27, 2016 at 12:18 AM, Reynold Xin  wrote: --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org +1 for Matei's point. On Thu, Oct 27, 2016 at 8:36 AM, Matei Zaharia  wrote: --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org +1 On Wed, Nov 2, 2016 at 5:40 PM, Reynold Xin  wrote: --------------------------------------------------------------------- To unsubscribe e-mail: dev-unsubscribe@spark.apache.org