Done -- have fun! :) On Mon, Oct 14, 2013 at 12:50 AM, karthik tunga  Hi, Looks like the appId thing was removed accidentally quite a while ago: https://github.com/apache/incubator-spark/commit/85a35c68401e171df0b72b172a689d8c4e412199 and has gone unnoticed since. I have no objections to adding it back in... On Tue, Nov 5, 2013 at 2:36 PM, Reynold Xin  wrote: By the way, there are a few places one can look for logs while testing: Unit test runner logs (should contain driver and worker logs): core/target/unit-tests.log Executor logs: work/app-* This should help find the root exception when you see one caught by the DAGScheduler, such as in this case. On Tue, Nov 12, 2013 at 6:21 PM, Kyle Ellrott  wrote: Hey Evan, Sorry about the issue with the Mesos urls. I submitted the patch that broke it originally -- at the time, I did not know about the zk:// urls mapping to Mesos. I went ahead and started PR #217 to correct this, which also includes a unit test for creating the schedulers to hopefully avoid this type of issue in the future (at least at this most basic level). I cannot speak for #2, though, which seems to be the more serious issue. On Wed, Nov 27, 2013 at 10:01 PM, Evan Chan  wrote: Hi guys, just wanted to share a little plugin I wrote for IntelliJ to help auto-organize Scala imports. Anyone who has submitted a patch to Spark has probably felt the exhilaration of manually sorting and bucketing your imports. Well, now you can let your IDE have some fun! It's in the plugin repository, so you can download it from within IntelliJ. are some Matei-approvedTM rules for Spark: import scala.language.* import java.* import scala.* import * import org.apache.spark.* To actually organize imports, just press Ctrl+Shift+O by default (I think If you find a bug or have a suggestion, please let me know. The IntelliJ plugin page is here (feel free to vote!) and the Github project is here . Also be sure to have a great day! I'd be fine with one-way mirrors here (Apache threads being reflected in Google groups) -- I have no idea how one is supposed to navigate the Apache list to look for historic threads. On Thu, Dec 19, 2013 at 7:58 PM, Mike Potts  wrote: I'm an idiot, but which part of the DAGScheduler is recursive here? Seems like processEvent shouldn't have inherently recursive properties. On Sat, Jan 25, 2014 at 9:57 PM, Reynold Xin  wrote: +1 On Jan 26, 2014 2:11 PM, "Christopher Nguyen"  wrote: Looks like your MyCustomKeyType.equals() method doesn't correctly handle a null argument. In general, the contract of equals is to return false if called with a null argument, which this code currently relies on. This could still be patched for the sake of backwards compatibility with similarly incomplete equals() methods which previously worked. On Mon, Jan 27, 2014 at 11:34 AM, Archit Thakur ERROR executor.Executor: Exception in task ID 20 The code would not call null.equals(b); only a.equals(null) is allowable (we check for k.eq(null) at the top). a.equals(null) returning false is part of the Object.equals() contract, so it is not invalid to use it, it just may be unintuitive if you expect never to have null references of MyCustomKeyType hanging around. On Mon, Jan 27, 2014 at 12:12 PM, Archit Thakur On Tue, Jan 28, 2014 at 1:25 AM, Aaron Davidson  wrote: Also, it may be intentional but the only PR-level comments seem to get forwarded. Comments on code are not sent. On Fri, Feb 7, 2014 at 12:31 PM, Shivaram Venkataraman  wrote: There are a few bits of the Scala style that are underspecified by both the Scala style guide  and our own supplemental notes. Often, this leads to inconsistent formatting within the codebase, so I'd like to propose some general guidelines which we can add to the wiki and use in the future: 1) Line-wrapped method return type is indented with two spaces: def longMethodName(... long param list ...) : Long = {2 } *Justification: *I think this is the most commonly used style in Spark today. It's also similar to the "extends" style used in classes, with the same justification: it is visually distinguished from the 4-indented parameter list. 2) URLs and code examples in comments should not be line-wrapped. Hereis an example of the latter. *Justification*: Line-wrapping can cause confusion when trying to copy-paste a URL or command. Can additionally cause IDE issues or, avoidably, Javadoc issues. Any thoughts on these, or additional style issues not explicitly covered in either the Scala style guide or Spark wiki? Shivaram, is your recommendation to wrap the parameter list even if it fits, but just the return type doesn't? Personally, I think the cost of moving from a single-line parameter list to an n-ine list is pretty high, as it takes up a lot more space. I am even in favor of allowing a parameter list to overflow into a second line (but not a third) instead of spreading them out, if it's a private helper method (where the parameters are probably not as important as the implementation, unlike a public API). On Mon, Feb 10, 2014 at 1:42 PM, Shivaram Venkataraman  wrote: Alright, makes sense -- consistency is more important than special casing for possible readability benefits. That is one of the main points behind a style guide after all. I switch my vote for (1) to Shivaram's proposal as well. On Mon, Feb 10, 2014 at 4:40 PM, Evan Chan  wrote: +1 On Mon, Feb 10, 2014 at 8:58 PM, Reynold Xin  wrote: I am not sure I fully understand this reasoning. I imagine that lift-json is only one of hundreds of packages that would have to be built if you wanted to build all of Spark's transitive dependencies from source. Additionally, to make sure I understand the impact -- this is only intended to simplify the process of packaging Spark on a new OS distribution that disallows pulling in binaries? On Sun, Feb 9, 2014 at 4:50 PM, Will Benton  wrote: My apologies, my intention was that it is fine for the next minor *or*major release, regardless of what comes next. I only wanted to distinguish that from the next maintenance release, since my understanding is that we wish to avoid changing dependencies during maintenance releases. Will, thanks for the clarifications. I think Spark's main use-case is "warm, small inputs" right now, but the change seems reasonable to me nevertheless. Paul, do you know if there are any issues relevant to Spark that we need from 2.3.2? We would also have to wait for json4s to release a new version that depends on 2.3.2, or else pull it in ourselves. On Wed, Feb 12, 2014 at 9:47 AM, Paul Brown  wrote: The version of json4s we're using (3.2.6 in the 2.10 branch) does seem to depend on Jackson 2.3.0 and Scala 2.10.0: http://mvnrepository.com/artifact/org.json4s/json4s-jackson_2.10/3.2.6 On Wed, Feb 12, 2014 at 11:29 AM, Paul Brown  wrote: Regarding styling: as we all know, constructor parameters in Scala are automatically upgraded to "private val" fields if they're referenced outside of the constructor. For instance: class Foo(a: Int, b: Int) {def getB = b } In the above case, 'b' is actually a "private val" field of Foo, whereas 'a' never left the constructor's stack. Is this usage kosher, or should we prefer that such fields are marked "private val" explicitly if they're intended to be used outside of the constructor? This behavior is often harmless, but it has some evil implications with regards to serialization and shadowing during inheritance. It's especially concerning when a field starts out as a constructor parameter and during a later patch becomes a field, and now we're serializing it. On Mon, Feb 10, 2014 at 9:00 PM, Aaron Davidson  wrote: Thanks for the clarification, Mark. So "private val" generates the Scala getter whereas "private[this] val" does not, and the field-ified constructor parameter mimics "private[this] val". However, the distinction between those two seems less important than the distinction between a constructor parameter and a field. In particular, the existence of the Scala private getter won't affect serialization and makes the shadowing explicit. Other than some weird uses of reflection, I'm not sure how the difference could cause an issue. On Wed, Feb 12, 2014 at 2:02 PM, Mark Hamstra  It's actually a little more complicated than that, mostly due to the I believe the performance implications are negligible due to the JIT. If that getter is not inlined at runtime, I will eat a shoe. I think the main question is still whether we want to avoid fields secretly springing into existence or we want the significantly more concise syntax of unannotated parameters. If we do want the former, then "private" versus "private[this]" is the next question. On Wed, Feb 12, 2014 at 2:34 PM, Mark Hamstra  There is at least potential for performance difference with the extra level This is due, unfortunately, to Apache policies that all development-related discussion should take place on the dev list. As we are attempting to graduate from an incubating project to an Apache top level project, there were some concerns raised about GitHub, and the fastest solution to avoid conflict related to our graduation was to CC dev@ for all GitHub messages. Once our graduation is complete, we may be able to find a less noisy way of dealing with these messages. In the meantime, one simple solution is to filter out all messages that come from git@git.apache.org and are destined to dev@spark.incubator.apache.org. On Tue, Feb 18, 2014 at 10:04 AM, Gerard Maas  wrote: One slight concern regarding primitive types -- in particular, Ints and Longs can have semantic differences when it comes to overflow, so it's often good to know what type of variable you're returning. Perhaps it is sufficient to say that Int is the "default" numeric type, and that other types should be specified explicitly. On Wed, Feb 19, 2014 at 8:37 AM, Patrick Wendell  wrote: I don't have an official policy to point you to, but Chris Mattmann (our Apache project mentor) summarized some of the points in this thread, and here is the original concern that caused us to make this change: http://mail-archives.apache.org/mod_mbox/incubator-general/201402.mbox/%3CCAAS6=7hkCiT093nXVMcUus8Z-5XCDn=cQ5trjN_Kz9ARe9H=RA@mail.gmail.com%3E On Fri, Feb 21, 2014 at 8:08 AM, Ethan Jewett  wrote: I think #2 sounds totally reasonable (and low-noise). I think we've been pretty good in the past about making sure dev discussion has happened on the dev@ mailing list, but there were definitely exceptions simply because not everyone may have realized that this was the policy. One mechanism which may help this is to create an alias on GitHub called sparkdev or something so we can explicitly CC dev@ from GitHub, if the discussion becomes relevant. On Sat, Feb 22, 2014 at 11:34 AM, Mattmann, Chris A (3980)  wrote: By the way, we still need to get our JIRAs migrated over to the Apache system. Unrelated, just... saying. On Mon, Feb 24, 2014 at 10:55 PM, Matei Zaharia  This is probably a snafu because we had a GitHub hook that was sending By the way, we still need to get our JIRAs migrated over to the Apache system. Unrelated, just... saying. On Mon, Feb 24, 2014 at 10:55 PM, Matei Zaharia  This is probably a snafu because we had a GitHub hook that was sending Should we try to deprecate these types of configs for 1.0.0? We can start by accepting both and giving a warning if you use the old one, and then actually remove them in the next minor release. I think "spark.speculation.enabled=true" is better than "spark.speculation=true", and if we decide to use typesafe configs again ourselves, this change is necessary. We actually don't have to ever complete the deprecation - we can always accept both spark.speculation and spark.speculation.enabled, and people just have to use the latter if they want to use typesafe config. On Wed, Mar 12, 2014 at 9:24 AM, Mark Hamstra  That's the whole reason why some of the intended configuration changes One solution for typesafe config is to use "spark.speculation" = true Typesafe will recognize the key as a string rather than a path, so the name will actually be "\"spark.speculation\"", so you need to handle this contingency when passing the config operations to spark (stripping the quotes from the key). Solving this in Spark itself is a little tricky because there are ~5 such conflicts (spark.serializer, spark.speculation, spark.locality.wait, spark.shuffle.spill, and spark.cleaner.ttl), some of which are used pretty frequently. We could provide aliases for all of these in Spark, but actually deprecating the old ones would affect many users, so we could only do that if enough users would benefit from fully hierarchical config options. On Wed, Mar 12, 2014 at 9:24 AM, Mark Hamstra  That's the whole reason why some of the intended configuration changes Matei's link seems to point to a specific starter project as part of the starter list, but here is the list itself: https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20labels%20%3D%20Starter%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened) On Mon, Apr 7, 2014 at 10:22 PM, Matei Zaharia  I'd suggest looking for the issues labeled "Starter" on JIRA. You can find This may have something to do with running the tests on a Mac, as there is a lot of File/URI/URL stuff going on in that test which may just have happened to work if run on a Linux system (like Jenkins). Note that this suite was added relatively recently: https://github.com/apache/spark/pull/217 On Mon, Apr 14, 2014 at 12:04 PM, Ye Xianjin  wrote: By all means, it would be greatly appreciated! On Mon, Apr 14, 2014 at 10:34 PM, Ye Xianjin  wrote: It was, but due to the apache infra issues, some may not have received the email yet... On Fri, May 16, 2014 at 10:48 AM, Henry Saputra  Hi Patrick, No. Only 3 of the responses. On Fri, May 16, 2014 at 10:38 AM, Nishkam Ravi  wrote: In Spark 0.9.0 and 0.9.1, we stopped using the FileSystem cache correctly, and we just recently resumed using it in 1.0 (and in 0.9.2) when this issue was fixed: https://issues.apache.org/jira/browse/SPARK-1676 Prior to this fix, each Spark task created and cached its own FileSystems due to a bug in how the FS cache handles UGIs. The big problem that arose was that these FileSystems were never closed, so they just kept piling up. There were two solutions we considered, with the following effects: (1) Share the FS cache among all tasks and (2) Each task effectively gets its own FS cache, and closes all of its FSes after the task completes. We chose solution (1) for 3 reasons: - It does not rely on the behavior of a bug in HDFS. - It is the most performant option. - It is most consistent with the semantics of the (albeit broken) FS cache. Since this behavior was changed in 1.0, it could be considered a regression. We should consider the exact behavior we want out of the FS cache. For Spark's purposes, it seems fine to cache FileSystems across tasks, as Spark does not close FileSystems. The issue that comes up is that user code which uses FileSystem.get() but then closes the FileSystem can screw up Spark processes which were using that FileSystem. The workaround for users would be to use FileSystem.newInstance() if they want full control over the lifecycle of their FileSystems. On Thu, May 22, 2014 at 12:06 PM, Colin McCabe  The FileSystem cache is something that has caused a lot of pain over the There's some discussion here as well on just using the Scala REPL for 2.11: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-on-Scala-2-11-td6506.html#a6523 Matei's response mentions the features we needed to change from the Scala REPL (class-based wrappers and where to output the generated classes), which were added as options to the 2.11 REPL, so we may be able to trim down a bunch when 2.11 becomes standard. On Fri, May 30, 2014 at 4:16 AM, Kan Zhang  wrote: I don't know of any way to avoid Akka doing a copy, but I would like to mention that it's on the priority list to piggy-back only the map statuses relevant to a particular map task on the task itself, thus reducing the total amount of data sent over the wire by a factor of N for N physical machines in your cluster. Ideally we would also avoid Akka entirely when sending the tasks, as these can get somewhat large and Akka doesn't work well with large messages. Do note that your solution of using broadcast to send the map tasks is very similar to how the executor returns the result of a task when it's too big for akka. We were thinking of refactoring this too, as using the block manager has much higher latency than a direct TCP send. On Mon, Jun 30, 2014 at 12:13 PM, Mridul Muralidharan  Our current hack is to use Broadcast variables when serialized Can you post the logs from any of the dying executors? On Tue, Jul 1, 2014 at 1:25 AM, qingyang li  i am using mesos0.19 and spark0.9.0 ,  the mesos cluster is started, when I Either Serializable works, scala Serializable extends Java's (originally intended a common interface for people who didn't want to run Scala on a JVM). Class fields require the class be serialized along with the object to access. If you declared "val n" inside a method's scope instead, though, we wouldn't need the class. E.g.: class TextToWordVector(csvData:RDD[Array[String]]) {def computeX() = {val n = 1 } lazy val x = computeX() } Note that if the class itself doesn't actually contain many (large) fields, though, it may not be an issue to actually transfer it around. On Thu, Jul 3, 2014 at 5:21 AM, Ulanov, Alexander  Thanks, this works both with Scala and Java Serializable. Which one should The issue you're seeing is not the same as the one you linked to -- your serialized task sizes are very small, and Mesos fine-grained mode doesn't use Akka anyway. The error log you printed seems to be from some sort of Mesos logs, but do you happen to have the logs from the actual executors themselves? These should be Spark logs which hopefully show the actual Exception (or lack thereof) before the executors die. The tasks are dying very quickly, so this is probably either related to your application logic throwing some sort of fatal JVM error or due to your Mesos setup. I'm not sure if that "Failed to fetch URIs for container" is fatal or not. On Wed, Jul 2, 2014 at 2:44 AM, qingyang li  executor always been removed. Tachyon should only be marginally less performant than memory_only, because we mmap the data from Tachyon's ramdisk. We do not have to, say, transfer the data over a pipe from Tachyon; we can directly read from the buffers in the same way that Shark reads from its in-memory columnar format. On Tue, Jul 8, 2014 at 1:18 AM, qingyang li  hi, when i create a table, i can point the cache strategy using Shark's in-memory format is already serialized (it's compressed and column-based). On Tue, Jul 8, 2014 at 9:50 AM, Mridul Muralidharan  You are ignoring serde costs :-) Agreed that the behavior of the Master killing off an Application when Executors from the same set of nodes repeatedly die is silly. This can also strike if a single node enters a state where any Executor created on it quickly dies (e.g., a block device becomes faulty). This prevents the Application from launching despite only one node being bad. On Wed, Jul 9, 2014 at 3:08 PM, Mark Hamstra  Actually, I'm thinking about re-purposing it.  There's a nasty behavior The full jstack would still be useful, but our current working theory is that this is due to the fact that Configuration#loadDefaults goes through every Configuration object that was ever created (via Configuration.REGISTRY) and locks it, thus introducing a dependency from new Configuration to old, otherwise unrelated, Configuration objects that our locking did not anticipate. I have created https://github.com/apache/spark/pull/1409 to hopefully fix this bug. On Mon, Jul 14, 2014 at 2:44 PM, Patrick Wendell  wrote: Out of curiosity, what problems are you seeing with Utils.getCallSite? On Mon, Jul 14, 2014 at 2:59 PM, Will Benton  wrote: The patch won't solve the problem where two people try to add a configuration option at the same time, but I think there is currently an issue where two people can try to initialize the Configuration at the same time and still run into a ConcurrentModificationException. This at least reduces (slightly) the scope of the exception although eliminating it may not be possible. On Mon, Jul 14, 2014 at 4:35 PM, Gary Malouf  wrote: One of the core problems here is the number of open streams we have, which is (# cores * # reduce partitions), which can easily climb into the tens of thousands for large jobs. This is a more general problem that we are planning on fixing for our largest shuffles, as even moderate buffer sizes can explode to use huge amounts of memory at that scale. On Mon, Jul 14, 2014 at 4:53 PM, Jon Hartlaub  wrote: Would you mind filing a JIRA for this? That does sound like something bogus happening on the JVM/YourKit level, but this sort of diagnosis is sufficiently important that we should be resilient against it. On Mon, Jul 14, 2014 at 6:01 PM, Will Benton  wrote: Whoops, I was mistaken in my original post last year. By default, there is one executor per node per Spark Context, as you said. "spark.executor.memory" is the amount of memory that the application requests for each of its executors. SPARK_WORKER_MEMORY is the amount of memory a Spark Worker is willing to allocate in executors. So if you were to set SPARK_WORKER_MEMORY to 8g everywhere on your cluster, and spark.executor.memory to 4g, you would be able to run 2 simultaneous Spark Contexts who get 4g per node. Similarly, if spark.executor.memory were 8g, you could only run 1 Spark Context at a time on the cluster, but it would get all the cluster's memory. On Thu, Jul 24, 2014 at 7:25 AM, Martin Goodson  Thank you Nishkam, This may be related: https://github.com/Parquet/parquet-mr/issues/211 Perhaps if we change our configuration settings for Parquet it would get better, but the performance characteristics of Snappy are pretty bad here under some circumstances. On Tue, Sep 23, 2014 at 10:13 AM, Cody Koeninger  wrote: I think I've seen something like +2 = "strong LGTM" and +1 = "weak LGTM; someone else should review" before. It's nice to have a shortcut which isn't a sentence when talking about weaker forms of LGTM. On Sat, Jan 17, 2015 at 6:59 PM,  wrote: For the specific question of supplementing Standalone Mode with a custom leader election protocol, this was actually already committed in master and will be available in Spark 1.3: https://github.com/apache/spark/pull/771/files You can specify spark.deploy.recoveryMode = "CUSTOM"and spark.deploy.recoveryMode.factory to a class which implements StandaloneRecoveryModeFactory. See the current implementations of FileSystemRecoveryModeFactory and ZooKeeperRecoveryModeFactory. I will update the JIRA you linked to be more current. On Sat, Jan 31, 2015 at 12:55 AM, Anjana Fernando  Hi everyone, You might be seeing the result of this patch: https://github.com/apache/spark/commit/d069c5d9d2f6ce06389ca2ddf0b3ae4db72c5797 which was introduced in 1.1.1. This patch disabled the ability for take() to run without launching a Spark job, which means that the latency is significantly increased for small jobs (but not for large ones). You can try enabling local execution and seeing if your problem goes away. On Wed, Feb 18, 2015 at 5:10 PM, Matt Cheah  wrote: That's kinda annoying, but it's just a little extra boilerplate. Can you call it as StorageLevel.DiskOnly() from Java? Would it also work if they were case classes with empty constructors, without the field? On Wed, Mar 4, 2015 at 11:35 PM, Xiangrui Meng  wrote: Perhaps the problem with Java enums that was brought up was actually that their hashCode is not stable across JVMs, as it depends on the memory location of the enum itself. On Mon, Mar 9, 2015 at 6:15 PM, Imran Rashid  wrote: It's unrelated to the proposal, but Enum#ordinal() should be much faster, assuming it's not serialized to JVMs with different versions of the enum :) On Mon, Mar 16, 2015 at 12:12 PM, Kevin Markey  In some applications, I have rather heavy use of Java enums which are Out of curiosity, why could we not use Netty's SslHandler injected into the TransportContext pipeline? On Mon, Mar 16, 2015 at 7:56 PM, turp1twin  wrote: The only issue I knew of with Java enums was that it does not appear in the Scala documentation. On Mon, Mar 23, 2015 at 1:46 PM, Sean Owen  wrote: Should we mention that you should synchronize on HadoopRDD.CONFIGURATION_INSTANTIATION_LOCK to avoid a possible race condition in cloning Hadoop Configuration objects prior to Hadoop 2.7.0? :) On Wed, Mar 25, 2015 at 7:16 PM, Patrick Wendell  wrote: A second advantage is that it allows individual Executors to go into GC pause (or even crash) and still allow other Executors to read shuffle data and make progress, which tends to improve stability of memory-intensive jobs. On Thu, Jun 25, 2015 at 11:42 PM, Sandy Ryza  Hi Yash, +1 On Tue, Dec 22, 2015 at 7:01 PM, Josh Rosen  +1