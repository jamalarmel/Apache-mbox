From dev-return-7186-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  1 00:22:00 2014
Return-Path: <dev-return-7186-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CB287106FD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Apr 2014 00:22:00 +0000 (UTC)
Received: (qmail 67859 invoked by uid 500); 1 Apr 2014 00:22:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67793 invoked by uid 500); 1 Apr 2014 00:21:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67783 invoked by uid 99); 1 Apr 2014 00:21:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Apr 2014 00:21:59 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.180] (HELO mail-pd0-f180.google.com) (209.85.192.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Apr 2014 00:21:55 +0000
Received: by mail-pd0-f180.google.com with SMTP id v10so8668690pde.25
        for <dev@spark.apache.org>; Mon, 31 Mar 2014 17:21:34 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=KpJEgnPuOfcbTTXBiv1zTHpTDeZxK5ylI0nqaWe08qc=;
        b=EzZGnzZ3XEd2kpnQ7Q/7zr/Prv+z/gSCfjoKvd8XzZlAKpWico4rvqwudXOTOJ1W74
         +gJTo26sUBmK1XsG9NXkCTqaXjKiEQrwJEZP0VkEwXj+eQDCOzpbkoD8mIvqIr8/LB/G
         Q772Cx0C8DGJp/AQKNOmY1b8HEm6bNOfpDNxLgRPfkq4ZQN4egbOuvFbhbhO4T4PXbym
         fUCYPLVU1IKIGn7QYbaTpjk1FohyojG+ytN0TbaTvRwgmQFyc3FpOJiMksmepdeng9Ww
         wVOO27m5S2RJpoO16DRGPtd7fM9s+lArYQ3FCp4bjlyWdCSCTCk5rn560sDQRVq6wBvk
         rPJQ==
X-Gm-Message-State: ALoCoQlywkfH421r6dPTYqfDWtjtj1rcqX+txGs8h4uCnaae4nV09qKjx1k7tYh8FdcxOBctxeK1
MIME-Version: 1.0
X-Received: by 10.66.218.234 with SMTP id pj10mr22405pac.156.1396311694595;
 Mon, 31 Mar 2014 17:21:34 -0700 (PDT)
Received: by 10.70.72.201 with HTTP; Mon, 31 Mar 2014 17:21:34 -0700 (PDT)
In-Reply-To: <5339FB60.1020106@oracle.com>
References: <CAMwrk0mhqhBf2ZNRFiC4LgWjLb2A7fw3nCo8TwAZvadNinWmaA@mail.gmail.com>
	<E75FBF52-0373-44E2-BCC2-BCB8445C995C@gmail.com>
	<CAPdPcW2bN3yTLgFRaf=5Sm05mJQ71ufeMv8dfmEsEQLsEPsHGg@mail.gmail.com>
	<CABPQxssGF-rOYE_oFiR2CArbMt-auvTBUT4+Yck79+Wk4dzYcA@mail.gmail.com>
	<1396278778.50575.YahooMailNeo@web140101.mail.bf1.yahoo.com>
	<CABPQxss_myw8XYRE1CKHw8SgTqx=3+Eb+2-OzzHi4iV5m9wqPw@mail.gmail.com>
	<CAMwrk0mVrYGnon0JQvMpFUz7SPS_ZcxG53t95M2F_GowTBP9UQ@mail.gmail.com>
	<5339FB60.1020106@oracle.com>
Date: Mon, 31 Mar 2014 17:21:34 -0700
Message-ID: <CAMJOb8kAx5Apxf5G4FmdRFRVM=K3d9hOJypSbZTt93UdZ3s32g@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 0.9.1 (RC3)
From: Andrew Or <andrew@databricks.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b5dba948e1d9204f5f02612
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b5dba948e1d9204f5f02612
Content-Type: text/plain; charset=ISO-8859-1

+1 tested on OSX


On Mon, Mar 31, 2014 at 4:33 PM, Kevin Markey <kevin.markey@oracle.com>wrote:

> I had specifically requested that the ASM shading be included in the RC,
> hence my testing focused on that, but I ran other tests as well.  Tested
> with a build of our project, running one of our applications from that
> build in yarn-standalone on a pseudocluster, and successfully redeploying
> and bringing up a web app that is integrated with Spark.  It is the latter
> where most ASM conflicts have typically occurred.  Successful build and
> passed both tests. So, my vote:
>
> +1
>
> One test which I'd like to run but can't because of unrelated library
> conflicts would have been to remove various ASM exclusions from other
> libraries, recompiling and redeploying.  But I'd incur the wrath of the
> rest of my team doing that, especially after a full day of tracking down
> yet another (totally unrelated) library conflict.
>
> Thanks for this maintenance release.
>
> Kevin Markey
>
>
>
> On 03/31/2014 12:32 PM, Tathagata Das wrote:
>
>> Yes, lets extend the vote for two more days from now. So the vote is open
>> till *Wednesday, April 02, at 20:00 UTC*
>>
>> On that note, my +1
>>
>> TD
>>
>>
>>
>>
>> On Mon, Mar 31, 2014 at 9:57 AM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>>  Yeah good point. Let's just extend this vote another few days?
>>>
>>>
>>> On Mon, Mar 31, 2014 at 8:12 AM, Tom Graves <tgraves_cs@yahoo.com>
>>> wrote:
>>>
>>>  I should probably pull this off into another thread, but going forward
>>>>
>>> can
>>>
>>>> we try to not have the release votes end on a weekend? Since we only
>>>> seem
>>>> to give 3 days, it makes it really hard for anyone who is offline for
>>>> the
>>>> weekend to try it out.   Either that or extend the voting for more then
>>>> 3
>>>> days.
>>>>
>>>> Tom
>>>> On Monday, March 31, 2014 12:50 AM, Patrick Wendell <pwendell@gmail.com
>>>> >
>>>> wrote:
>>>>
>>>> TD - I downloaded and did some local testing. Looks good to me!
>>>>
>>>> +1
>>>>
>>>> You should cast your own vote - at that point it's enough to pass.
>>>>
>>>> - Patrick
>>>>
>>>>
>>>>
>>>> On Sun, Mar 30, 2014 at 9:47 PM, prabeesh k <prabsmails@gmail.com>
>>>>
>>> wrote:
>>>
>>>> +1
>>>>> tested on Ubuntu12.04 64bit
>>>>>
>>>>>
>>>>> On Mon, Mar 31, 2014 at 3:56 AM, Matei Zaharia <
>>>>>
>>>> matei.zaharia@gmail.com
>>>
>>>> wrote:
>>>>>> +1 tested on Mac OS X.
>>>>>>
>>>>>> Matei
>>>>>>
>>>>>> On Mar 27, 2014, at 1:32 AM, Tathagata Das <
>>>>>>
>>>>> tathagata.das1565@gmail.com>
>>>>
>>>>> wrote:
>>>>>>
>>>>>>  Please vote on releasing the following candidate as Apache Spark
>>>>>>>
>>>>>> version
>>>>>
>>>>>> 0.9.1
>>>>>>
>>>>>>> A draft of the release notes along with the CHANGES.txt file is
>>>>>>> attached to this e-mail.
>>>>>>>
>>>>>>> The tag to be voted on is v0.9.1-rc3 (commit 4c43182b):
>>>>>>>
>>>>>>>  https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=
>>> 4c43182b6d1b0b7717423f386c0214fe93073208
>>>
>>>> The release files, including signatures, digests, etc. can be found
>>>>>>>
>>>>>> at:
>>>>
>>>>> http://people.apache.org/~tdas/spark-0.9.1-rc3/
>>>>>>>
>>>>>>> Release artifacts are signed with the following key:
>>>>>>> https://people.apache.org/keys/committer/tdas.asc
>>>>>>>
>>>>>>> The staging repository for this release can be found at:
>>>>>>>
>>>>>>>  https://repository.apache.org/content/repositories/
>>> orgapachespark-1009/
>>>
>>>> The documentation corresponding to this release can be found at:
>>>>>>> http://people.apache.org/~tdas/spark-0.9.1-rc3-docs/
>>>>>>>
>>>>>>> Please vote on releasing this package as Apache Spark 0.9.1!
>>>>>>>
>>>>>>> The vote is open until Sunday, March 30, at 10:00 UTC and passes if
>>>>>>> a majority of at least 3 +1 PMC votes are cast.
>>>>>>>
>>>>>>> [ ] +1 Release this package as Apache Spark 0.9.1
>>>>>>> [ ] -1 Do not release this package because ...
>>>>>>>
>>>>>>> To learn more about Apache Spark, please see
>>>>>>> http://spark.apache.org/
>>>>>>> <CHANGES.txt><RELEASE_NOTES.txt>
>>>>>>>
>>>>>>
>>>>>>
>

--047d7b5dba948e1d9204f5f02612--

From dev-return-7187-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  1 14:29:04 2014
Return-Path: <dev-return-7187-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1B58D10333
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Apr 2014 14:29:04 +0000 (UTC)
Received: (qmail 36860 invoked by uid 500); 1 Apr 2014 14:29:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36253 invoked by uid 500); 1 Apr 2014 14:28:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36218 invoked by uid 99); 1 Apr 2014 14:28:57 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Apr 2014 14:28:57 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [72.30.239.77] (HELO nm34-vm5.bullet.mail.bf1.yahoo.com) (72.30.239.77)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Apr 2014 14:28:51 +0000
Received: from [98.139.212.151] by nm34.bullet.mail.bf1.yahoo.com with NNFMP; 01 Apr 2014 14:28:29 -0000
Received: from [98.139.212.205] by tm8.bullet.mail.bf1.yahoo.com with NNFMP; 01 Apr 2014 14:28:29 -0000
Received: from [127.0.0.1] by omp1014.mail.bf1.yahoo.com with NNFMP; 01 Apr 2014 14:28:29 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 371989.92460.bm@omp1014.mail.bf1.yahoo.com
Received: (qmail 63247 invoked by uid 60001); 1 Apr 2014 14:28:29 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.com; s=s1024; t=1396362509; bh=IcxRk1Og9m1cr84fTTd6sNUw8XfVT9Nz1PZGU/70SX4=; h=X-YMail-OSG:Received:X-Rocket-MIMEInfo:X-Mailer:References:Message-ID:Date:From:Reply-To:Subject:To:In-Reply-To:MIME-Version:Content-Type; b=YOpu4PVb7HK6osCnXq++MW1C/sb4bC8tUXJe0hJxmFyfboDdIr3QDkV4c0d3L3ihUQuz0D4o1YNC3Z2jkZe9W6l25gqrWI51tgPKt6xOGu20HKozrcHuHGeFtloaO3sa1MNR9r/MPlduoitbwORsIGNb9jutHp6B+Fy7SJ9Vetk=
DomainKey-Signature:a=rsa-sha1; q=dns; c=nofws;
  s=s1024; d=yahoo.com;
  h=X-YMail-OSG:Received:X-Rocket-MIMEInfo:X-Mailer:References:Message-ID:Date:From:Reply-To:Subject:To:In-Reply-To:MIME-Version:Content-Type;
  b=UW4nFa2h0YL3XfvxtUiAPX0ROhR0MHKwzTDHvMtPv5E3JpecZKut+fEgD7OeKBNzbKdot+LjNUjfzIP/qFT9SKzOVKZGQgotcCN0WWBkxYEdEBxyNIaY0zxitJLoFJf9RsH3YDfD9BqZvj/ILqJBtaftajhKVA7U0DAkn8e5oGk=;
X-YMail-OSG: a1M5pocVM1msuwYRnM.r2dfxbH02g4JwE5HmSNl39jFbhHl
 YcuewExjGHyU1k.Podts.BCYWjk9mY6nAHC.x7motbs98MVrINL8gioQ5sKY
 eQ0yHaKZkMYfOsD.aK5AYaukYQVnY2SgWhLZglkd6Oc9k0M.nYpsYl2Yxm8Q
 2I8qDmvruDfF6mUcSBKBi9h_aMOLNRTTtHDIh9qbbqokZMaZkTEXIAX4pDsI
 s7CCovqWlCc5ykhU6Ghlr_ZsquGa7LfeNrLjfPbfjMMpGE4ZPFYPH1H.H5NC
 u02t9DWj9yNKv.QjcH4VhAFzaHLVEDTJapouXuI3mX5xubsl.ysDt_gjf88B
 RoT21wEHFYkqe2pKQXC8IpXgYkPDuyY0nEPa.zKBzo_MNa9k1.CZe2jwYm8D
 vc0xA8RuMIj8NLFKpHZFS7M_ZjByNLy9_o2GQ6TJyCqavWay5OkBwPRIzED9
 1haxYSDDq0ztVH_b6Lr9YVAa1UmZ.GmaWft2dcw0kjoheK9Z8HMbPMP192IN
 lM4DoQIPZX60HTxy83qWH2gq7ZAT0QPz6gQeFrglXHdbe95Rz7knVel1NRxn
 eSzvc.dVR4Ah4NNGTmeVbfLPZkD10rc_Pp0aofzzhFgR5ijJMwHqREVFLcpu
 eHR_3xALJDr1irH4Spshq5gJHPPwEpnijxHnAESxfHSxYqt1QyVp245bYTHT
 iGUn9jndc9bqNDy742AzTth_PZUGuHjTGw.0zFfUa5G01wCdlEl1WwiMgpHC
 k9A--
Received: from [204.11.79.50] by web140102.mail.bf1.yahoo.com via HTTP; Tue, 01 Apr 2014 07:28:28 PDT
X-Rocket-MIMEInfo: 002.001,VGhhbmtzIGZvciBleHRlbmRpbmcgdGhlIHZvdGluZy4KClVuZm9ydHVuYXRlbHkgSSd2ZSBmb3VuZCBhbiBpc3N1ZSB3aXRoIHRoZSBzcGFyay1zaGVsbCBpbiB5YXJuLWNsaWVudCBtb2RlLiDCoEl0IGRvZXNuJ3Qgd29yayB3aXRoIHNlY3VyZSBIREZTIHVubGVzcyB5b3XCoApleHBvcnQgU1BBUktfWUFSTl9NT0RFPXRydWUgYmVmb3JlIHN0YXJ0aW5nIHRoZSBzaGVsbCwgb3IgaWYgeW91IGhhcHBlbiB0byBkbyBzb21ldGhpbmcgaW1tZWRpYXRlbHkgd2l0aCBIREZTLiDCoElmIHlvdSB3YWl0IGZvciB0aGUBMAEBAQE-
X-Mailer: YahooMailWebService/0.8.182.648
References: <CAMwrk0mhqhBf2ZNRFiC4LgWjLb2A7fw3nCo8TwAZvadNinWmaA@mail.gmail.com> <E75FBF52-0373-44E2-BCC2-BCB8445C995C@gmail.com> <CAPdPcW2bN3yTLgFRaf=5Sm05mJQ71ufeMv8dfmEsEQLsEPsHGg@mail.gmail.com> <CABPQxssGF-rOYE_oFiR2CArbMt-auvTBUT4+Yck79+Wk4dzYcA@mail.gmail.com> <1396278778.50575.YahooMailNeo@web140101.mail.bf1.yahoo.com> <CABPQxss_myw8XYRE1CKHw8SgTqx=3+Eb+2-OzzHi4iV5m9wqPw@mail.gmail.com> <CAMwrk0mVrYGnon0JQvMpFUz7SPS_ZcxG53t95M2F_GowTBP9UQ@mail.gmail.com>
Message-ID: <1396362508.20704.YahooMailNeo@web140102.mail.bf1.yahoo.com>
Date: Tue, 1 Apr 2014 07:28:28 -0700 (PDT)
From: Tom Graves <tgraves_cs@yahoo.com>
Reply-To: Tom Graves <tgraves_cs@yahoo.com>
Subject: Re: [VOTE] Release Apache Spark 0.9.1 (RC3)
To: Tathagata Das <tathagata.das1565@gmail.com>,
  "dev@spark.apache.org" <dev@spark.apache.org>
In-Reply-To: <CAMwrk0mVrYGnon0JQvMpFUz7SPS_ZcxG53t95M2F_GowTBP9UQ@mail.gmail.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="-559291896-1516150972-1396362508=:20704"
X-Virus-Checked: Checked by ClamAV on apache.org

---559291896-1516150972-1396362508=:20704
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

Thanks for extending the voting.=0A=0AUnfortunately I've found an issue wit=
h the spark-shell in yarn-client mode. =A0It doesn't work with secure HDFS =
unless you=A0=0Aexport SPARK_YARN_MODE=3Dtrue before starting the shell, or=
 if you happen to do something immediately with HDFS. =A0If you wait for th=
e connection to the namenode to timeout it will fail.=A0=0A=0AI think it wa=
s actually this way in the 0.9 release also so I thought I would send this =
and get peoples feedback to see if you want it fixed?=A0=0A=0AAnother optio=
n would be to document that you have to export SPARK_YARN_MODE=3Dtrue for t=
he shell. =A0 The fix actually went in with the authentication changes I ma=
de in master but I never realized that change needed to apply to 0.9.=A0=0A=
=0Ahttps://github.com/apache/spark/commit/7edbea41b43e0dc11a2de156be220db8b=
7952d01#diff-0ae5b834ce90ec37c19af35aa7a5e1a0=0A=0ASee the SparkILoop diff.=
=0A=0ATom=0A=0A=0AOn Monday, March 31, 2014 1:33 PM, Tathagata Das <tathaga=
ta.das1565@gmail.com> wrote:=0A =0AYes, lets extend the vote for two more d=
ays from now. So the vote is open till=A0Wednesday, April 02, at 20:00 UTC=
=0A=0AOn that note, my +1=0A=0A=0ATD=0A=0A=0A=0A=0A=0A=0AOn Mon, Mar 31, 20=
14 at 9:57 AM, Patrick Wendell <pwendell@gmail.com> wrote:=0A=0AYeah good p=
oint. Let's just extend this vote another few days?=0A>=0A>=0A>=0A>On Mon, =
Mar 31, 2014 at 8:12 AM, Tom Graves <tgraves_cs@yahoo.com> wrote:=0A>=0A>> =
I should probably pull this off into another thread, but going forward can=
=0A>> we try to not have the release votes end on a weekend? Since we only =
seem=0A>> to give 3 days, it makes it really hard for anyone who is offline=
 for the=0A>> weekend to try it out. =A0 Either that or extend the voting f=
or more then 3=0A>> days.=0A>>=0A>> Tom=0A>> On Monday, March 31, 2014 12:5=
0 AM, Patrick Wendell <pwendell@gmail.com>=0A>> wrote:=0A>>=0A>> TD - I dow=
nloaded and did some local testing. Looks good to me!=0A>>=0A>> +1=0A>>=0A>=
> You should cast your own vote - at that point it's enough to pass.=0A>>=
=0A>> - Patrick=0A>>=0A>>=0A>>=0A>> On Sun, Mar 30, 2014 at 9:47 PM, prabee=
sh k <prabsmails@gmail.com> wrote:=0A>>=0A>> > +1=0A>> > tested on Ubuntu12=
.04 64bit=0A>> >=0A>> >=0A>> > On Mon, Mar 31, 2014 at 3:56 AM, Matei Zahar=
ia <matei.zaharia@gmail.com=0A>> > >wrote:=0A>> >=0A>> > > +1 tested on Mac=
 OS X.=0A>> > >=0A>> > > Matei=0A>> > >=0A>> > > On Mar 27, 2014, at 1:32 A=
M, Tathagata Das <=0A>> tathagata.das1565@gmail.com>=0A>> > > wrote:=0A>> >=
 >=0A>> > > > Please vote on releasing the following candidate as Apache Sp=
ark=0A>> > version=0A>> > > 0.9.1=0A>> > > >=0A>> > > > A draft of the rele=
ase notes along with the CHANGES.txt file is=0A>> > > > attached to this e-=
mail.=0A>> > > >=0A>> > > > The tag to be voted on is v0.9.1-rc3 (commit 4c=
43182b):=0A>> > > >=0A>> > >=0A>> >=0A>> https://git-wip-us.apache.org/repo=
s/asf?p=3Dspark.git;a=3Dcommit;h=3D4c43182b6d1b0b7717423f386c0214fe93073208=
=0A>> > > >=0A>> > > > The release files, including signatures, digests, et=
c. can be found=0A>> at:=0A>> > > > http://people.apache.org/~tdas/spark-0.=
9.1-rc3/=0A>> > > >=0A>> > > > Release artifacts are signed with the follow=
ing key:=0A>> > > > https://people.apache.org/keys/committer/tdas.asc=0A>> =
> > >=0A>> > > > The staging repository for this release can be found at:=
=0A>> > > >=0A>> > https://repository.apache.org/content/repositories/orgap=
achespark-1009/=0A>> > > >=0A>> > > > The documentation corresponding to th=
is release can be found at:=0A>> > > > http://people.apache.org/~tdas/spark=
-0.9.1-rc3-docs/=0A>> > > >=0A>> > > > Please vote on releasing this packag=
e as Apache Spark 0.9.1!=0A>> > > >=0A>> > > > The vote is open until Sunda=
y, March 30, at 10:00 UTC and passes if=0A>> > > > a majority of at least 3=
 +1 PMC votes are cast.=0A>> > > >=0A>> > > > [ ] +1 Release this package a=
s Apache Spark 0.9.1=0A>> > > > [ ] -1 Do not release this package because =
...=0A>> > > >=0A>> > > > To learn more about Apache Spark, please see=0A>>=
 > > > http://spark.apache.org/=0A>> > > > <CHANGES.txt><RELEASE_NOTES.txt>=
=0A>> > >=0A>> > >=0A>> >=0A>>=0A>
---559291896-1516150972-1396362508=:20704--

From dev-return-7188-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  1 18:24:52 2014
Return-Path: <dev-return-7188-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 18F9010EC7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Apr 2014 18:24:52 +0000 (UTC)
Received: (qmail 83022 invoked by uid 500); 1 Apr 2014 18:24:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82730 invoked by uid 500); 1 Apr 2014 18:24:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82717 invoked by uid 99); 1 Apr 2014 18:24:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Apr 2014 18:24:48 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_IMAGE_ONLY_24,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ev@ooyala.com designates 209.85.213.41 as permitted sender)
Received: from [209.85.213.41] (HELO mail-yh0-f41.google.com) (209.85.213.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Apr 2014 18:24:44 +0000
Received: by mail-yh0-f41.google.com with SMTP id v1so9441580yhn.14
        for <dev@spark.apache.org>; Tue, 01 Apr 2014 11:24:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=ooyala.com; s=google;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=EsJcDsasDy2M14J1aICX4W6DZhRJnqbZZ7RkU+KQ8Q4=;
        b=IeIpHuFXN8yMKT6aUFdEeCHnWNYJvGG9c8aU8lxdG/KthmzNtuedBmVVjFLugzO0aH
         vjY0ZMzEX8u5Wa/r2axnLBjgji3opfAuKauto/r6rH3FXuoAftpAW+Co3XlqRG10jHST
         FwzzvYADpWw7tbJzwZVjoUBFHNhtiHX2HYVCM=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=EsJcDsasDy2M14J1aICX4W6DZhRJnqbZZ7RkU+KQ8Q4=;
        b=PdN/aUsGPsZ8bSrX8fNjr6rBoP4M/E9eVCjy/vtug2kYH3s3bs7XMtq2pH+nVKn7Yd
         JPNSWcQcQZ+MfUpDugnd80AwqMKo56Kt72fDKywS02wVnFDfkkHcIovFNEICjm7dQyzx
         yWtF0kUsR8nl8YzrmLLbkioJh6FSJLVWUVn45AWq0u5q2M2YYwGEFIxzwGSQD+4k4Bsw
         8agv9EzDtSd8+vMvP07wJd0NpgDfHgle2PeCIzxT/Zv4iUzG3o8lQibucwdBHMsa4MjS
         B1Bl5aEx7GfonZ+tpWoVfy2bROtyJRg55wmgS0q2PtOV5hrh+U498t1iwMlz+UhqzH/6
         86bQ==
X-Gm-Message-State: ALoCoQmaGhzB4Tw/OWEEg751cOTnmgWDiDW04RwwhDo2QsDSq5/7XL3v1POr+InaRBH6Fb8sxyJn
MIME-Version: 1.0
X-Received: by 10.236.19.99 with SMTP id m63mr5269402yhm.134.1396376663563;
 Tue, 01 Apr 2014 11:24:23 -0700 (PDT)
Received: by 10.170.120.87 with HTTP; Tue, 1 Apr 2014 11:24:23 -0700 (PDT)
Date: Tue, 1 Apr 2014 11:24:23 -0700
Message-ID: <CADWPM3gZCfn3K8saPiPVMzd4S6=84D+BWVybTm5vR-m7WGCzBQ@mail.gmail.com>
Subject: sbt-package-bin
From: Evan Chan <ev@ooyala.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01634dd401317804f5ff478c
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01634dd401317804f5ff478c
Content-Type: text/plain; charset=ISO-8859-1

Hey folks,

We are in the middle of creating a Chef recipe for Spark.   As part of that
we want to create a Debian package for Spark.

What do folks think of adding the sbt-package-bin plugin to allow easy
creation of a Spark .deb file?  I believe it adds all dependency jars into
a single lib/ folder, so in some ways it's even easier to manage than the
assembly.

Also I'm not sure if there's an equivalent plugin for Maven.

thanks,
Evan


-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>

--089e01634dd401317804f5ff478c--

From dev-return-7189-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  1 18:25:50 2014
Return-Path: <dev-return-7189-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0B4C010ECE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Apr 2014 18:25:50 +0000 (UTC)
Received: (qmail 83957 invoked by uid 500); 1 Apr 2014 18:25:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83876 invoked by uid 500); 1 Apr 2014 18:25:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83612 invoked by uid 99); 1 Apr 2014 18:25:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Apr 2014 18:25:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ev@ooyala.com designates 209.85.160.172 as permitted sender)
Received: from [209.85.160.172] (HELO mail-yk0-f172.google.com) (209.85.160.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Apr 2014 18:25:16 +0000
Received: by mail-yk0-f172.google.com with SMTP id 200so7857236ykr.31
        for <dev@spark.apache.org>; Tue, 01 Apr 2014 11:24:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=ooyala.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=ACn2dTp2iYiKXjhcMkbJsP/K2jWNBNhChJ+P5spvvms=;
        b=U0PVjq3PNJmaRuI3l8rNRvzdQZa1LeuykYqGWldB9PlgselXNVt1bk6sEHXcgy6T2F
         T/Z9aV2G3O+EUcTO/AEBHs8FZumsJt5aeqRUm4DxICrpj4QKhA0V2gBdZIM7GN2xVIDa
         4U6quStRH0Pe5tbISvBxCQjYPEyDHyJ4Z7Eyg=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=ACn2dTp2iYiKXjhcMkbJsP/K2jWNBNhChJ+P5spvvms=;
        b=Rog5g4vde7xaiPGf2Kn2X+MMh9xWgQ2RzGVyBI8ggKQWLJfRiOtmYE6iSLrB/iOoWE
         6x+3TKmljfuBjgr2Fpu3uhV0p64K8Sgbk0fzFCKpTTkxQUVo4lg2W6f4Tjj//79/+CQd
         5epaoJVxv+rQYlVeMw8FVOJI0g2YcqubE7PksR6/BqBuZJChpZFaHsKPolFIWD6yJSJO
         1OJM2Rccg/Y2YxZHZ2tpVPF95Gw8395E4I1VGpQkhPCvposN8/gDD3YZXv1+HoHAfdKo
         /dHvgUbjpxjYUKqU85zjApvvTRh1BpkkYaBTrnnwS1vprHL/2nMLDO5aeH4TEk0Tiv5y
         mFaA==
X-Gm-Message-State: ALoCoQmV3LkxMO8N2zThVjnZX9Q4h7VL2iNO0xx+vNqeLZgJ9xn0NrOyFeX+Isg4WWC09pYt4qRE
MIME-Version: 1.0
X-Received: by 10.236.113.69 with SMTP id z45mr6563966yhg.0.1396376695548;
 Tue, 01 Apr 2014 11:24:55 -0700 (PDT)
Received: by 10.170.120.87 with HTTP; Tue, 1 Apr 2014 11:24:55 -0700 (PDT)
In-Reply-To: <CADWPM3gZCfn3K8saPiPVMzd4S6=84D+BWVybTm5vR-m7WGCzBQ@mail.gmail.com>
References: <CADWPM3gZCfn3K8saPiPVMzd4S6=84D+BWVybTm5vR-m7WGCzBQ@mail.gmail.com>
Date: Tue, 1 Apr 2014 11:24:55 -0700
Message-ID: <CADWPM3i=Yo7a6L-m5JR+Te=jjpp0++0g4vUvXsApdBKjA0oJmQ@mail.gmail.com>
Subject: Re: sbt-package-bin
From: Evan Chan <ev@ooyala.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf3010eb11e944d104f5ff48fb
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf3010eb11e944d104f5ff48fb
Content-Type: text/plain; charset=ISO-8859-1

Also, I understand this is the last week / merge window for 1.0, so if
folks are interested I'd like to get in a PR quickly.

thanks,
Evan



On Tue, Apr 1, 2014 at 11:24 AM, Evan Chan <ev@ooyala.com> wrote:

> Hey folks,
>
> We are in the middle of creating a Chef recipe for Spark.   As part of
> that we want to create a Debian package for Spark.
>
> What do folks think of adding the sbt-package-bin plugin to allow easy
> creation of a Spark .deb file?  I believe it adds all dependency jars into
> a single lib/ folder, so in some ways it's even easier to manage than the
> assembly.
>
> Also I'm not sure if there's an equivalent plugin for Maven.
>
> thanks,
> Evan
>
>
> --
> --
>  Evan Chan
> Staff Engineer
> ev@ooyala.com  |
>
> <http://www.ooyala.com/> <http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
>
>


-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>

--20cf3010eb11e944d104f5ff48fb--

From dev-return-7190-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  1 18:32:17 2014
Return-Path: <dev-return-7190-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4C30710F29
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Apr 2014 18:32:17 +0000 (UTC)
Received: (qmail 97128 invoked by uid 500); 1 Apr 2014 18:32:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96775 invoked by uid 500); 1 Apr 2014 18:31:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96238 invoked by uid 99); 1 Apr 2014 18:31:33 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Apr 2014 18:31:33 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mark@clearstorydata.com designates 209.85.212.182 as permitted sender)
Received: from [209.85.212.182] (HELO mail-wi0-f182.google.com) (209.85.212.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Apr 2014 18:31:29 +0000
Received: by mail-wi0-f182.google.com with SMTP id d1so5718977wiv.9
        for <dev@spark.apache.org>; Tue, 01 Apr 2014 11:31:07 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=4Zr2QKZ8U46o89U9YOZxOZ/fas5xaYtSSsO3bBu7kWM=;
        b=QsukJylLcsrFB2G6JY2VdzdkEye+iOS+GZCi4HEWGlXZ9Uyj0L6C531Jgp4d1A60ZG
         rlyURmYkCgPYU+hxnNuCS38qVcJC6TWW2+qkJFBdOnf5ez5eF2Msmoert6C+T/o4pvge
         OX/cuFlh8HBRX249Wlgoh4EY671uWOd/z3Q9CI/MtHmELq5uVKQEowsu4r5e8jYQLxCg
         mf6aPZiFRyydOM1etxMgErpdQSze8t9TMd4OHFbZ8go9oi2VnJWtBKd9o2Th3WkWq9uF
         CuEXBaftPZPb2t2c3saPbx5z4SOAz3MlW41dPs3Qc0vH3z0SuOwymgSbhcmD8fdssZgX
         gYFQ==
X-Gm-Message-State: ALoCoQld2JDWbaFua1ARqKLHk287JcCPSX74goG7B5KsbmZzgPzPgyHS6w71tfJ2yqnVheanibry
MIME-Version: 1.0
X-Received: by 10.180.98.232 with SMTP id el8mr22007322wib.27.1396377067287;
 Tue, 01 Apr 2014 11:31:07 -0700 (PDT)
Received: by 10.216.232.69 with HTTP; Tue, 1 Apr 2014 11:31:07 -0700 (PDT)
In-Reply-To: <CADWPM3i=Yo7a6L-m5JR+Te=jjpp0++0g4vUvXsApdBKjA0oJmQ@mail.gmail.com>
References: <CADWPM3gZCfn3K8saPiPVMzd4S6=84D+BWVybTm5vR-m7WGCzBQ@mail.gmail.com>
	<CADWPM3i=Yo7a6L-m5JR+Te=jjpp0++0g4vUvXsApdBKjA0oJmQ@mail.gmail.com>
Date: Tue, 1 Apr 2014 11:31:07 -0700
Message-ID: <CAAsvFPmKcH9Pp4x9PfuQOcZLH_mKUw05dBG6TZw+6=qhgazSqw@mail.gmail.com>
Subject: Re: sbt-package-bin
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d0442885e118b0104f5ff5fda
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0442885e118b0104f5ff5fda
Content-Type: text/plain; charset=ISO-8859-1

A basic Debian package can already be created from the Maven build: mvn
-Pdeb ...


On Tue, Apr 1, 2014 at 11:24 AM, Evan Chan <ev@ooyala.com> wrote:

> Also, I understand this is the last week / merge window for 1.0, so if
> folks are interested I'd like to get in a PR quickly.
>
> thanks,
> Evan
>
>
>
> On Tue, Apr 1, 2014 at 11:24 AM, Evan Chan <ev@ooyala.com> wrote:
>
> > Hey folks,
> >
> > We are in the middle of creating a Chef recipe for Spark.   As part of
> > that we want to create a Debian package for Spark.
> >
> > What do folks think of adding the sbt-package-bin plugin to allow easy
> > creation of a Spark .deb file?  I believe it adds all dependency jars
> into
> > a single lib/ folder, so in some ways it's even easier to manage than the
> > assembly.
> >
> > Also I'm not sure if there's an equivalent plugin for Maven.
> >
> > thanks,
> > Evan
> >
> >
> > --
> > --
> >  Evan Chan
> > Staff Engineer
> > ev@ooyala.com  |
> >
> > <http://www.ooyala.com/> <http://www.facebook.com/ooyala><
> http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
> >
> >
>
>
> --
> --
> Evan Chan
> Staff Engineer
> ev@ooyala.com  |
>
> <http://www.ooyala.com/>
> <http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><
> http://www.twitter.com/ooyala>
>

--f46d0442885e118b0104f5ff5fda--

From dev-return-7191-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  1 18:37:11 2014
Return-Path: <dev-return-7191-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D241E10F63
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Apr 2014 18:37:11 +0000 (UTC)
Received: (qmail 15040 invoked by uid 500); 1 Apr 2014 18:37:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15005 invoked by uid 500); 1 Apr 2014 18:37:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14997 invoked by uid 99); 1 Apr 2014 18:37:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Apr 2014 18:37:09 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.182 as permitted sender)
Received: from [209.85.214.182] (HELO mail-ob0-f182.google.com) (209.85.214.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Apr 2014 18:37:05 +0000
Received: by mail-ob0-f182.google.com with SMTP id uz6so11542501obc.13
        for <dev@spark.apache.org>; Tue, 01 Apr 2014 11:36:43 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=CS7/UuVpM1BlwnFLZHMTd6e3jZvdpQ98mwxQpY2889E=;
        b=tAcfg8AdKZiXZ4e+XgAHhNRJUOJDhzRtEDPySuD1EAzM3sf5sLWRumlidsnX0C4ruZ
         icZFaKSapI2xSnlWtmUwNR9OBWcFPZNZBJXbZ8J839GYeTHwlNnhvHVux67IKiWE+lHf
         iZiIH6WsrCowcVWnG5TbDYi3CB3Tr5aWzNq2h6GUtKoBgtL0xSF0dKEmysgU6Yu65ovh
         3fuB6CmXq5FkDnQSu8IP+JYGp0MXk+7KmbQYlGkGPEADfISiNi+oSCozrUaG3fFXtx4x
         41csCNwjVnoaeG29buMw1PQppCZno+NP/jKru04juwxasETZFrVn72oJoGq1A758fdF3
         6wlA==
MIME-Version: 1.0
X-Received: by 10.182.142.37 with SMTP id rt5mr2012167obb.76.1396377403103;
 Tue, 01 Apr 2014 11:36:43 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Tue, 1 Apr 2014 11:36:42 -0700 (PDT)
In-Reply-To: <CAAsvFPmKcH9Pp4x9PfuQOcZLH_mKUw05dBG6TZw+6=qhgazSqw@mail.gmail.com>
References: <CADWPM3gZCfn3K8saPiPVMzd4S6=84D+BWVybTm5vR-m7WGCzBQ@mail.gmail.com>
	<CADWPM3i=Yo7a6L-m5JR+Te=jjpp0++0g4vUvXsApdBKjA0oJmQ@mail.gmail.com>
	<CAAsvFPmKcH9Pp4x9PfuQOcZLH_mKUw05dBG6TZw+6=qhgazSqw@mail.gmail.com>
Date: Tue, 1 Apr 2014 11:36:42 -0700
Message-ID: <CABPQxss8qrxz75SHKuxEavz_5=Q8+dLkzZjkQ7WPhAmqtDHgsA@mail.gmail.com>
Subject: Re: sbt-package-bin
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2ec9a15a06e04f5ff73aa
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ec9a15a06e04f5ff73aa
Content-Type: text/plain; charset=ISO-8859-1

Ya there is already some fragmentation here. Maven has some "dist" targets
and there is also ./make-distribution.sh.


On Tue, Apr 1, 2014 at 11:31 AM, Mark Hamstra <mark@clearstorydata.com>wrote:

> A basic Debian package can already be created from the Maven build: mvn
> -Pdeb ...
>
>
> On Tue, Apr 1, 2014 at 11:24 AM, Evan Chan <ev@ooyala.com> wrote:
>
> > Also, I understand this is the last week / merge window for 1.0, so if
> > folks are interested I'd like to get in a PR quickly.
> >
> > thanks,
> > Evan
> >
> >
> >
> > On Tue, Apr 1, 2014 at 11:24 AM, Evan Chan <ev@ooyala.com> wrote:
> >
> > > Hey folks,
> > >
> > > We are in the middle of creating a Chef recipe for Spark.   As part of
> > > that we want to create a Debian package for Spark.
> > >
> > > What do folks think of adding the sbt-package-bin plugin to allow easy
> > > creation of a Spark .deb file?  I believe it adds all dependency jars
> > into
> > > a single lib/ folder, so in some ways it's even easier to manage than
> the
> > > assembly.
> > >
> > > Also I'm not sure if there's an equivalent plugin for Maven.
> > >
> > > thanks,
> > > Evan
> > >
> > >
> > > --
> > > --
> > >  Evan Chan
> > > Staff Engineer
> > > ev@ooyala.com  |
> > >
> > > <http://www.ooyala.com/> <http://www.facebook.com/ooyala><
> > http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
> > >
> > >
> >
> >
> > --
> > --
> > Evan Chan
> > Staff Engineer
> > ev@ooyala.com  |
> >
> > <http://www.ooyala.com/>
> > <http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala
> ><
> > http://www.twitter.com/ooyala>
> >
>

--001a11c2ec9a15a06e04f5ff73aa--

From dev-return-7192-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  1 18:37:33 2014
Return-Path: <dev-return-7192-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 920B210F70
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Apr 2014 18:37:33 +0000 (UTC)
Received: (qmail 16465 invoked by uid 500); 1 Apr 2014 18:37:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16438 invoked by uid 500); 1 Apr 2014 18:37:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16417 invoked by uid 99); 1 Apr 2014 18:37:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Apr 2014 18:37:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.42 as permitted sender)
Received: from [209.85.219.42] (HELO mail-oa0-f42.google.com) (209.85.219.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Apr 2014 18:37:20 +0000
Received: by mail-oa0-f42.google.com with SMTP id i4so11855449oah.29
        for <dev@spark.apache.org>; Tue, 01 Apr 2014 11:36:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=LKUZI3pxKpn8Co89wq2uAGwwz61/Pca4LrQd8uFbEeM=;
        b=wqMvvxvyRwENvbuEcMrBNnTX3JzCsL49xFVZLaW58aHGgYd9uZU4Wj+DlqOEQKVvfA
         cs1fFgAzQxFNl5sAZVTcQZnkJS0CCVh2eCVc5FVsZ6Nh1t7IYg8m+l98h6EGXU9mtojR
         iRIwXZQBQaD2dH/ADtmfBEjjxC16Nq436hbY0OUiMvtM7Dy6xJnko4H3XX+faPZ3IdaV
         4eJ10IZtlJFQBy+H24LPBESyaUXcOth3qQH6jUJL4OO27K1X6SUfRJA8dtkCesEytyLR
         /tFQh7+K9v97ERb3yNs3Ip0ud+3muXiPoO8WLIioo0G/hEI+UAiuFWAU11MVhHeZhd8T
         cVBA==
MIME-Version: 1.0
X-Received: by 10.60.92.170 with SMTP id cn10mr2006073oeb.76.1396377419259;
 Tue, 01 Apr 2014 11:36:59 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Tue, 1 Apr 2014 11:36:59 -0700 (PDT)
In-Reply-To: <CABPQxss8qrxz75SHKuxEavz_5=Q8+dLkzZjkQ7WPhAmqtDHgsA@mail.gmail.com>
References: <CADWPM3gZCfn3K8saPiPVMzd4S6=84D+BWVybTm5vR-m7WGCzBQ@mail.gmail.com>
	<CADWPM3i=Yo7a6L-m5JR+Te=jjpp0++0g4vUvXsApdBKjA0oJmQ@mail.gmail.com>
	<CAAsvFPmKcH9Pp4x9PfuQOcZLH_mKUw05dBG6TZw+6=qhgazSqw@mail.gmail.com>
	<CABPQxss8qrxz75SHKuxEavz_5=Q8+dLkzZjkQ7WPhAmqtDHgsA@mail.gmail.com>
Date: Tue, 1 Apr 2014 11:36:59 -0700
Message-ID: <CABPQxsurqik0xZOudS+qRo88FL_i4SCqiS3ysjW-VZFA4z-fvQ@mail.gmail.com>
Subject: Re: sbt-package-bin
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b33d31e0c75af04f5ff74f9
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b33d31e0c75af04f5ff74f9
Content-Type: text/plain; charset=ISO-8859-1

And there is a deb target as well - ah didn't see Mark's email.


On Tue, Apr 1, 2014 at 11:36 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> Ya there is already some fragmentation here. Maven has some "dist" targets
> and there is also ./make-distribution.sh.
>
>
> On Tue, Apr 1, 2014 at 11:31 AM, Mark Hamstra <mark@clearstorydata.com>wrote:
>
>> A basic Debian package can already be created from the Maven build: mvn
>> -Pdeb ...
>>
>>
>> On Tue, Apr 1, 2014 at 11:24 AM, Evan Chan <ev@ooyala.com> wrote:
>>
>> > Also, I understand this is the last week / merge window for 1.0, so if
>> > folks are interested I'd like to get in a PR quickly.
>> >
>> > thanks,
>> > Evan
>> >
>> >
>> >
>> > On Tue, Apr 1, 2014 at 11:24 AM, Evan Chan <ev@ooyala.com> wrote:
>> >
>> > > Hey folks,
>> > >
>> > > We are in the middle of creating a Chef recipe for Spark.   As part of
>> > > that we want to create a Debian package for Spark.
>> > >
>> > > What do folks think of adding the sbt-package-bin plugin to allow easy
>> > > creation of a Spark .deb file?  I believe it adds all dependency jars
>> > into
>> > > a single lib/ folder, so in some ways it's even easier to manage than
>> the
>> > > assembly.
>> > >
>> > > Also I'm not sure if there's an equivalent plugin for Maven.
>> > >
>> > > thanks,
>> > > Evan
>> > >
>> > >
>> > > --
>> > > --
>> > >  Evan Chan
>> > > Staff Engineer
>> > > ev@ooyala.com  |
>> > >
>> > > <http://www.ooyala.com/> <http://www.facebook.com/ooyala><
>> > http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
>> > >
>> > >
>> >
>> >
>> > --
>> > --
>> > Evan Chan
>> > Staff Engineer
>> > ev@ooyala.com  |
>> >
>> > <http://www.ooyala.com/>
>> > <http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala
>> ><
>> > http://www.twitter.com/ooyala>
>> >
>>
>
>

--047d7b33d31e0c75af04f5ff74f9--

From dev-return-7193-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  1 18:40:05 2014
Return-Path: <dev-return-7193-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 39B6410F9C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Apr 2014 18:40:05 +0000 (UTC)
Received: (qmail 21404 invoked by uid 500); 1 Apr 2014 18:40:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21134 invoked by uid 500); 1 Apr 2014 18:40:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21123 invoked by uid 99); 1 Apr 2014 18:40:02 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Apr 2014 18:40:02 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.173 as permitted sender)
Received: from [209.85.214.173] (HELO mail-ob0-f173.google.com) (209.85.214.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Apr 2014 18:39:58 +0000
Received: by mail-ob0-f173.google.com with SMTP id gq1so11475472obb.18
        for <dev@spark.apache.org>; Tue, 01 Apr 2014 11:39:38 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=n7ZeZuNF9nZ/3EvIbrEcZ/mmbiA69H7nR/Oh50C+z14=;
        b=MSCZYjpLlf+HLkvS9abf7GHQta/if+cut+dWjSHrayRPfXjg+kYdj5J5emv8cLxM4U
         NlxIqz1jAMyokK9i4jdqctJp5e9slTtpl3fhNUxvZtiK8P7U7RioNQMzySJrAi4EUbJk
         rqw/Qmg9MsRESf+5FeCpK4FVbxBbFaThOwFQLrlymnvDTpPqjUm92L0fpRCvnXay/vAp
         PjGlxQ6gxKKoGlmuh1KBLE20wNsLiIoTvxS9cmWeEgy/D8SjSqhRcedkfH6G/du3HaS7
         WHEx2OK9KdtDHYUZkKJgRJDOVG7+ZTaNcH7A76Gb2oG3uNgaHQ7WiZQqxc0TnWreyiEa
         5vlA==
MIME-Version: 1.0
X-Received: by 10.182.135.165 with SMTP id pt5mr2906795obb.66.1396377577975;
 Tue, 01 Apr 2014 11:39:37 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Tue, 1 Apr 2014 11:39:37 -0700 (PDT)
In-Reply-To: <1396362508.20704.YahooMailNeo@web140102.mail.bf1.yahoo.com>
References: <CAMwrk0mhqhBf2ZNRFiC4LgWjLb2A7fw3nCo8TwAZvadNinWmaA@mail.gmail.com>
	<E75FBF52-0373-44E2-BCC2-BCB8445C995C@gmail.com>
	<CAPdPcW2bN3yTLgFRaf=5Sm05mJQ71ufeMv8dfmEsEQLsEPsHGg@mail.gmail.com>
	<CABPQxssGF-rOYE_oFiR2CArbMt-auvTBUT4+Yck79+Wk4dzYcA@mail.gmail.com>
	<1396278778.50575.YahooMailNeo@web140101.mail.bf1.yahoo.com>
	<CABPQxss_myw8XYRE1CKHw8SgTqx=3+Eb+2-OzzHi4iV5m9wqPw@mail.gmail.com>
	<CAMwrk0mVrYGnon0JQvMpFUz7SPS_ZcxG53t95M2F_GowTBP9UQ@mail.gmail.com>
	<1396362508.20704.YahooMailNeo@web140102.mail.bf1.yahoo.com>
Date: Tue, 1 Apr 2014 11:39:37 -0700
Message-ID: <CABPQxsv92x+K+6T=MbFDWdMB6OHt-8qqyNeUXcUrk18Y09v_Eg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 0.9.1 (RC3)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>, Tom Graves <tgraves_cs@yahoo.com>
Cc: Tathagata Das <tathagata.das1565@gmail.com>
Content-Type: multipart/alternative; boundary=089e0112cb9e829f2b04f5ff7d87
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0112cb9e829f2b04f5ff7d87
Content-Type: text/plain; charset=ISO-8859-1

Tom,

Given this is a pretty straightforward workaround, what do yo think about
the following course of action:

(a) We can put the workaround in the docs for 0.9.1. We don't need to do a
new RC/vote for this since we can update the published docs independently.

(b) We try to get a fix in for this into the 0.9 branch so it can end up in
0.9.2. But this takes the fix off the critical path for this release.

- Patrick


On Tue, Apr 1, 2014 at 7:28 AM, Tom Graves <tgraves_cs@yahoo.com> wrote:

> Thanks for extending the voting.
>
> Unfortunately I've found an issue with the spark-shell in yarn-client
> mode.  It doesn't work with secure HDFS unless you
> export SPARK_YARN_MODE=true before starting the shell, or if you happen to
> do something immediately with HDFS.  If you wait for the connection to the
> namenode to timeout it will fail.
>
> I think it was actually this way in the 0.9 release also so I thought I
> would send this and get peoples feedback to see if you want it fixed?
>
> Another option would be to document that you have to export
> SPARK_YARN_MODE=true for the shell.   The fix actually went in with the
> authentication changes I made in master but I never realized that change
> needed to apply to 0.9.
>
>
> https://github.com/apache/spark/commit/7edbea41b43e0dc11a2de156be220db8b7952d01#diff-0ae5b834ce90ec37c19af35aa7a5e1a0
>
> See the SparkILoop diff.
>
> Tom
>
>
> On Monday, March 31, 2014 1:33 PM, Tathagata Das <
> tathagata.das1565@gmail.com> wrote:
>
> Yes, lets extend the vote for two more days from now. So the vote is open
> till Wednesday, April 02, at 20:00 UTC
>
> On that note, my +1
>
>
> TD
>
>
>
>
>
>
> On Mon, Mar 31, 2014 at 9:57 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> Yeah good point. Let's just extend this vote another few days?
> >
> >
> >
> >On Mon, Mar 31, 2014 at 8:12 AM, Tom Graves <tgraves_cs@yahoo.com> wrote:
> >
> >> I should probably pull this off into another thread, but going forward
> can
> >> we try to not have the release votes end on a weekend? Since we only
> seem
> >> to give 3 days, it makes it really hard for anyone who is offline for
> the
> >> weekend to try it out.   Either that or extend the voting for more then
> 3
> >> days.
> >>
> >> Tom
> >> On Monday, March 31, 2014 12:50 AM, Patrick Wendell <pwendell@gmail.com
> >
> >> wrote:
> >>
> >> TD - I downloaded and did some local testing. Looks good to me!
> >>
> >> +1
> >>
> >> You should cast your own vote - at that point it's enough to pass.
> >>
> >> - Patrick
> >>
> >>
> >>
> >> On Sun, Mar 30, 2014 at 9:47 PM, prabeesh k <prabsmails@gmail.com>
> wrote:
> >>
> >> > +1
> >> > tested on Ubuntu12.04 64bit
> >> >
> >> >
> >> > On Mon, Mar 31, 2014 at 3:56 AM, Matei Zaharia <
> matei.zaharia@gmail.com
> >> > >wrote:
> >> >
> >> > > +1 tested on Mac OS X.
> >> > >
> >> > > Matei
> >> > >
> >> > > On Mar 27, 2014, at 1:32 AM, Tathagata Das <
> >> tathagata.das1565@gmail.com>
> >> > > wrote:
> >> > >
> >> > > > Please vote on releasing the following candidate as Apache Spark
> >> > version
> >> > > 0.9.1
> >> > > >
> >> > > > A draft of the release notes along with the CHANGES.txt file is
> >> > > > attached to this e-mail.
> >> > > >
> >> > > > The tag to be voted on is v0.9.1-rc3 (commit 4c43182b):
> >> > > >
> >> > >
> >> >
> >>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4c43182b6d1b0b7717423f386c0214fe93073208
> >> > > >
> >> > > > The release files, including signatures, digests, etc. can be
> found
> >> at:
> >> > > > http://people.apache.org/~tdas/spark-0.9.1-rc3/
> >> > > >
> >> > > > Release artifacts are signed with the following key:
> >> > > > https://people.apache.org/keys/committer/tdas.asc
> >> > > >
> >> > > > The staging repository for this release can be found at:
> >> > > >
> >> >
> https://repository.apache.org/content/repositories/orgapachespark-1009/
> >> > > >
> >> > > > The documentation corresponding to this release can be found at:
> >> > > > http://people.apache.org/~tdas/spark-0.9.1-rc3-docs/
> >> > > >
> >> > > > Please vote on releasing this package as Apache Spark 0.9.1!
> >> > > >
> >> > > > The vote is open until Sunday, March 30, at 10:00 UTC and passes
> if
> >> > > > a majority of at least 3 +1 PMC votes are cast.
> >> > > >
> >> > > > [ ] +1 Release this package as Apache Spark 0.9.1
> >> > > > [ ] -1 Do not release this package because ...
> >> > > >
> >> > > > To learn more about Apache Spark, please see
> >> > > > http://spark.apache.org/
> >> > > > <CHANGES.txt><RELEASE_NOTES.txt>
> >> > >
> >> > >
> >> >
> >>
> >
>

--089e0112cb9e829f2b04f5ff7d87--

From dev-return-7194-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  1 18:44:32 2014
Return-Path: <dev-return-7194-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5ACAA10FC3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Apr 2014 18:44:32 +0000 (UTC)
Received: (qmail 33798 invoked by uid 500); 1 Apr 2014 18:44:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33208 invoked by uid 500); 1 Apr 2014 18:44:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33185 invoked by uid 99); 1 Apr 2014 18:44:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Apr 2014 18:44:28 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [216.109.114.170] (HELO nm43-vm9.bullet.mail.bf1.yahoo.com) (216.109.114.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Apr 2014 18:44:21 +0000
Received: from [98.139.212.149] by nm43.bullet.mail.bf1.yahoo.com with NNFMP; 01 Apr 2014 18:44:00 -0000
Received: from [98.139.212.209] by tm6.bullet.mail.bf1.yahoo.com with NNFMP; 01 Apr 2014 18:44:00 -0000
Received: from [127.0.0.1] by omp1018.mail.bf1.yahoo.com with NNFMP; 01 Apr 2014 18:44:00 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 650890.62619.bm@omp1018.mail.bf1.yahoo.com
Received: (qmail 33255 invoked by uid 60001); 1 Apr 2014 18:44:00 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.com; s=s1024; t=1396377840; bh=OOziyY04ZVqQBAPe/XLIPyykiLGBxNO+YTHzVkYGo0A=; h=X-YMail-OSG:Received:X-Rocket-MIMEInfo:X-Mailer:References:Message-ID:Date:From:Reply-To:Subject:To:Cc:In-Reply-To:MIME-Version:Content-Type; b=QWxKkbygrJ8A0rT9gH7WR6FCjIkappg0tVhQBF2g0icO4vavX6tvs5KNEdVG6AjFxQVU/be2cKAvFgIbEu44CpaWRBSb1/jGeeOnWWvcJ+n6qdq0OLQGKV+ePSvdNA7sBbHzhhZ8RdT037rZ1UnnEsy2CWCVp0v8EkSkWHee/JA=
DomainKey-Signature:a=rsa-sha1; q=dns; c=nofws;
  s=s1024; d=yahoo.com;
  h=X-YMail-OSG:Received:X-Rocket-MIMEInfo:X-Mailer:References:Message-ID:Date:From:Reply-To:Subject:To:Cc:In-Reply-To:MIME-Version:Content-Type;
  b=3aUFdWaPYeLvwiL5pzny4uzPq4Qnvkri9JOZ8f3WrEQpFn6n2Lw2tVPxjcs0zf7FOHk9u1Go7QxqMRrSByNV4CyF+rQGWjRucUWuQi4cm9InjvLQpJgd4TKSxqRRQP+W93vFQmzzUHk17GqinEJTGhJfSopLg1XlCCh1Gs+1UjE=;
X-YMail-OSG: GDWAZPoVM1kX_vSKuryoxtfMJ3tBqW7Y4FP.LhfqqSnvuEV
 EJxJNefhslZd5gn7TZM_oEqC4whaMDtAZy66sLh7tu_TGtvCRYYkdySw6XXm
 ixMEQcX36AimLi6jZV5H3Ga.xN2PRwpPOxEKLvcItFgv36I4lYCyn1vGeuVO
 v7iBiKehe7uWm9G8UdjyllfAj_EELrKmW2eoi2KiZHiHKmal4LqgsBRfs5NC
 8NYuvUPEJnadP3E.bYCDdyzbhzLYyGZ0sUT91z8Pr4.BqaTkqzBYwg32HsBT
 G3ZcS_dznblOj559k6VwTHC9kr.cc3mpj7zPOl28yfclcJ77VFSGakkMFOIp
 okl2ZRa7CRjlEgvn2bhXqxVEVAGWMLJL90WOgEbUdys7_3bRKMyqjSnEfqb4
 iFmUIxIIewnrDhztvhP6b7cc4U0dqjOPYQVGJ4jSfkgHdQf7FqrXiFmnvr95
 FGNUfBm6DSI8GUdEZlJibhHwZFxvGRKUDW3tZvye3WlChLM8XQ_SwJcagX.0
 kgJvu5ecxz3DaCRsMwZvcpq6ygh7HA8d5h4Xmz5eMb9O7pd20vynbqebN1Rx
 sAbhqbFQxTGXcwd8TnolS9tK0dSyqpqBY5hzt7YUAsnVDIvabJfGIp2zp_SC
 is04E3bcjP.bzm0A9lL44zFHqTRl.uTzjpgPA07I37figtBDQDhKG42OT3TM
 oXVkGGa2LpEnzMaoJqS9WwndaDlg0SfGozZAlSybWyhGRf7srVzfukaWl2fb
 WVw--
Received: from [204.11.79.50] by web140105.mail.bf1.yahoo.com via HTTP; Tue, 01 Apr 2014 11:44:00 PDT
X-Rocket-MIMEInfo: 002.001,Tm8gb25lIGVsc2UgaGFzIHJlcG9ydGVkIHNlZWluZyB0aGUgaXNzdWUgc28gSSB0aGluayBkb2N1bWVudGluZyBpdCBpcyBmaW5lLiDCoAoKVG9tCgoKT24gVHVlc2RheSwgQXByaWwgMSwgMjAxNCAxOjQwIFBNLCBQYXRyaWNrIFdlbmRlbGwgPHB3ZW5kZWxsQGdtYWlsLmNvbT4gd3JvdGU6CiAKVG9tLAoKR2l2ZW4gdGhpcyBpcyBhIHByZXR0eSBzdHJhaWdodGZvcndhcmQgd29ya2Fyb3VuZCwgd2hhdCBkbyB5byB0aGluayBhYm91dAp0aGUgZm9sbG93aW5nIGNvdXJzZSBvZiBhY3Rpb246CgooYSkgV2UgY2EBMAEBAQE-
X-Mailer: YahooMailWebService/0.8.182.648
References: <CAMwrk0mhqhBf2ZNRFiC4LgWjLb2A7fw3nCo8TwAZvadNinWmaA@mail.gmail.com>	<E75FBF52-0373-44E2-BCC2-BCB8445C995C@gmail.com>	<CAPdPcW2bN3yTLgFRaf=5Sm05mJQ71ufeMv8dfmEsEQLsEPsHGg@mail.gmail.com>	<CABPQxssGF-rOYE_oFiR2CArbMt-auvTBUT4+Yck79+Wk4dzYcA@mail.gmail.com>	<1396278778.50575.YahooMailNeo@web140101.mail.bf1.yahoo.com>	<CABPQxss_myw8XYRE1CKHw8SgTqx=3+Eb+2-OzzHi4iV5m9wqPw@mail.gmail.com>	<CAMwrk0mVrYGnon0JQvMpFUz7SPS_ZcxG53t95M2F_GowTBP9UQ@mail.gmail.com>	<1396362508.20704.YahooMailNeo@web140102.mail.bf1.yahoo.com> <CABPQxsv92x+K+6T=MbFDWdMB6OHt-8qqyNeUXcUrk18Y09v_Eg@mail.gmail.com>
Message-ID: <1396377840.28297.YahooMailNeo@web140105.mail.bf1.yahoo.com>
Date: Tue, 1 Apr 2014 11:44:00 -0700 (PDT)
From: Tom Graves <tgraves_cs@yahoo.com>
Reply-To: Tom Graves <tgraves_cs@yahoo.com>
Subject: Re: [VOTE] Release Apache Spark 0.9.1 (RC3)
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: Tathagata Das <tathagata.das1565@gmail.com>
In-Reply-To: <CABPQxsv92x+K+6T=MbFDWdMB6OHt-8qqyNeUXcUrk18Y09v_Eg@mail.gmail.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="66961340-1992632867-1396377840=:28297"
X-Virus-Checked: Checked by ClamAV on apache.org

--66961340-1992632867-1396377840=:28297
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

No one else has reported seeing the issue so I think documenting it is fine=
. =A0=0A=0ATom=0A=0A=0AOn Tuesday, April 1, 2014 1:40 PM, Patrick Wendell <=
pwendell@gmail.com> wrote:=0A =0ATom,=0A=0AGiven this is a pretty straightf=
orward workaround, what do yo think about=0Athe following course of action:=
=0A=0A(a) We can put the workaround in the docs for 0.9.1. We don't need to=
 do a=0Anew RC/vote for this since we can update the published docs indepen=
dently.=0A=0A(b) We try to get a fix in for this into the 0.9 branch so it =
can end up in=0A0.9.2. But this takes the fix off the critical path for thi=
s release.=0A=0A- Patrick=0A=0A=0A=0AOn Tue, Apr 1, 2014 at 7:28 AM, Tom Gr=
aves <tgraves_cs@yahoo.com> wrote:=0A=0A> Thanks for extending the voting.=
=0A>=0A> Unfortunately I've found an issue with the spark-shell in yarn-cli=
ent=0A> mode.=A0 It doesn't work with secure HDFS unless you=0A> export SPA=
RK_YARN_MODE=3Dtrue before starting the shell, or if you happen to=0A> do s=
omething immediately with HDFS.=A0 If you wait for the connection to the=0A=
> namenode to timeout it will fail.=0A>=0A> I think it was actually this wa=
y in the 0.9 release also so I thought I=0A> would send this and get people=
s feedback to see if you want it fixed?=0A>=0A> Another option would be to =
document that you have to export=0A> SPARK_YARN_MODE=3Dtrue for the shell.=
=A0  The fix actually went in with the=0A> authentication changes I made in=
 master but I never realized that change=0A> needed to apply to 0.9.=0A>=0A=
>=0A> https://github.com/apache/spark/commit/7edbea41b43e0dc11a2de156be220d=
b8b7952d01#diff-0ae5b834ce90ec37c19af35aa7a5e1a0=0A>=0A> See the SparkILoop=
 diff.=0A>=0A> Tom=0A>=0A>=0A> On Monday, March 31, 2014 1:33 PM, Tathagata=
 Das <=0A> tathagata.das1565@gmail.com> wrote:=0A>=0A> Yes, lets extend the=
 vote for two more days from now. So the vote is open=0A> till Wednesday, A=
pril 02, at 20:00 UTC=0A>=0A> On that note, my +1=0A>=0A>=0A> TD=0A>=0A>=0A=
>=0A>=0A>=0A>=0A> On Mon, Mar 31, 2014 at 9:57 AM, Patrick Wendell <pwendel=
l@gmail.com>=0A> wrote:=0A>=0A> Yeah good point. Let's just extend this vot=
e another few days?=0A> >=0A> >=0A> >=0A> >On Mon, Mar 31, 2014 at 8:12 AM,=
 Tom Graves <tgraves_cs@yahoo.com> wrote:=0A> >=0A> >> I should probably pu=
ll this off into another thread, but going forward=0A> can=0A> >> we try to=
 not have the release votes end on a weekend? Since we only=0A> seem=0A> >>=
 to give 3 days, it makes it really hard for anyone who is offline for=0A> =
the=0A> >> weekend to try it out.=A0  Either that or extend the voting for =
more then=0A> 3=0A> >> days.=0A> >>=0A> >> Tom=0A> >> On Monday, March 31, =
2014 12:50 AM, Patrick Wendell <pwendell@gmail.com=0A> >=0A> >> wrote:=0A> =
>>=0A> >> TD - I downloaded and did some local testing. Looks good to me!=
=0A> >>=0A> >> +1=0A> >>=0A> >> You should cast your own vote - at that poi=
nt it's enough to pass.=0A> >>=0A> >> - Patrick=0A> >>=0A> >>=0A> >>=0A> >>=
 On Sun, Mar 30, 2014 at 9:47 PM, prabeesh k <prabsmails@gmail.com>=0A> wro=
te:=0A> >>=0A> >> > +1=0A> >> > tested on Ubuntu12.04 64bit=0A> >> >=0A> >>=
 >=0A> >> > On Mon, Mar 31, 2014 at 3:56 AM, Matei Zaharia <=0A> matei.zaha=
ria@gmail.com=0A> >> > >wrote:=0A> >> >=0A> >> > > +1 tested on Mac OS X.=
=0A> >> > >=0A> >> > > Matei=0A> >> > >=0A> >> > > On Mar 27, 2014, at 1:32=
 AM, Tathagata Das <=0A> >> tathagata.das1565@gmail.com>=0A> >> > > wrote:=
=0A> >> > >=0A> >> > > > Please vote on releasing the following candidate a=
s Apache Spark=0A> >> > version=0A> >> > > 0.9.1=0A> >> > > >=0A> >> > > > =
A draft of the release notes along with the CHANGES.txt file is=0A> >> > > =
> attached to this e-mail.=0A> >> > > >=0A> >> > > > The tag to be voted on=
 is v0.9.1-rc3 (commit 4c43182b):=0A> >> > > >=0A> >> > >=0A> >> >=0A> >>=
=0A> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D4=
c43182b6d1b0b7717423f386c0214fe93073208=0A> >> > > >=0A> >> > > > The relea=
se files, including signatures, digests, etc. can be=0A> found=0A> >> at:=
=0A> >> > > > http://people.apache.org/~tdas/spark-0.9.1-rc3/=0A> >> > > >=
=0A> >> > > > Release artifacts are signed with the following key:=0A> >> >=
 > > https://people.apache.org/keys/committer/tdas.asc=0A> >> > > >=0A> >> =
> > > The staging repository for this release can be found at:=0A> >> > > >=
=0A> >> >=0A> https://repository.apache.org/content/repositories/orgapaches=
park-1009/=0A> >> > > >=0A> >> > > > The documentation corresponding to thi=
s release can be found at:=0A> >> > > > http://people.apache.org/~tdas/spar=
k-0.9.1-rc3-docs/=0A> >> > > >=0A> >> > > > Please vote on releasing this p=
ackage as Apache Spark 0.9.1!=0A> >> > > >=0A> >> > > > The vote is open un=
til Sunday, March 30, at 10:00 UTC and passes=0A> if=0A> >> > > > a majorit=
y of at least 3 +1 PMC votes are cast.=0A> >> > > >=0A> >> > > > [ ] +1 Rel=
ease this package as Apache Spark 0.9.1=0A> >> > > > [ ] -1 Do not release =
this package because ...=0A> >> > > >=0A> >> > > > To learn more about Apac=
he Spark, please see=0A> >> > > > http://spark.apache.org/=0A> >> > > > <CH=
ANGES.txt><RELEASE_NOTES.txt>=0A> >> > >=0A> >> > >=0A> >> >=0A> >>=0A> >=
=0A>
--66961340-1992632867-1396377840=:28297--

From dev-return-7195-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  1 22:02:04 2014
Return-Path: <dev-return-7195-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A626010881
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Apr 2014 22:02:04 +0000 (UTC)
Received: (qmail 40372 invoked by uid 500); 1 Apr 2014 22:02:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39779 invoked by uid 500); 1 Apr 2014 22:02:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39770 invoked by uid 99); 1 Apr 2014 22:01:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Apr 2014 22:01:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ev@ooyala.com designates 209.85.220.177 as permitted sender)
Received: from [209.85.220.177] (HELO mail-vc0-f177.google.com) (209.85.220.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Apr 2014 22:01:54 +0000
Received: by mail-vc0-f177.google.com with SMTP id if17so10364003vcb.8
        for <dev@spark.apache.org>; Tue, 01 Apr 2014 15:01:32 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=ooyala.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=1Uqf2qL4nFwr1+Gf9bVknWOX+sCsn4QsNSLtoi5Btdw=;
        b=jCXHy7aY7Ik1ks05+RL2tW4LogILQlVHiu1qnnrtMTAedaCn56O+8M0E8yulnTMZDx
         eZSN/qNFeDu1RsDbLwikonkZVut5M4H8gkUYDMwB1nmeJAv//zu/7+2v4A8cZCpKgmwx
         pI7zW6YldnuZDkJA2YX+SEAs9V86DVfV76u7M=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=1Uqf2qL4nFwr1+Gf9bVknWOX+sCsn4QsNSLtoi5Btdw=;
        b=bgpm5Kt3Y2bgnl3OWK+YBU9b1cm5uEcVShX/rgQblEnkc9qzfF1njHVjRj7qJbPVb1
         GfvN3pW9c10sGg4n27uYW1Q62k/Fyce2F/LXtAymanQANxpeU+udctC/tbqeiX2G1MWt
         sLX7FkrHolg6CbDMpFFyWLtk2E00Vg4v8wRFhaGRSwV2KTFNJ0Wv5LJYJeSMWuCvIhiB
         X/kqWy6cgV4Yin6MLmEowapjF7pb7U/NxRZ/ZehC3moVE920nakKmt+P6mN/rJfPviZo
         Mdovc704i0HJcigteYAW9I39WO3V6QlJTusz79aHV5ew7v4B1BUazlpncl/M+zv0eCLb
         Rtog==
X-Gm-Message-State: ALoCoQkEtnnsixj6WRjcqUALK39sCFGUHEtdphUSpiRswYjrgw0iYo0+4zF5urMkqNkyq9vx4+UK
MIME-Version: 1.0
X-Received: by 10.52.23.97 with SMTP id l1mr4066500vdf.11.1396389692620; Tue,
 01 Apr 2014 15:01:32 -0700 (PDT)
Received: by 10.52.177.170 with HTTP; Tue, 1 Apr 2014 15:01:32 -0700 (PDT)
In-Reply-To: <CABPQxsurqik0xZOudS+qRo88FL_i4SCqiS3ysjW-VZFA4z-fvQ@mail.gmail.com>
References: <CADWPM3gZCfn3K8saPiPVMzd4S6=84D+BWVybTm5vR-m7WGCzBQ@mail.gmail.com>
	<CADWPM3i=Yo7a6L-m5JR+Te=jjpp0++0g4vUvXsApdBKjA0oJmQ@mail.gmail.com>
	<CAAsvFPmKcH9Pp4x9PfuQOcZLH_mKUw05dBG6TZw+6=qhgazSqw@mail.gmail.com>
	<CABPQxss8qrxz75SHKuxEavz_5=Q8+dLkzZjkQ7WPhAmqtDHgsA@mail.gmail.com>
	<CABPQxsurqik0xZOudS+qRo88FL_i4SCqiS3ysjW-VZFA4z-fvQ@mail.gmail.com>
Date: Tue, 1 Apr 2014 15:01:32 -0700
Message-ID: <CADWPM3j6-gpCf-TTvFfJy7Qf7RPQ9AYGb4F-VAvG9OPn4tW0UQ@mail.gmail.com>
Subject: Re: sbt-package-bin
From: Evan Chan <ev@ooyala.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf3079bc6498d4ec04f6024f30
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf3079bc6498d4ec04f6024f30
Content-Type: text/plain; charset=ISO-8859-1

Mark - sorry, would you mind expanding what the "...." is?

Something like

mvn -Pdeb package

?

I get:

[ERROR] Plugin org.apache.maven.plugins:maven-compiler-plugin:3.1 or one of
its dependencies could not be resolved: Failed to read artifact descriptor
for org.apache.maven.plugins:maven-compiler-plugin:jar:3.1: Could not find
artifact org.apache:apache:pom:13 -> [Help 1]


On Tue, Apr 1, 2014 at 11:36 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> And there is a deb target as well - ah didn't see Mark's email.
>
>
> On Tue, Apr 1, 2014 at 11:36 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> > Ya there is already some fragmentation here. Maven has some "dist"
> targets
> > and there is also ./make-distribution.sh.
> >
> >
> > On Tue, Apr 1, 2014 at 11:31 AM, Mark Hamstra <mark@clearstorydata.com
> >wrote:
> >
> >> A basic Debian package can already be created from the Maven build: mvn
> >> -Pdeb ...
> >>
> >>
> >> On Tue, Apr 1, 2014 at 11:24 AM, Evan Chan <ev@ooyala.com> wrote:
> >>
> >> > Also, I understand this is the last week / merge window for 1.0, so if
> >> > folks are interested I'd like to get in a PR quickly.
> >> >
> >> > thanks,
> >> > Evan
> >> >
> >> >
> >> >
> >> > On Tue, Apr 1, 2014 at 11:24 AM, Evan Chan <ev@ooyala.com> wrote:
> >> >
> >> > > Hey folks,
> >> > >
> >> > > We are in the middle of creating a Chef recipe for Spark.   As part
> of
> >> > > that we want to create a Debian package for Spark.
> >> > >
> >> > > What do folks think of adding the sbt-package-bin plugin to allow
> easy
> >> > > creation of a Spark .deb file?  I believe it adds all dependency
> jars
> >> > into
> >> > > a single lib/ folder, so in some ways it's even easier to manage
> than
> >> the
> >> > > assembly.
> >> > >
> >> > > Also I'm not sure if there's an equivalent plugin for Maven.
> >> > >
> >> > > thanks,
> >> > > Evan
> >> > >
> >> > >
> >> > > --
> >> > > --
> >> > >  Evan Chan
> >> > > Staff Engineer
> >> > > ev@ooyala.com  |
> >> > >
> >> > > <http://www.ooyala.com/> <http://www.facebook.com/ooyala><
> >> > http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala
> >
> >> > >
> >> > >
> >> >
> >> >
> >> > --
> >> > --
> >> > Evan Chan
> >> > Staff Engineer
> >> > ev@ooyala.com  |
> >> >
> >> > <http://www.ooyala.com/>
> >> > <http://www.facebook.com/ooyala><
> http://www.linkedin.com/company/ooyala
> >> ><
> >> > http://www.twitter.com/ooyala>
> >> >
> >>
> >
> >
>



-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>

--20cf3079bc6498d4ec04f6024f30--

From dev-return-7196-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr  2 01:01:32 2014
Return-Path: <dev-return-7196-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2BA7A10F40
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Apr 2014 01:01:32 +0000 (UTC)
Received: (qmail 95053 invoked by uid 500); 2 Apr 2014 01:01:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95018 invoked by uid 500); 2 Apr 2014 01:01:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95007 invoked by uid 99); 2 Apr 2014 01:01:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Apr 2014 01:01:30 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mark@clearstorydata.com designates 209.85.212.179 as permitted sender)
Received: from [209.85.212.179] (HELO mail-wi0-f179.google.com) (209.85.212.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Apr 2014 01:01:26 +0000
Received: by mail-wi0-f179.google.com with SMTP id z2so4394499wiv.12
        for <dev@spark.apache.org>; Tue, 01 Apr 2014 18:01:04 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=XCNYJNq2kvSl2QlzwjyPNq3hPkwTo7nDaxMjENc/bCM=;
        b=SINOEeR0UJmvLXOnJvQoh5HjWe/qD6+olcfBX2Ax8tkoV1DPzp8gE7Jjmt11N/zYYj
         +diIIeS057N/NG81IB+X/R88M/93r5BYYnIcIWzQ6TkgQgWrvrJYV+35lTJvp3TEpQCq
         w9c52WpPodVKb5h13gKDcqFctIn8tGGF23tj9IkNxrQBI7QTVPVrwghWvuBTuVYkwL4k
         onx2kU8FEgM2oWawG5Y+WQTTG6oCba3TjwOXcNvupQSEbAFAm3+V+nJhSeh9p+yLL+1+
         cw175s+YDA3I+FTdDIeNVwgWY1L13943BjulVka+6sZTROg34xOpPiWsRylMJKqZ/ygE
         bYuA==
X-Gm-Message-State: ALoCoQnrcKc9EmhEcIjhrGgQYR4U5nXVZMFiWI1eu0CkazxYg553RUYryYIahObYORJd1Q1qKJCG
MIME-Version: 1.0
X-Received: by 10.181.8.66 with SMTP id di2mr24070945wid.43.1396400464364;
 Tue, 01 Apr 2014 18:01:04 -0700 (PDT)
Received: by 10.216.232.69 with HTTP; Tue, 1 Apr 2014 18:01:04 -0700 (PDT)
In-Reply-To: <CADWPM3j6-gpCf-TTvFfJy7Qf7RPQ9AYGb4F-VAvG9OPn4tW0UQ@mail.gmail.com>
References: <CADWPM3gZCfn3K8saPiPVMzd4S6=84D+BWVybTm5vR-m7WGCzBQ@mail.gmail.com>
	<CADWPM3i=Yo7a6L-m5JR+Te=jjpp0++0g4vUvXsApdBKjA0oJmQ@mail.gmail.com>
	<CAAsvFPmKcH9Pp4x9PfuQOcZLH_mKUw05dBG6TZw+6=qhgazSqw@mail.gmail.com>
	<CABPQxss8qrxz75SHKuxEavz_5=Q8+dLkzZjkQ7WPhAmqtDHgsA@mail.gmail.com>
	<CABPQxsurqik0xZOudS+qRo88FL_i4SCqiS3ysjW-VZFA4z-fvQ@mail.gmail.com>
	<CADWPM3j6-gpCf-TTvFfJy7Qf7RPQ9AYGb4F-VAvG9OPn4tW0UQ@mail.gmail.com>
Date: Tue, 1 Apr 2014 18:01:04 -0700
Message-ID: <CAAsvFPkEr3K7XDxcDe1JeOoFQOrOaGJM6KWuYgpPt7DTtBBDkg@mail.gmail.com>
Subject: Re: sbt-package-bin
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134d67ea4a28e04f604d19a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134d67ea4a28e04f604d19a
Content-Type: text/plain; charset=ISO-8859-1

What the "..." is kind of depends on what you're trying to accomplish.  You
could be setting Hadoop version and other stuff in there, but if you go too
much beyond a pretty basic build, you're probably also going to have to
modify the <configuration> of the jdeb plugin in assembly/pom.xml to
include/exclude just what you want/don't want in the Debian package.

Anyway, a typical build would look something like 'mvn -U -Pdeb -DskipTests
clean package', after which you should be able to find your .deb in
assembly/target.


On Tue, Apr 1, 2014 at 3:01 PM, Evan Chan <ev@ooyala.com> wrote:

> Mark - sorry, would you mind expanding what the "...." is?
>
> Something like
>
> mvn -Pdeb package
>
> ?
>
> I get:
>
> [ERROR] Plugin org.apache.maven.plugins:maven-compiler-plugin:3.1 or one of
> its dependencies could not be resolved: Failed to read artifact descriptor
> for org.apache.maven.plugins:maven-compiler-plugin:jar:3.1: Could not find
> artifact org.apache:apache:pom:13 -> [Help 1]
>
>
> On Tue, Apr 1, 2014 at 11:36 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> > And there is a deb target as well - ah didn't see Mark's email.
> >
> >
> > On Tue, Apr 1, 2014 at 11:36 AM, Patrick Wendell <pwendell@gmail.com>
> > wrote:
> >
> > > Ya there is already some fragmentation here. Maven has some "dist"
> > targets
> > > and there is also ./make-distribution.sh.
> > >
> > >
> > > On Tue, Apr 1, 2014 at 11:31 AM, Mark Hamstra <mark@clearstorydata.com
> > >wrote:
> > >
> > >> A basic Debian package can already be created from the Maven build:
> mvn
> > >> -Pdeb ...
> > >>
> > >>
> > >> On Tue, Apr 1, 2014 at 11:24 AM, Evan Chan <ev@ooyala.com> wrote:
> > >>
> > >> > Also, I understand this is the last week / merge window for 1.0, so
> if
> > >> > folks are interested I'd like to get in a PR quickly.
> > >> >
> > >> > thanks,
> > >> > Evan
> > >> >
> > >> >
> > >> >
> > >> > On Tue, Apr 1, 2014 at 11:24 AM, Evan Chan <ev@ooyala.com> wrote:
> > >> >
> > >> > > Hey folks,
> > >> > >
> > >> > > We are in the middle of creating a Chef recipe for Spark.   As
> part
> > of
> > >> > > that we want to create a Debian package for Spark.
> > >> > >
> > >> > > What do folks think of adding the sbt-package-bin plugin to allow
> > easy
> > >> > > creation of a Spark .deb file?  I believe it adds all dependency
> > jars
> > >> > into
> > >> > > a single lib/ folder, so in some ways it's even easier to manage
> > than
> > >> the
> > >> > > assembly.
> > >> > >
> > >> > > Also I'm not sure if there's an equivalent plugin for Maven.
> > >> > >
> > >> > > thanks,
> > >> > > Evan
> > >> > >
> > >> > >
> > >> > > --
> > >> > > --
> > >> > >  Evan Chan
> > >> > > Staff Engineer
> > >> > > ev@ooyala.com  |
> > >> > >
> > >> > > <http://www.ooyala.com/> <http://www.facebook.com/ooyala><
> > >> > http://www.linkedin.com/company/ooyala><
> http://www.twitter.com/ooyala
> > >
> > >> > >
> > >> > >
> > >> >
> > >> >
> > >> > --
> > >> > --
> > >> > Evan Chan
> > >> > Staff Engineer
> > >> > ev@ooyala.com  |
> > >> >
> > >> > <http://www.ooyala.com/>
> > >> > <http://www.facebook.com/ooyala><
> > http://www.linkedin.com/company/ooyala
> > >> ><
> > >> > http://www.twitter.com/ooyala>
> > >> >
> > >>
> > >
> > >
> >
>
>
>
> --
> --
> Evan Chan
> Staff Engineer
> ev@ooyala.com  |
>
> <http://www.ooyala.com/>
> <http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><
> http://www.twitter.com/ooyala>
>

--001a1134d67ea4a28e04f604d19a--

From dev-return-7197-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr  2 01:03:47 2014
Return-Path: <dev-return-7197-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5E11D10F4F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Apr 2014 01:03:47 +0000 (UTC)
Received: (qmail 97850 invoked by uid 500); 2 Apr 2014 01:03:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97800 invoked by uid 500); 2 Apr 2014 01:03:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 97792 invoked by uid 99); 2 Apr 2014 01:03:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Apr 2014 01:03:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mark@clearstorydata.com designates 209.85.212.173 as permitted sender)
Received: from [209.85.212.173] (HELO mail-wi0-f173.google.com) (209.85.212.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Apr 2014 01:03:41 +0000
Received: by mail-wi0-f173.google.com with SMTP id z2so4418069wiv.12
        for <dev@spark.apache.org>; Tue, 01 Apr 2014 18:03:20 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=4CGRdzPOJhyoZcLwiXCK8DGsVNiOD3nEqax0YFyF7Mo=;
        b=Y1tVyRsuVp0UZznOMq0i5vCgdqwUvMIIjWE7Z6kZa3CHiPZ0z3IA608uSI6/lsPwNu
         1M6STiSOUDQ9gqYnKuwdmOEdsjfDvthwgwIZTUAbgJZgXhpbdjWDQwQyL6Jmvk46iM7W
         I5/3W0upp7tuOvdJqTOW+GRemaVFCevVyNuvGCVkbuArLftPeh+105Wsj9SffsO25Jux
         vjnRL6VVD+nWUIhra5m79ld0azFrIhvWHRr7gaXyjAawOiVTWQLrp0qzHTB3Gd9lhbW7
         y77bb/wQ4f/f1t0iwghRc8nRFGY30uDSDqsY0Wny9d7nDn8jRCMMK1HlTDWBzmTxf5QP
         gCHQ==
X-Gm-Message-State: ALoCoQl8ztcxpL72SnIdU3YknuwKxE4pxgXep5dBGHP9JlYRv62HPis7SCOgZLhgklOtz983YsvO
MIME-Version: 1.0
X-Received: by 10.180.96.200 with SMTP id du8mr24084587wib.43.1396400600087;
 Tue, 01 Apr 2014 18:03:20 -0700 (PDT)
Received: by 10.216.232.69 with HTTP; Tue, 1 Apr 2014 18:03:20 -0700 (PDT)
In-Reply-To: <CAAsvFPkEr3K7XDxcDe1JeOoFQOrOaGJM6KWuYgpPt7DTtBBDkg@mail.gmail.com>
References: <CADWPM3gZCfn3K8saPiPVMzd4S6=84D+BWVybTm5vR-m7WGCzBQ@mail.gmail.com>
	<CADWPM3i=Yo7a6L-m5JR+Te=jjpp0++0g4vUvXsApdBKjA0oJmQ@mail.gmail.com>
	<CAAsvFPmKcH9Pp4x9PfuQOcZLH_mKUw05dBG6TZw+6=qhgazSqw@mail.gmail.com>
	<CABPQxss8qrxz75SHKuxEavz_5=Q8+dLkzZjkQ7WPhAmqtDHgsA@mail.gmail.com>
	<CABPQxsurqik0xZOudS+qRo88FL_i4SCqiS3ysjW-VZFA4z-fvQ@mail.gmail.com>
	<CADWPM3j6-gpCf-TTvFfJy7Qf7RPQ9AYGb4F-VAvG9OPn4tW0UQ@mail.gmail.com>
	<CAAsvFPkEr3K7XDxcDe1JeOoFQOrOaGJM6KWuYgpPt7DTtBBDkg@mail.gmail.com>
Date: Tue, 1 Apr 2014 18:03:20 -0700
Message-ID: <CAAsvFP=L9=RD-iqUnxqh+8fq3hb7Puz32ebhYOnpk2L4pWRcTg@mail.gmail.com>
Subject: Re: sbt-package-bin
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d043bdc0cbb957d04f604d917
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043bdc0cbb957d04f604d917
Content-Type: text/plain; charset=ISO-8859-1

...or at least you could do that if the Maven build wasn't broken right now.


On Tue, Apr 1, 2014 at 6:01 PM, Mark Hamstra <mark@clearstorydata.com>wrote:

> What the "..." is kind of depends on what you're trying to accomplish.
>  You could be setting Hadoop version and other stuff in there, but if you
> go too much beyond a pretty basic build, you're probably also going to have
> to modify the <configuration> of the jdeb plugin in assembly/pom.xml to
> include/exclude just what you want/don't want in the Debian package.
>
> Anyway, a typical build would look something like 'mvn -U -Pdeb
> -DskipTests clean package', after which you should be able to find your
> .deb in assembly/target.
>
>
> On Tue, Apr 1, 2014 at 3:01 PM, Evan Chan <ev@ooyala.com> wrote:
>
>> Mark - sorry, would you mind expanding what the "...." is?
>>
>> Something like
>>
>> mvn -Pdeb package
>>
>> ?
>>
>> I get:
>>
>> [ERROR] Plugin org.apache.maven.plugins:maven-compiler-plugin:3.1 or one
>> of
>> its dependencies could not be resolved: Failed to read artifact descriptor
>> for org.apache.maven.plugins:maven-compiler-plugin:jar:3.1: Could not find
>> artifact org.apache:apache:pom:13 -> [Help 1]
>>
>>
>> On Tue, Apr 1, 2014 at 11:36 AM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>> > And there is a deb target as well - ah didn't see Mark's email.
>> >
>> >
>> > On Tue, Apr 1, 2014 at 11:36 AM, Patrick Wendell <pwendell@gmail.com>
>> > wrote:
>> >
>> > > Ya there is already some fragmentation here. Maven has some "dist"
>> > targets
>> > > and there is also ./make-distribution.sh.
>> > >
>> > >
>> > > On Tue, Apr 1, 2014 at 11:31 AM, Mark Hamstra <
>> mark@clearstorydata.com
>> > >wrote:
>> > >
>> > >> A basic Debian package can already be created from the Maven build:
>> mvn
>> > >> -Pdeb ...
>> > >>
>> > >>
>> > >> On Tue, Apr 1, 2014 at 11:24 AM, Evan Chan <ev@ooyala.com> wrote:
>> > >>
>> > >> > Also, I understand this is the last week / merge window for 1.0,
>> so if
>> > >> > folks are interested I'd like to get in a PR quickly.
>> > >> >
>> > >> > thanks,
>> > >> > Evan
>> > >> >
>> > >> >
>> > >> >
>> > >> > On Tue, Apr 1, 2014 at 11:24 AM, Evan Chan <ev@ooyala.com> wrote:
>> > >> >
>> > >> > > Hey folks,
>> > >> > >
>> > >> > > We are in the middle of creating a Chef recipe for Spark.   As
>> part
>> > of
>> > >> > > that we want to create a Debian package for Spark.
>> > >> > >
>> > >> > > What do folks think of adding the sbt-package-bin plugin to allow
>> > easy
>> > >> > > creation of a Spark .deb file?  I believe it adds all dependency
>> > jars
>> > >> > into
>> > >> > > a single lib/ folder, so in some ways it's even easier to manage
>> > than
>> > >> the
>> > >> > > assembly.
>> > >> > >
>> > >> > > Also I'm not sure if there's an equivalent plugin for Maven.
>> > >> > >
>> > >> > > thanks,
>> > >> > > Evan
>> > >> > >
>> > >> > >
>> > >> > > --
>> > >> > > --
>> > >> > >  Evan Chan
>> > >> > > Staff Engineer
>> > >> > > ev@ooyala.com  |
>> > >> > >
>> > >> > > <http://www.ooyala.com/> <http://www.facebook.com/ooyala><
>> > >> > http://www.linkedin.com/company/ooyala><
>> http://www.twitter.com/ooyala
>> > >
>> > >> > >
>> > >> > >
>> > >> >
>> > >> >
>> > >> > --
>> > >> > --
>> > >> > Evan Chan
>> > >> > Staff Engineer
>> > >> > ev@ooyala.com  |
>> > >> >
>> > >> > <http://www.ooyala.com/>
>> > >> > <http://www.facebook.com/ooyala><
>> > http://www.linkedin.com/company/ooyala
>> > >> ><
>> > >> > http://www.twitter.com/ooyala>
>> > >> >
>> > >>
>> > >
>> > >
>> >
>>
>>
>>
>> --
>> --
>> Evan Chan
>> Staff Engineer
>> ev@ooyala.com  |
>>
>> <http://www.ooyala.com/>
>> <http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><
>> http://www.twitter.com/ooyala>
>>
>
>

--f46d043bdc0cbb957d04f604d917--

From dev-return-7198-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr  2 01:09:12 2014
Return-Path: <dev-return-7198-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9460610F91
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Apr 2014 01:09:12 +0000 (UTC)
Received: (qmail 12446 invoked by uid 500); 2 Apr 2014 01:09:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12400 invoked by uid 500); 2 Apr 2014 01:09:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12392 invoked by uid 99); 2 Apr 2014 01:09:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Apr 2014 01:09:10 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mark@clearstorydata.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Apr 2014 01:09:07 +0000
Received: by mail-wi0-f175.google.com with SMTP id cc10so6173915wib.2
        for <dev@spark.apache.org>; Tue, 01 Apr 2014 18:08:45 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=DXAjcj/dncNOQMljhnHLSmbmrid2/aQT3c5lhe8UgOM=;
        b=IH+givxUDQJXFVuqS+LrG1lpbedGIttRBuSBLcLAmQNxLMPTUMU3yFPdIwAw90DF/u
         aevB4WoLJpg+Kpu0ncJbsvHYc7/mTlkKTrTFAJ42fon4CtpD5D+3dNgr+bPWq6WLCpWd
         o3Qni0+sScHUpePhZYKLoEYLEiQDy3ZvcUUo64EHHfvmnC20BPW/C3ETZeo1M++6KdBT
         yFbmg7tV7VbSLbAx0ihMxY5ct1eePZUa/I3rOi5BL2O14BDzmq3dKnxPrsOcMrXQPUp7
         CXZKp2TZhLdTn6Wwx/IpniZHCLF3WoIzIOrDhNSBRnYVCQCtujONd0wmkYv0DAhwxDNE
         EumA==
X-Gm-Message-State: ALoCoQloh/HonoHp6XiIuo6Eo0XH/kvfF9uqH/l5nPKKQmIeJuGvaXXeFT6KqY81EGFi6LhQgpzV
MIME-Version: 1.0
X-Received: by 10.180.101.230 with SMTP id fj6mr23978009wib.27.1396400925208;
 Tue, 01 Apr 2014 18:08:45 -0700 (PDT)
Received: by 10.216.232.69 with HTTP; Tue, 1 Apr 2014 18:08:45 -0700 (PDT)
In-Reply-To: <CAAsvFP=L9=RD-iqUnxqh+8fq3hb7Puz32ebhYOnpk2L4pWRcTg@mail.gmail.com>
References: <CADWPM3gZCfn3K8saPiPVMzd4S6=84D+BWVybTm5vR-m7WGCzBQ@mail.gmail.com>
	<CADWPM3i=Yo7a6L-m5JR+Te=jjpp0++0g4vUvXsApdBKjA0oJmQ@mail.gmail.com>
	<CAAsvFPmKcH9Pp4x9PfuQOcZLH_mKUw05dBG6TZw+6=qhgazSqw@mail.gmail.com>
	<CABPQxss8qrxz75SHKuxEavz_5=Q8+dLkzZjkQ7WPhAmqtDHgsA@mail.gmail.com>
	<CABPQxsurqik0xZOudS+qRo88FL_i4SCqiS3ysjW-VZFA4z-fvQ@mail.gmail.com>
	<CADWPM3j6-gpCf-TTvFfJy7Qf7RPQ9AYGb4F-VAvG9OPn4tW0UQ@mail.gmail.com>
	<CAAsvFPkEr3K7XDxcDe1JeOoFQOrOaGJM6KWuYgpPt7DTtBBDkg@mail.gmail.com>
	<CAAsvFP=L9=RD-iqUnxqh+8fq3hb7Puz32ebhYOnpk2L4pWRcTg@mail.gmail.com>
Date: Tue, 1 Apr 2014 18:08:45 -0700
Message-ID: <CAAsvFPmhWWgwX4LYXEEiyyo0AfKSjoRFjuMJ1zUUGh7qwEDWAA@mail.gmail.com>
Subject: Re: sbt-package-bin
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d0442879e1c8c2004f604edc0
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0442879e1c8c2004f604edc0
Content-Type: text/plain; charset=ISO-8859-1

Whoops!  Looks like it was just my brain that was broken.


On Tue, Apr 1, 2014 at 6:03 PM, Mark Hamstra <mark@clearstorydata.com>wrote:

> ...or at least you could do that if the Maven build wasn't broken right
> now.
>
>
> On Tue, Apr 1, 2014 at 6:01 PM, Mark Hamstra <mark@clearstorydata.com>wrote:
>
>> What the "..." is kind of depends on what you're trying to accomplish.
>>  You could be setting Hadoop version and other stuff in there, but if you
>> go too much beyond a pretty basic build, you're probably also going to have
>> to modify the <configuration> of the jdeb plugin in assembly/pom.xml to
>> include/exclude just what you want/don't want in the Debian package.
>>
>> Anyway, a typical build would look something like 'mvn -U -Pdeb
>> -DskipTests clean package', after which you should be able to find your
>> .deb in assembly/target.
>>
>>
>> On Tue, Apr 1, 2014 at 3:01 PM, Evan Chan <ev@ooyala.com> wrote:
>>
>>> Mark - sorry, would you mind expanding what the "...." is?
>>>
>>> Something like
>>>
>>> mvn -Pdeb package
>>>
>>> ?
>>>
>>> I get:
>>>
>>> [ERROR] Plugin org.apache.maven.plugins:maven-compiler-plugin:3.1 or one
>>> of
>>> its dependencies could not be resolved: Failed to read artifact
>>> descriptor
>>> for org.apache.maven.plugins:maven-compiler-plugin:jar:3.1: Could not
>>> find
>>> artifact org.apache:apache:pom:13 -> [Help 1]
>>>
>>>
>>> On Tue, Apr 1, 2014 at 11:36 AM, Patrick Wendell <pwendell@gmail.com>
>>> wrote:
>>>
>>> > And there is a deb target as well - ah didn't see Mark's email.
>>> >
>>> >
>>> > On Tue, Apr 1, 2014 at 11:36 AM, Patrick Wendell <pwendell@gmail.com>
>>> > wrote:
>>> >
>>> > > Ya there is already some fragmentation here. Maven has some "dist"
>>> > targets
>>> > > and there is also ./make-distribution.sh.
>>> > >
>>> > >
>>> > > On Tue, Apr 1, 2014 at 11:31 AM, Mark Hamstra <
>>> mark@clearstorydata.com
>>> > >wrote:
>>> > >
>>> > >> A basic Debian package can already be created from the Maven build:
>>> mvn
>>> > >> -Pdeb ...
>>> > >>
>>> > >>
>>> > >> On Tue, Apr 1, 2014 at 11:24 AM, Evan Chan <ev@ooyala.com> wrote:
>>> > >>
>>> > >> > Also, I understand this is the last week / merge window for 1.0,
>>> so if
>>> > >> > folks are interested I'd like to get in a PR quickly.
>>> > >> >
>>> > >> > thanks,
>>> > >> > Evan
>>> > >> >
>>> > >> >
>>> > >> >
>>> > >> > On Tue, Apr 1, 2014 at 11:24 AM, Evan Chan <ev@ooyala.com> wrote:
>>> > >> >
>>> > >> > > Hey folks,
>>> > >> > >
>>> > >> > > We are in the middle of creating a Chef recipe for Spark.   As
>>> part
>>> > of
>>> > >> > > that we want to create a Debian package for Spark.
>>> > >> > >
>>> > >> > > What do folks think of adding the sbt-package-bin plugin to
>>> allow
>>> > easy
>>> > >> > > creation of a Spark .deb file?  I believe it adds all dependency
>>> > jars
>>> > >> > into
>>> > >> > > a single lib/ folder, so in some ways it's even easier to manage
>>> > than
>>> > >> the
>>> > >> > > assembly.
>>> > >> > >
>>> > >> > > Also I'm not sure if there's an equivalent plugin for Maven.
>>> > >> > >
>>> > >> > > thanks,
>>> > >> > > Evan
>>> > >> > >
>>> > >> > >
>>> > >> > > --
>>> > >> > > --
>>> > >> > >  Evan Chan
>>> > >> > > Staff Engineer
>>> > >> > > ev@ooyala.com  |
>>> > >> > >
>>> > >> > > <http://www.ooyala.com/> <http://www.facebook.com/ooyala><
>>> > >> > http://www.linkedin.com/company/ooyala><
>>> http://www.twitter.com/ooyala
>>> > >
>>> > >> > >
>>> > >> > >
>>> > >> >
>>> > >> >
>>> > >> > --
>>> > >> > --
>>> > >> > Evan Chan
>>> > >> > Staff Engineer
>>> > >> > ev@ooyala.com  |
>>> > >> >
>>> > >> > <http://www.ooyala.com/>
>>> > >> > <http://www.facebook.com/ooyala><
>>> > http://www.linkedin.com/company/ooyala
>>> > >> ><
>>> > >> > http://www.twitter.com/ooyala>
>>> > >> >
>>> > >>
>>> > >
>>> > >
>>> >
>>>
>>>
>>>
>>> --
>>> --
>>> Evan Chan
>>> Staff Engineer
>>> ev@ooyala.com  |
>>>
>>> <http://www.ooyala.com/>
>>> <http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala
>>> ><http://www.twitter.com/ooyala>
>>>
>>
>>
>

--f46d0442879e1c8c2004f604edc0--

From dev-return-7199-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr  2 03:15:05 2014
Return-Path: <dev-return-7199-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E090311215
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Apr 2014 03:15:04 +0000 (UTC)
Received: (qmail 79455 invoked by uid 500); 2 Apr 2014 03:15:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79387 invoked by uid 500); 2 Apr 2014 03:15:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79368 invoked by uid 99); 2 Apr 2014 03:15:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Apr 2014 03:15:01 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of leemighdoll@gmail.com designates 209.85.160.177 as permitted sender)
Received: from [209.85.160.177] (HELO mail-yk0-f177.google.com) (209.85.160.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Apr 2014 03:14:56 +0000
Received: by mail-yk0-f177.google.com with SMTP id q200so8280061ykb.22
        for <dev@spark.apache.org>; Tue, 01 Apr 2014 20:14:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:reply-to:sender:in-reply-to:references:date:message-id
         :subject:from:to:content-type;
        bh=NxrLQey+kBOgGajl4YYR3PKqzzv8ffC+wStbkFWlT6I=;
        b=GiVPkPW6dmU8LVxUJN9cjZ5Dog2Gi/Fqc1k8tqCyJ3u0o0AzR9Ar2DDESoJKkBDzhA
         fFnQJKot4XDXw65KFfy1xNITULn2mI8SSgodkxxNy5MlN4i8iXIof0vVFBYHGjYc14qs
         eNf1+76CkHQI3UNfz+Nk62SxTLz3cqVMaQrpuG9nEX21MzMjoBQIIplq8tVnX5wisSAj
         HiPtLMgpbxZFol+1BtNRDaVkTIwfpXo+1VQDh28uVDK4qlCLOVlxakVOpk8FqkWRyuYO
         ZKvvrVPWGFV6XqkI9yZSra/gpMKMPsABCr3VDorB6WjkoEFkgokrqBNs7CX0keEArY99
         Zsbg==
MIME-Version: 1.0
X-Received: by 10.236.53.5 with SMTP id f5mr3191093yhc.53.1396408474295; Tue,
 01 Apr 2014 20:14:34 -0700 (PDT)
Reply-To: lee@underneath.ca
Sender: leemighdoll@gmail.com
Received: by 10.170.78.66 with HTTP; Tue, 1 Apr 2014 20:14:34 -0700 (PDT)
In-Reply-To: <CADWPM3gZCfn3K8saPiPVMzd4S6=84D+BWVybTm5vR-m7WGCzBQ@mail.gmail.com>
References: <CADWPM3gZCfn3K8saPiPVMzd4S6=84D+BWVybTm5vR-m7WGCzBQ@mail.gmail.com>
Date: Tue, 1 Apr 2014 20:14:34 -0700
X-Google-Sender-Auth: pavtPqViEKAmsiV_GgmhoKeb2X0
Message-ID: <CAPif6rX87HazZK7-a38thJGB3PNUtBfZX6jvgsSJxbRmM8L0Aw@mail.gmail.com>
Subject: Re: sbt-package-bin
From: Lee Mighdoll <lee@underneath.ca>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0115e9f61263a204f606af93
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0115e9f61263a204f606af93
Content-Type: text/plain; charset=ISO-8859-1

> What do folks think of adding the sbt-package-bin plugin to allow easy
> creation of a Spark .deb file?  I believe it adds all dependency jars into
> a single lib/ folder, so in some ways it's even easier to manage than the
> assembly.
>

You might also check out the
sbt-native-packager<https://github.com/sbt/sbt-native-packager>.


Cheers,
Lee

--089e0115e9f61263a204f606af93--

From dev-return-7200-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr  2 13:47:04 2014
Return-Path: <dev-return-7200-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4E7A110632
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Apr 2014 13:47:04 +0000 (UTC)
Received: (qmail 2184 invoked by uid 500); 2 Apr 2014 13:47:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 872 invoked by uid 500); 2 Apr 2014 13:46:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 481 invoked by uid 99); 2 Apr 2014 13:46:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Apr 2014 13:46:56 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [216.109.114.251] (HELO nm49-vm10.bullet.mail.bf1.yahoo.com) (216.109.114.251)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Apr 2014 13:46:50 +0000
Received: from [98.139.212.153] by nm49.bullet.mail.bf1.yahoo.com with NNFMP; 02 Apr 2014 13:46:28 -0000
Received: from [98.139.212.238] by tm10.bullet.mail.bf1.yahoo.com with NNFMP; 02 Apr 2014 13:46:28 -0000
Received: from [127.0.0.1] by omp1047.mail.bf1.yahoo.com with NNFMP; 02 Apr 2014 13:46:28 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 633163.87227.bm@omp1047.mail.bf1.yahoo.com
Received: (qmail 74710 invoked by uid 60001); 2 Apr 2014 13:46:28 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.com; s=s1024; t=1396446388; bh=GnjytDbYTMTVWH6FEMjvcZkjX5mf/u8CSEdLh6oT2cU=; h=X-YMail-OSG:Received:X-Rocket-MIMEInfo:X-Mailer:References:Message-ID:Date:From:Reply-To:Subject:To:Cc:In-Reply-To:MIME-Version:Content-Type; b=wxA17Ad1lMOxrgY00K4XsIzxupi7C+zUYFHzwp7N2QOhMonifcGcZQOhxoG5U/nKNuCRRHbamJ+lgKAN73K4KYU5iBQA1dC1u/sNrkcCDcs3eX9KzEiqg76iNzct7gvIv7xmLylC+AP+H19GmaQYc7gtwNslljHshGW4roJK/Hw=
DomainKey-Signature:a=rsa-sha1; q=dns; c=nofws;
  s=s1024; d=yahoo.com;
  h=X-YMail-OSG:Received:X-Rocket-MIMEInfo:X-Mailer:References:Message-ID:Date:From:Reply-To:Subject:To:Cc:In-Reply-To:MIME-Version:Content-Type;
  b=CVY3xH4f8iwuwIyNTbvKlIF6LdSzFtfBEpeCGNZOO37J1GSb5IfZoqbYwl067Su517pOVwY8viRLM/crSoQVubP5tnCsUqQwvweL+79vklKeMTO0F62vLuPL49RzptBf1j5G3znLAvP5hie1lDBvwPbzQMm6uK2nq6t1J3y7oGA=;
X-YMail-OSG: vT1YuQUVM1lDhh6YcTNNaE3djAfKpXVGa7t8wspDmodQJrf
 WasyLozpBDzCV9aFheQo3Sr3v_6sZzBz5tOpHz0ldv2fHIm4J7rPh._QlBes
 Af5Ki2KP6lLcoqLO24Ej7JkuBAtH4vdJmO.zH5tqmSAk7bZ1DyHgfFmxNp1P
 bEKiA3RdNbNdO_LY6xb7hZIYMhEIa19lTerrGCLlYq_Py1M1bpqRU6EMmuUi
 i9TGlMHpzTfJwnJm4YgLdiO6wT3vE_Muvemq5ztVq1SS7CKod9TwE4IKu7qj
 Ts87SlUF851oMv5OD4UmwkE8f4VCHKWnfevxvHWKQafUZx252hVQN9ngjpoR
 I7PY_AH7iCY.qvAKjHLuRb_sYuS1lbEBorsZsOM2ATxa9YiWXTuT_W8nUXc1
 CaEy70x1gSgNZ5.nHfT8TwjD_NL6ymSNbUDKh7n3hiNu_QfvDIngqoXZMrrJ
 9yX2K8Scuqzm257yXZT1fi929id.rXRl7n1QVrl3yX9Wrcia9pUMj0LrkYZK
 FWwgjerNEEfKLb2iXhqdk1rF5dhVtDbgVmDT66gKZRO_P1kuIazMlRJvcApt
 6rHWYniW6jWwM3lqnaxgdnzdBIBVvi3MgMqRKjmHi8Ns8h3h8k.I76.Nps8y
 mFXLinIh7pSOg8xLPj9NcYII8EWEc6pfZDZ0LbRz6ttlG3X5ZaizaFlZyPSn
 fVCiE1IPuIaVRjUPqWFiGUg.1GcEzCdkx5jIekqvZg_iJCwXxV1xEcnxYXLO
 5ow--
Received: from [204.11.79.50] by web140103.mail.bf1.yahoo.com via HTTP; Wed, 02 Apr 2014 06:46:28 PDT
X-Rocket-MIMEInfo: 002.001,Tm90ZSBJJ20gKzEgd2l0aCB0aGUgZG9jIGNoYW5nZWQgdG8gdGVsbCB1c2VycyB0b8KgZXhwb3J0IFNQQVJLX1lBUk5fTU9ERT10cnVlIGJlZm9yZSB1c2luZyBzcGFyay1zaGVsbCBvbiB5YXJuLgoKSSB0ZXN0ZWQgaXQgb24gYm90aCBoYWRvb3AgMC4yMyBhbmQgMi4zIGNsdXN0ZXJzIHVzaW5nIHNlY3VyZSBoZGZzIG9uIGxpbnV4LgoKVG9tCk9uIFR1ZXNkYXksIEFwcmlsIDEsIDIwMTQgMTo0NCBQTSwgVG9tIEdyYXZlcyA8dGdyYXZlc19jc0B5YWhvby5jb20.IHdyb3RlOgogCk5vIG9uZSBlbHNlIGhhcyABMAEBAQE-
X-Mailer: YahooMailWebService/0.8.182.648
References: <CAMwrk0mhqhBf2ZNRFiC4LgWjLb2A7fw3nCo8TwAZvadNinWmaA@mail.gmail.com>	<E75FBF52-0373-44E2-BCC2-BCB8445C995C@gmail.com>	<CAPdPcW2bN3yTLgFRaf=5Sm05mJQ71ufeMv8dfmEsEQLsEPsHGg@mail.gmail.com>	<CABPQxssGF-rOYE_oFiR2CArbMt-auvTBUT4+Yck79+Wk4dzYcA@mail.gmail.com>	<1396278778.50575.YahooMailNeo@web140101.mail.bf1.yahoo.com>	<CABPQxss_myw8XYRE1CKHw8SgTqx=3+Eb+2-OzzHi4iV5m9wqPw@mail.gmail.com>	<CAMwrk0mVrYGnon0JQvMpFUz7SPS_ZcxG53t95M2F_GowTBP9UQ@mail.gmail.com>	<1396362508.20704.YahooMailNeo@web140102.mail.bf1.yahoo.com> <CABPQxsv92x+K+6T=MbFDWdMB6OHt-8qqyNeUXcUrk18Y09v_Eg@mail.gmail.com> <1396377840.28297.YahooMailNeo@web140105.mail.bf1.yahoo.com>
Message-ID: <1396446388.15960.YahooMailNeo@web140103.mail.bf1.yahoo.com>
Date: Wed, 2 Apr 2014 06:46:28 -0700 (PDT)
From: Tom Graves <tgraves_cs@yahoo.com>
Reply-To: Tom Graves <tgraves_cs@yahoo.com>
Subject: Re: [VOTE] Release Apache Spark 0.9.1 (RC3)
To: "dev@spark.apache.org" <dev@spark.apache.org>,
  Tom Graves <tgraves_cs@yahoo.com>
Cc: Tathagata Das <tathagata.das1565@gmail.com>
In-Reply-To: <1396377840.28297.YahooMailNeo@web140105.mail.bf1.yahoo.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="-916770207-2144317840-1396446388=:15960"
X-Virus-Checked: Checked by ClamAV on apache.org

---916770207-2144317840-1396446388=:15960
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

Note I'm +1 with the doc changed to tell users to=A0export SPARK_YARN_MODE=
=3Dtrue before using spark-shell on yarn.=0A=0AI tested it on both hadoop 0=
.23 and 2.3 clusters using secure hdfs on linux.=0A=0ATom=0AOn Tuesday, Apr=
il 1, 2014 1:44 PM, Tom Graves <tgraves_cs@yahoo.com> wrote:=0A =0ANo one e=
lse has reported seeing the issue so I think documenting it is fine. =A0=0A=
=0ATom=0A=0A=0A=0AOn Tuesday, April 1, 2014 1:40 PM, Patrick Wendell <pwend=
ell@gmail.com> wrote:=0A=0ATom,=0A=0AGiven this is a pretty straightforward=
 workaround, what do yo think about=0Athe following course of action:=0A=0A=
(a) We can put the workaround in the docs for 0.9.1. We don't need to do a=
=0Anew RC/vote for this since we can update the published docs independentl=
y.=0A=0A(b) We try to get a fix in for this into the 0.9 branch so it can e=
nd up in=0A0.9.2. But this takes the fix off the critical path for this rel=
ease.=0A=0A- Patrick=0A=0A=0A=0AOn Tue, Apr 1, 2014 at 7:28 AM, Tom Graves =
<tgraves_cs@yahoo.com> wrote:=0A=0A> Thanks for extending the voting.=0A>=
=0A> Unfortunately I've found an issue with the spark-shell in yarn-client=
=0A> mode.=A0 It doesn't work with secure HDFS unless you=0A> export SPARK_=
YARN_MODE=3Dtrue before starting the shell, or if you happen to=0A> do some=
thing immediately with HDFS.=A0 If you wait for the connection to the=0A> n=
amenode to timeout it will fail.=0A>=0A> I think it was actually this way i=
n the 0.9 release also so I thought I=0A> would send this and get peoples f=
eedback to see if you want it fixed?=0A>=0A> Another option would be to doc=
ument that you have to export=0A> SPARK_YARN_MODE=3Dtrue for the shell.=A0=
=A0 The fix actually went in with the=0A> authentication changes I made in =
master but I never realized that change=0A> needed to apply to 0.9.=0A>=0A>=
=0A> https://github.com/apache/spark/commit/7edbea41b43e0dc11a2de156be220db=
8b7952d01#diff-0ae5b834ce90ec37c19af35aa7a5e1a0=0A>=0A> See the SparkILoop =
diff.=0A>=0A> Tom=0A>=0A>=0A> On Monday, March 31, 2014 1:33 PM, Tathagata =
Das <=0A> tathagata.das1565@gmail.com> wrote:=0A>=0A> Yes, lets extend the =
vote for two more days from now. So the vote is open=0A> till Wednesday, Ap=
ril 02, at 20:00 UTC=0A>=0A> On that note, my +1=0A>=0A>=0A> TD=0A>=0A>=0A>=
=0A>=0A>=0A>=0A> On Mon, Mar 31, 2014 at 9:57 AM, Patrick Wendell <pwendell=
@gmail.com>=0A> wrote:=0A>=0A> Yeah good point. Let's just extend this vote=
 another few days?=0A> >=0A> >=0A> >=0A> >On Mon, Mar 31, 2014 at 8:12 AM, =
Tom Graves <tgraves_cs@yahoo.com> wrote:=0A> >=0A> >> I should probably pul=
l this off into another thread, but going forward=0A> can=0A> >> we try to =
not have the release votes end on a weekend? Since we only=0A> seem=0A> >> =
to give 3 days, it makes it really hard for anyone who is offline for=0A> t=
he=0A> >> weekend to try it out.=A0=A0 Either that or extend the voting for=
 more then=0A> 3=0A> >> days.=0A> >>=0A> >> Tom=0A> >> On Monday, March 31,=
 2014 12:50 AM, Patrick Wendell <pwendell@gmail.com=0A> >=0A> >> wrote:=0A>=
 >>=0A> >> TD - I downloaded and did some local testing. Looks good to me!=
=0A> >>=0A> >> +1=0A> >>=0A> >> You should cast your own vote - at that poi=
nt it's enough to pass.=0A> >>=0A> >> - Patrick=0A> >>=0A> >>=0A> >>=0A> >>=
 On Sun, Mar 30, 2014 at 9:47 PM, prabeesh k <prabsmails@gmail.com>=0A> wro=
te:=0A> >>=0A> >> > +1=0A> >> > tested on Ubuntu12.04 64bit=0A> >> >=0A> >>=
 >=0A> >> > On Mon, Mar 31, 2014 at 3:56 AM, Matei Zaharia <=0A> matei.zaha=
ria@gmail.com=0A> >> > >wrote:=0A> >> >=0A> >> > > +1 tested on Mac OS X.=
=0A> >> > >=0A> >> > > Matei=0A> >> > >=0A> >> > > On Mar 27, 2014, at 1:32=
 AM, Tathagata Das <=0A> >> tathagata.das1565@gmail.com>=0A> >> > > wrote:=
=0A> >> > >=0A> >> > > > Please vote on releasing the following candidate a=
s Apache Spark=0A> >> > version=0A> >> > > 0.9.1=0A> >> > > >=0A> >> > > > =
A draft of the release notes along with the CHANGES.txt file is=0A> >> > > =
> attached to this e-mail.=0A> >> > > >=0A> >> > > > The tag to be voted on=
 is v0.9.1-rc3 (commit 4c43182b):=0A> >> > > >=0A> >> > >=0A> >> >=0A> >>=
=0A> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D4=
c43182b6d1b0b7717423f386c0214fe93073208=0A> >> > > >=0A> >> > > > The relea=
se files, including signatures, digests, etc. can be=0A> found=0A> >> at:=
=0A> >> > > > http://people.apache.org/~tdas/spark-0.9.1-rc3/=0A> >> > > >=
=0A> >> > > > Release artifacts are signed with the following key:=0A> >> >=
 > > https://people.apache.org/keys/committer/tdas.asc=0A> >> > > >=0A> >> =
> > > The staging repository for this release can be found at:=0A> >> > > >=
=0A> >> >=0A> https://repository.apache.org/content/repositories/orgapaches=
park-1009/=0A> >> > > >=0A> >> > > > The documentation corresponding to thi=
s release can be found at:=0A> >> > > > http://people.apache.org/~tdas/spar=
k-0.9.1-rc3-docs/=0A> >> > > >=0A> >> > > > Please vote on releasing this p=
ackage as Apache Spark 0.9.1!=0A> >> > > >=0A> >> > > > The vote is open un=
til Sunday, March 30, at 10:00 UTC and passes=0A> if=0A> >> > > > a majorit=
y of at least 3 +1 PMC votes are cast.=0A> >> > > >=0A> >> > > > [ ] +1 Rel=
ease this package as Apache Spark 0.9.1=0A> >> > > > [ ] -1 Do not release =
this package because ...=0A> >> > > >=0A> >> > > > To learn more about Apac=
he Spark, please see=0A> >> > > > http://spark.apache.org/=0A> >> > > > <CH=
ANGES.txt><RELEASE_NOTES.txt>=0A> >> > >=0A> >> > >=0A> >> >=0A> >>=0A> >=
=0A>
---916770207-2144317840-1396446388=:15960--

From dev-return-7201-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr  2 17:22:55 2014
Return-Path: <dev-return-7201-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A977410F91
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Apr 2014 17:22:55 +0000 (UTC)
Received: (qmail 32650 invoked by uid 500); 2 Apr 2014 17:22:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32165 invoked by uid 500); 2 Apr 2014 17:22:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32141 invoked by uid 99); 2 Apr 2014 17:22:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Apr 2014 17:22:49 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_IMAGE_ONLY_28,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ev@ooyala.com designates 209.85.160.170 as permitted sender)
Received: from [209.85.160.170] (HELO mail-yk0-f170.google.com) (209.85.160.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Apr 2014 17:22:45 +0000
Received: by mail-yk0-f170.google.com with SMTP id 9so478072ykp.29
        for <dev@spark.apache.org>; Wed, 02 Apr 2014 10:22:25 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=ooyala.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=lWNaEKzu5q0CLOCZxGA7RDufysdkb1zGKd5yKdHxBkc=;
        b=eTns/lyBEpU6w5Sm6IZbekLaB02YPhPK0jyFGMfYrTHfzfaE3AzgQ29+oEzqT12nv8
         Qta9yaVG8arT10GxRHcSqZV5mi/KpVyqVgjfkWwChVH//QMQ91A1bgFZ49b+TrJL6GI8
         2JDcvEzG3vN2dPbR0IST7aXZgHvmIai7olkgI=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=lWNaEKzu5q0CLOCZxGA7RDufysdkb1zGKd5yKdHxBkc=;
        b=hMDpoq6i2ZD/tzwkVSQKf36mqHrV6ArYmxXUo3/CABuI99j683plVGJnQKBOtZY8RR
         B5mevCS48XKR/PvYAk8yAA2A9K8fguREEDecA9nLOJB3hThg6jmTLGUGG1JzsX1lIfnB
         81gK050O2OBGD+YnB6mVr7nlYuI7rTbjHqYMpN0l2M2uSpFEjnFYK8cCLfEvNWR3/5wJ
         8Mequ4r4ANAyPwN4nb3f5n9CJtzDCoAyCC0DoJu6G708xDI/h/TkINT2W3pb6Ht8Twh3
         xVhQQzTEDRr/HNqXAxkO73lPg+PidDeb2M0GeBC7rxKvshFyZFgYB0RA+pO/BGRaiFB+
         DB0Q==
X-Gm-Message-State: ALoCoQlWho1Pa5piUiR4PixFJq50RvCkfaf2x768EWQhPo25Wb8THaHugitr/qCEzuYV73RY5l0E
MIME-Version: 1.0
X-Received: by 10.236.113.69 with SMTP id z45mr2466983yhg.0.1396459345002;
 Wed, 02 Apr 2014 10:22:25 -0700 (PDT)
Received: by 10.170.120.87 with HTTP; Wed, 2 Apr 2014 10:22:24 -0700 (PDT)
In-Reply-To: <CAPif6rX87HazZK7-a38thJGB3PNUtBfZX6jvgsSJxbRmM8L0Aw@mail.gmail.com>
References: <CADWPM3gZCfn3K8saPiPVMzd4S6=84D+BWVybTm5vR-m7WGCzBQ@mail.gmail.com>
	<CAPif6rX87HazZK7-a38thJGB3PNUtBfZX6jvgsSJxbRmM8L0Aw@mail.gmail.com>
Date: Wed, 2 Apr 2014 10:22:24 -0700
Message-ID: <CADWPM3iSrkPrAM_7yqe2uhSgeEZsej=ddC9KdEd5pTn2pcUGgQ@mail.gmail.com>
Subject: Re: sbt-package-bin
From: Evan Chan <ev@ooyala.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>, lee@underneath.ca
Content-Type: multipart/alternative; boundary=20cf3010eb1133d40c04f6128714
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf3010eb1133d40c04f6128714
Content-Type: text/plain; charset=ISO-8859-1

Lee, sorry, I actually meant exactly that, sbt-native-packager.


On Tue, Apr 1, 2014 at 8:14 PM, Lee Mighdoll <lee@underneath.ca> wrote:

> > What do folks think of adding the sbt-package-bin plugin to allow easy
> > creation of a Spark .deb file?  I believe it adds all dependency jars
> into
> > a single lib/ folder, so in some ways it's even easier to manage than the
> > assembly.
> >
>
> You might also check out the
> sbt-native-packager<https://github.com/sbt/sbt-native-packager>.
>
>
> Cheers,
> Lee
>



-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>

--20cf3010eb1133d40c04f6128714--

From dev-return-7202-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr  2 17:26:14 2014
Return-Path: <dev-return-7202-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2960B10FB7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Apr 2014 17:26:14 +0000 (UTC)
Received: (qmail 39037 invoked by uid 500); 2 Apr 2014 17:26:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38617 invoked by uid 500); 2 Apr 2014 17:26:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38562 invoked by uid 99); 2 Apr 2014 17:26:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Apr 2014 17:26:10 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=HTML_IMAGE_ONLY_20,HTML_MESSAGE,HTML_SHORT_LINK_IMG_3,RCVD_IN_DNSWL_LOW,SPF_PASS,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ev@ooyala.com designates 209.85.160.174 as permitted sender)
Received: from [209.85.160.174] (HELO mail-yk0-f174.google.com) (209.85.160.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Apr 2014 17:26:06 +0000
Received: by mail-yk0-f174.google.com with SMTP id 20so478552yks.5
        for <dev@spark.apache.org>; Wed, 02 Apr 2014 10:25:46 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=ooyala.com; s=google;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=zYoQXGBiYWIGUypVIuXdCZQGIaSXcejEYYoymQKpwro=;
        b=TIrqhSA+Ys9h0IzpsntTVjWEOwN88Vo3EvCnPowQcJF4zmCf+02FzUTU2mHNZ9Lbf+
         Xh2PCdXnhhKCL325Z7/agjMjtASzz7wuvh5tDbJHMFuRtLL+4A9CHsFTwGJUUkMrH4v+
         wtxaH2rC8zS6yGdXLbg9k3vKL48V1IpVL3Bso=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=zYoQXGBiYWIGUypVIuXdCZQGIaSXcejEYYoymQKpwro=;
        b=OBPjd9Ir4o6+4ZUBFwVmbJWa1KahOBdcodAu3CuMYjy5l9aOaqp7+kx8X6YygW66q7
         wXSIiQJE+4Ks+c40LSYY7CrhkyzSOUHnOGmQ+RXoCyGfgXha/wJ2js2xHt/xvwBYb0iX
         oBrAOsmEosNUa5el9u2uRB4BTBuO7gVP+pOK9t1QYhQXdD22ygbXwUK4pf3Y38ISb2Ua
         UBoytccAis1eVUkOEfAbYiF+DT0KxneOrsGWbfa6A6ej+a2Od8QBfUzGUcBS5VitcyLy
         3D9pgEJRW44Y5A9mdO+frA8kxScuFnrJdQTGsRSXp4TQNahl1qKrNJgsSH7uDzHOQzIQ
         I/Kg==
X-Gm-Message-State: ALoCoQkBur6iCeViJnz7uHpEVEe5ZTiMBVJ0A76AXA6cqv0QZhNBx1ST7ydhtTGNP00fgIhMw9an
MIME-Version: 1.0
X-Received: by 10.236.23.168 with SMTP id v28mr1885354yhv.159.1396459545973;
 Wed, 02 Apr 2014 10:25:45 -0700 (PDT)
Received: by 10.170.120.87 with HTTP; Wed, 2 Apr 2014 10:25:45 -0700 (PDT)
Date: Wed, 2 Apr 2014 10:25:45 -0700
Message-ID: <CADWPM3ghKr+A3K3aY-VNnsdCZeujOdXSSTGJKgCpQPkxYM-xyg@mail.gmail.com>
Subject: Would anyone mind having a quick look at PR#288?
From: Evan Chan <ev@ooyala.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e013a0cec2e6b3d04f61293b7
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013a0cec2e6b3d04f61293b7
Content-Type: text/plain; charset=ISO-8859-1

https://github.com/apache/spark/pull/288

It's for fixing SPARK-1154, which would help Spark be a better citizen for
most deploys, and should be really small and easy to review.

thanks,
Evan


-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>

--089e013a0cec2e6b3d04f61293b7--

From dev-return-7203-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr  2 19:20:58 2014
Return-Path: <dev-return-7203-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8381B10659
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Apr 2014 19:20:58 +0000 (UTC)
Received: (qmail 93033 invoked by uid 500); 2 Apr 2014 19:20:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92667 invoked by uid 500); 2 Apr 2014 19:20:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92654 invoked by uid 99); 2 Apr 2014 19:20:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Apr 2014 19:20:55 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.52 as permitted sender)
Received: from [209.85.219.52] (HELO mail-oa0-f52.google.com) (209.85.219.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Apr 2014 19:20:51 +0000
Received: by mail-oa0-f52.google.com with SMTP id l6so797929oag.25
        for <dev@spark.apache.org>; Wed, 02 Apr 2014 12:20:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=TuXdR0gVbls2DHQDSj4xU0LMghjbiFSHhMyzKlLYSUc=;
        b=0a2NxEu30EDYHSbl6o/o1PYQpYfsehkfG4jzDOJavQe74LYwuiHw7zeMolSVz3g304
         4BXKTm8+0+KtzeyNkTC91dsZGFuAn7C2T+jfX/LP/slOqky0W4DtBb1zHm4+PbLNFF/a
         n1KWUWlTaCp7fwv+PzOcTuZb7x4wvLq/G3iUqc9G3yTnMKPjl2iGu5N7Yj0a1kL/1t//
         tl3MJST/jam6BTXdrZmuREu/aHBrzMRJ6koF0b04SyTCXpfy9L3ACqJEsSEKrgmyqpym
         8YxaZowG4cAW03pb5WC2WHkKVKOpXNAj+EOGyr3B0eUte8uiWM1MUlu1XsevEYBNmk/i
         UJBQ==
MIME-Version: 1.0
X-Received: by 10.60.54.38 with SMTP id g6mr1386840oep.79.1396466429109; Wed,
 02 Apr 2014 12:20:29 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Wed, 2 Apr 2014 12:20:29 -0700 (PDT)
In-Reply-To: <CADWPM3ghKr+A3K3aY-VNnsdCZeujOdXSSTGJKgCpQPkxYM-xyg@mail.gmail.com>
References: <CADWPM3ghKr+A3K3aY-VNnsdCZeujOdXSSTGJKgCpQPkxYM-xyg@mail.gmail.com>
Date: Wed, 2 Apr 2014 12:20:29 -0700
Message-ID: <CABPQxsvjVegBX4CD-Pqt1E7BLQm45nZfL=e0znQvtAKwdc27hw@mail.gmail.com>
Subject: Re: Would anyone mind having a quick look at PR#288?
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0112cf387612de04f6142d1f
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0112cf387612de04f6142d1f
Content-Type: text/plain; charset=ISO-8859-1

Hey Evan,

Ya thanks this is a pretty small patch. Should definitely be do-able for
1.0.

- Patrick


On Wed, Apr 2, 2014 at 10:25 AM, Evan Chan <ev@ooyala.com> wrote:

> https://github.com/apache/spark/pull/288
>
> It's for fixing SPARK-1154, which would help Spark be a better citizen for
> most deploys, and should be really small and easy to review.
>
> thanks,
> Evan
>
>
> --
> --
> Evan Chan
> Staff Engineer
> ev@ooyala.com  |
>
> <http://www.ooyala.com/>
> <http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><
> http://www.twitter.com/ooyala>
>

--089e0112cf387612de04f6142d1f--

From dev-return-7204-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr  3 13:56:48 2014
Return-Path: <dev-return-7204-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A5C1B1020A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Apr 2014 13:56:48 +0000 (UTC)
Received: (qmail 26035 invoked by uid 500); 3 Apr 2014 13:56:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25630 invoked by uid 500); 3 Apr 2014 13:56:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25621 invoked by uid 99); 3 Apr 2014 13:56:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Apr 2014 13:56:43 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of zsh912006@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Apr 2014 13:56:37 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <zsh912006@gmail.com>)
	id 1WVi7t-0003hg-8C
	for dev@spark.incubator.apache.org; Thu, 03 Apr 2014 06:56:17 -0700
Date: Thu, 3 Apr 2014 06:56:17 -0700 (PDT)
From: Dan <zsh912006@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1396533377184-6192.post@n3.nabble.com>
In-Reply-To: <9B0B13E719CD4E97BC2DA9C044E263EA@gmail.com>
References: <CAF_91RFE==O6846c9Gypg-=R=MRfJSEJPWGJOJ_tCXXkhDvRTA@mail.gmail.com> <9B0B13E719CD4E97BC2DA9C044E263EA@gmail.com>
Subject: Re: The difference between driver and master in Spark
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org


Can I think driver as client? Driver and master can be located in a single
machine or different machines, right? 

Thanks,
Dan 



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/The-difference-between-driver-and-master-in-Spark-tp6158p6192.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7205-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr  3 14:01:19 2014
Return-Path: <dev-return-7205-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8E9E710265
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Apr 2014 14:01:19 +0000 (UTC)
Received: (qmail 40725 invoked by uid 500); 3 Apr 2014 14:01:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40239 invoked by uid 500); 3 Apr 2014 14:01:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40125 invoked by uid 99); 3 Apr 2014 14:01:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Apr 2014 14:01:14 +0000
X-ASF-Spam-Status: No, hits=-3.7 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of junluan.xia@intel.com designates 134.134.136.20 as permitted sender)
Received: from [134.134.136.20] (HELO mga02.intel.com) (134.134.136.20)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Apr 2014 14:01:09 +0000
Received: from orsmga002.jf.intel.com ([10.7.209.21])
  by orsmga101.jf.intel.com with ESMTP; 03 Apr 2014 07:00:44 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="4.97,787,1389772800"; 
   d="scan'208";a="514105604"
Received: from fmsmsx104.amr.corp.intel.com ([10.19.9.35])
  by orsmga002.jf.intel.com with ESMTP; 03 Apr 2014 07:00:18 -0700
Received: from fmsmsx118.amr.corp.intel.com (10.18.116.18) by
 FMSMSX104.amr.corp.intel.com (10.19.9.35) with Microsoft SMTP Server (TLS) id
 14.3.123.3; Thu, 3 Apr 2014 07:00:17 -0700
Received: from shsmsx102.ccr.corp.intel.com (10.239.4.154) by
 fmsmsx118.amr.corp.intel.com (10.18.116.18) with Microsoft SMTP Server (TLS)
 id 14.3.123.3; Thu, 3 Apr 2014 07:00:17 -0700
Received: from shsmsx104.ccr.corp.intel.com ([169.254.5.158]) by
 shsmsx102.ccr.corp.intel.com ([169.254.2.188]) with mapi id 14.03.0123.003;
 Thu, 3 Apr 2014 22:00:16 +0800
From: "Xia, Junluan" <junluan.xia@intel.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>,
	"dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Subject: RE: The difference between driver and master in Spark
Thread-Topic: The difference between driver and master in Spark
Thread-Index: AQHPT0S3amh4OOmTik6T3MzadJIt95r/6zYQ
Date: Thu, 3 Apr 2014 14:00:15 +0000
Message-ID: <7841A4E0A9C8784C9D8B07DF5E48F0A8116B4620@SHSMSX104.ccr.corp.intel.com>
References: <CAF_91RFE==O6846c9Gypg-=R=MRfJSEJPWGJOJ_tCXXkhDvRTA@mail.gmail.com>
 <9B0B13E719CD4E97BC2DA9C044E263EA@gmail.com>
 <1396533377184-6192.post@n3.nabble.com>
In-Reply-To: <1396533377184-6192.post@n3.nabble.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Yes, driver is a client. Either single or different machine will be OK for =
driver and master

-----Original Message-----
From: Dan [mailto:zsh912006@gmail.com]=20
Sent: Thursday, April 03, 2014 9:56 PM
To: dev@spark.incubator.apache.org
Subject: Re: The difference between driver and master in Spark


Can I think driver as client? Driver and master can be located in a single =
machine or different machines, right?=20

Thanks,
Dan=20



--
View this message in context: http://apache-spark-developers-list.1001551.n=
3.nabble.com/The-difference-between-driver-and-master-in-Spark-tp6158p6192.=
html
Sent from the Apache Spark Developers List mailing list archive at Nabble.c=
om.

From dev-return-7206-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr  3 14:01:24 2014
Return-Path: <dev-return-7206-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ECD7610267
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Apr 2014 14:01:23 +0000 (UTC)
Received: (qmail 41035 invoked by uid 500); 3 Apr 2014 14:01:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40684 invoked by uid 500); 3 Apr 2014 14:01:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40124 invoked by uid 99); 3 Apr 2014 14:01:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Apr 2014 14:01:14 +0000
X-ASF-Spam-Status: No, hits=-3.7 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of junluan.xia@intel.com designates 134.134.136.20 as permitted sender)
Received: from [134.134.136.20] (HELO mga02.intel.com) (134.134.136.20)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Apr 2014 14:01:09 +0000
Received: from orsmga002.jf.intel.com ([10.7.209.21])
  by orsmga101.jf.intel.com with ESMTP; 03 Apr 2014 07:00:44 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="4.97,787,1389772800"; 
   d="scan'208";a="514105604"
Received: from fmsmsx104.amr.corp.intel.com ([10.19.9.35])
  by orsmga002.jf.intel.com with ESMTP; 03 Apr 2014 07:00:18 -0700
Received: from fmsmsx118.amr.corp.intel.com (10.18.116.18) by
 FMSMSX104.amr.corp.intel.com (10.19.9.35) with Microsoft SMTP Server (TLS) id
 14.3.123.3; Thu, 3 Apr 2014 07:00:17 -0700
Received: from shsmsx102.ccr.corp.intel.com (10.239.4.154) by
 fmsmsx118.amr.corp.intel.com (10.18.116.18) with Microsoft SMTP Server (TLS)
 id 14.3.123.3; Thu, 3 Apr 2014 07:00:17 -0700
Received: from shsmsx104.ccr.corp.intel.com ([169.254.5.158]) by
 shsmsx102.ccr.corp.intel.com ([169.254.2.188]) with mapi id 14.03.0123.003;
 Thu, 3 Apr 2014 22:00:16 +0800
From: "Xia, Junluan" <junluan.xia@intel.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>,
	"dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Subject: RE: The difference between driver and master in Spark
Thread-Topic: The difference between driver and master in Spark
Thread-Index: AQHPT0S3amh4OOmTik6T3MzadJIt95r/6zYQ
Date: Thu, 3 Apr 2014 14:00:15 +0000
Message-ID: <7841A4E0A9C8784C9D8B07DF5E48F0A8116B4620@SHSMSX104.ccr.corp.intel.com>
References: <CAF_91RFE==O6846c9Gypg-=R=MRfJSEJPWGJOJ_tCXXkhDvRTA@mail.gmail.com>
 <9B0B13E719CD4E97BC2DA9C044E263EA@gmail.com>
 <1396533377184-6192.post@n3.nabble.com>
In-Reply-To: <1396533377184-6192.post@n3.nabble.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Yes, driver is a client. Either single or different machine will be OK for =
driver and master

-----Original Message-----
From: Dan [mailto:zsh912006@gmail.com]=20
Sent: Thursday, April 03, 2014 9:56 PM
To: dev@spark.incubator.apache.org
Subject: Re: The difference between driver and master in Spark


Can I think driver as client? Driver and master can be located in a single =
machine or different machines, right?=20

Thanks,
Dan=20



--
View this message in context: http://apache-spark-developers-list.1001551.n=
3.nabble.com/The-difference-between-driver-and-master-in-Spark-tp6158p6192.=
html
Sent from the Apache Spark Developers List mailing list archive at Nabble.c=
om.

From dev-return-7207-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr  3 19:36:11 2014
Return-Path: <dev-return-7207-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 564E611094
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Apr 2014 19:36:11 +0000 (UTC)
Received: (qmail 83174 invoked by uid 500); 3 Apr 2014 19:36:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83133 invoked by uid 500); 3 Apr 2014 19:36:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83120 invoked by uid 99); 3 Apr 2014 19:36:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Apr 2014 19:36:07 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tgraves_cs@yahoo.com designates 72.30.239.69 as permitted sender)
Received: from [72.30.239.69] (HELO nm39.bullet.mail.bf1.yahoo.com) (72.30.239.69)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Apr 2014 19:36:00 +0000
Received: from [98.139.214.32] by nm39.bullet.mail.bf1.yahoo.com with NNFMP; 03 Apr 2014 19:35:39 -0000
Received: from [98.139.212.227] by tm15.bullet.mail.bf1.yahoo.com with NNFMP; 03 Apr 2014 19:35:39 -0000
Received: from [127.0.0.1] by omp1036.mail.bf1.yahoo.com with NNFMP; 03 Apr 2014 19:35:39 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 338318.80942.bm@omp1036.mail.bf1.yahoo.com
Received: (qmail 60566 invoked by uid 60001); 3 Apr 2014 19:35:39 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.com; s=s1024; t=1396553739; bh=7xb7xIiAEDKrVC9bWWMHW2DMOKHwvCaNZbEYvvINjHs=; h=X-YMail-OSG:Received:X-Rocket-MIMEInfo:X-Mailer:References:Message-ID:Date:From:Reply-To:Subject:To:Cc:In-Reply-To:MIME-Version:Content-Type; b=y0Xppip5HlxDo+HssnnVcE3AX7aG682NQKqCfN4BGPyOu0JKjqCSs/VdvobLCXzyfUNeTywfuW1R6b/ft7LdBjZo0z/68UDrzN2S85AZxCutn/2oXfdcMtXLuONFDvbwsbdnLvk0Z+clWb4S+89TcpPPGwYOvqtZtMRfiES1HpQ=
DomainKey-Signature:a=rsa-sha1; q=dns; c=nofws;
  s=s1024; d=yahoo.com;
  h=X-YMail-OSG:Received:X-Rocket-MIMEInfo:X-Mailer:References:Message-ID:Date:From:Reply-To:Subject:To:Cc:In-Reply-To:MIME-Version:Content-Type;
  b=SvzmqxRwIveO+8kyGbehMy2VivWHIjizc3o0pPYhTnU4jyvq1v81Tovee4vHP3xbf3KvVye1YvbZNLXRdCSEh2ywVJviInuuJvu1jtBGXHUhTU5VBHOEjOTe7syHcTPFrid//qyh4kzkE6AgkUSTn9x2nuRMSn+zHMP5IeosTi4=;
X-YMail-OSG: I7rmVTwVM1nfTMoOqpyftusTANZbbey2TXTzkw0NSA2UDLr
 HMyqPWw0EJiiD9voj1Ism5nhxi6VFWG7X4MwsUUyYrJTe8__mmGsnoWDQCI9
 xjxulTY_.Cr0dckEKdGHmPsY5kmuA0L.9qnqSl7oGUfsXwO3mfiO9vmIF8RU
 JsnCLXWkOHC.y9JCfrXp15YKItCvXITQXrL9HroCHmd9qGmRZWRGWffhuSmY
 GbgW7y_qXXIUs_t.cGNujE_hxLIeKAy2my3sinIHsKhVAGoFzQh1RQzu4S22
 b8kXnhaFEBtV_33JaXtHwYus768EgjCuhYHaXyTbNyM8Sie.NcSWniQFXXpg
 MGi__UWFspQ0MgI4Kawe312.31k5cmXiacB5KnO0r7FbIe3nshktp71loQG8
 DmkAI59e8MyMogmJZXXHXM8lQWZZQyg47ImRUUJvpUJQZ_yzC7z4dkeHewOi
 8oJBjZxLjgdEvmSDlFXa7_hasmYbwdXxaxnDd5hLMHBJ3tBWqDcfZugfCHZt
 sLztrJVDhsFZrdh6PQY1mHz5RFh53fIXYdB70oX6S3tQYH8EcUJBRx97SE4M
 L40ZHXL.hX3lr0FH1L8A0eg40ZGpRy3YzeHXfvbUNF0mkKVC88.bep9RVT7p
 Wpb2Ux2Cp7vk_K_GfzvvcWdM3dwrnIYr5gkUxFxHmy0Lt.Cq2wDjogn9bMpk
 wSdOV
Received: from [204.11.79.50] by web140106.mail.bf1.yahoo.com via HTTP; Thu, 03 Apr 2014 12:35:39 PDT
X-Rocket-MIMEInfo: 002.001,SSBwdXQgdXAgYSBwdWxsIHJlcXVlc3Qgd2l0aCBkb2N1bWVudGF0aW9uIGNoYW5nZXPCoGh0dHBzOi8vZ2l0aHViLmNvbS9hcGFjaGUvc3BhcmsvcHVsbC8zMTQKwqAKClRvbQpPbiBXZWRuZXNkYXksIEFwcmlsIDIsIDIwMTQgODo0NyBBTSwgVG9tIEdyYXZlcyA8dGdyYXZlc19jc0B5YWhvby5jb20.IHdyb3RlOgogCk5vdGUgSSdtICsxIHdpdGggdGhlIGRvYyBjaGFuZ2VkIHRvIHRlbGwgdXNlcnMgdG_CoGV4cG9ydCBTUEFSS19ZQVJOX01PREU9dHJ1ZSBiZWZvcmUgdXNpbmcgc3Bhcmstc2hlbGwgb24geWEBMAEBAQE-
X-Mailer: YahooMailWebService/0.8.182.648
References: <CAMwrk0mhqhBf2ZNRFiC4LgWjLb2A7fw3nCo8TwAZvadNinWmaA@mail.gmail.com>	<E75FBF52-0373-44E2-BCC2-BCB8445C995C@gmail.com>	<CAPdPcW2bN3yTLgFRaf=5Sm05mJQ71ufeMv8dfmEsEQLsEPsHGg@mail.gmail.com>	<CABPQxssGF-rOYE_oFiR2CArbMt-auvTBUT4+Yck79+Wk4dzYcA@mail.gmail.com>	<1396278778.50575.YahooMailNeo@web140101.mail.bf1.yahoo.com>	<CABPQxss_myw8XYRE1CKHw8SgTqx=3+Eb+2-OzzHi4iV5m9wqPw@mail.gmail.com>	<CAMwrk0mVrYGnon0JQvMpFUz7SPS_ZcxG53t95M2F_GowTBP9UQ@mail.gmail.com>	<1396362508.20704.YahooMailNeo@web140102.mail.bf1.yahoo.com> <CABPQxsv92x+K+6T=MbFDWdMB6OHt-8qqyNeUXcUrk18Y09v_Eg@mail.gmail.com> <1396377840.28297.YahooMailNeo@web140105.mail.bf1.yahoo.com> <1396446388.15960.YahooMailNeo@web140103.mail.bf1.yahoo.com>
Message-ID: <1396553739.2447.YahooMailNeo@web140106.mail.bf1.yahoo.com>
Date: Thu, 3 Apr 2014 12:35:39 -0700 (PDT)
From: Tom Graves <tgraves_cs@yahoo.com>
Reply-To: Tom Graves <tgraves_cs@yahoo.com>
Subject: Re: [VOTE] Release Apache Spark 0.9.1 (RC3)
To: "dev@spark.apache.org" <dev@spark.apache.org>,
  Tom Graves <tgraves_cs@yahoo.com>
Cc: Tathagata Das <tathagata.das1565@gmail.com>
In-Reply-To: <1396446388.15960.YahooMailNeo@web140103.mail.bf1.yahoo.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="-156808750-425326844-1396553739=:2447"
X-Virus-Checked: Checked by ClamAV on apache.org

---156808750-425326844-1396553739=:2447
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

I put up a pull request with documentation changes=A0https://github.com/apa=
che/spark/pull/314=0A=A0=0A=0ATom=0AOn Wednesday, April 2, 2014 8:47 AM, To=
m Graves <tgraves_cs@yahoo.com> wrote:=0A =0ANote I'm +1 with the doc chang=
ed to tell users to=A0export SPARK_YARN_MODE=3Dtrue before using spark-shel=
l on yarn.=0A=0AI tested it on both hadoop 0.23 and 2.3 clusters using secu=
re hdfs on linux.=0A=0ATom=0A=0AOn Tuesday, April 1, 2014 1:44 PM, Tom Grav=
es <tgraves_cs@yahoo.com> wrote:=0A=0ANo one else has reported seeing the i=
ssue so I think documenting it is fine. =A0=0A=0ATom=0A=0A=0A=0AOn Tuesday,=
 April 1, 2014 1:40 PM, Patrick Wendell <pwendell@gmail.com> wrote:=0A=0ATo=
m,=0A=0AGiven this is a pretty straightforward workaround, what do yo think=
 about=0Athe following course of action:=0A=0A(a) We can put the workaround=
 in the docs for 0.9.1. We don't need to do a=0Anew RC/vote for this since =
we can update the published docs independently.=0A=0A(b) We try to get a fi=
x in for this into the 0.9 branch so it can end up in=0A0.9.2. But this tak=
es the fix off the critical path for this release.=0A=0A- Patrick=0A=0A=0A=
=0AOn Tue, Apr 1, 2014 at 7:28 AM, Tom Graves <tgraves_cs@yahoo.com> wrote:=
=0A=0A> Thanks for extending the voting.=0A>=0A> Unfortunately I've found a=
n issue with the spark-shell in yarn-client=0A> mode.=A0 It doesn't work wi=
th secure HDFS unless you=0A> export SPARK_YARN_MODE=3Dtrue before starting=
 the shell, or if you happen to=0A> do something immediately with HDFS.=A0 =
If you wait for the connection to the=0A> namenode to timeout it will fail.=
=0A>=0A> I think it was actually this way in the 0.9 release also so I thou=
ght I=0A> would send this and get peoples feedback to see if you want it fi=
xed?=0A>=0A> Another option would be to document that you have to export=0A=
> SPARK_YARN_MODE=3Dtrue for the shell.=A0=A0 The fix actually went in with=
 the=0A> authentication changes I made in master but I never realized that =
change=0A> needed to apply to 0.9.=0A>=0A>=0A> https://github.com/apache/sp=
ark/commit/7edbea41b43e0dc11a2de156be220db8b7952d01#diff-0ae5b834ce90ec37c1=
9af35aa7a5e1a0=0A>=0A> See the SparkILoop diff.=0A>=0A> Tom=0A>=0A>=0A> On =
Monday, March 31, 2014 1:33 PM, Tathagata Das <=0A> tathagata.das1565@gmail=
.com> wrote:=0A>=0A> Yes, lets extend the vote for two more days from now. =
So the vote is open=0A> till Wednesday, April 02, at 20:00 UTC=0A>=0A> On t=
hat note, my +1=0A>=0A>=0A> TD=0A>=0A>=0A>=0A>=0A>=0A>=0A> On Mon, Mar 31, =
2014 at 9:57 AM, Patrick Wendell <pwendell@gmail.com>=0A> wrote:=0A>=0A> Ye=
ah good point. Let's just extend this vote another few days?=0A> >=0A> >=0A=
> >=0A> >On Mon, Mar 31, 2014 at 8:12 AM, Tom Graves <tgraves_cs@yahoo.com>=
 wrote:=0A> >=0A> >> I should probably pull this off into another thread, b=
ut going forward=0A> can=0A> >> we try to not have the release votes end on=
 a weekend? Since we only=0A> seem=0A> >> to give 3 days, it makes it reall=
y hard for anyone who is offline for=0A> the=0A> >> weekend to try it out.=
=A0=A0 Either that or extend the voting for more then=0A> 3=0A> >> days.=0A=
> >>=0A> >> Tom=0A> >> On Monday, March 31, 2014 12:50 AM, Patrick Wendell =
<pwendell@gmail.com=0A> >=0A> >> wrote:=0A> >>=0A> >> TD - I downloaded and=
 did some local testing. Looks good to me!=0A> >>=0A> >> +1=0A> >>=0A> >> Y=
ou should cast your own vote - at that point it's enough to pass.=0A> >>=0A=
> >> - Patrick=0A> >>=0A> >>=0A> >>=0A> >> On Sun, Mar 30, 2014 at 9:47 PM,=
 prabeesh k <prabsmails@gmail.com>=0A> wrote:=0A> >>=0A> >> > +1=0A> >> > t=
ested on Ubuntu12.04 64bit=0A> >> >=0A> >> >=0A> >> > On Mon, Mar 31, 2014 =
at 3:56 AM, Matei Zaharia <=0A> matei.zaharia@gmail.com=0A> >> > >wrote:=0A=
> >> >=0A> >> > > +1 tested on Mac OS X.=0A> >> > >=0A> >> > > Matei=0A> >>=
 > >=0A> >> > > On Mar 27, 2014, at 1:32 AM, Tathagata Das <=0A> >> tathaga=
ta.das1565@gmail.com>=0A> >> > > wrote:=0A> >> > >=0A> >> > > > Please vote=
 on releasing the following candidate as Apache Spark=0A> >> > version=0A> =
>> > > 0.9.1=0A> >> > > >=0A> >> > > > A draft of the release notes along w=
ith the CHANGES.txt file is=0A> >> > > > attached to this e-mail.=0A> >> > =
> >=0A> >> > > > The tag to be voted on is v0.9.1-rc3 (commit 4c43182b):=0A=
> >> > > >=0A> >> > >=0A> >> >=0A> >>=0A> https://git-wip-us.apache.org/rep=
os/asf?p=3Dspark.git;a=3Dcommit;h=3D4c43182b6d1b0b7717423f386c0214fe9307320=
8=0A> >> > > >=0A> >> > > > The release files, including signatures, digest=
s, etc. can be=0A> found=0A> >> at:=0A> >> > > > http://people.apache.org/~=
tdas/spark-0.9.1-rc3/=0A> >> > > >=0A> >> > > > Release artifacts are signe=
d with the following key:=0A> >> > > > https://people.apache.org/keys/commi=
tter/tdas.asc=0A> >> > > >=0A> >> > > > The staging repository for this rel=
ease can be found at:=0A> >> > > >=0A> >> >=0A> https://repository.apache.o=
rg/content/repositories/orgapachespark-1009/=0A> >> > > >=0A> >> > > > The =
documentation corresponding to this release can be found at:=0A> >> > > > h=
ttp://people.apache.org/~tdas/spark-0.9.1-rc3-docs/=0A> >> > > >=0A> >> > >=
 > Please vote on releasing this package as Apache Spark 0.9.1!=0A> >> > > =
>=0A> >> > > > The vote is open until Sunday, March 30, at 10:00 UTC and pa=
sses=0A> if=0A> >> > > > a majority of at least 3 +1 PMC votes are cast.=0A=
> >> > > >=0A> >> > > > [ ] +1 Release this package as Apache Spark 0.9.1=
=0A> >> > > > [ ] -1 Do not release this package because ...=0A> >> > > >=
=0A> >> > > > To learn more about Apache Spark, please see=0A> >> > > > htt=
p://spark.apache.org/=0A> >> > > > <CHANGES.txt><RELEASE_NOTES.txt>=0A> >> =
> >=0A> >> > >=0A> >> >=0A> >>=0A> >=0A>
---156808750-425326844-1396553739=:2447--

From dev-return-7208-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr  4 11:25:22 2014
Return-Path: <dev-return-7208-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 022B910D37
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  4 Apr 2014 11:25:22 +0000 (UTC)
Received: (qmail 58917 invoked by uid 500); 4 Apr 2014 11:25:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58413 invoked by uid 500); 4 Apr 2014 11:25:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58404 invoked by uid 99); 4 Apr 2014 11:25:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 04 Apr 2014 11:25:16 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pahomov.egor@gmail.com designates 209.85.216.49 as permitted sender)
Received: from [209.85.216.49] (HELO mail-qa0-f49.google.com) (209.85.216.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 04 Apr 2014 11:25:12 +0000
Received: by mail-qa0-f49.google.com with SMTP id j7so2911274qaq.22
        for <dev@spark.apache.org>; Fri, 04 Apr 2014 04:24:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=YrPbP61+8HkkM+P/vpSS8rY1gLNOQfGdH9RdjUxYEOc=;
        b=EDOWR0xn6TumEEylPi/oLgI1B6ld50o32KESsnRiqnGpcug/YDLjkOQZwJELuAvkpj
         b1X9CgjcCcWiYmgGKb86/MrJiehL6Y11p7cI/fERwjHQ8DeCz/uGKbcQIla3zI+DDILZ
         9ndpUUEYJYxddNHYC2FaH7U7l0DKUDPmW4Iw2Jt85vCi/X3bqUvGZ/upuZQ3M+GLCtWM
         lDjWD9HRgOf/nmYVFQ0+sOB7pd5TLLZXKn1AG+fRhaoyx0f0S5Lf5zBQbBea6YltUGoE
         mDdcssKXfAw/u+xr0STQ20wGSVCsbF3TspAWiIAeL0yQo+LytTJjvT7qDvEaAmHvHUZW
         vpqg==
MIME-Version: 1.0
X-Received: by 10.140.86.36 with SMTP id o33mr12731197qgd.67.1396610689983;
 Fri, 04 Apr 2014 04:24:49 -0700 (PDT)
Received: by 10.140.50.99 with HTTP; Fri, 4 Apr 2014 04:24:49 -0700 (PDT)
In-Reply-To: <CAMrx5DypxG_dVLxKDg6n7o8sf861n2ux9sDJ-61y_P=SwCNmNA@mail.gmail.com>
References: <CAMrx5DypxG_dVLxKDg6n7o8sf861n2ux9sDJ-61y_P=SwCNmNA@mail.gmail.com>
Date: Fri, 4 Apr 2014 15:24:49 +0400
Message-ID: <CAMrx5DxfKcsdgML=vaqAtsVKpdJk_KkQZB5pLL3_9DOmyMntAQ@mail.gmail.com>
Subject: Re: Ping on SPARK-1177
From: Egor Pahomov <pahomov.egor@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c129cc10ed2704f635c48a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c129cc10ed2704f635c48a
Content-Type: text/plain; charset=ISO-8859-1

Hi again. Can anyone help me create way to run spark from application by
reviewing it?


2014-03-16 13:10 GMT+04:00 Egor Pahomov <pahomov.egor@gmail.com>:

> Spark documentation and spark code helps you run your application from
> shell. In my company it's not convenient - we run cluster task from code in
> our web service. It took me a lot of time to bring as much configuration in
> code as I can, because configuration at process start - quite hard in our
> realities. I'd like to make some patches and write some documentation,
> which bring our practices to Spark. So please help me do it by reviewing
> this first patch  - https://github.com/apache/spark/pull/82
>
> --
>
>
>
> *Sincerely yours Egor PakhomovScala Developer, Yandex*
>



-- 



*Sincerely yoursEgor PakhomovScala Developer, Yandex*

--001a11c129cc10ed2704f635c48a--

From dev-return-7209-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Apr  5 01:14:15 2014
Return-Path: <dev-return-7209-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CC6E610D9E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  5 Apr 2014 01:14:15 +0000 (UTC)
Received: (qmail 83819 invoked by uid 500); 5 Apr 2014 01:14:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83785 invoked by uid 500); 5 Apr 2014 01:14:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83777 invoked by uid 99); 5 Apr 2014 01:14:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 01:14:14 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.220.45 as permitted sender)
Received: from [209.85.220.45] (HELO mail-pa0-f45.google.com) (209.85.220.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 01:14:08 +0000
Received: by mail-pa0-f45.google.com with SMTP id kl14so4195939pab.18
        for <dev@spark.apache.org>; Fri, 04 Apr 2014 18:13:45 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=jbA1pxEw9QFxgESj6X9Q0nxCNvdzzvaa+uqcOztqHJY=;
        b=P6CV8dAVRrje7yGY87W0b6Ad97Wm9p6fEI2A8HlQbqf0c8cNier2SQVSO6Sr+7MCQA
         lK2CmhjD33VkzwtPHEXz1vpWvYDfc22SXpxhSf5nodVKIFC80gdDaPP+67vQIbPkl0bh
         zpLlq6zYOxR+ej0oE124gPbHRSCH8SmTIMnV226aAjXW02jxvNQJFAZX4r5ft9oscVgr
         nNRfgcpPrtovlLvuf0fJrgT2rhJj0TjAAfX8SMKueuvG4p6wlwnfwqnGauOoQPNv7Ifz
         LZHGuse8rqWzCxAiYlF09TyeeGTlx5+bPT/SocgTnwGDmDCvxMCwO3EpCn8tRXuB+JfR
         kcqQ==
MIME-Version: 1.0
X-Received: by 10.67.23.135 with SMTP id ia7mr18112386pad.5.1396660425074;
 Fri, 04 Apr 2014 18:13:45 -0700 (PDT)
Received: by 10.68.12.39 with HTTP; Fri, 4 Apr 2014 18:13:45 -0700 (PDT)
Date: Fri, 4 Apr 2014 18:13:45 -0700
Message-ID: <CA+B-+fw8cM8LgW7ALhzaB_37882PGZwZfxvKFXPb3vSaXqH-Sw@mail.gmail.com>
Subject: Recent heartbeats
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134519a822c7904f6415888
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134519a822c7904f6415888
Content-Type: text/plain; charset=ISO-8859-1

Hi,

Also posted it on user but then I realized it might be more involved.

In my ALS runs I am noticing messages that complain about heart beats:

14/04/04 20:43:09 WARN BlockManagerMasterActor: Removing BlockManager
BlockManagerId(17, machine1, 53419, 0) with no recent heart beats: 48476ms
exceeds 45000ms
14/04/04 20:43:09 WARN BlockManagerMasterActor: Removing BlockManager
BlockManagerId(12, machine2, 60714, 0) with no recent heart beats: 45328ms
exceeds 45000ms
14/04/04 20:43:09 WARN BlockManagerMasterActor: Removing BlockManager
BlockManagerId(19, machine3, 39496, 0) with no recent heart beats: 53259ms
exceeds 45000ms

Is this some issue with the underlying jvm over which akka is run ? Can I
increase the heartbeat somehow to get these messages resolved ?

Any more insight about the possible cause for the heartbeat will be
helpful...

Thanks.
Deb

--001a1134519a822c7904f6415888--

From dev-return-7210-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Apr  5 05:36:21 2014
Return-Path: <dev-return-7210-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4320F111B7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  5 Apr 2014 05:36:21 +0000 (UTC)
Received: (qmail 77387 invoked by uid 500); 5 Apr 2014 05:36:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77346 invoked by uid 500); 5 Apr 2014 05:36:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77335 invoked by uid 99); 5 Apr 2014 05:36:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 05:36:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.182 as permitted sender)
Received: from [209.85.214.182] (HELO mail-ob0-f182.google.com) (209.85.214.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 05:36:09 +0000
Received: by mail-ob0-f182.google.com with SMTP id uz6so4542631obc.27
        for <dev@spark.apache.org>; Fri, 04 Apr 2014 22:35:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=zf9ESpLnwSVovxmyQC4QBzf4AoeDuvYLcYFHex5MXW0=;
        b=G0lyo+HXHUn9pDiWPUDvLgMPo5+cWS8+YyDMgEEa2tPk64Riv4d0OupbeQrjeZSHl3
         bd7zmhgW51+VRcUOb4zJ48x/sqDIQ9k4Bbuv1DUf8EFTI8NmqPvFnifMdl+De3s3qCJJ
         Rrfr0161aUoFyBVBUbmaKCGB3wrXrfxSDJt76HVqG4BsZUHS8fTnV6QIt+DcltfSL6CY
         LXGMSEn9HWe1yo9JtjONlargEmq34UPFuB1WFnuT1sgZsC2DmAKbXYL8116MGc08YCP2
         iP51ol9WbPsPr0r2iyPEysrC5uci13d4l6AjGEMiH+8NdNZmij0SmFV1IiPn3uwMDTGp
         zNDQ==
MIME-Version: 1.0
X-Received: by 10.182.40.201 with SMTP id z9mr412835obk.45.1396676148961; Fri,
 04 Apr 2014 22:35:48 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Fri, 4 Apr 2014 22:35:48 -0700 (PDT)
In-Reply-To: <CA+B-+fw8cM8LgW7ALhzaB_37882PGZwZfxvKFXPb3vSaXqH-Sw@mail.gmail.com>
References: <CA+B-+fw8cM8LgW7ALhzaB_37882PGZwZfxvKFXPb3vSaXqH-Sw@mail.gmail.com>
Date: Fri, 4 Apr 2014 22:35:48 -0700
Message-ID: <CABPQxst5z8Yyjx0yKsgreq2S9g-kWFxn4uL444oyWGArZTM-Qg@mail.gmail.com>
Subject: Re: Recent heartbeats
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c33b02b9a8cc04f6450163
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c33b02b9a8cc04f6450163
Content-Type: text/plain; charset=ISO-8859-1

I answered this over on the user list...


On Fri, Apr 4, 2014 at 6:13 PM, Debasish Das <debasish.das83@gmail.com>wrote:

> Hi,
>
> Also posted it on user but then I realized it might be more involved.
>
> In my ALS runs I am noticing messages that complain about heart beats:
>
> 14/04/04 20:43:09 WARN BlockManagerMasterActor: Removing BlockManager
> BlockManagerId(17, machine1, 53419, 0) with no recent heart beats: 48476ms
> exceeds 45000ms
> 14/04/04 20:43:09 WARN BlockManagerMasterActor: Removing BlockManager
> BlockManagerId(12, machine2, 60714, 0) with no recent heart beats: 45328ms
> exceeds 45000ms
> 14/04/04 20:43:09 WARN BlockManagerMasterActor: Removing BlockManager
> BlockManagerId(19, machine3, 39496, 0) with no recent heart beats: 53259ms
> exceeds 45000ms
>
> Is this some issue with the underlying jvm over which akka is run ? Can I
> increase the heartbeat somehow to get these messages resolved ?
>
> Any more insight about the possible cause for the heartbeat will be
> helpful...
>
> Thanks.
> Deb
>

--001a11c33b02b9a8cc04f6450163--

From dev-return-7211-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Apr  5 08:17:43 2014
Return-Path: <dev-return-7211-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 435C611353
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  5 Apr 2014 08:17:43 +0000 (UTC)
Received: (qmail 61288 invoked by uid 500); 5 Apr 2014 08:17:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60713 invoked by uid 500); 5 Apr 2014 08:17:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60702 invoked by uid 99); 5 Apr 2014 08:17:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 08:17:38 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.220.43 as permitted sender)
Received: from [209.85.220.43] (HELO mail-pa0-f43.google.com) (209.85.220.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 08:17:33 +0000
Received: by mail-pa0-f43.google.com with SMTP id bj1so4503510pad.30
        for <dev@spark.apache.org>; Sat, 05 Apr 2014 01:17:12 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=WGiKALUUuWwEAacKz/w+IEiC6b5gROLQeHNx2JJ6UmQ=;
        b=PqzH2DrTRPkThK7nYSQC+iwCwqAlsMpkmo7LjWQe99io12K/E03wW3KcUhDvY/nTcF
         p8SYnBf536iZI1O2a38hHLMp4GxFublbxczPlWzpTwf6HLoX1y14ToVzXfWAeYywIrLl
         udXeJ1eeFmfsizKj5CdIi7x/CE0SyS5kjTepTYF8S3Gw0Y4SyCgopvt+dhXNCYRMZk1V
         7kL7SArTCVd6ib5BxtVWJ1mN+I1BRc0wtxAZxGGYZipbtiM0lnQuHxgA9L9fZn9QjNwr
         K8AnqEb2kwmuXa8dU5Z+VgRw/8wyviQ/nGGo9yEFasXh3sIdAnAw2PnPJK8/hNHSLv0E
         hRXg==
MIME-Version: 1.0
X-Received: by 10.68.196.168 with SMTP id in8mr763334pbc.132.1396685832714;
 Sat, 05 Apr 2014 01:17:12 -0700 (PDT)
Received: by 10.68.12.39 with HTTP; Sat, 5 Apr 2014 01:17:12 -0700 (PDT)
In-Reply-To: <CABPQxst5z8Yyjx0yKsgreq2S9g-kWFxn4uL444oyWGArZTM-Qg@mail.gmail.com>
References: <CA+B-+fw8cM8LgW7ALhzaB_37882PGZwZfxvKFXPb3vSaXqH-Sw@mail.gmail.com>
	<CABPQxst5z8Yyjx0yKsgreq2S9g-kWFxn4uL444oyWGArZTM-Qg@mail.gmail.com>
Date: Sat, 5 Apr 2014 01:17:12 -0700
Message-ID: <CA+B-+fxq4c-qJ30QWNE5wUgBW1Pte4TTsAvdXHoayun=PHwZMA@mail.gmail.com>
Subject: Re: Recent heartbeats
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=e89a8ff1cc3eebff1704f64742e4
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8ff1cc3eebff1704f64742e4
Content-Type: text/plain; charset=ISO-8859-1

Thanks Patrick...I searched in the archives and found the answer...tuning
the akka and gc params....


On Fri, Apr 4, 2014 at 10:35 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> I answered this over on the user list...
>
>
> On Fri, Apr 4, 2014 at 6:13 PM, Debasish Das <debasish.das83@gmail.com
> >wrote:
>
> > Hi,
> >
> > Also posted it on user but then I realized it might be more involved.
> >
> > In my ALS runs I am noticing messages that complain about heart beats:
> >
> > 14/04/04 20:43:09 WARN BlockManagerMasterActor: Removing BlockManager
> > BlockManagerId(17, machine1, 53419, 0) with no recent heart beats:
> 48476ms
> > exceeds 45000ms
> > 14/04/04 20:43:09 WARN BlockManagerMasterActor: Removing BlockManager
> > BlockManagerId(12, machine2, 60714, 0) with no recent heart beats:
> 45328ms
> > exceeds 45000ms
> > 14/04/04 20:43:09 WARN BlockManagerMasterActor: Removing BlockManager
> > BlockManagerId(19, machine3, 39496, 0) with no recent heart beats:
> 53259ms
> > exceeds 45000ms
> >
> > Is this some issue with the underlying jvm over which akka is run ? Can I
> > increase the heartbeat somehow to get these messages resolved ?
> >
> > Any more insight about the possible cause for the heartbeat will be
> > helpful...
> >
> > Thanks.
> > Deb
> >
>

--e89a8ff1cc3eebff1704f64742e4--

From dev-return-7212-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Apr  5 21:04:54 2014
Return-Path: <dev-return-7212-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5BE8911F8E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  5 Apr 2014 21:04:54 +0000 (UTC)
Received: (qmail 12525 invoked by uid 500); 5 Apr 2014 21:04:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12225 invoked by uid 500); 5 Apr 2014 21:04:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12217 invoked by uid 99); 5 Apr 2014 21:04:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 21:04:50 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.220.48 as permitted sender)
Received: from [209.85.220.48] (HELO mail-pa0-f48.google.com) (209.85.220.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 21:04:44 +0000
Received: by mail-pa0-f48.google.com with SMTP id hz1so5004018pad.35
        for <dev@spark.apache.org>; Sat, 05 Apr 2014 14:04:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=+KJecuGk6J7AN7gukxyJditVnLaQs5TehjkbBCpBC+s=;
        b=cAgiFQ12Ek3TtCV8Ae1wnbzK8zsa4V+Dd/LF+OxzAwpFrFIsNm+E5qFgdtCvC3x3pJ
         vAIxWmw3JMo+vkzO9ovAwPgr0PWj4Vvo9NZ0bD8oOeztui20kP45e5BUBtJGC4NhGfLO
         Wl9Dfnj+GHojRuU//b9VluirueXFnQE8/N3WW+uFJ4k/tO7jL2HPDkAq3L+3ewOexMLG
         4yFHTPFQ31qgrm8LDGZdEbymJgNGtJpRKOi3nJTXHQLjSe+fTZTEd1mD74k8o6sYgsxC
         40VvX4p3gUOrgjTyWbN8qgNezSIUG80dRVXUPyBNOiZ9R4i0iHqWd7etBOEG75Fe4P3M
         oMBA==
MIME-Version: 1.0
X-Received: by 10.68.170.36 with SMTP id aj4mr22564415pbc.54.1396731862100;
 Sat, 05 Apr 2014 14:04:22 -0700 (PDT)
Received: by 10.68.12.39 with HTTP; Sat, 5 Apr 2014 14:04:22 -0700 (PDT)
Date: Sat, 5 Apr 2014 14:04:22 -0700
Message-ID: <CA+B-+fzZPVoYM1YWg2U7EqHwVPAfW5xe-Mg4A6C-S4JAhmAtpw@mail.gmail.com>
Subject: Master compilation
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bacb21c7cb19a04f651fa41
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bacb21c7cb19a04f651fa41
Content-Type: text/plain; charset=ISO-8859-1

I am synced with apache/spark master but getting error in spark/sql
compilation...

Is the master broken ?

[info] Compiling 34 Scala sources to
/home/debasish/spark_deploy/sql/core/target/scala-2.10/classes...
[error]
/home/debasish/spark_deploy/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.scala:106:
value getGlobal is not a member of object java.util.logging.Logger
[error]       logger.setParent(Logger.getGlobal)
[error]                               ^
[error] one error found
[error] (sql/compile:compile) Compilation failed
[error] Total time: 171 s, completed Apr 5, 2014 4:58:41 PM

Thanks.
Deb

--047d7bacb21c7cb19a04f651fa41--

From dev-return-7213-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Apr  5 21:20:26 2014
Return-Path: <dev-return-7213-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5171411FA7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  5 Apr 2014 21:20:26 +0000 (UTC)
Received: (qmail 18588 invoked by uid 500); 5 Apr 2014 21:20:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18194 invoked by uid 500); 5 Apr 2014 21:20:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18186 invoked by uid 99); 5 Apr 2014 21:20:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 21:20:22 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.216.49 as permitted sender)
Received: from [209.85.216.49] (HELO mail-qa0-f49.google.com) (209.85.216.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 21:20:16 +0000
Received: by mail-qa0-f49.google.com with SMTP id j7so4443384qaq.36
        for <dev@spark.apache.org>; Sat, 05 Apr 2014 14:19:55 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=tJVp6NURqR2AW1WrTcdCGznz87g+gVIckHENgua/knU=;
        b=baF5lGRDiwYNTILgbkPh3ZWW8LmhLvz2XmIMmwtu9v+KUKlFJ1K0rIwKE8zixLIMZj
         lTAMuzyCWqSiiqy6qowStjPTdgW8d9mFNCjh2y2vNSsG+QxtJFy0iUn5aZCELG0NWxOO
         wj0c9m8JutVmXGEdzLAVsziC1OtGWyHq/39Ia49P/TprprQCAKZOjDHDyvduHQzBphtU
         eDFqvHryZY/+Sh53vJubWQZr7bTW/QoJwKXvvo6Kv8xrc90t2bu9lx7D0EX5z7swmg+T
         3YkfCcabLxU1dtjLHqcpmltJuvM9u2gWQE/G3CASmxVBNfVRz+YCMxeuxbgbroR+PjFs
         lbqA==
X-Gm-Message-State: ALoCoQkevnUBID9HvJYSKZs3KyplGI6G9Yew1aSVgJ3WIUzg0gjJ5bVAkJqTv1ILZPAH/W6t6Jfe
X-Received: by 10.140.102.135 with SMTP id w7mr21917700qge.29.1396732795773;
 Sat, 05 Apr 2014 14:19:55 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.107.194 with HTTP; Sat, 5 Apr 2014 14:19:35 -0700 (PDT)
In-Reply-To: <CA+B-+fzZPVoYM1YWg2U7EqHwVPAfW5xe-Mg4A6C-S4JAhmAtpw@mail.gmail.com>
References: <CA+B-+fzZPVoYM1YWg2U7EqHwVPAfW5xe-Mg4A6C-S4JAhmAtpw@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Sat, 5 Apr 2014 22:19:35 +0100
Message-ID: <CAMAsSd+ys9hJQz+NCM86WyMZmUUhqiOCctZaEjVjdRKV1b77yQ@mail.gmail.com>
Subject: Re: Master compilation
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

That method was added in Java 7. The project is on Java 6, so I think
this was just an inadvertent error in a recent PR (it was the 'Spark
parquet improvements' one).

I'll open a hot-fix PR after looking for other stuff like this that
might have snuck in.
--
Sean Owen | Director, Data Science | London


On Sat, Apr 5, 2014 at 10:04 PM, Debasish Das <debasish.das83@gmail.com> wrote:
> I am synced with apache/spark master but getting error in spark/sql
> compilation...
>
> Is the master broken ?
>
> [info] Compiling 34 Scala sources to
> /home/debasish/spark_deploy/sql/core/target/scala-2.10/classes...
> [error]
> /home/debasish/spark_deploy/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.scala:106:
> value getGlobal is not a member of object java.util.logging.Logger
> [error]       logger.setParent(Logger.getGlobal)
> [error]                               ^
> [error] one error found
> [error] (sql/compile:compile) Compilation failed
> [error] Total time: 171 s, completed Apr 5, 2014 4:58:41 PM
>
> Thanks.
> Deb

From dev-return-7214-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Apr  5 21:23:18 2014
Return-Path: <dev-return-7214-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D6FD211FB3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  5 Apr 2014 21:23:18 +0000 (UTC)
Received: (qmail 22156 invoked by uid 500); 5 Apr 2014 21:23:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22106 invoked by uid 500); 5 Apr 2014 21:23:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22091 invoked by uid 99); 5 Apr 2014 21:23:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 21:23:14 +0000
X-ASF-Spam-Status: No, hits=2.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.177 as permitted sender)
Received: from [209.85.192.177] (HELO mail-pd0-f177.google.com) (209.85.192.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 21:23:09 +0000
Received: by mail-pd0-f177.google.com with SMTP id y10so4815986pdj.22
        for <dev@spark.apache.org>; Sat, 05 Apr 2014 14:22:46 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=8sZIcMzV4rGnd9pt2FT8kd8meSbpQiXXYNgf3pZ8Cbg=;
        b=LlCQJaaUXkmrMsvl4AM5E63z/oj0A0sDJeZ2w/9ZBBxk3B+FLt6Pm94EPCgK2qLpYx
         MCuLkn0LbjKKy7q70n4HzVBtubVRC4g6GjwwIFHyxa+uRDQRRqdwZXtlLd3P8ngJTNbn
         SpliS8vdhUYPJXz/jZV9Iu7Fbb1jIhsiVTWE8FXNLDNJTRHWao3ElHcRNxN7kbthrm/o
         /IWkB1gDjmmdhnGPTEsYcFvmmktghzaB9dEEVXStjBhJWd/tb3smInDc2+ek88DXXaAD
         kYkXEdKCcj8s64oXGSdya3SUb7zh2T40d3Op8SRB7EPdPZOTj4nJLYxm5VDfjkmjhIiZ
         Fe6Q==
MIME-Version: 1.0
X-Received: by 10.66.192.73 with SMTP id he9mr22490486pac.88.1396732966528;
 Sat, 05 Apr 2014 14:22:46 -0700 (PDT)
Received: by 10.68.12.39 with HTTP; Sat, 5 Apr 2014 14:22:46 -0700 (PDT)
In-Reply-To: <CAMAsSd+ys9hJQz+NCM86WyMZmUUhqiOCctZaEjVjdRKV1b77yQ@mail.gmail.com>
References: <CA+B-+fzZPVoYM1YWg2U7EqHwVPAfW5xe-Mg4A6C-S4JAhmAtpw@mail.gmail.com>
	<CAMAsSd+ys9hJQz+NCM86WyMZmUUhqiOCctZaEjVjdRKV1b77yQ@mail.gmail.com>
Date: Sat, 5 Apr 2014 14:22:46 -0700
Message-ID: <CA+B-+fwf+O5ix4rG8cJWSR_WBQEaVwHLxFOrP1RG7agVfR4vGg@mail.gmail.com>
Subject: Re: Master compilation
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bf0d57850ed6d04f6523c97
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bf0d57850ed6d04f6523c97
Content-Type: text/plain; charset=ISO-8859-1

I can compile with Java 7...let me try that...


On Sat, Apr 5, 2014 at 2:19 PM, Sean Owen <sowen@cloudera.com> wrote:

> That method was added in Java 7. The project is on Java 6, so I think
> this was just an inadvertent error in a recent PR (it was the 'Spark
> parquet improvements' one).
>
> I'll open a hot-fix PR after looking for other stuff like this that
> might have snuck in.
> --
> Sean Owen | Director, Data Science | London
>
>
> On Sat, Apr 5, 2014 at 10:04 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
> > I am synced with apache/spark master but getting error in spark/sql
> > compilation...
> >
> > Is the master broken ?
> >
> > [info] Compiling 34 Scala sources to
> > /home/debasish/spark_deploy/sql/core/target/scala-2.10/classes...
> > [error]
> >
> /home/debasish/spark_deploy/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.scala:106:
> > value getGlobal is not a member of object java.util.logging.Logger
> > [error]       logger.setParent(Logger.getGlobal)
> > [error]                               ^
> > [error] one error found
> > [error] (sql/compile:compile) Compilation failed
> > [error] Total time: 171 s, completed Apr 5, 2014 4:58:41 PM
> >
> > Thanks.
> > Deb
>

--047d7bf0d57850ed6d04f6523c97--

From dev-return-7215-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Apr  5 21:30:57 2014
Return-Path: <dev-return-7215-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3EBE611FDC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  5 Apr 2014 21:30:57 +0000 (UTC)
Received: (qmail 31488 invoked by uid 500); 5 Apr 2014 21:30:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30726 invoked by uid 500); 5 Apr 2014 21:30:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 29842 invoked by uid 99); 5 Apr 2014 21:30:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 21:30:51 +0000
X-ASF-Spam-Status: No, hits=2.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.182 as permitted sender)
Received: from [209.85.192.182] (HELO mail-pd0-f182.google.com) (209.85.192.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 21:30:46 +0000
Received: by mail-pd0-f182.google.com with SMTP id y10so4825674pdj.41
        for <dev@spark.apache.org>; Sat, 05 Apr 2014 14:30:24 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=yHSzMjFnm8+m826kPQ/0J95eCZ1Td4YU0GRPEIkiZxA=;
        b=jW95X3nHCpF+zcB9A/IX4I5K2HseatqYS6Z0yGPFfy2F6Y5dCRoTdMz1VyyVTklPmx
         Yzj0KGQOwDfCjcoyDJwvnN5OpRYHJDS8ce8KcmUAnjAyT8MfnWkdcbj1QA9JnYLpx+wl
         F7HD4xpoNA8pXadarYrKLov/ZVxmcjvL0UQZgLXPW3r7kAQZap1TjSN/YVnuCcHkfPlB
         BkkTnnPl5nklCFGPOEVYKS3ceOnoSH4NyKGZ1xN8OmWUJ6+SX495JMXVgQmD33zUuRPA
         6/tPjDgjJEWNP54ZH2WR5Ru4JQDhGOF8T8nzrYr8yDAfULAdbeBeul5NezCFv+PygZ1a
         w0XQ==
MIME-Version: 1.0
X-Received: by 10.68.186.33 with SMTP id fh1mr76998pbc.140.1396733423943; Sat,
 05 Apr 2014 14:30:23 -0700 (PDT)
Received: by 10.68.12.39 with HTTP; Sat, 5 Apr 2014 14:30:23 -0700 (PDT)
In-Reply-To: <CA+B-+fwf+O5ix4rG8cJWSR_WBQEaVwHLxFOrP1RG7agVfR4vGg@mail.gmail.com>
References: <CA+B-+fzZPVoYM1YWg2U7EqHwVPAfW5xe-Mg4A6C-S4JAhmAtpw@mail.gmail.com>
	<CAMAsSd+ys9hJQz+NCM86WyMZmUUhqiOCctZaEjVjdRKV1b77yQ@mail.gmail.com>
	<CA+B-+fwf+O5ix4rG8cJWSR_WBQEaVwHLxFOrP1RG7agVfR4vGg@mail.gmail.com>
Date: Sat, 5 Apr 2014 14:30:23 -0700
Message-ID: <CA+B-+fy_SeaYpZF8VfbOMaTp9Tuas40ipn4kZ15NQsi1DMrRXg@mail.gmail.com>
Subject: Re: Master compilation
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bd7642c94857c04f65257d2
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd7642c94857c04f65257d2
Content-Type: text/plain; charset=ISO-8859-1

I verified this is happening for both CDH4.5 and 1.0.4...My deploy
environment is Java 6...so Java 7 compilation is not going to help...

Is this the PR which caused it ?

Andre Schumacher

    fbebaed    Spark parquet improvements A few improvements to the Parquet
support for SQL queries: - Instead of files a ParquetRelation is now backed
by a directory, which simplifies importing data from other sources -
InsertIntoParquetTable operation now supports switching between overwriting
or appending (at least in HiveQL) - tests now use the new API - Parquet
logging can be set to WARNING level (Default) - Default compression for
Parquet files (GZIP, as in parquet-mr) Author: Andre Schumacher &...    2
days ago    SPARK-1383

I will go to a stable checkin before this




On Sat, Apr 5, 2014 at 2:22 PM, Debasish Das <debasish.das83@gmail.com>wrote:

> I can compile with Java 7...let me try that...
>
>
> On Sat, Apr 5, 2014 at 2:19 PM, Sean Owen <sowen@cloudera.com> wrote:
>
>> That method was added in Java 7. The project is on Java 6, so I think
>> this was just an inadvertent error in a recent PR (it was the 'Spark
>> parquet improvements' one).
>>
>> I'll open a hot-fix PR after looking for other stuff like this that
>> might have snuck in.
>> --
>> Sean Owen | Director, Data Science | London
>>
>>
>> On Sat, Apr 5, 2014 at 10:04 PM, Debasish Das <debasish.das83@gmail.com>
>> wrote:
>> > I am synced with apache/spark master but getting error in spark/sql
>> > compilation...
>> >
>> > Is the master broken ?
>> >
>> > [info] Compiling 34 Scala sources to
>> > /home/debasish/spark_deploy/sql/core/target/scala-2.10/classes...
>> > [error]
>> >
>> /home/debasish/spark_deploy/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.scala:106:
>> > value getGlobal is not a member of object java.util.logging.Logger
>> > [error]       logger.setParent(Logger.getGlobal)
>> > [error]                               ^
>> > [error] one error found
>> > [error] (sql/compile:compile) Compilation failed
>> > [error] Total time: 171 s, completed Apr 5, 2014 4:58:41 PM
>> >
>> > Thanks.
>> > Deb
>>
>
>

--047d7bd7642c94857c04f65257d2--

From dev-return-7216-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Apr  5 22:07:01 2014
Return-Path: <dev-return-7216-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 81D1C100A9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  5 Apr 2014 22:07:01 +0000 (UTC)
Received: (qmail 66366 invoked by uid 500); 5 Apr 2014 22:07:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66037 invoked by uid 500); 5 Apr 2014 22:06:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66029 invoked by uid 99); 5 Apr 2014 22:06:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 22:06:59 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.41 as permitted sender)
Received: from [209.85.219.41] (HELO mail-oa0-f41.google.com) (209.85.219.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 22:06:54 +0000
Received: by mail-oa0-f41.google.com with SMTP id j17so5174598oag.28
        for <dev@spark.apache.org>; Sat, 05 Apr 2014 15:06:32 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=lGAxn075XSaK4Cs4EBNg7QlkGt1HzuUrP6n/VGcEKaI=;
        b=XvK4a0jvfRZHlq3uXtp9E+AfJJJPe0sqC+f4q80pClqO1oJMzmFoJa3dEV8m2RI68n
         nYJ9QG66KPFOEN4c6auRIQWF9is9Lo89w0sKAABXdaPfqc5tY1+AgPA3w62prtor1Psy
         6xGkb4gXm4usM44o4y/Jsgc317OA8AQNnyOKSYtXdvUtQqd2NvTEbQYXe9QkPpiMoJgy
         yqrrQavgDyIGkKLwGN+LmxIomd0S4v0i0q8SahFBJ46vAoGu1iU9kEVFBzGSZirPDjvT
         uUVpMDP32u8ntWpsKeIVMDzN8YwB+xgUxs9KF8jVEHRvPk37ZKhfKbl3jQ5PbnBOKncd
         qpLA==
MIME-Version: 1.0
X-Received: by 10.182.112.231 with SMTP id it7mr9079621obb.8.1396735591898;
 Sat, 05 Apr 2014 15:06:31 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Sat, 5 Apr 2014 15:06:31 -0700 (PDT)
In-Reply-To: <CA+B-+fy_SeaYpZF8VfbOMaTp9Tuas40ipn4kZ15NQsi1DMrRXg@mail.gmail.com>
References: <CA+B-+fzZPVoYM1YWg2U7EqHwVPAfW5xe-Mg4A6C-S4JAhmAtpw@mail.gmail.com>
	<CAMAsSd+ys9hJQz+NCM86WyMZmUUhqiOCctZaEjVjdRKV1b77yQ@mail.gmail.com>
	<CA+B-+fwf+O5ix4rG8cJWSR_WBQEaVwHLxFOrP1RG7agVfR4vGg@mail.gmail.com>
	<CA+B-+fy_SeaYpZF8VfbOMaTp9Tuas40ipn4kZ15NQsi1DMrRXg@mail.gmail.com>
Date: Sat, 5 Apr 2014 15:06:31 -0700
Message-ID: <CABPQxss=pKDpwRKi=QAT4kx8zRrVYL+riBLr-YaZtLhRsnQSSw@mail.gmail.com>
Subject: Re: Master compilation
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0149c9f8cce4d004f652d877
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149c9f8cce4d004f652d877
Content-Type: text/plain; charset=ISO-8859-1

If you want to submit a hot fix for this issue specifically please do. I'm
not sure why it didn't fail our build...


On Sat, Apr 5, 2014 at 2:30 PM, Debasish Das <debasish.das83@gmail.com>wrote:

> I verified this is happening for both CDH4.5 and 1.0.4...My deploy
> environment is Java 6...so Java 7 compilation is not going to help...
>
> Is this the PR which caused it ?
>
> Andre Schumacher
>
>     fbebaed    Spark parquet improvements A few improvements to the Parquet
> support for SQL queries: - Instead of files a ParquetRelation is now backed
> by a directory, which simplifies importing data from other sources -
> InsertIntoParquetTable operation now supports switching between overwriting
> or appending (at least in HiveQL) - tests now use the new API - Parquet
> logging can be set to WARNING level (Default) - Default compression for
> Parquet files (GZIP, as in parquet-mr) Author: Andre Schumacher &...    2
> days ago    SPARK-1383
>
> I will go to a stable checkin before this
>
>
>
>
> On Sat, Apr 5, 2014 at 2:22 PM, Debasish Das <debasish.das83@gmail.com
> >wrote:
>
> > I can compile with Java 7...let me try that...
> >
> >
> > On Sat, Apr 5, 2014 at 2:19 PM, Sean Owen <sowen@cloudera.com> wrote:
> >
> >> That method was added in Java 7. The project is on Java 6, so I think
> >> this was just an inadvertent error in a recent PR (it was the 'Spark
> >> parquet improvements' one).
> >>
> >> I'll open a hot-fix PR after looking for other stuff like this that
> >> might have snuck in.
> >> --
> >> Sean Owen | Director, Data Science | London
> >>
> >>
> >> On Sat, Apr 5, 2014 at 10:04 PM, Debasish Das <debasish.das83@gmail.com
> >
> >> wrote:
> >> > I am synced with apache/spark master but getting error in spark/sql
> >> > compilation...
> >> >
> >> > Is the master broken ?
> >> >
> >> > [info] Compiling 34 Scala sources to
> >> > /home/debasish/spark_deploy/sql/core/target/scala-2.10/classes...
> >> > [error]
> >> >
> >>
> /home/debasish/spark_deploy/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.scala:106:
> >> > value getGlobal is not a member of object java.util.logging.Logger
> >> > [error]       logger.setParent(Logger.getGlobal)
> >> > [error]                               ^
> >> > [error] one error found
> >> > [error] (sql/compile:compile) Compilation failed
> >> > [error] Total time: 171 s, completed Apr 5, 2014 4:58:41 PM
> >> >
> >> > Thanks.
> >> > Deb
> >>
> >
> >
>

--089e0149c9f8cce4d004f652d877--

From dev-return-7217-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Apr  5 22:10:22 2014
Return-Path: <dev-return-7217-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B0FBD100B5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  5 Apr 2014 22:10:22 +0000 (UTC)
Received: (qmail 70059 invoked by uid 500); 5 Apr 2014 22:10:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69581 invoked by uid 500); 5 Apr 2014 22:10:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69557 invoked by uid 99); 5 Apr 2014 22:10:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 22:10:19 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.160.41 as permitted sender)
Received: from [209.85.160.41] (HELO mail-pb0-f41.google.com) (209.85.160.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 22:10:14 +0000
Received: by mail-pb0-f41.google.com with SMTP id jt11so5051207pbb.0
        for <dev@spark.apache.org>; Sat, 05 Apr 2014 15:09:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=RViYNXlL1CVCbkXFv/5CZg9QyA4zCsidAWQaqaWU1jQ=;
        b=p9b0azOn/tl7+wcORbp7QJSThzsEN27rHAyPMwkuvMh+DY4hReNRGV4QXtld3aipLp
         Rji4VnSUIxiJQ5fdPG9J+mEET3j6ccPF+MNKFBEktBOSY5+zmGcTzzW2S28HCFvtsWHs
         qxDO9cOzU+amv/R5iYCJcPSKLTYVzkUj9GjknIm6DPTlOdGLMU5uym7vT84oS5y3zsqZ
         ing2mBJstkTqYMEQQUrJZFF0G2zYCeuF/ibmAXXHgZHtsic4A2THS5xOE7dYLm0mWPk6
         vvu5fVK3qGv9ZgA+giXv1UruF564vHhhfgtLUT7uL2JmYr8J97hLz8RsCbNbzIPzhbsi
         qOnQ==
MIME-Version: 1.0
X-Received: by 10.66.156.228 with SMTP id wh4mr11688685pab.21.1396735793481;
 Sat, 05 Apr 2014 15:09:53 -0700 (PDT)
Received: by 10.68.12.39 with HTTP; Sat, 5 Apr 2014 15:09:53 -0700 (PDT)
In-Reply-To: <CABPQxss=pKDpwRKi=QAT4kx8zRrVYL+riBLr-YaZtLhRsnQSSw@mail.gmail.com>
References: <CA+B-+fzZPVoYM1YWg2U7EqHwVPAfW5xe-Mg4A6C-S4JAhmAtpw@mail.gmail.com>
	<CAMAsSd+ys9hJQz+NCM86WyMZmUUhqiOCctZaEjVjdRKV1b77yQ@mail.gmail.com>
	<CA+B-+fwf+O5ix4rG8cJWSR_WBQEaVwHLxFOrP1RG7agVfR4vGg@mail.gmail.com>
	<CA+B-+fy_SeaYpZF8VfbOMaTp9Tuas40ipn4kZ15NQsi1DMrRXg@mail.gmail.com>
	<CABPQxss=pKDpwRKi=QAT4kx8zRrVYL+riBLr-YaZtLhRsnQSSw@mail.gmail.com>
Date: Sat, 5 Apr 2014 15:09:53 -0700
Message-ID: <CA+B-+fxzqG+Wn+WyPEV0J_tOhw0TqPD=y_XoKqtm2dBNFHYErQ@mail.gmail.com>
Subject: Re: Master compilation
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b5db56cd0ce4d04f652e428
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b5db56cd0ce4d04f652e428
Content-Type: text/plain; charset=ISO-8859-1

@patrick our cluster still has java6 deployed...and I compiled using jdk6...

Sean is looking into it...this api is in java7 but not java6...



On Sat, Apr 5, 2014 at 3:06 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> If you want to submit a hot fix for this issue specifically please do. I'm
> not sure why it didn't fail our build...
>
>
> On Sat, Apr 5, 2014 at 2:30 PM, Debasish Das <debasish.das83@gmail.com
> >wrote:
>
> > I verified this is happening for both CDH4.5 and 1.0.4...My deploy
> > environment is Java 6...so Java 7 compilation is not going to help...
> >
> > Is this the PR which caused it ?
> >
> > Andre Schumacher
> >
> >     fbebaed    Spark parquet improvements A few improvements to the
> Parquet
> > support for SQL queries: - Instead of files a ParquetRelation is now
> backed
> > by a directory, which simplifies importing data from other sources -
> > InsertIntoParquetTable operation now supports switching between
> overwriting
> > or appending (at least in HiveQL) - tests now use the new API - Parquet
> > logging can be set to WARNING level (Default) - Default compression for
> > Parquet files (GZIP, as in parquet-mr) Author: Andre Schumacher &...    2
> > days ago    SPARK-1383
> >
> > I will go to a stable checkin before this
> >
> >
> >
> >
> > On Sat, Apr 5, 2014 at 2:22 PM, Debasish Das <debasish.das83@gmail.com
> > >wrote:
> >
> > > I can compile with Java 7...let me try that...
> > >
> > >
> > > On Sat, Apr 5, 2014 at 2:19 PM, Sean Owen <sowen@cloudera.com> wrote:
> > >
> > >> That method was added in Java 7. The project is on Java 6, so I think
> > >> this was just an inadvertent error in a recent PR (it was the 'Spark
> > >> parquet improvements' one).
> > >>
> > >> I'll open a hot-fix PR after looking for other stuff like this that
> > >> might have snuck in.
> > >> --
> > >> Sean Owen | Director, Data Science | London
> > >>
> > >>
> > >> On Sat, Apr 5, 2014 at 10:04 PM, Debasish Das <
> debasish.das83@gmail.com
> > >
> > >> wrote:
> > >> > I am synced with apache/spark master but getting error in spark/sql
> > >> > compilation...
> > >> >
> > >> > Is the master broken ?
> > >> >
> > >> > [info] Compiling 34 Scala sources to
> > >> > /home/debasish/spark_deploy/sql/core/target/scala-2.10/classes...
> > >> > [error]
> > >> >
> > >>
> >
> /home/debasish/spark_deploy/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.scala:106:
> > >> > value getGlobal is not a member of object java.util.logging.Logger
> > >> > [error]       logger.setParent(Logger.getGlobal)
> > >> > [error]                               ^
> > >> > [error] one error found
> > >> > [error] (sql/compile:compile) Compilation failed
> > >> > [error] Total time: 171 s, completed Apr 5, 2014 4:58:41 PM
> > >> >
> > >> > Thanks.
> > >> > Deb
> > >>
> > >
> > >
> >
>

--047d7b5db56cd0ce4d04f652e428--

From dev-return-7218-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Apr  5 22:10:28 2014
Return-Path: <dev-return-7218-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9D9A0100B6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  5 Apr 2014 22:10:28 +0000 (UTC)
Received: (qmail 70250 invoked by uid 500); 5 Apr 2014 22:10:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70217 invoked by uid 500); 5 Apr 2014 22:10:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70208 invoked by uid 99); 5 Apr 2014 22:10:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 22:10:26 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.192.48 as permitted sender)
Received: from [209.85.192.48] (HELO mail-qg0-f48.google.com) (209.85.192.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 22:10:21 +0000
Received: by mail-qg0-f48.google.com with SMTP id i50so4121261qgf.35
        for <dev@spark.apache.org>; Sat, 05 Apr 2014 15:09:59 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=O2L8wglfu4AZpabMMTsHEEGtYECsna6eVDEaDCIeFQs=;
        b=Nued6hU8LgbtBy+ElV6fIxJKJ4zS4jasOcd7lfbMvcJc3d8iySwHHCFyTM1YnbnRVX
         OG1HenfwWYRDL5tBtVExDI0yRmdvXRVgmgUIoqHFplMtUiTCYb+dd7pNHwtUt7y8p1T5
         F+/1MjF0Dhys1aZNp7O3VraRpw/S6/JpPSrP02ZLXVHn3B2COUQ1A09lyTVvAyGHVeId
         ugf9Z5cLwbBahhbsBu3rciDkwj5840MVuETEXa/t2o6kIL5uCC5/6rpc3Rx+9jqN2QrG
         A9xP3zpSgXdBMsTf/usqAPl0D4YzDEmg0fppWWpFDSMxiTkUqcjboGAAYfAc7uvWhjSh
         whJg==
X-Gm-Message-State: ALoCoQm0KKrw0yUSV08nmHnoZp2OUtq8BRcSpVafaYy+plHxra174SAe1bD7cCM+/aF7PqL+0wzm
X-Received: by 10.229.28.2 with SMTP id k2mr4842844qcc.16.1396735798916; Sat,
 05 Apr 2014 15:09:58 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.107.194 with HTTP; Sat, 5 Apr 2014 15:09:38 -0700 (PDT)
In-Reply-To: <CABPQxss=pKDpwRKi=QAT4kx8zRrVYL+riBLr-YaZtLhRsnQSSw@mail.gmail.com>
References: <CA+B-+fzZPVoYM1YWg2U7EqHwVPAfW5xe-Mg4A6C-S4JAhmAtpw@mail.gmail.com>
 <CAMAsSd+ys9hJQz+NCM86WyMZmUUhqiOCctZaEjVjdRKV1b77yQ@mail.gmail.com>
 <CA+B-+fwf+O5ix4rG8cJWSR_WBQEaVwHLxFOrP1RG7agVfR4vGg@mail.gmail.com>
 <CA+B-+fy_SeaYpZF8VfbOMaTp9Tuas40ipn4kZ15NQsi1DMrRXg@mail.gmail.com> <CABPQxss=pKDpwRKi=QAT4kx8zRrVYL+riBLr-YaZtLhRsnQSSw@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Sat, 5 Apr 2014 23:09:38 +0100
Message-ID: <CAMAsSd+msPg6KwxNgPYBO-0WFf4pzUGV8kytStYhgtHRhXAizQ@mail.gmail.com>
Subject: Re: Master compilation
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Will do. I'm just finishing a recompile to check for anything else like this.

The reason is because the tests run with Java 7 (like lots of us do
including me) so it used the Java 7 classpath and found the class.
It's possible to use Java 7 with the Java 6 -bootclasspath. Or just
use Java 6.
--
Sean Owen | Director, Data Science | London


On Sat, Apr 5, 2014 at 11:06 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> If you want to submit a hot fix for this issue specifically please do. I'm
> not sure why it didn't fail our build...
>
>
> On Sat, Apr 5, 2014 at 2:30 PM, Debasish Das <debasish.das83@gmail.com>wrote:
>
>> I verified this is happening for both CDH4.5 and 1.0.4...My deploy
>> environment is Java 6...so Java 7 compilation is not going to help...
>>
>> Is this the PR which caused it ?
>>
>> Andre Schumacher
>>
>>     fbebaed    Spark parquet improvements A few improvements to the Parquet
>> support for SQL queries: - Instead of files a ParquetRelation is now backed
>> by a directory, which simplifies importing data from other sources -
>> InsertIntoParquetTable operation now supports switching between overwriting
>> or appending (at least in HiveQL) - tests now use the new API - Parquet
>> logging can be set to WARNING level (Default) - Default compression for
>> Parquet files (GZIP, as in parquet-mr) Author: Andre Schumacher &...    2
>> days ago    SPARK-1383
>>
>> I will go to a stable checkin before this
>>
>>
>>
>>
>> On Sat, Apr 5, 2014 at 2:22 PM, Debasish Das <debasish.das83@gmail.com
>> >wrote:
>>
>> > I can compile with Java 7...let me try that...
>> >
>> >
>> > On Sat, Apr 5, 2014 at 2:19 PM, Sean Owen <sowen@cloudera.com> wrote:
>> >
>> >> That method was added in Java 7. The project is on Java 6, so I think
>> >> this was just an inadvertent error in a recent PR (it was the 'Spark
>> >> parquet improvements' one).
>> >>
>> >> I'll open a hot-fix PR after looking for other stuff like this that
>> >> might have snuck in.
>> >> --
>> >> Sean Owen | Director, Data Science | London
>> >>
>> >>
>> >> On Sat, Apr 5, 2014 at 10:04 PM, Debasish Das <debasish.das83@gmail.com
>> >
>> >> wrote:
>> >> > I am synced with apache/spark master but getting error in spark/sql
>> >> > compilation...
>> >> >
>> >> > Is the master broken ?
>> >> >
>> >> > [info] Compiling 34 Scala sources to
>> >> > /home/debasish/spark_deploy/sql/core/target/scala-2.10/classes...
>> >> > [error]
>> >> >
>> >>
>> /home/debasish/spark_deploy/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.scala:106:
>> >> > value getGlobal is not a member of object java.util.logging.Logger
>> >> > [error]       logger.setParent(Logger.getGlobal)
>> >> > [error]                               ^
>> >> > [error] one error found
>> >> > [error] (sql/compile:compile) Compilation failed
>> >> > [error] Total time: 171 s, completed Apr 5, 2014 4:58:41 PM
>> >> >
>> >> > Thanks.
>> >> > Deb
>> >>
>> >
>> >
>>

From dev-return-7219-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Apr  5 23:30:32 2014
Return-Path: <dev-return-7219-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E2E0C101BE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  5 Apr 2014 23:30:31 +0000 (UTC)
Received: (qmail 2813 invoked by uid 500); 5 Apr 2014 23:30:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2781 invoked by uid 500); 5 Apr 2014 23:30:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2773 invoked by uid 99); 5 Apr 2014 23:30:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 23:30:30 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.216.169 as permitted sender)
Received: from [209.85.216.169] (HELO mail-qc0-f169.google.com) (209.85.216.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 23:30:24 +0000
Received: by mail-qc0-f169.google.com with SMTP id i17so5069472qcy.0
        for <dev@spark.apache.org>; Sat, 05 Apr 2014 16:30:02 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=hGO9NVmBxhXd497AUhG4SziuW3ktEOLkJkfR7xIOGxk=;
        b=zyumnt1t4KkFZyDlt1QU4rQX+H29rLEGysMW/tXDKrVGOXEFQI6eIIh9hTbXTQuEhf
         ugUFZBSiJBxrp2vCtUHkt0gnI4FD5enc4wqXURdXXmkA8H0nLECJgbNQXQUC2ONhEBvs
         iL9B83cZZ4569fH8YS3vW+ArnCplhguMT8+NH+w8ajrJfsIHI1yrjYah2JCFNX5D0iyU
         Q77gV8tZzoyBd2Kx9poGcUwdDhy2jbQqrpBCkeosssWI0gogxgj+zErfkpgNhL1f+Wvg
         jvIVH9p6Ua19yHlP6Oa6fi7h+S+qwawdD/MFkA2D8+wKSjNzmAbClFr3zx8wZ7tYeFl+
         vfhg==
MIME-Version: 1.0
X-Received: by 10.224.14.77 with SMTP id f13mr23717773qaa.31.1396740602475;
 Sat, 05 Apr 2014 16:30:02 -0700 (PDT)
Received: by 10.140.51.195 with HTTP; Sat, 5 Apr 2014 16:30:02 -0700 (PDT)
Date: Sun, 6 Apr 2014 05:00:02 +0530
Message-ID: <CAJiQeYLDM+WvPBX_OXGGYQg4v2Pr5tsRYfJmrcbjzKA1_O_feg@mail.gmail.com>
Subject: ephemeral storage level in spark ?
From: Mridul Muralidharan <mridul@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

  We have a requirement to use a (potential) ephemeral storage, which
is not within the VM, which is strongly tied to a worker node. So
source of truth for a block would still be within spark; but to
actually do computation, we would need to copy data to external device
(where it might lie around for a while : so data locality really
really helps if we can avoid a subsequent copy if it is already
present on computations on same block again).

I was wondering if the recently added storage level for tachyon would
help in this case (note, tachyon wont help; just the storage level
might).
What sort of guarantees does it provide ? How extensible is it ? Or is
it strongly tied to tachyon with only a generic name ?


Thanks,
Mridul

From dev-return-7220-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Apr  5 23:49:40 2014
Return-Path: <dev-return-7220-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 48D68101E3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  5 Apr 2014 23:49:40 +0000 (UTC)
Received: (qmail 9664 invoked by uid 500); 5 Apr 2014 23:49:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9558 invoked by uid 500); 5 Apr 2014 23:49:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9550 invoked by uid 99); 5 Apr 2014 23:49:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 23:49:38 +0000
X-ASF-Spam-Status: No, hits=2.1 required=10.0
	tests=HK_RANDOM_ENVFROM,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of haoyuan.li@gmail.com designates 209.85.212.176 as permitted sender)
Received: from [209.85.212.176] (HELO mail-wi0-f176.google.com) (209.85.212.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Apr 2014 23:49:34 +0000
Received: by mail-wi0-f176.google.com with SMTP id r20so3174724wiv.9
        for <dev@spark.apache.org>; Sat, 05 Apr 2014 16:49:12 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=ZZHjOQ+5BBzRCxHsZUhvm40X42QXrgLIrh4d6UUZPqg=;
        b=SwdGWuz+CgblrLBnYuBaWIGXDE7o5nNcHONq2SNtUfRKvm/5cTvba1yOJQ7tzs2MGI
         FDifHWFzF4QDmfdkWEkaUZf1FIFW/TzmbvdnZItm/4eehc9wcoEK6BuFOHyoJWb32HHC
         9R73LkSnatWiZek6ffxi+z5c0hq/BdHpfNdUkGodzGL+heulGYHGkHLTyWlzSSVNg8kI
         nzT1JuLprc8qHFp4is/Ir0u73Z6HezADL++qQDEpOwC4QKXcjyt1AuwFAeRdvKOCqHUY
         WdyUJHPYU89O4lb6xLypGhKOaaYtL0nwjWPem15OLQCL5p2HSxlWcOgaD1PyqibsA3xo
         HSgA==
X-Received: by 10.180.107.136 with SMTP id hc8mr15080376wib.11.1396741752815;
 Sat, 05 Apr 2014 16:49:12 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.216.157.66 with HTTP; Sat, 5 Apr 2014 16:48:52 -0700 (PDT)
In-Reply-To: <CAJiQeYLDM+WvPBX_OXGGYQg4v2Pr5tsRYfJmrcbjzKA1_O_feg@mail.gmail.com>
References: <CAJiQeYLDM+WvPBX_OXGGYQg4v2Pr5tsRYfJmrcbjzKA1_O_feg@mail.gmail.com>
From: Haoyuan Li <haoyuan.li@gmail.com>
Date: Sat, 5 Apr 2014 16:48:52 -0700
Message-ID: <CAG2iju2wBimE+y=n6QQOy7QGO31oWKYoXzzJ=zCA_Ew4NWLG8Q@mail.gmail.com>
Subject: Re: ephemeral storage level in spark ?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=e89a8f3bad4505082304f65448c7
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f3bad4505082304f65448c7
Content-Type: text/plain; charset=ISO-8859-1

Hi Mridul,

Do you mean the scenario that different Spark applications need to read the
same raw data, which is stored in a remote cluster or machines. And the
goal is to load the remote raw data only once?

Haoyuan


On Sat, Apr 5, 2014 at 4:30 PM, Mridul Muralidharan <mridul@gmail.com>wrote:

> Hi,
>
>   We have a requirement to use a (potential) ephemeral storage, which
> is not within the VM, which is strongly tied to a worker node. So
> source of truth for a block would still be within spark; but to
> actually do computation, we would need to copy data to external device
> (where it might lie around for a while : so data locality really
> really helps if we can avoid a subsequent copy if it is already
> present on computations on same block again).
>
> I was wondering if the recently added storage level for tachyon would
> help in this case (note, tachyon wont help; just the storage level
> might).
> What sort of guarantees does it provide ? How extensible is it ? Or is
> it strongly tied to tachyon with only a generic name ?
>
>
> Thanks,
> Mridul
>



-- 
Haoyuan Li
Algorithms, Machines, People Lab, EECS, UC Berkeley
http://www.cs.berkeley.edu/~haoyuan/

--e89a8f3bad4505082304f65448c7--

From dev-return-7221-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Apr  6 00:15:16 2014
Return-Path: <dev-return-7221-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 37EED102B0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  6 Apr 2014 00:15:16 +0000 (UTC)
Received: (qmail 27356 invoked by uid 500); 6 Apr 2014 00:15:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27269 invoked by uid 500); 6 Apr 2014 00:15:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27261 invoked by uid 99); 6 Apr 2014 00:15:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 00:15:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.216.53 as permitted sender)
Received: from [209.85.216.53] (HELO mail-qa0-f53.google.com) (209.85.216.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 00:15:10 +0000
Received: by mail-qa0-f53.google.com with SMTP id w8so4493893qac.40
        for <dev@spark.apache.org>; Sat, 05 Apr 2014 17:14:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=INZOfW0CJ3eVvWBh9fW98ZVA0D9YUXlmyYO2oLEeobY=;
        b=jLFOiyBzPCu8zwYFNh4cq9p8l1vsQGl6npHQC2zgrxv1hbrzrSaWL4gHGTLd98jA9p
         M+j5jjYoLSC7yBlV1goQA6keE5YN4lO4mu4CUIgnYGl+XUSPazmC1XIs4314JP1ImwNc
         l6fKeuU8hHPJOBQ68UIi7f7WY3PE7b2NrRDmjZlHA/qFw/3bbE/KST8f8akYjN/SdJ+L
         Y1Gf/s4QsuWjEwD3msvUS6cYW3V1dG4MWddedqMZM2n8NRHa25DZz4Dl6ONftnWF/aC8
         diftueB9q5b03UxsL0IbXRnss6qBouTbl2UkQIb6WlYrmFj6vbzk+NtTDx5chkw8ZCG3
         gN8g==
MIME-Version: 1.0
X-Received: by 10.229.66.133 with SMTP id n5mr24048961qci.0.1396743289289;
 Sat, 05 Apr 2014 17:14:49 -0700 (PDT)
Received: by 10.140.51.195 with HTTP; Sat, 5 Apr 2014 17:14:49 -0700 (PDT)
Received: by 10.140.51.195 with HTTP; Sat, 5 Apr 2014 17:14:49 -0700 (PDT)
In-Reply-To: <CAG2iju2wBimE+y=n6QQOy7QGO31oWKYoXzzJ=zCA_Ew4NWLG8Q@mail.gmail.com>
References: <CAJiQeYLDM+WvPBX_OXGGYQg4v2Pr5tsRYfJmrcbjzKA1_O_feg@mail.gmail.com>
	<CAG2iju2wBimE+y=n6QQOy7QGO31oWKYoXzzJ=zCA_Ew4NWLG8Q@mail.gmail.com>
Date: Sun, 6 Apr 2014 05:44:49 +0530
Message-ID: <CAJiQeYJe+762P+nD4L6bO-SrW2_7CDsc7KfXNYOCPD4-h0zY_g@mail.gmail.com>
Subject: Re: ephemeral storage level in spark ?
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2ffee99c3b004f654a352
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ffee99c3b004f654a352
Content-Type: text/plain; charset=ISO-8859-1

No, I am thinking along lines of writing to an accelerator card or
dedicated card with its own memory.

Regards,
Mridul
On Apr 6, 2014 5:19 AM, "Haoyuan Li" <haoyuan.li@gmail.com> wrote:

> Hi Mridul,
>
> Do you mean the scenario that different Spark applications need to read the
> same raw data, which is stored in a remote cluster or machines. And the
> goal is to load the remote raw data only once?
>
> Haoyuan
>
>
> On Sat, Apr 5, 2014 at 4:30 PM, Mridul Muralidharan <mridul@gmail.com
> >wrote:
>
> > Hi,
> >
> >   We have a requirement to use a (potential) ephemeral storage, which
> > is not within the VM, which is strongly tied to a worker node. So
> > source of truth for a block would still be within spark; but to
> > actually do computation, we would need to copy data to external device
> > (where it might lie around for a while : so data locality really
> > really helps if we can avoid a subsequent copy if it is already
> > present on computations on same block again).
> >
> > I was wondering if the recently added storage level for tachyon would
> > help in this case (note, tachyon wont help; just the storage level
> > might).
> > What sort of guarantees does it provide ? How extensible is it ? Or is
> > it strongly tied to tachyon with only a generic name ?
> >
> >
> > Thanks,
> > Mridul
> >
>
>
>
> --
> Haoyuan Li
> Algorithms, Machines, People Lab, EECS, UC Berkeley
> http://www.cs.berkeley.edu/~haoyuan/
>

--001a11c2ffee99c3b004f654a352--

From dev-return-7222-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Apr  6 01:10:57 2014
Return-Path: <dev-return-7222-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CCB5710383
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  6 Apr 2014 01:10:57 +0000 (UTC)
Received: (qmail 55676 invoked by uid 500); 6 Apr 2014 01:10:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55607 invoked by uid 500); 6 Apr 2014 01:10:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55599 invoked by uid 99); 6 Apr 2014 01:10:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 01:10:56 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.160.53 as permitted sender)
Received: from [209.85.160.53] (HELO mail-pb0-f53.google.com) (209.85.160.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 01:10:51 +0000
Received: by mail-pb0-f53.google.com with SMTP id rp16so5099457pbb.40
        for <dev@spark.apache.org>; Sat, 05 Apr 2014 18:10:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=OjadvHmF/qT1/Cvv1XGG4wVNZ5Xd3hGqv8GbEvAufxQ=;
        b=nW6ZtErfTuF6gXnfaK6AsMWAkebJb5R6n02UHfZe2qzuc6OdrzVp5gv22NaWGuXRis
         8QuCQmmKsp4HEnAFOiGQF/YmQSQbUmMf9xSynAekf2Qf35azwrG6DP1/OqoHfN3rhJNq
         M+OB0fDvcLvXeOqwR1/SOIom+YmBROt1I9EFuPcbQHNWcazfSgB0ORGE+ncBvao91YNz
         6t3Fh8rMdpg2y4dBvTIu93Qow7sXSJTCglbhYSCaglniVRRUUiNWv2ta6nhlDrr547BK
         uez3wpRn6OJn+8BKvZ67akpiNF2oNTS7UEQIBz1z94UeX0pAeoKSG+tG6DGh4KZ3SaWu
         rwpg==
MIME-Version: 1.0
X-Received: by 10.66.149.7 with SMTP id tw7mr23255940pab.72.1396746631153;
 Sat, 05 Apr 2014 18:10:31 -0700 (PDT)
Received: by 10.68.12.39 with HTTP; Sat, 5 Apr 2014 18:10:31 -0700 (PDT)
In-Reply-To: <CAMAsSd+msPg6KwxNgPYBO-0WFf4pzUGV8kytStYhgtHRhXAizQ@mail.gmail.com>
References: <CA+B-+fzZPVoYM1YWg2U7EqHwVPAfW5xe-Mg4A6C-S4JAhmAtpw@mail.gmail.com>
	<CAMAsSd+ys9hJQz+NCM86WyMZmUUhqiOCctZaEjVjdRKV1b77yQ@mail.gmail.com>
	<CA+B-+fwf+O5ix4rG8cJWSR_WBQEaVwHLxFOrP1RG7agVfR4vGg@mail.gmail.com>
	<CA+B-+fy_SeaYpZF8VfbOMaTp9Tuas40ipn4kZ15NQsi1DMrRXg@mail.gmail.com>
	<CABPQxss=pKDpwRKi=QAT4kx8zRrVYL+riBLr-YaZtLhRsnQSSw@mail.gmail.com>
	<CAMAsSd+msPg6KwxNgPYBO-0WFf4pzUGV8kytStYhgtHRhXAizQ@mail.gmail.com>
Date: Sat, 5 Apr 2014 18:10:31 -0700
Message-ID: <CA+B-+fzE4ghtHXOPqSmhDRVX3SKc9AH=py1gZAEUaB+YEbk5mw@mail.gmail.com>
Subject: Re: Master compilation
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b6d8a44ca8f6404f6556afa
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6d8a44ca8f6404f6556afa
Content-Type: text/plain; charset=ISO-8859-1

With jdk7 I could compile it fine:

java version "1.7.0_51"
Java(TM) SE Runtime Environment (build 1.7.0_51-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode)

What happens if I say take the jar and try to deploy it on ancient centos6
default on cluster ?

java -version
java version "1.6.0_31"
Java(TM) SE Runtime Environment (build 1.6.0_31-b04)
Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode)

Breeze compilation also fails with jdk6, runs fine with jdk7 and breeze jar
is already included in spark mllib with Xiangrui's Sparse vector checkin....

Does that mean that classes compiled and generated using jdk7 will run fine
on jre6 ?

I am confused


On Sat, Apr 5, 2014 at 3:09 PM, Sean Owen <sowen@cloudera.com> wrote:

> Will do. I'm just finishing a recompile to check for anything else like
> this.
>
> The reason is because the tests run with Java 7 (like lots of us do
> including me) so it used the Java 7 classpath and found the class.
> It's possible to use Java 7 with the Java 6 -bootclasspath. Or just
> use Java 6.
> --
> Sean Owen | Director, Data Science | London
>
>
> On Sat, Apr 5, 2014 at 11:06 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > If you want to submit a hot fix for this issue specifically please do.
> I'm
> > not sure why it didn't fail our build...
> >
> >
> > On Sat, Apr 5, 2014 at 2:30 PM, Debasish Das <debasish.das83@gmail.com
> >wrote:
> >
> >> I verified this is happening for both CDH4.5 and 1.0.4...My deploy
> >> environment is Java 6...so Java 7 compilation is not going to help...
> >>
> >> Is this the PR which caused it ?
> >>
> >> Andre Schumacher
> >>
> >>     fbebaed    Spark parquet improvements A few improvements to the
> Parquet
> >> support for SQL queries: - Instead of files a ParquetRelation is now
> backed
> >> by a directory, which simplifies importing data from other sources -
> >> InsertIntoParquetTable operation now supports switching between
> overwriting
> >> or appending (at least in HiveQL) - tests now use the new API - Parquet
> >> logging can be set to WARNING level (Default) - Default compression for
> >> Parquet files (GZIP, as in parquet-mr) Author: Andre Schumacher &...
>  2
> >> days ago    SPARK-1383
> >>
> >> I will go to a stable checkin before this
> >>
> >>
> >>
> >>
> >> On Sat, Apr 5, 2014 at 2:22 PM, Debasish Das <debasish.das83@gmail.com
> >> >wrote:
> >>
> >> > I can compile with Java 7...let me try that...
> >> >
> >> >
> >> > On Sat, Apr 5, 2014 at 2:19 PM, Sean Owen <sowen@cloudera.com> wrote:
> >> >
> >> >> That method was added in Java 7. The project is on Java 6, so I think
> >> >> this was just an inadvertent error in a recent PR (it was the 'Spark
> >> >> parquet improvements' one).
> >> >>
> >> >> I'll open a hot-fix PR after looking for other stuff like this that
> >> >> might have snuck in.
> >> >> --
> >> >> Sean Owen | Director, Data Science | London
> >> >>
> >> >>
> >> >> On Sat, Apr 5, 2014 at 10:04 PM, Debasish Das <
> debasish.das83@gmail.com
> >> >
> >> >> wrote:
> >> >> > I am synced with apache/spark master but getting error in spark/sql
> >> >> > compilation...
> >> >> >
> >> >> > Is the master broken ?
> >> >> >
> >> >> > [info] Compiling 34 Scala sources to
> >> >> > /home/debasish/spark_deploy/sql/core/target/scala-2.10/classes...
> >> >> > [error]
> >> >> >
> >> >>
> >>
> /home/debasish/spark_deploy/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.scala:106:
> >> >> > value getGlobal is not a member of object java.util.logging.Logger
> >> >> > [error]       logger.setParent(Logger.getGlobal)
> >> >> > [error]                               ^
> >> >> > [error] one error found
> >> >> > [error] (sql/compile:compile) Compilation failed
> >> >> > [error] Total time: 171 s, completed Apr 5, 2014 4:58:41 PM
> >> >> >
> >> >> > Thanks.
> >> >> > Deb
> >> >>
> >> >
> >> >
> >>
>

--047d7b6d8a44ca8f6404f6556afa--

From dev-return-7223-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Apr  6 05:54:07 2014
Return-Path: <dev-return-7223-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 66E0410745
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  6 Apr 2014 05:54:07 +0000 (UTC)
Received: (qmail 85133 invoked by uid 500); 6 Apr 2014 05:54:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84570 invoked by uid 500); 6 Apr 2014 05:54:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84558 invoked by uid 99); 6 Apr 2014 05:54:02 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 05:54:02 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.220.51 as permitted sender)
Received: from [209.85.220.51] (HELO mail-pa0-f51.google.com) (209.85.220.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 05:53:57 +0000
Received: by mail-pa0-f51.google.com with SMTP id kq14so5288267pab.38
        for <dev@spark.apache.org>; Sat, 05 Apr 2014 22:53:36 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=avgvuxpGOIufLxQWQ3yvTziWE/Y45Nfa6s7eBXom+oQ=;
        b=g7uM9EXVMHDrJnEq+S2u73Y6A8TdtcQwSGoZLsNTH6T58g+dK3Ova013LYi3hDZUim
         KL5xoWhAMK7Xo220e3i5gxkDR9B8YCo5YDbeTBWE6IjwHGGmiOeR4uNYh2SSVXab3We6
         KESgGEqABSWk7M28iJGo4HSVVGuol4FJQ1Y3pSH8AzqUI/EPuCzchmDuYRwZlZhp4iYA
         z6BVlZeE0TBA9fEU/MYD2Tae2Xobt8/v5330XDU8lKP/tgCPUClemShcnow6bd0oV33t
         0q9QM+xhxJFUkfN3dSaRdrXlFklhJnrQq30622mdyp8yAmqaU7AWUNlFf4sYC00Ow5AU
         EzvA==
MIME-Version: 1.0
X-Received: by 10.66.249.165 with SMTP id yv5mr6823516pac.79.1396763616767;
 Sat, 05 Apr 2014 22:53:36 -0700 (PDT)
Received: by 10.68.12.39 with HTTP; Sat, 5 Apr 2014 22:53:36 -0700 (PDT)
Date: Sat, 5 Apr 2014 22:53:36 -0700
Message-ID: <CA+B-+fzYxLU6OUX6hufpsbAmJLoYbPQupMRK=MR8Yx6jkLOj2g@mail.gmail.com>
Subject: ALS array index out of bound with 50 factors
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b10d00f3672c604f6595f91
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b10d00f3672c604f6595f91
Content-Type: text/plain; charset=ISO-8859-1

Hi,

I deployed apache/spark master today and recently there were many ALS
related checkins and enhancements..

I am running ALS with explicit feedback and I remember most enhancements
were related to implicit feedback...

With 25 factors my runs were successful but with 50 factors I am getting
array index out of bound...

Note that I was hitting gc errors before with an older version of spark but
it seems like the sparse matrix partitioning scheme has changed now...data
caching looks much balanced now...earlier one node was becoming
bottleneck...Although I ran with 64g memory per node...

There are around 3M products, 25M users...

Anyone noticed this bug or something similar ?

14/04/05 23:03:15 WARN TaskSetManager: Loss was due to
java.lang.ArrayIndexOutOfBoundsException
java.lang.ArrayIndexOutOfBoundsException: 81029
    at
org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateBlock$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(ALS.scala:450)
    at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
    at
org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateBlock$1.apply$mcVI$sp(ALS.scala:446)
    at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
    at org.apache.spark.mllib.recommendation.ALS.org
$apache$spark$mllib$recommendation$ALS$$updateBlock(ALS.scala:445)
    at
org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:416)
    at
org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:415)
    at
org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31)
    at
org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at
org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:149)
    at
org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:147)
    at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:147)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
    at
org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
    at
org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
    at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
    at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)
    at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
    at org.apache.spark.scheduler.Task.run(Task.scala:52)
    at
org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
    at
org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:43)
    at
org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:396)
    at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
    at
org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:42)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)

Thanks.
Deb

--047d7b10d00f3672c604f6595f91--

From dev-return-7224-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Apr  6 15:42:16 2014
Return-Path: <dev-return-7224-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E3B4811122
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  6 Apr 2014 15:42:16 +0000 (UTC)
Received: (qmail 95250 invoked by uid 500); 6 Apr 2014 15:42:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95208 invoked by uid 500); 6 Apr 2014 15:42:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95200 invoked by uid 99); 6 Apr 2014 15:42:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 15:42:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.212.171] (HELO mail-wi0-f171.google.com) (209.85.212.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 15:42:09 +0000
Received: by mail-wi0-f171.google.com with SMTP id q5so3790839wiv.4
        for <dev@spark.apache.org>; Sun, 06 Apr 2014 08:41:47 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=Wfxwgeh5WNoSFRvBd9b8ajrLIToPidJziK8LOFBhYjw=;
        b=a1TaeRzsJoGefZ4NKvCqX0ixgVHz3lcNRLOwz2wN2eqhXsCIOL/QOijtRyP7xEF5gJ
         EZNkmsRy8oadbhhOuNtmthcmHixJAoBlcZJXOuVEffjDEgdDikQo+2AztgHUxeYoAqcf
         mznzLfIGu/fJCgIie+r9eR/lx4Qf/6LgUHFB/sYuf9p6trEQBe2x/7X84pDbvwhpK7m8
         SRWtRhEUG2MGnvFVHKSfR+DnZOKTVwQdqePILDOKdBjZIDYzVE4+Do1y74xY5GuORZP7
         4byQwQPr9WD2O5VhqBi9M5dqxLYL+vwpI3Lp/saIxv/eLYM+yyfxqVVFiqHxXh/OPIuX
         MpEw==
X-Gm-Message-State: ALoCoQlDzIIS/SJtQCLlN/EIDDdBGvvVWEU1bCHs4eB7Zh7ig3wVV09JmYm7CQIKZD8/3CVd3wuH
MIME-Version: 1.0
X-Received: by 10.180.87.233 with SMTP id bb9mr19681179wib.10.1396798907527;
 Sun, 06 Apr 2014 08:41:47 -0700 (PDT)
Received: by 10.217.66.129 with HTTP; Sun, 6 Apr 2014 08:41:47 -0700 (PDT)
X-Originating-IP: [209.150.41.132]
In-Reply-To: <CA+B-+fzE4ghtHXOPqSmhDRVX3SKc9AH=py1gZAEUaB+YEbk5mw@mail.gmail.com>
References: <CA+B-+fzZPVoYM1YWg2U7EqHwVPAfW5xe-Mg4A6C-S4JAhmAtpw@mail.gmail.com>
	<CAMAsSd+ys9hJQz+NCM86WyMZmUUhqiOCctZaEjVjdRKV1b77yQ@mail.gmail.com>
	<CA+B-+fwf+O5ix4rG8cJWSR_WBQEaVwHLxFOrP1RG7agVfR4vGg@mail.gmail.com>
	<CA+B-+fy_SeaYpZF8VfbOMaTp9Tuas40ipn4kZ15NQsi1DMrRXg@mail.gmail.com>
	<CABPQxss=pKDpwRKi=QAT4kx8zRrVYL+riBLr-YaZtLhRsnQSSw@mail.gmail.com>
	<CAMAsSd+msPg6KwxNgPYBO-0WFf4pzUGV8kytStYhgtHRhXAizQ@mail.gmail.com>
	<CA+B-+fzE4ghtHXOPqSmhDRVX3SKc9AH=py1gZAEUaB+YEbk5mw@mail.gmail.com>
Date: Sun, 6 Apr 2014 11:41:47 -0400
Message-ID: <CANx3uAhFQmrhcqCrvsNadMpF5WGKR2UD3FHuYCP6oO41T6BNRw@mail.gmail.com>
Subject: Re: Master compilation
From: Koert Kuipers <koert@tresata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d044480afb4c92f04f6619645
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d044480afb4c92f04f6619645
Content-Type: text/plain; charset=ISO-8859-1

classes compiled with java7 run fine on java6 if you specified "-target
1.6". however if thats the case generally you should also be able to also
then compile it with java 6 just fine.

something compiled with java7 with "-target 1.7" will not run on java 6



On Sat, Apr 5, 2014 at 9:10 PM, Debasish Das <debasish.das83@gmail.com>wrote:

> With jdk7 I could compile it fine:
>
> java version "1.7.0_51"
> Java(TM) SE Runtime Environment (build 1.7.0_51-b13)
> Java HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode)
>
> What happens if I say take the jar and try to deploy it on ancient centos6
> default on cluster ?
>
> java -version
> java version "1.6.0_31"
> Java(TM) SE Runtime Environment (build 1.6.0_31-b04)
> Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode)
>
> Breeze compilation also fails with jdk6, runs fine with jdk7 and breeze jar
> is already included in spark mllib with Xiangrui's Sparse vector
> checkin....
>
> Does that mean that classes compiled and generated using jdk7 will run fine
> on jre6 ?
>
> I am confused
>
>
> On Sat, Apr 5, 2014 at 3:09 PM, Sean Owen <sowen@cloudera.com> wrote:
>
> > Will do. I'm just finishing a recompile to check for anything else like
> > this.
> >
> > The reason is because the tests run with Java 7 (like lots of us do
> > including me) so it used the Java 7 classpath and found the class.
> > It's possible to use Java 7 with the Java 6 -bootclasspath. Or just
> > use Java 6.
> > --
> > Sean Owen | Director, Data Science | London
> >
> >
> > On Sat, Apr 5, 2014 at 11:06 PM, Patrick Wendell <pwendell@gmail.com>
> > wrote:
> > > If you want to submit a hot fix for this issue specifically please do.
> > I'm
> > > not sure why it didn't fail our build...
> > >
> > >
> > > On Sat, Apr 5, 2014 at 2:30 PM, Debasish Das <debasish.das83@gmail.com
> > >wrote:
> > >
> > >> I verified this is happening for both CDH4.5 and 1.0.4...My deploy
> > >> environment is Java 6...so Java 7 compilation is not going to help...
> > >>
> > >> Is this the PR which caused it ?
> > >>
> > >> Andre Schumacher
> > >>
> > >>     fbebaed    Spark parquet improvements A few improvements to the
> > Parquet
> > >> support for SQL queries: - Instead of files a ParquetRelation is now
> > backed
> > >> by a directory, which simplifies importing data from other sources -
> > >> InsertIntoParquetTable operation now supports switching between
> > overwriting
> > >> or appending (at least in HiveQL) - tests now use the new API -
> Parquet
> > >> logging can be set to WARNING level (Default) - Default compression
> for
> > >> Parquet files (GZIP, as in parquet-mr) Author: Andre Schumacher &...
> >  2
> > >> days ago    SPARK-1383
> > >>
> > >> I will go to a stable checkin before this
> > >>
> > >>
> > >>
> > >>
> > >> On Sat, Apr 5, 2014 at 2:22 PM, Debasish Das <
> debasish.das83@gmail.com
> > >> >wrote:
> > >>
> > >> > I can compile with Java 7...let me try that...
> > >> >
> > >> >
> > >> > On Sat, Apr 5, 2014 at 2:19 PM, Sean Owen <sowen@cloudera.com>
> wrote:
> > >> >
> > >> >> That method was added in Java 7. The project is on Java 6, so I
> think
> > >> >> this was just an inadvertent error in a recent PR (it was the
> 'Spark
> > >> >> parquet improvements' one).
> > >> >>
> > >> >> I'll open a hot-fix PR after looking for other stuff like this that
> > >> >> might have snuck in.
> > >> >> --
> > >> >> Sean Owen | Director, Data Science | London
> > >> >>
> > >> >>
> > >> >> On Sat, Apr 5, 2014 at 10:04 PM, Debasish Das <
> > debasish.das83@gmail.com
> > >> >
> > >> >> wrote:
> > >> >> > I am synced with apache/spark master but getting error in
> spark/sql
> > >> >> > compilation...
> > >> >> >
> > >> >> > Is the master broken ?
> > >> >> >
> > >> >> > [info] Compiling 34 Scala sources to
> > >> >> > /home/debasish/spark_deploy/sql/core/target/scala-2.10/classes...
> > >> >> > [error]
> > >> >> >
> > >> >>
> > >>
> >
> /home/debasish/spark_deploy/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.scala:106:
> > >> >> > value getGlobal is not a member of object
> java.util.logging.Logger
> > >> >> > [error]       logger.setParent(Logger.getGlobal)
> > >> >> > [error]                               ^
> > >> >> > [error] one error found
> > >> >> > [error] (sql/compile:compile) Compilation failed
> > >> >> > [error] Total time: 171 s, completed Apr 5, 2014 4:58:41 PM
> > >> >> >
> > >> >> > Thanks.
> > >> >> > Deb
> > >> >>
> > >> >
> > >> >
> > >>
> >
>

--f46d044480afb4c92f04f6619645--

From dev-return-7225-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Apr  6 16:00:37 2014
Return-Path: <dev-return-7225-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E93141113F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  6 Apr 2014 16:00:37 +0000 (UTC)
Received: (qmail 6714 invoked by uid 500); 6 Apr 2014 16:00:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6603 invoked by uid 500); 6 Apr 2014 16:00:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6595 invoked by uid 99); 6 Apr 2014 16:00:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 16:00:36 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.160.50 as permitted sender)
Received: from [209.85.160.50] (HELO mail-pb0-f50.google.com) (209.85.160.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 16:00:31 +0000
Received: by mail-pb0-f50.google.com with SMTP id md12so5628307pbc.37
        for <dev@spark.apache.org>; Sun, 06 Apr 2014 09:00:09 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=l5Drh2cEbtC3NVqIXZc4M/bjYHcYdwLsvYMOLsOw7+Y=;
        b=CdNrElVk/8LVBJz0UvUIT31a7/rc3uDxSif+trzgHuCpiddDlwZM3pH9QL5w/xOmno
         4qXNjtUQY7nlt16B3L9IYAglnBngpGZ71BoKOGNs4uU27qRINeOKWWTV46fKNXWLKhpZ
         Yiv24agGaTXBeWB1HZNVSbvin3HmZqzC2NTfSM700sdRWD/ZiYEEmuX02tZzue7pIe0T
         RPGduN91qGsUN36eAUHA03LkI4bgtnEkY5b3mwxS9YQjijMaU4kXbtLDE5SCejfbVeRa
         N59P2CblrRBMnhEJfRxyCvWTzCLdqXT71U8B7JNd6OnQKif3+6lkUOiuhxdvSDjLjfhP
         HoVA==
MIME-Version: 1.0
X-Received: by 10.68.252.165 with SMTP id zt5mr26244309pbc.17.1396800009071;
 Sun, 06 Apr 2014 09:00:09 -0700 (PDT)
Received: by 10.68.12.39 with HTTP; Sun, 6 Apr 2014 09:00:08 -0700 (PDT)
In-Reply-To: <CANx3uAhFQmrhcqCrvsNadMpF5WGKR2UD3FHuYCP6oO41T6BNRw@mail.gmail.com>
References: <CA+B-+fzZPVoYM1YWg2U7EqHwVPAfW5xe-Mg4A6C-S4JAhmAtpw@mail.gmail.com>
	<CAMAsSd+ys9hJQz+NCM86WyMZmUUhqiOCctZaEjVjdRKV1b77yQ@mail.gmail.com>
	<CA+B-+fwf+O5ix4rG8cJWSR_WBQEaVwHLxFOrP1RG7agVfR4vGg@mail.gmail.com>
	<CA+B-+fy_SeaYpZF8VfbOMaTp9Tuas40ipn4kZ15NQsi1DMrRXg@mail.gmail.com>
	<CABPQxss=pKDpwRKi=QAT4kx8zRrVYL+riBLr-YaZtLhRsnQSSw@mail.gmail.com>
	<CAMAsSd+msPg6KwxNgPYBO-0WFf4pzUGV8kytStYhgtHRhXAizQ@mail.gmail.com>
	<CA+B-+fzE4ghtHXOPqSmhDRVX3SKc9AH=py1gZAEUaB+YEbk5mw@mail.gmail.com>
	<CANx3uAhFQmrhcqCrvsNadMpF5WGKR2UD3FHuYCP6oO41T6BNRw@mail.gmail.com>
Date: Sun, 6 Apr 2014 09:00:08 -0700
Message-ID: <CA+B-+fxZypNKGAmwht=+UY+E=48YGbSwYgawsw+=TMPoA+HSGQ@mail.gmail.com>
Subject: Re: Master compilation
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b1638275cf2a904f661d8a6
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b1638275cf2a904f661d8a6
Content-Type: text/plain; charset=ISO-8859-1

Hi Koert,

How do I specify that in sbt ?

Is this the correct way ?
  javacOptions ++= Seq("-target", "1.6", "-source","1.6")

Breeze project for examples compiles fine with jdk7, fails with jdk6 and
the function it fails on:

error] /home/debasish/github/breeze/
src/main/scala/breeze/util/package.scala:200: value valueOf is not a member
of object java.util.BitSet
[error]       java.util.BitSet.valueOf(bs.toBitMask)

is not available in jdk6...

http://docs.oracle.com/javase/6/docs/api/java/util/BitSet.html

I have no clue how with target 1.6 solves the issue...are you saying jdk7
will put a function that's closest to java.util.BitSet.valueOf ?

Thanks.
Deb



On Sun, Apr 6, 2014 at 8:41 AM, Koert Kuipers <koert@tresata.com> wrote:

> classes compiled with java7 run fine on java6 if you specified "-target
> 1.6". however if thats the case generally you should also be able to also
> then compile it with java 6 just fine.
>
> something compiled with java7 with "-target 1.7" will not run on java 6
>
>
>
> On Sat, Apr 5, 2014 at 9:10 PM, Debasish Das <debasish.das83@gmail.com
> >wrote:
>
> > With jdk7 I could compile it fine:
> >
> > java version "1.7.0_51"
> > Java(TM) SE Runtime Environment (build 1.7.0_51-b13)
> > Java HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode)
> >
> > What happens if I say take the jar and try to deploy it on ancient
> centos6
> > default on cluster ?
> >
> > java -version
> > java version "1.6.0_31"
> > Java(TM) SE Runtime Environment (build 1.6.0_31-b04)
> > Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode)
> >
> > Breeze compilation also fails with jdk6, runs fine with jdk7 and breeze
> jar
> > is already included in spark mllib with Xiangrui's Sparse vector
> > checkin....
> >
> > Does that mean that classes compiled and generated using jdk7 will run
> fine
> > on jre6 ?
> >
> > I am confused
> >
> >
> > On Sat, Apr 5, 2014 at 3:09 PM, Sean Owen <sowen@cloudera.com> wrote:
> >
> > > Will do. I'm just finishing a recompile to check for anything else like
> > > this.
> > >
> > > The reason is because the tests run with Java 7 (like lots of us do
> > > including me) so it used the Java 7 classpath and found the class.
> > > It's possible to use Java 7 with the Java 6 -bootclasspath. Or just
> > > use Java 6.
> > > --
> > > Sean Owen | Director, Data Science | London
> > >
> > >
> > > On Sat, Apr 5, 2014 at 11:06 PM, Patrick Wendell <pwendell@gmail.com>
> > > wrote:
> > > > If you want to submit a hot fix for this issue specifically please
> do.
> > > I'm
> > > > not sure why it didn't fail our build...
> > > >
> > > >
> > > > On Sat, Apr 5, 2014 at 2:30 PM, Debasish Das <
> debasish.das83@gmail.com
> > > >wrote:
> > > >
> > > >> I verified this is happening for both CDH4.5 and 1.0.4...My deploy
> > > >> environment is Java 6...so Java 7 compilation is not going to
> help...
> > > >>
> > > >> Is this the PR which caused it ?
> > > >>
> > > >> Andre Schumacher
> > > >>
> > > >>     fbebaed    Spark parquet improvements A few improvements to the
> > > Parquet
> > > >> support for SQL queries: - Instead of files a ParquetRelation is now
> > > backed
> > > >> by a directory, which simplifies importing data from other sources -
> > > >> InsertIntoParquetTable operation now supports switching between
> > > overwriting
> > > >> or appending (at least in HiveQL) - tests now use the new API -
> > Parquet
> > > >> logging can be set to WARNING level (Default) - Default compression
> > for
> > > >> Parquet files (GZIP, as in parquet-mr) Author: Andre Schumacher &...
> > >  2
> > > >> days ago    SPARK-1383
> > > >>
> > > >> I will go to a stable checkin before this
> > > >>
> > > >>
> > > >>
> > > >>
> > > >> On Sat, Apr 5, 2014 at 2:22 PM, Debasish Das <
> > debasish.das83@gmail.com
> > > >> >wrote:
> > > >>
> > > >> > I can compile with Java 7...let me try that...
> > > >> >
> > > >> >
> > > >> > On Sat, Apr 5, 2014 at 2:19 PM, Sean Owen <sowen@cloudera.com>
> > wrote:
> > > >> >
> > > >> >> That method was added in Java 7. The project is on Java 6, so I
> > think
> > > >> >> this was just an inadvertent error in a recent PR (it was the
> > 'Spark
> > > >> >> parquet improvements' one).
> > > >> >>
> > > >> >> I'll open a hot-fix PR after looking for other stuff like this
> that
> > > >> >> might have snuck in.
> > > >> >> --
> > > >> >> Sean Owen | Director, Data Science | London
> > > >> >>
> > > >> >>
> > > >> >> On Sat, Apr 5, 2014 at 10:04 PM, Debasish Das <
> > > debasish.das83@gmail.com
> > > >> >
> > > >> >> wrote:
> > > >> >> > I am synced with apache/spark master but getting error in
> > spark/sql
> > > >> >> > compilation...
> > > >> >> >
> > > >> >> > Is the master broken ?
> > > >> >> >
> > > >> >> > [info] Compiling 34 Scala sources to
> > > >> >> >
> /home/debasish/spark_deploy/sql/core/target/scala-2.10/classes...
> > > >> >> > [error]
> > > >> >> >
> > > >> >>
> > > >>
> > >
> >
> /home/debasish/spark_deploy/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.scala:106:
> > > >> >> > value getGlobal is not a member of object
> > java.util.logging.Logger
> > > >> >> > [error]       logger.setParent(Logger.getGlobal)
> > > >> >> > [error]                               ^
> > > >> >> > [error] one error found
> > > >> >> > [error] (sql/compile:compile) Compilation failed
> > > >> >> > [error] Total time: 171 s, completed Apr 5, 2014 4:58:41 PM
> > > >> >> >
> > > >> >> > Thanks.
> > > >> >> > Deb
> > > >> >>
> > > >> >
> > > >> >
> > > >>
> > >
> >
>

--047d7b1638275cf2a904f661d8a6--

From dev-return-7226-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Apr  6 16:03:54 2014
Return-Path: <dev-return-7226-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 81E6711147
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  6 Apr 2014 16:03:54 +0000 (UTC)
Received: (qmail 8044 invoked by uid 500); 6 Apr 2014 16:03:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7992 invoked by uid 500); 6 Apr 2014 16:03:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7984 invoked by uid 99); 6 Apr 2014 16:03:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 16:03:53 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [74.125.82.51] (HELO mail-wg0-f51.google.com) (74.125.82.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 16:03:48 +0000
Received: by mail-wg0-f51.google.com with SMTP id k14so5770526wgh.10
        for <dev@spark.apache.org>; Sun, 06 Apr 2014 09:03:26 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=sMRpfaUgGmUUKkhc8Nheh+n0qxSuuvkJ/Z/KJmZ249c=;
        b=PVpBOBvoWHDq89QqBi96nq4XnMTgmtbmONyMYwuUE8n4wOqaBrEOTveU56WzewN+p7
         rh2/Fgx5gFgCGmilCpZ7381h3znE2eUkiDqmZ8s06QclsVyZUAGocf4eVSsTHoGJQhg2
         o/BTDP2uzht52PR38C7NSUnwjM9vB4ci+VbmDNvntJIdb4D7oTHhvGSsIpYsqk/TVHo3
         I8eXhvzTv2YRXCxTkGGCXs4EvokK8Kg2jwGVuElRuVyUzIlurewGItkwnzi1r0TW7ko5
         3QwZgyecRJC2xXbFR8jYl63210RGmMDAVuEKF1J6PMA+Q3nuLxbpdPEk6KSO70qUG59l
         V36Q==
X-Gm-Message-State: ALoCoQmGcbX7n1XfrNR1aQhe/Cr3t7OC8gW2YIJoDAzkR9cHwOznsNSLAJjdmHAHEMLrNNh8ilCh
MIME-Version: 1.0
X-Received: by 10.194.57.239 with SMTP id l15mr35981380wjq.40.1396800206615;
 Sun, 06 Apr 2014 09:03:26 -0700 (PDT)
Received: by 10.217.66.129 with HTTP; Sun, 6 Apr 2014 09:03:26 -0700 (PDT)
X-Originating-IP: [209.150.41.132]
In-Reply-To: <CA+B-+fxZypNKGAmwht=+UY+E=48YGbSwYgawsw+=TMPoA+HSGQ@mail.gmail.com>
References: <CA+B-+fzZPVoYM1YWg2U7EqHwVPAfW5xe-Mg4A6C-S4JAhmAtpw@mail.gmail.com>
	<CAMAsSd+ys9hJQz+NCM86WyMZmUUhqiOCctZaEjVjdRKV1b77yQ@mail.gmail.com>
	<CA+B-+fwf+O5ix4rG8cJWSR_WBQEaVwHLxFOrP1RG7agVfR4vGg@mail.gmail.com>
	<CA+B-+fy_SeaYpZF8VfbOMaTp9Tuas40ipn4kZ15NQsi1DMrRXg@mail.gmail.com>
	<CABPQxss=pKDpwRKi=QAT4kx8zRrVYL+riBLr-YaZtLhRsnQSSw@mail.gmail.com>
	<CAMAsSd+msPg6KwxNgPYBO-0WFf4pzUGV8kytStYhgtHRhXAizQ@mail.gmail.com>
	<CA+B-+fzE4ghtHXOPqSmhDRVX3SKc9AH=py1gZAEUaB+YEbk5mw@mail.gmail.com>
	<CANx3uAhFQmrhcqCrvsNadMpF5WGKR2UD3FHuYCP6oO41T6BNRw@mail.gmail.com>
	<CA+B-+fxZypNKGAmwht=+UY+E=48YGbSwYgawsw+=TMPoA+HSGQ@mail.gmail.com>
Date: Sun, 6 Apr 2014 12:03:26 -0400
Message-ID: <CANx3uAjo2+-6C0XVwbh_Rbi6tA073KMiqmqtzj6iQaW-rte2Kw@mail.gmail.com>
Subject: Re: Master compilation
From: Koert Kuipers <koert@tresata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b86e9ca234d4304f661e41f
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b86e9ca234d4304f661e41f
Content-Type: text/plain; charset=ISO-8859-1

thats confusing. it seems to me the breeze dependency has been compiled
with java 6, since the mllib tests passed fine for me with java 6


On Sun, Apr 6, 2014 at 12:00 PM, Debasish Das <debasish.das83@gmail.com>wrote:

> Hi Koert,
>
> How do I specify that in sbt ?
>
> Is this the correct way ?
>   javacOptions ++= Seq("-target", "1.6", "-source","1.6")
>
> Breeze project for examples compiles fine with jdk7, fails with jdk6 and
> the function it fails on:
>
> error] /home/debasish/github/breeze/
> src/main/scala/breeze/util/package.scala:200: value valueOf is not a member
> of object java.util.BitSet
> [error]       java.util.BitSet.valueOf(bs.toBitMask)
>
> is not available in jdk6...
>
> http://docs.oracle.com/javase/6/docs/api/java/util/BitSet.html
>
> I have no clue how with target 1.6 solves the issue...are you saying jdk7
> will put a function that's closest to java.util.BitSet.valueOf ?
>
> Thanks.
> Deb
>
>
>
> On Sun, Apr 6, 2014 at 8:41 AM, Koert Kuipers <koert@tresata.com> wrote:
>
> > classes compiled with java7 run fine on java6 if you specified "-target
> > 1.6". however if thats the case generally you should also be able to also
> > then compile it with java 6 just fine.
> >
> > something compiled with java7 with "-target 1.7" will not run on java 6
> >
> >
> >
> > On Sat, Apr 5, 2014 at 9:10 PM, Debasish Das <debasish.das83@gmail.com
> > >wrote:
> >
> > > With jdk7 I could compile it fine:
> > >
> > > java version "1.7.0_51"
> > > Java(TM) SE Runtime Environment (build 1.7.0_51-b13)
> > > Java HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode)
> > >
> > > What happens if I say take the jar and try to deploy it on ancient
> > centos6
> > > default on cluster ?
> > >
> > > java -version
> > > java version "1.6.0_31"
> > > Java(TM) SE Runtime Environment (build 1.6.0_31-b04)
> > > Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode)
> > >
> > > Breeze compilation also fails with jdk6, runs fine with jdk7 and breeze
> > jar
> > > is already included in spark mllib with Xiangrui's Sparse vector
> > > checkin....
> > >
> > > Does that mean that classes compiled and generated using jdk7 will run
> > fine
> > > on jre6 ?
> > >
> > > I am confused
> > >
> > >
> > > On Sat, Apr 5, 2014 at 3:09 PM, Sean Owen <sowen@cloudera.com> wrote:
> > >
> > > > Will do. I'm just finishing a recompile to check for anything else
> like
> > > > this.
> > > >
> > > > The reason is because the tests run with Java 7 (like lots of us do
> > > > including me) so it used the Java 7 classpath and found the class.
> > > > It's possible to use Java 7 with the Java 6 -bootclasspath. Or just
> > > > use Java 6.
> > > > --
> > > > Sean Owen | Director, Data Science | London
> > > >
> > > >
> > > > On Sat, Apr 5, 2014 at 11:06 PM, Patrick Wendell <pwendell@gmail.com
> >
> > > > wrote:
> > > > > If you want to submit a hot fix for this issue specifically please
> > do.
> > > > I'm
> > > > > not sure why it didn't fail our build...
> > > > >
> > > > >
> > > > > On Sat, Apr 5, 2014 at 2:30 PM, Debasish Das <
> > debasish.das83@gmail.com
> > > > >wrote:
> > > > >
> > > > >> I verified this is happening for both CDH4.5 and 1.0.4...My deploy
> > > > >> environment is Java 6...so Java 7 compilation is not going to
> > help...
> > > > >>
> > > > >> Is this the PR which caused it ?
> > > > >>
> > > > >> Andre Schumacher
> > > > >>
> > > > >>     fbebaed    Spark parquet improvements A few improvements to
> the
> > > > Parquet
> > > > >> support for SQL queries: - Instead of files a ParquetRelation is
> now
> > > > backed
> > > > >> by a directory, which simplifies importing data from other
> sources -
> > > > >> InsertIntoParquetTable operation now supports switching between
> > > > overwriting
> > > > >> or appending (at least in HiveQL) - tests now use the new API -
> > > Parquet
> > > > >> logging can be set to WARNING level (Default) - Default
> compression
> > > for
> > > > >> Parquet files (GZIP, as in parquet-mr) Author: Andre Schumacher
> &...
> > > >  2
> > > > >> days ago    SPARK-1383
> > > > >>
> > > > >> I will go to a stable checkin before this
> > > > >>
> > > > >>
> > > > >>
> > > > >>
> > > > >> On Sat, Apr 5, 2014 at 2:22 PM, Debasish Das <
> > > debasish.das83@gmail.com
> > > > >> >wrote:
> > > > >>
> > > > >> > I can compile with Java 7...let me try that...
> > > > >> >
> > > > >> >
> > > > >> > On Sat, Apr 5, 2014 at 2:19 PM, Sean Owen <sowen@cloudera.com>
> > > wrote:
> > > > >> >
> > > > >> >> That method was added in Java 7. The project is on Java 6, so I
> > > think
> > > > >> >> this was just an inadvertent error in a recent PR (it was the
> > > 'Spark
> > > > >> >> parquet improvements' one).
> > > > >> >>
> > > > >> >> I'll open a hot-fix PR after looking for other stuff like this
> > that
> > > > >> >> might have snuck in.
> > > > >> >> --
> > > > >> >> Sean Owen | Director, Data Science | London
> > > > >> >>
> > > > >> >>
> > > > >> >> On Sat, Apr 5, 2014 at 10:04 PM, Debasish Das <
> > > > debasish.das83@gmail.com
> > > > >> >
> > > > >> >> wrote:
> > > > >> >> > I am synced with apache/spark master but getting error in
> > > spark/sql
> > > > >> >> > compilation...
> > > > >> >> >
> > > > >> >> > Is the master broken ?
> > > > >> >> >
> > > > >> >> > [info] Compiling 34 Scala sources to
> > > > >> >> >
> > /home/debasish/spark_deploy/sql/core/target/scala-2.10/classes...
> > > > >> >> > [error]
> > > > >> >> >
> > > > >> >>
> > > > >>
> > > >
> > >
> >
> /home/debasish/spark_deploy/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.scala:106:
> > > > >> >> > value getGlobal is not a member of object
> > > java.util.logging.Logger
> > > > >> >> > [error]       logger.setParent(Logger.getGlobal)
> > > > >> >> > [error]                               ^
> > > > >> >> > [error] one error found
> > > > >> >> > [error] (sql/compile:compile) Compilation failed
> > > > >> >> > [error] Total time: 171 s, completed Apr 5, 2014 4:58:41 PM
> > > > >> >> >
> > > > >> >> > Thanks.
> > > > >> >> > Deb
> > > > >> >>
> > > > >> >
> > > > >> >
> > > > >>
> > > >
> > >
> >
>

--047d7b86e9ca234d4304f661e41f--

From dev-return-7227-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Apr  6 16:07:53 2014
Return-Path: <dev-return-7227-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 43B6F1114F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  6 Apr 2014 16:07:53 +0000 (UTC)
Received: (qmail 11187 invoked by uid 500); 6 Apr 2014 16:07:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11096 invoked by uid 500); 6 Apr 2014 16:07:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11087 invoked by uid 99); 6 Apr 2014 16:07:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 16:07:51 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.216.178 as permitted sender)
Received: from [209.85.216.178] (HELO mail-qc0-f178.google.com) (209.85.216.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 16:07:47 +0000
Received: by mail-qc0-f178.google.com with SMTP id i8so5375981qcq.23
        for <dev@spark.apache.org>; Sun, 06 Apr 2014 09:07:25 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=eB12HuEj6MKV/GnS75JuElaIqVXcOy9o4fJCqMo/mOg=;
        b=bUbZQ7a8KRbsxFNOMHJdENWErCqZS3+n0Eyw9UoD+q+pla1Akfo2K/Z0PrAlwrGKQu
         m2Hw8xs4AnxZxlvxU7rzQ7QQeiZppvymLy48+f+mWNWn65dZ0gGE09HG5wo3sMuu6BBN
         72WvbJTINY7YRnlMK8ibqDBw7G3wyjCdTOALJ+PE210SmOFaJyyGCvRwyv7OU4JwXZ+l
         wTDelFiX5a1gQ2KdsCeksaRJl/HpCznCfY227I1e85V1baYCHz4qyL3IAuMqlmFyjpPm
         h/ccBCaDrhQdBR7z3z90la1jWMIw/sBR8zSStvCMdPRdY9sVHLMdX/KWFYmDZzIbgceC
         Ht0g==
X-Gm-Message-State: ALoCoQkM/OUY0uergLTbvcrjxpA00FAYEyMpWWeWOrCaeFqpnK/iA6Kum3LiJuvwRa4OvjtqrE7Y
X-Received: by 10.140.50.231 with SMTP id s94mr26059437qga.33.1396800445771;
 Sun, 06 Apr 2014 09:07:25 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.107.194 with HTTP; Sun, 6 Apr 2014 09:07:05 -0700 (PDT)
In-Reply-To: <CA+B-+fxZypNKGAmwht=+UY+E=48YGbSwYgawsw+=TMPoA+HSGQ@mail.gmail.com>
References: <CA+B-+fzZPVoYM1YWg2U7EqHwVPAfW5xe-Mg4A6C-S4JAhmAtpw@mail.gmail.com>
 <CAMAsSd+ys9hJQz+NCM86WyMZmUUhqiOCctZaEjVjdRKV1b77yQ@mail.gmail.com>
 <CA+B-+fwf+O5ix4rG8cJWSR_WBQEaVwHLxFOrP1RG7agVfR4vGg@mail.gmail.com>
 <CA+B-+fy_SeaYpZF8VfbOMaTp9Tuas40ipn4kZ15NQsi1DMrRXg@mail.gmail.com>
 <CABPQxss=pKDpwRKi=QAT4kx8zRrVYL+riBLr-YaZtLhRsnQSSw@mail.gmail.com>
 <CAMAsSd+msPg6KwxNgPYBO-0WFf4pzUGV8kytStYhgtHRhXAizQ@mail.gmail.com>
 <CA+B-+fzE4ghtHXOPqSmhDRVX3SKc9AH=py1gZAEUaB+YEbk5mw@mail.gmail.com>
 <CANx3uAhFQmrhcqCrvsNadMpF5WGKR2UD3FHuYCP6oO41T6BNRw@mail.gmail.com> <CA+B-+fxZypNKGAmwht=+UY+E=48YGbSwYgawsw+=TMPoA+HSGQ@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Sun, 6 Apr 2014 17:07:05 +0100
Message-ID: <CAMAsSdJ-y22x9LHGDMVhc6TobSueHCJfX=61PQKVoCC5HLgaqw@mail.gmail.com>
Subject: Re: Master compilation
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

That's a Breeze question, no? you should not need to compile Breeze
yourself to compile Spark -- why do that?

That method indeed only exists in Java 7. But Breeze seems to target
Java 6 as expected:

https://github.com/scalanlp/breeze/blob/master/build.sbt#L59

I see this particular line of code was added after the last release:

https://github.com/scalanlp/breeze/commit/ff46ddfa66f98b8c8b0ef5b65a6e7a9f86b5a5c4

So it's possible it's an issue lurking in Breeze of just the same form
we just saw. It's worth opening an issue since, indeed, I would expect
exactly the compile error you see with Java 6.

But it should not stop you from building Spark.


On Sun, Apr 6, 2014 at 5:00 PM, Debasish Das <debasish.das83@gmail.com> wrote:
> Hi Koert,
>
> How do I specify that in sbt ?
>
> Is this the correct way ?
>   javacOptions ++= Seq("-target", "1.6", "-source","1.6")
>
> Breeze project for examples compiles fine with jdk7, fails with jdk6 and
> the function it fails on:
>
> error] /home/debasish/github/breeze/
> src/main/scala/breeze/util/package.scala:200: value valueOf is not a member
> of object java.util.BitSet
> [error]       java.util.BitSet.valueOf(bs.toBitMask)
>
> is not available in jdk6...
>
> http://docs.oracle.com/javase/6/docs/api/java/util/BitSet.html
>
> I have no clue how with target 1.6 solves the issue...are you saying jdk7
> will put a function that's closest to java.util.BitSet.valueOf ?
>
> Thanks.
> Deb
>

From dev-return-7228-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Apr  6 16:13:23 2014
Return-Path: <dev-return-7228-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9B3DE1115A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  6 Apr 2014 16:13:23 +0000 (UTC)
Received: (qmail 13570 invoked by uid 500); 6 Apr 2014 16:13:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13478 invoked by uid 500); 6 Apr 2014 16:13:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13453 invoked by uid 99); 6 Apr 2014 16:13:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 16:13:21 +0000
X-ASF-Spam-Status: No, hits=2.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.179 as permitted sender)
Received: from [209.85.192.179] (HELO mail-pd0-f179.google.com) (209.85.192.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 16:13:17 +0000
Received: by mail-pd0-f179.google.com with SMTP id w10so5433579pde.24
        for <dev@spark.apache.org>; Sun, 06 Apr 2014 09:12:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=pAMtkBb7A1eNmOK0b4bQOTns0J7aqeKLYsZib3Pfh90=;
        b=t4+aBIn7vgFeW4vInSC49g7ZEOj2yUtWvSAos+kA/954X/0rUTP96s/tyo43ydZlCv
         dYaqxjwqzDfwtkkTrZdNfwy7oQMRgBxB3zce7DleelXZphSM6zzohC2Vv4uMNTplKkss
         fW2dvc516ik7JdRr35UpUknnvpQXff0FM14kwbqUDUpGta/PXfmgBbNog/d1ka5haKCW
         alZoD+sG+delXTB/GNByS7ilMEzRaJLnPvKMqpYKaEDq1652m4h+FONcfZzdb0N5Nb40
         V2JMMBksI9n9r4x4RLnHux+QgdKotRkV2sPPPtRn5EkyAuFOSpAvpvW2G8trnDILdfEk
         Ivzw==
MIME-Version: 1.0
X-Received: by 10.66.136.103 with SMTP id pz7mr25549pab.140.1396800774998;
 Sun, 06 Apr 2014 09:12:54 -0700 (PDT)
Received: by 10.68.12.39 with HTTP; Sun, 6 Apr 2014 09:12:54 -0700 (PDT)
In-Reply-To: <CAMAsSdJ-y22x9LHGDMVhc6TobSueHCJfX=61PQKVoCC5HLgaqw@mail.gmail.com>
References: <CA+B-+fzZPVoYM1YWg2U7EqHwVPAfW5xe-Mg4A6C-S4JAhmAtpw@mail.gmail.com>
	<CAMAsSd+ys9hJQz+NCM86WyMZmUUhqiOCctZaEjVjdRKV1b77yQ@mail.gmail.com>
	<CA+B-+fwf+O5ix4rG8cJWSR_WBQEaVwHLxFOrP1RG7agVfR4vGg@mail.gmail.com>
	<CA+B-+fy_SeaYpZF8VfbOMaTp9Tuas40ipn4kZ15NQsi1DMrRXg@mail.gmail.com>
	<CABPQxss=pKDpwRKi=QAT4kx8zRrVYL+riBLr-YaZtLhRsnQSSw@mail.gmail.com>
	<CAMAsSd+msPg6KwxNgPYBO-0WFf4pzUGV8kytStYhgtHRhXAizQ@mail.gmail.com>
	<CA+B-+fzE4ghtHXOPqSmhDRVX3SKc9AH=py1gZAEUaB+YEbk5mw@mail.gmail.com>
	<CANx3uAhFQmrhcqCrvsNadMpF5WGKR2UD3FHuYCP6oO41T6BNRw@mail.gmail.com>
	<CA+B-+fxZypNKGAmwht=+UY+E=48YGbSwYgawsw+=TMPoA+HSGQ@mail.gmail.com>
	<CAMAsSdJ-y22x9LHGDMVhc6TobSueHCJfX=61PQKVoCC5HLgaqw@mail.gmail.com>
Date: Sun, 6 Apr 2014 09:12:54 -0700
Message-ID: <CA+B-+fx7_ihVMQ9Vc9oD+Gcy_15Wv0jpY+9WEmju5=RDs_QjTA@mail.gmail.com>
Subject: Re: Master compilation
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11332b6c04103804f66206b9
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11332b6c04103804f66206b9
Content-Type: text/plain; charset=ISO-8859-1

Yeah spark builds are fine...

For solvers we are planning to use breeze optimization since it has most of
the core functions we will need and we can enhance it further (QP solver
for example)

Right now sparse kmeans in spark mllib uses breeze and that might not even
need this line of code....But still I thought Xiangrui should be aware of
this issue...



On Sun, Apr 6, 2014 at 9:07 AM, Sean Owen <sowen@cloudera.com> wrote:

> That's a Breeze question, no? you should not need to compile Breeze
> yourself to compile Spark -- why do that?
>
> That method indeed only exists in Java 7. But Breeze seems to target
> Java 6 as expected:
>
> https://github.com/scalanlp/breeze/blob/master/build.sbt#L59
>
> I see this particular line of code was added after the last release:
>
>
> https://github.com/scalanlp/breeze/commit/ff46ddfa66f98b8c8b0ef5b65a6e7a9f86b5a5c4
>
> So it's possible it's an issue lurking in Breeze of just the same form
> we just saw. It's worth opening an issue since, indeed, I would expect
> exactly the compile error you see with Java 6.
>
> But it should not stop you from building Spark.
>
>
> On Sun, Apr 6, 2014 at 5:00 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
> > Hi Koert,
> >
> > How do I specify that in sbt ?
> >
> > Is this the correct way ?
> >   javacOptions ++= Seq("-target", "1.6", "-source","1.6")
> >
> > Breeze project for examples compiles fine with jdk7, fails with jdk6 and
> > the function it fails on:
> >
> > error] /home/debasish/github/breeze/
> > src/main/scala/breeze/util/package.scala:200: value valueOf is not a
> member
> > of object java.util.BitSet
> > [error]       java.util.BitSet.valueOf(bs.toBitMask)
> >
> > is not available in jdk6...
> >
> > http://docs.oracle.com/javase/6/docs/api/java/util/BitSet.html
> >
> > I have no clue how with target 1.6 solves the issue...are you saying jdk7
> > will put a function that's closest to java.util.BitSet.valueOf ?
> >
> > Thanks.
> > Deb
> >
>

--001a11332b6c04103804f66206b9--

From dev-return-7229-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Apr  6 16:14:25 2014
Return-Path: <dev-return-7229-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 43A9B1115C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  6 Apr 2014 16:14:25 +0000 (UTC)
Received: (qmail 14452 invoked by uid 500); 6 Apr 2014 16:14:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14405 invoked by uid 500); 6 Apr 2014 16:14:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14397 invoked by uid 99); 6 Apr 2014 16:14:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 16:14:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.212.174] (HELO mail-wi0-f174.google.com) (209.85.212.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 16:14:18 +0000
Received: by mail-wi0-f174.google.com with SMTP id d1so3812316wiv.7
        for <dev@spark.apache.org>; Sun, 06 Apr 2014 09:13:57 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=yOq1ZD5BEpkGggfKaVQN9O9zp/lZE6ttZCqBbKTGNJY=;
        b=kEJuS3m3XI1yKUX8IsSveP8xfv4sDuIhoZqiL5nHoaFOt/7Ek1wXxDTH9TK1KfQWZ/
         NEdgD0cR3Uxfm3Pmt+nF9STTxA5AojrW63rTbYBMG0p8X8zCkLfW0ggzvOYy5JfvilNc
         UW8lzMeZ/JRiwW410Pqze3xGr4phw1VeZiKY00JKpCZBlPN6BEHNLNvmzK35sSsJxuRJ
         AGi6i1NTDjabncQEKrLJWtrMJW8H1MBuG46mhZnY7YC0uakIsW2gPjMr2UMk0CpCjPFx
         4GIlGRXYWzT6blqrmMaNuNbrWmy5Sd5j5Q3jz1ZH1uW/ZxUQvViGMhulXMRUS+gdmRzt
         i7ng==
X-Gm-Message-State: ALoCoQnMSBjGeD7DaXbrQ4OCmnPtnaTcTSFtOLpzmXyCvUP1jFZv6l6bor/Qu9e5QJgmyDv0D+lr
MIME-Version: 1.0
X-Received: by 10.180.87.233 with SMTP id bb9mr19849800wib.10.1396800837007;
 Sun, 06 Apr 2014 09:13:57 -0700 (PDT)
Received: by 10.217.66.129 with HTTP; Sun, 6 Apr 2014 09:13:56 -0700 (PDT)
X-Originating-IP: [209.150.41.132]
In-Reply-To: <CABPQxss=pKDpwRKi=QAT4kx8zRrVYL+riBLr-YaZtLhRsnQSSw@mail.gmail.com>
References: <CA+B-+fzZPVoYM1YWg2U7EqHwVPAfW5xe-Mg4A6C-S4JAhmAtpw@mail.gmail.com>
	<CAMAsSd+ys9hJQz+NCM86WyMZmUUhqiOCctZaEjVjdRKV1b77yQ@mail.gmail.com>
	<CA+B-+fwf+O5ix4rG8cJWSR_WBQEaVwHLxFOrP1RG7agVfR4vGg@mail.gmail.com>
	<CA+B-+fy_SeaYpZF8VfbOMaTp9Tuas40ipn4kZ15NQsi1DMrRXg@mail.gmail.com>
	<CABPQxss=pKDpwRKi=QAT4kx8zRrVYL+riBLr-YaZtLhRsnQSSw@mail.gmail.com>
Date: Sun, 6 Apr 2014 12:13:56 -0400
Message-ID: <CANx3uAhJyhvMgchC8uHpQx3eixJtNG9-OoX_oUTUZ5wTTUyxbg@mail.gmail.com>
Subject: Re: Master compilation
From: Koert Kuipers <koert@tresata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d044480afb64f0d04f662095b
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d044480afb64f0d04f662095b
Content-Type: text/plain; charset=ISO-8859-1

patrick,
this has happened before, that a commit introduced java 7 code/dependencies
and your build didnt fail, i think it was when reynold upgraded to jetty 9.
must be that your entire build infrastructure runs java 7...


On Sat, Apr 5, 2014 at 6:06 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> If you want to submit a hot fix for this issue specifically please do. I'm
> not sure why it didn't fail our build...
>
>
> On Sat, Apr 5, 2014 at 2:30 PM, Debasish Das <debasish.das83@gmail.com
> >wrote:
>
> > I verified this is happening for both CDH4.5 and 1.0.4...My deploy
> > environment is Java 6...so Java 7 compilation is not going to help...
> >
> > Is this the PR which caused it ?
> >
> > Andre Schumacher
> >
> >     fbebaed    Spark parquet improvements A few improvements to the
> Parquet
> > support for SQL queries: - Instead of files a ParquetRelation is now
> backed
> > by a directory, which simplifies importing data from other sources -
> > InsertIntoParquetTable operation now supports switching between
> overwriting
> > or appending (at least in HiveQL) - tests now use the new API - Parquet
> > logging can be set to WARNING level (Default) - Default compression for
> > Parquet files (GZIP, as in parquet-mr) Author: Andre Schumacher &...    2
> > days ago    SPARK-1383
> >
> > I will go to a stable checkin before this
> >
> >
> >
> >
> > On Sat, Apr 5, 2014 at 2:22 PM, Debasish Das <debasish.das83@gmail.com
> > >wrote:
> >
> > > I can compile with Java 7...let me try that...
> > >
> > >
> > > On Sat, Apr 5, 2014 at 2:19 PM, Sean Owen <sowen@cloudera.com> wrote:
> > >
> > >> That method was added in Java 7. The project is on Java 6, so I think
> > >> this was just an inadvertent error in a recent PR (it was the 'Spark
> > >> parquet improvements' one).
> > >>
> > >> I'll open a hot-fix PR after looking for other stuff like this that
> > >> might have snuck in.
> > >> --
> > >> Sean Owen | Director, Data Science | London
> > >>
> > >>
> > >> On Sat, Apr 5, 2014 at 10:04 PM, Debasish Das <
> debasish.das83@gmail.com
> > >
> > >> wrote:
> > >> > I am synced with apache/spark master but getting error in spark/sql
> > >> > compilation...
> > >> >
> > >> > Is the master broken ?
> > >> >
> > >> > [info] Compiling 34 Scala sources to
> > >> > /home/debasish/spark_deploy/sql/core/target/scala-2.10/classes...
> > >> > [error]
> > >> >
> > >>
> >
> /home/debasish/spark_deploy/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.scala:106:
> > >> > value getGlobal is not a member of object java.util.logging.Logger
> > >> > [error]       logger.setParent(Logger.getGlobal)
> > >> > [error]                               ^
> > >> > [error] one error found
> > >> > [error] (sql/compile:compile) Compilation failed
> > >> > [error] Total time: 171 s, completed Apr 5, 2014 4:58:41 PM
> > >> >
> > >> > Thanks.
> > >> > Deb
> > >>
> > >
> > >
> >
>

--f46d044480afb64f0d04f662095b--

From dev-return-7230-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Apr  6 17:21:42 2014
Return-Path: <dev-return-7230-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AE93F11243
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  6 Apr 2014 17:21:42 +0000 (UTC)
Received: (qmail 64553 invoked by uid 500); 6 Apr 2014 17:21:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64460 invoked by uid 500); 6 Apr 2014 17:21:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64452 invoked by uid 99); 6 Apr 2014 17:21:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 17:21:41 +0000
X-ASF-Spam-Status: No, hits=2.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.172 as permitted sender)
Received: from [209.85.192.172] (HELO mail-pd0-f172.google.com) (209.85.192.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 17:21:36 +0000
Received: by mail-pd0-f172.google.com with SMTP id p10so5514670pdj.17
        for <dev@spark.incubator.apache.org>; Sun, 06 Apr 2014 10:21:13 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=CckZLOxBoY/BCIsWFqMrXavDOfrSyc2dR6/pqdhgQhA=;
        b=KAn/9krCNrAYwjmmNILnasK4BJD/cu7emHnM5lqJPcILJPh/Crju7JTpj8ZdcL5HbW
         6QDF2uWKlOvk0t86HS8T+LAg3M9zTIXa7+gPfMcor6XI98LRQxLo/SnoKzSKjrFCe+/i
         z7K9/1d9Co5KpcKxA6bsNFOHH+um2v7Ev1Fdv8ecQRmk7k6x44St67SIwCphsohsEEAF
         9uD5xdFJmvRw+tpYC2XhaFA2fNKcaKSyyhHdIXAvT8lNVRn7pjFcMxwQj+vTVNmY0etp
         4+PFvfvAZ6nFDQ49P9PwTRkwk5mxj3oumtW1MvbBvkf71tzTeIAOxrjW8w0FKOTPjP2G
         CXIQ==
MIME-Version: 1.0
X-Received: by 10.66.193.202 with SMTP id hq10mr26286963pac.57.1396804873730;
 Sun, 06 Apr 2014 10:21:13 -0700 (PDT)
Received: by 10.68.12.39 with HTTP; Sun, 6 Apr 2014 10:21:13 -0700 (PDT)
In-Reply-To: <CA+B-+fxfghzWOeiAvcdOyHmP0hUenAZ7Xk+uuE3MStV+8r3uNw@mail.gmail.com>
References: <7841A4E0A9C8784C9D8B07DF5E48F0A811656DB6@SHSMSX104.ccr.corp.intel.com>
	<CADWPM3jwcqhmDsNNspBn0YP6Bzvmxe22P442o=0b3+xcyh8G7g@mail.gmail.com>
	<51F0296D-3996-4DAD-9548-AD6444DA435D@gmail.com>
	<CA+B-+fxfghzWOeiAvcdOyHmP0hUenAZ7Xk+uuE3MStV+8r3uNw@mail.gmail.com>
Date: Sun, 6 Apr 2014 10:21:13 -0700
Message-ID: <CA+B-+fxQqzMDwt1FpBUK7zCp3mz10LQmN2p2X-JA4q_PtA3N2g@mail.gmail.com>
Subject: Re: Any suggestion about JIRA 1006 "MLlib ALS gets stack overflow
 with too many iterations"?
From: Debasish Das <debasish.das83@gmail.com>
To: dev <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=047d7bf1603a51bfd104f662fa27
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bf1603a51bfd104f662fa27
Content-Type: text/plain; charset=ISO-8859-1

At the head I see persist option in implicitPrefs but more cases like the
ones mentioned above why don't we use similar technique and take an input
that which iteration should we persist in explicit runs as well ?

for (iter <- 1 to iterations) {
        // perform ALS update
        logInfo("Re-computing I given U (Iteration %d/%d)".format(iter,
iterations))
        products = updateFeatures(users, userOutLinks, productInLinks,
partitioner, rank, lambda,
          alpha, YtY = None)
        logInfo("Re-computing U given I (Iteration %d/%d)".format(iter,
iterations))
        users = updateFeatures(products, productOutLinks, userInLinks,
partitioner, rank, lambda,
          alpha, YtY = None)
      }

Say if I want to persist at every k iterations out of N iterations of ALS
explicit, there shoud be an option to do that...implicit right now uses
persist at each iteration...

Does this option make sense or you guys want this issue to be fixed in a
different way...

I definitely see that for my 25M x 3M run, with 64 gb executor memory,
something is going wrong after 5-th iteration and I wanted to run for 10
iterations...

So my k is 4/5 for this particular problem...

I can ask for the PR after testing the fix on the dataset I have...I will
also try to see if we can make such datasets public for more research...

For the LDA problem mentioned earlier in this email chain, k is 10...NMF
can generate topics similar to LDA as well...Carrot2 project uses it...



On Thu, Mar 27, 2014 at 3:20 PM, Debasish Das <debasish.das83@gmail.com>wrote:

> Hi Matei,
>
> I am hitting similar problems with 10 ALS iterations...I am running with
> 24 gb executor memory on 10 nodes for 20M x 3 M matrix with rank =50
>
> The first iteration of flatMaps run fine which means that the memory
> requirements are good per iteration...
>
> If I do check-pointing on RDD, most likely rest 9 iterations will also run
> fine and I will get the results...
>
> Is there a plan to add checkpoint option to ALS for such large
> factorization jobs ?
>
> Thanks.
> Deb
>
>
>
>
>
> On Tue, Jan 28, 2014 at 11:10 PM, Matei Zaharia <matei.zaharia@gmail.com>wrote:
>
>> That would be great to add. Right now it would be easy to change it to
>> use another Hadoop FileSystem implementation at the very least (I think you
>> can just pass the URL for that), but for Cassandra you'd have to use a
>> different InputFormat or some direct Cassandra access API.
>>
>> Matei
>>
>> On Jan 28, 2014, at 5:02 PM, Evan Chan <ev@ooyala.com> wrote:
>>
>> > By the way, is there any plan to make a pluggable backend for
>> > checkpointing?   We might be interested in writing a, for example,
>> > Cassandra backend.
>> >
>> > On Sat, Jan 25, 2014 at 9:49 PM, Xia, Junluan <junluan.xia@intel.com>
>> wrote:
>> >> Hi all
>> >>
>> >> The description about this Bug submitted by Matei is as following
>> >>
>> >>
>> >> The tipping point seems to be around 50. We should fix this by
>> checkpointing the RDDs every 10-20 iterations to break the lineage chain,
>> but checkpointing currently requires HDFS installed, which not all users
>> will have.
>> >>
>> >> We might also be able to fix DAGScheduler to not be recursive.
>> >>
>> >>
>> >> regards,
>> >> Andrew
>> >>
>> >
>> >
>> >
>> > --
>> > --
>> > Evan Chan
>> > Staff Engineer
>> > ev@ooyala.com  |
>>
>>
>

--047d7bf1603a51bfd104f662fa27--

From dev-return-7231-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Apr  6 17:47:24 2014
Return-Path: <dev-return-7231-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B14BD11285
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  6 Apr 2014 17:47:24 +0000 (UTC)
Received: (qmail 89089 invoked by uid 500); 6 Apr 2014 17:47:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88991 invoked by uid 500); 6 Apr 2014 17:47:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88983 invoked by uid 99); 6 Apr 2014 17:47:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 17:47:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [74.125.82.46] (HELO mail-wg0-f46.google.com) (74.125.82.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 17:47:18 +0000
Received: by mail-wg0-f46.google.com with SMTP id b13so5806411wgh.5
        for <dev@spark.apache.org>; Sun, 06 Apr 2014 10:46:57 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=auF9zKu3P4rTnrQ2/fLTW+ATrWNhRMxsOL1oxiD9XjA=;
        b=HV+bckCozCQJv4rsjQmV0k/IbvLITPDNOgAF06IF7U8eQXIUwZ/FwL8K3rNT+hw8qf
         1OuGBg4aavyScE2M7fC2VzmwmCaPzwMqry9Ia1MKvPgPnmOLXktPZTCuX35Gag0TVQkX
         mJlw5DPd67jUJtAsGRAJDsv13k1h8mrZnP0truzcplThZ0zVuIYlTwuwecl/89tG1aEO
         ORMhKmnHBFBdbXn7wiMmzh6Yr2JwBEW1Y16sJsuoxJBo88HQlDTENOwZ+k9PGyhVSws0
         /ClH1w51OLPCRGZwZshJOHU26F1xk1vT7QVjWQtk64bvn+uQmb3cSo6XRS6Cfdd03n6w
         XY1g==
X-Gm-Message-State: ALoCoQlrT84j9oO2bvHRzrF57Gq8ChRveL+tYw75WrqIfNfHCve4XtT4NPDkjc7kn2NR5KqvVvci
MIME-Version: 1.0
X-Received: by 10.180.211.207 with SMTP id ne15mr20227089wic.31.1396806416512;
 Sun, 06 Apr 2014 10:46:56 -0700 (PDT)
Received: by 10.217.66.129 with HTTP; Sun, 6 Apr 2014 10:46:56 -0700 (PDT)
X-Originating-IP: [209.150.41.132]
In-Reply-To: <CANx3uAhJyhvMgchC8uHpQx3eixJtNG9-OoX_oUTUZ5wTTUyxbg@mail.gmail.com>
References: <CA+B-+fzZPVoYM1YWg2U7EqHwVPAfW5xe-Mg4A6C-S4JAhmAtpw@mail.gmail.com>
	<CAMAsSd+ys9hJQz+NCM86WyMZmUUhqiOCctZaEjVjdRKV1b77yQ@mail.gmail.com>
	<CA+B-+fwf+O5ix4rG8cJWSR_WBQEaVwHLxFOrP1RG7agVfR4vGg@mail.gmail.com>
	<CA+B-+fy_SeaYpZF8VfbOMaTp9Tuas40ipn4kZ15NQsi1DMrRXg@mail.gmail.com>
	<CABPQxss=pKDpwRKi=QAT4kx8zRrVYL+riBLr-YaZtLhRsnQSSw@mail.gmail.com>
	<CANx3uAhJyhvMgchC8uHpQx3eixJtNG9-OoX_oUTUZ5wTTUyxbg@mail.gmail.com>
Date: Sun, 6 Apr 2014 13:46:56 -0400
Message-ID: <CANx3uAieQ-ucCv0NS38zw-ZSh8e1YVntyrYufMcQDe8w32RrMA@mail.gmail.com>
Subject: Re: Master compilation
From: Koert Kuipers <koert@tresata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c38328494e0e04f66356f3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c38328494e0e04f66356f3
Content-Type: text/plain; charset=ISO-8859-1

also, i thought scala 2.10 was binary compatible, but does not seem to be
the case. the spark artifacts for scala 2.10.4 dont work for me, since we
are still on scala 2.10.3, but when i recompiled and published spark with
scala 2.10.3 everything was fine again.

errors i see:
java.lang.ClassNotFoundException: scala.None$

fun stuff!


On Sun, Apr 6, 2014 at 12:13 PM, Koert Kuipers <koert@tresata.com> wrote:

> patrick,
> this has happened before, that a commit introduced java 7
> code/dependencies and your build didnt fail, i think it was when reynold
> upgraded to jetty 9. must be that your entire build infrastructure runs
> java 7...
>
>
> On Sat, Apr 5, 2014 at 6:06 PM, Patrick Wendell <pwendell@gmail.com>wrote:
>
>> If you want to submit a hot fix for this issue specifically please do. I'm
>> not sure why it didn't fail our build...
>>
>>
>> On Sat, Apr 5, 2014 at 2:30 PM, Debasish Das <debasish.das83@gmail.com
>> >wrote:
>>
>> > I verified this is happening for both CDH4.5 and 1.0.4...My deploy
>> > environment is Java 6...so Java 7 compilation is not going to help...
>> >
>> > Is this the PR which caused it ?
>> >
>> > Andre Schumacher
>> >
>> >     fbebaed    Spark parquet improvements A few improvements to the
>> Parquet
>> > support for SQL queries: - Instead of files a ParquetRelation is now
>> backed
>> > by a directory, which simplifies importing data from other sources -
>> > InsertIntoParquetTable operation now supports switching between
>> overwriting
>> > or appending (at least in HiveQL) - tests now use the new API - Parquet
>> > logging can be set to WARNING level (Default) - Default compression for
>> > Parquet files (GZIP, as in parquet-mr) Author: Andre Schumacher &...
>>  2
>> > days ago    SPARK-1383
>> >
>> > I will go to a stable checkin before this
>> >
>> >
>> >
>> >
>> > On Sat, Apr 5, 2014 at 2:22 PM, Debasish Das <debasish.das83@gmail.com
>> > >wrote:
>> >
>> > > I can compile with Java 7...let me try that...
>> > >
>> > >
>> > > On Sat, Apr 5, 2014 at 2:19 PM, Sean Owen <sowen@cloudera.com> wrote:
>> > >
>> > >> That method was added in Java 7. The project is on Java 6, so I think
>> > >> this was just an inadvertent error in a recent PR (it was the 'Spark
>> > >> parquet improvements' one).
>> > >>
>> > >> I'll open a hot-fix PR after looking for other stuff like this that
>> > >> might have snuck in.
>> > >> --
>> > >> Sean Owen | Director, Data Science | London
>> > >>
>> > >>
>> > >> On Sat, Apr 5, 2014 at 10:04 PM, Debasish Das <
>> debasish.das83@gmail.com
>> > >
>> > >> wrote:
>> > >> > I am synced with apache/spark master but getting error in spark/sql
>> > >> > compilation...
>> > >> >
>> > >> > Is the master broken ?
>> > >> >
>> > >> > [info] Compiling 34 Scala sources to
>> > >> > /home/debasish/spark_deploy/sql/core/target/scala-2.10/classes...
>> > >> > [error]
>> > >> >
>> > >>
>> >
>> /home/debasish/spark_deploy/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.scala:106:
>> > >> > value getGlobal is not a member of object java.util.logging.Logger
>> > >> > [error]       logger.setParent(Logger.getGlobal)
>> > >> > [error]                               ^
>> > >> > [error] one error found
>> > >> > [error] (sql/compile:compile) Compilation failed
>> > >> > [error] Total time: 171 s, completed Apr 5, 2014 4:58:41 PM
>> > >> >
>> > >> > Thanks.
>> > >> > Deb
>> > >>
>> > >
>> > >
>> >
>>
>
>

--001a11c38328494e0e04f66356f3--

From dev-return-7232-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Apr  6 19:22:12 2014
Return-Path: <dev-return-7232-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D00C7113E4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  6 Apr 2014 19:22:12 +0000 (UTC)
Received: (qmail 96129 invoked by uid 500); 6 Apr 2014 19:22:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96062 invoked by uid 500); 6 Apr 2014 19:22:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96054 invoked by uid 99); 6 Apr 2014 19:22:10 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 19:22:10 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 19:22:05 +0000
Received: by mail-wi0-f175.google.com with SMTP id cc10so3957231wib.14
        for <dev@spark.apache.org>; Sun, 06 Apr 2014 12:21:44 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=38xDOzsnHFNTJOWIPhxnPUEcsvbXWXTXC0WR9xmGcR4=;
        b=X1fT7dbLW/4Syim2Se7c5drvqPl1oBJmLWCwtJ5ZTaD6cKugje4W6pGgwnuL34Xos6
         YslcA1YyRN0UrwSDzw/T9hgsZa9pfDtL/+IHAGugeSsLBZla13bc3hEc1Hd2cEd/vkuh
         usEc8yg14Fp+mEEbXSNCLgl/kEB0PXqDZMgHmGegm0K9nOiUQjMxcEpSrydt48s+M+L1
         UFG9B4FPrINKOQs9lQfCujOnfYczBOIU37/IgWJzfuuUXgNNrJ+PoQoSpR+PpPPgL3HY
         A1zxvj+zMYS2WHBQ2Em3OGuQnbGD5rLPp66EjuebioL1VESAGNQjezDmvgr3wSwMTZcG
         uyXg==
X-Gm-Message-State: ALoCoQlycu4/ryvWIBFMqwouaq9id75Lic2+X2MI/rnbBFdCEAbpoia9Ip8Huxb8UZuQSJRRB010
MIME-Version: 1.0
X-Received: by 10.180.185.232 with SMTP id ff8mr20512117wic.25.1396812103835;
 Sun, 06 Apr 2014 12:21:43 -0700 (PDT)
Received: by 10.217.66.129 with HTTP; Sun, 6 Apr 2014 12:21:43 -0700 (PDT)
X-Originating-IP: [209.150.41.132]
In-Reply-To: <CANx3uAieQ-ucCv0NS38zw-ZSh8e1YVntyrYufMcQDe8w32RrMA@mail.gmail.com>
References: <CA+B-+fzZPVoYM1YWg2U7EqHwVPAfW5xe-Mg4A6C-S4JAhmAtpw@mail.gmail.com>
	<CAMAsSd+ys9hJQz+NCM86WyMZmUUhqiOCctZaEjVjdRKV1b77yQ@mail.gmail.com>
	<CA+B-+fwf+O5ix4rG8cJWSR_WBQEaVwHLxFOrP1RG7agVfR4vGg@mail.gmail.com>
	<CA+B-+fy_SeaYpZF8VfbOMaTp9Tuas40ipn4kZ15NQsi1DMrRXg@mail.gmail.com>
	<CABPQxss=pKDpwRKi=QAT4kx8zRrVYL+riBLr-YaZtLhRsnQSSw@mail.gmail.com>
	<CANx3uAhJyhvMgchC8uHpQx3eixJtNG9-OoX_oUTUZ5wTTUyxbg@mail.gmail.com>
	<CANx3uAieQ-ucCv0NS38zw-ZSh8e1YVntyrYufMcQDe8w32RrMA@mail.gmail.com>
Date: Sun, 6 Apr 2014 15:21:43 -0400
Message-ID: <CANx3uAhyt8hJtjw1Aea718K+AcNrC-sd40-PiANDz9AizO2cHQ@mail.gmail.com>
Subject: Re: Master compilation
From: Koert Kuipers <koert@tresata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c364ba44768504f664a9a4
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c364ba44768504f664a9a4
Content-Type: text/plain; charset=ISO-8859-1

i suggest we stick to 2.10.3, since otherwise it seems that (surprisingly)
you force everyone to upgrade


On Sun, Apr 6, 2014 at 1:46 PM, Koert Kuipers <koert@tresata.com> wrote:

> also, i thought scala 2.10 was binary compatible, but does not seem to be
> the case. the spark artifacts for scala 2.10.4 dont work for me, since we
> are still on scala 2.10.3, but when i recompiled and published spark with
> scala 2.10.3 everything was fine again.
>
> errors i see:
> java.lang.ClassNotFoundException: scala.None$
>
> fun stuff!
>
>
> On Sun, Apr 6, 2014 at 12:13 PM, Koert Kuipers <koert@tresata.com> wrote:
>
>> patrick,
>> this has happened before, that a commit introduced java 7
>> code/dependencies and your build didnt fail, i think it was when reynold
>> upgraded to jetty 9. must be that your entire build infrastructure runs
>> java 7...
>>
>>
>> On Sat, Apr 5, 2014 at 6:06 PM, Patrick Wendell <pwendell@gmail.com>wrote:
>>
>>> If you want to submit a hot fix for this issue specifically please do.
>>> I'm
>>> not sure why it didn't fail our build...
>>>
>>>
>>> On Sat, Apr 5, 2014 at 2:30 PM, Debasish Das <debasish.das83@gmail.com
>>> >wrote:
>>>
>>> > I verified this is happening for both CDH4.5 and 1.0.4...My deploy
>>> > environment is Java 6...so Java 7 compilation is not going to help...
>>> >
>>> > Is this the PR which caused it ?
>>> >
>>> > Andre Schumacher
>>> >
>>> >     fbebaed    Spark parquet improvements A few improvements to the
>>> Parquet
>>> > support for SQL queries: - Instead of files a ParquetRelation is now
>>> backed
>>> > by a directory, which simplifies importing data from other sources -
>>> > InsertIntoParquetTable operation now supports switching between
>>> overwriting
>>> > or appending (at least in HiveQL) - tests now use the new API - Parquet
>>> > logging can be set to WARNING level (Default) - Default compression for
>>> > Parquet files (GZIP, as in parquet-mr) Author: Andre Schumacher &...
>>>  2
>>> > days ago    SPARK-1383
>>> >
>>> > I will go to a stable checkin before this
>>> >
>>> >
>>> >
>>> >
>>> > On Sat, Apr 5, 2014 at 2:22 PM, Debasish Das <debasish.das83@gmail.com
>>> > >wrote:
>>> >
>>> > > I can compile with Java 7...let me try that...
>>> > >
>>> > >
>>> > > On Sat, Apr 5, 2014 at 2:19 PM, Sean Owen <sowen@cloudera.com>
>>> wrote:
>>> > >
>>> > >> That method was added in Java 7. The project is on Java 6, so I
>>> think
>>> > >> this was just an inadvertent error in a recent PR (it was the 'Spark
>>> > >> parquet improvements' one).
>>> > >>
>>> > >> I'll open a hot-fix PR after looking for other stuff like this that
>>> > >> might have snuck in.
>>> > >> --
>>> > >> Sean Owen | Director, Data Science | London
>>> > >>
>>> > >>
>>> > >> On Sat, Apr 5, 2014 at 10:04 PM, Debasish Das <
>>> debasish.das83@gmail.com
>>> > >
>>> > >> wrote:
>>> > >> > I am synced with apache/spark master but getting error in
>>> spark/sql
>>> > >> > compilation...
>>> > >> >
>>> > >> > Is the master broken ?
>>> > >> >
>>> > >> > [info] Compiling 34 Scala sources to
>>> > >> > /home/debasish/spark_deploy/sql/core/target/scala-2.10/classes...
>>> > >> > [error]
>>> > >> >
>>> > >>
>>> >
>>> /home/debasish/spark_deploy/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetRelation.scala:106:
>>> > >> > value getGlobal is not a member of object java.util.logging.Logger
>>> > >> > [error]       logger.setParent(Logger.getGlobal)
>>> > >> > [error]                               ^
>>> > >> > [error] one error found
>>> > >> > [error] (sql/compile:compile) Compilation failed
>>> > >> > [error] Total time: 171 s, completed Apr 5, 2014 4:58:41 PM
>>> > >> >
>>> > >> > Thanks.
>>> > >> > Deb
>>> > >>
>>> > >
>>> > >
>>> >
>>>
>>
>>
>

--001a11c364ba44768504f664a9a4--

From dev-return-7233-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Apr  6 20:10:24 2014
Return-Path: <dev-return-7233-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4F12A11449
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  6 Apr 2014 20:10:24 +0000 (UTC)
Received: (qmail 19932 invoked by uid 500); 6 Apr 2014 20:10:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19815 invoked by uid 500); 6 Apr 2014 20:10:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19807 invoked by uid 99); 6 Apr 2014 20:10:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 20:10:22 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.216.172 as permitted sender)
Received: from [209.85.216.172] (HELO mail-qc0-f172.google.com) (209.85.216.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 20:10:15 +0000
Received: by mail-qc0-f172.google.com with SMTP id i8so5614653qcq.3
        for <dev@spark.apache.org>; Sun, 06 Apr 2014 13:09:53 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=DL26GaWHr6AxkX9E+EkVPJniWwP0mdJ6azjlVMidhVQ=;
        b=XYNVc867NJo+lMIlRdNrJLeL85XQjv9PvWYXcYHHfki6ISI5SRE3QEH/uaTqDrOmuZ
         D/+fLuN3q57wPHvpV2uv6fQphiP1MJxbdtPsCQRsd4f3Z38vag1iBp6ZEjpcqNioEX0v
         9j4VMshva8qaL4EHlplAW8V6Om9AbmgAdGFQUsOJ9jrD250Mm8zdrTRuEG4BE+dWo/NU
         tIkR8BW/eJ1MbwJMX1TpMIaQsbkvVFSWX0yNIlpUqw6ao5VI4U6erm459gg2YYjIoGfP
         Sw4VTnAMI1Gt+qZNd05QXQDmZVUjFBuupYVURvNRwlSmJfNXe1sZijn8D3cDWxulezJj
         MASg==
X-Gm-Message-State: ALoCoQk/qMDAShHmw/7Br5ag8wmsedAwc2AY7cWr4WQMCPADf4Ok1ehzbsir+9xb4+et6LmBUylj
X-Received: by 10.140.16.37 with SMTP id 34mr5514626qga.37.1396814993481; Sun,
 06 Apr 2014 13:09:53 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.107.194 with HTTP; Sun, 6 Apr 2014 13:09:33 -0700 (PDT)
In-Reply-To: <CANx3uAieQ-ucCv0NS38zw-ZSh8e1YVntyrYufMcQDe8w32RrMA@mail.gmail.com>
References: <CA+B-+fzZPVoYM1YWg2U7EqHwVPAfW5xe-Mg4A6C-S4JAhmAtpw@mail.gmail.com>
 <CAMAsSd+ys9hJQz+NCM86WyMZmUUhqiOCctZaEjVjdRKV1b77yQ@mail.gmail.com>
 <CA+B-+fwf+O5ix4rG8cJWSR_WBQEaVwHLxFOrP1RG7agVfR4vGg@mail.gmail.com>
 <CA+B-+fy_SeaYpZF8VfbOMaTp9Tuas40ipn4kZ15NQsi1DMrRXg@mail.gmail.com>
 <CABPQxss=pKDpwRKi=QAT4kx8zRrVYL+riBLr-YaZtLhRsnQSSw@mail.gmail.com>
 <CANx3uAhJyhvMgchC8uHpQx3eixJtNG9-OoX_oUTUZ5wTTUyxbg@mail.gmail.com> <CANx3uAieQ-ucCv0NS38zw-ZSh8e1YVntyrYufMcQDe8w32RrMA@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Sun, 6 Apr 2014 21:09:33 +0100
Message-ID: <CAMAsSdLwks4tk_0_y1ZKt=GufF_rn-OvSvR5UZvxfhMTw_P0dQ@mail.gmail.com>
Subject: Re: Master compilation
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

scala.None certainly isn't new in 2.10.4; it's ancient :
http://www.scala-lang.org/api/2.10.3/index.html#scala.None$

Surely this is some other problem?

On Sun, Apr 6, 2014 at 6:46 PM, Koert Kuipers <koert@tresata.com> wrote:
> also, i thought scala 2.10 was binary compatible, but does not seem to be
> the case. the spark artifacts for scala 2.10.4 dont work for me, since we
> are still on scala 2.10.3, but when i recompiled and published spark with
> scala 2.10.3 everything was fine again.
>
> errors i see:
> java.lang.ClassNotFoundException: scala.None$
>
> fun stuff!

From dev-return-7234-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Apr  6 22:32:25 2014
Return-Path: <dev-return-7234-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 974191160D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  6 Apr 2014 22:32:25 +0000 (UTC)
Received: (qmail 15384 invoked by uid 500); 6 Apr 2014 22:32:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15347 invoked by uid 500); 6 Apr 2014 22:32:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15334 invoked by uid 99); 6 Apr 2014 22:32:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 22:32:13 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [74.125.82.47] (HELO mail-wg0-f47.google.com) (74.125.82.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Apr 2014 22:32:09 +0000
Received: by mail-wg0-f47.google.com with SMTP id x12so5954790wgg.6
        for <dev@spark.apache.org>; Sun, 06 Apr 2014 15:31:48 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=z7H4ewcof7yyiRzJHtZs8Bka9uOwf2wZgn28sjIa8bA=;
        b=QgoV8HOsL1qv9FDPVb+omHdnrhPbp2IL8kjPzPfLeEf1Q+QG5sYYOVqfEc6zTjo4nY
         xvBRZpjBuONaCY2VPZrGKn0EjQFe9m4I4qU2o2tP9vI8FBfLDs2DsgMgm1IVSk+l5PWj
         yBpiIEQkY+WuV8KqMjs2QRpFzg4RV9HxaojN52gkovMFgmOJ/QzehQ1dZBGoEH3H/CWU
         LwrnyNpbSlxv6StNoal08+734Y2ZqCGRqMgaUmAjo2QTi+Gci7qhF2sy8oQ6bCZS9Uy9
         fOr2HwRGgd9PQAPZ3/1BUd6J54I9/XUh1jwgYc/Q9KgXpKPzfYdcKxyXDkzZ7oEauOyW
         AwhQ==
X-Gm-Message-State: ALoCoQkS1J3j3EwGJ+bgRNkb07jdg34vtSqJcLaIyTqM1AYQbjEwFqm8XqljeljlfTm3OEFOkGGT
MIME-Version: 1.0
X-Received: by 10.180.98.1 with SMTP id ee1mr21752752wib.10.1396823508031;
 Sun, 06 Apr 2014 15:31:48 -0700 (PDT)
Received: by 10.217.66.129 with HTTP; Sun, 6 Apr 2014 15:31:47 -0700 (PDT)
X-Originating-IP: [172.56.35.26]
Received: by 10.217.66.129 with HTTP; Sun, 6 Apr 2014 15:31:47 -0700 (PDT)
In-Reply-To: <CAMAsSdLwks4tk_0_y1ZKt=GufF_rn-OvSvR5UZvxfhMTw_P0dQ@mail.gmail.com>
References: <CA+B-+fzZPVoYM1YWg2U7EqHwVPAfW5xe-Mg4A6C-S4JAhmAtpw@mail.gmail.com>
	<CAMAsSd+ys9hJQz+NCM86WyMZmUUhqiOCctZaEjVjdRKV1b77yQ@mail.gmail.com>
	<CA+B-+fwf+O5ix4rG8cJWSR_WBQEaVwHLxFOrP1RG7agVfR4vGg@mail.gmail.com>
	<CA+B-+fy_SeaYpZF8VfbOMaTp9Tuas40ipn4kZ15NQsi1DMrRXg@mail.gmail.com>
	<CABPQxss=pKDpwRKi=QAT4kx8zRrVYL+riBLr-YaZtLhRsnQSSw@mail.gmail.com>
	<CANx3uAhJyhvMgchC8uHpQx3eixJtNG9-OoX_oUTUZ5wTTUyxbg@mail.gmail.com>
	<CANx3uAieQ-ucCv0NS38zw-ZSh8e1YVntyrYufMcQDe8w32RrMA@mail.gmail.com>
	<CAMAsSdLwks4tk_0_y1ZKt=GufF_rn-OvSvR5UZvxfhMTw_P0dQ@mail.gmail.com>
Date: Sun, 6 Apr 2014 18:31:47 -0400
Message-ID: <CANx3uAiTUi5kyktPZiJy0W4T1h9VcWjvwPJC2H0Sqe8ZvN2mjQ@mail.gmail.com>
Subject: Re: Master compilation
From: Koert Kuipers <koert@tresata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d041824f802aeda04f6675170
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d041824f802aeda04f6675170
Content-Type: text/plain; charset=ISO-8859-1

see here for similar issue

http://mail-archives.apache.org/mod_mbox/spark-user/201401.mbox/%3CCALNFXi2hBSyCkPpnBJBYJnPv3dSLNw8VpL_6caEn3yfXCykO=w@mail.gmail.com%3E
On Apr 6, 2014 4:10 PM, "Sean Owen" <sowen@cloudera.com> wrote:

> scala.None certainly isn't new in 2.10.4; it's ancient :
> http://www.scala-lang.org/api/2.10.3/index.html#scala.None$
>
> Surely this is some other problem?
>
> On Sun, Apr 6, 2014 at 6:46 PM, Koert Kuipers <koert@tresata.com> wrote:
> > also, i thought scala 2.10 was binary compatible, but does not seem to be
> > the case. the spark artifacts for scala 2.10.4 dont work for me, since we
> > are still on scala 2.10.3, but when i recompiled and published spark with
> > scala 2.10.3 everything was fine again.
> >
> > errors i see:
> > java.lang.ClassNotFoundException: scala.None$
> >
> > fun stuff!
>

--f46d041824f802aeda04f6675170--

From dev-return-7235-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 03:00:10 2014
Return-Path: <dev-return-7235-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 19D4A119E1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 03:00:10 +0000 (UTC)
Received: (qmail 77922 invoked by uid 500); 7 Apr 2014 03:00:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77479 invoked by uid 500); 7 Apr 2014 03:00:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77414 invoked by uid 99); 7 Apr 2014 03:00:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 03:00:00 +0000
X-ASF-Spam-Status: No, hits=1.0 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.172 as permitted sender)
Received: from [209.85.192.172] (HELO mail-pd0-f172.google.com) (209.85.192.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 02:59:55 +0000
Received: by mail-pd0-f172.google.com with SMTP id p10so5895479pdj.17
        for <dev@spark.apache.org>; Sun, 06 Apr 2014 19:59:35 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=KECx+Ba5FPDA7tx95qF5fH/XxquOvoatsDp6BJrL1/I=;
        b=y6nInBuyMd/ackLlrvIYAv3/Uk6rb11/dAPHPGb+Hp4mPVElhbTtNDNhuThErc4eFl
         MtA/YRkyWIuRpcGWxT1Zyzj0bYnyVs3GDhozMq65/UV4lCd6dMBw73g7veyVD2yfuukL
         TFtGGSs3atPaA0yNzJ+znPwfqX39aVDFcyzo+B8a0FOk5aMjJK5q7pkMJgolY1rYxjA0
         d07Q71pM4+ct7a6g7+ytJ0ECW8EXaV7w+95KArlR2BmH94BHaKETasQuRsjZXwN2FoyT
         79iHmP5hQClpWKAJ/OPgFVx+dIb4QjpwhknWDO3xl5UmoGbgKxPSmOttGjMJnxz7c9OG
         BqNg==
X-Received: by 10.68.202.230 with SMTP id kl6mr28443770pbc.55.1396839574893;
        Sun, 06 Apr 2014 19:59:34 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id ac5sm33214329pbc.37.2014.04.06.19.59.32
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 06 Apr 2014 19:59:33 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: ephemeral storage level in spark ?
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAJiQeYJe+762P+nD4L6bO-SrW2_7CDsc7KfXNYOCPD4-h0zY_g@mail.gmail.com>
Date: Sun, 6 Apr 2014 19:59:23 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <D6C25F30-73C0-4EFF-A7D8-8D26A7D03966@gmail.com>
References: <CAJiQeYLDM+WvPBX_OXGGYQg4v2Pr5tsRYfJmrcbjzKA1_O_feg@mail.gmail.com> <CAG2iju2wBimE+y=n6QQOy7QGO31oWKYoXzzJ=zCA_Ew4NWLG8Q@mail.gmail.com> <CAJiQeYJe+762P+nD4L6bO-SrW2_7CDsc7KfXNYOCPD4-h0zY_g@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

The off-heap storage level is currently tied to Tachyon, but it might =
support other forms of off-heap storage later. However it=92s not really =
designed to be mixed with the other ones. For this use case you may want =
to rely on memory locality and have some custom code to push the data to =
the accelerator. If you can think of a way to extend the storage level =
concept to handle this that would be general though, do send a proposal.

Matei

On Apr 5, 2014, at 5:14 PM, Mridul Muralidharan <mridul@gmail.com> =
wrote:

> No, I am thinking along lines of writing to an accelerator card or
> dedicated card with its own memory.
>=20
> Regards,
> Mridul
> On Apr 6, 2014 5:19 AM, "Haoyuan Li" <haoyuan.li@gmail.com> wrote:
>=20
>> Hi Mridul,
>>=20
>> Do you mean the scenario that different Spark applications need to =
read the
>> same raw data, which is stored in a remote cluster or machines. And =
the
>> goal is to load the remote raw data only once?
>>=20
>> Haoyuan
>>=20
>>=20
>> On Sat, Apr 5, 2014 at 4:30 PM, Mridul Muralidharan <mridul@gmail.com
>>> wrote:
>>=20
>>> Hi,
>>>=20
>>>  We have a requirement to use a (potential) ephemeral storage, which
>>> is not within the VM, which is strongly tied to a worker node. So
>>> source of truth for a block would still be within spark; but to
>>> actually do computation, we would need to copy data to external =
device
>>> (where it might lie around for a while : so data locality really
>>> really helps if we can avoid a subsequent copy if it is already
>>> present on computations on same block again).
>>>=20
>>> I was wondering if the recently added storage level for tachyon =
would
>>> help in this case (note, tachyon wont help; just the storage level
>>> might).
>>> What sort of guarantees does it provide ? How extensible is it ? Or =
is
>>> it strongly tied to tachyon with only a generic name ?
>>>=20
>>>=20
>>> Thanks,
>>> Mridul
>>>=20
>>=20
>>=20
>>=20
>> --
>> Haoyuan Li
>> Algorithms, Machines, People Lab, EECS, UC Berkeley
>> http://www.cs.berkeley.edu/~haoyuan/
>>=20


From dev-return-7236-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 04:08:29 2014
Return-Path: <dev-return-7236-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0B17111AFE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 04:08:29 +0000 (UTC)
Received: (qmail 20094 invoked by uid 500); 7 Apr 2014 04:08:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19493 invoked by uid 500); 7 Apr 2014 04:08:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19485 invoked by uid 99); 7 Apr 2014 04:08:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 04:08:26 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 74.125.82.176 as permitted sender)
Received: from [74.125.82.176] (HELO mail-we0-f176.google.com) (74.125.82.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 04:08:22 +0000
Received: by mail-we0-f176.google.com with SMTP id x48so6227396wes.35
        for <dev@spark.apache.org>; Sun, 06 Apr 2014 21:08:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=N+DDRWVgVJ1rAT3zJ3QsQ16fEtYHgT0HXQbIwXU4Stk=;
        b=PfgAnCUFN4B527/8WCWuItafNFQrBbLLSF6/rvOn32Q6I0oekoWJYo9GH9rmxGt38q
         kiCArg+ZzQ7ip8IfWpDn1Rr0nFY0Ee/+nv9w0uSKcn1lX8r701wXTpq0HgMkTixVunKy
         5Zn9zjWTGxfdeh+2kZXZykyCMdCivYS7MH8Tx51p7J+8cDiOx6o2ZPEZcbedt19M+3aA
         q/7CFCcViKPsEVAnfHLvblI7bUIiEB707YMqqj9fpqVE4uE/JfPlpYzKDv5wRM4yYXNG
         QU1ky8ejBybZ6aB0Q/54smBtYgcwrxrVSeXvWImXhX81NKFgWGffNwtukvJihfaiDrbD
         tHqQ==
MIME-Version: 1.0
X-Received: by 10.194.82.35 with SMTP id f3mr39998930wjy.36.1396843681237;
 Sun, 06 Apr 2014 21:08:01 -0700 (PDT)
Received: by 10.194.82.103 with HTTP; Sun, 6 Apr 2014 21:08:00 -0700 (PDT)
In-Reply-To: <CA+B-+fzYxLU6OUX6hufpsbAmJLoYbPQupMRK=MR8Yx6jkLOj2g@mail.gmail.com>
References: <CA+B-+fzYxLU6OUX6hufpsbAmJLoYbPQupMRK=MR8Yx6jkLOj2g@mail.gmail.com>
Date: Sun, 6 Apr 2014 21:08:00 -0700
Message-ID: <CAJgQjQ-nYkkMwDXkgiSKqtiE0Lk1kKRYeWHe-O=N=eHBHzgiBg@mail.gmail.com>
Subject: Re: ALS array index out of bound with 50 factors
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Deb,

Are you using the master branch or a particular commit? Do you have
negative or out-of-integer-range user or product ids? There is an
issue with ALS' partitioning
(https://spark-project.atlassian.net/browse/SPARK-1281), but I'm not
sure whether that is the reason. Could you try to see whether you can
reproduce the error on a public data set, e.g., movielens? Thanks!

Best,
Xiangrui

On Sat, Apr 5, 2014 at 10:53 PM, Debasish Das <debasish.das83@gmail.com> wrote:
> Hi,
>
> I deployed apache/spark master today and recently there were many ALS
> related checkins and enhancements..
>
> I am running ALS with explicit feedback and I remember most enhancements
> were related to implicit feedback...
>
> With 25 factors my runs were successful but with 50 factors I am getting
> array index out of bound...
>
> Note that I was hitting gc errors before with an older version of spark but
> it seems like the sparse matrix partitioning scheme has changed now...data
> caching looks much balanced now...earlier one node was becoming
> bottleneck...Although I ran with 64g memory per node...
>
> There are around 3M products, 25M users...
>
> Anyone noticed this bug or something similar ?
>
> 14/04/05 23:03:15 WARN TaskSetManager: Loss was due to
> java.lang.ArrayIndexOutOfBoundsException
> java.lang.ArrayIndexOutOfBoundsException: 81029
>     at
> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateBlock$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(ALS.scala:450)
>     at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
>     at
> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateBlock$1.apply$mcVI$sp(ALS.scala:446)
>     at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
>     at org.apache.spark.mllib.recommendation.ALS.org
> $apache$spark$mllib$recommendation$ALS$$updateBlock(ALS.scala:445)
>     at
> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:416)
>     at
> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:415)
>     at
> org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31)
>     at
> org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31)
>     at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>     at
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:149)
>     at
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:147)
>     at
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>     at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>     at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:147)
>     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
>     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
>     at
> org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
>     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
>     at
> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
>     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
>     at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
>     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
>     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
>     at
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)
>     at
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
>     at org.apache.spark.scheduler.Task.run(Task.scala:52)
>     at
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
>     at
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:43)
>     at
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
>     at java.security.AccessController.doPrivileged(Native Method)
>     at javax.security.auth.Subject.doAs(Subject.java:396)
>     at
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
>     at
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:42)
>     at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
>     at
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>     at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>     at java.lang.Thread.run(Thread.java:662)
>
> Thanks.
> Deb

From dev-return-7238-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 04:14:25 2014
Return-Path: <dev-return-7238-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 406D311B07
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 04:14:25 +0000 (UTC)
Received: (qmail 21313 invoked by uid 500); 7 Apr 2014 04:14:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21165 invoked by uid 500); 7 Apr 2014 04:14:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21132 invoked by uid 99); 7 Apr 2014 04:14:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 04:14:16 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 74.125.82.182 as permitted sender)
Received: from [74.125.82.182] (HELO mail-we0-f182.google.com) (74.125.82.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 04:14:12 +0000
Received: by mail-we0-f182.google.com with SMTP id p61so6234398wes.27
        for <dev@spark.incubator.apache.org>; Sun, 06 Apr 2014 21:13:51 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=gwyBufL/48Z8+FCFYU7C3nUzoljnHaxgh/o4miXK0Tw=;
        b=Li0oA/rxbDQJaSbMBVd1IOs62aGbfP8s7Dq+KFu7xCtEbwrM5XsrakOfEwwJMeLc8F
         8L80bBxdDvgARLwDhFMJ3gZOgR1+hUCgeEnF2Da9AlxyKshn3oHpD5lHlY0VKLyaK2S3
         FepAAShSI+kpKkWyqyjCfW4YNbL+OkIREBG3QUIJ3S0N2rPBQjlXGdtk8uQj0VMLpMIi
         JYK1nUSfrhWK471d+YMiO6zXCHL/L5vb1JAGPnuDzTbYqj+8MWE3K04x3Tgekle9wsiD
         TE9+xQ6ta4+K9KbqM2NzCGBhzEWk+v7yWGEr7ykFzOeWwEI4FUiJdVzRc4bm4U6byV0L
         +37Q==
MIME-Version: 1.0
X-Received: by 10.194.186.140 with SMTP id fk12mr38790851wjc.47.1396844031690;
 Sun, 06 Apr 2014 21:13:51 -0700 (PDT)
Received: by 10.194.82.103 with HTTP; Sun, 6 Apr 2014 21:13:51 -0700 (PDT)
In-Reply-To: <CA+B-+fxQqzMDwt1FpBUK7zCp3mz10LQmN2p2X-JA4q_PtA3N2g@mail.gmail.com>
References: <7841A4E0A9C8784C9D8B07DF5E48F0A811656DB6@SHSMSX104.ccr.corp.intel.com>
	<CADWPM3jwcqhmDsNNspBn0YP6Bzvmxe22P442o=0b3+xcyh8G7g@mail.gmail.com>
	<51F0296D-3996-4DAD-9548-AD6444DA435D@gmail.com>
	<CA+B-+fxfghzWOeiAvcdOyHmP0hUenAZ7Xk+uuE3MStV+8r3uNw@mail.gmail.com>
	<CA+B-+fxQqzMDwt1FpBUK7zCp3mz10LQmN2p2X-JA4q_PtA3N2g@mail.gmail.com>
Date: Sun, 6 Apr 2014 21:13:51 -0700
Message-ID: <CAJgQjQ8QU02qKe22-VSzEjjAX+=NFy=U=Ew4EZ8XZPH2UgDBtA@mail.gmail.com>
Subject: Re: Any suggestion about JIRA 1006 "MLlib ALS gets stack overflow
 with too many iterations"?
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

The persist used in implicit ALS doesn't help StackOverflow problem.
Persist doesn't cut lineage. We need to call count() and then
checkpoint() to cut the lineage. Did you try the workaround mentioned
in https://issues.apache.org/jira/browse/SPARK-958:

"I tune JVM thread stack size to 512k via option -Xss512k and it works."

Best,
Xiangrui

On Sun, Apr 6, 2014 at 10:21 AM, Debasish Das <debasish.das83@gmail.com> wrote:
> At the head I see persist option in implicitPrefs but more cases like the
> ones mentioned above why don't we use similar technique and take an input
> that which iteration should we persist in explicit runs as well ?
>
> for (iter <- 1 to iterations) {
>         // perform ALS update
>         logInfo("Re-computing I given U (Iteration %d/%d)".format(iter,
> iterations))
>         products = updateFeatures(users, userOutLinks, productInLinks,
> partitioner, rank, lambda,
>           alpha, YtY = None)
>         logInfo("Re-computing U given I (Iteration %d/%d)".format(iter,
> iterations))
>         users = updateFeatures(products, productOutLinks, userInLinks,
> partitioner, rank, lambda,
>           alpha, YtY = None)
>       }
>
> Say if I want to persist at every k iterations out of N iterations of ALS
> explicit, there shoud be an option to do that...implicit right now uses
> persist at each iteration...
>
> Does this option make sense or you guys want this issue to be fixed in a
> different way...
>
> I definitely see that for my 25M x 3M run, with 64 gb executor memory,
> something is going wrong after 5-th iteration and I wanted to run for 10
> iterations...
>
> So my k is 4/5 for this particular problem...
>
> I can ask for the PR after testing the fix on the dataset I have...I will
> also try to see if we can make such datasets public for more research...
>
> For the LDA problem mentioned earlier in this email chain, k is 10...NMF
> can generate topics similar to LDA as well...Carrot2 project uses it...
>
>
>
> On Thu, Mar 27, 2014 at 3:20 PM, Debasish Das <debasish.das83@gmail.com>wrote:
>
>> Hi Matei,
>>
>> I am hitting similar problems with 10 ALS iterations...I am running with
>> 24 gb executor memory on 10 nodes for 20M x 3 M matrix with rank =50
>>
>> The first iteration of flatMaps run fine which means that the memory
>> requirements are good per iteration...
>>
>> If I do check-pointing on RDD, most likely rest 9 iterations will also run
>> fine and I will get the results...
>>
>> Is there a plan to add checkpoint option to ALS for such large
>> factorization jobs ?
>>
>> Thanks.
>> Deb
>>
>>
>>
>>
>>
>> On Tue, Jan 28, 2014 at 11:10 PM, Matei Zaharia <matei.zaharia@gmail.com>wrote:
>>
>>> That would be great to add. Right now it would be easy to change it to
>>> use another Hadoop FileSystem implementation at the very least (I think you
>>> can just pass the URL for that), but for Cassandra you'd have to use a
>>> different InputFormat or some direct Cassandra access API.
>>>
>>> Matei
>>>
>>> On Jan 28, 2014, at 5:02 PM, Evan Chan <ev@ooyala.com> wrote:
>>>
>>> > By the way, is there any plan to make a pluggable backend for
>>> > checkpointing?   We might be interested in writing a, for example,
>>> > Cassandra backend.
>>> >
>>> > On Sat, Jan 25, 2014 at 9:49 PM, Xia, Junluan <junluan.xia@intel.com>
>>> wrote:
>>> >> Hi all
>>> >>
>>> >> The description about this Bug submitted by Matei is as following
>>> >>
>>> >>
>>> >> The tipping point seems to be around 50. We should fix this by
>>> checkpointing the RDDs every 10-20 iterations to break the lineage chain,
>>> but checkpointing currently requires HDFS installed, which not all users
>>> will have.
>>> >>
>>> >> We might also be able to fix DAGScheduler to not be recursive.
>>> >>
>>> >>
>>> >> regards,
>>> >> Andrew
>>> >>
>>> >
>>> >
>>> >
>>> > --
>>> > --
>>> > Evan Chan
>>> > Staff Engineer
>>> > ev@ooyala.com  |
>>>
>>>
>>

From dev-return-7237-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 04:14:25 2014
Return-Path: <dev-return-7237-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 428F511B09
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 04:14:25 +0000 (UTC)
Received: (qmail 21243 invoked by uid 500); 7 Apr 2014 04:14:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21152 invoked by uid 500); 7 Apr 2014 04:14:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21133 invoked by uid 99); 7 Apr 2014 04:14:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 04:14:16 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 74.125.82.172 as permitted sender)
Received: from [74.125.82.172] (HELO mail-we0-f172.google.com) (74.125.82.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 04:14:12 +0000
Received: by mail-we0-f172.google.com with SMTP id t61so6218309wes.17
        for <dev@spark.apache.org>; Sun, 06 Apr 2014 21:13:51 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=gwyBufL/48Z8+FCFYU7C3nUzoljnHaxgh/o4miXK0Tw=;
        b=Li0oA/rxbDQJaSbMBVd1IOs62aGbfP8s7Dq+KFu7xCtEbwrM5XsrakOfEwwJMeLc8F
         8L80bBxdDvgARLwDhFMJ3gZOgR1+hUCgeEnF2Da9AlxyKshn3oHpD5lHlY0VKLyaK2S3
         FepAAShSI+kpKkWyqyjCfW4YNbL+OkIREBG3QUIJ3S0N2rPBQjlXGdtk8uQj0VMLpMIi
         JYK1nUSfrhWK471d+YMiO6zXCHL/L5vb1JAGPnuDzTbYqj+8MWE3K04x3Tgekle9wsiD
         TE9+xQ6ta4+K9KbqM2NzCGBhzEWk+v7yWGEr7ykFzOeWwEI4FUiJdVzRc4bm4U6byV0L
         +37Q==
MIME-Version: 1.0
X-Received: by 10.194.186.140 with SMTP id fk12mr38790851wjc.47.1396844031690;
 Sun, 06 Apr 2014 21:13:51 -0700 (PDT)
Received: by 10.194.82.103 with HTTP; Sun, 6 Apr 2014 21:13:51 -0700 (PDT)
In-Reply-To: <CA+B-+fxQqzMDwt1FpBUK7zCp3mz10LQmN2p2X-JA4q_PtA3N2g@mail.gmail.com>
References: <7841A4E0A9C8784C9D8B07DF5E48F0A811656DB6@SHSMSX104.ccr.corp.intel.com>
	<CADWPM3jwcqhmDsNNspBn0YP6Bzvmxe22P442o=0b3+xcyh8G7g@mail.gmail.com>
	<51F0296D-3996-4DAD-9548-AD6444DA435D@gmail.com>
	<CA+B-+fxfghzWOeiAvcdOyHmP0hUenAZ7Xk+uuE3MStV+8r3uNw@mail.gmail.com>
	<CA+B-+fxQqzMDwt1FpBUK7zCp3mz10LQmN2p2X-JA4q_PtA3N2g@mail.gmail.com>
Date: Sun, 6 Apr 2014 21:13:51 -0700
Message-ID: <CAJgQjQ8QU02qKe22-VSzEjjAX+=NFy=U=Ew4EZ8XZPH2UgDBtA@mail.gmail.com>
Subject: Re: Any suggestion about JIRA 1006 "MLlib ALS gets stack overflow
 with too many iterations"?
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

The persist used in implicit ALS doesn't help StackOverflow problem.
Persist doesn't cut lineage. We need to call count() and then
checkpoint() to cut the lineage. Did you try the workaround mentioned
in https://issues.apache.org/jira/browse/SPARK-958:

"I tune JVM thread stack size to 512k via option -Xss512k and it works."

Best,
Xiangrui

On Sun, Apr 6, 2014 at 10:21 AM, Debasish Das <debasish.das83@gmail.com> wrote:
> At the head I see persist option in implicitPrefs but more cases like the
> ones mentioned above why don't we use similar technique and take an input
> that which iteration should we persist in explicit runs as well ?
>
> for (iter <- 1 to iterations) {
>         // perform ALS update
>         logInfo("Re-computing I given U (Iteration %d/%d)".format(iter,
> iterations))
>         products = updateFeatures(users, userOutLinks, productInLinks,
> partitioner, rank, lambda,
>           alpha, YtY = None)
>         logInfo("Re-computing U given I (Iteration %d/%d)".format(iter,
> iterations))
>         users = updateFeatures(products, productOutLinks, userInLinks,
> partitioner, rank, lambda,
>           alpha, YtY = None)
>       }
>
> Say if I want to persist at every k iterations out of N iterations of ALS
> explicit, there shoud be an option to do that...implicit right now uses
> persist at each iteration...
>
> Does this option make sense or you guys want this issue to be fixed in a
> different way...
>
> I definitely see that for my 25M x 3M run, with 64 gb executor memory,
> something is going wrong after 5-th iteration and I wanted to run for 10
> iterations...
>
> So my k is 4/5 for this particular problem...
>
> I can ask for the PR after testing the fix on the dataset I have...I will
> also try to see if we can make such datasets public for more research...
>
> For the LDA problem mentioned earlier in this email chain, k is 10...NMF
> can generate topics similar to LDA as well...Carrot2 project uses it...
>
>
>
> On Thu, Mar 27, 2014 at 3:20 PM, Debasish Das <debasish.das83@gmail.com>wrote:
>
>> Hi Matei,
>>
>> I am hitting similar problems with 10 ALS iterations...I am running with
>> 24 gb executor memory on 10 nodes for 20M x 3 M matrix with rank =50
>>
>> The first iteration of flatMaps run fine which means that the memory
>> requirements are good per iteration...
>>
>> If I do check-pointing on RDD, most likely rest 9 iterations will also run
>> fine and I will get the results...
>>
>> Is there a plan to add checkpoint option to ALS for such large
>> factorization jobs ?
>>
>> Thanks.
>> Deb
>>
>>
>>
>>
>>
>> On Tue, Jan 28, 2014 at 11:10 PM, Matei Zaharia <matei.zaharia@gmail.com>wrote:
>>
>>> That would be great to add. Right now it would be easy to change it to
>>> use another Hadoop FileSystem implementation at the very least (I think you
>>> can just pass the URL for that), but for Cassandra you'd have to use a
>>> different InputFormat or some direct Cassandra access API.
>>>
>>> Matei
>>>
>>> On Jan 28, 2014, at 5:02 PM, Evan Chan <ev@ooyala.com> wrote:
>>>
>>> > By the way, is there any plan to make a pluggable backend for
>>> > checkpointing?   We might be interested in writing a, for example,
>>> > Cassandra backend.
>>> >
>>> > On Sat, Jan 25, 2014 at 9:49 PM, Xia, Junluan <junluan.xia@intel.com>
>>> wrote:
>>> >> Hi all
>>> >>
>>> >> The description about this Bug submitted by Matei is as following
>>> >>
>>> >>
>>> >> The tipping point seems to be around 50. We should fix this by
>>> checkpointing the RDDs every 10-20 iterations to break the lineage chain,
>>> but checkpointing currently requires HDFS installed, which not all users
>>> will have.
>>> >>
>>> >> We might also be able to fix DAGScheduler to not be recursive.
>>> >>
>>> >>
>>> >> regards,
>>> >> Andrew
>>> >>
>>> >
>>> >
>>> >
>>> > --
>>> > --
>>> > Evan Chan
>>> > Staff Engineer
>>> > ev@ooyala.com  |
>>>
>>>
>>

From dev-return-7239-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 04:15:44 2014
Return-Path: <dev-return-7239-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EEF9111B11
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 04:15:44 +0000 (UTC)
Received: (qmail 23265 invoked by uid 500); 7 Apr 2014 04:15:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23164 invoked by uid 500); 7 Apr 2014 04:15:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23156 invoked by uid 99); 7 Apr 2014 04:15:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 04:15:42 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 74.125.82.47 as permitted sender)
Received: from [74.125.82.47] (HELO mail-wg0-f47.google.com) (74.125.82.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 04:15:38 +0000
Received: by mail-wg0-f47.google.com with SMTP id x12so6170267wgg.6
        for <dev@spark.incubator.apache.org>; Sun, 06 Apr 2014 21:15:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=7mZFTEoieWR+0VUO3sJ2UEWFMWpsA+Dlyay1TgyFSQw=;
        b=y5qUd8WLHgin0wWYy1N0DBpjkrbIE4ysJjNvXL7Bm6VSubfz2gcgxYmGHlJ8fB9b6i
         N6vXC/+GtTO8ps4kr3eLSjP80/AdOsLU99LcKVQxeago43rx67Js0kK1zy7sfadPL7gC
         vgq7D4I1v4u38KqDVNe5JhED48+z5EFfWXy98sYKOBaOx7JGdTDo9pLDWDB9sVlG/XdC
         Pst/0LPFhXXGY7+D8inJO9QuunsAAJjyUCmoOoFYULrC4dHw6U+PHAh9g5Fv0tgIrpmF
         7of9KqgCyG5Gw9l8lL7x6adMV/J4ym7vsYczx37TNFA/mXEZW5JBLHlnbD92DmipGTC7
         lw2w==
MIME-Version: 1.0
X-Received: by 10.194.204.199 with SMTP id la7mr40020518wjc.4.1396844117853;
 Sun, 06 Apr 2014 21:15:17 -0700 (PDT)
Received: by 10.194.82.103 with HTTP; Sun, 6 Apr 2014 21:15:17 -0700 (PDT)
In-Reply-To: <CAJgQjQ8QU02qKe22-VSzEjjAX+=NFy=U=Ew4EZ8XZPH2UgDBtA@mail.gmail.com>
References: <7841A4E0A9C8784C9D8B07DF5E48F0A811656DB6@SHSMSX104.ccr.corp.intel.com>
	<CADWPM3jwcqhmDsNNspBn0YP6Bzvmxe22P442o=0b3+xcyh8G7g@mail.gmail.com>
	<51F0296D-3996-4DAD-9548-AD6444DA435D@gmail.com>
	<CA+B-+fxfghzWOeiAvcdOyHmP0hUenAZ7Xk+uuE3MStV+8r3uNw@mail.gmail.com>
	<CA+B-+fxQqzMDwt1FpBUK7zCp3mz10LQmN2p2X-JA4q_PtA3N2g@mail.gmail.com>
	<CAJgQjQ8QU02qKe22-VSzEjjAX+=NFy=U=Ew4EZ8XZPH2UgDBtA@mail.gmail.com>
Date: Sun, 6 Apr 2014 21:15:17 -0700
Message-ID: <CAJgQjQ_1O-DfmhCUHTJy+=9K0mCXswN3PK_jKryXCjqveDwv6g@mail.gmail.com>
Subject: Re: Any suggestion about JIRA 1006 "MLlib ALS gets stack overflow
 with too many iterations"?
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Btw, explicit ALS doesn't need persist because each intermediate
factor is only used once. -Xiangrui

On Sun, Apr 6, 2014 at 9:13 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
> The persist used in implicit ALS doesn't help StackOverflow problem.
> Persist doesn't cut lineage. We need to call count() and then
> checkpoint() to cut the lineage. Did you try the workaround mentioned
> in https://issues.apache.org/jira/browse/SPARK-958:
>
> "I tune JVM thread stack size to 512k via option -Xss512k and it works."
>
> Best,
> Xiangrui
>
> On Sun, Apr 6, 2014 at 10:21 AM, Debasish Das <debasish.das83@gmail.com> wrote:
>> At the head I see persist option in implicitPrefs but more cases like the
>> ones mentioned above why don't we use similar technique and take an input
>> that which iteration should we persist in explicit runs as well ?
>>
>> for (iter <- 1 to iterations) {
>>         // perform ALS update
>>         logInfo("Re-computing I given U (Iteration %d/%d)".format(iter,
>> iterations))
>>         products = updateFeatures(users, userOutLinks, productInLinks,
>> partitioner, rank, lambda,
>>           alpha, YtY = None)
>>         logInfo("Re-computing U given I (Iteration %d/%d)".format(iter,
>> iterations))
>>         users = updateFeatures(products, productOutLinks, userInLinks,
>> partitioner, rank, lambda,
>>           alpha, YtY = None)
>>       }
>>
>> Say if I want to persist at every k iterations out of N iterations of ALS
>> explicit, there shoud be an option to do that...implicit right now uses
>> persist at each iteration...
>>
>> Does this option make sense or you guys want this issue to be fixed in a
>> different way...
>>
>> I definitely see that for my 25M x 3M run, with 64 gb executor memory,
>> something is going wrong after 5-th iteration and I wanted to run for 10
>> iterations...
>>
>> So my k is 4/5 for this particular problem...
>>
>> I can ask for the PR after testing the fix on the dataset I have...I will
>> also try to see if we can make such datasets public for more research...
>>
>> For the LDA problem mentioned earlier in this email chain, k is 10...NMF
>> can generate topics similar to LDA as well...Carrot2 project uses it...
>>
>>
>>
>> On Thu, Mar 27, 2014 at 3:20 PM, Debasish Das <debasish.das83@gmail.com>wrote:
>>
>>> Hi Matei,
>>>
>>> I am hitting similar problems with 10 ALS iterations...I am running with
>>> 24 gb executor memory on 10 nodes for 20M x 3 M matrix with rank =50
>>>
>>> The first iteration of flatMaps run fine which means that the memory
>>> requirements are good per iteration...
>>>
>>> If I do check-pointing on RDD, most likely rest 9 iterations will also run
>>> fine and I will get the results...
>>>
>>> Is there a plan to add checkpoint option to ALS for such large
>>> factorization jobs ?
>>>
>>> Thanks.
>>> Deb
>>>
>>>
>>>
>>>
>>>
>>> On Tue, Jan 28, 2014 at 11:10 PM, Matei Zaharia <matei.zaharia@gmail.com>wrote:
>>>
>>>> That would be great to add. Right now it would be easy to change it to
>>>> use another Hadoop FileSystem implementation at the very least (I think you
>>>> can just pass the URL for that), but for Cassandra you'd have to use a
>>>> different InputFormat or some direct Cassandra access API.
>>>>
>>>> Matei
>>>>
>>>> On Jan 28, 2014, at 5:02 PM, Evan Chan <ev@ooyala.com> wrote:
>>>>
>>>> > By the way, is there any plan to make a pluggable backend for
>>>> > checkpointing?   We might be interested in writing a, for example,
>>>> > Cassandra backend.
>>>> >
>>>> > On Sat, Jan 25, 2014 at 9:49 PM, Xia, Junluan <junluan.xia@intel.com>
>>>> wrote:
>>>> >> Hi all
>>>> >>
>>>> >> The description about this Bug submitted by Matei is as following
>>>> >>
>>>> >>
>>>> >> The tipping point seems to be around 50. We should fix this by
>>>> checkpointing the RDDs every 10-20 iterations to break the lineage chain,
>>>> but checkpointing currently requires HDFS installed, which not all users
>>>> will have.
>>>> >>
>>>> >> We might also be able to fix DAGScheduler to not be recursive.
>>>> >>
>>>> >>
>>>> >> regards,
>>>> >> Andrew
>>>> >>
>>>> >
>>>> >
>>>> >
>>>> > --
>>>> > --
>>>> > Evan Chan
>>>> > Staff Engineer
>>>> > ev@ooyala.com  |
>>>>
>>>>
>>>

From dev-return-7240-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 04:15:46 2014
Return-Path: <dev-return-7240-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DCD3311B12
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 04:15:46 +0000 (UTC)
Received: (qmail 23839 invoked by uid 500); 7 Apr 2014 04:15:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23790 invoked by uid 500); 7 Apr 2014 04:15:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23701 invoked by uid 99); 7 Apr 2014 04:15:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 04:15:43 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.212.179 as permitted sender)
Received: from [209.85.212.179] (HELO mail-wi0-f179.google.com) (209.85.212.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 04:15:39 +0000
Received: by mail-wi0-f179.google.com with SMTP id z2so4305671wiv.6
        for <dev@spark.apache.org>; Sun, 06 Apr 2014 21:15:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=7mZFTEoieWR+0VUO3sJ2UEWFMWpsA+Dlyay1TgyFSQw=;
        b=y5qUd8WLHgin0wWYy1N0DBpjkrbIE4ysJjNvXL7Bm6VSubfz2gcgxYmGHlJ8fB9b6i
         N6vXC/+GtTO8ps4kr3eLSjP80/AdOsLU99LcKVQxeago43rx67Js0kK1zy7sfadPL7gC
         vgq7D4I1v4u38KqDVNe5JhED48+z5EFfWXy98sYKOBaOx7JGdTDo9pLDWDB9sVlG/XdC
         Pst/0LPFhXXGY7+D8inJO9QuunsAAJjyUCmoOoFYULrC4dHw6U+PHAh9g5Fv0tgIrpmF
         7of9KqgCyG5Gw9l8lL7x6adMV/J4ym7vsYczx37TNFA/mXEZW5JBLHlnbD92DmipGTC7
         lw2w==
MIME-Version: 1.0
X-Received: by 10.194.204.199 with SMTP id la7mr40020518wjc.4.1396844117853;
 Sun, 06 Apr 2014 21:15:17 -0700 (PDT)
Received: by 10.194.82.103 with HTTP; Sun, 6 Apr 2014 21:15:17 -0700 (PDT)
In-Reply-To: <CAJgQjQ8QU02qKe22-VSzEjjAX+=NFy=U=Ew4EZ8XZPH2UgDBtA@mail.gmail.com>
References: <7841A4E0A9C8784C9D8B07DF5E48F0A811656DB6@SHSMSX104.ccr.corp.intel.com>
	<CADWPM3jwcqhmDsNNspBn0YP6Bzvmxe22P442o=0b3+xcyh8G7g@mail.gmail.com>
	<51F0296D-3996-4DAD-9548-AD6444DA435D@gmail.com>
	<CA+B-+fxfghzWOeiAvcdOyHmP0hUenAZ7Xk+uuE3MStV+8r3uNw@mail.gmail.com>
	<CA+B-+fxQqzMDwt1FpBUK7zCp3mz10LQmN2p2X-JA4q_PtA3N2g@mail.gmail.com>
	<CAJgQjQ8QU02qKe22-VSzEjjAX+=NFy=U=Ew4EZ8XZPH2UgDBtA@mail.gmail.com>
Date: Sun, 6 Apr 2014 21:15:17 -0700
Message-ID: <CAJgQjQ_1O-DfmhCUHTJy+=9K0mCXswN3PK_jKryXCjqveDwv6g@mail.gmail.com>
Subject: Re: Any suggestion about JIRA 1006 "MLlib ALS gets stack overflow
 with too many iterations"?
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Btw, explicit ALS doesn't need persist because each intermediate
factor is only used once. -Xiangrui

On Sun, Apr 6, 2014 at 9:13 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
> The persist used in implicit ALS doesn't help StackOverflow problem.
> Persist doesn't cut lineage. We need to call count() and then
> checkpoint() to cut the lineage. Did you try the workaround mentioned
> in https://issues.apache.org/jira/browse/SPARK-958:
>
> "I tune JVM thread stack size to 512k via option -Xss512k and it works."
>
> Best,
> Xiangrui
>
> On Sun, Apr 6, 2014 at 10:21 AM, Debasish Das <debasish.das83@gmail.com> wrote:
>> At the head I see persist option in implicitPrefs but more cases like the
>> ones mentioned above why don't we use similar technique and take an input
>> that which iteration should we persist in explicit runs as well ?
>>
>> for (iter <- 1 to iterations) {
>>         // perform ALS update
>>         logInfo("Re-computing I given U (Iteration %d/%d)".format(iter,
>> iterations))
>>         products = updateFeatures(users, userOutLinks, productInLinks,
>> partitioner, rank, lambda,
>>           alpha, YtY = None)
>>         logInfo("Re-computing U given I (Iteration %d/%d)".format(iter,
>> iterations))
>>         users = updateFeatures(products, productOutLinks, userInLinks,
>> partitioner, rank, lambda,
>>           alpha, YtY = None)
>>       }
>>
>> Say if I want to persist at every k iterations out of N iterations of ALS
>> explicit, there shoud be an option to do that...implicit right now uses
>> persist at each iteration...
>>
>> Does this option make sense or you guys want this issue to be fixed in a
>> different way...
>>
>> I definitely see that for my 25M x 3M run, with 64 gb executor memory,
>> something is going wrong after 5-th iteration and I wanted to run for 10
>> iterations...
>>
>> So my k is 4/5 for this particular problem...
>>
>> I can ask for the PR after testing the fix on the dataset I have...I will
>> also try to see if we can make such datasets public for more research...
>>
>> For the LDA problem mentioned earlier in this email chain, k is 10...NMF
>> can generate topics similar to LDA as well...Carrot2 project uses it...
>>
>>
>>
>> On Thu, Mar 27, 2014 at 3:20 PM, Debasish Das <debasish.das83@gmail.com>wrote:
>>
>>> Hi Matei,
>>>
>>> I am hitting similar problems with 10 ALS iterations...I am running with
>>> 24 gb executor memory on 10 nodes for 20M x 3 M matrix with rank =50
>>>
>>> The first iteration of flatMaps run fine which means that the memory
>>> requirements are good per iteration...
>>>
>>> If I do check-pointing on RDD, most likely rest 9 iterations will also run
>>> fine and I will get the results...
>>>
>>> Is there a plan to add checkpoint option to ALS for such large
>>> factorization jobs ?
>>>
>>> Thanks.
>>> Deb
>>>
>>>
>>>
>>>
>>>
>>> On Tue, Jan 28, 2014 at 11:10 PM, Matei Zaharia <matei.zaharia@gmail.com>wrote:
>>>
>>>> That would be great to add. Right now it would be easy to change it to
>>>> use another Hadoop FileSystem implementation at the very least (I think you
>>>> can just pass the URL for that), but for Cassandra you'd have to use a
>>>> different InputFormat or some direct Cassandra access API.
>>>>
>>>> Matei
>>>>
>>>> On Jan 28, 2014, at 5:02 PM, Evan Chan <ev@ooyala.com> wrote:
>>>>
>>>> > By the way, is there any plan to make a pluggable backend for
>>>> > checkpointing?   We might be interested in writing a, for example,
>>>> > Cassandra backend.
>>>> >
>>>> > On Sat, Jan 25, 2014 at 9:49 PM, Xia, Junluan <junluan.xia@intel.com>
>>>> wrote:
>>>> >> Hi all
>>>> >>
>>>> >> The description about this Bug submitted by Matei is as following
>>>> >>
>>>> >>
>>>> >> The tipping point seems to be around 50. We should fix this by
>>>> checkpointing the RDDs every 10-20 iterations to break the lineage chain,
>>>> but checkpointing currently requires HDFS installed, which not all users
>>>> will have.
>>>> >>
>>>> >> We might also be able to fix DAGScheduler to not be recursive.
>>>> >>
>>>> >>
>>>> >> regards,
>>>> >> Andrew
>>>> >>
>>>> >
>>>> >
>>>> >
>>>> > --
>>>> > --
>>>> > Evan Chan
>>>> > Staff Engineer
>>>> > ev@ooyala.com  |
>>>>
>>>>
>>>

From dev-return-7241-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 06:02:24 2014
Return-Path: <dev-return-7241-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A12DB11D42
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 06:02:24 +0000 (UTC)
Received: (qmail 5615 invoked by uid 500); 7 Apr 2014 06:02:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5279 invoked by uid 500); 7 Apr 2014 06:02:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4735 invoked by uid 99); 7 Apr 2014 06:02:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 06:02:18 +0000
X-ASF-Spam-Status: No, hits=2.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.180 as permitted sender)
Received: from [209.85.192.180] (HELO mail-pd0-f180.google.com) (209.85.192.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 06:02:13 +0000
Received: by mail-pd0-f180.google.com with SMTP id v10so6026262pde.25
        for <dev@spark.apache.org>; Sun, 06 Apr 2014 23:01:51 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=sJMlepLBMK24QGDK9LCwiNZPvlculAuTv3DGUmUVkZY=;
        b=jEEeweBE43WmGkJB+JIKthHm7vjHDT39iQO9WVjt1h0AwbB+j8u7RW0f7mAtBgM4My
         MCDZpsesIddItYFgvFl1FEGYgomb2yzHNsAmGYHUaCm2+G2UwCDGYL9jNhY81y9Se/rI
         6zyOTOCmBOniARsye/DcdBbNSdRy2JhBHvPv+Ca/IrYlNwKm+nSq/qkTWCKnR9b3gfI/
         lpxhLj03HNfh3TfhDZMrLrVVHWzDKoatre+Ji+Kn+U5ACgCra/hPCJN+uDccg0ywm/qD
         MeAMQaZezQZkdoJTpEvFU1d2gA/NMaNz6Y/jT2CG7+SDYDoBYWfGVtYtbwkJkkXSwlX2
         9OgA==
MIME-Version: 1.0
X-Received: by 10.66.129.133 with SMTP id nw5mr28984192pab.98.1396850511144;
 Sun, 06 Apr 2014 23:01:51 -0700 (PDT)
Received: by 10.68.12.39 with HTTP; Sun, 6 Apr 2014 23:01:51 -0700 (PDT)
In-Reply-To: <CAJgQjQ-nYkkMwDXkgiSKqtiE0Lk1kKRYeWHe-O=N=eHBHzgiBg@mail.gmail.com>
References: <CA+B-+fzYxLU6OUX6hufpsbAmJLoYbPQupMRK=MR8Yx6jkLOj2g@mail.gmail.com>
	<CAJgQjQ-nYkkMwDXkgiSKqtiE0Lk1kKRYeWHe-O=N=eHBHzgiBg@mail.gmail.com>
Date: Sun, 6 Apr 2014 23:01:51 -0700
Message-ID: <CA+B-+fxibq0ZMCoN27OnTbfrtVhz+ShPSVFtabUi63nUftBcag@mail.gmail.com>
Subject: Re: ALS array index out of bound with 50 factors
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11364e98856bd004f66d9a7d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11364e98856bd004f66d9a7d
Content-Type: text/plain; charset=ISO-8859-1

Hi Xiangrui,

With 4 ALS iterations it runs fine...If I run 10 I am failing...I believe I
have to cut the lineage chain and call checkpoint....Trying to follow the
other email chain on checkpointing...

Thanks.
Deb


On Sun, Apr 6, 2014 at 9:08 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Hi Deb,
>
> Are you using the master branch or a particular commit? Do you have
> negative or out-of-integer-range user or product ids? There is an
> issue with ALS' partitioning
> (https://spark-project.atlassian.net/browse/SPARK-1281), but I'm not
> sure whether that is the reason. Could you try to see whether you can
> reproduce the error on a public data set, e.g., movielens? Thanks!
>
> Best,
> Xiangrui
>
> On Sat, Apr 5, 2014 at 10:53 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
> > Hi,
> >
> > I deployed apache/spark master today and recently there were many ALS
> > related checkins and enhancements..
> >
> > I am running ALS with explicit feedback and I remember most enhancements
> > were related to implicit feedback...
> >
> > With 25 factors my runs were successful but with 50 factors I am getting
> > array index out of bound...
> >
> > Note that I was hitting gc errors before with an older version of spark
> but
> > it seems like the sparse matrix partitioning scheme has changed
> now...data
> > caching looks much balanced now...earlier one node was becoming
> > bottleneck...Although I ran with 64g memory per node...
> >
> > There are around 3M products, 25M users...
> >
> > Anyone noticed this bug or something similar ?
> >
> > 14/04/05 23:03:15 WARN TaskSetManager: Loss was due to
> > java.lang.ArrayIndexOutOfBoundsException
> > java.lang.ArrayIndexOutOfBoundsException: 81029
> >     at
> >
> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateBlock$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(ALS.scala:450)
> >     at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
> >     at
> >
> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateBlock$1.apply$mcVI$sp(ALS.scala:446)
> >     at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
> >     at org.apache.spark.mllib.recommendation.ALS.org
> > $apache$spark$mllib$recommendation$ALS$$updateBlock(ALS.scala:445)
> >     at
> >
> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:416)
> >     at
> >
> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:415)
> >     at
> >
> org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31)
> >     at
> >
> org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31)
> >     at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
> >     at
> >
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:149)
> >     at
> >
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:147)
> >     at
> >
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> >     at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> >     at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:147)
> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
> >     at
> > org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
> >     at
> >
> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
> >     at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
> >     at
> >
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)
> >     at
> >
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
> >     at org.apache.spark.scheduler.Task.run(Task.scala:52)
> >     at
> >
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
> >     at
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:43)
> >     at
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
> >     at java.security.AccessController.doPrivileged(Native Method)
> >     at javax.security.auth.Subject.doAs(Subject.java:396)
> >     at
> >
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
> >     at
> >
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:42)
> >     at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
> >     at
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
> >     at
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
> >     at java.lang.Thread.run(Thread.java:662)
> >
> > Thanks.
> > Deb
>

--001a11364e98856bd004f66d9a7d--

From dev-return-7242-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 06:09:55 2014
Return-Path: <dev-return-7242-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5C9B111D5A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 06:09:55 +0000 (UTC)
Received: (qmail 11318 invoked by uid 500); 7 Apr 2014 06:09:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10804 invoked by uid 500); 7 Apr 2014 06:09:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10773 invoked by uid 99); 7 Apr 2014 06:09:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 06:09:50 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.160.51 as permitted sender)
Received: from [209.85.160.51] (HELO mail-pb0-f51.google.com) (209.85.160.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 06:09:46 +0000
Received: by mail-pb0-f51.google.com with SMTP id uo5so6231086pbc.38
        for <dev@spark.apache.org>; Sun, 06 Apr 2014 23:09:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=zexaa13F8WjYSBjjQTmhqEdNCsLb3GX9zCrQ8XfYDLs=;
        b=L7k50gKG1c9Rk8OjIlQqmo2lOGPppB+e05Frct00lksJmT9DcJy8aW1LC7H+it6l9o
         Z3HCDG2aHnWBR+62ym8cpMH8l0E/xG5qgjQltSYTyqyLeVhBN0yRpkqPdQm0+zsCjBza
         vmSL6t9MXBIfxdVhpnuFlUnknXYRcvcheeH+8mZY/S6FPesOOSm3r1kMgDA/ldEkixcX
         CT0FQUzcOOPgUT1I+1NpLAF2bWoEi5Kj5cR3Yb+DrzzntmeSoqrM5NTTGJ44a5xF1NO1
         PQhynP/CFalBEaqLEpM/YiytVmlub4IspipZZo7vKR4Z2Z2stv2o+rBXSLSNnAg+NiEK
         EiBQ==
MIME-Version: 1.0
X-Received: by 10.68.237.99 with SMTP id vb3mr29004783pbc.76.1396850966219;
 Sun, 06 Apr 2014 23:09:26 -0700 (PDT)
Received: by 10.68.12.39 with HTTP; Sun, 6 Apr 2014 23:09:26 -0700 (PDT)
In-Reply-To: <CAJgQjQ_1O-DfmhCUHTJy+=9K0mCXswN3PK_jKryXCjqveDwv6g@mail.gmail.com>
References: <7841A4E0A9C8784C9D8B07DF5E48F0A811656DB6@SHSMSX104.ccr.corp.intel.com>
	<CADWPM3jwcqhmDsNNspBn0YP6Bzvmxe22P442o=0b3+xcyh8G7g@mail.gmail.com>
	<51F0296D-3996-4DAD-9548-AD6444DA435D@gmail.com>
	<CA+B-+fxfghzWOeiAvcdOyHmP0hUenAZ7Xk+uuE3MStV+8r3uNw@mail.gmail.com>
	<CA+B-+fxQqzMDwt1FpBUK7zCp3mz10LQmN2p2X-JA4q_PtA3N2g@mail.gmail.com>
	<CAJgQjQ8QU02qKe22-VSzEjjAX+=NFy=U=Ew4EZ8XZPH2UgDBtA@mail.gmail.com>
	<CAJgQjQ_1O-DfmhCUHTJy+=9K0mCXswN3PK_jKryXCjqveDwv6g@mail.gmail.com>
Date: Sun, 6 Apr 2014 23:09:26 -0700
Message-ID: <CA+B-+fxOXH4mdJM+e1d9D=2bxn3HzCvKuDeW8oqEeveg+55X2Q@mail.gmail.com>
Subject: Re: Any suggestion about JIRA 1006 "MLlib ALS gets stack overflow
 with too many iterations"?
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=047d7b33c8dea5505304f66db51a
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b33c8dea5505304f66db51a
Content-Type: text/plain; charset=ISO-8859-1

Sorry not persist...I meant adding a user parameter k which does checkpoint
after every k iterations...out of N ALS iterations...We have hdfs installed
so not a big deal...is there an issue of adding this user parameter in
ALS.scala ? If it is then I can add it to our internal branch...

For me tipping k seems like 4...With 4 iterations I can write out the
factors...if I run with 10 iterations, after 4 I can see that it restarts
the sparse matrix partition...tries to run all the iterations over again
and fails due to array index out of bound which does not seems like a real
bug...

Not sure if it can be reproduced in movielens as the dataset I have is 25M
x 3M (and counting)...whille movielens is tall and thin....

Another idea would be to give an option to restart ALS with previous
factors...that way ALS core algorithm does not need to change and it might
be more useful...and that way we can point to a location from where the old
factors can be load...I think @sean used similar idea in Oryx generations...

Let me know which way you guys prefer....I can add it in...




On Sun, Apr 6, 2014 at 9:15 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Btw, explicit ALS doesn't need persist because each intermediate
> factor is only used once. -Xiangrui
>
> On Sun, Apr 6, 2014 at 9:13 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
> > The persist used in implicit ALS doesn't help StackOverflow problem.
> > Persist doesn't cut lineage. We need to call count() and then
> > checkpoint() to cut the lineage. Did you try the workaround mentioned
> > in https://issues.apache.org/jira/browse/SPARK-958:
> >
> > "I tune JVM thread stack size to 512k via option -Xss512k and it works."
> >
> > Best,
> > Xiangrui
> >
> > On Sun, Apr 6, 2014 at 10:21 AM, Debasish Das <debasish.das83@gmail.com>
> wrote:
> >> At the head I see persist option in implicitPrefs but more cases like
> the
> >> ones mentioned above why don't we use similar technique and take an
> input
> >> that which iteration should we persist in explicit runs as well ?
> >>
> >> for (iter <- 1 to iterations) {
> >>         // perform ALS update
> >>         logInfo("Re-computing I given U (Iteration %d/%d)".format(iter,
> >> iterations))
> >>         products = updateFeatures(users, userOutLinks, productInLinks,
> >> partitioner, rank, lambda,
> >>           alpha, YtY = None)
> >>         logInfo("Re-computing U given I (Iteration %d/%d)".format(iter,
> >> iterations))
> >>         users = updateFeatures(products, productOutLinks, userInLinks,
> >> partitioner, rank, lambda,
> >>           alpha, YtY = None)
> >>       }
> >>
> >> Say if I want to persist at every k iterations out of N iterations of
> ALS
> >> explicit, there shoud be an option to do that...implicit right now uses
> >> persist at each iteration...
> >>
> >> Does this option make sense or you guys want this issue to be fixed in a
> >> different way...
> >>
> >> I definitely see that for my 25M x 3M run, with 64 gb executor memory,
> >> something is going wrong after 5-th iteration and I wanted to run for 10
> >> iterations...
> >>
> >> So my k is 4/5 for this particular problem...
> >>
> >> I can ask for the PR after testing the fix on the dataset I have...I
> will
> >> also try to see if we can make such datasets public for more research...
> >>
> >> For the LDA problem mentioned earlier in this email chain, k is 10...NMF
> >> can generate topics similar to LDA as well...Carrot2 project uses it...
> >>
> >>
> >>
> >> On Thu, Mar 27, 2014 at 3:20 PM, Debasish Das <debasish.das83@gmail.com
> >wrote:
> >>
> >>> Hi Matei,
> >>>
> >>> I am hitting similar problems with 10 ALS iterations...I am running
> with
> >>> 24 gb executor memory on 10 nodes for 20M x 3 M matrix with rank =50
> >>>
> >>> The first iteration of flatMaps run fine which means that the memory
> >>> requirements are good per iteration...
> >>>
> >>> If I do check-pointing on RDD, most likely rest 9 iterations will also
> run
> >>> fine and I will get the results...
> >>>
> >>> Is there a plan to add checkpoint option to ALS for such large
> >>> factorization jobs ?
> >>>
> >>> Thanks.
> >>> Deb
> >>>
> >>>
> >>>
> >>>
> >>>
> >>> On Tue, Jan 28, 2014 at 11:10 PM, Matei Zaharia <
> matei.zaharia@gmail.com>wrote:
> >>>
> >>>> That would be great to add. Right now it would be easy to change it to
> >>>> use another Hadoop FileSystem implementation at the very least (I
> think you
> >>>> can just pass the URL for that), but for Cassandra you'd have to use a
> >>>> different InputFormat or some direct Cassandra access API.
> >>>>
> >>>> Matei
> >>>>
> >>>> On Jan 28, 2014, at 5:02 PM, Evan Chan <ev@ooyala.com> wrote:
> >>>>
> >>>> > By the way, is there any plan to make a pluggable backend for
> >>>> > checkpointing?   We might be interested in writing a, for example,
> >>>> > Cassandra backend.
> >>>> >
> >>>> > On Sat, Jan 25, 2014 at 9:49 PM, Xia, Junluan <
> junluan.xia@intel.com>
> >>>> wrote:
> >>>> >> Hi all
> >>>> >>
> >>>> >> The description about this Bug submitted by Matei is as following
> >>>> >>
> >>>> >>
> >>>> >> The tipping point seems to be around 50. We should fix this by
> >>>> checkpointing the RDDs every 10-20 iterations to break the lineage
> chain,
> >>>> but checkpointing currently requires HDFS installed, which not all
> users
> >>>> will have.
> >>>> >>
> >>>> >> We might also be able to fix DAGScheduler to not be recursive.
> >>>> >>
> >>>> >>
> >>>> >> regards,
> >>>> >> Andrew
> >>>> >>
> >>>> >
> >>>> >
> >>>> >
> >>>> > --
> >>>> > --
> >>>> > Evan Chan
> >>>> > Staff Engineer
> >>>> > ev@ooyala.com  |
> >>>>
> >>>>
> >>>
>

--047d7b33c8dea5505304f66db51a--

From dev-return-7243-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 06:09:58 2014
Return-Path: <dev-return-7243-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 198F811D5B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 06:09:58 +0000 (UTC)
Received: (qmail 11468 invoked by uid 500); 7 Apr 2014 06:09:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11379 invoked by uid 500); 7 Apr 2014 06:09:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10774 invoked by uid 99); 7 Apr 2014 06:09:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 06:09:50 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.220.43 as permitted sender)
Received: from [209.85.220.43] (HELO mail-pa0-f43.google.com) (209.85.220.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 06:09:46 +0000
Received: by mail-pa0-f43.google.com with SMTP id bj1so6244085pad.2
        for <dev@spark.incubator.apache.org>; Sun, 06 Apr 2014 23:09:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=zexaa13F8WjYSBjjQTmhqEdNCsLb3GX9zCrQ8XfYDLs=;
        b=L7k50gKG1c9Rk8OjIlQqmo2lOGPppB+e05Frct00lksJmT9DcJy8aW1LC7H+it6l9o
         Z3HCDG2aHnWBR+62ym8cpMH8l0E/xG5qgjQltSYTyqyLeVhBN0yRpkqPdQm0+zsCjBza
         vmSL6t9MXBIfxdVhpnuFlUnknXYRcvcheeH+8mZY/S6FPesOOSm3r1kMgDA/ldEkixcX
         CT0FQUzcOOPgUT1I+1NpLAF2bWoEi5Kj5cR3Yb+DrzzntmeSoqrM5NTTGJ44a5xF1NO1
         PQhynP/CFalBEaqLEpM/YiytVmlub4IspipZZo7vKR4Z2Z2stv2o+rBXSLSNnAg+NiEK
         EiBQ==
MIME-Version: 1.0
X-Received: by 10.68.237.99 with SMTP id vb3mr29004783pbc.76.1396850966219;
 Sun, 06 Apr 2014 23:09:26 -0700 (PDT)
Received: by 10.68.12.39 with HTTP; Sun, 6 Apr 2014 23:09:26 -0700 (PDT)
In-Reply-To: <CAJgQjQ_1O-DfmhCUHTJy+=9K0mCXswN3PK_jKryXCjqveDwv6g@mail.gmail.com>
References: <7841A4E0A9C8784C9D8B07DF5E48F0A811656DB6@SHSMSX104.ccr.corp.intel.com>
	<CADWPM3jwcqhmDsNNspBn0YP6Bzvmxe22P442o=0b3+xcyh8G7g@mail.gmail.com>
	<51F0296D-3996-4DAD-9548-AD6444DA435D@gmail.com>
	<CA+B-+fxfghzWOeiAvcdOyHmP0hUenAZ7Xk+uuE3MStV+8r3uNw@mail.gmail.com>
	<CA+B-+fxQqzMDwt1FpBUK7zCp3mz10LQmN2p2X-JA4q_PtA3N2g@mail.gmail.com>
	<CAJgQjQ8QU02qKe22-VSzEjjAX+=NFy=U=Ew4EZ8XZPH2UgDBtA@mail.gmail.com>
	<CAJgQjQ_1O-DfmhCUHTJy+=9K0mCXswN3PK_jKryXCjqveDwv6g@mail.gmail.com>
Date: Sun, 6 Apr 2014 23:09:26 -0700
Message-ID: <CA+B-+fxOXH4mdJM+e1d9D=2bxn3HzCvKuDeW8oqEeveg+55X2Q@mail.gmail.com>
Subject: Re: Any suggestion about JIRA 1006 "MLlib ALS gets stack overflow
 with too many iterations"?
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=047d7b33c8dea5505304f66db51a
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b33c8dea5505304f66db51a
Content-Type: text/plain; charset=ISO-8859-1

Sorry not persist...I meant adding a user parameter k which does checkpoint
after every k iterations...out of N ALS iterations...We have hdfs installed
so not a big deal...is there an issue of adding this user parameter in
ALS.scala ? If it is then I can add it to our internal branch...

For me tipping k seems like 4...With 4 iterations I can write out the
factors...if I run with 10 iterations, after 4 I can see that it restarts
the sparse matrix partition...tries to run all the iterations over again
and fails due to array index out of bound which does not seems like a real
bug...

Not sure if it can be reproduced in movielens as the dataset I have is 25M
x 3M (and counting)...whille movielens is tall and thin....

Another idea would be to give an option to restart ALS with previous
factors...that way ALS core algorithm does not need to change and it might
be more useful...and that way we can point to a location from where the old
factors can be load...I think @sean used similar idea in Oryx generations...

Let me know which way you guys prefer....I can add it in...




On Sun, Apr 6, 2014 at 9:15 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Btw, explicit ALS doesn't need persist because each intermediate
> factor is only used once. -Xiangrui
>
> On Sun, Apr 6, 2014 at 9:13 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
> > The persist used in implicit ALS doesn't help StackOverflow problem.
> > Persist doesn't cut lineage. We need to call count() and then
> > checkpoint() to cut the lineage. Did you try the workaround mentioned
> > in https://issues.apache.org/jira/browse/SPARK-958:
> >
> > "I tune JVM thread stack size to 512k via option -Xss512k and it works."
> >
> > Best,
> > Xiangrui
> >
> > On Sun, Apr 6, 2014 at 10:21 AM, Debasish Das <debasish.das83@gmail.com>
> wrote:
> >> At the head I see persist option in implicitPrefs but more cases like
> the
> >> ones mentioned above why don't we use similar technique and take an
> input
> >> that which iteration should we persist in explicit runs as well ?
> >>
> >> for (iter <- 1 to iterations) {
> >>         // perform ALS update
> >>         logInfo("Re-computing I given U (Iteration %d/%d)".format(iter,
> >> iterations))
> >>         products = updateFeatures(users, userOutLinks, productInLinks,
> >> partitioner, rank, lambda,
> >>           alpha, YtY = None)
> >>         logInfo("Re-computing U given I (Iteration %d/%d)".format(iter,
> >> iterations))
> >>         users = updateFeatures(products, productOutLinks, userInLinks,
> >> partitioner, rank, lambda,
> >>           alpha, YtY = None)
> >>       }
> >>
> >> Say if I want to persist at every k iterations out of N iterations of
> ALS
> >> explicit, there shoud be an option to do that...implicit right now uses
> >> persist at each iteration...
> >>
> >> Does this option make sense or you guys want this issue to be fixed in a
> >> different way...
> >>
> >> I definitely see that for my 25M x 3M run, with 64 gb executor memory,
> >> something is going wrong after 5-th iteration and I wanted to run for 10
> >> iterations...
> >>
> >> So my k is 4/5 for this particular problem...
> >>
> >> I can ask for the PR after testing the fix on the dataset I have...I
> will
> >> also try to see if we can make such datasets public for more research...
> >>
> >> For the LDA problem mentioned earlier in this email chain, k is 10...NMF
> >> can generate topics similar to LDA as well...Carrot2 project uses it...
> >>
> >>
> >>
> >> On Thu, Mar 27, 2014 at 3:20 PM, Debasish Das <debasish.das83@gmail.com
> >wrote:
> >>
> >>> Hi Matei,
> >>>
> >>> I am hitting similar problems with 10 ALS iterations...I am running
> with
> >>> 24 gb executor memory on 10 nodes for 20M x 3 M matrix with rank =50
> >>>
> >>> The first iteration of flatMaps run fine which means that the memory
> >>> requirements are good per iteration...
> >>>
> >>> If I do check-pointing on RDD, most likely rest 9 iterations will also
> run
> >>> fine and I will get the results...
> >>>
> >>> Is there a plan to add checkpoint option to ALS for such large
> >>> factorization jobs ?
> >>>
> >>> Thanks.
> >>> Deb
> >>>
> >>>
> >>>
> >>>
> >>>
> >>> On Tue, Jan 28, 2014 at 11:10 PM, Matei Zaharia <
> matei.zaharia@gmail.com>wrote:
> >>>
> >>>> That would be great to add. Right now it would be easy to change it to
> >>>> use another Hadoop FileSystem implementation at the very least (I
> think you
> >>>> can just pass the URL for that), but for Cassandra you'd have to use a
> >>>> different InputFormat or some direct Cassandra access API.
> >>>>
> >>>> Matei
> >>>>
> >>>> On Jan 28, 2014, at 5:02 PM, Evan Chan <ev@ooyala.com> wrote:
> >>>>
> >>>> > By the way, is there any plan to make a pluggable backend for
> >>>> > checkpointing?   We might be interested in writing a, for example,
> >>>> > Cassandra backend.
> >>>> >
> >>>> > On Sat, Jan 25, 2014 at 9:49 PM, Xia, Junluan <
> junluan.xia@intel.com>
> >>>> wrote:
> >>>> >> Hi all
> >>>> >>
> >>>> >> The description about this Bug submitted by Matei is as following
> >>>> >>
> >>>> >>
> >>>> >> The tipping point seems to be around 50. We should fix this by
> >>>> checkpointing the RDDs every 10-20 iterations to break the lineage
> chain,
> >>>> but checkpointing currently requires HDFS installed, which not all
> users
> >>>> will have.
> >>>> >>
> >>>> >> We might also be able to fix DAGScheduler to not be recursive.
> >>>> >>
> >>>> >>
> >>>> >> regards,
> >>>> >> Andrew
> >>>> >>
> >>>> >
> >>>> >
> >>>> >
> >>>> > --
> >>>> > --
> >>>> > Evan Chan
> >>>> > Staff Engineer
> >>>> > ev@ooyala.com  |
> >>>>
> >>>>
> >>>
>

--047d7b33c8dea5505304f66db51a--

From dev-return-7244-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 13:57:34 2014
Return-Path: <dev-return-7244-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 378C710A10
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 13:57:34 +0000 (UTC)
Received: (qmail 61305 invoked by uid 500); 7 Apr 2014 13:57:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60677 invoked by uid 500); 7 Apr 2014 13:57:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60669 invoked by uid 99); 7 Apr 2014 13:57:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 13:57:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nick.pentreath@gmail.com designates 209.85.219.51 as permitted sender)
Received: from [209.85.219.51] (HELO mail-oa0-f51.google.com) (209.85.219.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 13:57:22 +0000
Received: by mail-oa0-f51.google.com with SMTP id i4so6591290oah.24
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 06:57:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=gXTtJs72PhBg63yiCZotWrJXBsFbVJ5Khf8ktV6qB9s=;
        b=FrRNVg6Y3LGbhBsNmAQgovBnZMK83sVDxffY3Z9eZiQuNZRo/Y+idVMAU9NIl2JlM4
         cTOlor5WXRs8Zg+D/ySCekl2ukkIn+j5Gs1TuzC0FUcoEp03YJBZ/kUT0NepCgxgZfN5
         4YNlZfAyVu28LaTFAwkDmwcEu90QZxUe8xZ1T5NivzuV+WkZUrpngy7qitGyUni/BhGA
         GTFy4fy4EYNd5o99I9SF2/9cJopDEhWCCvQsCW9gvRA5SLKTltk1Vmv36PPiS+2DRE4o
         ERg7aGtOO7DBgMUqbPxZfAdECdJds5CXY5dUXEDjDnkll7dRU5bgui8N1DTY64F+XH6j
         6RzA==
MIME-Version: 1.0
X-Received: by 10.60.51.4 with SMTP id g4mr2824942oeo.52.1396879021596; Mon,
 07 Apr 2014 06:57:01 -0700 (PDT)
Received: by 10.182.95.103 with HTTP; Mon, 7 Apr 2014 06:57:01 -0700 (PDT)
In-Reply-To: <CAJgQjQ-nYkkMwDXkgiSKqtiE0Lk1kKRYeWHe-O=N=eHBHzgiBg@mail.gmail.com>
References: <CA+B-+fzYxLU6OUX6hufpsbAmJLoYbPQupMRK=MR8Yx6jkLOj2g@mail.gmail.com>
	<CAJgQjQ-nYkkMwDXkgiSKqtiE0Lk1kKRYeWHe-O=N=eHBHzgiBg@mail.gmail.com>
Date: Mon, 7 Apr 2014 15:57:01 +0200
Message-ID: <CALD+6GOoDp3ymn25NQtgGVrTvizxSSSYg_Hi91dz1hPt-XUC5w@mail.gmail.com>
Subject: Re: ALS array index out of bound with 50 factors
From: Nick Pentreath <nick.pentreath@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c304a4e065f104f6743da3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c304a4e065f104f6743da3
Content-Type: text/plain; charset=ISO-8859-1

On the partitioning / id keys. If we would look at hash partitioning, how
feasible will it be to just allow the user and item ids to be strings? A
lot of the time these ids are strings anyway (UUIDs and so on), and it's
really painful to translate between String <-> Int the whole time.

Are there any obvious blockers to this? I am a bit rusty on the ALS code
but from a quick scan I think this may work. Performance may be an issue
with large String keys... Any majore issues/objections to this thinking?

I may be able to find time to take a stab at this if there is demand.


On Mon, Apr 7, 2014 at 6:08 AM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Hi Deb,
>
> Are you using the master branch or a particular commit? Do you have
> negative or out-of-integer-range user or product ids? There is an
> issue with ALS' partitioning
> (https://spark-project.atlassian.net/browse/SPARK-1281), but I'm not
> sure whether that is the reason. Could you try to see whether you can
> reproduce the error on a public data set, e.g., movielens? Thanks!
>
> Best,
> Xiangrui
>
> On Sat, Apr 5, 2014 at 10:53 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
> > Hi,
> >
> > I deployed apache/spark master today and recently there were many ALS
> > related checkins and enhancements..
> >
> > I am running ALS with explicit feedback and I remember most enhancements
> > were related to implicit feedback...
> >
> > With 25 factors my runs were successful but with 50 factors I am getting
> > array index out of bound...
> >
> > Note that I was hitting gc errors before with an older version of spark
> but
> > it seems like the sparse matrix partitioning scheme has changed
> now...data
> > caching looks much balanced now...earlier one node was becoming
> > bottleneck...Although I ran with 64g memory per node...
> >
> > There are around 3M products, 25M users...
> >
> > Anyone noticed this bug or something similar ?
> >
> > 14/04/05 23:03:15 WARN TaskSetManager: Loss was due to
> > java.lang.ArrayIndexOutOfBoundsException
> > java.lang.ArrayIndexOutOfBoundsException: 81029
> >     at
> >
> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateBlock$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(ALS.scala:450)
> >     at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
> >     at
> >
> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateBlock$1.apply$mcVI$sp(ALS.scala:446)
> >     at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
> >     at org.apache.spark.mllib.recommendation.ALS.org
> > $apache$spark$mllib$recommendation$ALS$$updateBlock(ALS.scala:445)
> >     at
> >
> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:416)
> >     at
> >
> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:415)
> >     at
> >
> org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31)
> >     at
> >
> org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31)
> >     at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
> >     at
> >
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:149)
> >     at
> >
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:147)
> >     at
> >
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> >     at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> >     at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:147)
> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
> >     at
> > org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
> >     at
> >
> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
> >     at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
> >     at
> >
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)
> >     at
> >
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
> >     at org.apache.spark.scheduler.Task.run(Task.scala:52)
> >     at
> >
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
> >     at
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:43)
> >     at
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
> >     at java.security.AccessController.doPrivileged(Native Method)
> >     at javax.security.auth.Subject.doAs(Subject.java:396)
> >     at
> >
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
> >     at
> >
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:42)
> >     at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
> >     at
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
> >     at
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
> >     at java.lang.Thread.run(Thread.java:662)
> >
> > Thanks.
> > Deb
>

--001a11c304a4e065f104f6743da3--

From dev-return-7245-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 14:13:00 2014
Return-Path: <dev-return-7245-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 264A310ABA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 14:13:00 +0000 (UTC)
Received: (qmail 96401 invoked by uid 500); 7 Apr 2014 14:12:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95786 invoked by uid 500); 7 Apr 2014 14:12:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95757 invoked by uid 99); 7 Apr 2014 14:12:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 14:12:52 +0000
X-ASF-Spam-Status: No, hits=2.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.173 as permitted sender)
Received: from [209.85.192.173] (HELO mail-pd0-f173.google.com) (209.85.192.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 14:12:44 +0000
Received: by mail-pd0-f173.google.com with SMTP id z10so6593032pdj.18
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 07:12:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=2wdm6EML4diZzQPFOxRlOS6AtKKMMf4rceWvzsFW3dU=;
        b=wEHY85tJRkpNZt7Q3vgyci3auWkDNYxSx/fxyFrfdIOSAOnGpdZFsY2ky/b1wT5sT5
         lOmpM4mtNHSvBmtXgwRi4eNMOaRt0DGT24x7Yt+LwnFFPkO9sIAOjP0uld273zDkZERH
         uHDliGf+aS/b3y53Gr+K1n8k8rmrW5onY7zMSIWTtYJIiMa5p1mSIAnw5yA1cAjpFhPO
         MtBsGFUxuAQy5OEmiaBzssoaES+sO0VUhAyml0NPxAkAUicBhJ8nDzacaSk++cPyv+t3
         f6oBkSY9OGr5r+eaAZqN1O6KNREhgJgO3PbZfgS2sC9pYoPTPqOxl4V9gM0+hW1NbX3y
         ShAA==
MIME-Version: 1.0
X-Received: by 10.66.141.231 with SMTP id rr7mr30901320pab.41.1396879941646;
 Mon, 07 Apr 2014 07:12:21 -0700 (PDT)
Received: by 10.68.12.39 with HTTP; Mon, 7 Apr 2014 07:12:21 -0700 (PDT)
In-Reply-To: <CALD+6GOoDp3ymn25NQtgGVrTvizxSSSYg_Hi91dz1hPt-XUC5w@mail.gmail.com>
References: <CA+B-+fzYxLU6OUX6hufpsbAmJLoYbPQupMRK=MR8Yx6jkLOj2g@mail.gmail.com>
	<CAJgQjQ-nYkkMwDXkgiSKqtiE0Lk1kKRYeWHe-O=N=eHBHzgiBg@mail.gmail.com>
	<CALD+6GOoDp3ymn25NQtgGVrTvizxSSSYg_Hi91dz1hPt-XUC5w@mail.gmail.com>
Date: Mon, 7 Apr 2014 07:12:21 -0700
Message-ID: <CA+B-+fxkZk60OCXF_kvJmNHFudSKNwZqq5vfE9FMivztQ_5BtQ@mail.gmail.com>
Subject: Re: ALS array index out of bound with 50 factors
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11331298b7402704f6747495
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11331298b7402704f6747495
Content-Type: text/plain; charset=ISO-8859-1

Nick,

I already have this code which calls dictionary generation and then maps
string etc to ints...I think the core algorithm should stay in ints...if
you like I can add this code in MFUtils.scala....that's the convention I
followed similar to MLUtils.scala...actually these functions should be even
made part of MLUtils.scala...

Only thing is that the join should be an option which makes it application
dependent...sometimes people would like to do map side joins if their
dictionaries are small...in my case user dictionary has 25M rows and
product dictionary has 3M rows...so join optimization did not help...

Thanks.
Deb



On Mon, Apr 7, 2014 at 6:57 AM, Nick Pentreath <nick.pentreath@gmail.com>wrote:

> On the partitioning / id keys. If we would look at hash partitioning, how
> feasible will it be to just allow the user and item ids to be strings? A
> lot of the time these ids are strings anyway (UUIDs and so on), and it's
> really painful to translate between String <-> Int the whole time.
>
> Are there any obvious blockers to this? I am a bit rusty on the ALS code
> but from a quick scan I think this may work. Performance may be an issue
> with large String keys... Any majore issues/objections to this thinking?
>
> I may be able to find time to take a stab at this if there is demand.
>
>
> On Mon, Apr 7, 2014 at 6:08 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
> > Hi Deb,
> >
> > Are you using the master branch or a particular commit? Do you have
> > negative or out-of-integer-range user or product ids? There is an
> > issue with ALS' partitioning
> > (https://spark-project.atlassian.net/browse/SPARK-1281), but I'm not
> > sure whether that is the reason. Could you try to see whether you can
> > reproduce the error on a public data set, e.g., movielens? Thanks!
> >
> > Best,
> > Xiangrui
> >
> > On Sat, Apr 5, 2014 at 10:53 PM, Debasish Das <debasish.das83@gmail.com>
> > wrote:
> > > Hi,
> > >
> > > I deployed apache/spark master today and recently there were many ALS
> > > related checkins and enhancements..
> > >
> > > I am running ALS with explicit feedback and I remember most
> enhancements
> > > were related to implicit feedback...
> > >
> > > With 25 factors my runs were successful but with 50 factors I am
> getting
> > > array index out of bound...
> > >
> > > Note that I was hitting gc errors before with an older version of spark
> > but
> > > it seems like the sparse matrix partitioning scheme has changed
> > now...data
> > > caching looks much balanced now...earlier one node was becoming
> > > bottleneck...Although I ran with 64g memory per node...
> > >
> > > There are around 3M products, 25M users...
> > >
> > > Anyone noticed this bug or something similar ?
> > >
> > > 14/04/05 23:03:15 WARN TaskSetManager: Loss was due to
> > > java.lang.ArrayIndexOutOfBoundsException
> > > java.lang.ArrayIndexOutOfBoundsException: 81029
> > >     at
> > >
> >
> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateBlock$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(ALS.scala:450)
> > >     at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
> > >     at
> > >
> >
> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateBlock$1.apply$mcVI$sp(ALS.scala:446)
> > >     at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
> > >     at org.apache.spark.mllib.recommendation.ALS.org
> > > $apache$spark$mllib$recommendation$ALS$$updateBlock(ALS.scala:445)
> > >     at
> > >
> >
> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:416)
> > >     at
> > >
> >
> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:415)
> > >     at
> > >
> >
> org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31)
> > >     at
> > >
> >
> org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31)
> > >     at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
> > >     at
> > >
> >
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:149)
> > >     at
> > >
> >
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:147)
> > >     at
> > >
> >
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> > >     at
> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> > >     at
> org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:147)
> > >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
> > >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
> > >     at
> > > org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
> > >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
> > >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
> > >     at
> > >
> >
> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
> > >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
> > >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
> > >     at
> org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
> > >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
> > >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
> > >     at
> > >
> >
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)
> > >     at
> > >
> >
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
> > >     at org.apache.spark.scheduler.Task.run(Task.scala:52)
> > >     at
> > >
> >
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
> > >     at
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:43)
> > >     at
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
> > >     at java.security.AccessController.doPrivileged(Native Method)
> > >     at javax.security.auth.Subject.doAs(Subject.java:396)
> > >     at
> > >
> >
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
> > >     at
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:42)
> > >     at
> > org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
> > >     at
> > >
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
> > >     at
> > >
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
> > >     at java.lang.Thread.run(Thread.java:662)
> > >
> > > Thanks.
> > > Deb
> >
>

--001a11331298b7402704f6747495--

From dev-return-7246-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 14:42:55 2014
Return-Path: <dev-return-7246-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 26BF210BEF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 14:42:55 +0000 (UTC)
Received: (qmail 69861 invoked by uid 500); 7 Apr 2014 14:42:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69814 invoked by uid 500); 7 Apr 2014 14:42:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69806 invoked by uid 99); 7 Apr 2014 14:42:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 14:42:51 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [74.125.82.52] (HELO mail-wg0-f52.google.com) (74.125.82.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 14:42:45 +0000
Received: by mail-wg0-f52.google.com with SMTP id k14so6836435wgh.11
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 07:42:23 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=5wxrPaDPmxGd9KKTtyiMiJqnYCS9Q3iuX3qCqsHPJ9E=;
        b=L3UZQkGxxnO9wQUuHQkX8DEs0xb+yhvvvctsWIbcLVUUOi5QAQceHPMayIZtKyB2F+
         5WKoAhpjnFPHYR6iGhcSm0wmo61fWo27iJwScnXnskXLKs9bLWderJ5jYyfGyVhg4yoC
         B9B/pXIfeO1Ll72Lnh5vdkYY+5iFjh9hMulYzs8XU+6lWpzpmuRZyAl9XsyQF/Yn4fWA
         0CWF4rCN84sZggucWluzPafWp0t7PxlsCc4X3PELGPx3lf6ZaoKXnVmdUCjLfgMImi07
         n1hUmGKT7zsJ53hZcnNZMYY43tfiT3QJxqEB5kEy1G0LytBPAP8la32Omt8UioD1Z9Q2
         Shew==
X-Gm-Message-State: ALoCoQkPjyLrFX1wsfw2htkj0dQ6B/aQLukbcDvwBxz/mKlOLT5Ng8naB8MZYk+lMtLF8d74XBBu
MIME-Version: 1.0
X-Received: by 10.180.87.233 with SMTP id bb9mr25930209wib.10.1396881743804;
 Mon, 07 Apr 2014 07:42:23 -0700 (PDT)
Received: by 10.217.66.129 with HTTP; Mon, 7 Apr 2014 07:42:23 -0700 (PDT)
X-Originating-IP: [199.47.72.31]
Date: Mon, 7 Apr 2014 10:42:23 -0400
Message-ID: <CANx3uAhnRDq_NBcuUANiQB9ObuQ5TUbUgMEJsYvsysvwzhNSnQ@mail.gmail.com>
Subject: tachyon dependency
From: Koert Kuipers <koert@tresata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d044480af2216d504f674e008
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d044480af2216d504f674e008
Content-Type: text/plain; charset=ISO-8859-1

i noticed there is a dependency on tachyon in spark core 1.0.0-SNAPSHOT.
how does that work? i believe tachyon is written in java 7, yet spark
claims to be java 6 compatible.

--f46d044480af2216d504f674e008--

From dev-return-7247-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 15:00:00 2014
Return-Path: <dev-return-7247-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8767910CBB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 15:00:00 +0000 (UTC)
Received: (qmail 7973 invoked by uid 500); 7 Apr 2014 14:59:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7356 invoked by uid 500); 7 Apr 2014 14:59:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7343 invoked by uid 99); 7 Apr 2014 14:59:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 14:59:55 +0000
X-ASF-Spam-Status: No, hits=2.1 required=10.0
	tests=HK_RANDOM_ENVFROM,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of haoyuan.li@gmail.com designates 209.85.212.171 as permitted sender)
Received: from [209.85.212.171] (HELO mail-wi0-f171.google.com) (209.85.212.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 14:59:51 +0000
Received: by mail-wi0-f171.google.com with SMTP id q5so5248739wiv.16
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 07:59:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=UWl5WxnWaVB3es8P8tHLN75UMwUAV//01PRtMU3zHNk=;
        b=z3rlP+YpI+nuOoHNd/WHz1ubg7jLh6C5JQ5xcqO9GYjnfRt0fFA299I3uNLJ11iCQ2
         NNeIRpJuviK+fniqHuuKXbXWLfSpORDW41B0mDeI2PiZ6r/Uw6gqdXB3kmnw2k6LE9eE
         upoOzuVjJ5bNgz0ySF5ePF7+AEwqk/79bYMrzeKbiA/cCK8o1i0WDTuFDKVwlrZ2F5iZ
         WrDFOxzJqKpowCqcOYByG+lt+lJP6yc2/CEMaT9bl4nHC+ovl3k2osZjQcoiwAytjRQ3
         nSWGbcYfR3PnQntqDIlqKNQAkdJPn4yHxeDLIL1AY76rNaobtb4LZpX2v/F6sIsRPGLT
         4/Iw==
X-Received: by 10.194.242.231 with SMTP id wt7mr43072618wjc.52.1396882770519;
 Mon, 07 Apr 2014 07:59:30 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.216.157.66 with HTTP; Mon, 7 Apr 2014 07:59:10 -0700 (PDT)
In-Reply-To: <CANx3uAhnRDq_NBcuUANiQB9ObuQ5TUbUgMEJsYvsysvwzhNSnQ@mail.gmail.com>
References: <CANx3uAhnRDq_NBcuUANiQB9ObuQ5TUbUgMEJsYvsysvwzhNSnQ@mail.gmail.com>
From: Haoyuan Li <haoyuan.li@gmail.com>
Date: Mon, 7 Apr 2014 07:59:10 -0700
Message-ID: <CAG2iju3-hScya2eBHN8q3_-5P-5p9gm7Sj8xSscz_pR7VO+sjA@mail.gmail.com>
Subject: Re: tachyon dependency
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e013d1c68546ee604f6751d0c
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013d1c68546ee604f6751d0c
Content-Type: text/plain; charset=ISO-8859-1

Tachyon is Java 6 compatible from version 0.4. Beside putting input/output
data in Tachyon ( http://tachyon-project.org/Running-Spark-on-Tachyon.html ),
Spark applications can also persist data into Tachyon (
https://github.com/apache/spark/blob/master/docs/scala-programming-guide.md
).


On Mon, Apr 7, 2014 at 7:42 AM, Koert Kuipers <koert@tresata.com> wrote:

> i noticed there is a dependency on tachyon in spark core 1.0.0-SNAPSHOT.
> how does that work? i believe tachyon is written in java 7, yet spark
> claims to be java 6 compatible.
>



-- 
Haoyuan Li
Algorithms, Machines, People Lab, EECS, UC Berkeley
http://www.cs.berkeley.edu/~haoyuan/

--089e013d1c68546ee604f6751d0c--

From dev-return-7248-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 16:21:07 2014
Return-Path: <dev-return-7248-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8BA38100AD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 16:21:07 +0000 (UTC)
Received: (qmail 94949 invoked by uid 500); 7 Apr 2014 16:21:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94651 invoked by uid 500); 7 Apr 2014 16:21:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94642 invoked by uid 99); 7 Apr 2014 16:21:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 16:21:04 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mukgbv@gmail.com designates 74.125.82.51 as permitted sender)
Received: from [74.125.82.51] (HELO mail-wg0-f51.google.com) (74.125.82.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 16:20:58 +0000
Received: by mail-wg0-f51.google.com with SMTP id k14so7113811wgh.22
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 09:20:36 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=EVyIA2cvWF9d+PuZgwfQr48+XtUN2Drjw/kpgIqbfHM=;
        b=n4YB0Ca3+k/wiRwPffpN3TnH+1Hm+NL0V3La7YWtr8wGr45npmnumaRhyAR5xo/xTT
         RWbSP7NulOrX16bRu1uWszk7WBC7Rl1TsPyXPDVQHDstIFvioyTJvLTTSUjb9UXPBDdF
         aMged8a/AMEptWeBp1NTWRqCRwcKfiiCCfT6o/+ejbySgN/Ud1HpP0SxAXdv5DSUmjiF
         kJisu3o6yzkXtwH2PG0BZhQ4gIFPqGYaF/1m0G9NPNv2dqjLjECQ5jLQ3CGzplMnR1hL
         OGhHJo2tHfNYfSGuC0Iedg+hrA/ot0RZXiCL30BB8abdSauq41nQv4j7S+tDP2A1N8t2
         wTmg==
MIME-Version: 1.0
X-Received: by 10.194.92.177 with SMTP id cn17mr43280943wjb.18.1396887636821;
 Mon, 07 Apr 2014 09:20:36 -0700 (PDT)
Received: by 10.180.77.135 with HTTP; Mon, 7 Apr 2014 09:20:36 -0700 (PDT)
Date: Mon, 7 Apr 2014 21:50:36 +0530
Message-ID: <CAGRER0deRfjEdguW5tjJPFBSuiap2P9-q21WBZyNyVbL4onHfA@mail.gmail.com>
Subject: Contributing to Spark
From: Mukesh G <mukgbv@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7beb91aa624d6004f6763f19
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7beb91aa624d6004f6763f19
Content-Type: text/plain; charset=ISO-8859-1

Hi,

   How I contribute to Spark and it's associated projects?

Appreciate the help...

Thanks

Mukesh

--047d7beb91aa624d6004f6763f19--

From dev-return-7249-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 16:44:51 2014
Return-Path: <dev-return-7249-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DA0F110163
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 16:44:51 +0000 (UTC)
Received: (qmail 63754 invoked by uid 500); 7 Apr 2014 16:44:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63347 invoked by uid 500); 7 Apr 2014 16:44:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63335 invoked by uid 99); 7 Apr 2014 16:44:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 16:44:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of svarakhedi@gopivotal.com designates 209.85.223.179 as permitted sender)
Received: from [209.85.223.179] (HELO mail-ie0-f179.google.com) (209.85.223.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 16:44:38 +0000
Received: by mail-ie0-f179.google.com with SMTP id lx4so6236083iec.24
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 09:44:17 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=I2E/Wqmh4JlcIjKfLofMV37GrMZYfHVU//3bAoBssHE=;
        b=KEWHOdJM4FsykuFvj4m50aptmxcdP1iMyddnxzWmSdOzUQLdblKLf6HS0d7N8AW5a1
         DrbXo0WkmWgBPTVxbk05IqVteZdQ+uFu5IUj8pNaW4E9xWTBy6Ker9gAQMVMPP1bbWkq
         St7mGm469RjrSbgVUP533uwg7XW9mbIjpf1KOYDStRUWChSVJMcRsGEGfsnX8xR3BYf7
         d41Z5GTIoDgzWBZGgHq3TZmuyWcPV66mqEs+iWCIAoOazMkbY6tv6vRiT0XGU2auLlWB
         JMRtBjohnkgwOKjYxm+qN6p1WjK2vyDpzs45a34lckIxcI3+ujmnpCf5DqmnQ6hSodbl
         /rYQ==
X-Gm-Message-State: ALoCoQn8hHXJFTukC8shYzWfSmfSaK+HK2r+UsLHL0HiguDlQj51x87bsJiw2FWljZajmwUDD8sU
MIME-Version: 1.0
X-Received: by 10.42.114.82 with SMTP id f18mr2882477icq.56.1396889057508;
 Mon, 07 Apr 2014 09:44:17 -0700 (PDT)
Received: by 10.42.227.133 with HTTP; Mon, 7 Apr 2014 09:44:17 -0700 (PDT)
In-Reply-To: <CAGRER0deRfjEdguW5tjJPFBSuiap2P9-q21WBZyNyVbL4onHfA@mail.gmail.com>
References: <CAGRER0deRfjEdguW5tjJPFBSuiap2P9-q21WBZyNyVbL4onHfA@mail.gmail.com>
Date: Mon, 7 Apr 2014 09:44:17 -0700
Message-ID: <CAJHwgdshcGJLNjT118A79OWg6Gb_qaaNvoQZiw1WcQgQfinS3g@mail.gmail.com>
Subject: Re: Contributing to Spark
From: Sujeet Varakhedi <svarakhedi@gopivotal.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=20cf303f6dce11017004f6769450
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf303f6dce11017004f6769450
Content-Type: text/plain; charset=ISO-8859-1

This is a good place to start:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

Sujeet


On Mon, Apr 7, 2014 at 9:20 AM, Mukesh G <mukgbv@gmail.com> wrote:

> Hi,
>
>    How I contribute to Spark and it's associated projects?
>
> Appreciate the help...
>
> Thanks
>
> Mukesh
>

--20cf303f6dce11017004f6769450--

From dev-return-7250-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 16:54:10 2014
Return-Path: <dev-return-7250-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C9D4E101F5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 16:54:10 +0000 (UTC)
Received: (qmail 96038 invoked by uid 500); 7 Apr 2014 16:54:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95964 invoked by uid 500); 7 Apr 2014 16:54:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95928 invoked by uid 99); 7 Apr 2014 16:54:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 16:54:04 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.212.173 as permitted sender)
Received: from [209.85.212.173] (HELO mail-wi0-f173.google.com) (209.85.212.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 16:54:00 +0000
Received: by mail-wi0-f173.google.com with SMTP id z2so5475265wiv.12
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 09:53:38 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=3FppKVfhDCzKZntrv3HUFV3jfAKiJzVdUygJQRTnobs=;
        b=XA80I02Veg74wYTajEcqLZUdVKmGAUrmrVt74d05WucXrbf56mcs30CLln43DjYOL8
         l1/5Kjgnfv1l6X7vTPjkhwhnoSIxnbNVl5MI6h4fd+hJfmhWqJp63T59FwWtPxeMPw82
         JoWZvhUAtJzNtOy018hzbSG0SV4AhNY01rLk5QASNy02CRP8kz/N55G1Ox0nuAUuB6if
         Q+uMoDBK0D3iHcSk5FHTiytO6e+ssBPCMZn5FzRw03KillRmXcUxfpDVKnmhypG541tQ
         gqZMCkmZtVIuVpI+anuZgbMtxJOH2248yftcxXzAW9JKSX7bS3XbHPRiqpCI3MCSy1Gz
         yGzQ==
MIME-Version: 1.0
X-Received: by 10.180.97.72 with SMTP id dy8mr26169548wib.5.1396889618239;
 Mon, 07 Apr 2014 09:53:38 -0700 (PDT)
Received: by 10.194.82.103 with HTTP; Mon, 7 Apr 2014 09:53:38 -0700 (PDT)
In-Reply-To: <CA+B-+fxibq0ZMCoN27OnTbfrtVhz+ShPSVFtabUi63nUftBcag@mail.gmail.com>
References: <CA+B-+fzYxLU6OUX6hufpsbAmJLoYbPQupMRK=MR8Yx6jkLOj2g@mail.gmail.com>
	<CAJgQjQ-nYkkMwDXkgiSKqtiE0Lk1kKRYeWHe-O=N=eHBHzgiBg@mail.gmail.com>
	<CA+B-+fxibq0ZMCoN27OnTbfrtVhz+ShPSVFtabUi63nUftBcag@mail.gmail.com>
Date: Mon, 7 Apr 2014 09:53:38 -0700
Message-ID: <CAJgQjQ8891QwVXmmVqhYZepV2kZP2w=U_R36im0R8hBnXA5QsQ@mail.gmail.com>
Subject: Re: ALS array index out of bound with 50 factors
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Deb,

This thread is for the out-of-bound error you described. I don't think
the number of iterations has any effect here. My questions were:

1) Are you using the master branch or a particular commit?

2) Do you have negative or out-of-integer-range user or product ids?
Try to print out the max/min value of user/product ids.

Best,
Xiangrui

On Sun, Apr 6, 2014 at 11:01 PM, Debasish Das <debasish.das83@gmail.com> wrote:
> Hi Xiangrui,
>
> With 4 ALS iterations it runs fine...If I run 10 I am failing...I believe I
> have to cut the lineage chain and call checkpoint....Trying to follow the
> other email chain on checkpointing...
>
> Thanks.
> Deb
>
>
> On Sun, Apr 6, 2014 at 9:08 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
>> Hi Deb,
>>
>> Are you using the master branch or a particular commit? Do you have
>> negative or out-of-integer-range user or product ids? There is an
>> issue with ALS' partitioning
>> (https://spark-project.atlassian.net/browse/SPARK-1281), but I'm not
>> sure whether that is the reason. Could you try to see whether you can
>> reproduce the error on a public data set, e.g., movielens? Thanks!
>>
>> Best,
>> Xiangrui
>>
>> On Sat, Apr 5, 2014 at 10:53 PM, Debasish Das <debasish.das83@gmail.com>
>> wrote:
>> > Hi,
>> >
>> > I deployed apache/spark master today and recently there were many ALS
>> > related checkins and enhancements..
>> >
>> > I am running ALS with explicit feedback and I remember most enhancements
>> > were related to implicit feedback...
>> >
>> > With 25 factors my runs were successful but with 50 factors I am getting
>> > array index out of bound...
>> >
>> > Note that I was hitting gc errors before with an older version of spark
>> but
>> > it seems like the sparse matrix partitioning scheme has changed
>> now...data
>> > caching looks much balanced now...earlier one node was becoming
>> > bottleneck...Although I ran with 64g memory per node...
>> >
>> > There are around 3M products, 25M users...
>> >
>> > Anyone noticed this bug or something similar ?
>> >
>> > 14/04/05 23:03:15 WARN TaskSetManager: Loss was due to
>> > java.lang.ArrayIndexOutOfBoundsException
>> > java.lang.ArrayIndexOutOfBoundsException: 81029
>> >     at
>> >
>> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateBlock$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(ALS.scala:450)
>> >     at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
>> >     at
>> >
>> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateBlock$1.apply$mcVI$sp(ALS.scala:446)
>> >     at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
>> >     at org.apache.spark.mllib.recommendation.ALS.org
>> > $apache$spark$mllib$recommendation$ALS$$updateBlock(ALS.scala:445)
>> >     at
>> >
>> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:416)
>> >     at
>> >
>> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:415)
>> >     at
>> >
>> org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31)
>> >     at
>> >
>> org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31)
>> >     at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>> >     at
>> >
>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:149)
>> >     at
>> >
>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:147)
>> >     at
>> >
>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>> >     at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>> >     at org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:147)
>> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
>> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
>> >     at
>> > org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
>> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
>> >     at
>> >
>> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
>> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
>> >     at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
>> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
>> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
>> >     at
>> >
>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)
>> >     at
>> >
>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
>> >     at org.apache.spark.scheduler.Task.run(Task.scala:52)
>> >     at
>> >
>> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
>> >     at
>> >
>> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:43)
>> >     at
>> >
>> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
>> >     at java.security.AccessController.doPrivileged(Native Method)
>> >     at javax.security.auth.Subject.doAs(Subject.java:396)
>> >     at
>> >
>> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
>> >     at
>> >
>> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:42)
>> >     at
>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
>> >     at
>> >
>> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>> >     at
>> >
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>> >     at java.lang.Thread.run(Thread.java:662)
>> >
>> > Thanks.
>> > Deb
>>

From dev-return-7251-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 17:37:05 2014
Return-Path: <dev-return-7251-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9747E1044F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 17:37:05 +0000 (UTC)
Received: (qmail 86586 invoked by uid 500); 7 Apr 2014 17:37:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86547 invoked by uid 500); 7 Apr 2014 17:37:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86537 invoked by uid 99); 7 Apr 2014 17:37:02 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 17:37:02 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.160.45 as permitted sender)
Received: from [209.85.160.45] (HELO mail-pb0-f45.google.com) (209.85.160.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 17:36:58 +0000
Received: by mail-pb0-f45.google.com with SMTP id uo5so7073118pbc.32
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 10:36:37 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=45qJHuZLSz9l6zvWAof91SpPrQ+6nVP+8cTMIegKK7I=;
        b=s4Km3iY7SvrPUQPtvXe8bfCMIeofr4zMZ+zhscoWK9EWiFaIJT5g08MQ+saCKXT/fQ
         IPWtilG4wYWyJthADB6o7Jt8CxeEvZ32PxLqauDyBV14+RBO3Ijq9P5VP4IgXYTBlAKf
         3PKLxeKUJyUSS/ul0fbdLvAtcXdhkzcjHDiq3WDuh90f8iGwfO+TGNYgnziiC38FJMUn
         pFmuR+nEXeM4YJ96nBj4XJtQC3YznDFW6YfkOUYGVdwbn0tyumf/i0hvn9RAqfYDAaZm
         WJuNcdiUFg32qtp+zmqLbiH0zE+o3Ok9IBd3dFZN+VmMhUPTP0O6/IA1aIluluTbf+ff
         wdBw==
MIME-Version: 1.0
X-Received: by 10.68.245.162 with SMTP id xp2mr32313024pbc.69.1396892197552;
 Mon, 07 Apr 2014 10:36:37 -0700 (PDT)
Received: by 10.68.12.39 with HTTP; Mon, 7 Apr 2014 10:36:37 -0700 (PDT)
Received: by 10.68.12.39 with HTTP; Mon, 7 Apr 2014 10:36:37 -0700 (PDT)
In-Reply-To: <CAJgQjQ8891QwVXmmVqhYZepV2kZP2w=U_R36im0R8hBnXA5QsQ@mail.gmail.com>
References: <CA+B-+fzYxLU6OUX6hufpsbAmJLoYbPQupMRK=MR8Yx6jkLOj2g@mail.gmail.com>
	<CAJgQjQ-nYkkMwDXkgiSKqtiE0Lk1kKRYeWHe-O=N=eHBHzgiBg@mail.gmail.com>
	<CA+B-+fxibq0ZMCoN27OnTbfrtVhz+ShPSVFtabUi63nUftBcag@mail.gmail.com>
	<CAJgQjQ8891QwVXmmVqhYZepV2kZP2w=U_R36im0R8hBnXA5QsQ@mail.gmail.com>
Date: Mon, 7 Apr 2014 10:36:37 -0700
Message-ID: <CA+B-+fyLP_2cQzEbHvW=FrbguOe-OL8tY-f3i=VOxtpDJCJCzQ@mail.gmail.com>
Subject: Re: ALS array index out of bound with 50 factors
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b1637ed39883b04f6774f3c
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b1637ed39883b04f6774f3c
Content-Type: text/plain; charset=ISO-8859-1

I am using master...

No negative indexes...

If I run with 4 iterations it runs fine and I can generate factors...

With 10 iterations run fails with array index out of bound...

25m users and 3m products are within int limits....

Does it help if I can point the logs for both the runs to you ?

I will debug it further today...
 On Apr 7, 2014 9:54 AM, "Xiangrui Meng" <mengxr@gmail.com> wrote:

> Hi Deb,
>
> This thread is for the out-of-bound error you described. I don't think
> the number of iterations has any effect here. My questions were:
>
> 1) Are you using the master branch or a particular commit?
>
> 2) Do you have negative or out-of-integer-range user or product ids?
> Try to print out the max/min value of user/product ids.
>
> Best,
> Xiangrui
>
> On Sun, Apr 6, 2014 at 11:01 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
> > Hi Xiangrui,
> >
> > With 4 ALS iterations it runs fine...If I run 10 I am failing...I
> believe I
> > have to cut the lineage chain and call checkpoint....Trying to follow the
> > other email chain on checkpointing...
> >
> > Thanks.
> > Deb
> >
> >
> > On Sun, Apr 6, 2014 at 9:08 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
> >
> >> Hi Deb,
> >>
> >> Are you using the master branch or a particular commit? Do you have
> >> negative or out-of-integer-range user or product ids? There is an
> >> issue with ALS' partitioning
> >> (https://spark-project.atlassian.net/browse/SPARK-1281), but I'm not
> >> sure whether that is the reason. Could you try to see whether you can
> >> reproduce the error on a public data set, e.g., movielens? Thanks!
> >>
> >> Best,
> >> Xiangrui
> >>
> >> On Sat, Apr 5, 2014 at 10:53 PM, Debasish Das <debasish.das83@gmail.com
> >
> >> wrote:
> >> > Hi,
> >> >
> >> > I deployed apache/spark master today and recently there were many ALS
> >> > related checkins and enhancements..
> >> >
> >> > I am running ALS with explicit feedback and I remember most
> enhancements
> >> > were related to implicit feedback...
> >> >
> >> > With 25 factors my runs were successful but with 50 factors I am
> getting
> >> > array index out of bound...
> >> >
> >> > Note that I was hitting gc errors before with an older version of
> spark
> >> but
> >> > it seems like the sparse matrix partitioning scheme has changed
> >> now...data
> >> > caching looks much balanced now...earlier one node was becoming
> >> > bottleneck...Although I ran with 64g memory per node...
> >> >
> >> > There are around 3M products, 25M users...
> >> >
> >> > Anyone noticed this bug or something similar ?
> >> >
> >> > 14/04/05 23:03:15 WARN TaskSetManager: Loss was due to
> >> > java.lang.ArrayIndexOutOfBoundsException
> >> > java.lang.ArrayIndexOutOfBoundsException: 81029
> >> >     at
> >> >
> >>
> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateBlock$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(ALS.scala:450)
> >> >     at
> scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
> >> >     at
> >> >
> >>
> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateBlock$1.apply$mcVI$sp(ALS.scala:446)
> >> >     at
> scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
> >> >     at org.apache.spark.mllib.recommendation.ALS.org
> >> > $apache$spark$mllib$recommendation$ALS$$updateBlock(ALS.scala:445)
> >> >     at
> >> >
> >>
> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:416)
> >> >     at
> >> >
> >>
> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:415)
> >> >     at
> >> >
> >>
> org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31)
> >> >     at
> >> >
> >>
> org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31)
> >> >     at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
> >> >     at
> >> >
> >>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:149)
> >> >     at
> >> >
> >>
> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:147)
> >> >     at
> >> >
> >>
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> >> >     at
> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> >> >     at
> org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:147)
> >> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
> >> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
> >> >     at
> >> > org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
> >> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
> >> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
> >> >     at
> >> >
> >>
> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
> >> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
> >> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
> >> >     at
> org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
> >> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
> >> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
> >> >     at
> >> >
> >>
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)
> >> >     at
> >> >
> >>
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
> >> >     at org.apache.spark.scheduler.Task.run(Task.scala:52)
> >> >     at
> >> >
> >>
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
> >> >     at
> >> >
> >>
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:43)
> >> >     at
> >> >
> >>
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
> >> >     at java.security.AccessController.doPrivileged(Native Method)
> >> >     at javax.security.auth.Subject.doAs(Subject.java:396)
> >> >     at
> >> >
> >>
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
> >> >     at
> >> >
> >>
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:42)
> >> >     at
> >> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
> >> >     at
> >> >
> >>
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
> >> >     at
> >> >
> >>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
> >> >     at java.lang.Thread.run(Thread.java:662)
> >> >
> >> > Thanks.
> >> > Deb
> >>
>

--047d7b1637ed39883b04f6774f3c--

From dev-return-7252-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 17:52:57 2014
Return-Path: <dev-return-7252-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A2640106AD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 17:52:57 +0000 (UTC)
Received: (qmail 24033 invoked by uid 500); 7 Apr 2014 17:52:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23510 invoked by uid 500); 7 Apr 2014 17:52:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23500 invoked by uid 99); 7 Apr 2014 17:52:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 17:52:53 +0000
X-ASF-Spam-Status: No, hits=0.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_SOFTFAIL
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of keo@eecs.berkeley.edu does not designate 169.229.218.145 as permitted sender)
Received: from [169.229.218.145] (HELO cm04fe.IST.Berkeley.EDU) (169.229.218.145)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 17:52:48 +0000
Received: from mail-yk0-f178.google.com ([209.85.160.178])
	by cm04fe.ist.berkeley.edu with esmtpsa (TLSv1:RC4-SHA:128)
	(Exim 4.76)
	(auth plain:keo@eecs.berkeley.edu)
	(envelope-from <keo@eecs.berkeley.edu>)
	id 1WXDiW-0006s1-Fj
	for dev@spark.apache.org; Mon, 07 Apr 2014 10:52:24 -0700
Received: by mail-yk0-f178.google.com with SMTP id 79so5829546ykr.37
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 10:52:18 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.236.135.197 with SMTP id u45mr4477574yhi.150.1396893138924;
 Mon, 07 Apr 2014 10:52:18 -0700 (PDT)
Received: by 10.170.51.18 with HTTP; Mon, 7 Apr 2014 10:52:18 -0700 (PDT)
Date: Mon, 7 Apr 2014 10:52:18 -0700
Message-ID: <CAKJXNjHeL2fh9f-D68+f6=XuJMb-iFcR8gcDw1zzgc5xM1AcAg@mail.gmail.com>
Subject: Flaky streaming tests
From: Kay Ousterhout <keo@eecs.berkeley.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=20cf3011db9d55bf3004f67787a7
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf3011db9d55bf3004f67787a7
Content-Type: text/plain; charset=ISO-8859-1

Hi all,

The InputStreamsSuite seems to have some serious flakiness issues -- I've
seen the file input stream fail many times and now I'm seeing some actor
input stream test failures (
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13846/consoleFull)
on what I think is an unrelated change.  Does anyone know anything about
these?  Should we just remove some of these tests since they seem to be
constantly failing?

-Kay

--20cf3011db9d55bf3004f67787a7--

From dev-return-7253-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 18:29:25 2014
Return-Path: <dev-return-7253-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6567E107F7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 18:29:25 +0000 (UTC)
Received: (qmail 14263 invoked by uid 500); 7 Apr 2014 18:29:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13428 invoked by uid 500); 7 Apr 2014 18:29:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13299 invoked by uid 99); 7 Apr 2014 18:29:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 18:29:20 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 74.125.82.48 as permitted sender)
Received: from [74.125.82.48] (HELO mail-wg0-f48.google.com) (74.125.82.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 18:29:15 +0000
Received: by mail-wg0-f48.google.com with SMTP id l18so7289686wgh.19
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 11:28:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=ZWo6WDAcL488Utn7Nwz1if3HWzZNtyiMJhw45WbFAl0=;
        b=FnzNSxcMoV7lBRitGyUB3Jw1CvHI1pT6MLD30vwXWp6iYVIzjHYYbKcBjt6z9z4kWG
         NPl8Q1VLtYj1q6ENjwp7gs0O8k6EQOKXwjnK20IAuGRBg3QiIYoGsp83TTKq1FWC07MF
         XzjLwbDIBRq1g9lqRQPUAPyp8cZmqYnKGdlVp4yudgSOh/CK5JA/BzS24hIWlXfl+hXu
         PUJUSQk6Aq9R4ovHOtprSkpvPiech4eIaaLIfN+qn3rx1gTs079Ui8g1geJtQl1qsuZa
         YWJmZj7q2occFwNVntoFmYfmHhVAk67kd+Vm94oZGeQAgBfec0GmxET4n3RqA6hDvU+s
         K60A==
MIME-Version: 1.0
X-Received: by 10.180.72.239 with SMTP id g15mr27145967wiv.45.1396895333828;
 Mon, 07 Apr 2014 11:28:53 -0700 (PDT)
Received: by 10.194.82.103 with HTTP; Mon, 7 Apr 2014 11:28:53 -0700 (PDT)
In-Reply-To: <CA+B-+fyLP_2cQzEbHvW=FrbguOe-OL8tY-f3i=VOxtpDJCJCzQ@mail.gmail.com>
References: <CA+B-+fzYxLU6OUX6hufpsbAmJLoYbPQupMRK=MR8Yx6jkLOj2g@mail.gmail.com>
	<CAJgQjQ-nYkkMwDXkgiSKqtiE0Lk1kKRYeWHe-O=N=eHBHzgiBg@mail.gmail.com>
	<CA+B-+fxibq0ZMCoN27OnTbfrtVhz+ShPSVFtabUi63nUftBcag@mail.gmail.com>
	<CAJgQjQ8891QwVXmmVqhYZepV2kZP2w=U_R36im0R8hBnXA5QsQ@mail.gmail.com>
	<CA+B-+fyLP_2cQzEbHvW=FrbguOe-OL8tY-f3i=VOxtpDJCJCzQ@mail.gmail.com>
Date: Mon, 7 Apr 2014 11:28:53 -0700
Message-ID: <CAJgQjQ9jM0+UcQf33=yaEFjJjGE__6o9vr5kmYDiW5eFTwSa7w@mail.gmail.com>
Subject: Re: ALS array index out of bound with 50 factors
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Deb,

It would be helpful if you can attached the logs. It is strange to see
that you can make 4 iterations but not 10.

Xiangrui

On Mon, Apr 7, 2014 at 10:36 AM, Debasish Das <debasish.das83@gmail.com> wrote:
> I am using master...
>
> No negative indexes...
>
> If I run with 4 iterations it runs fine and I can generate factors...
>
> With 10 iterations run fails with array index out of bound...
>
> 25m users and 3m products are within int limits....
>
> Does it help if I can point the logs for both the runs to you ?
>
> I will debug it further today...
>  On Apr 7, 2014 9:54 AM, "Xiangrui Meng" <mengxr@gmail.com> wrote:
>
>> Hi Deb,
>>
>> This thread is for the out-of-bound error you described. I don't think
>> the number of iterations has any effect here. My questions were:
>>
>> 1) Are you using the master branch or a particular commit?
>>
>> 2) Do you have negative or out-of-integer-range user or product ids?
>> Try to print out the max/min value of user/product ids.
>>
>> Best,
>> Xiangrui
>>
>> On Sun, Apr 6, 2014 at 11:01 PM, Debasish Das <debasish.das83@gmail.com>
>> wrote:
>> > Hi Xiangrui,
>> >
>> > With 4 ALS iterations it runs fine...If I run 10 I am failing...I
>> believe I
>> > have to cut the lineage chain and call checkpoint....Trying to follow the
>> > other email chain on checkpointing...
>> >
>> > Thanks.
>> > Deb
>> >
>> >
>> > On Sun, Apr 6, 2014 at 9:08 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>> >
>> >> Hi Deb,
>> >>
>> >> Are you using the master branch or a particular commit? Do you have
>> >> negative or out-of-integer-range user or product ids? There is an
>> >> issue with ALS' partitioning
>> >> (https://spark-project.atlassian.net/browse/SPARK-1281), but I'm not
>> >> sure whether that is the reason. Could you try to see whether you can
>> >> reproduce the error on a public data set, e.g., movielens? Thanks!
>> >>
>> >> Best,
>> >> Xiangrui
>> >>
>> >> On Sat, Apr 5, 2014 at 10:53 PM, Debasish Das <debasish.das83@gmail.com
>> >
>> >> wrote:
>> >> > Hi,
>> >> >
>> >> > I deployed apache/spark master today and recently there were many ALS
>> >> > related checkins and enhancements..
>> >> >
>> >> > I am running ALS with explicit feedback and I remember most
>> enhancements
>> >> > were related to implicit feedback...
>> >> >
>> >> > With 25 factors my runs were successful but with 50 factors I am
>> getting
>> >> > array index out of bound...
>> >> >
>> >> > Note that I was hitting gc errors before with an older version of
>> spark
>> >> but
>> >> > it seems like the sparse matrix partitioning scheme has changed
>> >> now...data
>> >> > caching looks much balanced now...earlier one node was becoming
>> >> > bottleneck...Although I ran with 64g memory per node...
>> >> >
>> >> > There are around 3M products, 25M users...
>> >> >
>> >> > Anyone noticed this bug or something similar ?
>> >> >
>> >> > 14/04/05 23:03:15 WARN TaskSetManager: Loss was due to
>> >> > java.lang.ArrayIndexOutOfBoundsException
>> >> > java.lang.ArrayIndexOutOfBoundsException: 81029
>> >> >     at
>> >> >
>> >>
>> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateBlock$1$$anonfun$apply$mcVI$sp$1.apply$mcVI$sp(ALS.scala:450)
>> >> >     at
>> scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
>> >> >     at
>> >> >
>> >>
>> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateBlock$1.apply$mcVI$sp(ALS.scala:446)
>> >> >     at
>> scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
>> >> >     at org.apache.spark.mllib.recommendation.ALS.org
>> >> > $apache$spark$mllib$recommendation$ALS$$updateBlock(ALS.scala:445)
>> >> >     at
>> >> >
>> >>
>> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:416)
>> >> >     at
>> >> >
>> >>
>> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$updateFeatures$2.apply(ALS.scala:415)
>> >> >     at
>> >> >
>> >>
>> org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31)
>> >> >     at
>> >> >
>> >>
>> org.apache.spark.rdd.MappedValuesRDD$$anonfun$compute$1.apply(MappedValuesRDD.scala:31)
>> >> >     at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>> >> >     at
>> >> >
>> >>
>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:149)
>> >> >     at
>> >> >
>> >>
>> org.apache.spark.rdd.CoGroupedRDD$$anonfun$compute$4.apply(CoGroupedRDD.scala:147)
>> >> >     at
>> >> >
>> >>
>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>> >> >     at
>> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>> >> >     at
>> org.apache.spark.rdd.CoGroupedRDD.compute(CoGroupedRDD.scala:147)
>> >> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
>> >> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
>> >> >     at
>> >> > org.apache.spark.rdd.MappedValuesRDD.compute(MappedValuesRDD.scala:31)
>> >> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
>> >> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
>> >> >     at
>> >> >
>> >>
>> org.apache.spark.rdd.FlatMappedValuesRDD.compute(FlatMappedValuesRDD.scala:31)
>> >> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
>> >> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
>> >> >     at
>> org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
>> >> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:229)
>> >> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:220)
>> >> >     at
>> >> >
>> >>
>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:161)
>> >> >     at
>> >> >
>> >>
>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:102)
>> >> >     at org.apache.spark.scheduler.Task.run(Task.scala:52)
>> >> >     at
>> >> >
>> >>
>> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
>> >> >     at
>> >> >
>> >>
>> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:43)
>> >> >     at
>> >> >
>> >>
>> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
>> >> >     at java.security.AccessController.doPrivileged(Native Method)
>> >> >     at javax.security.auth.Subject.doAs(Subject.java:396)
>> >> >     at
>> >> >
>> >>
>> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
>> >> >     at
>> >> >
>> >>
>> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:42)
>> >> >     at
>> >> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
>> >> >     at
>> >> >
>> >>
>> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>> >> >     at
>> >> >
>> >>
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>> >> >     at java.lang.Thread.run(Thread.java:662)
>> >> >
>> >> > Thanks.
>> >> > Deb
>> >>
>>

From dev-return-7254-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 18:31:37 2014
Return-Path: <dev-return-7254-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CBFBB1080B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 18:31:37 +0000 (UTC)
Received: (qmail 18699 invoked by uid 500); 7 Apr 2014 18:31:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18458 invoked by uid 500); 7 Apr 2014 18:31:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18309 invoked by uid 99); 7 Apr 2014 18:31:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 18:31:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.216.179 as permitted sender)
Received: from [209.85.216.179] (HELO mail-qc0-f179.google.com) (209.85.216.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 18:31:11 +0000
Received: by mail-qc0-f179.google.com with SMTP id m20so6784604qcx.24
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 11:30:51 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=MEj7xt2wShBi4v5bRH0e4bP+Yc1DUftDcoq/oJ9BP8I=;
        b=XG+5+FO1ON5xTU4FioEMcaDQrR0U0Ay4uUYqo3u0cYGAhRgNCsPR2C/1OepxURUyAf
         0OEgW3LZqob7xJPx0BxwpsTpw1H/pXqiLS1XiLmjenlhQ8cPhdXAwo8tDTlbtxJwd8/C
         Cwjp6JrT/eFq+4p4mPOnTq/c7hTDx0+OyyWNGoJEGlu3Tej+Bt8vXNy9A6ovv5eKAYrl
         5EXirm8pNFHDh/8BZ0GoxOqArkX7ym8BHB+DHW+sPUs2Y4Lz7AZEe4hRgs0rUntWkKEd
         EApwpi/h9pMFuXwFfTK2KptQUDOOajYvrI8yxeSWlPKV5mXs+ukSwF2hVE/UYTwBGeAW
         /Tng==
MIME-Version: 1.0
X-Received: by 10.140.48.77 with SMTP id n71mr10284635qga.90.1396895450829;
 Mon, 07 Apr 2014 11:30:50 -0700 (PDT)
Received: by 10.140.100.198 with HTTP; Mon, 7 Apr 2014 11:30:50 -0700 (PDT)
In-Reply-To: <CAKJXNjHeL2fh9f-D68+f6=XuJMb-iFcR8gcDw1zzgc5xM1AcAg@mail.gmail.com>
References: <CAKJXNjHeL2fh9f-D68+f6=XuJMb-iFcR8gcDw1zzgc5xM1AcAg@mail.gmail.com>
Date: Mon, 7 Apr 2014 14:30:50 -0400
Message-ID: <CAMtqZefqCJET6T3bLg+N4gEy1ux=+1n-xj7iyx7rmK6X4T94tw@mail.gmail.com>
Subject: Re: Flaky streaming tests
From: Nan Zhu <zhunanmcgill@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11352330229d2c04f67811d4
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11352330229d2c04f67811d4
Content-Type: text/plain; charset=ISO-8859-1

I met this issue when Jenkins seems to be very busy....

On Monday, April 7, 2014, Kay Ousterhout <keo@eecs.berkeley.edu> wrote:

> Hi all,
>
> The InputStreamsSuite seems to have some serious flakiness issues -- I've
> seen the file input stream fail many times and now I'm seeing some actor
> input stream test failures (
>
> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13846/consoleFull
> )
> on what I think is an unrelated change.  Does anyone know anything about
> these?  Should we just remove some of these tests since they seem to be
> constantly failing?
>
> -Kay
>

--001a11352330229d2c04f67811d4--

From dev-return-7255-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 18:32:34 2014
Return-Path: <dev-return-7255-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B712610817
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 18:32:34 +0000 (UTC)
Received: (qmail 23253 invoked by uid 500); 7 Apr 2014 18:32:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23193 invoked by uid 500); 7 Apr 2014 18:32:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23185 invoked by uid 99); 7 Apr 2014 18:32:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 18:32:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.173 as permitted sender)
Received: from [209.85.214.173] (HELO mail-ob0-f173.google.com) (209.85.214.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 18:32:24 +0000
Received: by mail-ob0-f173.google.com with SMTP id gq1so7081056obb.32
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 11:32:02 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=2ZBWuuImoulhZxbbdk0TOtOdVbHBhZzWNz3rRR1fRH4=;
        b=a8tMSycQSp5mZxt75EXUi9zoCfWmVoWd//QB/kmi1cbXUm/nKEsQLzzPMWD+V24xyx
         +qZsze+nvJgP/YQBrhRYxOT3HtlXpLIQjYolcb1BgW9xR30f12h2ie2Lg/M0pOsiExWB
         q6+sQpTJKQnEHYUMNqmRDdS/9O9Oz2FHulfFGgpOOMIssPwGuns5BjxHfCIwJlVvudqN
         l6Q6YrwIGNBflRxTQgunL7QOfT9PK8/v6ETO5Of3CsNbxt4MukJxSjdpmQ3JNLOat/Vt
         +leZPACeeK6QLtn/ZEWqJjFPTj/4/CTWUKYWW9KrymDKYavgur5NQzpnT8lFqlVIvVM2
         H5Bw==
MIME-Version: 1.0
X-Received: by 10.182.213.194 with SMTP id nu2mr1900806obc.82.1396895522454;
 Mon, 07 Apr 2014 11:32:02 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Mon, 7 Apr 2014 11:32:02 -0700 (PDT)
In-Reply-To: <CAKJXNjHeL2fh9f-D68+f6=XuJMb-iFcR8gcDw1zzgc5xM1AcAg@mail.gmail.com>
References: <CAKJXNjHeL2fh9f-D68+f6=XuJMb-iFcR8gcDw1zzgc5xM1AcAg@mail.gmail.com>
Date: Mon, 7 Apr 2014 11:32:02 -0700
Message-ID: <CABPQxsuVyF64PUfLbieurd6xHjzM6kFXNjHYBqk35jVuRy4UcQ@mail.gmail.com>
Subject: Re: Flaky streaming tests
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2e15c67816804f678155b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2e15c67816804f678155b
Content-Type: text/plain; charset=ISO-8859-1

TD - do you know what is going on here?

I looked into this ab it and at least a few of these that use
Thread.sleep() and assume the sleep will be exact, which is wrong. We
should disable all the tests that do and probably they should be re-written
to virtualize time.

- Patrick


On Mon, Apr 7, 2014 at 10:52 AM, Kay Ousterhout <keo@eecs.berkeley.edu>wrote:

> Hi all,
>
> The InputStreamsSuite seems to have some serious flakiness issues -- I've
> seen the file input stream fail many times and now I'm seeing some actor
> input stream test failures (
>
> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13846/consoleFull
> )
> on what I think is an unrelated change.  Does anyone know anything about
> these?  Should we just remove some of these tests since they seem to be
> constantly failing?
>
> -Kay
>

--001a11c2e15c67816804f678155b--

From dev-return-7256-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 18:34:14 2014
Return-Path: <dev-return-7256-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7F7B41083D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 18:34:14 +0000 (UTC)
Received: (qmail 30708 invoked by uid 500); 7 Apr 2014 18:34:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30669 invoked by uid 500); 7 Apr 2014 18:34:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30655 invoked by uid 99); 7 Apr 2014 18:34:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 18:34:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.51] (HELO mail-qa0-f51.google.com) (209.85.216.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 18:34:06 +0000
Received: by mail-qa0-f51.google.com with SMTP id j7so6210728qaq.24
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 11:33:45 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=cD854Paz8FcDgRBYhU4Z6qaIgR7vW6R/nfyxycthADc=;
        b=AfzKy5wdPQ+eA/TjY+PHs6JlQAJAnOcXWxMea+VdHzckXAGE9D+1d9xdsT+TR9FHoE
         RXN8TuZf0mgBkt58em+9k4KHpOe4gaWHAxIwCkvceZQsy7oYhULm9pIb7ztk1HR3wLdn
         YBVkYFnf29kIEXhowWhHzjWaABdwWgl7n7Z67I/N7OQ8yJjC+6H2o27Y3+Oo1Pc+VCN+
         DYsqZNFn4c28hNFIsUUbHxwoCE4nLt5hz87uhcUTz2ZHJZOiOwtxxp4PqyLa8C3WX2lM
         tFZzgQ3LLneFVNMRZ1P6fEAvCrTP73VnE9DxtiExSOhskFTZ+bDG5+GfYoRjnEHe/oy+
         ITKQ==
X-Gm-Message-State: ALoCoQmUCHDYpzQI3/c3h9zSctrYUdBfIU6oc3gEzjxYNdsZC+rudadHNPm6Nh/zHnA8nCIeo4mv
X-Received: by 10.224.163.10 with SMTP id y10mr4755747qax.102.1396895625528;
 Mon, 07 Apr 2014 11:33:45 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.125.5 with HTTP; Mon, 7 Apr 2014 11:33:25 -0700 (PDT)
In-Reply-To: <CABPQxsuVyF64PUfLbieurd6xHjzM6kFXNjHYBqk35jVuRy4UcQ@mail.gmail.com>
References: <CAKJXNjHeL2fh9f-D68+f6=XuJMb-iFcR8gcDw1zzgc5xM1AcAg@mail.gmail.com>
 <CABPQxsuVyF64PUfLbieurd6xHjzM6kFXNjHYBqk35jVuRy4UcQ@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Mon, 7 Apr 2014 11:33:25 -0700
Message-ID: <CAAswR-7Qdr8gLy8xKQVG3bCUE5GT=DfjLgFFdWFeih_ZD5p1rA@mail.gmail.com>
Subject: Re: Flaky streaming tests
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e013cbaa88c717204f6781bf7
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013cbaa88c717204f6781bf7
Content-Type: text/plain; charset=ISO-8859-1

There is a JIRA for one of the flakey tests here:
https://issues.apache.org/jira/browse/SPARK-1409


On Mon, Apr 7, 2014 at 11:32 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> TD - do you know what is going on here?
>
> I looked into this ab it and at least a few of these that use
> Thread.sleep() and assume the sleep will be exact, which is wrong. We
> should disable all the tests that do and probably they should be re-written
> to virtualize time.
>
> - Patrick
>
>
> On Mon, Apr 7, 2014 at 10:52 AM, Kay Ousterhout <keo@eecs.berkeley.edu
> >wrote:
>
> > Hi all,
> >
> > The InputStreamsSuite seems to have some serious flakiness issues -- I've
> > seen the file input stream fail many times and now I'm seeing some actor
> > input stream test failures (
> >
> >
> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13846/consoleFull
> > )
> > on what I think is an unrelated change.  Does anyone know anything about
> > these?  Should we just remove some of these tests since they seem to be
> > constantly failing?
> >
> > -Kay
> >
>

--089e013cbaa88c717204f6781bf7--

From dev-return-7257-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 18:34:49 2014
Return-Path: <dev-return-7257-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 62C9A10841
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 18:34:49 +0000 (UTC)
Received: (qmail 32378 invoked by uid 500); 7 Apr 2014 18:34:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32334 invoked by uid 500); 7 Apr 2014 18:34:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32325 invoked by uid 99); 7 Apr 2014 18:34:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 18:34:47 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.223.181 as permitted sender)
Received: from [209.85.223.181] (HELO mail-ie0-f181.google.com) (209.85.223.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 18:34:42 +0000
Received: by mail-ie0-f181.google.com with SMTP id tp5so6257329ieb.26
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 11:34:20 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=zfSrZNyHfZ7Kg69QY1XSRm0Qol3bDbwJ9gZCyn0Euvk=;
        b=YwOQUY52jFFeARd8he3Vkp0PSd5STbaAzQqSkUmC5zb3bFE0iVnas+Sy8jrEj+zxp2
         iGYNGDJSe+nwoUOd/bIWxQV1FeRehHX0CGmzTbbwLxvRcm++fT+5kFWJGB/jfMpmVvkM
         s2wW6ePaJqaUyruz05lqacgkp0pQa1T8m5mEQVA8ouLFfu9o6g54odfRr1INPnWMwptH
         hVDZUfV78WfXsIr9JgvQ6HA/ChNe1ZlFIvi5VBWCIph2ke4PZHfISzB1kHr+XdO49wPM
         2WPbc5n2EuJRlv/i02GFgCLRnHDRXk7Oblug18BlCwZBHT3rUN5zWRh3uRh3/mZ8OFIf
         5z1Q==
X-Received: by 10.50.6.75 with SMTP id y11mr21537853igy.6.1396895660386; Mon,
 07 Apr 2014 11:34:20 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.50.11.195 with HTTP; Mon, 7 Apr 2014 11:33:50 -0700 (PDT)
In-Reply-To: <CABPQxsuVyF64PUfLbieurd6xHjzM6kFXNjHYBqk35jVuRy4UcQ@mail.gmail.com>
References: <CAKJXNjHeL2fh9f-D68+f6=XuJMb-iFcR8gcDw1zzgc5xM1AcAg@mail.gmail.com>
 <CABPQxsuVyF64PUfLbieurd6xHjzM6kFXNjHYBqk35jVuRy4UcQ@mail.gmail.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Mon, 7 Apr 2014 11:33:50 -0700
Message-ID: <CAMwrk0=sn0RbLKTeQThyP2_1G6_JgC-_yFH+fjyDO0BwY62nYw@mail.gmail.com>
Subject: Re: Flaky streaming tests
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bd6ba0ea0a93804f6781d24
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd6ba0ea0a93804f6781d24
Content-Type: text/plain; charset=ISO-8859-1

Yes, I will take a look at those tests ASAP.

TD



On Mon, Apr 7, 2014 at 11:32 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> TD - do you know what is going on here?
>
> I looked into this ab it and at least a few of these that use
> Thread.sleep() and assume the sleep will be exact, which is wrong. We
> should disable all the tests that do and probably they should be re-written
> to virtualize time.
>
> - Patrick
>
>
> On Mon, Apr 7, 2014 at 10:52 AM, Kay Ousterhout <keo@eecs.berkeley.edu
> >wrote:
>
> > Hi all,
> >
> > The InputStreamsSuite seems to have some serious flakiness issues -- I've
> > seen the file input stream fail many times and now I'm seeing some actor
> > input stream test failures (
> >
> >
> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13846/consoleFull
> > )
> > on what I think is an unrelated change.  Does anyone know anything about
> > these?  Should we just remove some of these tests since they seem to be
> > constantly failing?
> >
> > -Kay
> >
>

--047d7bd6ba0ea0a93804f6781d24--

From dev-return-7258-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 18:34:53 2014
Return-Path: <dev-return-7258-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E46F010842
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 18:34:52 +0000 (UTC)
Received: (qmail 33037 invoked by uid 500); 7 Apr 2014 18:34:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32987 invoked by uid 500); 7 Apr 2014 18:34:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32976 invoked by uid 99); 7 Apr 2014 18:34:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 18:34:50 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.43] (HELO mail-qg0-f43.google.com) (209.85.192.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 18:34:46 +0000
Received: by mail-qg0-f43.google.com with SMTP id f51so6616612qge.16
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 11:34:23 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=pS34Z45Tds/koNWIVI1vKz3fgKpUm1IhyQjtuFJiUPU=;
        b=YcoIC8+4DdVNDrdlJai28YXOdkrJ/9zOmqsvPGIDAjG+1Q6z4ftmFSmISN+cz2FY70
         vlAng6HCZGM059ljLpbB+TnGB5Wd8QWdqSEQl419M5KJXJ1sqI+f2J6r7lJZNSqjPEDl
         Ed220IFiga/Oxy6ZusyKa9XfvYFSDRkUzONM3nE7JbLwUnsAy8gGB3EUwFB5uaH0fgX8
         im4ynQgqRSYApjor6oxFy6sanU7kSlmKqwFosyNWkUmQZDBZxIaJD0ngOQ40LwO06cvh
         bYs6aJdMN0KbeqFQoR91W233Ahsn8Vr9d9Gn67PvRkdOtD9ymTILr6g1EWcqZ/cRfX8C
         +KfA==
X-Gm-Message-State: ALoCoQn5H9ryWnghnocGTHKcHiYMh4iFUX47jE+s5Dumdox2TXEOANQruimzfJPet45k4fivLE/1
X-Received: by 10.140.95.142 with SMTP id i14mr34279931qge.6.1396895663365;
 Mon, 07 Apr 2014 11:34:23 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.125.5 with HTTP; Mon, 7 Apr 2014 11:34:03 -0700 (PDT)
In-Reply-To: <CAAswR-7Qdr8gLy8xKQVG3bCUE5GT=DfjLgFFdWFeih_ZD5p1rA@mail.gmail.com>
References: <CAKJXNjHeL2fh9f-D68+f6=XuJMb-iFcR8gcDw1zzgc5xM1AcAg@mail.gmail.com>
 <CABPQxsuVyF64PUfLbieurd6xHjzM6kFXNjHYBqk35jVuRy4UcQ@mail.gmail.com> <CAAswR-7Qdr8gLy8xKQVG3bCUE5GT=DfjLgFFdWFeih_ZD5p1rA@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Mon, 7 Apr 2014 11:34:03 -0700
Message-ID: <CAAswR-4+ytjcPLpvtoLAEz4f-hHAaARrNL+GbxsmDFN3Q=uD_A@mail.gmail.com>
Subject: Re: Flaky streaming tests
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c168fecdb42b04f6781d03
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c168fecdb42b04f6781d03
Content-Type: text/plain; charset=ISO-8859-1

I agree these should be disabled right away, and the JIRA can be used to
track fixing / turning them back on.


On Mon, Apr 7, 2014 at 11:33 AM, Michael Armbrust <michael@databricks.com>wrote:

> There is a JIRA for one of the flakey tests here:
> https://issues.apache.org/jira/browse/SPARK-1409
>
>
> On Mon, Apr 7, 2014 at 11:32 AM, Patrick Wendell <pwendell@gmail.com>wrote:
>
>> TD - do you know what is going on here?
>>
>> I looked into this ab it and at least a few of these that use
>> Thread.sleep() and assume the sleep will be exact, which is wrong. We
>> should disable all the tests that do and probably they should be
>> re-written
>> to virtualize time.
>>
>> - Patrick
>>
>>
>> On Mon, Apr 7, 2014 at 10:52 AM, Kay Ousterhout <keo@eecs.berkeley.edu
>> >wrote:
>>
>> > Hi all,
>> >
>> > The InputStreamsSuite seems to have some serious flakiness issues --
>> I've
>> > seen the file input stream fail many times and now I'm seeing some actor
>> > input stream test failures (
>> >
>> >
>> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/13846/consoleFull
>> > )
>> > on what I think is an unrelated change.  Does anyone know anything about
>> > these?  Should we just remove some of these tests since they seem to be
>> > constantly failing?
>> >
>> > -Kay
>> >
>>
>
>

--001a11c168fecdb42b04f6781d03--

From dev-return-7259-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 19:20:18 2014
Return-Path: <dev-return-7259-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CF99710A93
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 19:20:18 +0000 (UTC)
Received: (qmail 38123 invoked by uid 500); 7 Apr 2014 19:20:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37647 invoked by uid 500); 7 Apr 2014 19:20:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37630 invoked by uid 99); 7 Apr 2014 19:20:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 19:20:14 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.178] (HELO mail-pd0-f178.google.com) (209.85.192.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 19:20:07 +0000
Received: by mail-pd0-f178.google.com with SMTP id x10so6969246pdj.23
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 12:19:45 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:message-id:date:from:user-agent:mime-version:to
         :subject:content-type:content-transfer-encoding;
        bh=GMJpP5xT6NxINN7jlakAIbL3ROS2zSsVEkiIGzLugvw=;
        b=iJcU0vYU4I31PD4U+NICW0S68ZMhrXE4Hf6oWNNMhwwRfQbEZSYZ/n/WluBLmX87fN
         ud9trjQvKIYjC+teQRISRz8jPyyS66znrOxi2P1+rlUyy2lX8P+/DqFSdHvRZ0W+S4IR
         EPQEf3F1+X0jtNbvxjU+v2OpVdwnN7BRVdrDohvsjKKHkuVZK/GLVnyoUMSVju+Es80C
         cvvkdv9r9XW0xoeGmhGFhLEXZzzAmWvDMQdjK29q09NlBhXYXq+FOMO+DV/gHemrtXIQ
         fYsHd9y38OwI4oT5b5J8L/tja/ynqvVFd+Qwefz8+lRwq4pFWdu9GEGqN3iKqp+GGQTO
         y1Rw==
X-Gm-Message-State: ALoCoQk+qf16rV9JyYArKoBYo5/+S0S/TRyInjSB+MBrRhQXjuHZjlF1hU5dk1ePMEzEEbD6YPF8
X-Received: by 10.66.176.143 with SMTP id ci15mr33149313pac.35.1396898385308;
        Mon, 07 Apr 2014 12:19:45 -0700 (PDT)
Received: from Christophes-MacBook-Pro.local ([63.135.84.68])
        by mx.google.com with ESMTPSA id gg3sm38503081pbc.34.2014.04.07.12.19.44
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 07 Apr 2014 12:19:44 -0700 (PDT)
Message-ID: <5342FA4F.40203@christophe.cc>
Date: Mon, 07 Apr 2014 12:19:43 -0700
From: Christophe Clapp <christophe@christophe.cc>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:24.0) Gecko/20100101 Thunderbird/24.4.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: Spark Streaming and Flume Avro RPC Servers
Content-Type: text/plain; charset=ISO-8859-1; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

 From my testing of Spark Streaming with Flume, it seems that there's 
only one of the Spark worker nodes that runs a Flume Avro RPC server to 
receive messages at any given time, as opposed to every Spark worker 
running an Avro RPC server to receive messages. Is this the case? Our 
use-case would benefit from balancing the load across Workers because of 
our volume of messages. We would be using a load balancer in front of 
the Spark workers running the Avro RPC servers, essentially 
round-robinning the messages across all of them.

If this is something that is currently not supported, I'd be interested 
in contributing to the code to make it happen.

- Christophe

From dev-return-7260-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 19:24:38 2014
Return-Path: <dev-return-7260-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 87AB810AC7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 19:24:38 +0000 (UTC)
Received: (qmail 51005 invoked by uid 500); 7 Apr 2014 19:24:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50961 invoked by uid 500); 7 Apr 2014 19:24:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50953 invoked by uid 99); 7 Apr 2014 19:24:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 19:24:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mfernest@cloudera.com designates 209.85.160.178 as permitted sender)
Received: from [209.85.160.178] (HELO mail-yk0-f178.google.com) (209.85.160.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 19:24:30 +0000
Received: by mail-yk0-f178.google.com with SMTP id 79so5915362ykr.23
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 12:24:08 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=F+Mftez2b/3vboxkmPXV+meJKy8Lnqr43SWUuW+IVgQ=;
        b=H6B5/4Ne37Ppw2lXg18USJleQXx2yyeYQO3XaD59UqQMB49S7COz/n6oATzcASnI1b
         ywpXpMx+fkAvmLUASrQ4LJGRw/SSk3E1aqqA22p1EHL2/xl4dq9Hd4XFOMoQmAVZOjKs
         kHobVoxb+1UaVaMxV8biLBLNZafXMQuGnxYLwb1tiuKgGuS/e1sucYV+tzmMzOICgImd
         bL5PfI0vuIEPrCGiCND5LEA01AeEOM1lYsr7Ki3HVg4e/Hn+aJAhvCWZPIrKbJtAgh7y
         2mup6UZRXKrbTtKDd+8AE2VA+aWvYqzUG5CqScs8ipF2WvSFrR2hnRguKwvSoTsFLqUp
         qFFg==
X-Gm-Message-State: ALoCoQltjTmHAS/tFJbEfB37m9pXHRH+xrjR2U7kjU5YSKO/UzW4Yo66/9GN3FN0/ZTDiVKCSZDX
X-Received: by 10.236.19.99 with SMTP id m63mr5187323yhm.134.1396898648569;
 Mon, 07 Apr 2014 12:24:08 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.170.50.210 with HTTP; Mon, 7 Apr 2014 12:23:38 -0700 (PDT)
In-Reply-To: <5342FA4F.40203@christophe.cc>
References: <5342FA4F.40203@christophe.cc>
From: Michael Ernest <mfernest@cloudera.com>
Date: Mon, 7 Apr 2014 15:23:38 -0400
Message-ID: <CADpnHP2QX=XENZAeH+C+8ipuuMfHr--n=rsKhmdce-qvjRY_YA@mail.gmail.com>
Subject: Re: Spark Streaming and Flume Avro RPC Servers
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01634dd4bc43d304f678cfca
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01634dd4bc43d304f678cfca
Content-Type: text/plain; charset=ISO-8859-1

You can configure your sinks to write to one or more Avro sources in a
load-balanced configuration.

https://flume.apache.org/FlumeUserGuide.html#flume-sink-processors

mfe


On Mon, Apr 7, 2014 at 3:19 PM, Christophe Clapp
<christophe@christophe.cc>wrote:

> Hi,
>
> From my testing of Spark Streaming with Flume, it seems that there's only
> one of the Spark worker nodes that runs a Flume Avro RPC server to receive
> messages at any given time, as opposed to every Spark worker running an
> Avro RPC server to receive messages. Is this the case? Our use-case would
> benefit from balancing the load across Workers because of our volume of
> messages. We would be using a load balancer in front of the Spark workers
> running the Avro RPC servers, essentially round-robinning the messages
> across all of them.
>
> If this is something that is currently not supported, I'd be interested in
> contributing to the code to make it happen.
>
> - Christophe
>



-- 
Michael Ernest
Sr. Solutions Consultant
West Coast

--089e01634dd4bc43d304f678cfca--

From dev-return-7261-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 19:37:57 2014
Return-Path: <dev-return-7261-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 73C2A10B23
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 19:37:57 +0000 (UTC)
Received: (qmail 81262 invoked by uid 500); 7 Apr 2014 19:37:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81223 invoked by uid 500); 7 Apr 2014 19:37:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81215 invoked by uid 99); 7 Apr 2014 19:37:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 19:37:54 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 19:37:50 +0000
Received: by mail-qg0-f50.google.com with SMTP id q108so6817272qgd.9
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 12:37:29 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=kRDRN2q62k72HMwBk8l7+fc6d8/KzKCTqM3jmmeO+FM=;
        b=ZOB8Ym1C4a1fG86geLZ4fUDud3oxDqdZq106akxsnc96msQiMP4t6a62fOfVBD9WFo
         uupAbBz7pVXn527+tMVx28FdpTP2hVS70o1UDhzFiYmufLzcB9OGI9gRPCBr7ciq6QWt
         pCr3Ds/LI4or0It4DJLYdHrsqm2I3j0uscSer3IgaSUwrOr9icbXanoDCdspBPFE1GNe
         hM+QCJQteNvlpCE1GcgtHzEx84i30d8+VXGjfdEeDmixNWqOJWTAm7IiVpb4Wz3YCzvk
         yRDT4dpvzvEzhfyLKAQg9hQPGJz7k5dlu1yIhtUJv+RPCFZRKlLxS/FeaE3XMii4UVT0
         2xJQ==
X-Gm-Message-State: ALoCoQltyYP8GUIw1/0gny1oc0eqwTUcLy2yp3Ij7fF99C9nEk5u/omsoqDLTiEAZU1HylqXVawO
MIME-Version: 1.0
X-Received: by 10.229.58.68 with SMTP id f4mr6660819qch.18.1396899449474; Mon,
 07 Apr 2014 12:37:29 -0700 (PDT)
Received: by 10.96.87.228 with HTTP; Mon, 7 Apr 2014 12:37:29 -0700 (PDT)
Received: by 10.96.87.228 with HTTP; Mon, 7 Apr 2014 12:37:29 -0700 (PDT)
In-Reply-To: <CADpnHP2QX=XENZAeH+C+8ipuuMfHr--n=rsKhmdce-qvjRY_YA@mail.gmail.com>
References: <5342FA4F.40203@christophe.cc>
	<CADpnHP2QX=XENZAeH+C+8ipuuMfHr--n=rsKhmdce-qvjRY_YA@mail.gmail.com>
Date: Mon, 7 Apr 2014 12:37:29 -0700
Message-ID: <CALseEE9M9gA6Oc-FXo5TmV-Fq9tmrxS-E3c_YmxWUG6co4YV9g@mail.gmail.com>
Subject: Re: Spark Streaming and Flume Avro RPC Servers
From: Christophe Clapp <christophe@christophe.cc>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113306b079246f04f678ffe1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113306b079246f04f678ffe1
Content-Type: text/plain; charset=ISO-8859-1

Right, but at least in my case, no avro RPC server was started on any of
the spark worker nodes except for one. I don't know if that's just some
configuration issue with my setup or if it's expected behavior. I would
need spark to start avro RPC servers on every worker rather than just one.

- Christophe
On Apr 7, 2014 12:24 PM, "Michael Ernest" <mfernest@cloudera.com> wrote:

> You can configure your sinks to write to one or more Avro sources in a
> load-balanced configuration.
>
> https://flume.apache.org/FlumeUserGuide.html#flume-sink-processors
>
> mfe
>
>
> On Mon, Apr 7, 2014 at 3:19 PM, Christophe Clapp
> <christophe@christophe.cc>wrote:
>
> > Hi,
> >
> > From my testing of Spark Streaming with Flume, it seems that there's only
> > one of the Spark worker nodes that runs a Flume Avro RPC server to
> receive
> > messages at any given time, as opposed to every Spark worker running an
> > Avro RPC server to receive messages. Is this the case? Our use-case would
> > benefit from balancing the load across Workers because of our volume of
> > messages. We would be using a load balancer in front of the Spark workers
> > running the Avro RPC servers, essentially round-robinning the messages
> > across all of them.
> >
> > If this is something that is currently not supported, I'd be interested
> in
> > contributing to the code to make it happen.
> >
> > - Christophe
> >
>
>
>
> --
> Michael Ernest
> Sr. Solutions Consultant
> West Coast
>

--001a113306b079246f04f678ffe1--

From dev-return-7262-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 19:58:57 2014
Return-Path: <dev-return-7262-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 18DC310BD0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 19:58:57 +0000 (UTC)
Received: (qmail 19372 invoked by uid 500); 7 Apr 2014 19:58:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19090 invoked by uid 500); 7 Apr 2014 19:58:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19070 invoked by uid 99); 7 Apr 2014 19:58:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 19:58:51 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.51] (HELO mail-pa0-f51.google.com) (209.85.220.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 19:58:46 +0000
Received: by mail-pa0-f51.google.com with SMTP id kq14so7274553pab.10
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 12:58:24 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:message-id:date:from:user-agent:mime-version:to
         :subject:references:in-reply-to:content-type
         :content-transfer-encoding;
        bh=Zf0u/b44PhiijH6XxOUedJIyW/a+XfRWm0HaWSrevME=;
        b=XAn1KGeZMUZO3EGlMFSQ0TlLdJVjVMmRHJZNLK+jn4jtHENOydK+gcNYN/anE4OIaP
         wJ5fhzWro1cTOWdeg/ZEhjxG9c8Dq+2Ay4+jDwndUzE3AyoCO6NU8DqrjWZYDcXVS1HA
         1kNhChTVUrfyRo08hdHKA3b2jZbFRRDvGc4wTgJ5VsrdebSE7ImwVPbzsEJq6akBbKDT
         GNAY9bYZRm1QEs+HgqIGI/GgOjJrOs8ckqV6kZe/D1EEG10ScXUqgUIULegr5ec8V9JS
         gGwyqh3+c0g6HLcpp5UDsRwRGwgbAQlcyg3ThCgDHQL0xGEu5VmxB8x7PU4M0ZeLWpft
         v2BQ==
X-Gm-Message-State: ALoCoQmMSstYrvosqCrn7xi/ZmOCtKWNfej/aQB+/RFBS0MCItgxWJY0rhi2M49HTt9P8iwCHuu1
X-Received: by 10.68.100.1 with SMTP id eu1mr33044257pbb.36.1396900704071;
        Mon, 07 Apr 2014 12:58:24 -0700 (PDT)
Received: from Christophes-MacBook-Pro.local ([63.135.84.68])
        by mx.google.com with ESMTPSA id vd8sm90258793pac.12.2014.04.07.12.58.22
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 07 Apr 2014 12:58:22 -0700 (PDT)
Message-ID: <5343035D.4020908@christophe.cc>
Date: Mon, 07 Apr 2014 12:58:21 -0700
From: Christophe Clapp <christophe@christophe.cc>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:24.0) Gecko/20100101 Thunderbird/24.4.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: Re: Spark Streaming and Flume Avro RPC Servers
References: <5342FA4F.40203@christophe.cc> <CADpnHP2QX=XENZAeH+C+8ipuuMfHr--n=rsKhmdce-qvjRY_YA@mail.gmail.com>
In-Reply-To: <CADpnHP2QX=XENZAeH+C+8ipuuMfHr--n=rsKhmdce-qvjRY_YA@mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Based on the source code here:
https://github.com/apache/spark/blob/master/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeUtils.scala

It looks like in its current version, FlumeUtils does not support 
starting an Avro RPC server on more than one worker.

- Christophe

On 4/7/14, 12:23 PM, Michael Ernest wrote:
> You can configure your sinks to write to one or more Avro sources in a
> load-balanced configuration.
>
> https://flume.apache.org/FlumeUserGuide.html#flume-sink-processors
>
> mfe
>
>
> On Mon, Apr 7, 2014 at 3:19 PM, Christophe Clapp
> <christophe@christophe.cc>wrote:
>
>> Hi,
>>
>>  From my testing of Spark Streaming with Flume, it seems that there's only
>> one of the Spark worker nodes that runs a Flume Avro RPC server to receive
>> messages at any given time, as opposed to every Spark worker running an
>> Avro RPC server to receive messages. Is this the case? Our use-case would
>> benefit from balancing the load across Workers because of our volume of
>> messages. We would be using a load balancer in front of the Spark workers
>> running the Avro RPC servers, essentially round-robinning the messages
>> across all of them.
>>
>> If this is something that is currently not supported, I'd be interested in
>> contributing to the code to make it happen.
>>
>> - Christophe
>>
>
>


From dev-return-7263-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 20:16:51 2014
Return-Path: <dev-return-7263-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AAB5810C4C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 20:16:51 +0000 (UTC)
Received: (qmail 59159 invoked by uid 500); 7 Apr 2014 20:16:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58549 invoked by uid 500); 7 Apr 2014 20:16:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58490 invoked by uid 99); 7 Apr 2014 20:16:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 20:16:45 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.49] (HELO mail-pa0-f49.google.com) (209.85.220.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 20:16:41 +0000
Received: by mail-pa0-f49.google.com with SMTP id lj1so7257295pab.36
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 13:16:20 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:message-id:date:from:user-agent:mime-version:to
         :subject:references:in-reply-to:content-type
         :content-transfer-encoding;
        bh=HfAgPqmRjCIxmh1pKRx+A+xM1rDnCUps6hMCcegAFRQ=;
        b=HlScmCyrakBkMlxfOkx417jW1VUFbXeEnLMBljYZe+BN57RYsCfJkCFqiRCjbILUDl
         0NzQt70/9dmIXlP2keG3Ao9R/ATsTuo75JoqH4lSskFR74vR+vnCMsjziHckkO82kWu6
         quBCTf4tx+SqeSpGTOOyxEtwjNcpXCXrp52tp/qSLjm4t3bcOnrEfZd7Ysqpe32PSgfu
         CFsgqNh90E9F6SXgtaxSJEYLMi8acmvvFdqKSKIqz0EKiGZWy9dTtE8SmbWr8eCBYQrB
         OFTJfQuTigSVIbglhMvUd7HgudqPlUOxh7glRqqirLj7k9OfZjdKXU7KEFXNs9Zxopbj
         R0Ug==
X-Gm-Message-State: ALoCoQmwJlPGpyOWrw0y3CNuhoOTucxZq9OWOcc9qPhVZU7UWTVDiEi6nJ+CscM7Fwq8Onh5/zxc
X-Received: by 10.68.222.105 with SMTP id ql9mr33224102pbc.4.1396901780511;
        Mon, 07 Apr 2014 13:16:20 -0700 (PDT)
Received: from Christophes-MacBook-Pro.local ([63.135.84.68])
        by mx.google.com with ESMTPSA id id10sm38686944pbc.35.2014.04.07.13.16.19
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 07 Apr 2014 13:16:19 -0700 (PDT)
Message-ID: <53430792.7030807@christophe.cc>
Date: Mon, 07 Apr 2014 13:16:18 -0700
From: Christophe Clapp <christophe@christophe.cc>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:24.0) Gecko/20100101 Thunderbird/24.4.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: Re: Spark Streaming and Flume Avro RPC Servers
References: <5342FA4F.40203@christophe.cc> <CADpnHP2QX=XENZAeH+C+8ipuuMfHr--n=rsKhmdce-qvjRY_YA@mail.gmail.com> <5343035D.4020908@christophe.cc>
In-Reply-To: <5343035D.4020908@christophe.cc>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Could it be as simple as just changing FlumeUtils to accept a list of 
host/port number pairs to start the RPC servers on?


On 4/7/14, 12:58 PM, Christophe Clapp wrote:
> Based on the source code here:
> https://github.com/apache/spark/blob/master/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeUtils.scala 
>
>
> It looks like in its current version, FlumeUtils does not support 
> starting an Avro RPC server on more than one worker.
>
> - Christophe
>
> On 4/7/14, 12:23 PM, Michael Ernest wrote:
>> You can configure your sinks to write to one or more Avro sources in a
>> load-balanced configuration.
>>
>> https://flume.apache.org/FlumeUserGuide.html#flume-sink-processors
>>
>> mfe
>>
>>
>> On Mon, Apr 7, 2014 at 3:19 PM, Christophe Clapp
>> <christophe@christophe.cc>wrote:
>>
>>> Hi,
>>>
>>>  From my testing of Spark Streaming with Flume, it seems that 
>>> there's only
>>> one of the Spark worker nodes that runs a Flume Avro RPC server to 
>>> receive
>>> messages at any given time, as opposed to every Spark worker running an
>>> Avro RPC server to receive messages. Is this the case? Our use-case 
>>> would
>>> benefit from balancing the load across Workers because of our volume of
>>> messages. We would be using a load balancer in front of the Spark 
>>> workers
>>> running the Avro RPC servers, essentially round-robinning the messages
>>> across all of them.
>>>
>>> If this is something that is currently not supported, I'd be 
>>> interested in
>>> contributing to the code to make it happen.
>>>
>>> - Christophe
>>>
>>
>>
>


From dev-return-7264-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 20:51:28 2014
Return-Path: <dev-return-7264-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 396B510DDE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 20:51:28 +0000 (UTC)
Received: (qmail 35985 invoked by uid 500); 7 Apr 2014 20:51:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35861 invoked by uid 500); 7 Apr 2014 20:51:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35828 invoked by uid 99); 7 Apr 2014 20:51:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 20:51:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mfernest@cloudera.com designates 209.85.213.53 as permitted sender)
Received: from [209.85.213.53] (HELO mail-yh0-f53.google.com) (209.85.213.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 20:51:17 +0000
Received: by mail-yh0-f53.google.com with SMTP id v1so6481183yhn.12
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 13:50:55 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=D2clMPerNw/XmvFrOOj8M1SkBQR+uSAOTBipLS+ZRvQ=;
        b=QyQ/24JHNTI5DGlmT710xXG8m4M0vvcez2uOhHwIAMMry+SfLVLHQtlhzIWF4qRnvh
         dN67FLBAc9yQ3z8BjWMvhF4aChBp1CW8WG5L5EGzVIhkfH8HOMi5JY6OBIKWooVbVrJl
         NHoaPKbJY2eX+bo+SlAQy6RSKgds8v0A6ri5K4WZzKkLKEuVrOOjvD/P9X784gZSdILl
         qJcYnpSZwaQJJIXhOopgbP20vu5QScmBM2vJ5KJvFjEV2DvBJ7aGyJGxcJpq4lrnWQ4x
         GmvYTU1Qw+ATHq4l1WY3/AfiCvwRfzz8edRJVwpAvzXSrDrkSJ9SWkCsWFfgb96HBSpl
         GLBw==
X-Gm-Message-State: ALoCoQl9vygEScHy6H7hETKKSLx7ocFCywJZAGqqDX2dsQ5SZzu0TSz0Kxa4Pwm2MwlB2IDLBW6n
X-Received: by 10.236.61.45 with SMTP id v33mr49083426yhc.20.1396903855113;
 Mon, 07 Apr 2014 13:50:55 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.170.50.210 with HTTP; Mon, 7 Apr 2014 13:50:25 -0700 (PDT)
In-Reply-To: <53430792.7030807@christophe.cc>
References: <5342FA4F.40203@christophe.cc> <CADpnHP2QX=XENZAeH+C+8ipuuMfHr--n=rsKhmdce-qvjRY_YA@mail.gmail.com>
 <5343035D.4020908@christophe.cc> <53430792.7030807@christophe.cc>
From: Michael Ernest <mfernest@cloudera.com>
Date: Mon, 7 Apr 2014 16:50:25 -0400
Message-ID: <CADpnHP300C-+PbTJ9pyg+d8BE7VwHxQpt5zWNBS1D5zTMEiK7w@mail.gmail.com>
Subject: Re: Spark Streaming and Flume Avro RPC Servers
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0160a73c11deae04f67a063c
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160a73c11deae04f67a063c
Content-Type: text/plain; charset=ISO-8859-1

I don't see why not. If one were doing something similar with straight
Flume, you'd start an agent on each node you care to receive Avro/RPC
events. In the absence of clearer insight to your use case, I'm puzzling
just a little why it's necessary for each Worker to be its own receiver,
but there's no real objection or concern to fuel the puzzlement, just
curiosity.


On Mon, Apr 7, 2014 at 4:16 PM, Christophe Clapp
<christophe@christophe.cc>wrote:

> Could it be as simple as just changing FlumeUtils to accept a list of
> host/port number pairs to start the RPC servers on?
>
>
>
> On 4/7/14, 12:58 PM, Christophe Clapp wrote:
>
>> Based on the source code here:
>> https://github.com/apache/spark/blob/master/external/
>> flume/src/main/scala/org/apache/spark/streaming/flume/FlumeUtils.scala
>>
>> It looks like in its current version, FlumeUtils does not support
>> starting an Avro RPC server on more than one worker.
>>
>> - Christophe
>>
>> On 4/7/14, 12:23 PM, Michael Ernest wrote:
>>
>>> You can configure your sinks to write to one or more Avro sources in a
>>> load-balanced configuration.
>>>
>>> https://flume.apache.org/FlumeUserGuide.html#flume-sink-processors
>>>
>>> mfe
>>>
>>>
>>> On Mon, Apr 7, 2014 at 3:19 PM, Christophe Clapp
>>> <christophe@christophe.cc>wrote:
>>>
>>>  Hi,
>>>>
>>>>  From my testing of Spark Streaming with Flume, it seems that there's
>>>> only
>>>> one of the Spark worker nodes that runs a Flume Avro RPC server to
>>>> receive
>>>> messages at any given time, as opposed to every Spark worker running an
>>>> Avro RPC server to receive messages. Is this the case? Our use-case
>>>> would
>>>> benefit from balancing the load across Workers because of our volume of
>>>> messages. We would be using a load balancer in front of the Spark
>>>> workers
>>>> running the Avro RPC servers, essentially round-robinning the messages
>>>> across all of them.
>>>>
>>>> If this is something that is currently not supported, I'd be interested
>>>> in
>>>> contributing to the code to make it happen.
>>>>
>>>> - Christophe
>>>>
>>>>
>>>
>>>
>>
>


-- 
Michael Ernest
Sr. Solutions Consultant
West Coast

--089e0160a73c11deae04f67a063c--

From dev-return-7265-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr  7 21:37:58 2014
Return-Path: <dev-return-7265-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9EEFA10FE8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Apr 2014 21:37:58 +0000 (UTC)
Received: (qmail 36962 invoked by uid 500); 7 Apr 2014 21:37:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36487 invoked by uid 500); 7 Apr 2014 21:37:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36437 invoked by uid 99); 7 Apr 2014 21:37:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 21:37:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.44] (HELO mail-qa0-f44.google.com) (209.85.216.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Apr 2014 21:37:45 +0000
Received: by mail-qa0-f44.google.com with SMTP id hw13so39744qab.31
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 14:37:23 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=ulScPQshIswsfgg2OD1DQwygcOjrm2UwayJX20Q1WK8=;
        b=SKvWcc/ze5mANT/xmh1nXkxP9J6vUVEny+DoWvbx7LMSjN1mxPvhZoSbPmza1Rb+xc
         6n5mct5eFDSIw0v7PaRggsfhz/YZA0RaeYRp7RkUEAqdW6RLv3ZB6jdoRtdAVTQncBLq
         HkqHNblLrSW9mbeSkK7XrtXYzJE0IjjJuSY+Bm8+LeHKWKmn1WsfLsxgQLjlSSg4I+Ra
         B+fJPfPq2zFmefbM+sJackhtOUeScIsL1J7AEX4EOddhriOeh4yql1tdngmv9Uyo1toN
         pOG9V6Mk7Bmn1Ne6MPHLjd5wk/HsG8+EXhHFPtZ6cErztKjb9+nXWgioL35r3gksb2zb
         ZXkQ==
X-Gm-Message-State: ALoCoQnFZmb9jOWf67D+HHnxng3ZSGXLOY3IH0bpjUEGru9tzwkfCYW++5K8QEUMA92Qja4IvMp7
MIME-Version: 1.0
X-Received: by 10.224.112.6 with SMTP id u6mr12586636qap.78.1396906642996;
 Mon, 07 Apr 2014 14:37:22 -0700 (PDT)
Received: by 10.96.87.228 with HTTP; Mon, 7 Apr 2014 14:37:22 -0700 (PDT)
Received: by 10.96.87.228 with HTTP; Mon, 7 Apr 2014 14:37:22 -0700 (PDT)
In-Reply-To: <CADpnHP300C-+PbTJ9pyg+d8BE7VwHxQpt5zWNBS1D5zTMEiK7w@mail.gmail.com>
References: <5342FA4F.40203@christophe.cc>
	<CADpnHP2QX=XENZAeH+C+8ipuuMfHr--n=rsKhmdce-qvjRY_YA@mail.gmail.com>
	<5343035D.4020908@christophe.cc>
	<53430792.7030807@christophe.cc>
	<CADpnHP300C-+PbTJ9pyg+d8BE7VwHxQpt5zWNBS1D5zTMEiK7w@mail.gmail.com>
Date: Mon, 7 Apr 2014 14:37:22 -0700
Message-ID: <CALseEE_ZRKWvgVNeMDndTjb8f0myq41Ddwx_GCAx6bZ+542ZpQ@mail.gmail.com>
Subject: Re: Spark Streaming and Flume Avro RPC Servers
From: Christophe Clapp <christophe@christophe.cc>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2e8483daf8804f67aac3b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2e8483daf8804f67aac3b
Content-Type: text/plain; charset=ISO-8859-1

Cool. I'll look at making the code change in FlumeUtils and generating a
pull request.

As far as the use case, the volume of messages we have is currently about
30 MB per second which may grow to over what a 1 Gbit network adapter can
handle.

- Christophe
On Apr 7, 2014 1:51 PM, "Michael Ernest" <mfernest@cloudera.com> wrote:

> I don't see why not. If one were doing something similar with straight
> Flume, you'd start an agent on each node you care to receive Avro/RPC
> events. In the absence of clearer insight to your use case, I'm puzzling
> just a little why it's necessary for each Worker to be its own receiver,
> but there's no real objection or concern to fuel the puzzlement, just
> curiosity.
>
>
> On Mon, Apr 7, 2014 at 4:16 PM, Christophe Clapp
> <christophe@christophe.cc>wrote:
>
> > Could it be as simple as just changing FlumeUtils to accept a list of
> > host/port number pairs to start the RPC servers on?
> >
> >
> >
> > On 4/7/14, 12:58 PM, Christophe Clapp wrote:
> >
> >> Based on the source code here:
> >> https://github.com/apache/spark/blob/master/external/
> >> flume/src/main/scala/org/apache/spark/streaming/flume/FlumeUtils.scala
> >>
> >> It looks like in its current version, FlumeUtils does not support
> >> starting an Avro RPC server on more than one worker.
> >>
> >> - Christophe
> >>
> >> On 4/7/14, 12:23 PM, Michael Ernest wrote:
> >>
> >>> You can configure your sinks to write to one or more Avro sources in a
> >>> load-balanced configuration.
> >>>
> >>> https://flume.apache.org/FlumeUserGuide.html#flume-sink-processors
> >>>
> >>> mfe
> >>>
> >>>
> >>> On Mon, Apr 7, 2014 at 3:19 PM, Christophe Clapp
> >>> <christophe@christophe.cc>wrote:
> >>>
> >>>  Hi,
> >>>>
> >>>>  From my testing of Spark Streaming with Flume, it seems that there's
> >>>> only
> >>>> one of the Spark worker nodes that runs a Flume Avro RPC server to
> >>>> receive
> >>>> messages at any given time, as opposed to every Spark worker running
> an
> >>>> Avro RPC server to receive messages. Is this the case? Our use-case
> >>>> would
> >>>> benefit from balancing the load across Workers because of our volume
> of
> >>>> messages. We would be using a load balancer in front of the Spark
> >>>> workers
> >>>> running the Avro RPC servers, essentially round-robinning the messages
> >>>> across all of them.
> >>>>
> >>>> If this is something that is currently not supported, I'd be
> interested
> >>>> in
> >>>> contributing to the code to make it happen.
> >>>>
> >>>> - Christophe
> >>>>
> >>>>
> >>>
> >>>
> >>
> >
>
>
> --
> Michael Ernest
> Sr. Solutions Consultant
> West Coast
>

--001a11c2e8483daf8804f67aac3b--

From dev-return-7266-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 02:00:35 2014
Return-Path: <dev-return-7266-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 752371093E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 02:00:35 +0000 (UTC)
Received: (qmail 84455 invoked by uid 500); 8 Apr 2014 02:00:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84418 invoked by uid 500); 8 Apr 2014 02:00:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84409 invoked by uid 99); 8 Apr 2014 02:00:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 02:00:33 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 02:00:28 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 56A6C101C9C
	for <dev@spark.apache.org>; Mon,  7 Apr 2014 19:00:05 -0700 (PDT)
Received: from mail-qa0-f43.google.com (mail-qa0-f43.google.com [209.85.216.43])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id C079D101BF4
	for <dev@spark.apache.org>; Mon,  7 Apr 2014 19:00:03 -0700 (PDT)
Received: by mail-qa0-f43.google.com with SMTP id j15so268466qaq.30
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 19:00:02 -0700 (PDT)
X-Gm-Message-State: ALoCoQmnp9vWxLAfflpQTrx124v3tGZgu3HLtXm66KCl2Nw1Io6W1HCiCdorXhPOsmr76OSo5bWo
MIME-Version: 1.0
X-Received: by 10.224.49.67 with SMTP id u3mr865154qaf.63.1396922402962; Mon,
 07 Apr 2014 19:00:02 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Mon, 7 Apr 2014 19:00:02 -0700 (PDT)
In-Reply-To: <CAFkVJifBb14oGvE4zHSL0=s1na9neiKVRGRWmUC24wWnP_9Miw@mail.gmail.com>
References: <CAFkVJifBb14oGvE4zHSL0=s1na9neiKVRGRWmUC24wWnP_9Miw@mail.gmail.com>
Date: Mon, 7 Apr 2014 19:00:02 -0700
Message-ID: <CAEYYnxbo10PHJsY=s4s2f-3PsSed-F+9zmsz35ELGSfGVhnuHg@mail.gmail.com>
Subject: Re: MLLib - Thoughts about refactoring Updater for LBFGS?
From: DB Tsai <dbtsai@stanford.edu>
To: dlwh@cs.berkeley.edu, debasish.das83@gmail.com, mengxr@gmail.com, 
	dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi guys,

The latest PR uses Breeze's L-BFGS implement which is introduced by
Xiangrui's sparse input format work in SPARK-1212.

https://github.com/apache/spark/pull/353

Now, it works with the new sparse framework!

Any feedback would be greatly appreciated.

Thanks.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Thu, Apr 3, 2014 at 5:02 PM, DB Tsai <dbtsai@alpinenow.com> wrote:
> ---------- Forwarded message ----------
> From: David Hall <dlwh@cs.berkeley.edu>
> Date: Sat, Mar 15, 2014 at 10:02 AM
> Subject: Re: MLLib - Thoughts about refactoring Updater for LBFGS?
> To: DB Tsai <dbtsai@alpinenow.com>
>
>
> On Fri, Mar 7, 2014 at 10:56 PM, DB Tsai <dbtsai@alpinenow.com> wrote:
>>
>> Hi David,
>>
>> Please let me know the version of Breeze that LBFGS can be serialized,
>> and CachedDiffFunction is built-in in LBFGS once you finish. I'll
>> update the PR to Spark from using RISO implementation to Breeze
>> implementation.
>
>
> The current master (0.7-SNAPSHOT) has these problems fixed.
>
>>
>>
>> Thanks.
>>
>> Sincerely,
>>
>> DB Tsai
>> Machine Learning Engineer
>> Alpine Data Labs
>> --------------------------------------
>> Web: http://alpinenow.com/
>>
>>
>> On Thu, Mar 6, 2014 at 4:26 PM, David Hall <dlwh@cs.berkeley.edu> wrote:
>> > On Thu, Mar 6, 2014 at 4:21 PM, DB Tsai <dbtsai@alpinenow.com> wrote:
>> >
>> >> Hi David,
>> >>
>> >> I can converge to the same result with your breeze LBFGS and Fortran
>> >> implementations now. Probably, I made some mistakes when I tried
>> >> breeze before. I apologize that I claimed it's not stable.
>> >>
>> >> See the test case in BreezeLBFGSSuite.scala
>> >> https://github.com/AlpineNow/spark/tree/dbtsai-breezeLBFGS
>> >>
>> >> This is training multinomial logistic regression against iris dataset,
>> >> and both optimizers can train the models with 98% training accuracy.
>> >>
>> >
>> > great to hear! There were some bugs in LBFGS about 6 months ago, so
>> > depending on the last time you tried it, it might indeed have been
>> > bugged.
>> >
>> >
>> >>
>> >> There are two issues to use Breeze in Spark,
>> >>
>> >> 1) When the gradientSum and lossSum are computed distributively in
>> >> custom defined DiffFunction which will be passed into your optimizer,
>> >> Spark will complain LBFGS class is not serializable. In
>> >> BreezeLBFGS.scala, I've to convert RDD to array to make it work
>> >> locally. It should be easy to fix by just having LBFGS to implement
>> >> Serializable.
>> >>
>> >
>> > I'm not sure why Spark should be serializing LBFGS? Shouldn't it live on
>> > the controller node? Or is this a per-node thing?
>> >
>> > But no problem to make it serializable.
>> >
>> >
>> >>
>> >> 2) Breeze computes redundant gradient and loss. See the following log
>> >> from both Fortran and Breeze implementations.
>> >>
>> >
>> > Err, yeah. I should probably have LBFGS do this automatically, but
>> > there's
>> > a CachedDiffFunction that gets rid of the redundant calculations.
>> >
>> > -- David
>> >
>> >
>> >>
>> >> Thanks.
>> >>
>> >> Fortran:
>> >> Iteration -1: loss 1.3862943611198926, diff 1.0
>> >> Iteration 0: loss 1.5846343143210866, diff 0.14307193024217352
>> >> Iteration 1: loss 1.1242501524477688, diff 0.29053004039012126
>> >> Iteration 2: loss 1.0930151243303563, diff 0.027782962952189336
>> >> Iteration 3: loss 1.054036932835569, diff 0.03566113127440601
>> >> Iteration 4: loss 0.9907956302751622, diff 0.05999907649459571
>> >> Iteration 5: loss 0.9184205380342829, diff 0.07304737423337761
>> >> Iteration 6: loss 0.8259870936519937, diff 0.10064381175132982
>> >> Iteration 7: loss 0.6327447552109574, diff 0.23395293458364716
>> >> Iteration 8: loss 0.5534101162436359, diff 0.1253815427665277
>> >> Iteration 9: loss 0.4045020086612566, diff 0.26907321376758075
>> >> Iteration 10: loss 0.3078824990823728, diff 0.23885980452569627
>> >>
>> >> Breeze:
>> >> Iteration -1: loss 1.3862943611198926, diff 1.0
>> >> Mar 6, 2014 3:59:11 PM com.github.fommil.netlib.BLAS <clinit>
>> >> WARNING: Failed to load implementation from:
>> >> com.github.fommil.netlib.NativeSystemBLAS
>> >> Mar 6, 2014 3:59:11 PM com.github.fommil.netlib.BLAS <clinit>
>> >> WARNING: Failed to load implementation from:
>> >> com.github.fommil.netlib.NativeRefBLAS
>> >> Iteration 0: loss 1.3862943611198926, diff 0.0
>> >> Iteration 1: loss 1.5846343143210866, diff 0.14307193024217352
>> >> Iteration 2: loss 1.1242501524477688, diff 0.29053004039012126
>> >> Iteration 3: loss 1.1242501524477688, diff 0.0
>> >> Iteration 4: loss 1.1242501524477688, diff 0.0
>> >> Iteration 5: loss 1.0930151243303563, diff 0.027782962952189336
>> >> Iteration 6: loss 1.0930151243303563, diff 0.0
>> >> Iteration 7: loss 1.0930151243303563, diff 0.0
>> >> Iteration 8: loss 1.054036932835569, diff 0.03566113127440601
>> >> Iteration 9: loss 1.054036932835569, diff 0.0
>> >> Iteration 10: loss 1.054036932835569, diff 0.0
>> >> Iteration 11: loss 0.9907956302751622, diff 0.05999907649459571
>> >> Iteration 12: loss 0.9907956302751622, diff 0.0
>> >> Iteration 13: loss 0.9907956302751622, diff 0.0
>> >> Iteration 14: loss 0.9184205380342829, diff 0.07304737423337761
>> >> Iteration 15: loss 0.9184205380342829, diff 0.0
>> >> Iteration 16: loss 0.9184205380342829, diff 0.0
>> >> Iteration 17: loss 0.8259870936519939, diff 0.1006438117513297
>> >> Iteration 18: loss 0.8259870936519939, diff 0.0
>> >> Iteration 19: loss 0.8259870936519939, diff 0.0
>> >> Iteration 20: loss 0.6327447552109576, diff 0.233952934583647
>> >> Iteration 21: loss 0.6327447552109576, diff 0.0
>> >> Iteration 22: loss 0.6327447552109576, diff 0.0
>> >> Iteration 23: loss 0.5534101162436362, diff 0.12538154276652747
>> >> Iteration 24: loss 0.5534101162436362, diff 0.0
>> >> Iteration 25: loss 0.5534101162436362, diff 0.0
>> >> Iteration 26: loss 0.40450200866125635, diff 0.2690732137675816
>> >> Iteration 27: loss 0.40450200866125635, diff 0.0
>> >> Iteration 28: loss 0.40450200866125635, diff 0.0
>> >> Iteration 29: loss 0.30788249908237314, diff 0.23885980452569502
>> >>
>> >> Sincerely,
>> >>
>> >> DB Tsai
>> >> Machine Learning Engineer
>> >> Alpine Data Labs
>> >> --------------------------------------
>> >> Web: http://alpinenow.com/
>> >>
>> >>
>> >> On Wed, Mar 5, 2014 at 2:00 PM, David Hall <dlwh@cs.berkeley.edu>
>> >> wrote:
>> >> > On Wed, Mar 5, 2014 at 1:57 PM, DB Tsai <dbtsai@alpinenow.com> wrote:
>> >> >
>> >> >> Hi David,
>> >> >>
>> >> >> On Tue, Mar 4, 2014 at 8:13 PM, dlwh <david.lw.hall@gmail.com>
>> >> >> wrote:
>> >> >> > I'm happy to help fix any problems. I've verified at points that
>> >> >> > the
>> >> >> > implementation gives the exact same sequence of iterates for a few
>> >> >> different
>> >> >> > functions (with a particular line search) as the c port of lbfgs.
>> >> >> > So
>> >> I'm
>> >> >> a
>> >> >> > little surprised it fails where Fortran succeeds... but only a
>> >> >> > little.
>> >> >> This
>> >> >> > was fixed late last year.
>> >> >> I'm working on a reproducible test case using breeze vs fortran
>> >> >> implementation to show the problem I've run into. The test will be
>> >> >> in
>> >> >> one of the test cases in my Spark fork, is it okay for you to
>> >> >> investigate the issue? Or do I need to make it as a standalone test?
>> >> >>
>> >> >
>> >> >
>> >> > Um, as long as it wouldn't be too hard to pull out.
>> >> >
>> >> >
>> >> >>
>> >> >> Will send you the test later today.
>> >> >>
>> >> >> Thanks.
>> >> >>
>> >> >> Sincerely,
>> >> >>
>> >> >> DB Tsai
>> >> >> Machine Learning Engineer
>> >> >> Alpine Data Labs
>> >> >> --------------------------------------
>> >> >> Web: http://alpinenow.com/
>> >> >>
>> >>
>
>
>

From dev-return-7267-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 04:45:43 2014
Return-Path: <dev-return-7267-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7935C10C22
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 04:45:43 +0000 (UTC)
Received: (qmail 5711 invoked by uid 500); 8 Apr 2014 04:45:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5100 invoked by uid 500); 8 Apr 2014 04:45:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5092 invoked by uid 99); 8 Apr 2014 04:45:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 04:45:39 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mukgbv@gmail.com designates 74.125.82.171 as permitted sender)
Received: from [74.125.82.171] (HELO mail-we0-f171.google.com) (74.125.82.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 04:45:35 +0000
Received: by mail-we0-f171.google.com with SMTP id t61so371546wes.2
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 21:45:14 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=CMkQPgu8YUvhlFSvJMKsmdMY5Wg5AAtfdGeYyivSgSY=;
        b=RBhHSxqJXqfqWUB/A4wNJzru4HTwDcUN8AVPTt8cQHhD1QT2tCDxMXw3SuQ3R9ODkN
         LYHIlz0Lqt1+PWd+Hd1wyjGjABfuPc7ijF0J0CH1esKBEv20wNk4h8Niwp5TeBlYTzla
         pK/fg6CxJwZfRxF7NJOoevIScvorg6BqzCF+fmiXgkWckW4iUKmI7RPmsEwYtPgdexD8
         L/1PClxJujrQ611u2tSFmV6kIq9g7VH+Hae7pcVXzElOmFY0a7pujC4ehxiN1rzpy6nm
         MnoqnQlhykJL+ETiRXGMD1zu0ZJpo9g2+JRoqh030O00YigK0pe8QTAdzzsRgLvSNxOz
         MKKw==
MIME-Version: 1.0
X-Received: by 10.194.60.114 with SMTP id g18mr381352wjr.61.1396932314494;
 Mon, 07 Apr 2014 21:45:14 -0700 (PDT)
Received: by 10.180.77.135 with HTTP; Mon, 7 Apr 2014 21:45:14 -0700 (PDT)
In-Reply-To: <CAJHwgdshcGJLNjT118A79OWg6Gb_qaaNvoQZiw1WcQgQfinS3g@mail.gmail.com>
References: <CAGRER0deRfjEdguW5tjJPFBSuiap2P9-q21WBZyNyVbL4onHfA@mail.gmail.com>
	<CAJHwgdshcGJLNjT118A79OWg6Gb_qaaNvoQZiw1WcQgQfinS3g@mail.gmail.com>
Date: Tue, 8 Apr 2014 10:15:14 +0530
Message-ID: <CAGRER0c_tMqMep23RsXDBZGNbU0=ZgedsSzh3BELYfhaUFabnw@mail.gmail.com>
Subject: Re: Contributing to Spark
From: Mukesh G <mukgbv@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b66f33d6179e304f680a665
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b66f33d6179e304f680a665
Content-Type: text/plain; charset=ISO-8859-1

Hi Sujeet,

    Thanks. I went thru the website and looks great. Is there a list of
items that I can choose from, for contribution?

Thanks

Mukesh


On Mon, Apr 7, 2014 at 10:14 PM, Sujeet Varakhedi
<svarakhedi@gopivotal.com>wrote:

> This is a good place to start:
> https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
>
> Sujeet
>
>
> On Mon, Apr 7, 2014 at 9:20 AM, Mukesh G <mukgbv@gmail.com> wrote:
>
> > Hi,
> >
> >    How I contribute to Spark and it's associated projects?
> >
> > Appreciate the help...
> >
> > Thanks
> >
> > Mukesh
> >
>

--047d7b66f33d6179e304f680a665--

From dev-return-7268-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 05:22:58 2014
Return-Path: <dev-return-7268-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8D22210CEF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 05:22:58 +0000 (UTC)
Received: (qmail 42339 invoked by uid 500); 8 Apr 2014 05:22:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42117 invoked by uid 500); 8 Apr 2014 05:22:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42108 invoked by uid 99); 8 Apr 2014 05:22:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 05:22:55 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.42 as permitted sender)
Received: from [209.85.160.42] (HELO mail-pb0-f42.google.com) (209.85.160.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 05:22:50 +0000
Received: by mail-pb0-f42.google.com with SMTP id rr13so511073pbb.15
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 22:22:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=Eejh6jG1JuXCjr8eiw6Pq07gwv6R7Cita1IwqLZgFwo=;
        b=od8ijS3kov42B3H1B0wkoqQ7i981ZiQQwt6cYDQNKg3ES7qWmPhiGAKSxuUHkpwww6
         he8YWlqLMMZ72DaAPgjAPFH4Pomvfyqs44qUlqEs3p0U/yDQ+tefoIDn3+CVVE9K59Ic
         cyS7y8TZHs2h/VPet8MHGcok9PIVP+Jtqc7WvpK+Bc16PDd7N+57CWObs34d+rUrP72U
         gQu9NXvXcNUcAiw/YQIJ5O7f1P507wVgpasZzsHpfKvfdwKBmUWxP/gT6Lno46ynlVIq
         VS0tOthbBP2gezJ/mgvyklCCVNWI8/1++uBZQR9ML713QGMSinFAqHrKsynJZO9RM+cM
         beTw==
X-Received: by 10.68.237.38 with SMTP id uz6mr2047729pbc.72.1396934547682;
        Mon, 07 Apr 2014 22:22:27 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id vb7sm1796137pbc.13.2014.04.07.22.22.23
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 07 Apr 2014 22:22:25 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: Contributing to Spark
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAGRER0c_tMqMep23RsXDBZGNbU0=ZgedsSzh3BELYfhaUFabnw@mail.gmail.com>
Date: Mon, 7 Apr 2014 22:22:21 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <3A236731-EDDE-4B26-A8A8-5E66C287687E@gmail.com>
References: <CAGRER0deRfjEdguW5tjJPFBSuiap2P9-q21WBZyNyVbL4onHfA@mail.gmail.com> <CAJHwgdshcGJLNjT118A79OWg6Gb_qaaNvoQZiw1WcQgQfinS3g@mail.gmail.com> <CAGRER0c_tMqMep23RsXDBZGNbU0=ZgedsSzh3BELYfhaUFabnw@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

I=92d suggest looking for the issues labeled =93Starter=94 on JIRA. You =
can find them here: =
https://issues.apache.org/jira/browse/SPARK-1438?jql=3Dproject%20%3D%20SPA=
RK%20AND%20labels%20%3D%20Starter%20AND%20status%20in%20(Open%2C%20%22In%2=
0Progress%22%2C%20Reopened)

Matei

On Apr 7, 2014, at 9:45 PM, Mukesh G <mukgbv@gmail.com> wrote:

> Hi Sujeet,
>=20
>    Thanks. I went thru the website and looks great. Is there a list of
> items that I can choose from, for contribution?
>=20
> Thanks
>=20
> Mukesh
>=20
>=20
> On Mon, Apr 7, 2014 at 10:14 PM, Sujeet Varakhedi
> <svarakhedi@gopivotal.com>wrote:
>=20
>> This is a good place to start:
>> =
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
>>=20
>> Sujeet
>>=20
>>=20
>> On Mon, Apr 7, 2014 at 9:20 AM, Mukesh G <mukgbv@gmail.com> wrote:
>>=20
>>> Hi,
>>>=20
>>>   How I contribute to Spark and it's associated projects?
>>>=20
>>> Appreciate the help...
>>>=20
>>> Thanks
>>>=20
>>> Mukesh
>>>=20
>>=20


From dev-return-7269-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 06:23:48 2014
Return-Path: <dev-return-7269-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 74A4D10E60
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 06:23:48 +0000 (UTC)
Received: (qmail 7312 invoked by uid 500); 8 Apr 2014 06:23:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7253 invoked by uid 500); 8 Apr 2014 06:23:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7244 invoked by uid 99); 8 Apr 2014 06:23:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 06:23:44 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.214.170 as permitted sender)
Received: from [209.85.214.170] (HELO mail-ob0-f170.google.com) (209.85.214.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 06:23:41 +0000
Received: by mail-ob0-f170.google.com with SMTP id uz6so524979obc.29
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 23:23:20 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=ik7IRmb/8cSFG9lOdHkj33J3o8JESc7h7MGaeeJBL2I=;
        b=wC8/ynZnfhbyJkglPSU3kHHZB2ioYnv8rt6+X/tubGo9rpKpVffnsb0ND028BvE3u2
         MlJFK7tOWmjoxpk3EN57NLiF8vu4dc7ihs8klfyjNQzwMM9MOPHasNYbqIucI2YqqG3+
         yoV5B2LO9jXT9m+lEC+8H5UQ+i4/mvNT4jHZmZcxMzjuzgWivX6IBl51uQZkxO0d2vtn
         e6VRcT4RH9IcPEAbQ/wLsOrIAhWjE2MYxVw1f8h/cLOqztkhikmbDprms1UEMNU8vQR9
         cfVx+XZpt0h7tZo1zEgRK1ZpVTnNK4+DF4w12I76SvCHDJ8p6G/q5dIyZ7ySkAHR2Fgt
         Xgeg==
MIME-Version: 1.0
X-Received: by 10.182.144.194 with SMTP id so2mr1634507obb.31.1396938200349;
 Mon, 07 Apr 2014 23:23:20 -0700 (PDT)
Received: by 10.182.142.163 with HTTP; Mon, 7 Apr 2014 23:23:20 -0700 (PDT)
In-Reply-To: <CAEYYnxbo10PHJsY=s4s2f-3PsSed-F+9zmsz35ELGSfGVhnuHg@mail.gmail.com>
References: <CAFkVJifBb14oGvE4zHSL0=s1na9neiKVRGRWmUC24wWnP_9Miw@mail.gmail.com>
	<CAEYYnxbo10PHJsY=s4s2f-3PsSed-F+9zmsz35ELGSfGVhnuHg@mail.gmail.com>
Date: Mon, 7 Apr 2014 23:23:20 -0700
Message-ID: <CA+B-+fy-Cr9f4OZ4ztuqSnLNKwUMYHKN7GOg1nU2dmFO87rSjA@mail.gmail.com>
Subject: Re: MLLib - Thoughts about refactoring Updater for LBFGS?
From: Debasish Das <debasish.das83@gmail.com>
To: DB Tsai <dbtsai@stanford.edu>
Cc: dlwh@cs.berkeley.edu, mengxr@gmail.com, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0158ab963482f204f682057a
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158ab963482f204f682057a
Content-Type: text/plain; charset=ISO-8859-1

I got your checkin....I need to run logistic regression SGD vs BFGS for my
current usecases but your next checkin will update the logistic regression
with LBFGS right ? Are you adding it to regression package as well ?

Thanks.
Deb


On Mon, Apr 7, 2014 at 7:00 PM, DB Tsai <dbtsai@stanford.edu> wrote:

> Hi guys,
>
> The latest PR uses Breeze's L-BFGS implement which is introduced by
> Xiangrui's sparse input format work in SPARK-1212.
>
> https://github.com/apache/spark/pull/353
>
> Now, it works with the new sparse framework!
>
> Any feedback would be greatly appreciated.
>
> Thanks.
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Thu, Apr 3, 2014 at 5:02 PM, DB Tsai <dbtsai@alpinenow.com> wrote:
> > ---------- Forwarded message ----------
> > From: David Hall <dlwh@cs.berkeley.edu>
> > Date: Sat, Mar 15, 2014 at 10:02 AM
> > Subject: Re: MLLib - Thoughts about refactoring Updater for LBFGS?
> > To: DB Tsai <dbtsai@alpinenow.com>
> >
> >
> > On Fri, Mar 7, 2014 at 10:56 PM, DB Tsai <dbtsai@alpinenow.com> wrote:
> >>
> >> Hi David,
> >>
> >> Please let me know the version of Breeze that LBFGS can be serialized,
> >> and CachedDiffFunction is built-in in LBFGS once you finish. I'll
> >> update the PR to Spark from using RISO implementation to Breeze
> >> implementation.
> >
> >
> > The current master (0.7-SNAPSHOT) has these problems fixed.
> >
> >>
> >>
> >> Thanks.
> >>
> >> Sincerely,
> >>
> >> DB Tsai
> >> Machine Learning Engineer
> >> Alpine Data Labs
> >> --------------------------------------
> >> Web: http://alpinenow.com/
> >>
> >>
> >> On Thu, Mar 6, 2014 at 4:26 PM, David Hall <dlwh@cs.berkeley.edu>
> wrote:
> >> > On Thu, Mar 6, 2014 at 4:21 PM, DB Tsai <dbtsai@alpinenow.com> wrote:
> >> >
> >> >> Hi David,
> >> >>
> >> >> I can converge to the same result with your breeze LBFGS and Fortran
> >> >> implementations now. Probably, I made some mistakes when I tried
> >> >> breeze before. I apologize that I claimed it's not stable.
> >> >>
> >> >> See the test case in BreezeLBFGSSuite.scala
> >> >> https://github.com/AlpineNow/spark/tree/dbtsai-breezeLBFGS
> >> >>
> >> >> This is training multinomial logistic regression against iris
> dataset,
> >> >> and both optimizers can train the models with 98% training accuracy.
> >> >>
> >> >
> >> > great to hear! There were some bugs in LBFGS about 6 months ago, so
> >> > depending on the last time you tried it, it might indeed have been
> >> > bugged.
> >> >
> >> >
> >> >>
> >> >> There are two issues to use Breeze in Spark,
> >> >>
> >> >> 1) When the gradientSum and lossSum are computed distributively in
> >> >> custom defined DiffFunction which will be passed into your optimizer,
> >> >> Spark will complain LBFGS class is not serializable. In
> >> >> BreezeLBFGS.scala, I've to convert RDD to array to make it work
> >> >> locally. It should be easy to fix by just having LBFGS to implement
> >> >> Serializable.
> >> >>
> >> >
> >> > I'm not sure why Spark should be serializing LBFGS? Shouldn't it live
> on
> >> > the controller node? Or is this a per-node thing?
> >> >
> >> > But no problem to make it serializable.
> >> >
> >> >
> >> >>
> >> >> 2) Breeze computes redundant gradient and loss. See the following log
> >> >> from both Fortran and Breeze implementations.
> >> >>
> >> >
> >> > Err, yeah. I should probably have LBFGS do this automatically, but
> >> > there's
> >> > a CachedDiffFunction that gets rid of the redundant calculations.
> >> >
> >> > -- David
> >> >
> >> >
> >> >>
> >> >> Thanks.
> >> >>
> >> >> Fortran:
> >> >> Iteration -1: loss 1.3862943611198926, diff 1.0
> >> >> Iteration 0: loss 1.5846343143210866, diff 0.14307193024217352
> >> >> Iteration 1: loss 1.1242501524477688, diff 0.29053004039012126
> >> >> Iteration 2: loss 1.0930151243303563, diff 0.027782962952189336
> >> >> Iteration 3: loss 1.054036932835569, diff 0.03566113127440601
> >> >> Iteration 4: loss 0.9907956302751622, diff 0.05999907649459571
> >> >> Iteration 5: loss 0.9184205380342829, diff 0.07304737423337761
> >> >> Iteration 6: loss 0.8259870936519937, diff 0.10064381175132982
> >> >> Iteration 7: loss 0.6327447552109574, diff 0.23395293458364716
> >> >> Iteration 8: loss 0.5534101162436359, diff 0.1253815427665277
> >> >> Iteration 9: loss 0.4045020086612566, diff 0.26907321376758075
> >> >> Iteration 10: loss 0.3078824990823728, diff 0.23885980452569627
> >> >>
> >> >> Breeze:
> >> >> Iteration -1: loss 1.3862943611198926, diff 1.0
> >> >> Mar 6, 2014 3:59:11 PM com.github.fommil.netlib.BLAS <clinit>
> >> >> WARNING: Failed to load implementation from:
> >> >> com.github.fommil.netlib.NativeSystemBLAS
> >> >> Mar 6, 2014 3:59:11 PM com.github.fommil.netlib.BLAS <clinit>
> >> >> WARNING: Failed to load implementation from:
> >> >> com.github.fommil.netlib.NativeRefBLAS
> >> >> Iteration 0: loss 1.3862943611198926, diff 0.0
> >> >> Iteration 1: loss 1.5846343143210866, diff 0.14307193024217352
> >> >> Iteration 2: loss 1.1242501524477688, diff 0.29053004039012126
> >> >> Iteration 3: loss 1.1242501524477688, diff 0.0
> >> >> Iteration 4: loss 1.1242501524477688, diff 0.0
> >> >> Iteration 5: loss 1.0930151243303563, diff 0.027782962952189336
> >> >> Iteration 6: loss 1.0930151243303563, diff 0.0
> >> >> Iteration 7: loss 1.0930151243303563, diff 0.0
> >> >> Iteration 8: loss 1.054036932835569, diff 0.03566113127440601
> >> >> Iteration 9: loss 1.054036932835569, diff 0.0
> >> >> Iteration 10: loss 1.054036932835569, diff 0.0
> >> >> Iteration 11: loss 0.9907956302751622, diff 0.05999907649459571
> >> >> Iteration 12: loss 0.9907956302751622, diff 0.0
> >> >> Iteration 13: loss 0.9907956302751622, diff 0.0
> >> >> Iteration 14: loss 0.9184205380342829, diff 0.07304737423337761
> >> >> Iteration 15: loss 0.9184205380342829, diff 0.0
> >> >> Iteration 16: loss 0.9184205380342829, diff 0.0
> >> >> Iteration 17: loss 0.8259870936519939, diff 0.1006438117513297
> >> >> Iteration 18: loss 0.8259870936519939, diff 0.0
> >> >> Iteration 19: loss 0.8259870936519939, diff 0.0
> >> >> Iteration 20: loss 0.6327447552109576, diff 0.233952934583647
> >> >> Iteration 21: loss 0.6327447552109576, diff 0.0
> >> >> Iteration 22: loss 0.6327447552109576, diff 0.0
> >> >> Iteration 23: loss 0.5534101162436362, diff 0.12538154276652747
> >> >> Iteration 24: loss 0.5534101162436362, diff 0.0
> >> >> Iteration 25: loss 0.5534101162436362, diff 0.0
> >> >> Iteration 26: loss 0.40450200866125635, diff 0.2690732137675816
> >> >> Iteration 27: loss 0.40450200866125635, diff 0.0
> >> >> Iteration 28: loss 0.40450200866125635, diff 0.0
> >> >> Iteration 29: loss 0.30788249908237314, diff 0.23885980452569502
> >> >>
> >> >> Sincerely,
> >> >>
> >> >> DB Tsai
> >> >> Machine Learning Engineer
> >> >> Alpine Data Labs
> >> >> --------------------------------------
> >> >> Web: http://alpinenow.com/
> >> >>
> >> >>
> >> >> On Wed, Mar 5, 2014 at 2:00 PM, David Hall <dlwh@cs.berkeley.edu>
> >> >> wrote:
> >> >> > On Wed, Mar 5, 2014 at 1:57 PM, DB Tsai <dbtsai@alpinenow.com>
> wrote:
> >> >> >
> >> >> >> Hi David,
> >> >> >>
> >> >> >> On Tue, Mar 4, 2014 at 8:13 PM, dlwh <david.lw.hall@gmail.com>
> >> >> >> wrote:
> >> >> >> > I'm happy to help fix any problems. I've verified at points that
> >> >> >> > the
> >> >> >> > implementation gives the exact same sequence of iterates for a
> few
> >> >> >> different
> >> >> >> > functions (with a particular line search) as the c port of
> lbfgs.
> >> >> >> > So
> >> >> I'm
> >> >> >> a
> >> >> >> > little surprised it fails where Fortran succeeds... but only a
> >> >> >> > little.
> >> >> >> This
> >> >> >> > was fixed late last year.
> >> >> >> I'm working on a reproducible test case using breeze vs fortran
> >> >> >> implementation to show the problem I've run into. The test will be
> >> >> >> in
> >> >> >> one of the test cases in my Spark fork, is it okay for you to
> >> >> >> investigate the issue? Or do I need to make it as a standalone
> test?
> >> >> >>
> >> >> >
> >> >> >
> >> >> > Um, as long as it wouldn't be too hard to pull out.
> >> >> >
> >> >> >
> >> >> >>
> >> >> >> Will send you the test later today.
> >> >> >>
> >> >> >> Thanks.
> >> >> >>
> >> >> >> Sincerely,
> >> >> >>
> >> >> >> DB Tsai
> >> >> >> Machine Learning Engineer
> >> >> >> Alpine Data Labs
> >> >> >> --------------------------------------
> >> >> >> Web: http://alpinenow.com/
> >> >> >>
> >> >>
> >
> >
> >
>

--089e0158ab963482f204f682057a--

From dev-return-7270-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 06:42:29 2014
Return-Path: <dev-return-7270-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6BF4510EAA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 06:42:29 +0000 (UTC)
Received: (qmail 24962 invoked by uid 500); 8 Apr 2014 06:42:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24531 invoked by uid 500); 8 Apr 2014 06:42:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24160 invoked by uid 99); 8 Apr 2014 06:42:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 06:42:21 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.219.53 as permitted sender)
Received: from [209.85.219.53] (HELO mail-oa0-f53.google.com) (209.85.219.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 06:42:16 +0000
Received: by mail-oa0-f53.google.com with SMTP id j17so529778oag.26
        for <dev@spark.apache.org>; Mon, 07 Apr 2014 23:41:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=9osjY6jAiyvfJXpBs8/wKIXGLZaKO249zz2sllyfWW4=;
        b=AoUMuFJjTCngEWA7qrnp9ftLUtfrjoMlsAfCkQFGy7Y1vQHqACYKLvGxS0gxuRb5mN
         6uM5qH1Oc9diD7ZTID2A9XBcGcmrBPLNuCksHbJtpH/eia15isOapDftPV21QfscEYdu
         +UmTORntliWMEIApEtJ94etoiKP79UHJBmSdOXp0SU1pfWn6AJvb33Qdn+aRKyClHWNY
         a/FX2NQJz8OG1dHdi+xMDf0P0osS4D37uHNjqiRqBZlaH0jVY2FLrL5nrx2fa5nkVMBH
         ZwW4IAOcxkZA/XwIwS+NSgQXz+17x6dZFHEXeDWPm9WbeODrCfI2cT63qziA+Y1Vh9ET
         aTng==
MIME-Version: 1.0
X-Received: by 10.60.55.97 with SMTP id r1mr1761985oep.5.1396939316143; Mon,
 07 Apr 2014 23:41:56 -0700 (PDT)
Received: by 10.182.142.163 with HTTP; Mon, 7 Apr 2014 23:41:56 -0700 (PDT)
In-Reply-To: <CA+B-+fy-Cr9f4OZ4ztuqSnLNKwUMYHKN7GOg1nU2dmFO87rSjA@mail.gmail.com>
References: <CAFkVJifBb14oGvE4zHSL0=s1na9neiKVRGRWmUC24wWnP_9Miw@mail.gmail.com>
	<CAEYYnxbo10PHJsY=s4s2f-3PsSed-F+9zmsz35ELGSfGVhnuHg@mail.gmail.com>
	<CA+B-+fy-Cr9f4OZ4ztuqSnLNKwUMYHKN7GOg1nU2dmFO87rSjA@mail.gmail.com>
Date: Mon, 7 Apr 2014 23:41:56 -0700
Message-ID: <CA+B-+fzvBLfJXhMbvRbwK05YPJPyavsUE3ktJN7mc_75q0UKAg@mail.gmail.com>
Subject: Re: MLLib - Thoughts about refactoring Updater for LBFGS?
From: Debasish Das <debasish.das83@gmail.com>
To: DB Tsai <dbtsai@stanford.edu>
Cc: dlwh@cs.berkeley.edu, mengxr@gmail.com, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0115f08ab62a7304f68247a9
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0115f08ab62a7304f68247a9
Content-Type: text/plain; charset=ISO-8859-1

By the way...what's the idea...the labeled data set is a RDD which is
cached on all nodes..

The bfgs solver is maintained on the master or each worker is supposed to
maintain it's own bfgs...


On Mon, Apr 7, 2014 at 11:23 PM, Debasish Das <debasish.das83@gmail.com>wrote:

> I got your checkin....I need to run logistic regression SGD vs BFGS for my
> current usecases but your next checkin will update the logistic regression
> with LBFGS right ? Are you adding it to regression package as well ?
>
> Thanks.
> Deb
>
>
> On Mon, Apr 7, 2014 at 7:00 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>
>> Hi guys,
>>
>> The latest PR uses Breeze's L-BFGS implement which is introduced by
>> Xiangrui's sparse input format work in SPARK-1212.
>>
>> https://github.com/apache/spark/pull/353
>>
>> Now, it works with the new sparse framework!
>>
>> Any feedback would be greatly appreciated.
>>
>> Thanks.
>>
>> Sincerely,
>>
>> DB Tsai
>> -------------------------------------------------------
>> My Blog: https://www.dbtsai.com
>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>
>>
>> On Thu, Apr 3, 2014 at 5:02 PM, DB Tsai <dbtsai@alpinenow.com> wrote:
>> > ---------- Forwarded message ----------
>> > From: David Hall <dlwh@cs.berkeley.edu>
>> > Date: Sat, Mar 15, 2014 at 10:02 AM
>> > Subject: Re: MLLib - Thoughts about refactoring Updater for LBFGS?
>> > To: DB Tsai <dbtsai@alpinenow.com>
>> >
>> >
>> > On Fri, Mar 7, 2014 at 10:56 PM, DB Tsai <dbtsai@alpinenow.com> wrote:
>> >>
>> >> Hi David,
>> >>
>> >> Please let me know the version of Breeze that LBFGS can be serialized,
>> >> and CachedDiffFunction is built-in in LBFGS once you finish. I'll
>> >> update the PR to Spark from using RISO implementation to Breeze
>> >> implementation.
>> >
>> >
>> > The current master (0.7-SNAPSHOT) has these problems fixed.
>> >
>> >>
>> >>
>> >> Thanks.
>> >>
>> >> Sincerely,
>> >>
>> >> DB Tsai
>> >> Machine Learning Engineer
>> >> Alpine Data Labs
>> >> --------------------------------------
>> >> Web: http://alpinenow.com/
>> >>
>> >>
>> >> On Thu, Mar 6, 2014 at 4:26 PM, David Hall <dlwh@cs.berkeley.edu>
>> wrote:
>> >> > On Thu, Mar 6, 2014 at 4:21 PM, DB Tsai <dbtsai@alpinenow.com>
>> wrote:
>> >> >
>> >> >> Hi David,
>> >> >>
>> >> >> I can converge to the same result with your breeze LBFGS and Fortran
>> >> >> implementations now. Probably, I made some mistakes when I tried
>> >> >> breeze before. I apologize that I claimed it's not stable.
>> >> >>
>> >> >> See the test case in BreezeLBFGSSuite.scala
>> >> >> https://github.com/AlpineNow/spark/tree/dbtsai-breezeLBFGS
>> >> >>
>> >> >> This is training multinomial logistic regression against iris
>> dataset,
>> >> >> and both optimizers can train the models with 98% training accuracy.
>> >> >>
>> >> >
>> >> > great to hear! There were some bugs in LBFGS about 6 months ago, so
>> >> > depending on the last time you tried it, it might indeed have been
>> >> > bugged.
>> >> >
>> >> >
>> >> >>
>> >> >> There are two issues to use Breeze in Spark,
>> >> >>
>> >> >> 1) When the gradientSum and lossSum are computed distributively in
>> >> >> custom defined DiffFunction which will be passed into your
>> optimizer,
>> >> >> Spark will complain LBFGS class is not serializable. In
>> >> >> BreezeLBFGS.scala, I've to convert RDD to array to make it work
>> >> >> locally. It should be easy to fix by just having LBFGS to implement
>> >> >> Serializable.
>> >> >>
>> >> >
>> >> > I'm not sure why Spark should be serializing LBFGS? Shouldn't it
>> live on
>> >> > the controller node? Or is this a per-node thing?
>> >> >
>> >> > But no problem to make it serializable.
>> >> >
>> >> >
>> >> >>
>> >> >> 2) Breeze computes redundant gradient and loss. See the following
>> log
>> >> >> from both Fortran and Breeze implementations.
>> >> >>
>> >> >
>> >> > Err, yeah. I should probably have LBFGS do this automatically, but
>> >> > there's
>> >> > a CachedDiffFunction that gets rid of the redundant calculations.
>> >> >
>> >> > -- David
>> >> >
>> >> >
>> >> >>
>> >> >> Thanks.
>> >> >>
>> >> >> Fortran:
>> >> >> Iteration -1: loss 1.3862943611198926, diff 1.0
>> >> >> Iteration 0: loss 1.5846343143210866, diff 0.14307193024217352
>> >> >> Iteration 1: loss 1.1242501524477688, diff 0.29053004039012126
>> >> >> Iteration 2: loss 1.0930151243303563, diff 0.027782962952189336
>> >> >> Iteration 3: loss 1.054036932835569, diff 0.03566113127440601
>> >> >> Iteration 4: loss 0.9907956302751622, diff 0.05999907649459571
>> >> >> Iteration 5: loss 0.9184205380342829, diff 0.07304737423337761
>> >> >> Iteration 6: loss 0.8259870936519937, diff 0.10064381175132982
>> >> >> Iteration 7: loss 0.6327447552109574, diff 0.23395293458364716
>> >> >> Iteration 8: loss 0.5534101162436359, diff 0.1253815427665277
>> >> >> Iteration 9: loss 0.4045020086612566, diff 0.26907321376758075
>> >> >> Iteration 10: loss 0.3078824990823728, diff 0.23885980452569627
>> >> >>
>> >> >> Breeze:
>> >> >> Iteration -1: loss 1.3862943611198926, diff 1.0
>> >> >> Mar 6, 2014 3:59:11 PM com.github.fommil.netlib.BLAS <clinit>
>> >> >> WARNING: Failed to load implementation from:
>> >> >> com.github.fommil.netlib.NativeSystemBLAS
>> >> >> Mar 6, 2014 3:59:11 PM com.github.fommil.netlib.BLAS <clinit>
>> >> >> WARNING: Failed to load implementation from:
>> >> >> com.github.fommil.netlib.NativeRefBLAS
>> >> >> Iteration 0: loss 1.3862943611198926, diff 0.0
>> >> >> Iteration 1: loss 1.5846343143210866, diff 0.14307193024217352
>> >> >> Iteration 2: loss 1.1242501524477688, diff 0.29053004039012126
>> >> >> Iteration 3: loss 1.1242501524477688, diff 0.0
>> >> >> Iteration 4: loss 1.1242501524477688, diff 0.0
>> >> >> Iteration 5: loss 1.0930151243303563, diff 0.027782962952189336
>> >> >> Iteration 6: loss 1.0930151243303563, diff 0.0
>> >> >> Iteration 7: loss 1.0930151243303563, diff 0.0
>> >> >> Iteration 8: loss 1.054036932835569, diff 0.03566113127440601
>> >> >> Iteration 9: loss 1.054036932835569, diff 0.0
>> >> >> Iteration 10: loss 1.054036932835569, diff 0.0
>> >> >> Iteration 11: loss 0.9907956302751622, diff 0.05999907649459571
>> >> >> Iteration 12: loss 0.9907956302751622, diff 0.0
>> >> >> Iteration 13: loss 0.9907956302751622, diff 0.0
>> >> >> Iteration 14: loss 0.9184205380342829, diff 0.07304737423337761
>> >> >> Iteration 15: loss 0.9184205380342829, diff 0.0
>> >> >> Iteration 16: loss 0.9184205380342829, diff 0.0
>> >> >> Iteration 17: loss 0.8259870936519939, diff 0.1006438117513297
>> >> >> Iteration 18: loss 0.8259870936519939, diff 0.0
>> >> >> Iteration 19: loss 0.8259870936519939, diff 0.0
>> >> >> Iteration 20: loss 0.6327447552109576, diff 0.233952934583647
>> >> >> Iteration 21: loss 0.6327447552109576, diff 0.0
>> >> >> Iteration 22: loss 0.6327447552109576, diff 0.0
>> >> >> Iteration 23: loss 0.5534101162436362, diff 0.12538154276652747
>> >> >> Iteration 24: loss 0.5534101162436362, diff 0.0
>> >> >> Iteration 25: loss 0.5534101162436362, diff 0.0
>> >> >> Iteration 26: loss 0.40450200866125635, diff 0.2690732137675816
>> >> >> Iteration 27: loss 0.40450200866125635, diff 0.0
>> >> >> Iteration 28: loss 0.40450200866125635, diff 0.0
>> >> >> Iteration 29: loss 0.30788249908237314, diff 0.23885980452569502
>> >> >>
>> >> >> Sincerely,
>> >> >>
>> >> >> DB Tsai
>> >> >> Machine Learning Engineer
>> >> >> Alpine Data Labs
>> >> >> --------------------------------------
>> >> >> Web: http://alpinenow.com/
>> >> >>
>> >> >>
>> >> >> On Wed, Mar 5, 2014 at 2:00 PM, David Hall <dlwh@cs.berkeley.edu>
>> >> >> wrote:
>> >> >> > On Wed, Mar 5, 2014 at 1:57 PM, DB Tsai <dbtsai@alpinenow.com>
>> wrote:
>> >> >> >
>> >> >> >> Hi David,
>> >> >> >>
>> >> >> >> On Tue, Mar 4, 2014 at 8:13 PM, dlwh <david.lw.hall@gmail.com>
>> >> >> >> wrote:
>> >> >> >> > I'm happy to help fix any problems. I've verified at points
>> that
>> >> >> >> > the
>> >> >> >> > implementation gives the exact same sequence of iterates for a
>> few
>> >> >> >> different
>> >> >> >> > functions (with a particular line search) as the c port of
>> lbfgs.
>> >> >> >> > So
>> >> >> I'm
>> >> >> >> a
>> >> >> >> > little surprised it fails where Fortran succeeds... but only a
>> >> >> >> > little.
>> >> >> >> This
>> >> >> >> > was fixed late last year.
>> >> >> >> I'm working on a reproducible test case using breeze vs fortran
>> >> >> >> implementation to show the problem I've run into. The test will
>> be
>> >> >> >> in
>> >> >> >> one of the test cases in my Spark fork, is it okay for you to
>> >> >> >> investigate the issue? Or do I need to make it as a standalone
>> test?
>> >> >> >>
>> >> >> >
>> >> >> >
>> >> >> > Um, as long as it wouldn't be too hard to pull out.
>> >> >> >
>> >> >> >
>> >> >> >>
>> >> >> >> Will send you the test later today.
>> >> >> >>
>> >> >> >> Thanks.
>> >> >> >>
>> >> >> >> Sincerely,
>> >> >> >>
>> >> >> >> DB Tsai
>> >> >> >> Machine Learning Engineer
>> >> >> >> Alpine Data Labs
>> >> >> >> --------------------------------------
>> >> >> >> Web: http://alpinenow.com/
>> >> >> >>
>> >> >>
>> >
>> >
>> >
>>
>
>

--089e0115f08ab62a7304f68247a9--

From dev-return-7271-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 16:35:20 2014
Return-Path: <dev-return-7271-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 267D5112E6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 16:35:20 +0000 (UTC)
Received: (qmail 55232 invoked by uid 500); 8 Apr 2014 16:35:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54819 invoked by uid 500); 8 Apr 2014 16:35:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54809 invoked by uid 99); 8 Apr 2014 16:35:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 16:35:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ilikerps@gmail.com designates 209.85.216.50 as permitted sender)
Received: from [209.85.216.50] (HELO mail-qa0-f50.google.com) (209.85.216.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 16:35:12 +0000
Received: by mail-qa0-f50.google.com with SMTP id ih12so1227535qab.9
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 09:34:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=WumDc9Ulnag1a99wngdmhDNP+vGj+Lxcm4Iee3SGTHk=;
        b=dTcd8ZDhAhvoFEoPk11HO9YFwZB3bBpDkZ65qgZR+yFAjW1ydijYXfQltCxBRsrgA+
         cL4+PN0NeyKpNqRWN8HCsvjKg3QRQ8DUE/UpvZHacU7t9AUmMAVDuxR4CkvLnO9h7YYW
         2qUFqzUv39FabyawUBZ12qTjynV0fAsZoJzuaJNQBX1UvMl78jBjDiTzqx8yt+Z7kZde
         MGlSm7g+G6T4b7tMoKyXJt/n6v+d8HdFcamgoSSaGuqU3ooda03IDXQPJDvzYInIkOVr
         16Hpg43zQjf52nknb47lQNDlbr58tZD6yo4JF3fG+P46U1xM0wXWsKlGxPagvGb0cj4r
         2eFQ==
X-Received: by 10.140.93.198 with SMTP id d64mr5616576qge.1.1396974890617;
 Tue, 08 Apr 2014 09:34:50 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.37.170 with HTTP; Tue, 8 Apr 2014 09:34:30 -0700 (PDT)
In-Reply-To: <3A236731-EDDE-4B26-A8A8-5E66C287687E@gmail.com>
References: <CAGRER0deRfjEdguW5tjJPFBSuiap2P9-q21WBZyNyVbL4onHfA@mail.gmail.com>
 <CAJHwgdshcGJLNjT118A79OWg6Gb_qaaNvoQZiw1WcQgQfinS3g@mail.gmail.com>
 <CAGRER0c_tMqMep23RsXDBZGNbU0=ZgedsSzh3BELYfhaUFabnw@mail.gmail.com> <3A236731-EDDE-4B26-A8A8-5E66C287687E@gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Tue, 8 Apr 2014 09:34:30 -0700
Message-ID: <CANGvG8rYs_vPM8t_9Q3HvzTSnmYQWND06RCP5=b0H1iJsNNTyA@mail.gmail.com>
Subject: Re: Contributing to Spark
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11393ace1de21d04f68a9046
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11393ace1de21d04f68a9046
Content-Type: text/plain; charset=ISO-8859-1

Matei's link seems to point to a specific starter project as part of the
starter list, but here is the list itself:
https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20labels%20%3D%20Starter%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)


On Mon, Apr 7, 2014 at 10:22 PM, Matei Zaharia <matei.zaharia@gmail.com>wrote:

> I'd suggest looking for the issues labeled "Starter" on JIRA. You can find
> them here:
> https://issues.apache.org/jira/browse/SPARK-1438?jql=project%20%3D%20SPARK%20AND%20labels%20%3D%20Starter%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)
>
> Matei
>
> On Apr 7, 2014, at 9:45 PM, Mukesh G <mukgbv@gmail.com> wrote:
>
> > Hi Sujeet,
> >
> >    Thanks. I went thru the website and looks great. Is there a list of
> > items that I can choose from, for contribution?
> >
> > Thanks
> >
> > Mukesh
> >
> >
> > On Mon, Apr 7, 2014 at 10:14 PM, Sujeet Varakhedi
> > <svarakhedi@gopivotal.com>wrote:
> >
> >> This is a good place to start:
> >> https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
> >>
> >> Sujeet
> >>
> >>
> >> On Mon, Apr 7, 2014 at 9:20 AM, Mukesh G <mukgbv@gmail.com> wrote:
> >>
> >>> Hi,
> >>>
> >>>   How I contribute to Spark and it's associated projects?
> >>>
> >>> Appreciate the help...
> >>>
> >>> Thanks
> >>>
> >>> Mukesh
> >>>
> >>
>
>

--001a11393ace1de21d04f68a9046--

From dev-return-7272-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 16:38:02 2014
Return-Path: <dev-return-7272-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 27166112FD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 16:38:02 +0000 (UTC)
Received: (qmail 62639 invoked by uid 500); 8 Apr 2014 16:37:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62426 invoked by uid 500); 8 Apr 2014 16:37:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62418 invoked by uid 99); 8 Apr 2014 16:37:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 16:37:57 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.54 as permitted sender)
Received: from [209.85.160.54] (HELO mail-pb0-f54.google.com) (209.85.160.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 16:37:51 +0000
Received: by mail-pb0-f54.google.com with SMTP id ma3so1250623pbc.41
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 09:37:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=YS3ifo4ef0JGJBL2ku+eZxpcnKkgaRExTuJjjc+IplQ=;
        b=PQu3EmwJizI6r3JBnzGhYAcGHqnCwt7uGhZrZc+aCpOfE7NyVnXNrwVSBSkKk2bppG
         rxFaosKSsyy2BnmEK7l/H7vAcm4fXB0RrJwP38Wdmm788iLOY9orJv6iLT7ZlPGkJ+kn
         qlC62NPOsYna1cS/RV8R35l+qWTd6DSxCUoRbepzcDWlqRYjW/mWzOxum4SH1L3UenQM
         jfTUHNORcQSH3lpVHzObZ4ybY+KwyJWtAn/V8n5DqBK8svGF2dqmA1km/HYzF5/SdhjK
         8WbgRIKYc1lpsADGJI0HWnP7RPHlxZDoYHnU3/1b7GguTxvlM7yBmqLQTNVghb4kH+I7
         MTNA==
X-Received: by 10.68.226.35 with SMTP id rp3mr3877453pbc.73.1396975049050;
        Tue, 08 Apr 2014 09:37:29 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id yi3sm5635044pbb.51.2014.04.08.09.37.27
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 08 Apr 2014 09:37:27 -0700 (PDT)
Content-Type: text/plain; charset=iso-8859-1
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: Contributing to Spark
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CANGvG8rYs_vPM8t_9Q3HvzTSnmYQWND06RCP5=b0H1iJsNNTyA@mail.gmail.com>
Date: Tue, 8 Apr 2014 09:37:24 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <EDCA85D5-3E3F-44D6-BA7F-4DEEFD863A2D@gmail.com>
References: <CAGRER0deRfjEdguW5tjJPFBSuiap2P9-q21WBZyNyVbL4onHfA@mail.gmail.com> <CAJHwgdshcGJLNjT118A79OWg6Gb_qaaNvoQZiw1WcQgQfinS3g@mail.gmail.com> <CAGRER0c_tMqMep23RsXDBZGNbU0=ZgedsSzh3BELYfhaUFabnw@mail.gmail.com> <3A236731-EDDE-4B26-A8A8-5E66C287687E@gmail.com> <CANGvG8rYs_vPM8t_9Q3HvzTSnmYQWND06RCP5=b0H1iJsNNTyA@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

Shh, maybe I really wanted people to fix that one issue.

On Apr 8, 2014, at 9:34 AM, Aaron Davidson <ilikerps@gmail.com> wrote:

> Matei's link seems to point to a specific starter project as part of =
the
> starter list, but here is the list itself:
> =
https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%20SPARK%20AND%2=
0labels%20%3D%20Starter%20AND%20status%20in%20(Open%2C%20%22In%20Progress%=
22%2C%20Reopened)
>=20
>=20
> On Mon, Apr 7, 2014 at 10:22 PM, Matei Zaharia =
<matei.zaharia@gmail.com>wrote:
>=20
>> I'd suggest looking for the issues labeled "Starter" on JIRA. You can =
find
>> them here:
>> =
https://issues.apache.org/jira/browse/SPARK-1438?jql=3Dproject%20%3D%20SPA=
RK%20AND%20labels%20%3D%20Starter%20AND%20status%20in%20(Open%2C%20%22In%2=
0Progress%22%2C%20Reopened)
>>=20
>> Matei
>>=20
>> On Apr 7, 2014, at 9:45 PM, Mukesh G <mukgbv@gmail.com> wrote:
>>=20
>>> Hi Sujeet,
>>>=20
>>>   Thanks. I went thru the website and looks great. Is there a list =
of
>>> items that I can choose from, for contribution?
>>>=20
>>> Thanks
>>>=20
>>> Mukesh
>>>=20
>>>=20
>>> On Mon, Apr 7, 2014 at 10:14 PM, Sujeet Varakhedi
>>> <svarakhedi@gopivotal.com>wrote:
>>>=20
>>>> This is a good place to start:
>>>> =
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
>>>>=20
>>>> Sujeet
>>>>=20
>>>>=20
>>>> On Mon, Apr 7, 2014 at 9:20 AM, Mukesh G <mukgbv@gmail.com> wrote:
>>>>=20
>>>>> Hi,
>>>>>=20
>>>>>  How I contribute to Spark and it's associated projects?
>>>>>=20
>>>>> Appreciate the help...
>>>>>=20
>>>>> Thanks
>>>>>=20
>>>>> Mukesh
>>>>>=20
>>>>=20
>>=20
>>=20


From dev-return-7273-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 16:42:40 2014
Return-Path: <dev-return-7273-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 17A1E11311
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 16:42:40 +0000 (UTC)
Received: (qmail 69358 invoked by uid 500); 8 Apr 2014 16:42:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69128 invoked by uid 500); 8 Apr 2014 16:42:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69120 invoked by uid 99); 8 Apr 2014 16:42:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 16:42:37 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.214.177 as permitted sender)
Received: from [209.85.214.177] (HELO mail-ob0-f177.google.com) (209.85.214.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 16:42:33 +0000
Received: by mail-ob0-f177.google.com with SMTP id wo20so1261133obc.22
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 09:42:13 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=o7DtNl7dEOgg6iagj3Y9wx8PSvLqt1rPmvyXKDlqF6g=;
        b=seqHLVJnTmUCRMnJb6D5L/tPWwHyhdi+mQKpSZdoQKy0xqQu0StIlIxx70InOv9iQC
         AoKxqr7BlTacdG2EbhrNBl01F4kC5DHc8tB/TZlCBBKYmp7W+ehVJzOTp4VPlAYli7Zl
         UFt5fA8k4wjCw4GCPWlhhAsFM9uU6ejRUIS0DaTOulo/Mpzk/h+gg8Bh1toUzUqHUTty
         V7CimZBJCE8dVLUotN//qYqyspkgLDzwbr1Flgg3as7sIZd0rPsjSBT0vxqvctk5rygZ
         2HLPvtpgCDdelODZp4qKvwMk5UJm1V6vCkgdNDG+UyRfASIbGdNXWnOUU9UaNwVvrpr4
         FK+A==
MIME-Version: 1.0
X-Received: by 10.182.166.40 with SMTP id zd8mr4031970obb.25.1396975333085;
 Tue, 08 Apr 2014 09:42:13 -0700 (PDT)
Received: by 10.182.142.163 with HTTP; Tue, 8 Apr 2014 09:42:13 -0700 (PDT)
In-Reply-To: <CA+B-+fy-Cr9f4OZ4ztuqSnLNKwUMYHKN7GOg1nU2dmFO87rSjA@mail.gmail.com>
References: <CAFkVJifBb14oGvE4zHSL0=s1na9neiKVRGRWmUC24wWnP_9Miw@mail.gmail.com>
	<CAEYYnxbo10PHJsY=s4s2f-3PsSed-F+9zmsz35ELGSfGVhnuHg@mail.gmail.com>
	<CA+B-+fy-Cr9f4OZ4ztuqSnLNKwUMYHKN7GOg1nU2dmFO87rSjA@mail.gmail.com>
Date: Tue, 8 Apr 2014 09:42:13 -0700
Message-ID: <CA+B-+fxaN-oYuxiDar8RL9DdUcU73Y_giOLrDt_rvFT_657-iA@mail.gmail.com>
Subject: Re: MLLib - Thoughts about refactoring Updater for LBFGS?
From: Debasish Das <debasish.das83@gmail.com>
To: DB Tsai <dbtsai@stanford.edu>
Cc: dlwh@cs.berkeley.edu, Xiangrui Meng <mengxr@gmail.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=e89a8ff1ce027d1bae04f68aaa33
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8ff1ce027d1bae04f68aaa33
Content-Type: text/plain; charset=ISO-8859-1

Hi DB,

Are we going to clean up the function:

class LogisticRegressionWithSGD private (
    var stepSize: Double,
    var numIterations: Int,
    var regParam: Double,
    var miniBatchFraction: Double)
  extends GeneralizedLinearAlgorithm[LogisticRegressionModel] with
Serializable {

  val gradient = new LogisticGradient()
  val updater = new SimpleUpdater()
  override val optimizer = new GradientDescent(gradient, updater)

Or add a new one ?

class LogisticRegressionWithBFGS ?

The WithABC is optional since optimizer could be picked up either based on
a flag...there are only 3 options for optimizor:

1. GradientDescent
2. Quasi Newton
3. Newton

May be we add an enum for optimization type....and then under
GradientDescent family people can add their variants of SGD....Not sure if
ConjugateGradient comes under 1 or 2....may be we need 4 options...

Thanks.
Deb


On Mon, Apr 7, 2014 at 11:23 PM, Debasish Das <debasish.das83@gmail.com>wrote:

> I got your checkin....I need to run logistic regression SGD vs BFGS for my
> current usecases but your next checkin will update the logistic regression
> with LBFGS right ? Are you adding it to regression package as well ?
>
> Thanks.
> Deb
>
>
> On Mon, Apr 7, 2014 at 7:00 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>
>> Hi guys,
>>
>> The latest PR uses Breeze's L-BFGS implement which is introduced by
>> Xiangrui's sparse input format work in SPARK-1212.
>>
>> https://github.com/apache/spark/pull/353
>>
>> Now, it works with the new sparse framework!
>>
>> Any feedback would be greatly appreciated.
>>
>> Thanks.
>>
>> Sincerely,
>>
>> DB Tsai
>> -------------------------------------------------------
>> My Blog: https://www.dbtsai.com
>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>
>>
>> On Thu, Apr 3, 2014 at 5:02 PM, DB Tsai <dbtsai@alpinenow.com> wrote:
>> > ---------- Forwarded message ----------
>> > From: David Hall <dlwh@cs.berkeley.edu>
>> > Date: Sat, Mar 15, 2014 at 10:02 AM
>> > Subject: Re: MLLib - Thoughts about refactoring Updater for LBFGS?
>> > To: DB Tsai <dbtsai@alpinenow.com>
>> >
>> >
>> > On Fri, Mar 7, 2014 at 10:56 PM, DB Tsai <dbtsai@alpinenow.com> wrote:
>> >>
>> >> Hi David,
>> >>
>> >> Please let me know the version of Breeze that LBFGS can be serialized,
>> >> and CachedDiffFunction is built-in in LBFGS once you finish. I'll
>> >> update the PR to Spark from using RISO implementation to Breeze
>> >> implementation.
>> >
>> >
>> > The current master (0.7-SNAPSHOT) has these problems fixed.
>> >
>> >>
>> >>
>> >> Thanks.
>> >>
>> >> Sincerely,
>> >>
>> >> DB Tsai
>> >> Machine Learning Engineer
>> >> Alpine Data Labs
>> >> --------------------------------------
>> >> Web: http://alpinenow.com/
>> >>
>> >>
>> >> On Thu, Mar 6, 2014 at 4:26 PM, David Hall <dlwh@cs.berkeley.edu>
>> wrote:
>> >> > On Thu, Mar 6, 2014 at 4:21 PM, DB Tsai <dbtsai@alpinenow.com>
>> wrote:
>> >> >
>> >> >> Hi David,
>> >> >>
>> >> >> I can converge to the same result with your breeze LBFGS and Fortran
>> >> >> implementations now. Probably, I made some mistakes when I tried
>> >> >> breeze before. I apologize that I claimed it's not stable.
>> >> >>
>> >> >> See the test case in BreezeLBFGSSuite.scala
>> >> >> https://github.com/AlpineNow/spark/tree/dbtsai-breezeLBFGS
>> >> >>
>> >> >> This is training multinomial logistic regression against iris
>> dataset,
>> >> >> and both optimizers can train the models with 98% training accuracy.
>> >> >>
>> >> >
>> >> > great to hear! There were some bugs in LBFGS about 6 months ago, so
>> >> > depending on the last time you tried it, it might indeed have been
>> >> > bugged.
>> >> >
>> >> >
>> >> >>
>> >> >> There are two issues to use Breeze in Spark,
>> >> >>
>> >> >> 1) When the gradientSum and lossSum are computed distributively in
>> >> >> custom defined DiffFunction which will be passed into your
>> optimizer,
>> >> >> Spark will complain LBFGS class is not serializable. In
>> >> >> BreezeLBFGS.scala, I've to convert RDD to array to make it work
>> >> >> locally. It should be easy to fix by just having LBFGS to implement
>> >> >> Serializable.
>> >> >>
>> >> >
>> >> > I'm not sure why Spark should be serializing LBFGS? Shouldn't it
>> live on
>> >> > the controller node? Or is this a per-node thing?
>> >> >
>> >> > But no problem to make it serializable.
>> >> >
>> >> >
>> >> >>
>> >> >> 2) Breeze computes redundant gradient and loss. See the following
>> log
>> >> >> from both Fortran and Breeze implementations.
>> >> >>
>> >> >
>> >> > Err, yeah. I should probably have LBFGS do this automatically, but
>> >> > there's
>> >> > a CachedDiffFunction that gets rid of the redundant calculations.
>> >> >
>> >> > -- David
>> >> >
>> >> >
>> >> >>
>> >> >> Thanks.
>> >> >>
>> >> >> Fortran:
>> >> >> Iteration -1: loss 1.3862943611198926, diff 1.0
>> >> >> Iteration 0: loss 1.5846343143210866, diff 0.14307193024217352
>> >> >> Iteration 1: loss 1.1242501524477688, diff 0.29053004039012126
>> >> >> Iteration 2: loss 1.0930151243303563, diff 0.027782962952189336
>> >> >> Iteration 3: loss 1.054036932835569, diff 0.03566113127440601
>> >> >> Iteration 4: loss 0.9907956302751622, diff 0.05999907649459571
>> >> >> Iteration 5: loss 0.9184205380342829, diff 0.07304737423337761
>> >> >> Iteration 6: loss 0.8259870936519937, diff 0.10064381175132982
>> >> >> Iteration 7: loss 0.6327447552109574, diff 0.23395293458364716
>> >> >> Iteration 8: loss 0.5534101162436359, diff 0.1253815427665277
>> >> >> Iteration 9: loss 0.4045020086612566, diff 0.26907321376758075
>> >> >> Iteration 10: loss 0.3078824990823728, diff 0.23885980452569627
>> >> >>
>> >> >> Breeze:
>> >> >> Iteration -1: loss 1.3862943611198926, diff 1.0
>> >> >> Mar 6, 2014 3:59:11 PM com.github.fommil.netlib.BLAS <clinit>
>> >> >> WARNING: Failed to load implementation from:
>> >> >> com.github.fommil.netlib.NativeSystemBLAS
>> >> >> Mar 6, 2014 3:59:11 PM com.github.fommil.netlib.BLAS <clinit>
>> >> >> WARNING: Failed to load implementation from:
>> >> >> com.github.fommil.netlib.NativeRefBLAS
>> >> >> Iteration 0: loss 1.3862943611198926, diff 0.0
>> >> >> Iteration 1: loss 1.5846343143210866, diff 0.14307193024217352
>> >> >> Iteration 2: loss 1.1242501524477688, diff 0.29053004039012126
>> >> >> Iteration 3: loss 1.1242501524477688, diff 0.0
>> >> >> Iteration 4: loss 1.1242501524477688, diff 0.0
>> >> >> Iteration 5: loss 1.0930151243303563, diff 0.027782962952189336
>> >> >> Iteration 6: loss 1.0930151243303563, diff 0.0
>> >> >> Iteration 7: loss 1.0930151243303563, diff 0.0
>> >> >> Iteration 8: loss 1.054036932835569, diff 0.03566113127440601
>> >> >> Iteration 9: loss 1.054036932835569, diff 0.0
>> >> >> Iteration 10: loss 1.054036932835569, diff 0.0
>> >> >> Iteration 11: loss 0.9907956302751622, diff 0.05999907649459571
>> >> >> Iteration 12: loss 0.9907956302751622, diff 0.0
>> >> >> Iteration 13: loss 0.9907956302751622, diff 0.0
>> >> >> Iteration 14: loss 0.9184205380342829, diff 0.07304737423337761
>> >> >> Iteration 15: loss 0.9184205380342829, diff 0.0
>> >> >> Iteration 16: loss 0.9184205380342829, diff 0.0
>> >> >> Iteration 17: loss 0.8259870936519939, diff 0.1006438117513297
>> >> >> Iteration 18: loss 0.8259870936519939, diff 0.0
>> >> >> Iteration 19: loss 0.8259870936519939, diff 0.0
>> >> >> Iteration 20: loss 0.6327447552109576, diff 0.233952934583647
>> >> >> Iteration 21: loss 0.6327447552109576, diff 0.0
>> >> >> Iteration 22: loss 0.6327447552109576, diff 0.0
>> >> >> Iteration 23: loss 0.5534101162436362, diff 0.12538154276652747
>> >> >> Iteration 24: loss 0.5534101162436362, diff 0.0
>> >> >> Iteration 25: loss 0.5534101162436362, diff 0.0
>> >> >> Iteration 26: loss 0.40450200866125635, diff 0.2690732137675816
>> >> >> Iteration 27: loss 0.40450200866125635, diff 0.0
>> >> >> Iteration 28: loss 0.40450200866125635, diff 0.0
>> >> >> Iteration 29: loss 0.30788249908237314, diff 0.23885980452569502
>> >> >>
>> >> >> Sincerely,
>> >> >>
>> >> >> DB Tsai
>> >> >> Machine Learning Engineer
>> >> >> Alpine Data Labs
>> >> >> --------------------------------------
>> >> >> Web: http://alpinenow.com/
>> >> >>
>> >> >>
>> >> >> On Wed, Mar 5, 2014 at 2:00 PM, David Hall <dlwh@cs.berkeley.edu>
>> >> >> wrote:
>> >> >> > On Wed, Mar 5, 2014 at 1:57 PM, DB Tsai <dbtsai@alpinenow.com>
>> wrote:
>> >> >> >
>> >> >> >> Hi David,
>> >> >> >>
>> >> >> >> On Tue, Mar 4, 2014 at 8:13 PM, dlwh <david.lw.hall@gmail.com>
>> >> >> >> wrote:
>> >> >> >> > I'm happy to help fix any problems. I've verified at points
>> that
>> >> >> >> > the
>> >> >> >> > implementation gives the exact same sequence of iterates for a
>> few
>> >> >> >> different
>> >> >> >> > functions (with a particular line search) as the c port of
>> lbfgs.
>> >> >> >> > So
>> >> >> I'm
>> >> >> >> a
>> >> >> >> > little surprised it fails where Fortran succeeds... but only a
>> >> >> >> > little.
>> >> >> >> This
>> >> >> >> > was fixed late last year.
>> >> >> >> I'm working on a reproducible test case using breeze vs fortran
>> >> >> >> implementation to show the problem I've run into. The test will
>> be
>> >> >> >> in
>> >> >> >> one of the test cases in my Spark fork, is it okay for you to
>> >> >> >> investigate the issue? Or do I need to make it as a standalone
>> test?
>> >> >> >>
>> >> >> >
>> >> >> >
>> >> >> > Um, as long as it wouldn't be too hard to pull out.
>> >> >> >
>> >> >> >
>> >> >> >>
>> >> >> >> Will send you the test later today.
>> >> >> >>
>> >> >> >> Thanks.
>> >> >> >>
>> >> >> >> Sincerely,
>> >> >> >>
>> >> >> >> DB Tsai
>> >> >> >> Machine Learning Engineer
>> >> >> >> Alpine Data Labs
>> >> >> >> --------------------------------------
>> >> >> >> Web: http://alpinenow.com/
>> >> >> >>
>> >> >>
>> >
>> >
>> >
>>
>
>

--e89a8ff1ce027d1bae04f68aaa33--

From dev-return-7274-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 16:45:19 2014
Return-Path: <dev-return-7274-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9776911323
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 16:45:19 +0000 (UTC)
Received: (qmail 75622 invoked by uid 500); 8 Apr 2014 16:45:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75574 invoked by uid 500); 8 Apr 2014 16:45:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75566 invoked by uid 99); 8 Apr 2014 16:45:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 16:45:16 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.219.44 as permitted sender)
Received: from [209.85.219.44] (HELO mail-oa0-f44.google.com) (209.85.219.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 16:45:12 +0000
Received: by mail-oa0-f44.google.com with SMTP id n16so1331218oag.31
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 09:44:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=N/laU86+9jh6NPQkdee1nGm9e/w810pEwo260+G3n8s=;
        b=OMKFfj8EdbqF7wzNZ6SywtgKGeDKMuBxb4U7l8JnSNBIVyqQu/mMJ2MHEE5pgux1xr
         Z8K/jWcD/PmIIsDrYwlYnYCvXdXVlGL8HoQ0L2pYN86jAxhSX4jNCJPS1lIM0oRCOTlG
         GU8bD9/UpQ6G1H3NLYv4UrE1j/UYlR4YNdCMElNAI+3NawfI98pyyPPLhrPUtWE+zkkc
         92IyfT7AufyWr2xX/DUf+U9qKgKzpIAU7L4JTaaHHYrxXZOabq7WAsVG4Gucm4JJR35s
         km42OiTs9/zcLI1jKvC1z1O4EJPMVjSEMGJ5NNAP7UxZbDCi07Q+qrFxTaA3o3BflYq3
         iaew==
MIME-Version: 1.0
X-Received: by 10.60.78.165 with SMTP id c5mr1199514oex.83.1396975489714; Tue,
 08 Apr 2014 09:44:49 -0700 (PDT)
Received: by 10.182.142.163 with HTTP; Tue, 8 Apr 2014 09:44:49 -0700 (PDT)
In-Reply-To: <CA+B-+fxaN-oYuxiDar8RL9DdUcU73Y_giOLrDt_rvFT_657-iA@mail.gmail.com>
References: <CAFkVJifBb14oGvE4zHSL0=s1na9neiKVRGRWmUC24wWnP_9Miw@mail.gmail.com>
	<CAEYYnxbo10PHJsY=s4s2f-3PsSed-F+9zmsz35ELGSfGVhnuHg@mail.gmail.com>
	<CA+B-+fy-Cr9f4OZ4ztuqSnLNKwUMYHKN7GOg1nU2dmFO87rSjA@mail.gmail.com>
	<CA+B-+fxaN-oYuxiDar8RL9DdUcU73Y_giOLrDt_rvFT_657-iA@mail.gmail.com>
Date: Tue, 8 Apr 2014 09:44:49 -0700
Message-ID: <CA+B-+fxp7AWgxB+oQKgZJysF0M96MzpFKf1VQSy0WjQ40NMeaA@mail.gmail.com>
Subject: Re: MLLib - Thoughts about refactoring Updater for LBFGS?
From: Debasish Das <debasish.das83@gmail.com>
To: DB Tsai <dbtsai@stanford.edu>
Cc: dlwh@cs.berkeley.edu, Xiangrui Meng <mengxr@gmail.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0111de62d3123e04f68ab3e5
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0111de62d3123e04f68ab3e5
Content-Type: text/plain; charset=ISO-8859-1

By the way these changes are needed in mllib.regression as well....

Right now my usecases need BFGS support in logistic regression and MLOR so
we can focus on cleaning up the classification package first ?



On Tue, Apr 8, 2014 at 9:42 AM, Debasish Das <debasish.das83@gmail.com>wrote:

> Hi DB,
>
> Are we going to clean up the function:
>
> class LogisticRegressionWithSGD private (
>     var stepSize: Double,
>     var numIterations: Int,
>     var regParam: Double,
>     var miniBatchFraction: Double)
>   extends GeneralizedLinearAlgorithm[LogisticRegressionModel] with
> Serializable {
>
>   val gradient = new LogisticGradient()
>   val updater = new SimpleUpdater()
>   override val optimizer = new GradientDescent(gradient, updater)
>
> Or add a new one ?
>
> class LogisticRegressionWithBFGS ?
>
> The WithABC is optional since optimizer could be picked up either based on
> a flag...there are only 3 options for optimizor:
>
> 1. GradientDescent
> 2. Quasi Newton
> 3. Newton
>
> May be we add an enum for optimization type....and then under
> GradientDescent family people can add their variants of SGD....Not sure if
> ConjugateGradient comes under 1 or 2....may be we need 4 options...
>
> Thanks.
> Deb
>
>
> On Mon, Apr 7, 2014 at 11:23 PM, Debasish Das <debasish.das83@gmail.com>wrote:
>
>> I got your checkin....I need to run logistic regression SGD vs BFGS for
>> my current usecases but your next checkin will update the logistic
>> regression with LBFGS right ? Are you adding it to regression package as
>> well ?
>>
>> Thanks.
>> Deb
>>
>>
>> On Mon, Apr 7, 2014 at 7:00 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>
>>> Hi guys,
>>>
>>> The latest PR uses Breeze's L-BFGS implement which is introduced by
>>> Xiangrui's sparse input format work in SPARK-1212.
>>>
>>> https://github.com/apache/spark/pull/353
>>>
>>> Now, it works with the new sparse framework!
>>>
>>> Any feedback would be greatly appreciated.
>>>
>>> Thanks.
>>>
>>> Sincerely,
>>>
>>> DB Tsai
>>> -------------------------------------------------------
>>> My Blog: https://www.dbtsai.com
>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>
>>>
>>> On Thu, Apr 3, 2014 at 5:02 PM, DB Tsai <dbtsai@alpinenow.com> wrote:
>>> > ---------- Forwarded message ----------
>>> > From: David Hall <dlwh@cs.berkeley.edu>
>>> > Date: Sat, Mar 15, 2014 at 10:02 AM
>>> > Subject: Re: MLLib - Thoughts about refactoring Updater for LBFGS?
>>> > To: DB Tsai <dbtsai@alpinenow.com>
>>> >
>>> >
>>> > On Fri, Mar 7, 2014 at 10:56 PM, DB Tsai <dbtsai@alpinenow.com> wrote:
>>> >>
>>> >> Hi David,
>>> >>
>>> >> Please let me know the version of Breeze that LBFGS can be serialized,
>>> >> and CachedDiffFunction is built-in in LBFGS once you finish. I'll
>>> >> update the PR to Spark from using RISO implementation to Breeze
>>> >> implementation.
>>> >
>>> >
>>> > The current master (0.7-SNAPSHOT) has these problems fixed.
>>> >
>>> >>
>>> >>
>>> >> Thanks.
>>> >>
>>> >> Sincerely,
>>> >>
>>> >> DB Tsai
>>> >> Machine Learning Engineer
>>> >> Alpine Data Labs
>>> >> --------------------------------------
>>> >> Web: http://alpinenow.com/
>>> >>
>>> >>
>>> >> On Thu, Mar 6, 2014 at 4:26 PM, David Hall <dlwh@cs.berkeley.edu>
>>> wrote:
>>> >> > On Thu, Mar 6, 2014 at 4:21 PM, DB Tsai <dbtsai@alpinenow.com>
>>> wrote:
>>> >> >
>>> >> >> Hi David,
>>> >> >>
>>> >> >> I can converge to the same result with your breeze LBFGS and
>>> Fortran
>>> >> >> implementations now. Probably, I made some mistakes when I tried
>>> >> >> breeze before. I apologize that I claimed it's not stable.
>>> >> >>
>>> >> >> See the test case in BreezeLBFGSSuite.scala
>>> >> >> https://github.com/AlpineNow/spark/tree/dbtsai-breezeLBFGS
>>> >> >>
>>> >> >> This is training multinomial logistic regression against iris
>>> dataset,
>>> >> >> and both optimizers can train the models with 98% training
>>> accuracy.
>>> >> >>
>>> >> >
>>> >> > great to hear! There were some bugs in LBFGS about 6 months ago, so
>>> >> > depending on the last time you tried it, it might indeed have been
>>> >> > bugged.
>>> >> >
>>> >> >
>>> >> >>
>>> >> >> There are two issues to use Breeze in Spark,
>>> >> >>
>>> >> >> 1) When the gradientSum and lossSum are computed distributively in
>>> >> >> custom defined DiffFunction which will be passed into your
>>> optimizer,
>>> >> >> Spark will complain LBFGS class is not serializable. In
>>> >> >> BreezeLBFGS.scala, I've to convert RDD to array to make it work
>>> >> >> locally. It should be easy to fix by just having LBFGS to implement
>>> >> >> Serializable.
>>> >> >>
>>> >> >
>>> >> > I'm not sure why Spark should be serializing LBFGS? Shouldn't it
>>> live on
>>> >> > the controller node? Or is this a per-node thing?
>>> >> >
>>> >> > But no problem to make it serializable.
>>> >> >
>>> >> >
>>> >> >>
>>> >> >> 2) Breeze computes redundant gradient and loss. See the following
>>> log
>>> >> >> from both Fortran and Breeze implementations.
>>> >> >>
>>> >> >
>>> >> > Err, yeah. I should probably have LBFGS do this automatically, but
>>> >> > there's
>>> >> > a CachedDiffFunction that gets rid of the redundant calculations.
>>> >> >
>>> >> > -- David
>>> >> >
>>> >> >
>>> >> >>
>>> >> >> Thanks.
>>> >> >>
>>> >> >> Fortran:
>>> >> >> Iteration -1: loss 1.3862943611198926, diff 1.0
>>> >> >> Iteration 0: loss 1.5846343143210866, diff 0.14307193024217352
>>> >> >> Iteration 1: loss 1.1242501524477688, diff 0.29053004039012126
>>> >> >> Iteration 2: loss 1.0930151243303563, diff 0.027782962952189336
>>> >> >> Iteration 3: loss 1.054036932835569, diff 0.03566113127440601
>>> >> >> Iteration 4: loss 0.9907956302751622, diff 0.05999907649459571
>>> >> >> Iteration 5: loss 0.9184205380342829, diff 0.07304737423337761
>>> >> >> Iteration 6: loss 0.8259870936519937, diff 0.10064381175132982
>>> >> >> Iteration 7: loss 0.6327447552109574, diff 0.23395293458364716
>>> >> >> Iteration 8: loss 0.5534101162436359, diff 0.1253815427665277
>>> >> >> Iteration 9: loss 0.4045020086612566, diff 0.26907321376758075
>>> >> >> Iteration 10: loss 0.3078824990823728, diff 0.23885980452569627
>>> >> >>
>>> >> >> Breeze:
>>> >> >> Iteration -1: loss 1.3862943611198926, diff 1.0
>>> >> >> Mar 6, 2014 3:59:11 PM com.github.fommil.netlib.BLAS <clinit>
>>> >> >> WARNING: Failed to load implementation from:
>>> >> >> com.github.fommil.netlib.NativeSystemBLAS
>>> >> >> Mar 6, 2014 3:59:11 PM com.github.fommil.netlib.BLAS <clinit>
>>> >> >> WARNING: Failed to load implementation from:
>>> >> >> com.github.fommil.netlib.NativeRefBLAS
>>> >> >> Iteration 0: loss 1.3862943611198926, diff 0.0
>>> >> >> Iteration 1: loss 1.5846343143210866, diff 0.14307193024217352
>>> >> >> Iteration 2: loss 1.1242501524477688, diff 0.29053004039012126
>>> >> >> Iteration 3: loss 1.1242501524477688, diff 0.0
>>> >> >> Iteration 4: loss 1.1242501524477688, diff 0.0
>>> >> >> Iteration 5: loss 1.0930151243303563, diff 0.027782962952189336
>>> >> >> Iteration 6: loss 1.0930151243303563, diff 0.0
>>> >> >> Iteration 7: loss 1.0930151243303563, diff 0.0
>>> >> >> Iteration 8: loss 1.054036932835569, diff 0.03566113127440601
>>> >> >> Iteration 9: loss 1.054036932835569, diff 0.0
>>> >> >> Iteration 10: loss 1.054036932835569, diff 0.0
>>> >> >> Iteration 11: loss 0.9907956302751622, diff 0.05999907649459571
>>> >> >> Iteration 12: loss 0.9907956302751622, diff 0.0
>>> >> >> Iteration 13: loss 0.9907956302751622, diff 0.0
>>> >> >> Iteration 14: loss 0.9184205380342829, diff 0.07304737423337761
>>> >> >> Iteration 15: loss 0.9184205380342829, diff 0.0
>>> >> >> Iteration 16: loss 0.9184205380342829, diff 0.0
>>> >> >> Iteration 17: loss 0.8259870936519939, diff 0.1006438117513297
>>> >> >> Iteration 18: loss 0.8259870936519939, diff 0.0
>>> >> >> Iteration 19: loss 0.8259870936519939, diff 0.0
>>> >> >> Iteration 20: loss 0.6327447552109576, diff 0.233952934583647
>>> >> >> Iteration 21: loss 0.6327447552109576, diff 0.0
>>> >> >> Iteration 22: loss 0.6327447552109576, diff 0.0
>>> >> >> Iteration 23: loss 0.5534101162436362, diff 0.12538154276652747
>>> >> >> Iteration 24: loss 0.5534101162436362, diff 0.0
>>> >> >> Iteration 25: loss 0.5534101162436362, diff 0.0
>>> >> >> Iteration 26: loss 0.40450200866125635, diff 0.2690732137675816
>>> >> >> Iteration 27: loss 0.40450200866125635, diff 0.0
>>> >> >> Iteration 28: loss 0.40450200866125635, diff 0.0
>>> >> >> Iteration 29: loss 0.30788249908237314, diff 0.23885980452569502
>>> >> >>
>>> >> >> Sincerely,
>>> >> >>
>>> >> >> DB Tsai
>>> >> >> Machine Learning Engineer
>>> >> >> Alpine Data Labs
>>> >> >> --------------------------------------
>>> >> >> Web: http://alpinenow.com/
>>> >> >>
>>> >> >>
>>> >> >> On Wed, Mar 5, 2014 at 2:00 PM, David Hall <dlwh@cs.berkeley.edu>
>>> >> >> wrote:
>>> >> >> > On Wed, Mar 5, 2014 at 1:57 PM, DB Tsai <dbtsai@alpinenow.com>
>>> wrote:
>>> >> >> >
>>> >> >> >> Hi David,
>>> >> >> >>
>>> >> >> >> On Tue, Mar 4, 2014 at 8:13 PM, dlwh <david.lw.hall@gmail.com>
>>> >> >> >> wrote:
>>> >> >> >> > I'm happy to help fix any problems. I've verified at points
>>> that
>>> >> >> >> > the
>>> >> >> >> > implementation gives the exact same sequence of iterates for
>>> a few
>>> >> >> >> different
>>> >> >> >> > functions (with a particular line search) as the c port of
>>> lbfgs.
>>> >> >> >> > So
>>> >> >> I'm
>>> >> >> >> a
>>> >> >> >> > little surprised it fails where Fortran succeeds... but only a
>>> >> >> >> > little.
>>> >> >> >> This
>>> >> >> >> > was fixed late last year.
>>> >> >> >> I'm working on a reproducible test case using breeze vs fortran
>>> >> >> >> implementation to show the problem I've run into. The test will
>>> be
>>> >> >> >> in
>>> >> >> >> one of the test cases in my Spark fork, is it okay for you to
>>> >> >> >> investigate the issue? Or do I need to make it as a standalone
>>> test?
>>> >> >> >>
>>> >> >> >
>>> >> >> >
>>> >> >> > Um, as long as it wouldn't be too hard to pull out.
>>> >> >> >
>>> >> >> >
>>> >> >> >>
>>> >> >> >> Will send you the test later today.
>>> >> >> >>
>>> >> >> >> Thanks.
>>> >> >> >>
>>> >> >> >> Sincerely,
>>> >> >> >>
>>> >> >> >> DB Tsai
>>> >> >> >> Machine Learning Engineer
>>> >> >> >> Alpine Data Labs
>>> >> >> >> --------------------------------------
>>> >> >> >> Web: http://alpinenow.com/
>>> >> >> >>
>>> >> >>
>>> >
>>> >
>>> >
>>>
>>
>>
>

--089e0111de62d3123e04f68ab3e5--

From dev-return-7275-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 16:51:29 2014
Return-Path: <dev-return-7275-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2589E11350
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 16:51:29 +0000 (UTC)
Received: (qmail 89980 invoked by uid 500); 8 Apr 2014 16:51:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89930 invoked by uid 500); 8 Apr 2014 16:51:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89921 invoked by uid 99); 8 Apr 2014 16:51:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 16:51:27 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mfernest@cloudera.com designates 209.85.213.48 as permitted sender)
Received: from [209.85.213.48] (HELO mail-yh0-f48.google.com) (209.85.213.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 16:51:22 +0000
Received: by mail-yh0-f48.google.com with SMTP id z6so1136354yhz.7
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 09:51:00 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=j9fm5X7IPusUOOPajo7HiqZqyj/KTSUZiMwGQce5vKA=;
        b=d4LDEl+bw4NMCaY7/dop/blqAV9WjwgwplGvkaDvDI/mem58w3TKr9azP7M3+b+tmv
         vOrnm6omDQiY3T5BD93jpvI6Yt9nF+aVcU8H53lOuatfHSpdgW9UsaxmFEY0EpyBJYI+
         3D7JfHt3g7cOvRvS4/iEFeQFhr+QfPjjHoeRDU0qzUcQw9+hMGLeFYEedkY27CnG7zlh
         9nocPSn/rDz6aPJ/I8Z/2l1d0DBIZuwQavxQ9EogU2x0at5cq96bk7F/fMXXcBxxrno9
         jANNt29vDZdNODUOzI8uRl5hldxI90ZiOo24ZEO7JLJuvqwguT+jtw74A47AloqZGLRx
         EbfA==
X-Gm-Message-State: ALoCoQn+8gNtBqTFju27Afg5gWpjAlneTNGza2QOmSoyGW6JfFQSNPh+K0C8NVJURXQy7IabUO/4
X-Received: by 10.236.10.82 with SMTP id 58mr4602269yhu.118.1396975860225;
 Tue, 08 Apr 2014 09:51:00 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.170.50.210 with HTTP; Tue, 8 Apr 2014 09:50:30 -0700 (PDT)
In-Reply-To: <EDCA85D5-3E3F-44D6-BA7F-4DEEFD863A2D@gmail.com>
References: <CAGRER0deRfjEdguW5tjJPFBSuiap2P9-q21WBZyNyVbL4onHfA@mail.gmail.com>
 <CAJHwgdshcGJLNjT118A79OWg6Gb_qaaNvoQZiw1WcQgQfinS3g@mail.gmail.com>
 <CAGRER0c_tMqMep23RsXDBZGNbU0=ZgedsSzh3BELYfhaUFabnw@mail.gmail.com>
 <3A236731-EDDE-4B26-A8A8-5E66C287687E@gmail.com> <CANGvG8rYs_vPM8t_9Q3HvzTSnmYQWND06RCP5=b0H1iJsNNTyA@mail.gmail.com>
 <EDCA85D5-3E3F-44D6-BA7F-4DEEFD863A2D@gmail.com>
From: Michael Ernest <mfernest@cloudera.com>
Date: Tue, 8 Apr 2014 12:50:30 -0400
Message-ID: <CADpnHP3iKL7Xi9mXTQiByMbNs-JYureMz2A=7rnK-jWicw0H-w@mail.gmail.com>
Subject: Re: Contributing to Spark
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1137feb4e8a65004f68ac9ff
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1137feb4e8a65004f68ac9ff
Content-Type: text/plain; charset=ISO-8859-1

Ha ha! nice try, sheepherder! ;-)


On Tue, Apr 8, 2014 at 12:37 PM, Matei Zaharia <matei.zaharia@gmail.com>wrote:

> Shh, maybe I really wanted people to fix that one issue.
>
> On Apr 8, 2014, at 9:34 AM, Aaron Davidson <ilikerps@gmail.com> wrote:
>
> > Matei's link seems to point to a specific starter project as part of the
> > starter list, but here is the list itself:
> >
> https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20labels%20%3D%20Starter%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)
> >
> >
> > On Mon, Apr 7, 2014 at 10:22 PM, Matei Zaharia <matei.zaharia@gmail.com
> >wrote:
> >
> >> I'd suggest looking for the issues labeled "Starter" on JIRA. You can
> find
> >> them here:
> >>
> https://issues.apache.org/jira/browse/SPARK-1438?jql=project%20%3D%20SPARK%20AND%20labels%20%3D%20Starter%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)
> >>
> >> Matei
> >>
> >> On Apr 7, 2014, at 9:45 PM, Mukesh G <mukgbv@gmail.com> wrote:
> >>
> >>> Hi Sujeet,
> >>>
> >>>   Thanks. I went thru the website and looks great. Is there a list of
> >>> items that I can choose from, for contribution?
> >>>
> >>> Thanks
> >>>
> >>> Mukesh
> >>>
> >>>
> >>> On Mon, Apr 7, 2014 at 10:14 PM, Sujeet Varakhedi
> >>> <svarakhedi@gopivotal.com>wrote:
> >>>
> >>>> This is a good place to start:
> >>>>
> https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
> >>>>
> >>>> Sujeet
> >>>>
> >>>>
> >>>> On Mon, Apr 7, 2014 at 9:20 AM, Mukesh G <mukgbv@gmail.com> wrote:
> >>>>
> >>>>> Hi,
> >>>>>
> >>>>>  How I contribute to Spark and it's associated projects?
> >>>>>
> >>>>> Appreciate the help...
> >>>>>
> >>>>> Thanks
> >>>>>
> >>>>> Mukesh
> >>>>>
> >>>>
> >>
> >>
>
>


-- 
Michael Ernest
Sr. Solutions Consultant
West Coast

--001a1137feb4e8a65004f68ac9ff--

From dev-return-7276-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 20:02:58 2014
Return-Path: <dev-return-7276-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E5A7311927
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 20:02:57 +0000 (UTC)
Received: (qmail 36373 invoked by uid 500); 8 Apr 2014 20:02:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36134 invoked by uid 500); 8 Apr 2014 20:02:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36118 invoked by uid 99); 8 Apr 2014 20:02:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 20:02:54 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of love2dishtech@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 20:02:49 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <love2dishtech@gmail.com>)
	id 1WXcDz-0000LO-Rl
	for dev@spark.incubator.apache.org; Tue, 08 Apr 2014 13:02:27 -0700
Date: Tue, 8 Apr 2014 13:02:27 -0700 (PDT)
From: love2dishtech <love2dishtech@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1396987347761-6261.post@n3.nabble.com>
Subject: Apache Spark and Graphx for Real Time Analytics
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

Is Graphx on top of Apache Spark, is able to process the large scale
distributed graph traversal and compute, in real time. What is the query
execution engine distributing the query on top of graphx and apache spark.
My typical use case is a large scale distributed graph traversal in real
time, with billions of nodes.

Thanks,
Love.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-and-Graphx-for-Real-Time-Analytics-tp6261.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7277-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 20:23:40 2014
Return-Path: <dev-return-7277-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AED3311A20
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 20:23:40 +0000 (UTC)
Received: (qmail 83773 invoked by uid 500); 8 Apr 2014 20:23:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83314 invoked by uid 500); 8 Apr 2014 20:23:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83302 invoked by uid 99); 8 Apr 2014 20:23:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 20:23:37 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nick.pentreath@gmail.com designates 209.85.219.50 as permitted sender)
Received: from [209.85.219.50] (HELO mail-oa0-f50.google.com) (209.85.219.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 20:23:32 +0000
Received: by mail-oa0-f50.google.com with SMTP id i7so1665718oag.9
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 13:23:12 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=W9mL1cbOrh7OkC0r/DWxIFEk5iEwUOvueTZVlQEbCMc=;
        b=IFSz1TtoJGSLK1Nt7mmgpALx2l+P0sRZ5VrSgphNGvz/CTWaRGcvw/19s6rLSGjKQf
         5W0TFiuEdUjmxIRiMjE1R4f6uczWbDKPc8FFx8x3mL2td5GZws//eMKKY2Dw/9PPjwIw
         h3Cq3UHdavAaWECV4FsCSRo5LcPM+Tp0XXlh9OTNgtDkPnEF0moWrSCwyQRIq6IJqe6K
         J0qFob5/hM1sahaiaeNytV9Pecr7R75nXozPaoADhNp3LavLvkrh8crln5bUkslNytkA
         9JP6SHrfjtWdnDJiTh89XvPebzCO+v8KtC15b+x+Fn2SRcQ1UmwlWA2NAHG6eRtI1xzc
         qA/A==
MIME-Version: 1.0
X-Received: by 10.60.51.4 with SMTP id g4mr5002219oeo.52.1396988592244; Tue,
 08 Apr 2014 13:23:12 -0700 (PDT)
Received: by 10.182.95.103 with HTTP; Tue, 8 Apr 2014 13:23:12 -0700 (PDT)
In-Reply-To: <1396987347761-6261.post@n3.nabble.com>
References: <1396987347761-6261.post@n3.nabble.com>
Date: Tue, 8 Apr 2014 22:23:12 +0200
Message-ID: <CALD+6GN8-7qmHaaQHAc6=1GP+hf4_=bD6u5o83FV2aPArHMW8Q@mail.gmail.com>
Subject: Re: Apache Spark and Graphx for Real Time Analytics
From: Nick Pentreath <nick.pentreath@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c304a4cbec4b04f68dc0b9
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c304a4cbec4b04f68dc0b9
Content-Type: text/plain; charset=ISO-8859-1

GraphX, like Spark, will not typically be "real-time" (where by "real-time"
here I assume you mean of the order of a few 10s-100s ms, up to a few
seconds).

Spark can in some cases approach the upper boundary of this definition (a
second or two, possibly less) when data is cached in memory and the
computation is not "too heavy", while Spark Streaming may be able to get
closer to the mid-to-upper boundary of this under similar conditions,
especially if aggregating over relatively small windows.

However, for this use case (while I haven't used GraphX yet) I would say
something like Titan (https://github.com/thinkaurelius/titan/wiki) or a
similar OLTP graph DB may be what you're after. But this depends on what
kind of graph traversal you need.




On Tue, Apr 8, 2014 at 10:02 PM, love2dishtech <love2dishtech@gmail.com>wrote:

> Hi,
>
> Is Graphx on top of Apache Spark, is able to process the large scale
> distributed graph traversal and compute, in real time. What is the query
> execution engine distributing the query on top of graphx and apache spark.
> My typical use case is a large scale distributed graph traversal in real
> time, with billions of nodes.
>
> Thanks,
> Love.
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-and-Graphx-for-Real-Time-Analytics-tp6261.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c304a4cbec4b04f68dc0b9--

From dev-return-7278-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 20:36:56 2014
Return-Path: <dev-return-7278-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 330B011A7E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 20:36:56 +0000 (UTC)
Received: (qmail 3689 invoked by uid 500); 8 Apr 2014 20:36:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3388 invoked by uid 500); 8 Apr 2014 20:36:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 3374 invoked by uid 99); 8 Apr 2014 20:36:51 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 20:36:51 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of anurag.phadke@gmail.com designates 209.85.214.47 as permitted sender)
Received: from [209.85.214.47] (HELO mail-bk0-f47.google.com) (209.85.214.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 20:36:46 +0000
Received: by mail-bk0-f47.google.com with SMTP id w10so1232359bkz.6
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 13:36:24 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=+yFDIPl3TOfEpdtUAn1BIr6woXxWuGCVnMT8G1X/Gmw=;
        b=IXdm4inCxe5rkZylVkn2hDgBTNVk/DygrF253C0N2m2QHcbhqiIHXrEo2U6/grxUbs
         LJ9y6Z0Kv3MaiMuUIHW0l6HxGD6hVC/1hLTztFMQ0l+i/p53G5rV69eK2tsQj6jbR/VN
         Y12gOuFQhRNF9LLMFQSQ6pVXM/3+Z3v3h+kC0iOP3a9rwkhrbQH5KZ9Mji4XXGHL1bg1
         FgEvB+rqU1slk3gT1wvHpl0/9QiDEHR68WT0eQPno1S9RgwVOpgnx2DsiCFycgzJaC42
         aoOtjLxgXRBWHEPL4ZcjHnfK/HgI6IirO/d93c9H77jOB+TEGDnwuxrYA5fBq7Ydg6uC
         nH0Q==
X-Received: by 10.152.42.144 with SMTP id o16mr4361881lal.9.1396989384681;
 Tue, 08 Apr 2014 13:36:24 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.184.99 with HTTP; Tue, 8 Apr 2014 13:36:04 -0700 (PDT)
From: Anurag <anurag.phadke@gmail.com>
Date: Tue, 8 Apr 2014 13:36:04 -0700
Message-ID: <CAK0jf17=0MEs=YmZ3N=SxiGhXU2ftVvAWFVvqm=R8XSsN1PDrQ@mail.gmail.com>
Subject: reading custom input format in Spark
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c33ef207bd5104f68df09f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c33ef207bd5104f68df09f
Content-Type: text/plain; charset=ISO-8859-1

Hi,
I am able to read a custom input format in spark.
scala> val inputRead = sc.newAPIHadoopFile("hdfs://
127.0.0.1/user/cloudera/date_dataset/
",classOf[io.reader.PatternInputFormat],classOf[org.apache.hadoop.io.LongWritable],classOf[org.apache.hadoop.io.Text])

However, doing a
inputRead.count()
results in null pointer exception.
14/04/08 13:33:39 INFO FileInputFormat: Total input paths to process : 1
14/04/08 13:33:39 INFO SparkContext: Starting job: count at <console>:15
14/04/08 13:33:39 INFO DAGScheduler: Got job 8 (count at <console>:15) with
1 output partitions (allowLocal=false)
14/04/08 13:33:39 INFO DAGScheduler: Final stage: Stage 9 (count at
<console>:15)
14/04/08 13:33:39 INFO DAGScheduler: Parents of final stage: List()
14/04/08 13:33:39 INFO DAGScheduler: Missing parents: List()
14/04/08 13:33:39 INFO DAGScheduler: Submitting Stage 9 (NewHadoopRDD[19]
at newAPIHadoopFile at <console>:12), which has no missing parents
14/04/08 13:33:39 INFO DAGScheduler: Submitting 1 missing tasks from Stage
9 (NewHadoopRDD[19] at newAPIHadoopFile at <console>:12)
14/04/08 13:33:39 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
14/04/08 13:33:39 INFO TaskSetManager: Starting task 9.0:0 as TID 8 on
executor localhost: localhost (PROCESS_LOCAL)
14/04/08 13:33:39 INFO TaskSetManager: Serialized task 9.0:0 as 1297 bytes
in 0 ms
14/04/08 13:33:39 INFO Executor: Running task ID 8
14/04/08 13:33:39 INFO BlockManager: Found block broadcast_5 locally
14/04/08 13:33:39 INFO NewHadoopRDD: Input split: hdfs://
127.0.0.1/user/cloudera/date_dataset/sample.txt:0+759
14/04/08 13:33:39 WARN TaskSetManager: Lost TID 8 (task 9.0:0)
14/04/08 13:33:39 WARN TaskSetManager: Loss was due to
java.lang.NullPointerException
java.lang.NullPointerException
    at java.util.regex.Pattern.<init>(Pattern.java:1132)
    at java.util.regex.Pattern.compile(Pattern.java:823)
    at io.reader.PatternRecordReader.initialize(PatternRecordReader.java:42)
    at
org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:96)
    at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:84)
    at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:48)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
    at org.apache.spark.scheduler.Task.run(Task.scala:53)
    at
org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
    at
org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
14/04/08 13:33:39 ERROR TaskSetManager: Task 9.0:0 failed 1 times; aborting
job
14/04/08 13:33:39 INFO DAGScheduler: Failed to run count at <console>:15
14/04/08 13:33:39 INFO TaskSchedulerImpl: Remove TaskSet 9.0 from pool
14/04/08 13:33:39 ERROR Executor: Exception in task ID 8
java.lang.NullPointerException
    at java.util.regex.Pattern.<init>(Pattern.java:1132)
    at java.util.regex.Pattern.compile(Pattern.java:823)
    at io.reader.PatternRecordReader.initialize(PatternRecordReader.java:42)
    at
org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:96)
    at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:84)
    at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:48)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
    at org.apache.spark.scheduler.Task.run(Task.scala:53)
    at
org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
    at
org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
org.apache.spark.SparkException: Job aborted: Task 9.0:0 failed 1 times
(most recent failure: Exception failure: java.lang.NullPointerException)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1028)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1026)
    at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1026)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)
    at scala.Option.foreach(Option.scala:236)
    at
org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:619)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:207)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
    at akka.actor.ActorCell.invoke(ActorCell.scala:456)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
    at akka.dispatch.Mailbox.run(Mailbox.scala:219)
    at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
    at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
    at
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
    at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


any idea what might be happening here?

-anurag



-- 
Twitter: @anuragphadke (https://twitter.com/#!/anuragphadke)

--001a11c33ef207bd5104f68df09f--

From dev-return-7279-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 20:42:43 2014
Return-Path: <dev-return-7279-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6B6CD11AB6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 20:42:43 +0000 (UTC)
Received: (qmail 11820 invoked by uid 500); 8 Apr 2014 20:42:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11740 invoked by uid 500); 8 Apr 2014 20:42:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11724 invoked by uid 99); 8 Apr 2014 20:42:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 20:42:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nick.pentreath@gmail.com designates 209.85.219.49 as permitted sender)
Received: from [209.85.219.49] (HELO mail-oa0-f49.google.com) (209.85.219.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 20:42:35 +0000
Received: by mail-oa0-f49.google.com with SMTP id o6so1670431oag.22
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 13:42:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=N4ShOHIfhSm4IdXR/lfKnmHoSLnZfUF4vF1pd83WnUw=;
        b=yYrOxbTqB3NdX63bGlL3wMIH46dGhltAt28hhzxyTDLvpr9GktJwihbJm4NG1JUoeP
         rfRUQa1J9hDeOza3VEUvjq0bl50VbJpEzzqMcj8uWOoKkJzRfuJ2P85GWv+gK74gr/nQ
         MtarQCQX+NgJzKyhVtGURDGiixk9D7Fcz81XAs9N66BsXY2Ois2KFYPAjszzdukAPKPy
         tiy5T5+ah/SoxhEGl4kV/4gGyhWVaQHScsEs9dvoBOsB+leAHkQhiwDUMdKIYTn6PTpM
         trJiuMSfHuCHFwwurs8cIr3Qj/f5s9r/52zLxp70bMfExRsalUqSkPtKTLrAhN+QuAfg
         Fkyw==
MIME-Version: 1.0
X-Received: by 10.182.153.226 with SMTP id vj2mr5226447obb.26.1396989734974;
 Tue, 08 Apr 2014 13:42:14 -0700 (PDT)
Received: by 10.182.95.103 with HTTP; Tue, 8 Apr 2014 13:42:14 -0700 (PDT)
In-Reply-To: <CAK0jf17=0MEs=YmZ3N=SxiGhXU2ftVvAWFVvqm=R8XSsN1PDrQ@mail.gmail.com>
References: <CAK0jf17=0MEs=YmZ3N=SxiGhXU2ftVvAWFVvqm=R8XSsN1PDrQ@mail.gmail.com>
Date: Tue, 8 Apr 2014 22:42:14 +0200
Message-ID: <CALD+6GN-Lo0Zc+hQ0s3RxXhAk1WkkZWBG3cN54wyfQrBV2Ytqw@mail.gmail.com>
Subject: Re: reading custom input format in Spark
From: Nick Pentreath <nick.pentreath@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e013d0dc0e8ba4304f68e0422
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013d0dc0e8ba4304f68e0422
Content-Type: text/plain; charset=ISO-8859-1

Seems like you need to initialise a regex pattern for that inputformat. How
is this done? Perhaps via a config option?

In which case you need to first create a hadoop configuration, set the
appropriate config option for the regex, and pass that into
newAPIHadoopFile.


On Tue, Apr 8, 2014 at 10:36 PM, Anurag <anurag.phadke@gmail.com> wrote:

> Hi,
> I am able to read a custom input format in spark.
> scala> val inputRead = sc.newAPIHadoopFile("hdfs://
> 127.0.0.1/user/cloudera/date_dataset/
>
> ",classOf[io.reader.PatternInputFormat],classOf[org.apache.hadoop.io.LongWritable],classOf[org.apache.hadoop.io.Text])
>
> However, doing a
> inputRead.count()
> results in null pointer exception.
> 14/04/08 13:33:39 INFO FileInputFormat: Total input paths to process : 1
> 14/04/08 13:33:39 INFO SparkContext: Starting job: count at <console>:15
> 14/04/08 13:33:39 INFO DAGScheduler: Got job 8 (count at <console>:15) with
> 1 output partitions (allowLocal=false)
> 14/04/08 13:33:39 INFO DAGScheduler: Final stage: Stage 9 (count at
> <console>:15)
> 14/04/08 13:33:39 INFO DAGScheduler: Parents of final stage: List()
> 14/04/08 13:33:39 INFO DAGScheduler: Missing parents: List()
> 14/04/08 13:33:39 INFO DAGScheduler: Submitting Stage 9 (NewHadoopRDD[19]
> at newAPIHadoopFile at <console>:12), which has no missing parents
> 14/04/08 13:33:39 INFO DAGScheduler: Submitting 1 missing tasks from Stage
> 9 (NewHadoopRDD[19] at newAPIHadoopFile at <console>:12)
> 14/04/08 13:33:39 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
> 14/04/08 13:33:39 INFO TaskSetManager: Starting task 9.0:0 as TID 8 on
> executor localhost: localhost (PROCESS_LOCAL)
> 14/04/08 13:33:39 INFO TaskSetManager: Serialized task 9.0:0 as 1297 bytes
> in 0 ms
> 14/04/08 13:33:39 INFO Executor: Running task ID 8
> 14/04/08 13:33:39 INFO BlockManager: Found block broadcast_5 locally
> 14/04/08 13:33:39 INFO NewHadoopRDD: Input split: hdfs://
> 127.0.0.1/user/cloudera/date_dataset/sample.txt:0+759
> 14/04/08 13:33:39 WARN TaskSetManager: Lost TID 8 (task 9.0:0)
> 14/04/08 13:33:39 WARN TaskSetManager: Loss was due to
> java.lang.NullPointerException
> java.lang.NullPointerException
>     at java.util.regex.Pattern.<init>(Pattern.java:1132)
>     at java.util.regex.Pattern.compile(Pattern.java:823)
>     at
> io.reader.PatternRecordReader.initialize(PatternRecordReader.java:42)
>     at
> org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:96)
>     at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:84)
>     at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:48)
>     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
>     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
>     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
>     at org.apache.spark.scheduler.Task.run(Task.scala:53)
>     at
>
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
>     at
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
>     at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
>     at
>
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>     at
>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>     at java.lang.Thread.run(Thread.java:662)
> 14/04/08 13:33:39 ERROR TaskSetManager: Task 9.0:0 failed 1 times; aborting
> job
> 14/04/08 13:33:39 INFO DAGScheduler: Failed to run count at <console>:15
> 14/04/08 13:33:39 INFO TaskSchedulerImpl: Remove TaskSet 9.0 from pool
> 14/04/08 13:33:39 ERROR Executor: Exception in task ID 8
> java.lang.NullPointerException
>     at java.util.regex.Pattern.<init>(Pattern.java:1132)
>     at java.util.regex.Pattern.compile(Pattern.java:823)
>     at
> io.reader.PatternRecordReader.initialize(PatternRecordReader.java:42)
>     at
> org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:96)
>     at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:84)
>     at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:48)
>     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
>     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
>     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
>     at org.apache.spark.scheduler.Task.run(Task.scala:53)
>     at
>
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
>     at
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
>     at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
>     at
>
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>     at
>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>     at java.lang.Thread.run(Thread.java:662)
> org.apache.spark.SparkException: Job aborted: Task 9.0:0 failed 1 times
> (most recent failure: Exception failure: java.lang.NullPointerException)
>     at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1028)
>     at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1026)
>     at
>
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>     at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>     at org.apache.spark.scheduler.DAGScheduler.org
> $apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1026)
>     at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)
>     at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)
>     at scala.Option.foreach(Option.scala:236)
>     at
>
> org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:619)
>     at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:207)
>     at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>     at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>     at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>     at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>     at
>
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>     at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>     at
>
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>     at
> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>     at
>
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>
>
> any idea what might be happening here?
>
> -anurag
>
>
>
> --
> Twitter: @anuragphadke (https://twitter.com/#!/anuragphadke)
>

--089e013d0dc0e8ba4304f68e0422--

From dev-return-7280-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 20:42:49 2014
Return-Path: <dev-return-7280-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6417911AB9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 20:42:49 +0000 (UTC)
Received: (qmail 13522 invoked by uid 500); 8 Apr 2014 20:42:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13109 invoked by uid 500); 8 Apr 2014 20:42:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12971 invoked by uid 99); 8 Apr 2014 20:42:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 20:42:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.128.176] (HELO mail-ve0-f176.google.com) (209.85.128.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 20:42:40 +0000
Received: by mail-ve0-f176.google.com with SMTP id db11so1259909veb.21
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 13:42:19 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=9YC78v4GvgYAmMNOZW6ma/A4IAZ8vyU9dodF0VuZrOM=;
        b=iXmarAFILMF5O02EsYNVylZgbYcCKyOX0djVn1EplwDSjdWNDhKHKvENvai4qunqWF
         7atbrlkA2ag/W4ISHHM0t4tilIgsDoaK3vZKdjuCh3VEL5HnCQQMLNNKY/TMoVQtworr
         33q4YOsCLvjvbkUPpw7PmYM0ZsCEaoh4t5uNiOlCDKgyHtJ5D5c9rMYZ8ZSP0dxwOs+y
         T0ZjA7bJ2BmotH63OHy9w93EML7PqgWIlcC/1uCIiPMCdrZPXeeezsGpMNHtx1LMfkRb
         08QkLyl4Srv+zEJ3MHstwX4f7cQeGTtCIb5s21qU3GcqwWz41GDCXPp+//Pgn5AQipxn
         oTeA==
X-Gm-Message-State: ALoCoQki05s7VnWYnNbe40rXSd+Yc+dtmUfMJo02iyTdpzFCBMccEeIkXGemqMeRRKyGwfAtTp+p
X-Received: by 10.58.185.145 with SMTP id fc17mr5184707vec.14.1396989739189;
        Tue, 08 Apr 2014 13:42:19 -0700 (PDT)
Received: from mail-vc0-f179.google.com (mail-vc0-f179.google.com [209.85.220.179])
        by mx.google.com with ESMTPSA id ga9sm5896092vdc.22.2014.04.08.13.42.17
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 08 Apr 2014 13:42:17 -0700 (PDT)
Received: by mail-vc0-f179.google.com with SMTP id ij19so1266797vcb.38
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 13:42:17 -0700 (PDT)
X-Received: by 10.220.106.84 with SMTP id w20mr4978020vco.18.1396989737654;
 Tue, 08 Apr 2014 13:42:17 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Tue, 8 Apr 2014 13:41:57 -0700 (PDT)
In-Reply-To: <CAK0jf17=0MEs=YmZ3N=SxiGhXU2ftVvAWFVvqm=R8XSsN1PDrQ@mail.gmail.com>
References: <CAK0jf17=0MEs=YmZ3N=SxiGhXU2ftVvAWFVvqm=R8XSsN1PDrQ@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Tue, 8 Apr 2014 13:41:57 -0700
Message-ID: <CA+-p3AGeOL1OcVLa+dWbj-rp7Ey3LV61aouCK0boVg4peEna=g@mail.gmail.com>
Subject: Re: reading custom input format in Spark
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b3435c011634a04f68e0543
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3435c011634a04f68e0543
Content-Type: text/plain; charset=UTF-8

Are you using the PatternInputFormat from this blog post?

https://hadoopi.wordpress.com/2013/05/31/custom-recordreader-processing-string-pattern-delimited-records/

If so you need to set the pattern in the configuration before attempting to
read data with that InputFormat:

String regex = "^[A-Za-z]{3},\\s\\d{2}\\s[A-Za-z]{3}.*";
 Configuration conf = new Configuration(true);
 conf.set("record.delimiter.regex", regex);


On Tue, Apr 8, 2014 at 1:36 PM, Anurag <anurag.phadke@gmail.com> wrote:

> Hi,
> I am able to read a custom input format in spark.
> scala> val inputRead = sc.newAPIHadoopFile("hdfs://
> 127.0.0.1/user/cloudera/date_dataset/
>
> ",classOf[io.reader.PatternInputFormat],classOf[org.apache.hadoop.io.LongWritable],classOf[org.apache.hadoop.io.Text])
>
> However, doing a
> inputRead.count()
> results in null pointer exception.
> 14/04/08 13:33:39 INFO FileInputFormat: Total input paths to process : 1
> 14/04/08 13:33:39 INFO SparkContext: Starting job: count at <console>:15
> 14/04/08 13:33:39 INFO DAGScheduler: Got job 8 (count at <console>:15) with
> 1 output partitions (allowLocal=false)
> 14/04/08 13:33:39 INFO DAGScheduler: Final stage: Stage 9 (count at
> <console>:15)
> 14/04/08 13:33:39 INFO DAGScheduler: Parents of final stage: List()
> 14/04/08 13:33:39 INFO DAGScheduler: Missing parents: List()
> 14/04/08 13:33:39 INFO DAGScheduler: Submitting Stage 9 (NewHadoopRDD[19]
> at newAPIHadoopFile at <console>:12), which has no missing parents
> 14/04/08 13:33:39 INFO DAGScheduler: Submitting 1 missing tasks from Stage
> 9 (NewHadoopRDD[19] at newAPIHadoopFile at <console>:12)
> 14/04/08 13:33:39 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
> 14/04/08 13:33:39 INFO TaskSetManager: Starting task 9.0:0 as TID 8 on
> executor localhost: localhost (PROCESS_LOCAL)
> 14/04/08 13:33:39 INFO TaskSetManager: Serialized task 9.0:0 as 1297 bytes
> in 0 ms
> 14/04/08 13:33:39 INFO Executor: Running task ID 8
> 14/04/08 13:33:39 INFO BlockManager: Found block broadcast_5 locally
> 14/04/08 13:33:39 INFO NewHadoopRDD: Input split: hdfs://
> 127.0.0.1/user/cloudera/date_dataset/sample.txt:0+759
> 14/04/08 13:33:39 WARN TaskSetManager: Lost TID 8 (task 9.0:0)
> 14/04/08 13:33:39 WARN TaskSetManager: Loss was due to
> java.lang.NullPointerException
> java.lang.NullPointerException
>     at java.util.regex.Pattern.<init>(Pattern.java:1132)
>     at java.util.regex.Pattern.compile(Pattern.java:823)
>     at
> io.reader.PatternRecordReader.initialize(PatternRecordReader.java:42)
>     at
> org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:96)
>     at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:84)
>     at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:48)
>     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
>     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
>     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
>     at org.apache.spark.scheduler.Task.run(Task.scala:53)
>     at
>
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
>     at
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
>     at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
>     at
>
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>     at
>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>     at java.lang.Thread.run(Thread.java:662)
> 14/04/08 13:33:39 ERROR TaskSetManager: Task 9.0:0 failed 1 times; aborting
> job
> 14/04/08 13:33:39 INFO DAGScheduler: Failed to run count at <console>:15
> 14/04/08 13:33:39 INFO TaskSchedulerImpl: Remove TaskSet 9.0 from pool
> 14/04/08 13:33:39 ERROR Executor: Exception in task ID 8
> java.lang.NullPointerException
>     at java.util.regex.Pattern.<init>(Pattern.java:1132)
>     at java.util.regex.Pattern.compile(Pattern.java:823)
>     at
> io.reader.PatternRecordReader.initialize(PatternRecordReader.java:42)
>     at
> org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:96)
>     at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:84)
>     at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:48)
>     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
>     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
>     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
>     at org.apache.spark.scheduler.Task.run(Task.scala:53)
>     at
>
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
>     at
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
>     at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
>     at
>
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>     at
>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>     at java.lang.Thread.run(Thread.java:662)
> org.apache.spark.SparkException: Job aborted: Task 9.0:0 failed 1 times
> (most recent failure: Exception failure: java.lang.NullPointerException)
>     at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1028)
>     at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1026)
>     at
>
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>     at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>     at org.apache.spark.scheduler.DAGScheduler.org
> $apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1026)
>     at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)
>     at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)
>     at scala.Option.foreach(Option.scala:236)
>     at
>
> org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:619)
>     at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:207)
>     at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>     at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>     at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>     at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>     at
>
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>     at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>     at
>
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>     at
> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>     at
>
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>
>
> any idea what might be happening here?
>
> -anurag
>
>
>
> --
> Twitter: @anuragphadke (https://twitter.com/#!/anuragphadke)
>

--047d7b3435c011634a04f68e0543--

From dev-return-7281-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 20:47:58 2014
Return-Path: <dev-return-7281-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3922311AD3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 20:47:58 +0000 (UTC)
Received: (qmail 19038 invoked by uid 500); 8 Apr 2014 20:47:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19008 invoked by uid 500); 8 Apr 2014 20:47:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19000 invoked by uid 99); 8 Apr 2014 20:47:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 20:47:55 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of anurag.phadke@gmail.com designates 209.85.214.42 as permitted sender)
Received: from [209.85.214.42] (HELO mail-bk0-f42.google.com) (209.85.214.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 20:47:50 +0000
Received: by mail-bk0-f42.google.com with SMTP id mx12so1301878bkb.29
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 13:47:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=zjHA9dC140dvMTnKYkPPDH7H6/5oPWE7Ctq2wGYs4Dc=;
        b=oMuAQgRaR751/hXkt5/GJORX7eBSNk73uM+1w4uzCQkOYQk51f+JpVe4rCh/ruFG40
         s++WUJe9WsRzcAu9EaSOktPErNMM/m0AXzcSeFmN+TL50iFiYS2SyATUkO/7zDcLnhAa
         KuixoD89FPqhzDJxt+qiIfG/7r6y+zv91R5Ee9jHiVMoGe9o2tJtTwP2AL6UFsR9VzKb
         bn6VWRubNL6eHybpn5KKCrf1MxqTZtFT7Q7TGm7sCCDthwZvp1sE83UrCLdFUOsN/B/l
         OAb/G5Pq/frEU/AZy/05PpFJ3oTWV+QPdJlzX6gSRvwNvxDz5RoLG/1+Q6Y1VJ6PpdlN
         u4oQ==
X-Received: by 10.112.27.133 with SMTP id t5mr4113966lbg.21.1396990048819;
 Tue, 08 Apr 2014 13:47:28 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.184.99 with HTTP; Tue, 8 Apr 2014 13:47:08 -0700 (PDT)
In-Reply-To: <CA+-p3AGeOL1OcVLa+dWbj-rp7Ey3LV61aouCK0boVg4peEna=g@mail.gmail.com>
References: <CAK0jf17=0MEs=YmZ3N=SxiGhXU2ftVvAWFVvqm=R8XSsN1PDrQ@mail.gmail.com>
 <CA+-p3AGeOL1OcVLa+dWbj-rp7Ey3LV61aouCK0boVg4peEna=g@mail.gmail.com>
From: Anurag <anurag.phadke@gmail.com>
Date: Tue, 8 Apr 2014 13:47:08 -0700
Message-ID: <CAK0jf17u6=wfe-oo-S3Y8jd-z47a4+vP60u5HrOveVJtaH+ZJg@mail.gmail.com>
Subject: Re: reading custom input format in Spark
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1133b04a9d5ed404f68e17b0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133b04a9d5ed404f68e17b0
Content-Type: text/plain; charset=ISO-8859-1

andrew - yes, i am using the PatternInputFormat from the blog post you
referenced.
I know how to set the pattern in configuration while writing a MR job, how
do i do that from a spark shell?

-anurag



On Tue, Apr 8, 2014 at 1:41 PM, Andrew Ash <andrew@andrewash.com> wrote:

> Are you using the PatternInputFormat from this blog post?
>
>
> https://hadoopi.wordpress.com/2013/05/31/custom-recordreader-processing-string-pattern-delimited-records/
>
> If so you need to set the pattern in the configuration before attempting to
> read data with that InputFormat:
>
> String regex = "^[A-Za-z]{3},\\s\\d{2}\\s[A-Za-z]{3}.*";
>  Configuration conf = new Configuration(true);
>  conf.set("record.delimiter.regex", regex);
>
>
> On Tue, Apr 8, 2014 at 1:36 PM, Anurag <anurag.phadke@gmail.com> wrote:
>
> > Hi,
> > I am able to read a custom input format in spark.
> > scala> val inputRead = sc.newAPIHadoopFile("hdfs://
> > 127.0.0.1/user/cloudera/date_dataset/
> >
> >
> ",classOf[io.reader.PatternInputFormat],classOf[org.apache.hadoop.io.LongWritable],classOf[org.apache.hadoop.io.Text])
> >
> > However, doing a
> > inputRead.count()
> > results in null pointer exception.
> > 14/04/08 13:33:39 INFO FileInputFormat: Total input paths to process : 1
> > 14/04/08 13:33:39 INFO SparkContext: Starting job: count at <console>:15
> > 14/04/08 13:33:39 INFO DAGScheduler: Got job 8 (count at <console>:15)
> with
> > 1 output partitions (allowLocal=false)
> > 14/04/08 13:33:39 INFO DAGScheduler: Final stage: Stage 9 (count at
> > <console>:15)
> > 14/04/08 13:33:39 INFO DAGScheduler: Parents of final stage: List()
> > 14/04/08 13:33:39 INFO DAGScheduler: Missing parents: List()
> > 14/04/08 13:33:39 INFO DAGScheduler: Submitting Stage 9 (NewHadoopRDD[19]
> > at newAPIHadoopFile at <console>:12), which has no missing parents
> > 14/04/08 13:33:39 INFO DAGScheduler: Submitting 1 missing tasks from
> Stage
> > 9 (NewHadoopRDD[19] at newAPIHadoopFile at <console>:12)
> > 14/04/08 13:33:39 INFO TaskSchedulerImpl: Adding task set 9.0 with 1
> tasks
> > 14/04/08 13:33:39 INFO TaskSetManager: Starting task 9.0:0 as TID 8 on
> > executor localhost: localhost (PROCESS_LOCAL)
> > 14/04/08 13:33:39 INFO TaskSetManager: Serialized task 9.0:0 as 1297
> bytes
> > in 0 ms
> > 14/04/08 13:33:39 INFO Executor: Running task ID 8
> > 14/04/08 13:33:39 INFO BlockManager: Found block broadcast_5 locally
> > 14/04/08 13:33:39 INFO NewHadoopRDD: Input split: hdfs://
> > 127.0.0.1/user/cloudera/date_dataset/sample.txt:0+759
> > 14/04/08 13:33:39 WARN TaskSetManager: Lost TID 8 (task 9.0:0)
> > 14/04/08 13:33:39 WARN TaskSetManager: Loss was due to
> > java.lang.NullPointerException
> > java.lang.NullPointerException
> >     at java.util.regex.Pattern.<init>(Pattern.java:1132)
> >     at java.util.regex.Pattern.compile(Pattern.java:823)
> >     at
> > io.reader.PatternRecordReader.initialize(PatternRecordReader.java:42)
> >     at
> > org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:96)
> >     at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:84)
> >     at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:48)
> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
> >     at
> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
> >     at org.apache.spark.scheduler.Task.run(Task.scala:53)
> >     at
> >
> >
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
> >     at
> >
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
> >     at
> > org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
> >     at
> >
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
> >     at
> >
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
> >     at java.lang.Thread.run(Thread.java:662)
> > 14/04/08 13:33:39 ERROR TaskSetManager: Task 9.0:0 failed 1 times;
> aborting
> > job
> > 14/04/08 13:33:39 INFO DAGScheduler: Failed to run count at <console>:15
> > 14/04/08 13:33:39 INFO TaskSchedulerImpl: Remove TaskSet 9.0 from pool
> > 14/04/08 13:33:39 ERROR Executor: Exception in task ID 8
> > java.lang.NullPointerException
> >     at java.util.regex.Pattern.<init>(Pattern.java:1132)
> >     at java.util.regex.Pattern.compile(Pattern.java:823)
> >     at
> > io.reader.PatternRecordReader.initialize(PatternRecordReader.java:42)
> >     at
> > org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:96)
> >     at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:84)
> >     at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:48)
> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
> >     at
> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
> >     at org.apache.spark.scheduler.Task.run(Task.scala:53)
> >     at
> >
> >
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
> >     at
> >
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
> >     at
> > org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
> >     at
> >
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
> >     at
> >
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
> >     at java.lang.Thread.run(Thread.java:662)
> > org.apache.spark.SparkException: Job aborted: Task 9.0:0 failed 1 times
> > (most recent failure: Exception failure: java.lang.NullPointerException)
> >     at
> >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1028)
> >     at
> >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1026)
> >     at
> >
> >
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> >     at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> >     at org.apache.spark.scheduler.DAGScheduler.org
> > $apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1026)
> >     at
> >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)
> >     at
> >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)
> >     at scala.Option.foreach(Option.scala:236)
> >     at
> >
> >
> org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:619)
> >     at
> >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:207)
> >     at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
> >     at akka.actor.ActorCell.invoke(ActorCell.scala:456)
> >     at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
> >     at akka.dispatch.Mailbox.run(Mailbox.scala:219)
> >     at
> >
> >
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
> >     at
> scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
> >     at
> >
> >
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
> >     at
> > scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
> >     at
> >
> >
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
> >
> >
> > any idea what might be happening here?
> >
> > -anurag
> >
> >
> >
> > --
> > Twitter: @anuragphadke (https://twitter.com/#!/anuragphadke)
> >
>



-- 
Twitter: @anuragphadke (https://twitter.com/#!/anuragphadke)

--001a1133b04a9d5ed404f68e17b0--

From dev-return-7282-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 20:54:21 2014
Return-Path: <dev-return-7282-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 88FAA11AF8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 20:54:21 +0000 (UTC)
Received: (qmail 40173 invoked by uid 500); 8 Apr 2014 20:54:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39544 invoked by uid 500); 8 Apr 2014 20:54:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39505 invoked by uid 99); 8 Apr 2014 20:54:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 20:54:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.174] (HELO mail-vc0-f174.google.com) (209.85.220.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 20:54:10 +0000
Received: by mail-vc0-f174.google.com with SMTP id ld13so1267744vcb.5
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 13:53:47 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=GiTjq6gShzWGiD81wvqbOWj2mozuQVGAS2dEKbR0Iqg=;
        b=f7J1kSz+QDY2UIPIb5fJvu3x0QHF9ak9ODpg78kveJAAwh/4XEt7CfZzxoxD+Jdlix
         Qd38bigi1vevgbQmFIesh4Wz6CYeBVH0cUMAtQdfST08m/KoiIXnzRYZv/sUavdVAEgY
         TyWNKheSiK0coee2T9eA1WHcUDfpiiFPXbyo8XrmM0VTFzMg0Xttkv3DUEQS/dEBS8Bd
         tPBbhzhLDOlEhOQqgqO/lCxCIwDxqxvlwKstakouxyCizUTk77I2omNbJhRyWAfdS/cU
         0HXOyO/E3zYs/xtwWszdIG+1Xve05rao3AYUIk7clod/BTORb4WPOpVVQhlkhvx43h6A
         FGIw==
X-Gm-Message-State: ALoCoQllqAe7PsinkFwg8mBUqm/HmnsNfm6/nHAcTmpCXoVZIQvw9HsFYwFKxU6COh9sZM9I4BUU
X-Received: by 10.52.78.231 with SMTP id e7mr2997731vdx.28.1396990427714;
        Tue, 08 Apr 2014 13:53:47 -0700 (PDT)
Received: from mail-ve0-f172.google.com (mail-ve0-f172.google.com [209.85.128.172])
        by mx.google.com with ESMTPSA id u3sm5966797vdb.13.2014.04.08.13.53.46
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 08 Apr 2014 13:53:46 -0700 (PDT)
Received: by mail-ve0-f172.google.com with SMTP id jx11so1304919veb.31
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 13:53:46 -0700 (PDT)
X-Received: by 10.221.74.200 with SMTP id yx8mr4980660vcb.3.1396990426211;
 Tue, 08 Apr 2014 13:53:46 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Tue, 8 Apr 2014 13:53:25 -0700 (PDT)
In-Reply-To: <CAK0jf17u6=wfe-oo-S3Y8jd-z47a4+vP60u5HrOveVJtaH+ZJg@mail.gmail.com>
References: <CAK0jf17=0MEs=YmZ3N=SxiGhXU2ftVvAWFVvqm=R8XSsN1PDrQ@mail.gmail.com>
 <CA+-p3AGeOL1OcVLa+dWbj-rp7Ey3LV61aouCK0boVg4peEna=g@mail.gmail.com> <CAK0jf17u6=wfe-oo-S3Y8jd-z47a4+vP60u5HrOveVJtaH+ZJg@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Tue, 8 Apr 2014 13:53:25 -0700
Message-ID: <CA+-p3AHdDxGH2xsXH5-U7bvxRYs0KW-GQsXEVgK64p8L8D5wEQ@mail.gmail.com>
Subject: Re: reading custom input format in Spark
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134a56e1becdf04f68e2e12
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134a56e1becdf04f68e2e12
Content-Type: text/plain; charset=UTF-8

Anurag,

There is another method called newAPIHadoopRDD that takes in a
Configuration object rather than a path.  Give that a shot?

https://spark.apache.org/docs/latest/api/core/index.html#org.apache.spark.SparkContext


On Tue, Apr 8, 2014 at 1:47 PM, Anurag <anurag.phadke@gmail.com> wrote:

> andrew - yes, i am using the PatternInputFormat from the blog post you
> referenced.
> I know how to set the pattern in configuration while writing a MR job, how
> do i do that from a spark shell?
>
> -anurag
>
>
>
> On Tue, Apr 8, 2014 at 1:41 PM, Andrew Ash <andrew@andrewash.com> wrote:
>
> > Are you using the PatternInputFormat from this blog post?
> >
> >
> >
> https://hadoopi.wordpress.com/2013/05/31/custom-recordreader-processing-string-pattern-delimited-records/
> >
> > If so you need to set the pattern in the configuration before attempting
> to
> > read data with that InputFormat:
> >
> > String regex = "^[A-Za-z]{3},\\s\\d{2}\\s[A-Za-z]{3}.*";
> >  Configuration conf = new Configuration(true);
> >  conf.set("record.delimiter.regex", regex);
> >
> >
> > On Tue, Apr 8, 2014 at 1:36 PM, Anurag <anurag.phadke@gmail.com> wrote:
> >
> > > Hi,
> > > I am able to read a custom input format in spark.
> > > scala> val inputRead = sc.newAPIHadoopFile("hdfs://
> > > 127.0.0.1/user/cloudera/date_dataset/
> > >
> > >
> >
> ",classOf[io.reader.PatternInputFormat],classOf[org.apache.hadoop.io.LongWritable],classOf[org.apache.hadoop.io.Text])
> > >
> > > However, doing a
> > > inputRead.count()
> > > results in null pointer exception.
> > > 14/04/08 13:33:39 INFO FileInputFormat: Total input paths to process :
> 1
> > > 14/04/08 13:33:39 INFO SparkContext: Starting job: count at
> <console>:15
> > > 14/04/08 13:33:39 INFO DAGScheduler: Got job 8 (count at <console>:15)
> > with
> > > 1 output partitions (allowLocal=false)
> > > 14/04/08 13:33:39 INFO DAGScheduler: Final stage: Stage 9 (count at
> > > <console>:15)
> > > 14/04/08 13:33:39 INFO DAGScheduler: Parents of final stage: List()
> > > 14/04/08 13:33:39 INFO DAGScheduler: Missing parents: List()
> > > 14/04/08 13:33:39 INFO DAGScheduler: Submitting Stage 9
> (NewHadoopRDD[19]
> > > at newAPIHadoopFile at <console>:12), which has no missing parents
> > > 14/04/08 13:33:39 INFO DAGScheduler: Submitting 1 missing tasks from
> > Stage
> > > 9 (NewHadoopRDD[19] at newAPIHadoopFile at <console>:12)
> > > 14/04/08 13:33:39 INFO TaskSchedulerImpl: Adding task set 9.0 with 1
> > tasks
> > > 14/04/08 13:33:39 INFO TaskSetManager: Starting task 9.0:0 as TID 8 on
> > > executor localhost: localhost (PROCESS_LOCAL)
> > > 14/04/08 13:33:39 INFO TaskSetManager: Serialized task 9.0:0 as 1297
> > bytes
> > > in 0 ms
> > > 14/04/08 13:33:39 INFO Executor: Running task ID 8
> > > 14/04/08 13:33:39 INFO BlockManager: Found block broadcast_5 locally
> > > 14/04/08 13:33:39 INFO NewHadoopRDD: Input split: hdfs://
> > > 127.0.0.1/user/cloudera/date_dataset/sample.txt:0+759
> > > 14/04/08 13:33:39 WARN TaskSetManager: Lost TID 8 (task 9.0:0)
> > > 14/04/08 13:33:39 WARN TaskSetManager: Loss was due to
> > > java.lang.NullPointerException
> > > java.lang.NullPointerException
> > >     at java.util.regex.Pattern.<init>(Pattern.java:1132)
> > >     at java.util.regex.Pattern.compile(Pattern.java:823)
> > >     at
> > > io.reader.PatternRecordReader.initialize(PatternRecordReader.java:42)
> > >     at
> > > org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:96)
> > >     at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:84)
> > >     at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:48)
> > >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
> > >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
> > >     at
> > org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
> > >     at org.apache.spark.scheduler.Task.run(Task.scala:53)
> > >     at
> > >
> > >
> >
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
> > >     at
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
> > >     at
> > > org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
> > >     at
> > >
> > >
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
> > >     at
> > >
> > >
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
> > >     at java.lang.Thread.run(Thread.java:662)
> > > 14/04/08 13:33:39 ERROR TaskSetManager: Task 9.0:0 failed 1 times;
> > aborting
> > > job
> > > 14/04/08 13:33:39 INFO DAGScheduler: Failed to run count at
> <console>:15
> > > 14/04/08 13:33:39 INFO TaskSchedulerImpl: Remove TaskSet 9.0 from pool
> > > 14/04/08 13:33:39 ERROR Executor: Exception in task ID 8
> > > java.lang.NullPointerException
> > >     at java.util.regex.Pattern.<init>(Pattern.java:1132)
> > >     at java.util.regex.Pattern.compile(Pattern.java:823)
> > >     at
> > > io.reader.PatternRecordReader.initialize(PatternRecordReader.java:42)
> > >     at
> > > org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:96)
> > >     at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:84)
> > >     at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:48)
> > >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
> > >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
> > >     at
> > org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
> > >     at org.apache.spark.scheduler.Task.run(Task.scala:53)
> > >     at
> > >
> > >
> >
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
> > >     at
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
> > >     at
> > > org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
> > >     at
> > >
> > >
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
> > >     at
> > >
> > >
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
> > >     at java.lang.Thread.run(Thread.java:662)
> > > org.apache.spark.SparkException: Job aborted: Task 9.0:0 failed 1 times
> > > (most recent failure: Exception failure:
> java.lang.NullPointerException)
> > >     at
> > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1028)
> > >     at
> > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1026)
> > >     at
> > >
> > >
> >
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> > >     at
> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> > >     at org.apache.spark.scheduler.DAGScheduler.org
> > >
> $apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1026)
> > >     at
> > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)
> > >     at
> > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)
> > >     at scala.Option.foreach(Option.scala:236)
> > >     at
> > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:619)
> > >     at
> > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:207)
> > >     at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
> > >     at akka.actor.ActorCell.invoke(ActorCell.scala:456)
> > >     at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
> > >     at akka.dispatch.Mailbox.run(Mailbox.scala:219)
> > >     at
> > >
> > >
> >
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
> > >     at
> > scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
> > >     at
> > >
> > >
> >
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
> > >     at
> > >
> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
> > >     at
> > >
> > >
> >
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
> > >
> > >
> > > any idea what might be happening here?
> > >
> > > -anurag
> > >
> > >
> > >
> > > --
> > > Twitter: @anuragphadke (https://twitter.com/#!/anuragphadke)
> > >
> >
>
>
>
> --
> Twitter: @anuragphadke (https://twitter.com/#!/anuragphadke)
>

--001a1134a56e1becdf04f68e2e12--

From dev-return-7283-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 20:54:49 2014
Return-Path: <dev-return-7283-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 63E7B11B00
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 20:54:49 +0000 (UTC)
Received: (qmail 43331 invoked by uid 500); 8 Apr 2014 20:54:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43293 invoked by uid 500); 8 Apr 2014 20:54:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43281 invoked by uid 99); 8 Apr 2014 20:54:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 20:54:46 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of anurag.phadke@gmail.com designates 209.85.214.41 as permitted sender)
Received: from [209.85.214.41] (HELO mail-bk0-f41.google.com) (209.85.214.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 20:54:40 +0000
Received: by mail-bk0-f41.google.com with SMTP id d7so1299857bkh.14
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 13:54:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=D/2xW4b+GfOPKCGTWGL/J2h6KGMeirVXHENnERaMIlY=;
        b=kYRqcpgg0Nn02CRCElUjgyOhIQ3BMQWg0/FHiE/Lc7bOiE3CCDwDw7E8p7LvIPiBrh
         mInUtRsQqlof9eW3miYPVuVl5AYNFwl3ouPk7mFPhggP+Hzv4OG/WEm/HOvKRF76L79C
         jtBrKVeIU0Ck9T/t87zPE0YeUwSGnevOz7Es2bjPo4bNXGA7pQO5q6BQ9vNh/OtoAcO1
         O8Du55gyHNFt9GAl6/xk3TTVpawop0fPA6geWqYikI21dbpnfp/pzXQ+4eKkb4g3wbhG
         ZP9RFZNEN7ZNxrOR6zxZ+4AptGqwuLwzOpr5rUo3WyQIX5+Wr2knFUK5+IK3Ld6p/w/4
         +9tg==
X-Received: by 10.152.115.178 with SMTP id jp18mr4326274lab.23.1396990458756;
 Tue, 08 Apr 2014 13:54:18 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.184.99 with HTTP; Tue, 8 Apr 2014 13:53:58 -0700 (PDT)
In-Reply-To: <CAK0jf17u6=wfe-oo-S3Y8jd-z47a4+vP60u5HrOveVJtaH+ZJg@mail.gmail.com>
References: <CAK0jf17=0MEs=YmZ3N=SxiGhXU2ftVvAWFVvqm=R8XSsN1PDrQ@mail.gmail.com>
 <CA+-p3AGeOL1OcVLa+dWbj-rp7Ey3LV61aouCK0boVg4peEna=g@mail.gmail.com> <CAK0jf17u6=wfe-oo-S3Y8jd-z47a4+vP60u5HrOveVJtaH+ZJg@mail.gmail.com>
From: Anurag <anurag.phadke@gmail.com>
Date: Tue, 8 Apr 2014 13:53:58 -0700
Message-ID: <CAK0jf17ogVJod-mMhxVybMcert-BaGznexDa2msPzW6J5QD0KA@mail.gmail.com>
Subject: Re: reading custom input format in Spark
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c3474c0c82a604f68e30a7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3474c0c82a604f68e30a7
Content-Type: text/plain; charset=ISO-8859-1

andrew/nick,
thx for the input, got it to work:

sc.hadoopConfiguration.set("record.delimiter.regex",
"^[A-Za-z]{3},\\s\\d{2}\\s[A-Za-z]{3}.*")

:-)

-anurag



On Tue, Apr 8, 2014 at 1:47 PM, Anurag <anurag.phadke@gmail.com> wrote:

> andrew - yes, i am using the PatternInputFormat from the blog post you
> referenced.
> I know how to set the pattern in configuration while writing a MR job, how
> do i do that from a spark shell?
>
> -anurag
>
>
>
> On Tue, Apr 8, 2014 at 1:41 PM, Andrew Ash <andrew@andrewash.com> wrote:
>
>> Are you using the PatternInputFormat from this blog post?
>>
>>
>> https://hadoopi.wordpress.com/2013/05/31/custom-recordreader-processing-string-pattern-delimited-records/
>>
>> If so you need to set the pattern in the configuration before attempting
>> to
>> read data with that InputFormat:
>>
>> String regex = "^[A-Za-z]{3},\\s\\d{2}\\s[A-Za-z]{3}.*";
>>  Configuration conf = new Configuration(true);
>>  conf.set("record.delimiter.regex", regex);
>>
>>
>> On Tue, Apr 8, 2014 at 1:36 PM, Anurag <anurag.phadke@gmail.com> wrote:
>>
>> > Hi,
>> > I am able to read a custom input format in spark.
>> > scala> val inputRead = sc.newAPIHadoopFile("hdfs://
>> > 127.0.0.1/user/cloudera/date_dataset/
>> >
>> >
>> ",classOf[io.reader.PatternInputFormat],classOf[org.apache.hadoop.io.LongWritable],classOf[org.apache.hadoop.io.Text])
>> >
>> > However, doing a
>> > inputRead.count()
>> > results in null pointer exception.
>> > 14/04/08 13:33:39 INFO FileInputFormat: Total input paths to process : 1
>> > 14/04/08 13:33:39 INFO SparkContext: Starting job: count at <console>:15
>> > 14/04/08 13:33:39 INFO DAGScheduler: Got job 8 (count at <console>:15)
>> with
>> > 1 output partitions (allowLocal=false)
>> > 14/04/08 13:33:39 INFO DAGScheduler: Final stage: Stage 9 (count at
>> > <console>:15)
>> > 14/04/08 13:33:39 INFO DAGScheduler: Parents of final stage: List()
>> > 14/04/08 13:33:39 INFO DAGScheduler: Missing parents: List()
>> > 14/04/08 13:33:39 INFO DAGScheduler: Submitting Stage 9
>> (NewHadoopRDD[19]
>> > at newAPIHadoopFile at <console>:12), which has no missing parents
>> > 14/04/08 13:33:39 INFO DAGScheduler: Submitting 1 missing tasks from
>> Stage
>> > 9 (NewHadoopRDD[19] at newAPIHadoopFile at <console>:12)
>> > 14/04/08 13:33:39 INFO TaskSchedulerImpl: Adding task set 9.0 with 1
>> tasks
>> > 14/04/08 13:33:39 INFO TaskSetManager: Starting task 9.0:0 as TID 8 on
>> > executor localhost: localhost (PROCESS_LOCAL)
>> > 14/04/08 13:33:39 INFO TaskSetManager: Serialized task 9.0:0 as 1297
>> bytes
>> > in 0 ms
>> > 14/04/08 13:33:39 INFO Executor: Running task ID 8
>> > 14/04/08 13:33:39 INFO BlockManager: Found block broadcast_5 locally
>> > 14/04/08 13:33:39 INFO NewHadoopRDD: Input split: hdfs://
>> > 127.0.0.1/user/cloudera/date_dataset/sample.txt:0+759
>> > 14/04/08 13:33:39 WARN TaskSetManager: Lost TID 8 (task 9.0:0)
>> > 14/04/08 13:33:39 WARN TaskSetManager: Loss was due to
>> > java.lang.NullPointerException
>> > java.lang.NullPointerException
>> >     at java.util.regex.Pattern.<init>(Pattern.java:1132)
>> >     at java.util.regex.Pattern.compile(Pattern.java:823)
>> >     at
>> > io.reader.PatternRecordReader.initialize(PatternRecordReader.java:42)
>> >     at
>> > org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:96)
>> >     at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:84)
>> >     at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:48)
>> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
>> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
>> >     at
>> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
>> >     at org.apache.spark.scheduler.Task.run(Task.scala:53)
>> >     at
>> >
>> >
>> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
>> >     at
>> >
>> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
>> >     at
>> > org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
>> >     at
>> >
>> >
>> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>> >     at
>> >
>> >
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>> >     at java.lang.Thread.run(Thread.java:662)
>> > 14/04/08 13:33:39 ERROR TaskSetManager: Task 9.0:0 failed 1 times;
>> aborting
>> > job
>> > 14/04/08 13:33:39 INFO DAGScheduler: Failed to run count at <console>:15
>> > 14/04/08 13:33:39 INFO TaskSchedulerImpl: Remove TaskSet 9.0 from pool
>> > 14/04/08 13:33:39 ERROR Executor: Exception in task ID 8
>> > java.lang.NullPointerException
>> >     at java.util.regex.Pattern.<init>(Pattern.java:1132)
>> >     at java.util.regex.Pattern.compile(Pattern.java:823)
>> >     at
>> > io.reader.PatternRecordReader.initialize(PatternRecordReader.java:42)
>> >     at
>> > org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:96)
>> >     at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:84)
>> >     at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:48)
>> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
>> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
>> >     at
>> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
>> >     at org.apache.spark.scheduler.Task.run(Task.scala:53)
>> >     at
>> >
>> >
>> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:213)
>> >     at
>> >
>> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
>> >     at
>> > org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)
>> >     at
>> >
>> >
>> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>> >     at
>> >
>> >
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>> >     at java.lang.Thread.run(Thread.java:662)
>> > org.apache.spark.SparkException: Job aborted: Task 9.0:0 failed 1 times
>> > (most recent failure: Exception failure: java.lang.NullPointerException)
>> >     at
>> >
>> >
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1028)
>> >     at
>> >
>> >
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1026)
>> >     at
>> >
>> >
>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>> >     at
>> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>> >     at org.apache.spark.scheduler.DAGScheduler.org
>> >
>> $apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1026)
>> >     at
>> >
>> >
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)
>> >     at
>> >
>> >
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:619)
>> >     at scala.Option.foreach(Option.scala:236)
>> >     at
>> >
>> >
>> org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:619)
>> >     at
>> >
>> >
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:207)
>> >     at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>> >     at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>> >     at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>> >     at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>> >     at
>> >
>> >
>> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>> >     at
>> scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>> >     at
>> >
>> >
>> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>> >     at
>> > scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>> >     at
>> >
>> >
>> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>> >
>> >
>> > any idea what might be happening here?
>> >
>> > -anurag
>> >
>> >
>> >
>> > --
>> > Twitter: @anuragphadke (https://twitter.com/#!/anuragphadke)
>> >
>>
>
>
>
> --
> Twitter: @anuragphadke (https://twitter.com/#!/anuragphadke)
>



-- 
Twitter: @anuragphadke (https://twitter.com/#!/anuragphadke)

--001a11c3474c0c82a604f68e30a7--

From dev-return-7284-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 20:56:32 2014
Return-Path: <dev-return-7284-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8F05411B04
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 20:56:32 +0000 (UTC)
Received: (qmail 44617 invoked by uid 500); 8 Apr 2014 20:56:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44582 invoked by uid 500); 8 Apr 2014 20:56:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44560 invoked by uid 99); 8 Apr 2014 20:56:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 20:56:28 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,T_REMOTE_IMAGE,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ev@ooyala.com designates 209.85.213.47 as permitted sender)
Received: from [209.85.213.47] (HELO mail-yh0-f47.google.com) (209.85.213.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 20:56:24 +0000
Received: by mail-yh0-f47.google.com with SMTP id 29so1476941yhl.20
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 13:56:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=ooyala.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=k0Sxw8Ex1ZRFCe8RRi1/d/Nw8p6T18cHCPBlbSnydXk=;
        b=BHWYTry9PX6NQK999ao1CKI0AvBJuYqnUCsx7nuHk5ljn10BLP2Fbex67OVt+c3qY7
         SPfcapmidw66DXKF3f/bKRye4A8Yknpns2kpdVEBOenFpracBmJmMWPJP/FbC6MXI87b
         +NLfTVqiEZuHjIj5+LvLeJFDB9TqOGMGHnkhg=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=k0Sxw8Ex1ZRFCe8RRi1/d/Nw8p6T18cHCPBlbSnydXk=;
        b=KhZ0IjaeL0LEMbrSEYZtPUpMms6Olt7AcemIUdGndW46zo6gA6k7DXB9xhFSFC0B2N
         X+5dhu5jRVhKU/vmbjapXoi4n1xIEu9DWK8IJYuGNt4zmdKUJ5rCx4NYt/XkRHv+4zbh
         4FCsfJXXwTswmaoXqaqkgpG0oGuCoikUh9V9IRv9cCG/dzetNzXcSzRMwfGCzAh6j7Om
         OG6iSyxC14ywINGNgt1KaV6Se+4bSXmZXahhYr9dzGJoYoS6n6mJwLpJYjd8jsRRiaHD
         Vvg8/sAq0Zuhnu8yQZ3ysIbbHFkoaKNfVYntoMoSY1ieLKfzPG09cY42o/oDXI+oREPb
         mrmg==
X-Gm-Message-State: ALoCoQnZBwDihLPx+r8wkoJW2o7dH4EahRysFxN/fDUKFA8k+Dq1Hn8Ar8UNAsRmtTa0FvUS6N+L
MIME-Version: 1.0
X-Received: by 10.236.101.198 with SMTP id b46mr8753058yhg.68.1396990561454;
 Tue, 08 Apr 2014 13:56:01 -0700 (PDT)
Received: by 10.170.120.87 with HTTP; Tue, 8 Apr 2014 13:56:01 -0700 (PDT)
In-Reply-To: <CALD+6GN8-7qmHaaQHAc6=1GP+hf4_=bD6u5o83FV2aPArHMW8Q@mail.gmail.com>
References: <1396987347761-6261.post@n3.nabble.com>
	<CALD+6GN8-7qmHaaQHAc6=1GP+hf4_=bD6u5o83FV2aPArHMW8Q@mail.gmail.com>
Date: Tue, 8 Apr 2014 13:56:01 -0700
Message-ID: <CADWPM3jLXuF+iuN2WaN-Qd4mw4pywS8jo54++uVVeG9PEGFFaw@mail.gmail.com>
Subject: Re: Apache Spark and Graphx for Real Time Analytics
From: Evan Chan <ev@ooyala.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf30050b222ba2b404f68e3609
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf30050b222ba2b404f68e3609
Content-Type: text/plain; charset=ISO-8859-1

I doubt Titan would be able to give you traversal of billions of nodes in
real-time either.   In-memory traversal is typically much faster than
Cassandra-based tree traversal, even including in-memory caching.


On Tue, Apr 8, 2014 at 1:23 PM, Nick Pentreath <nick.pentreath@gmail.com>wrote:

> GraphX, like Spark, will not typically be "real-time" (where by "real-time"
> here I assume you mean of the order of a few 10s-100s ms, up to a few
> seconds).
>
> Spark can in some cases approach the upper boundary of this definition (a
> second or two, possibly less) when data is cached in memory and the
> computation is not "too heavy", while Spark Streaming may be able to get
> closer to the mid-to-upper boundary of this under similar conditions,
> especially if aggregating over relatively small windows.
>
> However, for this use case (while I haven't used GraphX yet) I would say
> something like Titan (https://github.com/thinkaurelius/titan/wiki) or a
> similar OLTP graph DB may be what you're after. But this depends on what
> kind of graph traversal you need.
>
>
>
>
> On Tue, Apr 8, 2014 at 10:02 PM, love2dishtech <love2dishtech@gmail.com
> >wrote:
>
> > Hi,
> >
> > Is Graphx on top of Apache Spark, is able to process the large scale
> > distributed graph traversal and compute, in real time. What is the query
> > execution engine distributing the query on top of graphx and apache
> spark.
> > My typical use case is a large scale distributed graph traversal in real
> > time, with billions of nodes.
> >
> > Thanks,
> > Love.
> >
> >
> >
> > --
> > View this message in context:
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-and-Graphx-for-Real-Time-Analytics-tp6261.html
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> >
>



-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>

--20cf30050b222ba2b404f68e3609--

From dev-return-7285-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 21:02:37 2014
Return-Path: <dev-return-7285-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D285111B45
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 21:02:37 +0000 (UTC)
Received: (qmail 61591 invoked by uid 500); 8 Apr 2014 21:02:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60983 invoked by uid 500); 8 Apr 2014 21:02:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60809 invoked by uid 99); 8 Apr 2014 21:02:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 21:02:26 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.212.179] (HELO mail-wi0-f179.google.com) (209.85.212.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 21:02:21 +0000
Received: by mail-wi0-f179.google.com with SMTP id z2so2146232wiv.12
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 14:01:59 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=l83qr1EAcWIFtUs93d02hZvp/YOERiatDk4ILfTTAtM=;
        b=I6f2z6tZwl1J2XNQHIKO2K1qcydFH2ySx3KvUnpdOk1zhr8YahRlv7AtqW1FoBj8xc
         0CUHwuIfhVGsl7mfXAHVlqgrmypm/Akjd/KVTl//kY8C7iGdhdEXFVlON3muWAwTf2PF
         k8nTD/LHuKZ/RpXQsJ7tJ6f/MUdRdAIst/LuoJ0YX2lToiJI/hEjEOVlIThO4vSvqPap
         3LnNVdiFe7Bt6QkHitTHrLJc4io4q2Z6ccJUyDzMDB0HNQ5RUvVe64L+lc7e93KAZ9eA
         x+iHqc23t4CirhDCtgW3dPbLXxGkb7GQW1YOKvo3t8l2j2L4bfDo5acSkRY0DS659I6X
         qIxg==
X-Gm-Message-State: ALoCoQneYvNy6+EmDytcUq9inQS7Zzu0LAVAsZJLH/ckknFMmply+AI4WwqF7RUGszFoB3CYbl3l
MIME-Version: 1.0
X-Received: by 10.194.192.132 with SMTP id hg4mr5898134wjc.28.1396990919508;
 Tue, 08 Apr 2014 14:01:59 -0700 (PDT)
Received: by 10.217.66.129 with HTTP; Tue, 8 Apr 2014 14:01:59 -0700 (PDT)
X-Originating-IP: [209.150.41.132]
In-Reply-To: <CADWPM3jLXuF+iuN2WaN-Qd4mw4pywS8jo54++uVVeG9PEGFFaw@mail.gmail.com>
References: <1396987347761-6261.post@n3.nabble.com>
	<CALD+6GN8-7qmHaaQHAc6=1GP+hf4_=bD6u5o83FV2aPArHMW8Q@mail.gmail.com>
	<CADWPM3jLXuF+iuN2WaN-Qd4mw4pywS8jo54++uVVeG9PEGFFaw@mail.gmail.com>
Date: Tue, 8 Apr 2014 17:01:59 -0400
Message-ID: <CANx3uAibvM5qoV_QqBs9yzYRmG8m=v28TJQH3AyLzHsZ56RKxQ@mail.gmail.com>
Subject: Re: Apache Spark and Graphx for Real Time Analytics
From: Koert Kuipers <koert@tresata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b873896831c0004f68e4b93
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b873896831c0004f68e4b93
Content-Type: text/plain; charset=ISO-8859-1

it all depends on what kind of traversing. if its point traversing then a
random access based something would be great.

if its more scan-like traversl then spark will fit


On Tue, Apr 8, 2014 at 4:56 PM, Evan Chan <ev@ooyala.com> wrote:

> I doubt Titan would be able to give you traversal of billions of nodes in
> real-time either.   In-memory traversal is typically much faster than
> Cassandra-based tree traversal, even including in-memory caching.
>
>
> On Tue, Apr 8, 2014 at 1:23 PM, Nick Pentreath <nick.pentreath@gmail.com
> >wrote:
>
> > GraphX, like Spark, will not typically be "real-time" (where by
> "real-time"
> > here I assume you mean of the order of a few 10s-100s ms, up to a few
> > seconds).
> >
> > Spark can in some cases approach the upper boundary of this definition (a
> > second or two, possibly less) when data is cached in memory and the
> > computation is not "too heavy", while Spark Streaming may be able to get
> > closer to the mid-to-upper boundary of this under similar conditions,
> > especially if aggregating over relatively small windows.
> >
> > However, for this use case (while I haven't used GraphX yet) I would say
> > something like Titan (https://github.com/thinkaurelius/titan/wiki) or a
> > similar OLTP graph DB may be what you're after. But this depends on what
> > kind of graph traversal you need.
> >
> >
> >
> >
> > On Tue, Apr 8, 2014 at 10:02 PM, love2dishtech <love2dishtech@gmail.com
> > >wrote:
> >
> > > Hi,
> > >
> > > Is Graphx on top of Apache Spark, is able to process the large scale
> > > distributed graph traversal and compute, in real time. What is the
> query
> > > execution engine distributing the query on top of graphx and apache
> > spark.
> > > My typical use case is a large scale distributed graph traversal in
> real
> > > time, with billions of nodes.
> > >
> > > Thanks,
> > > Love.
> > >
> > >
> > >
> > > --
> > > View this message in context:
> > >
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-and-Graphx-for-Real-Time-Analytics-tp6261.html
> > > Sent from the Apache Spark Developers List mailing list archive at
> > > Nabble.com.
> > >
> >
>
>
>
> --
> --
> Evan Chan
> Staff Engineer
> ev@ooyala.com  |
>
> <http://www.ooyala.com/>
> <http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><
> http://www.twitter.com/ooyala>
>

--047d7b873896831c0004f68e4b93--

From dev-return-7286-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 21:03:45 2014
Return-Path: <dev-return-7286-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CFD4811B58
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 21:03:45 +0000 (UTC)
Received: (qmail 64252 invoked by uid 500); 8 Apr 2014 21:03:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64184 invoked by uid 500); 8 Apr 2014 21:03:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64161 invoked by uid 99); 8 Apr 2014 21:03:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 21:03:40 +0000
X-ASF-Spam-Status: No, hits=3.8 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nick.pentreath@gmail.com designates 209.85.214.176 as permitted sender)
Received: from [209.85.214.176] (HELO mail-ob0-f176.google.com) (209.85.214.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 21:03:37 +0000
Received: by mail-ob0-f176.google.com with SMTP id wp18so1681342obc.35
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 14:03:16 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=FxjlnPQIEGx5JQgZ44BnhkEvUrYXxYx8ItVHrZMPQUc=;
        b=YGmQqOUiwZRpm7X37DKF/kG6s6UgyJEQt7pe6yTN/3wlkdcGqqI3XGx7P87FcBveNL
         dGyTCMNrSVSshpuJHBicCPGbf3NMfepc9b7BHxE4S/MRd/epqEzOWpU2Cy3jkqorsprS
         LHHNX670h0yMmT+H6X5632IZQO+sfXe39SOD+KsEgSqv8XfDZVv1xzrxXV7I1unCt0Ec
         NxJQaYfnRooEQ3k716mkB7dxpp8VbKYbNiMB3MTvTQbaLexxHpgQJE4rAUfR1mkHKlKg
         SVMkjn8iU/iLOPunuk3ZqngJktUj5H0Md8URv1Ei5Acc+uoznmNr9IqDlL4VlENQlrBP
         nZjA==
MIME-Version: 1.0
X-Received: by 10.182.153.226 with SMTP id vj2mr5302950obb.26.1396990996374;
 Tue, 08 Apr 2014 14:03:16 -0700 (PDT)
Received: by 10.182.95.103 with HTTP; Tue, 8 Apr 2014 14:03:16 -0700 (PDT)
In-Reply-To: <CADWPM3jLXuF+iuN2WaN-Qd4mw4pywS8jo54++uVVeG9PEGFFaw@mail.gmail.com>
References: <1396987347761-6261.post@n3.nabble.com>
	<CALD+6GN8-7qmHaaQHAc6=1GP+hf4_=bD6u5o83FV2aPArHMW8Q@mail.gmail.com>
	<CADWPM3jLXuF+iuN2WaN-Qd4mw4pywS8jo54++uVVeG9PEGFFaw@mail.gmail.com>
Date: Tue, 8 Apr 2014 23:03:16 +0200
Message-ID: <CALD+6GO68dZ4Mhu_nRgcbUK66=uMCFp650ewfBT+bE8=rsw87g@mail.gmail.com>
Subject: Re: Apache Spark and Graphx for Real Time Analytics
From: Nick Pentreath <nick.pentreath@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e013d0dc017ec7c04f68e5024
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013d0dc017ec7c04f68e5024
Content-Type: text/plain; charset=ISO-8859-1

Likely neither will give real-time for full-graph traversal, no. And once
in memory, GraphX would definitely be faster for "breadth-first" traversal.

But for "vertex-centric" traversals (starting from a vertex and traversing
edges from there, such as "friends of friends" queries etc) then Titan is
optimized for that use case.




On Tue, Apr 8, 2014 at 10:56 PM, Evan Chan <ev@ooyala.com> wrote:

> I doubt Titan would be able to give you traversal of billions of nodes in
> real-time either.   In-memory traversal is typically much faster than
> Cassandra-based tree traversal, even including in-memory caching.
>
>
> On Tue, Apr 8, 2014 at 1:23 PM, Nick Pentreath <nick.pentreath@gmail.com
> >wrote:
>
> > GraphX, like Spark, will not typically be "real-time" (where by
> "real-time"
> > here I assume you mean of the order of a few 10s-100s ms, up to a few
> > seconds).
> >
> > Spark can in some cases approach the upper boundary of this definition (a
> > second or two, possibly less) when data is cached in memory and the
> > computation is not "too heavy", while Spark Streaming may be able to get
> > closer to the mid-to-upper boundary of this under similar conditions,
> > especially if aggregating over relatively small windows.
> >
> > However, for this use case (while I haven't used GraphX yet) I would say
> > something like Titan (https://github.com/thinkaurelius/titan/wiki) or a
> > similar OLTP graph DB may be what you're after. But this depends on what
> > kind of graph traversal you need.
> >
> >
> >
> >
> > On Tue, Apr 8, 2014 at 10:02 PM, love2dishtech <love2dishtech@gmail.com
> > >wrote:
> >
> > > Hi,
> > >
> > > Is Graphx on top of Apache Spark, is able to process the large scale
> > > distributed graph traversal and compute, in real time. What is the
> query
> > > execution engine distributing the query on top of graphx and apache
> > spark.
> > > My typical use case is a large scale distributed graph traversal in
> real
> > > time, with billions of nodes.
> > >
> > > Thanks,
> > > Love.
> > >
> > >
> > >
> > > --
> > > View this message in context:
> > >
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-and-Graphx-for-Real-Time-Analytics-tp6261.html
> > > Sent from the Apache Spark Developers List mailing list archive at
> > > Nabble.com.
> > >
> >
>
>
>
> --
> --
> Evan Chan
> Staff Engineer
> ev@ooyala.com  |
>
> <http://www.ooyala.com/>
> <http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><
> http://www.twitter.com/ooyala>
>

--089e013d0dc017ec7c04f68e5024--

From dev-return-7287-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 21:13:25 2014
Return-Path: <dev-return-7287-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BC06C11BAF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 21:13:25 +0000 (UTC)
Received: (qmail 82302 invoked by uid 500); 8 Apr 2014 21:13:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82174 invoked by uid 500); 8 Apr 2014 21:13:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82161 invoked by uid 99); 8 Apr 2014 21:13:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 21:13:20 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.48] (HELO mail-qa0-f48.google.com) (209.85.216.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 21:13:16 +0000
Received: by mail-qa0-f48.google.com with SMTP id s7so1069664qap.35
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 14:12:53 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=UzBLjtwFw68c573v2vmmUaYtRtdsRPDiOCz5kp83/DA=;
        b=RUC8rS4+DSCV8Tlb+EApL4CVma4BRWIkNUQxKJpl1wryZkvxNe4JW989KdBEwqwJni
         MumtWOly3hr7MB3cM5dI53vANHkqL29dkHUtikfcJ6Upj7UQm4WHTGRi6iUeOR0ztWrW
         553aHVDAovycVBQmYOHhghay1Zl6JBExG4Zq5kYvLWpaep19G1LXKtLLO59j2gpnMGIX
         SUqQhNBiNisJjIupf062NTSl173r2tr7uyMzTWtFknqy1U7CCY8b+qGiozcIwIN1MN7R
         Ca6bSGgDu1I550QJ1StTRp4UZ2hSkwlQ7AFaCtf+6qs9/RFBz3YPaEumbK3YPPYwTiZB
         2UUg==
X-Gm-Message-State: ALoCoQljvhtfOkr3PfrbMGBKIuFt7y/cGSXURdGtlCa/OzKTe257vOMg5DP++LlvEKbC7OMhbG+9
MIME-Version: 1.0
X-Received: by 10.140.95.142 with SMTP id i14mr7451986qge.6.1396991573797;
 Tue, 08 Apr 2014 14:12:53 -0700 (PDT)
Received: by 10.96.126.1 with HTTP; Tue, 8 Apr 2014 14:12:53 -0700 (PDT)
In-Reply-To: <CALD+6GO68dZ4Mhu_nRgcbUK66=uMCFp650ewfBT+bE8=rsw87g@mail.gmail.com>
References: <1396987347761-6261.post@n3.nabble.com>
	<CALD+6GN8-7qmHaaQHAc6=1GP+hf4_=bD6u5o83FV2aPArHMW8Q@mail.gmail.com>
	<CADWPM3jLXuF+iuN2WaN-Qd4mw4pywS8jo54++uVVeG9PEGFFaw@mail.gmail.com>
	<CALD+6GO68dZ4Mhu_nRgcbUK66=uMCFp650ewfBT+bE8=rsw87g@mail.gmail.com>
Date: Tue, 8 Apr 2014 14:12:53 -0700
Message-ID: <CAPh_B=bQB_W-pS1+fCiKuNn0MauTQ7uUWaCwPKUUXUn1kLR_dw@mail.gmail.com>
Subject: Re: Apache Spark and Graphx for Real Time Analytics
From: Reynold Xin <rxin@databricks.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c168fe82d9e404f68e72f4
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c168fe82d9e404f68e72f4
Content-Type: text/plain; charset=ISO-8859-1

Nick and Koert summarized it pretty well. Just to clarify and give some
concrete examples.

If you want to start with a specific vertex, and follow some path, it is
probably easier and faster to use some key values store or even MySQL or a
graph database.

If you want to count the average length of paths between all nodes, or if
you want to compute the pair wise shortest path for all vertices, GraphX
will likely be way faster.






On Tue, Apr 8, 2014 at 2:03 PM, Nick Pentreath <nick.pentreath@gmail.com>wrote:

> Likely neither will give real-time for full-graph traversal, no. And once
> in memory, GraphX would definitely be faster for "breadth-first" traversal.
>
> But for "vertex-centric" traversals (starting from a vertex and traversing
> edges from there, such as "friends of friends" queries etc) then Titan is
> optimized for that use case.
>
>
>
>
> On Tue, Apr 8, 2014 at 10:56 PM, Evan Chan <ev@ooyala.com> wrote:
>
> > I doubt Titan would be able to give you traversal of billions of nodes in
> > real-time either.   In-memory traversal is typically much faster than
> > Cassandra-based tree traversal, even including in-memory caching.
> >
> >
> > On Tue, Apr 8, 2014 at 1:23 PM, Nick Pentreath <nick.pentreath@gmail.com
> > >wrote:
> >
> > > GraphX, like Spark, will not typically be "real-time" (where by
> > "real-time"
> > > here I assume you mean of the order of a few 10s-100s ms, up to a few
> > > seconds).
> > >
> > > Spark can in some cases approach the upper boundary of this definition
> (a
> > > second or two, possibly less) when data is cached in memory and the
> > > computation is not "too heavy", while Spark Streaming may be able to
> get
> > > closer to the mid-to-upper boundary of this under similar conditions,
> > > especially if aggregating over relatively small windows.
> > >
> > > However, for this use case (while I haven't used GraphX yet) I would
> say
> > > something like Titan (https://github.com/thinkaurelius/titan/wiki) or
> a
> > > similar OLTP graph DB may be what you're after. But this depends on
> what
> > > kind of graph traversal you need.
> > >
> > >
> > >
> > >
> > > On Tue, Apr 8, 2014 at 10:02 PM, love2dishtech <
> love2dishtech@gmail.com
> > > >wrote:
> > >
> > > > Hi,
> > > >
> > > > Is Graphx on top of Apache Spark, is able to process the large scale
> > > > distributed graph traversal and compute, in real time. What is the
> > query
> > > > execution engine distributing the query on top of graphx and apache
> > > spark.
> > > > My typical use case is a large scale distributed graph traversal in
> > real
> > > > time, with billions of nodes.
> > > >
> > > > Thanks,
> > > > Love.
> > > >
> > > >
> > > >
> > > > --
> > > > View this message in context:
> > > >
> > >
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-and-Graphx-for-Real-Time-Analytics-tp6261.html
> > > > Sent from the Apache Spark Developers List mailing list archive at
> > > > Nabble.com.
> > > >
> > >
> >
> >
> >
> > --
> > --
> > Evan Chan
> > Staff Engineer
> > ev@ooyala.com  |
> >
> > <http://www.ooyala.com/>
> > <http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala
> ><
> > http://www.twitter.com/ooyala>
> >
>

--001a11c168fe82d9e404f68e72f4--

From dev-return-7288-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 22:44:42 2014
Return-Path: <dev-return-7288-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7624D11F81
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 22:44:42 +0000 (UTC)
Received: (qmail 11380 invoked by uid 500); 8 Apr 2014 22:44:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11105 invoked by uid 500); 8 Apr 2014 22:44:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11080 invoked by uid 99); 8 Apr 2014 22:44:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 22:44:38 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.81 as permitted sender)
Received: from [171.67.219.81] (HELO smtp.stanford.edu) (171.67.219.81)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 22:44:34 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id CB64120E9D
	for <dev@spark.apache.org>; Tue,  8 Apr 2014 15:44:13 -0700 (PDT)
Received: from mail-qg0-f44.google.com (mail-qg0-f44.google.com [209.85.192.44])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 03D5820F0D
	for <dev@spark.apache.org>; Tue,  8 Apr 2014 15:44:12 -0700 (PDT)
Received: by mail-qg0-f44.google.com with SMTP id a108so1532800qge.31
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 15:44:11 -0700 (PDT)
X-Gm-Message-State: ALoCoQnalDiXdWg45Qd2+SqCko9xvHstH8Pv4gqsiucStwJ+6tDEukorBYD1bUWJT5O3TZpbjMoI
MIME-Version: 1.0
X-Received: by 10.229.216.72 with SMTP id hh8mr8296137qcb.9.1396997051180;
 Tue, 08 Apr 2014 15:44:11 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Tue, 8 Apr 2014 15:44:11 -0700 (PDT)
In-Reply-To: <CA+B-+fxaN-oYuxiDar8RL9DdUcU73Y_giOLrDt_rvFT_657-iA@mail.gmail.com>
References: <CAFkVJifBb14oGvE4zHSL0=s1na9neiKVRGRWmUC24wWnP_9Miw@mail.gmail.com>
	<CAEYYnxbo10PHJsY=s4s2f-3PsSed-F+9zmsz35ELGSfGVhnuHg@mail.gmail.com>
	<CA+B-+fy-Cr9f4OZ4ztuqSnLNKwUMYHKN7GOg1nU2dmFO87rSjA@mail.gmail.com>
	<CA+B-+fxaN-oYuxiDar8RL9DdUcU73Y_giOLrDt_rvFT_657-iA@mail.gmail.com>
Date: Tue, 8 Apr 2014 15:44:11 -0700
Message-ID: <CAEYYnxbi4u1j7Z0DT-W_MTg+xH2A_CB59-PDX8r-PXvb9SczSg@mail.gmail.com>
Subject: Re: MLLib - Thoughts about refactoring Updater for LBFGS?
From: DB Tsai <dbtsai@stanford.edu>
To: Debasish Das <debasish.das83@gmail.com>
Cc: dlwh@cs.berkeley.edu, Xiangrui Meng <mengxr@gmail.com>, dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Debasish,

The L-BFGS solver will be in the master like GD solver, and the part
that is parallelized is computing the gradient of each input row, and
summing them up.

I prefer to make the optimizer plug-able instead of adding new
LogisticRegressionWithLBFGS since 98% of the code will be the same.

Nice to have something like this,

class LogisticRegression private (
    var optimizer: Optimizer)
  extends GeneralizedLinearAlgorithm[LogisticRegressionModel]

The following parameters will be setup in the optimizers, and they
should because they are part of optimization parameters.

    var stepSize: Double,
    var numIterations: Int,
    var regParam: Double,
    var miniBatchFraction: Double

Xiangrui, what do you think?

For now, you can use my L-BFGS solver by copying and pasting the
LogisticRegressionWithSGD code, and changing the optimizer to L-BFGS.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Tue, Apr 8, 2014 at 9:42 AM, Debasish Das <debasish.das83@gmail.com> wrote:
> Hi DB,
>
> Are we going to clean up the function:
>
> class LogisticRegressionWithSGD private (
>     var stepSize: Double,
>     var numIterations: Int,
>     var regParam: Double,
>     var miniBatchFraction: Double)
>   extends GeneralizedLinearAlgorithm[LogisticRegressionModel] with
> Serializable {
>
>   val gradient = new LogisticGradient()
>   val updater = new SimpleUpdater()
>   override val optimizer = new GradientDescent(gradient, updater)
>
> Or add a new one ?
>
> class LogisticRegressionWithBFGS ?
>
> The WithABC is optional since optimizer could be picked up either based on a
> flag...there are only 3 options for optimizor:
>
> 1. GradientDescent
> 2. Quasi Newton
> 3. Newton
>
> May be we add an enum for optimization type....and then under
> GradientDescent family people can add their variants of SGD....Not sure if
> ConjugateGradient comes under 1 or 2....may be we need 4 options...
>
> Thanks.
> Deb
>
>
> On Mon, Apr 7, 2014 at 11:23 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>>
>> I got your checkin....I need to run logistic regression SGD vs BFGS for my
>> current usecases but your next checkin will update the logistic regression
>> with LBFGS right ? Are you adding it to regression package as well ?
>>
>> Thanks.
>> Deb
>>
>>
>> On Mon, Apr 7, 2014 at 7:00 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>
>>> Hi guys,
>>>
>>> The latest PR uses Breeze's L-BFGS implement which is introduced by
>>> Xiangrui's sparse input format work in SPARK-1212.
>>>
>>> https://github.com/apache/spark/pull/353
>>>
>>> Now, it works with the new sparse framework!
>>>
>>> Any feedback would be greatly appreciated.
>>>
>>> Thanks.
>>>
>>> Sincerely,
>>>
>>> DB Tsai
>>> -------------------------------------------------------
>>> My Blog: https://www.dbtsai.com
>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>
>>>
>>> On Thu, Apr 3, 2014 at 5:02 PM, DB Tsai <dbtsai@alpinenow.com> wrote:
>>> > ---------- Forwarded message ----------
>>> > From: David Hall <dlwh@cs.berkeley.edu>
>>> > Date: Sat, Mar 15, 2014 at 10:02 AM
>>> > Subject: Re: MLLib - Thoughts about refactoring Updater for LBFGS?
>>> > To: DB Tsai <dbtsai@alpinenow.com>
>>> >
>>> >
>>> > On Fri, Mar 7, 2014 at 10:56 PM, DB Tsai <dbtsai@alpinenow.com> wrote:
>>> >>
>>> >> Hi David,
>>> >>
>>> >> Please let me know the version of Breeze that LBFGS can be serialized,
>>> >> and CachedDiffFunction is built-in in LBFGS once you finish. I'll
>>> >> update the PR to Spark from using RISO implementation to Breeze
>>> >> implementation.
>>> >
>>> >
>>> > The current master (0.7-SNAPSHOT) has these problems fixed.
>>> >
>>> >>
>>> >>
>>> >> Thanks.
>>> >>
>>> >> Sincerely,
>>> >>
>>> >> DB Tsai
>>> >> Machine Learning Engineer
>>> >> Alpine Data Labs
>>> >> --------------------------------------
>>> >> Web: http://alpinenow.com/
>>> >>
>>> >>
>>> >> On Thu, Mar 6, 2014 at 4:26 PM, David Hall <dlwh@cs.berkeley.edu>
>>> >> wrote:
>>> >> > On Thu, Mar 6, 2014 at 4:21 PM, DB Tsai <dbtsai@alpinenow.com>
>>> >> > wrote:
>>> >> >
>>> >> >> Hi David,
>>> >> >>
>>> >> >> I can converge to the same result with your breeze LBFGS and
>>> >> >> Fortran
>>> >> >> implementations now. Probably, I made some mistakes when I tried
>>> >> >> breeze before. I apologize that I claimed it's not stable.
>>> >> >>
>>> >> >> See the test case in BreezeLBFGSSuite.scala
>>> >> >> https://github.com/AlpineNow/spark/tree/dbtsai-breezeLBFGS
>>> >> >>
>>> >> >> This is training multinomial logistic regression against iris
>>> >> >> dataset,
>>> >> >> and both optimizers can train the models with 98% training
>>> >> >> accuracy.
>>> >> >>
>>> >> >
>>> >> > great to hear! There were some bugs in LBFGS about 6 months ago, so
>>> >> > depending on the last time you tried it, it might indeed have been
>>> >> > bugged.
>>> >> >
>>> >> >
>>> >> >>
>>> >> >> There are two issues to use Breeze in Spark,
>>> >> >>
>>> >> >> 1) When the gradientSum and lossSum are computed distributively in
>>> >> >> custom defined DiffFunction which will be passed into your
>>> >> >> optimizer,
>>> >> >> Spark will complain LBFGS class is not serializable. In
>>> >> >> BreezeLBFGS.scala, I've to convert RDD to array to make it work
>>> >> >> locally. It should be easy to fix by just having LBFGS to implement
>>> >> >> Serializable.
>>> >> >>
>>> >> >
>>> >> > I'm not sure why Spark should be serializing LBFGS? Shouldn't it
>>> >> > live on
>>> >> > the controller node? Or is this a per-node thing?
>>> >> >
>>> >> > But no problem to make it serializable.
>>> >> >
>>> >> >
>>> >> >>
>>> >> >> 2) Breeze computes redundant gradient and loss. See the following
>>> >> >> log
>>> >> >> from both Fortran and Breeze implementations.
>>> >> >>
>>> >> >
>>> >> > Err, yeah. I should probably have LBFGS do this automatically, but
>>> >> > there's
>>> >> > a CachedDiffFunction that gets rid of the redundant calculations.
>>> >> >
>>> >> > -- David
>>> >> >
>>> >> >
>>> >> >>
>>> >> >> Thanks.
>>> >> >>
>>> >> >> Fortran:
>>> >> >> Iteration -1: loss 1.3862943611198926, diff 1.0
>>> >> >> Iteration 0: loss 1.5846343143210866, diff 0.14307193024217352
>>> >> >> Iteration 1: loss 1.1242501524477688, diff 0.29053004039012126
>>> >> >> Iteration 2: loss 1.0930151243303563, diff 0.027782962952189336
>>> >> >> Iteration 3: loss 1.054036932835569, diff 0.03566113127440601
>>> >> >> Iteration 4: loss 0.9907956302751622, diff 0.05999907649459571
>>> >> >> Iteration 5: loss 0.9184205380342829, diff 0.07304737423337761
>>> >> >> Iteration 6: loss 0.8259870936519937, diff 0.10064381175132982
>>> >> >> Iteration 7: loss 0.6327447552109574, diff 0.23395293458364716
>>> >> >> Iteration 8: loss 0.5534101162436359, diff 0.1253815427665277
>>> >> >> Iteration 9: loss 0.4045020086612566, diff 0.26907321376758075
>>> >> >> Iteration 10: loss 0.3078824990823728, diff 0.23885980452569627
>>> >> >>
>>> >> >> Breeze:
>>> >> >> Iteration -1: loss 1.3862943611198926, diff 1.0
>>> >> >> Mar 6, 2014 3:59:11 PM com.github.fommil.netlib.BLAS <clinit>
>>> >> >> WARNING: Failed to load implementation from:
>>> >> >> com.github.fommil.netlib.NativeSystemBLAS
>>> >> >> Mar 6, 2014 3:59:11 PM com.github.fommil.netlib.BLAS <clinit>
>>> >> >> WARNING: Failed to load implementation from:
>>> >> >> com.github.fommil.netlib.NativeRefBLAS
>>> >> >> Iteration 0: loss 1.3862943611198926, diff 0.0
>>> >> >> Iteration 1: loss 1.5846343143210866, diff 0.14307193024217352
>>> >> >> Iteration 2: loss 1.1242501524477688, diff 0.29053004039012126
>>> >> >> Iteration 3: loss 1.1242501524477688, diff 0.0
>>> >> >> Iteration 4: loss 1.1242501524477688, diff 0.0
>>> >> >> Iteration 5: loss 1.0930151243303563, diff 0.027782962952189336
>>> >> >> Iteration 6: loss 1.0930151243303563, diff 0.0
>>> >> >> Iteration 7: loss 1.0930151243303563, diff 0.0
>>> >> >> Iteration 8: loss 1.054036932835569, diff 0.03566113127440601
>>> >> >> Iteration 9: loss 1.054036932835569, diff 0.0
>>> >> >> Iteration 10: loss 1.054036932835569, diff 0.0
>>> >> >> Iteration 11: loss 0.9907956302751622, diff 0.05999907649459571
>>> >> >> Iteration 12: loss 0.9907956302751622, diff 0.0
>>> >> >> Iteration 13: loss 0.9907956302751622, diff 0.0
>>> >> >> Iteration 14: loss 0.9184205380342829, diff 0.07304737423337761
>>> >> >> Iteration 15: loss 0.9184205380342829, diff 0.0
>>> >> >> Iteration 16: loss 0.9184205380342829, diff 0.0
>>> >> >> Iteration 17: loss 0.8259870936519939, diff 0.1006438117513297
>>> >> >> Iteration 18: loss 0.8259870936519939, diff 0.0
>>> >> >> Iteration 19: loss 0.8259870936519939, diff 0.0
>>> >> >> Iteration 20: loss 0.6327447552109576, diff 0.233952934583647
>>> >> >> Iteration 21: loss 0.6327447552109576, diff 0.0
>>> >> >> Iteration 22: loss 0.6327447552109576, diff 0.0
>>> >> >> Iteration 23: loss 0.5534101162436362, diff 0.12538154276652747
>>> >> >> Iteration 24: loss 0.5534101162436362, diff 0.0
>>> >> >> Iteration 25: loss 0.5534101162436362, diff 0.0
>>> >> >> Iteration 26: loss 0.40450200866125635, diff 0.2690732137675816
>>> >> >> Iteration 27: loss 0.40450200866125635, diff 0.0
>>> >> >> Iteration 28: loss 0.40450200866125635, diff 0.0
>>> >> >> Iteration 29: loss 0.30788249908237314, diff 0.23885980452569502
>>> >> >>
>>> >> >> Sincerely,
>>> >> >>
>>> >> >> DB Tsai
>>> >> >> Machine Learning Engineer
>>> >> >> Alpine Data Labs
>>> >> >> --------------------------------------
>>> >> >> Web: http://alpinenow.com/
>>> >> >>
>>> >> >>
>>> >> >> On Wed, Mar 5, 2014 at 2:00 PM, David Hall <dlwh@cs.berkeley.edu>
>>> >> >> wrote:
>>> >> >> > On Wed, Mar 5, 2014 at 1:57 PM, DB Tsai <dbtsai@alpinenow.com>
>>> >> >> > wrote:
>>> >> >> >
>>> >> >> >> Hi David,
>>> >> >> >>
>>> >> >> >> On Tue, Mar 4, 2014 at 8:13 PM, dlwh <david.lw.hall@gmail.com>
>>> >> >> >> wrote:
>>> >> >> >> > I'm happy to help fix any problems. I've verified at points
>>> >> >> >> > that
>>> >> >> >> > the
>>> >> >> >> > implementation gives the exact same sequence of iterates for a
>>> >> >> >> > few
>>> >> >> >> different
>>> >> >> >> > functions (with a particular line search) as the c port of
>>> >> >> >> > lbfgs.
>>> >> >> >> > So
>>> >> >> I'm
>>> >> >> >> a
>>> >> >> >> > little surprised it fails where Fortran succeeds... but only a
>>> >> >> >> > little.
>>> >> >> >> This
>>> >> >> >> > was fixed late last year.
>>> >> >> >> I'm working on a reproducible test case using breeze vs fortran
>>> >> >> >> implementation to show the problem I've run into. The test will
>>> >> >> >> be
>>> >> >> >> in
>>> >> >> >> one of the test cases in my Spark fork, is it okay for you to
>>> >> >> >> investigate the issue? Or do I need to make it as a standalone
>>> >> >> >> test?
>>> >> >> >>
>>> >> >> >
>>> >> >> >
>>> >> >> > Um, as long as it wouldn't be too hard to pull out.
>>> >> >> >
>>> >> >> >
>>> >> >> >>
>>> >> >> >> Will send you the test later today.
>>> >> >> >>
>>> >> >> >> Thanks.
>>> >> >> >>
>>> >> >> >> Sincerely,
>>> >> >> >>
>>> >> >> >> DB Tsai
>>> >> >> >> Machine Learning Engineer
>>> >> >> >> Alpine Data Labs
>>> >> >> >> --------------------------------------
>>> >> >> >> Web: http://alpinenow.com/
>>> >> >> >>
>>> >> >>
>>> >
>>> >
>>> >
>>
>>
>

From dev-return-7289-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 23:05:46 2014
Return-Path: <dev-return-7289-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4B6B110028
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 23:05:46 +0000 (UTC)
Received: (qmail 46682 invoked by uid 500); 8 Apr 2014 23:05:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46559 invoked by uid 500); 8 Apr 2014 23:05:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46550 invoked by uid 99); 8 Apr 2014 23:05:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 23:05:43 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.214.182 as permitted sender)
Received: from [209.85.214.182] (HELO mail-ob0-f182.google.com) (209.85.214.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 23:05:39 +0000
Received: by mail-ob0-f182.google.com with SMTP id uz6so1836515obc.13
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 16:05:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=yTWVPW+H/2hr/arVvve9cu8A6NNmyUrCpVL4mvZZFXc=;
        b=nw3j2ld1g+sAg9u/YO2arzXTwhWZ7Z7UE4/p6QK1IxxUQ/9B2KrFBfulP9jhgjLINT
         9xjO26ZcPHnRGtEgwP/q6E720RaNFJznLy+7THNbHWybCk3vnf6e3La+EFF+FseOMDHP
         YURVJmYRt0wr0rKz+5myg58o/ig4cunc8NGfYAa4kHsGEt6c+XyjGnt031H6wixrkBsV
         BBuUH98l62lNg4uNKYoGKSqL8d+W7ccbdzr0Dtw7ck1h5V7INk8RmXiabprnbPzQp3UB
         QUxx8zontURQ73f+VH4hLrrUdJD/YU67VKSq+Chkn8UVHQC7knwQvpT9gkvWfTtwPXo5
         DbyQ==
MIME-Version: 1.0
X-Received: by 10.182.112.231 with SMTP id it7mr5619661obb.8.1396998318833;
 Tue, 08 Apr 2014 16:05:18 -0700 (PDT)
Received: by 10.182.142.163 with HTTP; Tue, 8 Apr 2014 16:05:18 -0700 (PDT)
In-Reply-To: <CAEYYnxbi4u1j7Z0DT-W_MTg+xH2A_CB59-PDX8r-PXvb9SczSg@mail.gmail.com>
References: <CAFkVJifBb14oGvE4zHSL0=s1na9neiKVRGRWmUC24wWnP_9Miw@mail.gmail.com>
	<CAEYYnxbo10PHJsY=s4s2f-3PsSed-F+9zmsz35ELGSfGVhnuHg@mail.gmail.com>
	<CA+B-+fy-Cr9f4OZ4ztuqSnLNKwUMYHKN7GOg1nU2dmFO87rSjA@mail.gmail.com>
	<CA+B-+fxaN-oYuxiDar8RL9DdUcU73Y_giOLrDt_rvFT_657-iA@mail.gmail.com>
	<CAEYYnxbi4u1j7Z0DT-W_MTg+xH2A_CB59-PDX8r-PXvb9SczSg@mail.gmail.com>
Date: Tue, 8 Apr 2014 16:05:18 -0700
Message-ID: <CA+B-+fz2-bAMLi+kBnT36UGSKcJ32gi2RrzsttL9nDVgRD=cjg@mail.gmail.com>
Subject: Re: MLLib - Thoughts about refactoring Updater for LBFGS?
From: Debasish Das <debasish.das83@gmail.com>
To: DB Tsai <dbtsai@stanford.edu>
Cc: dlwh@cs.berkeley.edu, Xiangrui Meng <mengxr@gmail.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0149c9f88bc50c04f6900421
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149c9f88bc50c04f6900421
Content-Type: text/plain; charset=ISO-8859-1

Yup that's what I expected...L-BFGS solver is in the master and gradient
computation per RDD is done on each of the workers...

This miniBatchFraction is also a heuristic which I don't think makes sense
for LogisticRegressionWithBFGS...does it ?


On Tue, Apr 8, 2014 at 3:44 PM, DB Tsai <dbtsai@stanford.edu> wrote:

> Hi Debasish,
>
> The L-BFGS solver will be in the master like GD solver, and the part
> that is parallelized is computing the gradient of each input row, and
> summing them up.
>
> I prefer to make the optimizer plug-able instead of adding new
> LogisticRegressionWithLBFGS since 98% of the code will be the same.
>
> Nice to have something like this,
>
> class LogisticRegression private (
>     var optimizer: Optimizer)
>   extends GeneralizedLinearAlgorithm[LogisticRegressionModel]
>
> The following parameters will be setup in the optimizers, and they
> should because they are part of optimization parameters.
>
>     var stepSize: Double,
>     var numIterations: Int,
>     var regParam: Double,
>     var miniBatchFraction: Double
>
> Xiangrui, what do you think?
>
> For now, you can use my L-BFGS solver by copying and pasting the
> LogisticRegressionWithSGD code, and changing the optimizer to L-BFGS.
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Tue, Apr 8, 2014 at 9:42 AM, Debasish Das <debasish.das83@gmail.com>
> wrote:
> > Hi DB,
> >
> > Are we going to clean up the function:
> >
> > class LogisticRegressionWithSGD private (
> >     var stepSize: Double,
> >     var numIterations: Int,
> >     var regParam: Double,
> >     var miniBatchFraction: Double)
> >   extends GeneralizedLinearAlgorithm[LogisticRegressionModel] with
> > Serializable {
> >
> >   val gradient = new LogisticGradient()
> >   val updater = new SimpleUpdater()
> >   override val optimizer = new GradientDescent(gradient, updater)
> >
> > Or add a new one ?
> >
> > class LogisticRegressionWithBFGS ?
> >
> > The WithABC is optional since optimizer could be picked up either based
> on a
> > flag...there are only 3 options for optimizor:
> >
> > 1. GradientDescent
> > 2. Quasi Newton
> > 3. Newton
> >
> > May be we add an enum for optimization type....and then under
> > GradientDescent family people can add their variants of SGD....Not sure
> if
> > ConjugateGradient comes under 1 or 2....may be we need 4 options...
> >
> > Thanks.
> > Deb
> >
> >
> > On Mon, Apr 7, 2014 at 11:23 PM, Debasish Das <debasish.das83@gmail.com>
> > wrote:
> >>
> >> I got your checkin....I need to run logistic regression SGD vs BFGS for
> my
> >> current usecases but your next checkin will update the logistic
> regression
> >> with LBFGS right ? Are you adding it to regression package as well ?
> >>
> >> Thanks.
> >> Deb
> >>
> >>
> >> On Mon, Apr 7, 2014 at 7:00 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> >>>
> >>> Hi guys,
> >>>
> >>> The latest PR uses Breeze's L-BFGS implement which is introduced by
> >>> Xiangrui's sparse input format work in SPARK-1212.
> >>>
> >>> https://github.com/apache/spark/pull/353
> >>>
> >>> Now, it works with the new sparse framework!
> >>>
> >>> Any feedback would be greatly appreciated.
> >>>
> >>> Thanks.
> >>>
> >>> Sincerely,
> >>>
> >>> DB Tsai
> >>> -------------------------------------------------------
> >>> My Blog: https://www.dbtsai.com
> >>> LinkedIn: https://www.linkedin.com/in/dbtsai
> >>>
> >>>
> >>> On Thu, Apr 3, 2014 at 5:02 PM, DB Tsai <dbtsai@alpinenow.com> wrote:
> >>> > ---------- Forwarded message ----------
> >>> > From: David Hall <dlwh@cs.berkeley.edu>
> >>> > Date: Sat, Mar 15, 2014 at 10:02 AM
> >>> > Subject: Re: MLLib - Thoughts about refactoring Updater for LBFGS?
> >>> > To: DB Tsai <dbtsai@alpinenow.com>
> >>> >
> >>> >
> >>> > On Fri, Mar 7, 2014 at 10:56 PM, DB Tsai <dbtsai@alpinenow.com>
> wrote:
> >>> >>
> >>> >> Hi David,
> >>> >>
> >>> >> Please let me know the version of Breeze that LBFGS can be
> serialized,
> >>> >> and CachedDiffFunction is built-in in LBFGS once you finish. I'll
> >>> >> update the PR to Spark from using RISO implementation to Breeze
> >>> >> implementation.
> >>> >
> >>> >
> >>> > The current master (0.7-SNAPSHOT) has these problems fixed.
> >>> >
> >>> >>
> >>> >>
> >>> >> Thanks.
> >>> >>
> >>> >> Sincerely,
> >>> >>
> >>> >> DB Tsai
> >>> >> Machine Learning Engineer
> >>> >> Alpine Data Labs
> >>> >> --------------------------------------
> >>> >> Web: http://alpinenow.com/
> >>> >>
> >>> >>
> >>> >> On Thu, Mar 6, 2014 at 4:26 PM, David Hall <dlwh@cs.berkeley.edu>
> >>> >> wrote:
> >>> >> > On Thu, Mar 6, 2014 at 4:21 PM, DB Tsai <dbtsai@alpinenow.com>
> >>> >> > wrote:
> >>> >> >
> >>> >> >> Hi David,
> >>> >> >>
> >>> >> >> I can converge to the same result with your breeze LBFGS and
> >>> >> >> Fortran
> >>> >> >> implementations now. Probably, I made some mistakes when I tried
> >>> >> >> breeze before. I apologize that I claimed it's not stable.
> >>> >> >>
> >>> >> >> See the test case in BreezeLBFGSSuite.scala
> >>> >> >> https://github.com/AlpineNow/spark/tree/dbtsai-breezeLBFGS
> >>> >> >>
> >>> >> >> This is training multinomial logistic regression against iris
> >>> >> >> dataset,
> >>> >> >> and both optimizers can train the models with 98% training
> >>> >> >> accuracy.
> >>> >> >>
> >>> >> >
> >>> >> > great to hear! There were some bugs in LBFGS about 6 months ago,
> so
> >>> >> > depending on the last time you tried it, it might indeed have been
> >>> >> > bugged.
> >>> >> >
> >>> >> >
> >>> >> >>
> >>> >> >> There are two issues to use Breeze in Spark,
> >>> >> >>
> >>> >> >> 1) When the gradientSum and lossSum are computed distributively
> in
> >>> >> >> custom defined DiffFunction which will be passed into your
> >>> >> >> optimizer,
> >>> >> >> Spark will complain LBFGS class is not serializable. In
> >>> >> >> BreezeLBFGS.scala, I've to convert RDD to array to make it work
> >>> >> >> locally. It should be easy to fix by just having LBFGS to
> implement
> >>> >> >> Serializable.
> >>> >> >>
> >>> >> >
> >>> >> > I'm not sure why Spark should be serializing LBFGS? Shouldn't it
> >>> >> > live on
> >>> >> > the controller node? Or is this a per-node thing?
> >>> >> >
> >>> >> > But no problem to make it serializable.
> >>> >> >
> >>> >> >
> >>> >> >>
> >>> >> >> 2) Breeze computes redundant gradient and loss. See the following
> >>> >> >> log
> >>> >> >> from both Fortran and Breeze implementations.
> >>> >> >>
> >>> >> >
> >>> >> > Err, yeah. I should probably have LBFGS do this automatically, but
> >>> >> > there's
> >>> >> > a CachedDiffFunction that gets rid of the redundant calculations.
> >>> >> >
> >>> >> > -- David
> >>> >> >
> >>> >> >
> >>> >> >>
> >>> >> >> Thanks.
> >>> >> >>
> >>> >> >> Fortran:
> >>> >> >> Iteration -1: loss 1.3862943611198926, diff 1.0
> >>> >> >> Iteration 0: loss 1.5846343143210866, diff 0.14307193024217352
> >>> >> >> Iteration 1: loss 1.1242501524477688, diff 0.29053004039012126
> >>> >> >> Iteration 2: loss 1.0930151243303563, diff 0.027782962952189336
> >>> >> >> Iteration 3: loss 1.054036932835569, diff 0.03566113127440601
> >>> >> >> Iteration 4: loss 0.9907956302751622, diff 0.05999907649459571
> >>> >> >> Iteration 5: loss 0.9184205380342829, diff 0.07304737423337761
> >>> >> >> Iteration 6: loss 0.8259870936519937, diff 0.10064381175132982
> >>> >> >> Iteration 7: loss 0.6327447552109574, diff 0.23395293458364716
> >>> >> >> Iteration 8: loss 0.5534101162436359, diff 0.1253815427665277
> >>> >> >> Iteration 9: loss 0.4045020086612566, diff 0.26907321376758075
> >>> >> >> Iteration 10: loss 0.3078824990823728, diff 0.23885980452569627
> >>> >> >>
> >>> >> >> Breeze:
> >>> >> >> Iteration -1: loss 1.3862943611198926, diff 1.0
> >>> >> >> Mar 6, 2014 3:59:11 PM com.github.fommil.netlib.BLAS <clinit>
> >>> >> >> WARNING: Failed to load implementation from:
> >>> >> >> com.github.fommil.netlib.NativeSystemBLAS
> >>> >> >> Mar 6, 2014 3:59:11 PM com.github.fommil.netlib.BLAS <clinit>
> >>> >> >> WARNING: Failed to load implementation from:
> >>> >> >> com.github.fommil.netlib.NativeRefBLAS
> >>> >> >> Iteration 0: loss 1.3862943611198926, diff 0.0
> >>> >> >> Iteration 1: loss 1.5846343143210866, diff 0.14307193024217352
> >>> >> >> Iteration 2: loss 1.1242501524477688, diff 0.29053004039012126
> >>> >> >> Iteration 3: loss 1.1242501524477688, diff 0.0
> >>> >> >> Iteration 4: loss 1.1242501524477688, diff 0.0
> >>> >> >> Iteration 5: loss 1.0930151243303563, diff 0.027782962952189336
> >>> >> >> Iteration 6: loss 1.0930151243303563, diff 0.0
> >>> >> >> Iteration 7: loss 1.0930151243303563, diff 0.0
> >>> >> >> Iteration 8: loss 1.054036932835569, diff 0.03566113127440601
> >>> >> >> Iteration 9: loss 1.054036932835569, diff 0.0
> >>> >> >> Iteration 10: loss 1.054036932835569, diff 0.0
> >>> >> >> Iteration 11: loss 0.9907956302751622, diff 0.05999907649459571
> >>> >> >> Iteration 12: loss 0.9907956302751622, diff 0.0
> >>> >> >> Iteration 13: loss 0.9907956302751622, diff 0.0
> >>> >> >> Iteration 14: loss 0.9184205380342829, diff 0.07304737423337761
> >>> >> >> Iteration 15: loss 0.9184205380342829, diff 0.0
> >>> >> >> Iteration 16: loss 0.9184205380342829, diff 0.0
> >>> >> >> Iteration 17: loss 0.8259870936519939, diff 0.1006438117513297
> >>> >> >> Iteration 18: loss 0.8259870936519939, diff 0.0
> >>> >> >> Iteration 19: loss 0.8259870936519939, diff 0.0
> >>> >> >> Iteration 20: loss 0.6327447552109576, diff 0.233952934583647
> >>> >> >> Iteration 21: loss 0.6327447552109576, diff 0.0
> >>> >> >> Iteration 22: loss 0.6327447552109576, diff 0.0
> >>> >> >> Iteration 23: loss 0.5534101162436362, diff 0.12538154276652747
> >>> >> >> Iteration 24: loss 0.5534101162436362, diff 0.0
> >>> >> >> Iteration 25: loss 0.5534101162436362, diff 0.0
> >>> >> >> Iteration 26: loss 0.40450200866125635, diff 0.2690732137675816
> >>> >> >> Iteration 27: loss 0.40450200866125635, diff 0.0
> >>> >> >> Iteration 28: loss 0.40450200866125635, diff 0.0
> >>> >> >> Iteration 29: loss 0.30788249908237314, diff 0.23885980452569502
> >>> >> >>
> >>> >> >> Sincerely,
> >>> >> >>
> >>> >> >> DB Tsai
> >>> >> >> Machine Learning Engineer
> >>> >> >> Alpine Data Labs
> >>> >> >> --------------------------------------
> >>> >> >> Web: http://alpinenow.com/
> >>> >> >>
> >>> >> >>
> >>> >> >> On Wed, Mar 5, 2014 at 2:00 PM, David Hall <dlwh@cs.berkeley.edu
> >
> >>> >> >> wrote:
> >>> >> >> > On Wed, Mar 5, 2014 at 1:57 PM, DB Tsai <dbtsai@alpinenow.com>
> >>> >> >> > wrote:
> >>> >> >> >
> >>> >> >> >> Hi David,
> >>> >> >> >>
> >>> >> >> >> On Tue, Mar 4, 2014 at 8:13 PM, dlwh <david.lw.hall@gmail.com
> >
> >>> >> >> >> wrote:
> >>> >> >> >> > I'm happy to help fix any problems. I've verified at points
> >>> >> >> >> > that
> >>> >> >> >> > the
> >>> >> >> >> > implementation gives the exact same sequence of iterates
> for a
> >>> >> >> >> > few
> >>> >> >> >> different
> >>> >> >> >> > functions (with a particular line search) as the c port of
> >>> >> >> >> > lbfgs.
> >>> >> >> >> > So
> >>> >> >> I'm
> >>> >> >> >> a
> >>> >> >> >> > little surprised it fails where Fortran succeeds... but
> only a
> >>> >> >> >> > little.
> >>> >> >> >> This
> >>> >> >> >> > was fixed late last year.
> >>> >> >> >> I'm working on a reproducible test case using breeze vs
> fortran
> >>> >> >> >> implementation to show the problem I've run into. The test
> will
> >>> >> >> >> be
> >>> >> >> >> in
> >>> >> >> >> one of the test cases in my Spark fork, is it okay for you to
> >>> >> >> >> investigate the issue? Or do I need to make it as a standalone
> >>> >> >> >> test?
> >>> >> >> >>
> >>> >> >> >
> >>> >> >> >
> >>> >> >> > Um, as long as it wouldn't be too hard to pull out.
> >>> >> >> >
> >>> >> >> >
> >>> >> >> >>
> >>> >> >> >> Will send you the test later today.
> >>> >> >> >>
> >>> >> >> >> Thanks.
> >>> >> >> >>
> >>> >> >> >> Sincerely,
> >>> >> >> >>
> >>> >> >> >> DB Tsai
> >>> >> >> >> Machine Learning Engineer
> >>> >> >> >> Alpine Data Labs
> >>> >> >> >> --------------------------------------
> >>> >> >> >> Web: http://alpinenow.com/
> >>> >> >> >>
> >>> >> >>
> >>> >
> >>> >
> >>> >
> >>
> >>
> >
>

--089e0149c9f88bc50c04f6900421--

From dev-return-7290-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 23:06:42 2014
Return-Path: <dev-return-7290-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 35DCD10033
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 23:06:42 +0000 (UTC)
Received: (qmail 48569 invoked by uid 500); 8 Apr 2014 23:06:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48493 invoked by uid 500); 8 Apr 2014 23:06:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48484 invoked by uid 99); 8 Apr 2014 23:06:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 23:06:39 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.81 as permitted sender)
Received: from [171.67.219.81] (HELO smtp.stanford.edu) (171.67.219.81)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 23:06:35 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id DBA6B21575
	for <dev@spark.apache.org>; Tue,  8 Apr 2014 16:06:14 -0700 (PDT)
Received: from mail-qc0-f174.google.com (mail-qc0-f174.google.com [209.85.216.174])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 87839214C0
	for <dev@spark.apache.org>; Tue,  8 Apr 2014 16:06:14 -0700 (PDT)
Received: by mail-qc0-f174.google.com with SMTP id c9so1883219qcz.5
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 16:06:13 -0700 (PDT)
X-Gm-Message-State: ALoCoQkgdTuJXzrde2KNtTYK8jmB5cmLX4/D0s/BfwBLNRUE9Y+WrSaJof0gjRawzBp47piaJnWG
MIME-Version: 1.0
X-Received: by 10.140.51.161 with SMTP id u30mr7872210qga.69.1396998373411;
 Tue, 08 Apr 2014 16:06:13 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Tue, 8 Apr 2014 16:06:13 -0700 (PDT)
Date: Tue, 8 Apr 2014 16:06:13 -0700
Message-ID: <CAEYYnxaPhCzFJ-zLTSaLy_PcpWP2b+miZiEBpTPnMvdXw6w51A@mail.gmail.com>
Subject: A series of meetups about machine learning with Spark in San Francisco
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org, user@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi guys,

We're going to hold a series of meetups about machine learning with
Spark in San Francisco.

The first one will be on April 24. Xiangrui Meng from Databricks will
talk about Spark, Spark/Python, features engineering, and MLlib.

See http://www.meetup.com/sfmachinelearning/events/174560212/ for detail.

The next one on May 1 will be join event with Cloudera talking about
unsupervised learning and multinomial logistic regression with L-BFGS
with Spark.

See http://www.meetup.com/sfmachinelearning/events/176105932/

If you would like to share anything related to machine learning with
Spark in SF Machine Learning Meetup, please let me know.

Thanks.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai

From dev-return-7291-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 23:20:11 2014
Return-Path: <dev-return-7291-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 541A7100BC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 23:20:11 +0000 (UTC)
Received: (qmail 71900 invoked by uid 500); 8 Apr 2014 23:20:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71855 invoked by uid 500); 8 Apr 2014 23:20:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71847 invoked by uid 99); 8 Apr 2014 23:20:08 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 23:20:08 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.81 as permitted sender)
Received: from [171.67.219.81] (HELO smtp.stanford.edu) (171.67.219.81)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 23:20:03 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id A6C792171A
	for <dev@spark.apache.org>; Tue,  8 Apr 2014 16:19:40 -0700 (PDT)
Received: from mail-qc0-f176.google.com (mail-qc0-f176.google.com [209.85.216.176])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 7376D2106A
	for <dev@spark.apache.org>; Tue,  8 Apr 2014 16:19:39 -0700 (PDT)
Received: by mail-qc0-f176.google.com with SMTP id m20so1914076qcx.7
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 16:19:37 -0700 (PDT)
X-Gm-Message-State: ALoCoQn2FcYUckmmiU7DO5Zv9RiLhgp+xB/dpoKIfKRSKU46DigO+kYqibRoLZJNNbbYSo+shxOl
MIME-Version: 1.0
X-Received: by 10.224.49.67 with SMTP id u3mr8392910qaf.63.1396999177582; Tue,
 08 Apr 2014 16:19:37 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Tue, 8 Apr 2014 16:19:37 -0700 (PDT)
In-Reply-To: <CA+B-+fz2-bAMLi+kBnT36UGSKcJ32gi2RrzsttL9nDVgRD=cjg@mail.gmail.com>
References: <CAFkVJifBb14oGvE4zHSL0=s1na9neiKVRGRWmUC24wWnP_9Miw@mail.gmail.com>
	<CAEYYnxbo10PHJsY=s4s2f-3PsSed-F+9zmsz35ELGSfGVhnuHg@mail.gmail.com>
	<CA+B-+fy-Cr9f4OZ4ztuqSnLNKwUMYHKN7GOg1nU2dmFO87rSjA@mail.gmail.com>
	<CA+B-+fxaN-oYuxiDar8RL9DdUcU73Y_giOLrDt_rvFT_657-iA@mail.gmail.com>
	<CAEYYnxbi4u1j7Z0DT-W_MTg+xH2A_CB59-PDX8r-PXvb9SczSg@mail.gmail.com>
	<CA+B-+fz2-bAMLi+kBnT36UGSKcJ32gi2RrzsttL9nDVgRD=cjg@mail.gmail.com>
Date: Tue, 8 Apr 2014 16:19:37 -0700
Message-ID: <CAEYYnxbFhGMD+jvxc9h-BB0evPL5YLqJUZxD7s0JfyWx8DcHbw@mail.gmail.com>
Subject: Re: MLLib - Thoughts about refactoring Updater for LBFGS?
From: DB Tsai <dbtsai@stanford.edu>
To: Debasish Das <debasish.das83@gmail.com>
Cc: dlwh@cs.berkeley.edu, Xiangrui Meng <mengxr@gmail.com>, dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I think mini batch is still useful for L-BFGS.

One of the use-cases can be initialized the weights by training with
the smaller subsamples of data using mini batch with L-BFGS.

Then we could use the weights trained with mini batch to start another
training process with full data.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Tue, Apr 8, 2014 at 4:05 PM, Debasish Das <debasish.das83@gmail.com> wrote:
> Yup that's what I expected...L-BFGS solver is in the master and gradient
> computation per RDD is done on each of the workers...
>
> This miniBatchFraction is also a heuristic which I don't think makes sense
> for LogisticRegressionWithBFGS...does it ?
>
>
> On Tue, Apr 8, 2014 at 3:44 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>
>> Hi Debasish,
>>
>> The L-BFGS solver will be in the master like GD solver, and the part
>> that is parallelized is computing the gradient of each input row, and
>> summing them up.
>>
>> I prefer to make the optimizer plug-able instead of adding new
>> LogisticRegressionWithLBFGS since 98% of the code will be the same.
>>
>> Nice to have something like this,
>>
>> class LogisticRegression private (
>>     var optimizer: Optimizer)
>>   extends GeneralizedLinearAlgorithm[LogisticRegressionModel]
>>
>> The following parameters will be setup in the optimizers, and they
>> should because they are part of optimization parameters.
>>
>>     var stepSize: Double,
>>     var numIterations: Int,
>>     var regParam: Double,
>>     var miniBatchFraction: Double
>>
>> Xiangrui, what do you think?
>>
>> For now, you can use my L-BFGS solver by copying and pasting the
>> LogisticRegressionWithSGD code, and changing the optimizer to L-BFGS.
>>
>> Sincerely,
>>
>> DB Tsai
>> -------------------------------------------------------
>> My Blog: https://www.dbtsai.com
>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>
>>
>> On Tue, Apr 8, 2014 at 9:42 AM, Debasish Das <debasish.das83@gmail.com>
>> wrote:
>> > Hi DB,
>> >
>> > Are we going to clean up the function:
>> >
>> > class LogisticRegressionWithSGD private (
>> >     var stepSize: Double,
>> >     var numIterations: Int,
>> >     var regParam: Double,
>> >     var miniBatchFraction: Double)
>> >   extends GeneralizedLinearAlgorithm[LogisticRegressionModel] with
>> > Serializable {
>> >
>> >   val gradient = new LogisticGradient()
>> >   val updater = new SimpleUpdater()
>> >   override val optimizer = new GradientDescent(gradient, updater)
>> >
>> > Or add a new one ?
>> >
>> > class LogisticRegressionWithBFGS ?
>> >
>> > The WithABC is optional since optimizer could be picked up either based
>> > on a
>> > flag...there are only 3 options for optimizor:
>> >
>> > 1. GradientDescent
>> > 2. Quasi Newton
>> > 3. Newton
>> >
>> > May be we add an enum for optimization type....and then under
>> > GradientDescent family people can add their variants of SGD....Not sure
>> > if
>> > ConjugateGradient comes under 1 or 2....may be we need 4 options...
>> >
>> > Thanks.
>> > Deb
>> >
>> >
>> > On Mon, Apr 7, 2014 at 11:23 PM, Debasish Das <debasish.das83@gmail.com>
>> > wrote:
>> >>
>> >> I got your checkin....I need to run logistic regression SGD vs BFGS for
>> >> my
>> >> current usecases but your next checkin will update the logistic
>> >> regression
>> >> with LBFGS right ? Are you adding it to regression package as well ?
>> >>
>> >> Thanks.
>> >> Deb
>> >>
>> >>
>> >> On Mon, Apr 7, 2014 at 7:00 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>> >>>
>> >>> Hi guys,
>> >>>
>> >>> The latest PR uses Breeze's L-BFGS implement which is introduced by
>> >>> Xiangrui's sparse input format work in SPARK-1212.
>> >>>
>> >>> https://github.com/apache/spark/pull/353
>> >>>
>> >>> Now, it works with the new sparse framework!
>> >>>
>> >>> Any feedback would be greatly appreciated.
>> >>>
>> >>> Thanks.
>> >>>
>> >>> Sincerely,
>> >>>
>> >>> DB Tsai
>> >>> -------------------------------------------------------
>> >>> My Blog: https://www.dbtsai.com
>> >>> LinkedIn: https://www.linkedin.com/in/dbtsai
>> >>>
>> >>>
>> >>> On Thu, Apr 3, 2014 at 5:02 PM, DB Tsai <dbtsai@alpinenow.com> wrote:
>> >>> > ---------- Forwarded message ----------
>> >>> > From: David Hall <dlwh@cs.berkeley.edu>
>> >>> > Date: Sat, Mar 15, 2014 at 10:02 AM
>> >>> > Subject: Re: MLLib - Thoughts about refactoring Updater for LBFGS?
>> >>> > To: DB Tsai <dbtsai@alpinenow.com>
>> >>> >
>> >>> >
>> >>> > On Fri, Mar 7, 2014 at 10:56 PM, DB Tsai <dbtsai@alpinenow.com>
>> >>> > wrote:
>> >>> >>
>> >>> >> Hi David,
>> >>> >>
>> >>> >> Please let me know the version of Breeze that LBFGS can be
>> >>> >> serialized,
>> >>> >> and CachedDiffFunction is built-in in LBFGS once you finish. I'll
>> >>> >> update the PR to Spark from using RISO implementation to Breeze
>> >>> >> implementation.
>> >>> >
>> >>> >
>> >>> > The current master (0.7-SNAPSHOT) has these problems fixed.
>> >>> >
>> >>> >>
>> >>> >>
>> >>> >> Thanks.
>> >>> >>
>> >>> >> Sincerely,
>> >>> >>
>> >>> >> DB Tsai
>> >>> >> Machine Learning Engineer
>> >>> >> Alpine Data Labs
>> >>> >> --------------------------------------
>> >>> >> Web: http://alpinenow.com/
>> >>> >>
>> >>> >>
>> >>> >> On Thu, Mar 6, 2014 at 4:26 PM, David Hall <dlwh@cs.berkeley.edu>
>> >>> >> wrote:
>> >>> >> > On Thu, Mar 6, 2014 at 4:21 PM, DB Tsai <dbtsai@alpinenow.com>
>> >>> >> > wrote:
>> >>> >> >
>> >>> >> >> Hi David,
>> >>> >> >>
>> >>> >> >> I can converge to the same result with your breeze LBFGS and
>> >>> >> >> Fortran
>> >>> >> >> implementations now. Probably, I made some mistakes when I tried
>> >>> >> >> breeze before. I apologize that I claimed it's not stable.
>> >>> >> >>
>> >>> >> >> See the test case in BreezeLBFGSSuite.scala
>> >>> >> >> https://github.com/AlpineNow/spark/tree/dbtsai-breezeLBFGS
>> >>> >> >>
>> >>> >> >> This is training multinomial logistic regression against iris
>> >>> >> >> dataset,
>> >>> >> >> and both optimizers can train the models with 98% training
>> >>> >> >> accuracy.
>> >>> >> >>
>> >>> >> >
>> >>> >> > great to hear! There were some bugs in LBFGS about 6 months ago,
>> >>> >> > so
>> >>> >> > depending on the last time you tried it, it might indeed have
>> >>> >> > been
>> >>> >> > bugged.
>> >>> >> >
>> >>> >> >
>> >>> >> >>
>> >>> >> >> There are two issues to use Breeze in Spark,
>> >>> >> >>
>> >>> >> >> 1) When the gradientSum and lossSum are computed distributively
>> >>> >> >> in
>> >>> >> >> custom defined DiffFunction which will be passed into your
>> >>> >> >> optimizer,
>> >>> >> >> Spark will complain LBFGS class is not serializable. In
>> >>> >> >> BreezeLBFGS.scala, I've to convert RDD to array to make it work
>> >>> >> >> locally. It should be easy to fix by just having LBFGS to
>> >>> >> >> implement
>> >>> >> >> Serializable.
>> >>> >> >>
>> >>> >> >
>> >>> >> > I'm not sure why Spark should be serializing LBFGS? Shouldn't it
>> >>> >> > live on
>> >>> >> > the controller node? Or is this a per-node thing?
>> >>> >> >
>> >>> >> > But no problem to make it serializable.
>> >>> >> >
>> >>> >> >
>> >>> >> >>
>> >>> >> >> 2) Breeze computes redundant gradient and loss. See the
>> >>> >> >> following
>> >>> >> >> log
>> >>> >> >> from both Fortran and Breeze implementations.
>> >>> >> >>
>> >>> >> >
>> >>> >> > Err, yeah. I should probably have LBFGS do this automatically,
>> >>> >> > but
>> >>> >> > there's
>> >>> >> > a CachedDiffFunction that gets rid of the redundant calculations.
>> >>> >> >
>> >>> >> > -- David
>> >>> >> >
>> >>> >> >
>> >>> >> >>
>> >>> >> >> Thanks.
>> >>> >> >>
>> >>> >> >> Fortran:
>> >>> >> >> Iteration -1: loss 1.3862943611198926, diff 1.0
>> >>> >> >> Iteration 0: loss 1.5846343143210866, diff 0.14307193024217352
>> >>> >> >> Iteration 1: loss 1.1242501524477688, diff 0.29053004039012126
>> >>> >> >> Iteration 2: loss 1.0930151243303563, diff 0.027782962952189336
>> >>> >> >> Iteration 3: loss 1.054036932835569, diff 0.03566113127440601
>> >>> >> >> Iteration 4: loss 0.9907956302751622, diff 0.05999907649459571
>> >>> >> >> Iteration 5: loss 0.9184205380342829, diff 0.07304737423337761
>> >>> >> >> Iteration 6: loss 0.8259870936519937, diff 0.10064381175132982
>> >>> >> >> Iteration 7: loss 0.6327447552109574, diff 0.23395293458364716
>> >>> >> >> Iteration 8: loss 0.5534101162436359, diff 0.1253815427665277
>> >>> >> >> Iteration 9: loss 0.4045020086612566, diff 0.26907321376758075
>> >>> >> >> Iteration 10: loss 0.3078824990823728, diff 0.23885980452569627
>> >>> >> >>
>> >>> >> >> Breeze:
>> >>> >> >> Iteration -1: loss 1.3862943611198926, diff 1.0
>> >>> >> >> Mar 6, 2014 3:59:11 PM com.github.fommil.netlib.BLAS <clinit>
>> >>> >> >> WARNING: Failed to load implementation from:
>> >>> >> >> com.github.fommil.netlib.NativeSystemBLAS
>> >>> >> >> Mar 6, 2014 3:59:11 PM com.github.fommil.netlib.BLAS <clinit>
>> >>> >> >> WARNING: Failed to load implementation from:
>> >>> >> >> com.github.fommil.netlib.NativeRefBLAS
>> >>> >> >> Iteration 0: loss 1.3862943611198926, diff 0.0
>> >>> >> >> Iteration 1: loss 1.5846343143210866, diff 0.14307193024217352
>> >>> >> >> Iteration 2: loss 1.1242501524477688, diff 0.29053004039012126
>> >>> >> >> Iteration 3: loss 1.1242501524477688, diff 0.0
>> >>> >> >> Iteration 4: loss 1.1242501524477688, diff 0.0
>> >>> >> >> Iteration 5: loss 1.0930151243303563, diff 0.027782962952189336
>> >>> >> >> Iteration 6: loss 1.0930151243303563, diff 0.0
>> >>> >> >> Iteration 7: loss 1.0930151243303563, diff 0.0
>> >>> >> >> Iteration 8: loss 1.054036932835569, diff 0.03566113127440601
>> >>> >> >> Iteration 9: loss 1.054036932835569, diff 0.0
>> >>> >> >> Iteration 10: loss 1.054036932835569, diff 0.0
>> >>> >> >> Iteration 11: loss 0.9907956302751622, diff 0.05999907649459571
>> >>> >> >> Iteration 12: loss 0.9907956302751622, diff 0.0
>> >>> >> >> Iteration 13: loss 0.9907956302751622, diff 0.0
>> >>> >> >> Iteration 14: loss 0.9184205380342829, diff 0.07304737423337761
>> >>> >> >> Iteration 15: loss 0.9184205380342829, diff 0.0
>> >>> >> >> Iteration 16: loss 0.9184205380342829, diff 0.0
>> >>> >> >> Iteration 17: loss 0.8259870936519939, diff 0.1006438117513297
>> >>> >> >> Iteration 18: loss 0.8259870936519939, diff 0.0
>> >>> >> >> Iteration 19: loss 0.8259870936519939, diff 0.0
>> >>> >> >> Iteration 20: loss 0.6327447552109576, diff 0.233952934583647
>> >>> >> >> Iteration 21: loss 0.6327447552109576, diff 0.0
>> >>> >> >> Iteration 22: loss 0.6327447552109576, diff 0.0
>> >>> >> >> Iteration 23: loss 0.5534101162436362, diff 0.12538154276652747
>> >>> >> >> Iteration 24: loss 0.5534101162436362, diff 0.0
>> >>> >> >> Iteration 25: loss 0.5534101162436362, diff 0.0
>> >>> >> >> Iteration 26: loss 0.40450200866125635, diff 0.2690732137675816
>> >>> >> >> Iteration 27: loss 0.40450200866125635, diff 0.0
>> >>> >> >> Iteration 28: loss 0.40450200866125635, diff 0.0
>> >>> >> >> Iteration 29: loss 0.30788249908237314, diff 0.23885980452569502
>> >>> >> >>
>> >>> >> >> Sincerely,
>> >>> >> >>
>> >>> >> >> DB Tsai
>> >>> >> >> Machine Learning Engineer
>> >>> >> >> Alpine Data Labs
>> >>> >> >> --------------------------------------
>> >>> >> >> Web: http://alpinenow.com/
>> >>> >> >>
>> >>> >> >>
>> >>> >> >> On Wed, Mar 5, 2014 at 2:00 PM, David Hall
>> >>> >> >> <dlwh@cs.berkeley.edu>
>> >>> >> >> wrote:
>> >>> >> >> > On Wed, Mar 5, 2014 at 1:57 PM, DB Tsai <dbtsai@alpinenow.com>
>> >>> >> >> > wrote:
>> >>> >> >> >
>> >>> >> >> >> Hi David,
>> >>> >> >> >>
>> >>> >> >> >> On Tue, Mar 4, 2014 at 8:13 PM, dlwh
>> >>> >> >> >> <david.lw.hall@gmail.com>
>> >>> >> >> >> wrote:
>> >>> >> >> >> > I'm happy to help fix any problems. I've verified at points
>> >>> >> >> >> > that
>> >>> >> >> >> > the
>> >>> >> >> >> > implementation gives the exact same sequence of iterates
>> >>> >> >> >> > for a
>> >>> >> >> >> > few
>> >>> >> >> >> different
>> >>> >> >> >> > functions (with a particular line search) as the c port of
>> >>> >> >> >> > lbfgs.
>> >>> >> >> >> > So
>> >>> >> >> I'm
>> >>> >> >> >> a
>> >>> >> >> >> > little surprised it fails where Fortran succeeds... but
>> >>> >> >> >> > only a
>> >>> >> >> >> > little.
>> >>> >> >> >> This
>> >>> >> >> >> > was fixed late last year.
>> >>> >> >> >> I'm working on a reproducible test case using breeze vs
>> >>> >> >> >> fortran
>> >>> >> >> >> implementation to show the problem I've run into. The test
>> >>> >> >> >> will
>> >>> >> >> >> be
>> >>> >> >> >> in
>> >>> >> >> >> one of the test cases in my Spark fork, is it okay for you to
>> >>> >> >> >> investigate the issue? Or do I need to make it as a
>> >>> >> >> >> standalone
>> >>> >> >> >> test?
>> >>> >> >> >>
>> >>> >> >> >
>> >>> >> >> >
>> >>> >> >> > Um, as long as it wouldn't be too hard to pull out.
>> >>> >> >> >
>> >>> >> >> >
>> >>> >> >> >>
>> >>> >> >> >> Will send you the test later today.
>> >>> >> >> >>
>> >>> >> >> >> Thanks.
>> >>> >> >> >>
>> >>> >> >> >> Sincerely,
>> >>> >> >> >>
>> >>> >> >> >> DB Tsai
>> >>> >> >> >> Machine Learning Engineer
>> >>> >> >> >> Alpine Data Labs
>> >>> >> >> >> --------------------------------------
>> >>> >> >> >> Web: http://alpinenow.com/
>> >>> >> >> >>
>> >>> >> >>
>> >>> >
>> >>> >
>> >>> >
>> >>
>> >>
>> >
>
>

From dev-return-7292-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 23:46:21 2014
Return-Path: <dev-return-7292-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4B957101E4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 23:46:21 +0000 (UTC)
Received: (qmail 34029 invoked by uid 500); 8 Apr 2014 23:46:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33989 invoked by uid 500); 8 Apr 2014 23:46:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33959 invoked by uid 99); 8 Apr 2014 23:46:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 23:46:14 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.219.43 as permitted sender)
Received: from [209.85.219.43] (HELO mail-oa0-f43.google.com) (209.85.219.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 23:46:09 +0000
Received: by mail-oa0-f43.google.com with SMTP id eb12so1896940oac.30
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 16:45:46 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=b75tl+75uNAfT0lwIX89RjRaEAi8Yo+/m/YIX2JJ3bQ=;
        b=bwjqY1oIIjiu/qt/r2EJGEqGrJobkSRY6NyD1CF22vsEdRhGs2EKF8dztbNj/KQWVG
         z/m57Mdg22U/95eHJ0QOeKswsTbUntyPUCOqQoeYMMMKK+1I7RRQGRM4R4HWYG6QCWDr
         Hpy2Ak7xFenvnGhwKwVErmgQGMA5QFmiTaRy1pejrVmXtbnhGDar8uH6IlofYgA2SZdY
         6aU1E0M+yQaf0PSdIqwHeSVhn/V8ib1LQAqUruZy9wznMb1etakfyBibNMF9+VOfSZAi
         Lskl4pjdhm5ZGBDj+Y+wWm9LfqE03OpqtFe07Rl7padFqOmgcBD2O6vSbeCS9Zm0Isv9
         KGXA==
MIME-Version: 1.0
X-Received: by 10.60.141.70 with SMTP id rm6mr5761715oeb.27.1397000746775;
 Tue, 08 Apr 2014 16:45:46 -0700 (PDT)
Received: by 10.182.142.163 with HTTP; Tue, 8 Apr 2014 16:45:46 -0700 (PDT)
In-Reply-To: <CAEYYnxbFhGMD+jvxc9h-BB0evPL5YLqJUZxD7s0JfyWx8DcHbw@mail.gmail.com>
References: <CAFkVJifBb14oGvE4zHSL0=s1na9neiKVRGRWmUC24wWnP_9Miw@mail.gmail.com>
	<CAEYYnxbo10PHJsY=s4s2f-3PsSed-F+9zmsz35ELGSfGVhnuHg@mail.gmail.com>
	<CA+B-+fy-Cr9f4OZ4ztuqSnLNKwUMYHKN7GOg1nU2dmFO87rSjA@mail.gmail.com>
	<CA+B-+fxaN-oYuxiDar8RL9DdUcU73Y_giOLrDt_rvFT_657-iA@mail.gmail.com>
	<CAEYYnxbi4u1j7Z0DT-W_MTg+xH2A_CB59-PDX8r-PXvb9SczSg@mail.gmail.com>
	<CA+B-+fz2-bAMLi+kBnT36UGSKcJ32gi2RrzsttL9nDVgRD=cjg@mail.gmail.com>
	<CAEYYnxbFhGMD+jvxc9h-BB0evPL5YLqJUZxD7s0JfyWx8DcHbw@mail.gmail.com>
Date: Tue, 8 Apr 2014 16:45:46 -0700
Message-ID: <CA+B-+fw_ysbNUUe_1vvFvDjuRWSCEqdV2er73y0e1ktSj_KjHQ@mail.gmail.com>
Subject: Re: MLLib - Thoughts about refactoring Updater for LBFGS?
From: Debasish Das <debasish.das83@gmail.com>
To: DB Tsai <dbtsai@stanford.edu>
Cc: dlwh@cs.berkeley.edu, Xiangrui Meng <mengxr@gmail.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b33c8fa444c8104f6909510
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b33c8fa444c8104f6909510
Content-Type: text/plain; charset=ISO-8859-1

Have you experimented with it ? For logistic regression at least given
enough iterations/tolerance that you are giving, BFGS in both ways should
converge to same solution....


On Tue, Apr 8, 2014 at 4:19 PM, DB Tsai <dbtsai@stanford.edu> wrote:

> I think mini batch is still useful for L-BFGS.
>
> One of the use-cases can be initialized the weights by training with
> the smaller subsamples of data using mini batch with L-BFGS.
>
> Then we could use the weights trained with mini batch to start another
> training process with full data.
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Tue, Apr 8, 2014 at 4:05 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
> > Yup that's what I expected...L-BFGS solver is in the master and gradient
> > computation per RDD is done on each of the workers...
> >
> > This miniBatchFraction is also a heuristic which I don't think makes
> sense
> > for LogisticRegressionWithBFGS...does it ?
> >
> >
> > On Tue, Apr 8, 2014 at 3:44 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> >>
> >> Hi Debasish,
> >>
> >> The L-BFGS solver will be in the master like GD solver, and the part
> >> that is parallelized is computing the gradient of each input row, and
> >> summing them up.
> >>
> >> I prefer to make the optimizer plug-able instead of adding new
> >> LogisticRegressionWithLBFGS since 98% of the code will be the same.
> >>
> >> Nice to have something like this,
> >>
> >> class LogisticRegression private (
> >>     var optimizer: Optimizer)
> >>   extends GeneralizedLinearAlgorithm[LogisticRegressionModel]
> >>
> >> The following parameters will be setup in the optimizers, and they
> >> should because they are part of optimization parameters.
> >>
> >>     var stepSize: Double,
> >>     var numIterations: Int,
> >>     var regParam: Double,
> >>     var miniBatchFraction: Double
> >>
> >> Xiangrui, what do you think?
> >>
> >> For now, you can use my L-BFGS solver by copying and pasting the
> >> LogisticRegressionWithSGD code, and changing the optimizer to L-BFGS.
> >>
> >> Sincerely,
> >>
> >> DB Tsai
> >> -------------------------------------------------------
> >> My Blog: https://www.dbtsai.com
> >> LinkedIn: https://www.linkedin.com/in/dbtsai
> >>
> >>
> >> On Tue, Apr 8, 2014 at 9:42 AM, Debasish Das <debasish.das83@gmail.com>
> >> wrote:
> >> > Hi DB,
> >> >
> >> > Are we going to clean up the function:
> >> >
> >> > class LogisticRegressionWithSGD private (
> >> >     var stepSize: Double,
> >> >     var numIterations: Int,
> >> >     var regParam: Double,
> >> >     var miniBatchFraction: Double)
> >> >   extends GeneralizedLinearAlgorithm[LogisticRegressionModel] with
> >> > Serializable {
> >> >
> >> >   val gradient = new LogisticGradient()
> >> >   val updater = new SimpleUpdater()
> >> >   override val optimizer = new GradientDescent(gradient, updater)
> >> >
> >> > Or add a new one ?
> >> >
> >> > class LogisticRegressionWithBFGS ?
> >> >
> >> > The WithABC is optional since optimizer could be picked up either
> based
> >> > on a
> >> > flag...there are only 3 options for optimizor:
> >> >
> >> > 1. GradientDescent
> >> > 2. Quasi Newton
> >> > 3. Newton
> >> >
> >> > May be we add an enum for optimization type....and then under
> >> > GradientDescent family people can add their variants of SGD....Not
> sure
> >> > if
> >> > ConjugateGradient comes under 1 or 2....may be we need 4 options...
> >> >
> >> > Thanks.
> >> > Deb
> >> >
> >> >
> >> > On Mon, Apr 7, 2014 at 11:23 PM, Debasish Das <
> debasish.das83@gmail.com>
> >> > wrote:
> >> >>
> >> >> I got your checkin....I need to run logistic regression SGD vs BFGS
> for
> >> >> my
> >> >> current usecases but your next checkin will update the logistic
> >> >> regression
> >> >> with LBFGS right ? Are you adding it to regression package as well ?
> >> >>
> >> >> Thanks.
> >> >> Deb
> >> >>
> >> >>
> >> >> On Mon, Apr 7, 2014 at 7:00 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> >> >>>
> >> >>> Hi guys,
> >> >>>
> >> >>> The latest PR uses Breeze's L-BFGS implement which is introduced by
> >> >>> Xiangrui's sparse input format work in SPARK-1212.
> >> >>>
> >> >>> https://github.com/apache/spark/pull/353
> >> >>>
> >> >>> Now, it works with the new sparse framework!
> >> >>>
> >> >>> Any feedback would be greatly appreciated.
> >> >>>
> >> >>> Thanks.
> >> >>>
> >> >>> Sincerely,
> >> >>>
> >> >>> DB Tsai
> >> >>> -------------------------------------------------------
> >> >>> My Blog: https://www.dbtsai.com
> >> >>> LinkedIn: https://www.linkedin.com/in/dbtsai
> >> >>>
> >> >>>
> >> >>> On Thu, Apr 3, 2014 at 5:02 PM, DB Tsai <dbtsai@alpinenow.com>
> wrote:
> >> >>> > ---------- Forwarded message ----------
> >> >>> > From: David Hall <dlwh@cs.berkeley.edu>
> >> >>> > Date: Sat, Mar 15, 2014 at 10:02 AM
> >> >>> > Subject: Re: MLLib - Thoughts about refactoring Updater for LBFGS?
> >> >>> > To: DB Tsai <dbtsai@alpinenow.com>
> >> >>> >
> >> >>> >
> >> >>> > On Fri, Mar 7, 2014 at 10:56 PM, DB Tsai <dbtsai@alpinenow.com>
> >> >>> > wrote:
> >> >>> >>
> >> >>> >> Hi David,
> >> >>> >>
> >> >>> >> Please let me know the version of Breeze that LBFGS can be
> >> >>> >> serialized,
> >> >>> >> and CachedDiffFunction is built-in in LBFGS once you finish. I'll
> >> >>> >> update the PR to Spark from using RISO implementation to Breeze
> >> >>> >> implementation.
> >> >>> >
> >> >>> >
> >> >>> > The current master (0.7-SNAPSHOT) has these problems fixed.
> >> >>> >
> >> >>> >>
> >> >>> >>
> >> >>> >> Thanks.
> >> >>> >>
> >> >>> >> Sincerely,
> >> >>> >>
> >> >>> >> DB Tsai
> >> >>> >> Machine Learning Engineer
> >> >>> >> Alpine Data Labs
> >> >>> >> --------------------------------------
> >> >>> >> Web: http://alpinenow.com/
> >> >>> >>
> >> >>> >>
> >> >>> >> On Thu, Mar 6, 2014 at 4:26 PM, David Hall <dlwh@cs.berkeley.edu
> >
> >> >>> >> wrote:
> >> >>> >> > On Thu, Mar 6, 2014 at 4:21 PM, DB Tsai <dbtsai@alpinenow.com>
> >> >>> >> > wrote:
> >> >>> >> >
> >> >>> >> >> Hi David,
> >> >>> >> >>
> >> >>> >> >> I can converge to the same result with your breeze LBFGS and
> >> >>> >> >> Fortran
> >> >>> >> >> implementations now. Probably, I made some mistakes when I
> tried
> >> >>> >> >> breeze before. I apologize that I claimed it's not stable.
> >> >>> >> >>
> >> >>> >> >> See the test case in BreezeLBFGSSuite.scala
> >> >>> >> >> https://github.com/AlpineNow/spark/tree/dbtsai-breezeLBFGS
> >> >>> >> >>
> >> >>> >> >> This is training multinomial logistic regression against iris
> >> >>> >> >> dataset,
> >> >>> >> >> and both optimizers can train the models with 98% training
> >> >>> >> >> accuracy.
> >> >>> >> >>
> >> >>> >> >
> >> >>> >> > great to hear! There were some bugs in LBFGS about 6 months
> ago,
> >> >>> >> > so
> >> >>> >> > depending on the last time you tried it, it might indeed have
> >> >>> >> > been
> >> >>> >> > bugged.
> >> >>> >> >
> >> >>> >> >
> >> >>> >> >>
> >> >>> >> >> There are two issues to use Breeze in Spark,
> >> >>> >> >>
> >> >>> >> >> 1) When the gradientSum and lossSum are computed
> distributively
> >> >>> >> >> in
> >> >>> >> >> custom defined DiffFunction which will be passed into your
> >> >>> >> >> optimizer,
> >> >>> >> >> Spark will complain LBFGS class is not serializable. In
> >> >>> >> >> BreezeLBFGS.scala, I've to convert RDD to array to make it
> work
> >> >>> >> >> locally. It should be easy to fix by just having LBFGS to
> >> >>> >> >> implement
> >> >>> >> >> Serializable.
> >> >>> >> >>
> >> >>> >> >
> >> >>> >> > I'm not sure why Spark should be serializing LBFGS? Shouldn't
> it
> >> >>> >> > live on
> >> >>> >> > the controller node? Or is this a per-node thing?
> >> >>> >> >
> >> >>> >> > But no problem to make it serializable.
> >> >>> >> >
> >> >>> >> >
> >> >>> >> >>
> >> >>> >> >> 2) Breeze computes redundant gradient and loss. See the
> >> >>> >> >> following
> >> >>> >> >> log
> >> >>> >> >> from both Fortran and Breeze implementations.
> >> >>> >> >>
> >> >>> >> >
> >> >>> >> > Err, yeah. I should probably have LBFGS do this automatically,
> >> >>> >> > but
> >> >>> >> > there's
> >> >>> >> > a CachedDiffFunction that gets rid of the redundant
> calculations.
> >> >>> >> >
> >> >>> >> > -- David
> >> >>> >> >
> >> >>> >> >
> >> >>> >> >>
> >> >>> >> >> Thanks.
> >> >>> >> >>
> >> >>> >> >> Fortran:
> >> >>> >> >> Iteration -1: loss 1.3862943611198926, diff 1.0
> >> >>> >> >> Iteration 0: loss 1.5846343143210866, diff 0.14307193024217352
> >> >>> >> >> Iteration 1: loss 1.1242501524477688, diff 0.29053004039012126
> >> >>> >> >> Iteration 2: loss 1.0930151243303563, diff
> 0.027782962952189336
> >> >>> >> >> Iteration 3: loss 1.054036932835569, diff 0.03566113127440601
> >> >>> >> >> Iteration 4: loss 0.9907956302751622, diff 0.05999907649459571
> >> >>> >> >> Iteration 5: loss 0.9184205380342829, diff 0.07304737423337761
> >> >>> >> >> Iteration 6: loss 0.8259870936519937, diff 0.10064381175132982
> >> >>> >> >> Iteration 7: loss 0.6327447552109574, diff 0.23395293458364716
> >> >>> >> >> Iteration 8: loss 0.5534101162436359, diff 0.1253815427665277
> >> >>> >> >> Iteration 9: loss 0.4045020086612566, diff 0.26907321376758075
> >> >>> >> >> Iteration 10: loss 0.3078824990823728, diff
> 0.23885980452569627
> >> >>> >> >>
> >> >>> >> >> Breeze:
> >> >>> >> >> Iteration -1: loss 1.3862943611198926, diff 1.0
> >> >>> >> >> Mar 6, 2014 3:59:11 PM com.github.fommil.netlib.BLAS <clinit>
> >> >>> >> >> WARNING: Failed to load implementation from:
> >> >>> >> >> com.github.fommil.netlib.NativeSystemBLAS
> >> >>> >> >> Mar 6, 2014 3:59:11 PM com.github.fommil.netlib.BLAS <clinit>
> >> >>> >> >> WARNING: Failed to load implementation from:
> >> >>> >> >> com.github.fommil.netlib.NativeRefBLAS
> >> >>> >> >> Iteration 0: loss 1.3862943611198926, diff 0.0
> >> >>> >> >> Iteration 1: loss 1.5846343143210866, diff 0.14307193024217352
> >> >>> >> >> Iteration 2: loss 1.1242501524477688, diff 0.29053004039012126
> >> >>> >> >> Iteration 3: loss 1.1242501524477688, diff 0.0
> >> >>> >> >> Iteration 4: loss 1.1242501524477688, diff 0.0
> >> >>> >> >> Iteration 5: loss 1.0930151243303563, diff
> 0.027782962952189336
> >> >>> >> >> Iteration 6: loss 1.0930151243303563, diff 0.0
> >> >>> >> >> Iteration 7: loss 1.0930151243303563, diff 0.0
> >> >>> >> >> Iteration 8: loss 1.054036932835569, diff 0.03566113127440601
> >> >>> >> >> Iteration 9: loss 1.054036932835569, diff 0.0
> >> >>> >> >> Iteration 10: loss 1.054036932835569, diff 0.0
> >> >>> >> >> Iteration 11: loss 0.9907956302751622, diff
> 0.05999907649459571
> >> >>> >> >> Iteration 12: loss 0.9907956302751622, diff 0.0
> >> >>> >> >> Iteration 13: loss 0.9907956302751622, diff 0.0
> >> >>> >> >> Iteration 14: loss 0.9184205380342829, diff
> 0.07304737423337761
> >> >>> >> >> Iteration 15: loss 0.9184205380342829, diff 0.0
> >> >>> >> >> Iteration 16: loss 0.9184205380342829, diff 0.0
> >> >>> >> >> Iteration 17: loss 0.8259870936519939, diff 0.1006438117513297
> >> >>> >> >> Iteration 18: loss 0.8259870936519939, diff 0.0
> >> >>> >> >> Iteration 19: loss 0.8259870936519939, diff 0.0
> >> >>> >> >> Iteration 20: loss 0.6327447552109576, diff 0.233952934583647
> >> >>> >> >> Iteration 21: loss 0.6327447552109576, diff 0.0
> >> >>> >> >> Iteration 22: loss 0.6327447552109576, diff 0.0
> >> >>> >> >> Iteration 23: loss 0.5534101162436362, diff
> 0.12538154276652747
> >> >>> >> >> Iteration 24: loss 0.5534101162436362, diff 0.0
> >> >>> >> >> Iteration 25: loss 0.5534101162436362, diff 0.0
> >> >>> >> >> Iteration 26: loss 0.40450200866125635, diff
> 0.2690732137675816
> >> >>> >> >> Iteration 27: loss 0.40450200866125635, diff 0.0
> >> >>> >> >> Iteration 28: loss 0.40450200866125635, diff 0.0
> >> >>> >> >> Iteration 29: loss 0.30788249908237314, diff
> 0.23885980452569502
> >> >>> >> >>
> >> >>> >> >> Sincerely,
> >> >>> >> >>
> >> >>> >> >> DB Tsai
> >> >>> >> >> Machine Learning Engineer
> >> >>> >> >> Alpine Data Labs
> >> >>> >> >> --------------------------------------
> >> >>> >> >> Web: http://alpinenow.com/
> >> >>> >> >>
> >> >>> >> >>
> >> >>> >> >> On Wed, Mar 5, 2014 at 2:00 PM, David Hall
> >> >>> >> >> <dlwh@cs.berkeley.edu>
> >> >>> >> >> wrote:
> >> >>> >> >> > On Wed, Mar 5, 2014 at 1:57 PM, DB Tsai <
> dbtsai@alpinenow.com>
> >> >>> >> >> > wrote:
> >> >>> >> >> >
> >> >>> >> >> >> Hi David,
> >> >>> >> >> >>
> >> >>> >> >> >> On Tue, Mar 4, 2014 at 8:13 PM, dlwh
> >> >>> >> >> >> <david.lw.hall@gmail.com>
> >> >>> >> >> >> wrote:
> >> >>> >> >> >> > I'm happy to help fix any problems. I've verified at
> points
> >> >>> >> >> >> > that
> >> >>> >> >> >> > the
> >> >>> >> >> >> > implementation gives the exact same sequence of iterates
> >> >>> >> >> >> > for a
> >> >>> >> >> >> > few
> >> >>> >> >> >> different
> >> >>> >> >> >> > functions (with a particular line search) as the c port
> of
> >> >>> >> >> >> > lbfgs.
> >> >>> >> >> >> > So
> >> >>> >> >> I'm
> >> >>> >> >> >> a
> >> >>> >> >> >> > little surprised it fails where Fortran succeeds... but
> >> >>> >> >> >> > only a
> >> >>> >> >> >> > little.
> >> >>> >> >> >> This
> >> >>> >> >> >> > was fixed late last year.
> >> >>> >> >> >> I'm working on a reproducible test case using breeze vs
> >> >>> >> >> >> fortran
> >> >>> >> >> >> implementation to show the problem I've run into. The test
> >> >>> >> >> >> will
> >> >>> >> >> >> be
> >> >>> >> >> >> in
> >> >>> >> >> >> one of the test cases in my Spark fork, is it okay for you
> to
> >> >>> >> >> >> investigate the issue? Or do I need to make it as a
> >> >>> >> >> >> standalone
> >> >>> >> >> >> test?
> >> >>> >> >> >>
> >> >>> >> >> >
> >> >>> >> >> >
> >> >>> >> >> > Um, as long as it wouldn't be too hard to pull out.
> >> >>> >> >> >
> >> >>> >> >> >
> >> >>> >> >> >>
> >> >>> >> >> >> Will send you the test later today.
> >> >>> >> >> >>
> >> >>> >> >> >> Thanks.
> >> >>> >> >> >>
> >> >>> >> >> >> Sincerely,
> >> >>> >> >> >>
> >> >>> >> >> >> DB Tsai
> >> >>> >> >> >> Machine Learning Engineer
> >> >>> >> >> >> Alpine Data Labs
> >> >>> >> >> >> --------------------------------------
> >> >>> >> >> >> Web: http://alpinenow.com/
> >> >>> >> >> >>
> >> >>> >> >>
> >> >>> >
> >> >>> >
> >> >>> >
> >> >>
> >> >>
> >> >
> >
> >
>

--047d7b33c8fa444c8104f6909510--

From dev-return-7293-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr  8 23:48:46 2014
Return-Path: <dev-return-7293-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9A447101F5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Apr 2014 23:48:46 +0000 (UTC)
Received: (qmail 36210 invoked by uid 500); 8 Apr 2014 23:48:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36118 invoked by uid 500); 8 Apr 2014 23:48:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36107 invoked by uid 99); 8 Apr 2014 23:48:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 23:48:44 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Apr 2014 23:48:40 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 6FBF81013B2
	for <dev@spark.apache.org>; Tue,  8 Apr 2014 16:48:20 -0700 (PDT)
Received: from mail-qa0-f41.google.com (mail-qa0-f41.google.com [209.85.216.41])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 7788B1013B1
	for <dev@spark.apache.org>; Tue,  8 Apr 2014 16:48:19 -0700 (PDT)
Received: by mail-qa0-f41.google.com with SMTP id j5so1732322qaq.28
        for <dev@spark.apache.org>; Tue, 08 Apr 2014 16:48:17 -0700 (PDT)
X-Gm-Message-State: ALoCoQnY06gSAgGsDqkrqwhM3PNTF1acho+8F5LH+2LyUOelqqlID/2ERNTm9s5YUvIzHo8CTSMy
MIME-Version: 1.0
X-Received: by 10.229.198.2 with SMTP id em2mr8527419qcb.21.1397000897776;
 Tue, 08 Apr 2014 16:48:17 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Tue, 8 Apr 2014 16:48:17 -0700 (PDT)
In-Reply-To: <CA+B-+fw_ysbNUUe_1vvFvDjuRWSCEqdV2er73y0e1ktSj_KjHQ@mail.gmail.com>
References: <CAFkVJifBb14oGvE4zHSL0=s1na9neiKVRGRWmUC24wWnP_9Miw@mail.gmail.com>
	<CAEYYnxbo10PHJsY=s4s2f-3PsSed-F+9zmsz35ELGSfGVhnuHg@mail.gmail.com>
	<CA+B-+fy-Cr9f4OZ4ztuqSnLNKwUMYHKN7GOg1nU2dmFO87rSjA@mail.gmail.com>
	<CA+B-+fxaN-oYuxiDar8RL9DdUcU73Y_giOLrDt_rvFT_657-iA@mail.gmail.com>
	<CAEYYnxbi4u1j7Z0DT-W_MTg+xH2A_CB59-PDX8r-PXvb9SczSg@mail.gmail.com>
	<CA+B-+fz2-bAMLi+kBnT36UGSKcJ32gi2RrzsttL9nDVgRD=cjg@mail.gmail.com>
	<CAEYYnxbFhGMD+jvxc9h-BB0evPL5YLqJUZxD7s0JfyWx8DcHbw@mail.gmail.com>
	<CA+B-+fw_ysbNUUe_1vvFvDjuRWSCEqdV2er73y0e1ktSj_KjHQ@mail.gmail.com>
Date: Tue, 8 Apr 2014 16:48:17 -0700
Message-ID: <CAEYYnxae1cTin33vCWbx00b5WNb8FHg4SRjaUFhig+o+X7Uk_Q@mail.gmail.com>
Subject: Re: MLLib - Thoughts about refactoring Updater for LBFGS?
From: DB Tsai <dbtsai@stanford.edu>
To: Debasish Das <debasish.das83@gmail.com>
Cc: dlwh@cs.berkeley.edu, Xiangrui Meng <mengxr@gmail.com>, dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I don't experiment it. That's the use-case in theory I could think of. ^^

However, from what I saw, BFGS converges really fast so that I only
need 20~30 iterations in general.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Tue, Apr 8, 2014 at 4:45 PM, Debasish Das <debasish.das83@gmail.com> wrote:
> Have you experimented with it ? For logistic regression at least given
> enough iterations/tolerance that you are giving, BFGS in both ways should
> converge to same solution....
>
>
> On Tue, Apr 8, 2014 at 4:19 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>
>> I think mini batch is still useful for L-BFGS.
>>
>> One of the use-cases can be initialized the weights by training with
>> the smaller subsamples of data using mini batch with L-BFGS.
>>
>> Then we could use the weights trained with mini batch to start another
>> training process with full data.
>>
>> Sincerely,
>>
>> DB Tsai
>> -------------------------------------------------------
>> My Blog: https://www.dbtsai.com
>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>
>>
>> On Tue, Apr 8, 2014 at 4:05 PM, Debasish Das <debasish.das83@gmail.com>
>> wrote:
>> > Yup that's what I expected...L-BFGS solver is in the master and gradient
>> > computation per RDD is done on each of the workers...
>> >
>> > This miniBatchFraction is also a heuristic which I don't think makes
>> > sense
>> > for LogisticRegressionWithBFGS...does it ?
>> >
>> >
>> > On Tue, Apr 8, 2014 at 3:44 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>> >>
>> >> Hi Debasish,
>> >>
>> >> The L-BFGS solver will be in the master like GD solver, and the part
>> >> that is parallelized is computing the gradient of each input row, and
>> >> summing them up.
>> >>
>> >> I prefer to make the optimizer plug-able instead of adding new
>> >> LogisticRegressionWithLBFGS since 98% of the code will be the same.
>> >>
>> >> Nice to have something like this,
>> >>
>> >> class LogisticRegression private (
>> >>     var optimizer: Optimizer)
>> >>   extends GeneralizedLinearAlgorithm[LogisticRegressionModel]
>> >>
>> >> The following parameters will be setup in the optimizers, and they
>> >> should because they are part of optimization parameters.
>> >>
>> >>     var stepSize: Double,
>> >>     var numIterations: Int,
>> >>     var regParam: Double,
>> >>     var miniBatchFraction: Double
>> >>
>> >> Xiangrui, what do you think?
>> >>
>> >> For now, you can use my L-BFGS solver by copying and pasting the
>> >> LogisticRegressionWithSGD code, and changing the optimizer to L-BFGS.
>> >>
>> >> Sincerely,
>> >>
>> >> DB Tsai
>> >> -------------------------------------------------------
>> >> My Blog: https://www.dbtsai.com
>> >> LinkedIn: https://www.linkedin.com/in/dbtsai
>> >>
>> >>
>> >> On Tue, Apr 8, 2014 at 9:42 AM, Debasish Das <debasish.das83@gmail.com>
>> >> wrote:
>> >> > Hi DB,
>> >> >
>> >> > Are we going to clean up the function:
>> >> >
>> >> > class LogisticRegressionWithSGD private (
>> >> >     var stepSize: Double,
>> >> >     var numIterations: Int,
>> >> >     var regParam: Double,
>> >> >     var miniBatchFraction: Double)
>> >> >   extends GeneralizedLinearAlgorithm[LogisticRegressionModel] with
>> >> > Serializable {
>> >> >
>> >> >   val gradient = new LogisticGradient()
>> >> >   val updater = new SimpleUpdater()
>> >> >   override val optimizer = new GradientDescent(gradient, updater)
>> >> >
>> >> > Or add a new one ?
>> >> >
>> >> > class LogisticRegressionWithBFGS ?
>> >> >
>> >> > The WithABC is optional since optimizer could be picked up either
>> >> > based
>> >> > on a
>> >> > flag...there are only 3 options for optimizor:
>> >> >
>> >> > 1. GradientDescent
>> >> > 2. Quasi Newton
>> >> > 3. Newton
>> >> >
>> >> > May be we add an enum for optimization type....and then under
>> >> > GradientDescent family people can add their variants of SGD....Not
>> >> > sure
>> >> > if
>> >> > ConjugateGradient comes under 1 or 2....may be we need 4 options...
>> >> >
>> >> > Thanks.
>> >> > Deb
>> >> >
>> >> >
>> >> > On Mon, Apr 7, 2014 at 11:23 PM, Debasish Das
>> >> > <debasish.das83@gmail.com>
>> >> > wrote:
>> >> >>
>> >> >> I got your checkin....I need to run logistic regression SGD vs BFGS
>> >> >> for
>> >> >> my
>> >> >> current usecases but your next checkin will update the logistic
>> >> >> regression
>> >> >> with LBFGS right ? Are you adding it to regression package as well ?
>> >> >>
>> >> >> Thanks.
>> >> >> Deb
>> >> >>
>> >> >>
>> >> >> On Mon, Apr 7, 2014 at 7:00 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>> >> >>>
>> >> >>> Hi guys,
>> >> >>>
>> >> >>> The latest PR uses Breeze's L-BFGS implement which is introduced by
>> >> >>> Xiangrui's sparse input format work in SPARK-1212.
>> >> >>>
>> >> >>> https://github.com/apache/spark/pull/353
>> >> >>>
>> >> >>> Now, it works with the new sparse framework!
>> >> >>>
>> >> >>> Any feedback would be greatly appreciated.
>> >> >>>
>> >> >>> Thanks.
>> >> >>>
>> >> >>> Sincerely,
>> >> >>>
>> >> >>> DB Tsai
>> >> >>> -------------------------------------------------------
>> >> >>> My Blog: https://www.dbtsai.com
>> >> >>> LinkedIn: https://www.linkedin.com/in/dbtsai
>> >> >>>
>> >> >>>
>> >> >>> On Thu, Apr 3, 2014 at 5:02 PM, DB Tsai <dbtsai@alpinenow.com>
>> >> >>> wrote:
>> >> >>> > ---------- Forwarded message ----------
>> >> >>> > From: David Hall <dlwh@cs.berkeley.edu>
>> >> >>> > Date: Sat, Mar 15, 2014 at 10:02 AM
>> >> >>> > Subject: Re: MLLib - Thoughts about refactoring Updater for
>> >> >>> > LBFGS?
>> >> >>> > To: DB Tsai <dbtsai@alpinenow.com>
>> >> >>> >
>> >> >>> >
>> >> >>> > On Fri, Mar 7, 2014 at 10:56 PM, DB Tsai <dbtsai@alpinenow.com>
>> >> >>> > wrote:
>> >> >>> >>
>> >> >>> >> Hi David,
>> >> >>> >>
>> >> >>> >> Please let me know the version of Breeze that LBFGS can be
>> >> >>> >> serialized,
>> >> >>> >> and CachedDiffFunction is built-in in LBFGS once you finish.
>> >> >>> >> I'll
>> >> >>> >> update the PR to Spark from using RISO implementation to Breeze
>> >> >>> >> implementation.
>> >> >>> >
>> >> >>> >
>> >> >>> > The current master (0.7-SNAPSHOT) has these problems fixed.
>> >> >>> >
>> >> >>> >>
>> >> >>> >>
>> >> >>> >> Thanks.
>> >> >>> >>
>> >> >>> >> Sincerely,
>> >> >>> >>
>> >> >>> >> DB Tsai
>> >> >>> >> Machine Learning Engineer
>> >> >>> >> Alpine Data Labs
>> >> >>> >> --------------------------------------
>> >> >>> >> Web: http://alpinenow.com/
>> >> >>> >>
>> >> >>> >>
>> >> >>> >> On Thu, Mar 6, 2014 at 4:26 PM, David Hall
>> >> >>> >> <dlwh@cs.berkeley.edu>
>> >> >>> >> wrote:
>> >> >>> >> > On Thu, Mar 6, 2014 at 4:21 PM, DB Tsai <dbtsai@alpinenow.com>
>> >> >>> >> > wrote:
>> >> >>> >> >
>> >> >>> >> >> Hi David,
>> >> >>> >> >>
>> >> >>> >> >> I can converge to the same result with your breeze LBFGS and
>> >> >>> >> >> Fortran
>> >> >>> >> >> implementations now. Probably, I made some mistakes when I
>> >> >>> >> >> tried
>> >> >>> >> >> breeze before. I apologize that I claimed it's not stable.
>> >> >>> >> >>
>> >> >>> >> >> See the test case in BreezeLBFGSSuite.scala
>> >> >>> >> >> https://github.com/AlpineNow/spark/tree/dbtsai-breezeLBFGS
>> >> >>> >> >>
>> >> >>> >> >> This is training multinomial logistic regression against iris
>> >> >>> >> >> dataset,
>> >> >>> >> >> and both optimizers can train the models with 98% training
>> >> >>> >> >> accuracy.
>> >> >>> >> >>
>> >> >>> >> >
>> >> >>> >> > great to hear! There were some bugs in LBFGS about 6 months
>> >> >>> >> > ago,
>> >> >>> >> > so
>> >> >>> >> > depending on the last time you tried it, it might indeed have
>> >> >>> >> > been
>> >> >>> >> > bugged.
>> >> >>> >> >
>> >> >>> >> >
>> >> >>> >> >>
>> >> >>> >> >> There are two issues to use Breeze in Spark,
>> >> >>> >> >>
>> >> >>> >> >> 1) When the gradientSum and lossSum are computed
>> >> >>> >> >> distributively
>> >> >>> >> >> in
>> >> >>> >> >> custom defined DiffFunction which will be passed into your
>> >> >>> >> >> optimizer,
>> >> >>> >> >> Spark will complain LBFGS class is not serializable. In
>> >> >>> >> >> BreezeLBFGS.scala, I've to convert RDD to array to make it
>> >> >>> >> >> work
>> >> >>> >> >> locally. It should be easy to fix by just having LBFGS to
>> >> >>> >> >> implement
>> >> >>> >> >> Serializable.
>> >> >>> >> >>
>> >> >>> >> >
>> >> >>> >> > I'm not sure why Spark should be serializing LBFGS? Shouldn't
>> >> >>> >> > it
>> >> >>> >> > live on
>> >> >>> >> > the controller node? Or is this a per-node thing?
>> >> >>> >> >
>> >> >>> >> > But no problem to make it serializable.
>> >> >>> >> >
>> >> >>> >> >
>> >> >>> >> >>
>> >> >>> >> >> 2) Breeze computes redundant gradient and loss. See the
>> >> >>> >> >> following
>> >> >>> >> >> log
>> >> >>> >> >> from both Fortran and Breeze implementations.
>> >> >>> >> >>
>> >> >>> >> >
>> >> >>> >> > Err, yeah. I should probably have LBFGS do this automatically,
>> >> >>> >> > but
>> >> >>> >> > there's
>> >> >>> >> > a CachedDiffFunction that gets rid of the redundant
>> >> >>> >> > calculations.
>> >> >>> >> >
>> >> >>> >> > -- David
>> >> >>> >> >
>> >> >>> >> >
>> >> >>> >> >>
>> >> >>> >> >> Thanks.
>> >> >>> >> >>
>> >> >>> >> >> Fortran:
>> >> >>> >> >> Iteration -1: loss 1.3862943611198926, diff 1.0
>> >> >>> >> >> Iteration 0: loss 1.5846343143210866, diff
>> >> >>> >> >> 0.14307193024217352
>> >> >>> >> >> Iteration 1: loss 1.1242501524477688, diff
>> >> >>> >> >> 0.29053004039012126
>> >> >>> >> >> Iteration 2: loss 1.0930151243303563, diff
>> >> >>> >> >> 0.027782962952189336
>> >> >>> >> >> Iteration 3: loss 1.054036932835569, diff 0.03566113127440601
>> >> >>> >> >> Iteration 4: loss 0.9907956302751622, diff
>> >> >>> >> >> 0.05999907649459571
>> >> >>> >> >> Iteration 5: loss 0.9184205380342829, diff
>> >> >>> >> >> 0.07304737423337761
>> >> >>> >> >> Iteration 6: loss 0.8259870936519937, diff
>> >> >>> >> >> 0.10064381175132982
>> >> >>> >> >> Iteration 7: loss 0.6327447552109574, diff
>> >> >>> >> >> 0.23395293458364716
>> >> >>> >> >> Iteration 8: loss 0.5534101162436359, diff 0.1253815427665277
>> >> >>> >> >> Iteration 9: loss 0.4045020086612566, diff
>> >> >>> >> >> 0.26907321376758075
>> >> >>> >> >> Iteration 10: loss 0.3078824990823728, diff
>> >> >>> >> >> 0.23885980452569627
>> >> >>> >> >>
>> >> >>> >> >> Breeze:
>> >> >>> >> >> Iteration -1: loss 1.3862943611198926, diff 1.0
>> >> >>> >> >> Mar 6, 2014 3:59:11 PM com.github.fommil.netlib.BLAS <clinit>
>> >> >>> >> >> WARNING: Failed to load implementation from:
>> >> >>> >> >> com.github.fommil.netlib.NativeSystemBLAS
>> >> >>> >> >> Mar 6, 2014 3:59:11 PM com.github.fommil.netlib.BLAS <clinit>
>> >> >>> >> >> WARNING: Failed to load implementation from:
>> >> >>> >> >> com.github.fommil.netlib.NativeRefBLAS
>> >> >>> >> >> Iteration 0: loss 1.3862943611198926, diff 0.0
>> >> >>> >> >> Iteration 1: loss 1.5846343143210866, diff
>> >> >>> >> >> 0.14307193024217352
>> >> >>> >> >> Iteration 2: loss 1.1242501524477688, diff
>> >> >>> >> >> 0.29053004039012126
>> >> >>> >> >> Iteration 3: loss 1.1242501524477688, diff 0.0
>> >> >>> >> >> Iteration 4: loss 1.1242501524477688, diff 0.0
>> >> >>> >> >> Iteration 5: loss 1.0930151243303563, diff
>> >> >>> >> >> 0.027782962952189336
>> >> >>> >> >> Iteration 6: loss 1.0930151243303563, diff 0.0
>> >> >>> >> >> Iteration 7: loss 1.0930151243303563, diff 0.0
>> >> >>> >> >> Iteration 8: loss 1.054036932835569, diff 0.03566113127440601
>> >> >>> >> >> Iteration 9: loss 1.054036932835569, diff 0.0
>> >> >>> >> >> Iteration 10: loss 1.054036932835569, diff 0.0
>> >> >>> >> >> Iteration 11: loss 0.9907956302751622, diff
>> >> >>> >> >> 0.05999907649459571
>> >> >>> >> >> Iteration 12: loss 0.9907956302751622, diff 0.0
>> >> >>> >> >> Iteration 13: loss 0.9907956302751622, diff 0.0
>> >> >>> >> >> Iteration 14: loss 0.9184205380342829, diff
>> >> >>> >> >> 0.07304737423337761
>> >> >>> >> >> Iteration 15: loss 0.9184205380342829, diff 0.0
>> >> >>> >> >> Iteration 16: loss 0.9184205380342829, diff 0.0
>> >> >>> >> >> Iteration 17: loss 0.8259870936519939, diff
>> >> >>> >> >> 0.1006438117513297
>> >> >>> >> >> Iteration 18: loss 0.8259870936519939, diff 0.0
>> >> >>> >> >> Iteration 19: loss 0.8259870936519939, diff 0.0
>> >> >>> >> >> Iteration 20: loss 0.6327447552109576, diff 0.233952934583647
>> >> >>> >> >> Iteration 21: loss 0.6327447552109576, diff 0.0
>> >> >>> >> >> Iteration 22: loss 0.6327447552109576, diff 0.0
>> >> >>> >> >> Iteration 23: loss 0.5534101162436362, diff
>> >> >>> >> >> 0.12538154276652747
>> >> >>> >> >> Iteration 24: loss 0.5534101162436362, diff 0.0
>> >> >>> >> >> Iteration 25: loss 0.5534101162436362, diff 0.0
>> >> >>> >> >> Iteration 26: loss 0.40450200866125635, diff
>> >> >>> >> >> 0.2690732137675816
>> >> >>> >> >> Iteration 27: loss 0.40450200866125635, diff 0.0
>> >> >>> >> >> Iteration 28: loss 0.40450200866125635, diff 0.0
>> >> >>> >> >> Iteration 29: loss 0.30788249908237314, diff
>> >> >>> >> >> 0.23885980452569502
>> >> >>> >> >>
>> >> >>> >> >> Sincerely,
>> >> >>> >> >>
>> >> >>> >> >> DB Tsai
>> >> >>> >> >> Machine Learning Engineer
>> >> >>> >> >> Alpine Data Labs
>> >> >>> >> >> --------------------------------------
>> >> >>> >> >> Web: http://alpinenow.com/
>> >> >>> >> >>
>> >> >>> >> >>
>> >> >>> >> >> On Wed, Mar 5, 2014 at 2:00 PM, David Hall
>> >> >>> >> >> <dlwh@cs.berkeley.edu>
>> >> >>> >> >> wrote:
>> >> >>> >> >> > On Wed, Mar 5, 2014 at 1:57 PM, DB Tsai
>> >> >>> >> >> > <dbtsai@alpinenow.com>
>> >> >>> >> >> > wrote:
>> >> >>> >> >> >
>> >> >>> >> >> >> Hi David,
>> >> >>> >> >> >>
>> >> >>> >> >> >> On Tue, Mar 4, 2014 at 8:13 PM, dlwh
>> >> >>> >> >> >> <david.lw.hall@gmail.com>
>> >> >>> >> >> >> wrote:
>> >> >>> >> >> >> > I'm happy to help fix any problems. I've verified at
>> >> >>> >> >> >> > points
>> >> >>> >> >> >> > that
>> >> >>> >> >> >> > the
>> >> >>> >> >> >> > implementation gives the exact same sequence of iterates
>> >> >>> >> >> >> > for a
>> >> >>> >> >> >> > few
>> >> >>> >> >> >> different
>> >> >>> >> >> >> > functions (with a particular line search) as the c port
>> >> >>> >> >> >> > of
>> >> >>> >> >> >> > lbfgs.
>> >> >>> >> >> >> > So
>> >> >>> >> >> I'm
>> >> >>> >> >> >> a
>> >> >>> >> >> >> > little surprised it fails where Fortran succeeds... but
>> >> >>> >> >> >> > only a
>> >> >>> >> >> >> > little.
>> >> >>> >> >> >> This
>> >> >>> >> >> >> > was fixed late last year.
>> >> >>> >> >> >> I'm working on a reproducible test case using breeze vs
>> >> >>> >> >> >> fortran
>> >> >>> >> >> >> implementation to show the problem I've run into. The test
>> >> >>> >> >> >> will
>> >> >>> >> >> >> be
>> >> >>> >> >> >> in
>> >> >>> >> >> >> one of the test cases in my Spark fork, is it okay for you
>> >> >>> >> >> >> to
>> >> >>> >> >> >> investigate the issue? Or do I need to make it as a
>> >> >>> >> >> >> standalone
>> >> >>> >> >> >> test?
>> >> >>> >> >> >>
>> >> >>> >> >> >
>> >> >>> >> >> >
>> >> >>> >> >> > Um, as long as it wouldn't be too hard to pull out.
>> >> >>> >> >> >
>> >> >>> >> >> >
>> >> >>> >> >> >>
>> >> >>> >> >> >> Will send you the test later today.
>> >> >>> >> >> >>
>> >> >>> >> >> >> Thanks.
>> >> >>> >> >> >>
>> >> >>> >> >> >> Sincerely,
>> >> >>> >> >> >>
>> >> >>> >> >> >> DB Tsai
>> >> >>> >> >> >> Machine Learning Engineer
>> >> >>> >> >> >> Alpine Data Labs
>> >> >>> >> >> >> --------------------------------------
>> >> >>> >> >> >> Web: http://alpinenow.com/
>> >> >>> >> >> >>
>> >> >>> >> >>
>> >> >>> >
>> >> >>> >
>> >> >>> >
>> >> >>
>> >> >>
>> >> >
>> >
>> >
>
>

From dev-return-7294-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr  9 09:31:59 2014
Return-Path: <dev-return-7294-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9C0991033B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Apr 2014 09:31:59 +0000 (UTC)
Received: (qmail 84783 invoked by uid 500); 9 Apr 2014 09:31:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84154 invoked by uid 500); 9 Apr 2014 09:31:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84139 invoked by uid 99); 9 Apr 2014 09:31:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Apr 2014 09:31:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.182 as permitted sender)
Received: from [209.85.214.182] (HELO mail-ob0-f182.google.com) (209.85.214.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Apr 2014 09:31:46 +0000
Received: by mail-ob0-f182.google.com with SMTP id uz6so2374823obc.41
        for <dev@spark.apache.org>; Wed, 09 Apr 2014 02:31:24 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=fqwXTPHIAv7XGU8idhQUW0ZE4Q+IggG4DeeOmBoX+BA=;
        b=jDqFNzO9DpaalqKABhjg+stDbq59A7CVGpzF8RnKBgzSPV5r0pC9BOslwZQ0ABaECx
         pLLvBhWWagsOP87CdfiOnGo+RRFQrssSGWEDDzXqm2UMANbPbqJxjys603vkTQ7Bqj34
         S8fW6Yu0PYnPnKNomJZmpgsOi5uuobQ/I3jZl1JeUiYqFUmOExGviYmemh/oE66TFdRz
         S10C74A+LhTAuRIJcCZRxDBUhv2tK71ZbS3gIDj+Dwm0M2gY/ZEzXiD69RN6XLxYLCNQ
         vW56/1adATpg6Ac1dXhzW+gub9gzl1tcnz2HVO9n1n3900KKvDM5ryExe7KCHdk6zLOM
         BUMg==
MIME-Version: 1.0
X-Received: by 10.60.131.40 with SMTP id oj8mr7804030oeb.14.1397035884493;
 Wed, 09 Apr 2014 02:31:24 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Wed, 9 Apr 2014 02:31:24 -0700 (PDT)
Date: Wed, 9 Apr 2014 02:31:24 -0700
Message-ID: <CABPQxsv_fV9eDh37Vtndn19WsfJPqdYDcPZxCdj9LRrVoYTm7g@mail.gmail.com>
Subject: branch-1.0 cut
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e013cb828a242e404f698c374
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013cb828a242e404f698c374
Content-Type: text/plain; charset=ISO-8859-1

Hey All,

In accordance with the scheduled window for the release I've cut a 1.0
branch. Thanks a ton to everyone for being so active in reviews during the
last week. In the last 7 days we've merged 66 new patches, and every one of
them has undergone thorough peer-review. Tons of committers have been
active in code review - pretty cool!

At this point the 1.0 branch transitions to a normal maintenance branch*.
Bug fixes, documentation are still welcome or additions to higher level
libraries (e.g. MLLib). The focus though is shifting to QA, fixes, and
documentation for the release.

Thanks again to everyone who participated in the last week!

- Patrick

*caveat: we will still merge in some API visibility patches and a few
remaining loose ends in the next day or two.

--089e013cb828a242e404f698c374--

From dev-return-7295-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr  9 19:40:06 2014
Return-Path: <dev-return-7295-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 009CB11AF1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Apr 2014 19:40:06 +0000 (UTC)
Received: (qmail 95530 invoked by uid 500); 9 Apr 2014 19:40:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95479 invoked by uid 500); 9 Apr 2014 19:40:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95470 invoked by uid 99); 9 Apr 2014 19:40:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Apr 2014 19:40:01 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of andykonwinski@gmail.com designates 209.85.215.51 as permitted sender)
Received: from [209.85.215.51] (HELO mail-la0-f51.google.com) (209.85.215.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Apr 2014 19:39:56 +0000
Received: by mail-la0-f51.google.com with SMTP id pv20so1511573lab.24
        for <dev@spark.apache.org>; Wed, 09 Apr 2014 12:39:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=FyXXZyZoMSCjK8kS/cKqntrS/oBsw6qj1wv3xnUkRmI=;
        b=CzMnNySWYYv5pQskvFfZWZ7xnMV6QUwj4qNDYQfZc/Mm473wnz4RjILhR1T5tknpm8
         nS9lCJjSbi4OwNe7LUzAiSZK6UBrSYSifnXe1v/kf+e2u7Fqs/BP6/J8mXIcw+SGYYS3
         IitQuix5kZ9hfmxcvWZMhfGgVzF2JY/mEg0PIYDIgRBHK7N8EMvPnA6H5S7sEc7aYgDf
         lxF4hxt55fheOjd0NpsYCsz+k5tRdOKkgV7N7mJE1gEb87GJmUHf6zdeFExMtYQhvu19
         Uh6ZSxeR2nlyfJImn7n4uW7uzbtsZKPPD8dpwrlUFzxW69bGYL9wGZo2Cnv+KPL+kNWI
         KIWw==
MIME-Version: 1.0
X-Received: by 10.153.7.69 with SMTP id da5mr3015553lad.38.1397072374308; Wed,
 09 Apr 2014 12:39:34 -0700 (PDT)
Received: by 10.113.3.163 with HTTP; Wed, 9 Apr 2014 12:39:34 -0700 (PDT)
In-Reply-To: <CABPQxsv_fV9eDh37Vtndn19WsfJPqdYDcPZxCdj9LRrVoYTm7g@mail.gmail.com>
References: <CABPQxsv_fV9eDh37Vtndn19WsfJPqdYDcPZxCdj9LRrVoYTm7g@mail.gmail.com>
Date: Wed, 9 Apr 2014 12:39:34 -0700
Message-ID: <CALEZFQwjkGSdmkEArtLYPW65GLitX5W1jwxvV31+2kKA56FNgA@mail.gmail.com>
Subject: Re: branch-1.0 cut
From: Andy Konwinski <andykonwinski@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134563098a56c04f6a14270
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134563098a56c04f6a14270
Content-Type: text/plain; charset=ISO-8859-1

Wow, great work. Very impressive sticking to the schedule!


On Wed, Apr 9, 2014 at 2:31 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> Hey All,
>
> In accordance with the scheduled window for the release I've cut a 1.0
> branch. Thanks a ton to everyone for being so active in reviews during the
> last week. In the last 7 days we've merged 66 new patches, and every one of
> them has undergone thorough peer-review. Tons of committers have been
> active in code review - pretty cool!
>
> At this point the 1.0 branch transitions to a normal maintenance branch*.
> Bug fixes, documentation are still welcome or additions to higher level
> libraries (e.g. MLLib). The focus though is shifting to QA, fixes, and
> documentation for the release.
>
> Thanks again to everyone who participated in the last week!
>
> - Patrick
>
> *caveat: we will still merge in some API visibility patches and a few
> remaining loose ends in the next day or two.
>

--001a1134563098a56c04f6a14270--

From dev-return-7296-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr  9 21:55:05 2014
Return-Path: <dev-return-7296-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1CE1411F3E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Apr 2014 21:55:05 +0000 (UTC)
Received: (qmail 10236 invoked by uid 500); 9 Apr 2014 21:55:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10076 invoked by uid 500); 9 Apr 2014 21:55:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10060 invoked by uid 99); 9 Apr 2014 21:55:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Apr 2014 21:55:01 +0000
X-ASF-Spam-Status: No, hits=-0.5 required=5.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.128.181 as permitted sender)
Received: from [209.85.128.181] (HELO mail-ve0-f181.google.com) (209.85.128.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Apr 2014 21:54:55 +0000
Received: by mail-ve0-f181.google.com with SMTP id oy12so2613110veb.26
        for <multiple recipients>; Wed, 09 Apr 2014 14:54:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type
         :content-transfer-encoding;
        bh=au6/LQ6WgU3afTcSSKOE1X1WALC0AjblswyZlDJlPf8=;
        b=y8aO4cW2VwgZLWaYRYaP98JlNmoU9Ycx7/Uvo54DuKmtHeoc0ccYQO+AuMck0V7XIp
         yaPR/bEGknVe9A6RLvLmuK5sHJY7DnGXcZ0gyBs6ev+x16XkYlmNc1vG5K5TKkS3w3tB
         PeNLVQJLsTFqliO24gxWOgxUH5dWnWZQPUHn1EhP3A81k+Ns6aUUQ1DlGXSYn3MCnoLG
         k7U08R1ih44vUBo4KaTncuOGjXzE8kLwSI1xQjXhZd99Iu46e3yrEyfjDTLAdhvjEb3D
         nqmmicNTarMv67TO/th6h3yJJUDDczLZ+9yXO8F33eps9hLJVjPO16cnhE/beOfDnmQc
         JyEw==
X-Received: by 10.52.33.136 with SMTP id r8mr9150600vdi.2.1397080473442; Wed,
 09 Apr 2014 14:54:33 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.191.72 with HTTP; Wed, 9 Apr 2014 14:54:03 -0700 (PDT)
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Wed, 9 Apr 2014 14:54:03 -0700
Message-ID: <CAMwrk0=i9nhengQ2b3E3gCvUz4fD06mF_d7upY-U2rRuhvcB=Q@mail.gmail.com>
Subject: Spark 0.9.1 released
To: dev@spark.apache.org, user@spark.apache.org
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hi everyone,

We have just posted Spark 0.9.1, which is a maintenance release with
bug fixes, performance improvements, better stability with YARN and
improved parity of the Scala and Python API. We recommend all 0.9.0
users to upgrade to this stable release.

This is the first release since Spark graduated as a top level Apache
project. Contributions to this release came from 37 developers.

The full release notes are at:
http://spark.apache.org/releases/spark-release-0-9-1.html

You can download the release at:
http://spark.apache.org/downloads.html

Thanks all the developers who contributed to this release:
Aaron Davidson, Aaron Kimball, Andrew Ash, Andrew Or, Andrew Tulloch,
Bijay Bisht, Bouke van der Bijl, Bryn Keller, Chen Chao,
Christian Lundgren, Diana Carroll, Emtiaz Ahmed, Frank Dai,
Henry Saputra, jianghan, Josh Rosen, Jyotiska NK, Kay Ousterhout,
Kousuke Saruta, Mark Grover, Matei Zaharia, Nan Zhu, Nick Lanham,
Patrick Wendell, Prabin Banka, Prashant Sharma, Qiuzhuang,
Raymond Liu, Reynold Xin, Sandy Ryza, Sean Owen, Shixiong Zhu,
shiyun.wxm, Stevo Slavi=C4=87, Tathagata Das, Tom Graves, Xiangrui Meng

TD

From dev-return-7297-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr  9 22:00:09 2014
Return-Path: <dev-return-7297-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E524B11F70
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Apr 2014 22:00:09 +0000 (UTC)
Received: (qmail 22689 invoked by uid 500); 9 Apr 2014 22:00:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22550 invoked by uid 500); 9 Apr 2014 22:00:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22532 invoked by uid 99); 9 Apr 2014 22:00:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Apr 2014 22:00:05 +0000
X-ASF-Spam-Status: No, hits=1.7 required=5.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.128.170 as permitted sender)
Received: from [209.85.128.170] (HELO mail-ve0-f170.google.com) (209.85.128.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Apr 2014 22:00:00 +0000
Received: by mail-ve0-f170.google.com with SMTP id pa12so2773332veb.1
        for <multiple recipients>; Wed, 09 Apr 2014 14:59:38 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=9y8fGSAQdHKNl9rz+VUMGjFw/ELT/7YmMiAnY6eeDV0=;
        b=ed3dGzOSNZm/I4KXrDvjrv7uwLKr4/ZiMt1VFkpViIlp6rru9H5HVQiF2qPRYjKfXm
         6hlG/xjSVtmUcToP5jkXRoODycFwwx71Bgyr5r4db3WNbGwzTbVdVfn4XgPfY8POkrEI
         9VgWsCkKnK6A6jAmjL4XiFoesj6z/NBWnl1ibusSmQH3PLFqu7KpKuBF3zrieDW2OGmd
         Gr9A+m1IFXdFokfAEhqpPcDfZhppdJB7Z1OOxMVyRobPc1SC+nEA5fJ2V+ZMBq95IGAo
         G2R81zfoFpz1/5nFabDVhx2PjAw30Bxwlk8d0f19SwSEiJm1fCovBgFlLJtHBZ7SVIG9
         3R1A==
X-Received: by 10.58.55.170 with SMTP id t10mr2598751vep.29.1397080777938;
 Wed, 09 Apr 2014 14:59:37 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.191.72 with HTTP; Wed, 9 Apr 2014 14:59:07 -0700 (PDT)
In-Reply-To: <CAMwrk0=i9nhengQ2b3E3gCvUz4fD06mF_d7upY-U2rRuhvcB=Q@mail.gmail.com>
References: <CAMwrk0=i9nhengQ2b3E3gCvUz4fD06mF_d7upY-U2rRuhvcB=Q@mail.gmail.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Wed, 9 Apr 2014 14:59:07 -0700
Message-ID: <CAMwrk0kB2aySkRbJhqqxx_mYCuungGpu8BqgqnNJ73tgmxyLXg@mail.gmail.com>
Subject: Re: Spark 0.9.1 released
To: dev@spark.apache.org, user@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b2e53367ddf7804f6a337a4
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b2e53367ddf7804f6a337a4
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

A small additional note: Please use the direct download links in the Spark
Downloads <http://spark.apache.org/downloads.html> page. The Apache mirrors
take a day or so to sync from the main repo, so may not work immediately.

TD


On Wed, Apr 9, 2014 at 2:54 PM, Tathagata Das
<tathagata.das1565@gmail.com>wrote:

> Hi everyone,
>
> We have just posted Spark 0.9.1, which is a maintenance release with
> bug fixes, performance improvements, better stability with YARN and
> improved parity of the Scala and Python API. We recommend all 0.9.0
> users to upgrade to this stable release.
>
> This is the first release since Spark graduated as a top level Apache
> project. Contributions to this release came from 37 developers.
>
> The full release notes are at:
> http://spark.apache.org/releases/spark-release-0-9-1.html
>
> You can download the release at:
> http://spark.apache.org/downloads.html
>
> Thanks all the developers who contributed to this release:
> Aaron Davidson, Aaron Kimball, Andrew Ash, Andrew Or, Andrew Tulloch,
> Bijay Bisht, Bouke van der Bijl, Bryn Keller, Chen Chao,
> Christian Lundgren, Diana Carroll, Emtiaz Ahmed, Frank Dai,
> Henry Saputra, jianghan, Josh Rosen, Jyotiska NK, Kay Ousterhout,
> Kousuke Saruta, Mark Grover, Matei Zaharia, Nan Zhu, Nick Lanham,
> Patrick Wendell, Prabin Banka, Prashant Sharma, Qiuzhuang,
> Raymond Liu, Reynold Xin, Sandy Ryza, Sean Owen, Shixiong Zhu,
> shiyun.wxm, Stevo Slavi=C4=87, Tathagata Das, Tom Graves, Xiangrui Meng
>
> TD
>

--047d7b2e53367ddf7804f6a337a4--

From dev-return-7298-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr  9 22:01:03 2014
Return-Path: <dev-return-7298-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6180011F77
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Apr 2014 22:01:03 +0000 (UTC)
Received: (qmail 24447 invoked by uid 500); 9 Apr 2014 22:01:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24401 invoked by uid 500); 9 Apr 2014 22:01:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24393 invoked by uid 99); 9 Apr 2014 22:01:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Apr 2014 22:01:01 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of svsujeet@gmail.com designates 209.85.216.48 as permitted sender)
Received: from [209.85.216.48] (HELO mail-qa0-f48.google.com) (209.85.216.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Apr 2014 22:00:57 +0000
Received: by mail-qa0-f48.google.com with SMTP id s7so2572989qap.35
        for <dev@spark.apache.org>; Wed, 09 Apr 2014 15:00:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=VOnxCIU4TiT4/h74uq8KfEwxYoTgczVGROVHHspEHwU=;
        b=FBBPwVr2dy0Cl/EP+dVCHqnfnKejxBY4vEqcazs/qrzCKnkFmCcHIg2enMlkJ69I/1
         UyT8awp/8ZoVI4TzVxucHyYBpZCTBk5JgRVIVibmEKN/JwfpJAcMRRe0QoTiH7qf24wW
         Ulgp4IkHQlerk1m9naFIprKJgQRqiM2xq++NCa8qpRjAoVP5FMqqWmk16NnBh1oLrKDj
         fJuiWr9yu1JqJiHzyQoZ29A0gOtI73u/NrGmFcijE0sbEqn740jwgaPWJLiyaG/2yL2x
         M0bITlIkfXyenmnUW6JAxg9gWLAyHvN9pJcFw2fl1ZMrC+JnHcbfL+m9ZBhxgLInoTjm
         fS5g==
MIME-Version: 1.0
X-Received: by 10.140.33.244 with SMTP id j107mr15144143qgj.81.1397080834930;
 Wed, 09 Apr 2014 15:00:34 -0700 (PDT)
Received: by 10.140.104.46 with HTTP; Wed, 9 Apr 2014 15:00:34 -0700 (PDT)
In-Reply-To: <CADpnHP3iKL7Xi9mXTQiByMbNs-JYureMz2A=7rnK-jWicw0H-w@mail.gmail.com>
References: <CAGRER0deRfjEdguW5tjJPFBSuiap2P9-q21WBZyNyVbL4onHfA@mail.gmail.com>
	<CAJHwgdshcGJLNjT118A79OWg6Gb_qaaNvoQZiw1WcQgQfinS3g@mail.gmail.com>
	<CAGRER0c_tMqMep23RsXDBZGNbU0=ZgedsSzh3BELYfhaUFabnw@mail.gmail.com>
	<3A236731-EDDE-4B26-A8A8-5E66C287687E@gmail.com>
	<CANGvG8rYs_vPM8t_9Q3HvzTSnmYQWND06RCP5=b0H1iJsNNTyA@mail.gmail.com>
	<EDCA85D5-3E3F-44D6-BA7F-4DEEFD863A2D@gmail.com>
	<CADpnHP3iKL7Xi9mXTQiByMbNs-JYureMz2A=7rnK-jWicw0H-w@mail.gmail.com>
Date: Wed, 9 Apr 2014 15:00:34 -0700
Message-ID: <CANJXpKdAZmma0kpDj=W6=XcndV-V7OmsQBYA4jTsA=oaUcU-fw@mail.gmail.com>
Subject: Re: Contributing to Spark
From: Sujeet Varakhedi <svsujeet@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113aa980e37db104f6a33a76
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113aa980e37db104f6a33a76
Content-Type: text/plain; charset=ISO-8859-1

Another starter question which probably should have asked before is what is
the most efficient way to iterate quickly on dev/test. I am currently using
a local cluster (via vagrant and shared folders)  and also spark-shell.

Sujeet


On Tue, Apr 8, 2014 at 9:50 AM, Michael Ernest <mfernest@cloudera.com>wrote:

> Ha ha! nice try, sheepherder! ;-)
>
>
> On Tue, Apr 8, 2014 at 12:37 PM, Matei Zaharia <matei.zaharia@gmail.com
> >wrote:
>
> > Shh, maybe I really wanted people to fix that one issue.
> >
> > On Apr 8, 2014, at 9:34 AM, Aaron Davidson <ilikerps@gmail.com> wrote:
> >
> > > Matei's link seems to point to a specific starter project as part of
> the
> > > starter list, but here is the list itself:
> > >
> >
> https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20labels%20%3D%20Starter%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)
> > >
> > >
> > > On Mon, Apr 7, 2014 at 10:22 PM, Matei Zaharia <
> matei.zaharia@gmail.com
> > >wrote:
> > >
> > >> I'd suggest looking for the issues labeled "Starter" on JIRA. You can
> > find
> > >> them here:
> > >>
> >
> https://issues.apache.org/jira/browse/SPARK-1438?jql=project%20%3D%20SPARK%20AND%20labels%20%3D%20Starter%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)
> > >>
> > >> Matei
> > >>
> > >> On Apr 7, 2014, at 9:45 PM, Mukesh G <mukgbv@gmail.com> wrote:
> > >>
> > >>> Hi Sujeet,
> > >>>
> > >>>   Thanks. I went thru the website and looks great. Is there a list of
> > >>> items that I can choose from, for contribution?
> > >>>
> > >>> Thanks
> > >>>
> > >>> Mukesh
> > >>>
> > >>>
> > >>> On Mon, Apr 7, 2014 at 10:14 PM, Sujeet Varakhedi
> > >>> <svarakhedi@gopivotal.com>wrote:
> > >>>
> > >>>> This is a good place to start:
> > >>>>
> > https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
> > >>>>
> > >>>> Sujeet
> > >>>>
> > >>>>
> > >>>> On Mon, Apr 7, 2014 at 9:20 AM, Mukesh G <mukgbv@gmail.com> wrote:
> > >>>>
> > >>>>> Hi,
> > >>>>>
> > >>>>>  How I contribute to Spark and it's associated projects?
> > >>>>>
> > >>>>> Appreciate the help...
> > >>>>>
> > >>>>> Thanks
> > >>>>>
> > >>>>> Mukesh
> > >>>>>
> > >>>>
> > >>
> > >>
> >
> >
>
>
> --
> Michael Ernest
> Sr. Solutions Consultant
> West Coast
>

--001a113aa980e37db104f6a33a76--

From dev-return-7299-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr  9 22:08:37 2014
Return-Path: <dev-return-7299-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4F20411FAB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Apr 2014 22:08:37 +0000 (UTC)
Received: (qmail 39326 invoked by uid 500); 9 Apr 2014 22:08:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39221 invoked by uid 500); 9 Apr 2014 22:08:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38985 invoked by uid 99); 9 Apr 2014 22:08:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Apr 2014 22:08:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.41 as permitted sender)
Received: from [209.85.220.41] (HELO mail-pa0-f41.google.com) (209.85.220.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Apr 2014 22:08:09 +0000
Received: by mail-pa0-f41.google.com with SMTP id fa1so3097264pad.0
        for <multiple recipients>; Wed, 09 Apr 2014 15:07:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :message-id:references:to;
        bh=ZYhqeGjPbDx5dQDoqWPYb5iJCf8yvq6/7Csl9n5e+r0=;
        b=jGDbPMfqQeXwdEJ83crZHH/qtypa2YBSBZPhAzVYfL7CBMzao5Tb9P6uryh3LMkrmF
         UMZuZKHr3/qMqmT6+xcdyYgt4957QECWnin4G1UC1+e/DJtofeJvn5d8Jal+ain9Xq3S
         +6Omn0gtZNmVy6NRFCJh8gELbhAX0CVJxxaN7umkfANKbWWSn7fpynyzUhyReqND/aZk
         50AIABM59Em3Qr+srjH6+kLCZqSRoJGizt+Q50mBtbtm1usrEXFjhmCOiHNjqyP+fT4V
         UgMkF/8IPhGm+D0zr/tcua01lkQdfwaXeQch37OVYIz8a1YslI42aTg1qrEEKfSiKRDV
         pu0w==
X-Received: by 10.68.248.7 with SMTP id yi7mr15247072pbc.31.1397081268929;
        Wed, 09 Apr 2014 15:07:48 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id rk15sm10476243pab.37.2014.04.09.15.07.42
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 09 Apr 2014 15:07:44 -0700 (PDT)
Content-Type: multipart/alternative; boundary="Apple-Mail=_41E43D0C-9E4E-40F9-84AA-57595163ACAB"
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: Spark 0.9.1 released
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAMwrk0kB2aySkRbJhqqxx_mYCuungGpu8BqgqnNJ73tgmxyLXg@mail.gmail.com>
Date: Wed, 9 Apr 2014 15:07:38 -0700
Cc: dev@spark.apache.org
Message-Id: <677A81B8-2CAC-419E-8721-9C7A884AD2B8@gmail.com>
References: <CAMwrk0=i9nhengQ2b3E3gCvUz4fD06mF_d7upY-U2rRuhvcB=Q@mail.gmail.com> <CAMwrk0kB2aySkRbJhqqxx_mYCuungGpu8BqgqnNJ73tgmxyLXg@mail.gmail.com>
To: user@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_41E43D0C-9E4E-40F9-84AA-57595163ACAB
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=utf-8

Thanks TD for managing this release, and thanks to everyone who =
contributed!

Matei

On Apr 9, 2014, at 2:59 PM, Tathagata Das <tathagata.das1565@gmail.com> =
wrote:

> A small additional note: Please use the direct download links in the =
Spark Downloads page. The Apache mirrors take a day or so to sync from =
the main repo, so may not work immediately.
>=20
> TD
>=20
>=20
> On Wed, Apr 9, 2014 at 2:54 PM, Tathagata Das =
<tathagata.das1565@gmail.com> wrote:
> Hi everyone,
>=20
> We have just posted Spark 0.9.1, which is a maintenance release with
> bug fixes, performance improvements, better stability with YARN and
> improved parity of the Scala and Python API. We recommend all 0.9.0
> users to upgrade to this stable release.
>=20
> This is the first release since Spark graduated as a top level Apache
> project. Contributions to this release came from 37 developers.
>=20
> The full release notes are at:
> http://spark.apache.org/releases/spark-release-0-9-1.html
>=20
> You can download the release at:
> http://spark.apache.org/downloads.html
>=20
> Thanks all the developers who contributed to this release:
> Aaron Davidson, Aaron Kimball, Andrew Ash, Andrew Or, Andrew Tulloch,
> Bijay Bisht, Bouke van der Bijl, Bryn Keller, Chen Chao,
> Christian Lundgren, Diana Carroll, Emtiaz Ahmed, Frank Dai,
> Henry Saputra, jianghan, Josh Rosen, Jyotiska NK, Kay Ousterhout,
> Kousuke Saruta, Mark Grover, Matei Zaharia, Nan Zhu, Nick Lanham,
> Patrick Wendell, Prabin Banka, Prashant Sharma, Qiuzhuang,
> Raymond Liu, Reynold Xin, Sandy Ryza, Sean Owen, Shixiong Zhu,
> shiyun.wxm, Stevo Slavi=C4=87, Tathagata Das, Tom Graves, Xiangrui =
Meng
>=20
> TD
>=20


--Apple-Mail=_41E43D0C-9E4E-40F9-84AA-57595163ACAB--

From dev-return-7300-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr  9 22:24:22 2014
Return-Path: <dev-return-7300-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5461110067
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Apr 2014 22:24:22 +0000 (UTC)
Received: (qmail 72247 invoked by uid 500); 9 Apr 2014 22:24:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72213 invoked by uid 500); 9 Apr 2014 22:24:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72205 invoked by uid 99); 9 Apr 2014 22:24:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Apr 2014 22:24:20 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.41] (HELO mail-qg0-f41.google.com) (209.85.192.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Apr 2014 22:24:16 +0000
Received: by mail-qg0-f41.google.com with SMTP id z60so1493177qgd.14
        for <dev@spark.apache.org>; Wed, 09 Apr 2014 15:23:53 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=IgvGr2EEbsoWB2jWhMRIjmam+wTKCc55Hh0EEov076E=;
        b=i74q/nWYSf/4QKMouQ5OVHbbElUVsLnCWHM44CxbwT6VBVkL6bkPKek8QtXi9NAZtS
         sa1bfaQTmn2UVBJ9/FyUM/yMZeiA9mu8lF1eQ42gF0XcdYPakqaOg+WxboPqR4kaVhEb
         2U0v9uuU8y5fSGwn7ZXHMAmP4ID1PfH3ww8yP45puM1K3W0Tp6XbGizDmYNgEbV1GmAu
         WJAXiWMsJKmEOyVdcaJ8Og1yxgLgCNU6SpE+fd/pqHJHLk5DaXjlUIJi1gEFwEQFckov
         IhZdo7jEjrbFDJIXsbpgYqLxTAUjQLKeRUpdc3igfgNW5wJYNEHbF9/n4tsXm5yNwFMD
         TxmQ==
X-Gm-Message-State: ALoCoQlBUmk6TKTPpvZR1jCT6W1mLNlaVrrHb+bXCFn5T5bTOYOH0TbDDSXAqB0gAKs3pIm7RY7h
MIME-Version: 1.0
X-Received: by 10.140.49.109 with SMTP id p100mr15531332qga.47.1397082233056;
 Wed, 09 Apr 2014 15:23:53 -0700 (PDT)
Received: by 10.96.126.1 with HTTP; Wed, 9 Apr 2014 15:23:52 -0700 (PDT)
In-Reply-To: <CANJXpKdAZmma0kpDj=W6=XcndV-V7OmsQBYA4jTsA=oaUcU-fw@mail.gmail.com>
References: <CAGRER0deRfjEdguW5tjJPFBSuiap2P9-q21WBZyNyVbL4onHfA@mail.gmail.com>
	<CAJHwgdshcGJLNjT118A79OWg6Gb_qaaNvoQZiw1WcQgQfinS3g@mail.gmail.com>
	<CAGRER0c_tMqMep23RsXDBZGNbU0=ZgedsSzh3BELYfhaUFabnw@mail.gmail.com>
	<3A236731-EDDE-4B26-A8A8-5E66C287687E@gmail.com>
	<CANGvG8rYs_vPM8t_9Q3HvzTSnmYQWND06RCP5=b0H1iJsNNTyA@mail.gmail.com>
	<EDCA85D5-3E3F-44D6-BA7F-4DEEFD863A2D@gmail.com>
	<CADpnHP3iKL7Xi9mXTQiByMbNs-JYureMz2A=7rnK-jWicw0H-w@mail.gmail.com>
	<CANJXpKdAZmma0kpDj=W6=XcndV-V7OmsQBYA4jTsA=oaUcU-fw@mail.gmail.com>
Date: Wed, 9 Apr 2014 15:23:52 -0700
Message-ID: <CAPh_B=aew_6LFXiQogtPcoikZUosBjCs7GXo6bWJm1gj7couzQ@mail.gmail.com>
Subject: Re: Contributing to Spark
From: Reynold Xin <rxin@databricks.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1137087c39474104f6a38e5b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1137087c39474104f6a38e5b
Content-Type: text/plain; charset=ISO-8859-1

Usually you can just run Spark in local mode on a single machine for most
dev/testing.

If you want to simulate a cluster locally using multiple Spark worker
processes, you can use the undocumented local cluster mode, e.g.

local-cluster[2,1,512]

this launches two worker processes, each with one core and 512m of ram.





On Wed, Apr 9, 2014 at 3:00 PM, Sujeet Varakhedi <svsujeet@gmail.com> wrote:

> Another starter question which probably should have asked before is what is
> the most efficient way to iterate quickly on dev/test. I am currently using
> a local cluster (via vagrant and shared folders)  and also spark-shell.
>
> Sujeet
>
>
> On Tue, Apr 8, 2014 at 9:50 AM, Michael Ernest <mfernest@cloudera.com
> >wrote:
>
> > Ha ha! nice try, sheepherder! ;-)
> >
> >
> > On Tue, Apr 8, 2014 at 12:37 PM, Matei Zaharia <matei.zaharia@gmail.com
> > >wrote:
> >
> > > Shh, maybe I really wanted people to fix that one issue.
> > >
> > > On Apr 8, 2014, at 9:34 AM, Aaron Davidson <ilikerps@gmail.com> wrote:
> > >
> > > > Matei's link seems to point to a specific starter project as part of
> > the
> > > > starter list, but here is the list itself:
> > > >
> > >
> >
> https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20labels%20%3D%20Starter%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)
> > > >
> > > >
> > > > On Mon, Apr 7, 2014 at 10:22 PM, Matei Zaharia <
> > matei.zaharia@gmail.com
> > > >wrote:
> > > >
> > > >> I'd suggest looking for the issues labeled "Starter" on JIRA. You
> can
> > > find
> > > >> them here:
> > > >>
> > >
> >
> https://issues.apache.org/jira/browse/SPARK-1438?jql=project%20%3D%20SPARK%20AND%20labels%20%3D%20Starter%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)
> > > >>
> > > >> Matei
> > > >>
> > > >> On Apr 7, 2014, at 9:45 PM, Mukesh G <mukgbv@gmail.com> wrote:
> > > >>
> > > >>> Hi Sujeet,
> > > >>>
> > > >>>   Thanks. I went thru the website and looks great. Is there a list
> of
> > > >>> items that I can choose from, for contribution?
> > > >>>
> > > >>> Thanks
> > > >>>
> > > >>> Mukesh
> > > >>>
> > > >>>
> > > >>> On Mon, Apr 7, 2014 at 10:14 PM, Sujeet Varakhedi
> > > >>> <svarakhedi@gopivotal.com>wrote:
> > > >>>
> > > >>>> This is a good place to start:
> > > >>>>
> > >
> https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
> > > >>>>
> > > >>>> Sujeet
> > > >>>>
> > > >>>>
> > > >>>> On Mon, Apr 7, 2014 at 9:20 AM, Mukesh G <mukgbv@gmail.com>
> wrote:
> > > >>>>
> > > >>>>> Hi,
> > > >>>>>
> > > >>>>>  How I contribute to Spark and it's associated projects?
> > > >>>>>
> > > >>>>> Appreciate the help...
> > > >>>>>
> > > >>>>> Thanks
> > > >>>>>
> > > >>>>> Mukesh
> > > >>>>>
> > > >>>>
> > > >>
> > > >>
> > >
> > >
> >
> >
> > --
> > Michael Ernest
> > Sr. Solutions Consultant
> > West Coast
> >
>

--001a1137087c39474104f6a38e5b--

From dev-return-7301-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr  9 23:22:38 2014
Return-Path: <dev-return-7301-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 43CC210280
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Apr 2014 23:22:38 +0000 (UTC)
Received: (qmail 74997 invoked by uid 500); 9 Apr 2014 23:22:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74847 invoked by uid 500); 9 Apr 2014 23:22:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74831 invoked by uid 99); 9 Apr 2014 23:22:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Apr 2014 23:22:34 +0000
X-ASF-Spam-Status: No, hits=2.7 required=5.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.128.177 as permitted sender)
Received: from [209.85.128.177] (HELO mail-ve0-f177.google.com) (209.85.128.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Apr 2014 23:22:29 +0000
Received: by mail-ve0-f177.google.com with SMTP id sa20so2676998veb.22
        for <multiple recipients>; Wed, 09 Apr 2014 16:22:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=xmOARgWiXFty6U8oKSrY3mI7RDf+mCI9qSymwXWMdQI=;
        b=JMK7pIg9IJF8BnSME36hu0m3592fjPsaKG+h0bQvtdGiB99jIM8I+ysN18LnP/9/3Z
         Xg/yRuOzN+QHLr7GIaIPnRxjteRbM+aC5BjWFGEKZu+M3uk+1P8GuxwUODiklKTkhgZm
         nm1Gph9huta7HR6yDlvJS4MOEDcwUUza2NYmE8gv6VhhCv4AvkcT6ss+Ehc9aL5OVGTW
         1J6yim4ch7afUnFO0tIh5ILm+bX5FEsZeB0C2wOfVubn467Vx3LuVEoRsO2vK41l16Kr
         8WCHKqcvroVmxniwPY32ovr5CZh9sy4PG7taqIS8UazZiYx2vIoXkrx9fzEdhv+y+pWV
         V6Iw==
X-Received: by 10.52.104.7 with SMTP id ga7mr2747521vdb.29.1397085728163; Wed,
 09 Apr 2014 16:22:08 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.191.72 with HTTP; Wed, 9 Apr 2014 16:21:38 -0700 (PDT)
In-Reply-To: <CAOhmDzfuYNHOxtUszPsSRhurV6S86K-5kDh4qoxn0A4vyUoyMA@mail.gmail.com>
References: <CAMwrk0=i9nhengQ2b3E3gCvUz4fD06mF_d7upY-U2rRuhvcB=Q@mail.gmail.com>
 <CAMwrk0kB2aySkRbJhqqxx_mYCuungGpu8BqgqnNJ73tgmxyLXg@mail.gmail.com>
 <677A81B8-2CAC-419E-8721-9C7A884AD2B8@gmail.com> <CAOhmDzfuYNHOxtUszPsSRhurV6S86K-5kDh4qoxn0A4vyUoyMA@mail.gmail.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Wed, 9 Apr 2014 16:21:38 -0700
Message-ID: <CAMwrk0=erPpDEoU558U61_2Yj2sBJig7k_z24m88ysCnm6NKww@mail.gmail.com>
Subject: Re: Spark 0.9.1 released
To: user@spark.apache.org
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1136be728c4b7604f6a45ed9
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1136be728c4b7604f6a45ed9
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Thanks Nick for pointing that out! I have updated the release
notes<http://spark.apache.org/releases/spark-release-0-9-1.html>.
But I see the new operations like repartition in the latest PySpark
RDD docs<http://spark.apache.org/docs/latest/api/pyspark/index.html>.
Maybe refresh the page couple of times?

TD


On Wed, Apr 9, 2014 at 3:58 PM, Nicholas Chammas <nicholas.chammas@gmail.co=
m
> wrote:

> A very nice addition for us PySpark users in 0.9.1 is the addition of
> RDD.repartition(), which is not mentioned in the release notes<http://spa=
rk.apache.org/releases/spark-release-0-9-1.html>
> !
>
> This is super helpful for when you create an RDD from a gzipped file and
> then need to explicitly shuffle the data around to parallelize operations
> on it appropriately.
>
> Thanks people!
>
> FYI, docs/latest<http://spark.apache.org/docs/latest/api/pyspark/index.ht=
ml>hasn't been updated yet to reflect the new additions to PySpark.
>
> Nick
>
>
>
> On Wed, Apr 9, 2014 at 6:07 PM, Matei Zaharia <matei.zaharia@gmail.com>wr=
ote:
>
>> Thanks TD for managing this release, and thanks to everyone who
>> contributed!
>>
>> Matei
>>
>> On Apr 9, 2014, at 2:59 PM, Tathagata Das <tathagata.das1565@gmail.com>
>> wrote:
>>
>> A small additional note: Please use the direct download links in the
>> Spark Downloads <http://spark.apache.org/downloads.html> page. The
>> Apache mirrors take a day or so to sync from the main repo, so may not w=
ork
>> immediately.
>>
>> TD
>>
>>
>> On Wed, Apr 9, 2014 at 2:54 PM, Tathagata Das <
>> tathagata.das1565@gmail.com> wrote:
>>
>>> Hi everyone,
>>>
>>> We have just posted Spark 0.9.1, which is a maintenance release with
>>> bug fixes, performance improvements, better stability with YARN and
>>> improved parity of the Scala and Python API. We recommend all 0.9.0
>>> users to upgrade to this stable release.
>>>
>>> This is the first release since Spark graduated as a top level Apache
>>> project. Contributions to this release came from 37 developers.
>>>
>>> The full release notes are at:
>>> http://spark.apache.org/releases/spark-release-0-9-1.html
>>>
>>> You can download the release at:
>>> http://spark.apache.org/downloads.html
>>>
>>> Thanks all the developers who contributed to this release:
>>> Aaron Davidson, Aaron Kimball, Andrew Ash, Andrew Or, Andrew Tulloch,
>>> Bijay Bisht, Bouke van der Bijl, Bryn Keller, Chen Chao,
>>> Christian Lundgren, Diana Carroll, Emtiaz Ahmed, Frank Dai,
>>> Henry Saputra, jianghan, Josh Rosen, Jyotiska NK, Kay Ousterhout,
>>> Kousuke Saruta, Mark Grover, Matei Zaharia, Nan Zhu, Nick Lanham,
>>> Patrick Wendell, Prabin Banka, Prashant Sharma, Qiuzhuang,
>>> Raymond Liu, Reynold Xin, Sandy Ryza, Sean Owen, Shixiong Zhu,
>>> shiyun.wxm, Stevo Slavi=C4=87, Tathagata Das, Tom Graves, Xiangrui Meng
>>>
>>> TD
>>>
>>
>>
>>
>

--001a1136be728c4b7604f6a45ed9--

From dev-return-7302-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 10 00:48:28 2014
Return-Path: <dev-return-7302-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 56E0E105C2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Apr 2014 00:48:28 +0000 (UTC)
Received: (qmail 75022 invoked by uid 500); 10 Apr 2014 00:48:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74911 invoked by uid 500); 10 Apr 2014 00:48:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74903 invoked by uid 99); 10 Apr 2014 00:48:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 00:48:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of andykonwinski@gmail.com designates 209.85.217.176 as permitted sender)
Received: from [209.85.217.176] (HELO mail-lb0-f176.google.com) (209.85.217.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 00:48:22 +0000
Received: by mail-lb0-f176.google.com with SMTP id 10so1719899lbg.7
        for <dev@spark.incubator.apache.org>; Wed, 09 Apr 2014 17:48:00 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=Y2GTO3Ayg2DsYMsxLSs8QVqKt7k6WDgdstx60DfjxBo=;
        b=kpVcf1Lgyf5/Gv4E0wNTFHFYRELR0G5giK5u1QmMHtMVUcaR9ThhJHYSnceb3IuXon
         9H4DvrGQ2UVJhkbBN7x4flaZs+Uf0sBcXH2zhvhko18B8NZTRSwztElGrwOfRzk9YV2H
         V3w1niCyfiNMciLTYaaBPIb/PI80JazA8Xr2BFz/FU+oji96DvP8Uk6KqD/kwRs31sLu
         ZixTOqYa7XgPFPDMZ1HJyobXMzFnBiSmKzfTumi5i57vI2Tanaf37vEoytSlnOYj+ND8
         2W2P+tDfwww+wc2JQj2wWBnuCOFaob5//4wm1n5ynS4l3nvd7jrcchgZud0yxVFiCjWn
         yuiQ==
MIME-Version: 1.0
X-Received: by 10.112.137.5 with SMTP id qe5mr9315561lbb.16.1397090880604;
 Wed, 09 Apr 2014 17:48:00 -0700 (PDT)
Received: by 10.113.3.163 with HTTP; Wed, 9 Apr 2014 17:48:00 -0700 (PDT)
Date: Wed, 9 Apr 2014 17:48:00 -0700
Message-ID: <CALEZFQyiLNmTN9aVxFnN0c-87bAkEiMPUygHOeTwA=oQqwQTYA@mail.gmail.com>
Subject: Updating all references to github.com/apache/incubator-spark on spark website
From: Andy Konwinski <andykonwinski@gmail.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e0115fd3ca8507204f6a5918a
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0115fd3ca8507204f6a5918a
Content-Type: text/plain; charset=ISO-8859-1

Since http://github.com/apache/incubator-spark and any links underneath it
now return 404, I propose we do a global search and replace to change all
instances to remove "incubator-", including those in docs/0.8.0 docs/0.8.1
and docs/0.9.0.

I'm happy to do this.

Any discussion before I do?

Andy

--089e0115fd3ca8507204f6a5918a--

From dev-return-7303-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 10 19:56:44 2014
Return-Path: <dev-return-7303-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 04D6B11D03
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Apr 2014 19:56:44 +0000 (UTC)
Received: (qmail 5786 invoked by uid 500); 10 Apr 2014 19:56:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5689 invoked by uid 500); 10 Apr 2014 19:56:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5679 invoked by uid 99); 10 Apr 2014 19:56:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 19:56:37 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 19:56:33 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <sandeep@techaddict.me>)
	id 1WYL52-0005P1-Lg
	for dev@spark.incubator.apache.org; Thu, 10 Apr 2014 12:56:12 -0700
Date: Thu, 10 Apr 2014 12:56:12 -0700 (PDT)
From: techaddict <sandeep@techaddict.me>
To: dev@spark.incubator.apache.org
Message-ID: <1397159772542-6288.post@n3.nabble.com>
Subject: org.apache.spark.util.Vector is deprecated what next ?
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

org.apache.spark.util.Vector is deprecated so what should be done to use say
if want to create a vector with zeros, def zeros(length: Int) in util.Vector
using new mllib.linalg.Vector ?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/org-apache-spark-util-Vector-is-deprecated-what-next-tp6288.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7304-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 10 20:13:08 2014
Return-Path: <dev-return-7304-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6E08F11DA2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Apr 2014 20:13:08 +0000 (UTC)
Received: (qmail 40710 invoked by uid 500); 10 Apr 2014 20:13:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40670 invoked by uid 500); 10 Apr 2014 20:13:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40645 invoked by uid 99); 10 Apr 2014 20:13:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 20:13:05 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.171 as permitted sender)
Received: from [209.85.214.171] (HELO mail-ob0-f171.google.com) (209.85.214.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 20:13:01 +0000
Received: by mail-ob0-f171.google.com with SMTP id wn1so4986362obc.2
        for <dev@spark.incubator.apache.org>; Thu, 10 Apr 2014 13:12:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=hyPcXDK7InieWZU+v35tTFUwlcWjdITIeSG9KhM8OFY=;
        b=GOgRGy2ng3i+oNqTtFmWinC1G+KNuwvegSRhxjaoHhe38YSkmVeKdsKjGxFqBWGLQs
         4ibcuCYoK1JP1k++2NZk4ICj05TZ/NgMB6SkYpCFcySRGBBTpPX8Xn3A+5uqeWN3GF3R
         ej/F4Fg6JQiUDkj9OY6+Yyv9XD6AwFyRHTyX3xU+gAVIAJ9LD8vinNycU9VmANu/0Wue
         Z8U+arwRnX1xcYfefWaQ9Zo331uXVIlYqoGoNEzeRT973T/qZimOy+e1L3XegV1sZEtN
         IPxmYI7PpCNavGRuqFQBr5cOFyuY34Ja/YVY6x3SobozVlYJbYevcHswHyaC1sKw0OWr
         lvFw==
MIME-Version: 1.0
X-Received: by 10.60.77.35 with SMTP id p3mr15629910oew.46.1397160761285; Thu,
 10 Apr 2014 13:12:41 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Thu, 10 Apr 2014 13:12:41 -0700 (PDT)
In-Reply-To: <1397159772542-6288.post@n3.nabble.com>
References: <1397159772542-6288.post@n3.nabble.com>
Date: Thu, 10 Apr 2014 13:12:41 -0700
Message-ID: <CABPQxssec_sJDsn3mmXdH_4Z7cuY4x9ZYBmX5cE2OJyDgO0=4g@mail.gmail.com>
Subject: Re: org.apache.spark.util.Vector is deprecated what next ?
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=047d7b33d244dee6f804f6b5d659
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b33d244dee6f804f6b5d659
Content-Type: text/plain; charset=ISO-8859-1

You'll need to use the associated functionality in Breeze and then create a
dense vector from a Breeze vector. I have a JIRA for us to update the
examples for 1.0...  I'm hoping Xiangrui can take a look at it.

https://issues.apache.org/jira/browse/SPARK-1464

https://github.com/scalanlp/breeze/wiki/Breeze-Linear-Algebra



On Thu, Apr 10, 2014 at 12:56 PM, techaddict <sandeep@techaddict.me> wrote:

> org.apache.spark.util.Vector is deprecated so what should be done to use
> say
> if want to create a vector with zeros, def zeros(length: Int) in
> util.Vector
> using new mllib.linalg.Vector ?
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/org-apache-spark-util-Vector-is-deprecated-what-next-tp6288.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--047d7b33d244dee6f804f6b5d659--

From dev-return-7305-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 10 20:13:10 2014
Return-Path: <dev-return-7305-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 58AB211DA3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Apr 2014 20:13:10 +0000 (UTC)
Received: (qmail 41351 invoked by uid 500); 10 Apr 2014 20:13:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40673 invoked by uid 500); 10 Apr 2014 20:13:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40646 invoked by uid 99); 10 Apr 2014 20:13:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 20:13:05 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.54 as permitted sender)
Received: from [209.85.219.54] (HELO mail-oa0-f54.google.com) (209.85.219.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 20:13:01 +0000
Received: by mail-oa0-f54.google.com with SMTP id n16so5065435oag.41
        for <dev@spark.apache.org>; Thu, 10 Apr 2014 13:12:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=hyPcXDK7InieWZU+v35tTFUwlcWjdITIeSG9KhM8OFY=;
        b=GOgRGy2ng3i+oNqTtFmWinC1G+KNuwvegSRhxjaoHhe38YSkmVeKdsKjGxFqBWGLQs
         4ibcuCYoK1JP1k++2NZk4ICj05TZ/NgMB6SkYpCFcySRGBBTpPX8Xn3A+5uqeWN3GF3R
         ej/F4Fg6JQiUDkj9OY6+Yyv9XD6AwFyRHTyX3xU+gAVIAJ9LD8vinNycU9VmANu/0Wue
         Z8U+arwRnX1xcYfefWaQ9Zo331uXVIlYqoGoNEzeRT973T/qZimOy+e1L3XegV1sZEtN
         IPxmYI7PpCNavGRuqFQBr5cOFyuY34Ja/YVY6x3SobozVlYJbYevcHswHyaC1sKw0OWr
         lvFw==
MIME-Version: 1.0
X-Received: by 10.60.77.35 with SMTP id p3mr15629910oew.46.1397160761285; Thu,
 10 Apr 2014 13:12:41 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Thu, 10 Apr 2014 13:12:41 -0700 (PDT)
In-Reply-To: <1397159772542-6288.post@n3.nabble.com>
References: <1397159772542-6288.post@n3.nabble.com>
Date: Thu, 10 Apr 2014 13:12:41 -0700
Message-ID: <CABPQxssec_sJDsn3mmXdH_4Z7cuY4x9ZYBmX5cE2OJyDgO0=4g@mail.gmail.com>
Subject: Re: org.apache.spark.util.Vector is deprecated what next ?
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=047d7b33d244dee6f804f6b5d659
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b33d244dee6f804f6b5d659
Content-Type: text/plain; charset=ISO-8859-1

You'll need to use the associated functionality in Breeze and then create a
dense vector from a Breeze vector. I have a JIRA for us to update the
examples for 1.0...  I'm hoping Xiangrui can take a look at it.

https://issues.apache.org/jira/browse/SPARK-1464

https://github.com/scalanlp/breeze/wiki/Breeze-Linear-Algebra



On Thu, Apr 10, 2014 at 12:56 PM, techaddict <sandeep@techaddict.me> wrote:

> org.apache.spark.util.Vector is deprecated so what should be done to use
> say
> if want to create a vector with zeros, def zeros(length: Int) in
> util.Vector
> using new mllib.linalg.Vector ?
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/org-apache-spark-util-Vector-is-deprecated-what-next-tp6288.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--047d7b33d244dee6f804f6b5d659--

From dev-return-7306-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 10 20:48:56 2014
Return-Path: <dev-return-7306-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7557211EF3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Apr 2014 20:48:56 +0000 (UTC)
Received: (qmail 17430 invoked by uid 500); 10 Apr 2014 20:48:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17382 invoked by uid 500); 10 Apr 2014 20:48:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17346 invoked by uid 99); 10 Apr 2014 20:48:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 20:48:50 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ignacio.zendejas.cs@gmail.com designates 209.85.160.48 as permitted sender)
Received: from [209.85.160.48] (HELO mail-pb0-f48.google.com) (209.85.160.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 20:48:46 +0000
Received: by mail-pb0-f48.google.com with SMTP id md12so4434200pbc.21
        for <dev@spark.apache.org>; Thu, 10 Apr 2014 13:48:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=QhoErUqKL40v/yIpnIU8q72ZVj8Xx+iGUU9ekNjO6nw=;
        b=PXTpMyxoBsa038mNAr7BQXzCSEHjoMugkPicd1o4j8G3cjaMRehZ6cAr8T7c/qLvA+
         K4WVRlWruD4mS73+YD//vfAMQz0sg2Osr9OyUBeDYvOuMDLOlgF97u9LkTzAnk1glRuC
         SOEMimOiEBJJ834RlXj6BCmDxJV1t7tpFhogfPIF2FE3su0Kgiy7+zDeYOcibrJvPYd3
         /kZNxed2KQUcSC9ZZBqXrWVU1u5Kxb5RlEFdVpbvwMRItrIqAzr4Xtp0lGeUfvE1iwtv
         QxReCnz4LC/zh7qrV9wqKuf+rYK3iSZLwHCfSKnb9cjH+hM5peMXzmiBO631AGFbactk
         wvkw==
MIME-Version: 1.0
X-Received: by 10.69.31.235 with SMTP id kp11mr22443226pbd.33.1397162903818;
 Thu, 10 Apr 2014 13:48:23 -0700 (PDT)
Received: by 10.70.51.225 with HTTP; Thu, 10 Apr 2014 13:48:23 -0700 (PDT)
Date: Thu, 10 Apr 2014 13:48:23 -0700
Message-ID: <CANJrAvCKOfu7cB5mJT0RQnM9FxVcaVLsvOYkE7JUWMdKgwO9UQ@mail.gmail.com>
Subject: minor optimizations to get my feet wet
From: Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134caaa9355f504f6b65680
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134caaa9355f504f6b65680
Content-Type: text/plain; charset=ISO-8859-1

Hi, all -

First off, I want to say that I love spark and am very excited about
MLBase. I'd love to contribute now that I have some time, but before I do
that I'd like to familiarize myself with the process.

In looking for a few projects and settling on one which I'll discuss in
another thread, I found some very minor optimizations I could contribute,
again, as part of this first step.

Before I initiate a PR, I've gone ahead and tested style, ran tests, etc
per the instructions, but I'd still like to have someone quickly glance
over it and ensure that these are JIRA worthy.

Commit:
https://github.com/izendejas/spark/commit/81065aed9987c1b08cd5784b7a6153e26f3f7402

To summarize:

* I got rid of some SeqLike.reverse calls when sorting by descending order
* replaced slice(1, length) calls with the much safer (avoids IOOBEs) and
more readable .tail calls
* used a foldleft to avoid using mutable variables in NaiveBayes code

This last one is meant to understand what's valued more between idiomatic
Scala development or readability. I'm personally a fan of foldLefts where
applicable, but do think they're a bit less readable.

Thanks,
Ignacio

--001a1134caaa9355f504f6b65680--

From dev-return-7307-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 10 21:11:35 2014
Return-Path: <dev-return-7307-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1842E1001E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Apr 2014 21:11:35 +0000 (UTC)
Received: (qmail 93586 invoked by uid 500); 10 Apr 2014 21:11:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93187 invoked by uid 500); 10 Apr 2014 21:11:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93172 invoked by uid 99); 10 Apr 2014 21:11:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 21:11:24 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.170] (HELO mail-qc0-f170.google.com) (209.85.216.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 21:11:19 +0000
Received: by mail-qc0-f170.google.com with SMTP id x13so5159276qcv.1
        for <dev@spark.apache.org>; Thu, 10 Apr 2014 14:10:57 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=OHVHF6u/V21YCJroVWZGWuE+xlmZMnxw6yHRG0Zsi8w=;
        b=TaWGFA6VLsO/4hHxt+rEmOEWDKEZjPLGA5eL5O4FJnSkfb54fWeNT4Bav4OJX1EE4b
         m79oDbsc3HkEAyw7RWePxJ0XuMi8LSN0Nul/0owLKU4nwnYODHsMihpA4s40C2pEm/9M
         f0IeLJC/q7LIdRkW8z5yeiF2K8S34/i7a05XmBRr1BL2oJYdUjsG4DsaJ8L4ztDWP4mp
         u69qhXHoy1gJwHUFpoNZ66Axi5rO4nRWrlW3AFkHwgCDw6b9LUT6T92hxcjkDFtcr+bx
         ne2NunlatM+qM9Bo5OQiOvRjIgVBufz5KL5TjAkw6VRdiWJ0Nut+SR/SSBC3D2btKXt1
         Wj6g==
X-Gm-Message-State: ALoCoQn4MJq/2N67AISjQ4d4nc5hcBMWh6/k/hWZjIJGm922lCj6kkttuxgEzFYIoWNjZIf4WPjE
MIME-Version: 1.0
X-Received: by 10.224.20.72 with SMTP id e8mr23641995qab.86.1397164257079;
 Thu, 10 Apr 2014 14:10:57 -0700 (PDT)
Received: by 10.96.126.1 with HTTP; Thu, 10 Apr 2014 14:10:56 -0700 (PDT)
In-Reply-To: <CANJrAvCKOfu7cB5mJT0RQnM9FxVcaVLsvOYkE7JUWMdKgwO9UQ@mail.gmail.com>
References: <CANJrAvCKOfu7cB5mJT0RQnM9FxVcaVLsvOYkE7JUWMdKgwO9UQ@mail.gmail.com>
Date: Thu, 10 Apr 2014 14:10:56 -0700
Message-ID: <CAPh_B=bG=v84JCu5jt5HV3Poin-cWpUuRFL-kNDUi1ZRddxPKQ@mail.gmail.com>
Subject: Re: minor optimizations to get my feet wet
From: Reynold Xin <rxin@databricks.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c1bfea3ca3b004f6b6a727
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1bfea3ca3b004f6b6a727
Content-Type: text/plain; charset=ISO-8859-1

Thanks for contributing!

I think often unless the feature is gigantic, you can send a pull request
directly for discussion. One rule of thumb in the Spark code base is that
we typically prefer readability over conciseness, and thus we tend to avoid
using too much Scala magic or operator overloading.

In this specific case, do you know if using - instead of reverse improve
performance? I personally find it slightly awkward to use underscore right
after negation ...


The tail change looks good to me.

For foldLeft, I agree with you that the old way is more readable (although
less idiomatic scala).




On Thu, Apr 10, 2014 at 1:48 PM, Ignacio Zendejas <
ignacio.zendejas.cs@gmail.com> wrote:

> Hi, all -
>
> First off, I want to say that I love spark and am very excited about
> MLBase. I'd love to contribute now that I have some time, but before I do
> that I'd like to familiarize myself with the process.
>
> In looking for a few projects and settling on one which I'll discuss in
> another thread, I found some very minor optimizations I could contribute,
> again, as part of this first step.
>
> Before I initiate a PR, I've gone ahead and tested style, ran tests, etc
> per the instructions, but I'd still like to have someone quickly glance
> over it and ensure that these are JIRA worthy.
>
> Commit:
>
> https://github.com/izendejas/spark/commit/81065aed9987c1b08cd5784b7a6153e26f3f7402
>
> To summarize:
>
> * I got rid of some SeqLike.reverse calls when sorting by descending order
> * replaced slice(1, length) calls with the much safer (avoids IOOBEs) and
> more readable .tail calls
> * used a foldleft to avoid using mutable variables in NaiveBayes code
>
> This last one is meant to understand what's valued more between idiomatic
> Scala development or readability. I'm personally a fan of foldLefts where
> applicable, but do think they're a bit less readable.
>
> Thanks,
> Ignacio
>

--001a11c1bfea3ca3b004f6b6a727--

From dev-return-7308-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 10 21:14:43 2014
Return-Path: <dev-return-7308-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8CB2A10038
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Apr 2014 21:14:43 +0000 (UTC)
Received: (qmail 99809 invoked by uid 500); 10 Apr 2014 21:14:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99766 invoked by uid 500); 10 Apr 2014 21:14:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99657 invoked by uid 99); 10 Apr 2014 21:14:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 21:14:38 +0000
X-ASF-Spam-Status: No, hits=-1.0 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.82 as permitted sender)
Received: from [171.67.219.82] (HELO smtp.stanford.edu) (171.67.219.82)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 21:14:34 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 58F6E342102
	for <dev@spark.incubator.apache.org>; Thu, 10 Apr 2014 14:14:10 -0700 (PDT)
Received: from mail-qg0-f54.google.com (mail-qg0-f54.google.com [209.85.192.54])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 07F6B34239C
	for <dev@spark.incubator.apache.org>; Thu, 10 Apr 2014 14:14:09 -0700 (PDT)
Received: by mail-qg0-f54.google.com with SMTP id a108so4551580qge.13
        for <dev@spark.incubator.apache.org>; Thu, 10 Apr 2014 14:14:07 -0700 (PDT)
X-Gm-Message-State: ALoCoQn+KV90l1Kt3T2MfZk94INqvSV7JQha5hlIGKro5cJFZkVCNem8Q6nKGC9+4A8AFlAXlFVv
MIME-Version: 1.0
X-Received: by 10.224.160.206 with SMTP id o14mr14859521qax.44.1397164447853;
 Thu, 10 Apr 2014 14:14:07 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Thu, 10 Apr 2014 14:14:07 -0700 (PDT)
In-Reply-To: <CABPQxssec_sJDsn3mmXdH_4Z7cuY4x9ZYBmX5cE2OJyDgO0=4g@mail.gmail.com>
References: <1397159772542-6288.post@n3.nabble.com>
	<CABPQxssec_sJDsn3mmXdH_4Z7cuY4x9ZYBmX5cE2OJyDgO0=4g@mail.gmail.com>
Date: Thu, 10 Apr 2014 14:14:07 -0700
Message-ID: <CAEYYnxZ+wGfmRn4PfUBCc29ggxe-_qjra2Gh797G5cm3ccV-+Q@mail.gmail.com>
Subject: Re: org.apache.spark.util.Vector is deprecated what next ?
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

You can construct the Breeze vector by

    val breezeVector = breeze.linalg.DenseVector.zeros[Double](length)

If you want to convert to mllib vector, you can do

    val mllibVector = Vectors.fromBreeze(breezeVector)

If you want to convert back to breeze vector,

    val newBreezeVector = mllibVector.toBreeze

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Thu, Apr 10, 2014 at 1:12 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> You'll need to use the associated functionality in Breeze and then create a
> dense vector from a Breeze vector. I have a JIRA for us to update the
> examples for 1.0...  I'm hoping Xiangrui can take a look at it.
>
> https://issues.apache.org/jira/browse/SPARK-1464
>
> https://github.com/scalanlp/breeze/wiki/Breeze-Linear-Algebra
>
>
>
> On Thu, Apr 10, 2014 at 12:56 PM, techaddict <sandeep@techaddict.me> wrote:
>
>> org.apache.spark.util.Vector is deprecated so what should be done to use
>> say
>> if want to create a vector with zeros, def zeros(length: Int) in
>> util.Vector
>> using new mllib.linalg.Vector ?
>>
>>
>>
>> --
>> View this message in context:
>> http://apache-spark-developers-list.1001551.n3.nabble.com/org-apache-spark-util-Vector-is-deprecated-what-next-tp6288.html
>> Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com.
>>

From dev-return-7309-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 10 21:14:44 2014
Return-Path: <dev-return-7309-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8E00410039
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Apr 2014 21:14:43 +0000 (UTC)
Received: (qmail 537 invoked by uid 500); 10 Apr 2014 21:14:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99771 invoked by uid 500); 10 Apr 2014 21:14:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99658 invoked by uid 99); 10 Apr 2014 21:14:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 21:14:38 +0000
X-ASF-Spam-Status: No, hits=-1.0 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.82 as permitted sender)
Received: from [171.67.219.82] (HELO smtp.stanford.edu) (171.67.219.82)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 21:14:34 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 4B9B43428C6
	for <dev@spark.apache.org>; Thu, 10 Apr 2014 14:14:10 -0700 (PDT)
Received: from mail-qa0-f47.google.com (mail-qa0-f47.google.com [209.85.216.47])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id D8C393428B1
	for <dev@spark.apache.org>; Thu, 10 Apr 2014 14:14:09 -0700 (PDT)
Received: by mail-qa0-f47.google.com with SMTP id m5so2490696qaj.34
        for <dev@spark.apache.org>; Thu, 10 Apr 2014 14:14:07 -0700 (PDT)
X-Gm-Message-State: ALoCoQmOJNc0TR0UPQz7pmKLNvpxocOz3Y/ZEruvQTKGurv2PtKzKHp9IpGeuW+1OfDfR2PaxVBl
MIME-Version: 1.0
X-Received: by 10.224.160.206 with SMTP id o14mr14859521qax.44.1397164447853;
 Thu, 10 Apr 2014 14:14:07 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Thu, 10 Apr 2014 14:14:07 -0700 (PDT)
In-Reply-To: <CABPQxssec_sJDsn3mmXdH_4Z7cuY4x9ZYBmX5cE2OJyDgO0=4g@mail.gmail.com>
References: <1397159772542-6288.post@n3.nabble.com>
	<CABPQxssec_sJDsn3mmXdH_4Z7cuY4x9ZYBmX5cE2OJyDgO0=4g@mail.gmail.com>
Date: Thu, 10 Apr 2014 14:14:07 -0700
Message-ID: <CAEYYnxZ+wGfmRn4PfUBCc29ggxe-_qjra2Gh797G5cm3ccV-+Q@mail.gmail.com>
Subject: Re: org.apache.spark.util.Vector is deprecated what next ?
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

You can construct the Breeze vector by

    val breezeVector = breeze.linalg.DenseVector.zeros[Double](length)

If you want to convert to mllib vector, you can do

    val mllibVector = Vectors.fromBreeze(breezeVector)

If you want to convert back to breeze vector,

    val newBreezeVector = mllibVector.toBreeze

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Thu, Apr 10, 2014 at 1:12 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> You'll need to use the associated functionality in Breeze and then create a
> dense vector from a Breeze vector. I have a JIRA for us to update the
> examples for 1.0...  I'm hoping Xiangrui can take a look at it.
>
> https://issues.apache.org/jira/browse/SPARK-1464
>
> https://github.com/scalanlp/breeze/wiki/Breeze-Linear-Algebra
>
>
>
> On Thu, Apr 10, 2014 at 12:56 PM, techaddict <sandeep@techaddict.me> wrote:
>
>> org.apache.spark.util.Vector is deprecated so what should be done to use
>> say
>> if want to create a vector with zeros, def zeros(length: Int) in
>> util.Vector
>> using new mllib.linalg.Vector ?
>>
>>
>>
>> --
>> View this message in context:
>> http://apache-spark-developers-list.1001551.n3.nabble.com/org-apache-spark-util-Vector-is-deprecated-what-next-tp6288.html
>> Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com.
>>

From dev-return-7310-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 10 21:18:53 2014
Return-Path: <dev-return-7310-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 20FA310064
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Apr 2014 21:18:53 +0000 (UTC)
Received: (qmail 11150 invoked by uid 500); 10 Apr 2014 21:18:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10809 invoked by uid 500); 10 Apr 2014 21:18:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10799 invoked by uid 99); 10 Apr 2014 21:18:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 21:18:49 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ignacio.zendejas.cs@gmail.com designates 209.85.192.181 as permitted sender)
Received: from [209.85.192.181] (HELO mail-pd0-f181.google.com) (209.85.192.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 21:18:44 +0000
Received: by mail-pd0-f181.google.com with SMTP id p10so4368322pdj.12
        for <dev@spark.apache.org>; Thu, 10 Apr 2014 14:18:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=MzHxPlWP8qoVuh5eKCXiE8p+gRZvJI4vfDzWTVWEqrU=;
        b=0JQifaHL4T026fq0o+rMZKvBJyOPqROSFkd+xhMSgBbugjyQnQQngPNblb7aiOiAxK
         pLhb+B0QIFusUQFn8cSe5GmZw0Z+fTUX4w+5oDG7UKks1t96KvsThKkRvHxbTBLKAQQR
         F5swg/lQPfcILCYQpFUrUSP+nYfSVQkDLRLA+Xh1wh5MloHrutgkYaG1we0lowdAkjgI
         b2zweCrfyprh0Q3XABXM3zFcNPlua6iAnJcyYKj1HQV21ht5K6kBbWQSWwzhDpiUZT5+
         5cLvxAuG5S3J508R/8T2GkPHH7wIuD/cvp8ZhDkQa9eCh5h++DuG1C3aMT58UPuZ6FdW
         doYg==
MIME-Version: 1.0
X-Received: by 10.66.102.39 with SMTP id fl7mr22299453pab.43.1397164702051;
 Thu, 10 Apr 2014 14:18:22 -0700 (PDT)
Received: by 10.70.51.225 with HTTP; Thu, 10 Apr 2014 14:18:21 -0700 (PDT)
Date: Thu, 10 Apr 2014 14:18:21 -0700
Message-ID: <CANJrAvBtDTMs9YHE=EPZ6_HR8m5QDHUjcAyK2vpVWYB2GOfcHA@mail.gmail.com>
Subject: feature selection and sparse vector support
From: Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bd9164ec2320f04f6b6c19d
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd9164ec2320f04f6b6c19d
Content-Type: text/plain; charset=ISO-8859-1

Hi, again -

As part of the next step, I'd like to make a more substantive contribution
and propose some initial work on feature selection, primarily as it relates
to text classification.

Specifically, I'd like to contribute very straightforward code to perform
information gain feature evaluation. Below's a good primer that shows that
Information Gain is a very good option in many cases. If successful, BNS
(introduced in the paper), would be another approach worth looking into as
it actually improves the f score with a smaller feature space.

http://machinelearning.wustl.edu/mlpapers/paper_files/Forman03.pdf

And here's my first cut:
https://github.com/izendejas/spark/commit/e5a0620838841c99865ffa4fb0d2b449751236a8

I don't like that I do two passes to compute the class priors and joint
distributions, so I'll look into using combineByKey as in the NaiveBayes
implementation.  Also, this is still untested code, but it gets my ideas
out there and think it'd be best to define a FeatureEval trait or whatnot
that helps with ranking and selecting.

I also realize the above methods are probably more suitable for MLI than
MLlib, but there doesn't seem to be much activity on the former.

Second, is there a plan to support sparse vector representations for
NaiveBayes. This will probably be more efficient in, for example, text
classification tasks with lots of features (consider the case where n-grams
with n > 1 are used).

And on a related note, MLUtils.loadLabeledData doesn't support loading
sparse data. Any plans here to do so? There also doesn't seem to be a
defined file format for MLlib. Has there been any consideration to support
multiple standard formats, rather than defining one: eg, csv, tsv, Weka's
arff, etc?

Thanks for your time,
Ignacio

--047d7bd9164ec2320f04f6b6c19d--

From dev-return-7311-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 10 22:13:21 2014
Return-Path: <dev-return-7311-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D09F8102E6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Apr 2014 22:13:21 +0000 (UTC)
Received: (qmail 58559 invoked by uid 500); 10 Apr 2014 22:13:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58518 invoked by uid 500); 10 Apr 2014 22:13:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58508 invoked by uid 99); 10 Apr 2014 22:13:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 22:13:18 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of henry.saputra@gmail.com designates 209.85.212.180 as permitted sender)
Received: from [209.85.212.180] (HELO mail-wi0-f180.google.com) (209.85.212.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 22:13:14 +0000
Received: by mail-wi0-f180.google.com with SMTP id q5so92777wiv.7
        for <dev@spark.apache.org>; Thu, 10 Apr 2014 15:12:52 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=qj5X73WudBjrliVSURSYKieuzObIoCLlyU+cgfnpe60=;
        b=nAteqP+o0W9iBldJNFAkBEwdMXUUFze16abW2E1lnDMCMUT+WaOnxNXnJ22XwE4/D2
         K+Dwo3t5PJIfQd3uGS6gIGGTgHYmS5B8R/82hYa68dZHcNGRn1PNjI0NJ/XXm+t8y1xE
         ZOchpVwMrgmgetJms3llPiXHHm+oHbArJdeCYnyX3y9O81ZSLombot53/VBE46zejcqf
         i4edTUbZ88gasIWlOqgneC8b93we8B+N6rqAR7Zeg2p321gvqjsuCb8TyyhBntwzfwLC
         f8eLfj3msi1YFbQdvixOFWex5p/MD+8HqQgBiE5wTuR8usnZ5wrhXcKs7CfSoiGGEEQE
         /F1Q==
MIME-Version: 1.0
X-Received: by 10.180.89.211 with SMTP id bq19mr25985wib.58.1397167972611;
 Thu, 10 Apr 2014 15:12:52 -0700 (PDT)
Received: by 10.216.165.71 with HTTP; Thu, 10 Apr 2014 15:12:52 -0700 (PDT)
In-Reply-To: <CANJrAvCKOfu7cB5mJT0RQnM9FxVcaVLsvOYkE7JUWMdKgwO9UQ@mail.gmail.com>
References: <CANJrAvCKOfu7cB5mJT0RQnM9FxVcaVLsvOYkE7JUWMdKgwO9UQ@mail.gmail.com>
Date: Thu, 10 Apr 2014 15:12:52 -0700
Message-ID: <CALuGr6b9HJMxJ9BqHh=GPHcm1rFa248UFuLQPW0LYGaSYOY35g@mail.gmail.com>
Subject: Re: minor optimizations to get my feet wet
From: Henry Saputra <henry.saputra@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

HI Ignacio,

Thank you for your contribution.

Just a friendly reminder, in case you have not contributed to Apache
Software Foundation projects before please submit ASF ICLA form [1] or
if you are sponsored by your company also ask the company to send CCLA
[2] to clear the intellectual property for your contributions.

You can ignore "preferred Apache id" section for now.


Thank you,

Henry Saputra

[1] https://www.apache.org/licenses/icla.txt
[2] http://www.apache.org/licenses/cla-corporate.txt


On Thu, Apr 10, 2014 at 1:48 PM, Ignacio Zendejas
<ignacio.zendejas.cs@gmail.com> wrote:
> Hi, all -
>
> First off, I want to say that I love spark and am very excited about
> MLBase. I'd love to contribute now that I have some time, but before I do
> that I'd like to familiarize myself with the process.
>
> In looking for a few projects and settling on one which I'll discuss in
> another thread, I found some very minor optimizations I could contribute,
> again, as part of this first step.
>
> Before I initiate a PR, I've gone ahead and tested style, ran tests, etc
> per the instructions, but I'd still like to have someone quickly glance
> over it and ensure that these are JIRA worthy.
>
> Commit:
> https://github.com/izendejas/spark/commit/81065aed9987c1b08cd5784b7a6153e26f3f7402
>
> To summarize:
>
> * I got rid of some SeqLike.reverse calls when sorting by descending order
> * replaced slice(1, length) calls with the much safer (avoids IOOBEs) and
> more readable .tail calls
> * used a foldleft to avoid using mutable variables in NaiveBayes code
>
> This last one is meant to understand what's valued more between idiomatic
> Scala development or readability. I'm personally a fan of foldLefts where
> applicable, but do think they're a bit less readable.
>
> Thanks,
> Ignacio

From dev-return-7312-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 10 22:18:11 2014
Return-Path: <dev-return-7312-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7617E102F6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Apr 2014 22:18:11 +0000 (UTC)
Received: (qmail 62223 invoked by uid 500); 10 Apr 2014 22:18:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61903 invoked by uid 500); 10 Apr 2014 22:18:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61895 invoked by uid 99); 10 Apr 2014 22:18:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 22:18:09 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ignacio.zendejas.cs@gmail.com designates 209.85.220.45 as permitted sender)
Received: from [209.85.220.45] (HELO mail-pa0-f45.google.com) (209.85.220.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 22:18:05 +0000
Received: by mail-pa0-f45.google.com with SMTP id kl14so4542693pab.32
        for <dev@spark.apache.org>; Thu, 10 Apr 2014 15:17:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=zEZR2CVwCgkhSvjbnYCqbeFbYk8Tzg4gu2uI0t/0QO0=;
        b=WEYhPHhMmdOSzFc/1V5Z+DKmrQhudQh2vFzK9HQYTadJPSRkWLC6UopKC8cwZlPcK3
         /BX+Mv1qhj0rO+rFakVs8nlCJxguCm5jF1tw9fE/oOxU0mskKUfwDVh0PN4IZgcUX5cd
         WN2e0tUqItBy00mX4DkntdfF9fUqjqYwTbpx4oqPv4jZBjfSJLSUB+UTfNw84UbC23S3
         4Oj/D6hFL/6frRmMKAM7yWmcTVFTNnwlPpqCF5xl2DH6Hb/1HZukDew/jZk2NoxfKJHH
         S/m9okvyJisUm8H9e1LZ6+x4jVRq2Lb8hyggbMc1kEv49hxpwRxT9i/wSUyjkeemJlhn
         lNuA==
MIME-Version: 1.0
X-Received: by 10.66.122.72 with SMTP id lq8mr22732206pab.69.1397168264548;
 Thu, 10 Apr 2014 15:17:44 -0700 (PDT)
Received: by 10.70.51.225 with HTTP; Thu, 10 Apr 2014 15:17:44 -0700 (PDT)
In-Reply-To: <CAPh_B=bG=v84JCu5jt5HV3Poin-cWpUuRFL-kNDUi1ZRddxPKQ@mail.gmail.com>
References: <CANJrAvCKOfu7cB5mJT0RQnM9FxVcaVLsvOYkE7JUWMdKgwO9UQ@mail.gmail.com>
	<CAPh_B=bG=v84JCu5jt5HV3Poin-cWpUuRFL-kNDUi1ZRddxPKQ@mail.gmail.com>
Date: Thu, 10 Apr 2014 15:17:44 -0700
Message-ID: <CANJrAvBm=_CX7TsFrW-mHGWA+nC_YoD9b+3dnnWhuz2HWJ=DFQ@mail.gmail.com>
Subject: Re: minor optimizations to get my feet wet
From: Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b2e0b851995eb04f6b796cd
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b2e0b851995eb04f6b796cd
Content-Type: text/plain; charset=ISO-8859-1

I don't think there's a noticeable performance hit by the use of reverse in
those cases. It was a quick set of changes and it helped understand what
you look for. I didn't intend to nitpick, so I'll leave as is. I could have
used a scala.Ordering implicitly/explicitly also, but seems overkill and
don't want to necessarily start a discussion about what's best--unless one
of the admins deems this important.

I'll only keep the use of take and tail over using slice and switch over to
math.min where indicated.

This after I follow Henry's timely advice--thanks, Henry.

cheers.




On Thu, Apr 10, 2014 at 2:10 PM, Reynold Xin <rxin@databricks.com> wrote:

> Thanks for contributing!
>
> I think often unless the feature is gigantic, you can send a pull request
> directly for discussion. One rule of thumb in the Spark code base is that
> we typically prefer readability over conciseness, and thus we tend to avoid
> using too much Scala magic or operator overloading.
>
> In this specific case, do you know if using - instead of reverse improve
> performance? I personally find it slightly awkward to use underscore right
> after negation ...
>
>
> The tail change looks good to me.
>
> For foldLeft, I agree with you that the old way is more readable (although
> less idiomatic scala).
>
>
>
>
> On Thu, Apr 10, 2014 at 1:48 PM, Ignacio Zendejas <
> ignacio.zendejas.cs@gmail.com> wrote:
>
> > Hi, all -
> >
> > First off, I want to say that I love spark and am very excited about
> > MLBase. I'd love to contribute now that I have some time, but before I do
> > that I'd like to familiarize myself with the process.
> >
> > In looking for a few projects and settling on one which I'll discuss in
> > another thread, I found some very minor optimizations I could contribute,
> > again, as part of this first step.
> >
> > Before I initiate a PR, I've gone ahead and tested style, ran tests, etc
> > per the instructions, but I'd still like to have someone quickly glance
> > over it and ensure that these are JIRA worthy.
> >
> > Commit:
> >
> >
> https://github.com/izendejas/spark/commit/81065aed9987c1b08cd5784b7a6153e26f3f7402
> >
> > To summarize:
> >
> > * I got rid of some SeqLike.reverse calls when sorting by descending
> order
> > * replaced slice(1, length) calls with the much safer (avoids IOOBEs) and
> > more readable .tail calls
> > * used a foldleft to avoid using mutable variables in NaiveBayes code
> >
> > This last one is meant to understand what's valued more between idiomatic
> > Scala development or readability. I'm personally a fan of foldLefts where
> > applicable, but do think they're a bit less readable.
> >
> > Thanks,
> > Ignacio
> >
>

--047d7b2e0b851995eb04f6b796cd--

From dev-return-7313-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 10 23:10:06 2014
Return-Path: <dev-return-7313-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2177210510
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Apr 2014 23:10:06 +0000 (UTC)
Received: (qmail 34278 invoked by uid 500); 10 Apr 2014 23:10:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34202 invoked by uid 500); 10 Apr 2014 23:10:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34190 invoked by uid 99); 10 Apr 2014 23:10:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 23:10:04 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of henry.saputra@gmail.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Apr 2014 23:10:00 +0000
Received: by mail-wi0-f175.google.com with SMTP id cc10so142050wib.2
        for <dev@spark.apache.org>; Thu, 10 Apr 2014 16:09:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=Oj007PpktBYovVnRE0C4MxonMhvfmm/0kDm9AInHI10=;
        b=CH0hcSH49imJzQYiyceCqy8BooFviZpgLBIdPOg8eqG6goSWUCZaNWJ/27jqge4uI4
         7O8FS1GXtoRTPxa4CRoR0o/AK4SBxnT4s74DAe7qx2QI3eVQWAACrbGGsz5DDltckBtt
         bforHyd5NMtA0mEjG+n/YVX/GcQpGzdbhW9xBmwpOAfub+cueTMvXIVwMDDPG4BrKfIV
         Mu7sPQdGtraaOepWat6yoD5BBpgRcH4kSCjQmWinT6igL28TRooabsogvzeL4AP0yPWW
         d1qAR6q4cOgeeK33+j2RPfp1lhzL0E56BjqqoqD0hwS7butE7Wth/WOOZlQmKDZbItU+
         ywmA==
MIME-Version: 1.0
X-Received: by 10.180.149.143 with SMTP id ua15mr201011wib.36.1397171379678;
 Thu, 10 Apr 2014 16:09:39 -0700 (PDT)
Received: by 10.216.165.71 with HTTP; Thu, 10 Apr 2014 16:09:39 -0700 (PDT)
In-Reply-To: <CANJrAvBm=_CX7TsFrW-mHGWA+nC_YoD9b+3dnnWhuz2HWJ=DFQ@mail.gmail.com>
References: <CANJrAvCKOfu7cB5mJT0RQnM9FxVcaVLsvOYkE7JUWMdKgwO9UQ@mail.gmail.com>
	<CAPh_B=bG=v84JCu5jt5HV3Poin-cWpUuRFL-kNDUi1ZRddxPKQ@mail.gmail.com>
	<CANJrAvBm=_CX7TsFrW-mHGWA+nC_YoD9b+3dnnWhuz2HWJ=DFQ@mail.gmail.com>
Date: Thu, 10 Apr 2014 16:09:39 -0700
Message-ID: <CALuGr6bRoyj3KcSomh3fvo9omcz=m5ff5P_5eXrYnc9ZGZvvWA@mail.gmail.com>
Subject: Re: minor optimizations to get my feet wet
From: Henry Saputra <henry.saputra@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

You are welcome, thanks again for contributing =)

- Henry

On Thu, Apr 10, 2014 at 3:17 PM, Ignacio Zendejas
<ignacio.zendejas.cs@gmail.com> wrote:
> I don't think there's a noticeable performance hit by the use of reverse in
> those cases. It was a quick set of changes and it helped understand what
> you look for. I didn't intend to nitpick, so I'll leave as is. I could have
> used a scala.Ordering implicitly/explicitly also, but seems overkill and
> don't want to necessarily start a discussion about what's best--unless one
> of the admins deems this important.
>
> I'll only keep the use of take and tail over using slice and switch over to
> math.min where indicated.
>
> This after I follow Henry's timely advice--thanks, Henry.
>
> cheers.
>
>
>
>
> On Thu, Apr 10, 2014 at 2:10 PM, Reynold Xin <rxin@databricks.com> wrote:
>
>> Thanks for contributing!
>>
>> I think often unless the feature is gigantic, you can send a pull request
>> directly for discussion. One rule of thumb in the Spark code base is that
>> we typically prefer readability over conciseness, and thus we tend to avoid
>> using too much Scala magic or operator overloading.
>>
>> In this specific case, do you know if using - instead of reverse improve
>> performance? I personally find it slightly awkward to use underscore right
>> after negation ...
>>
>>
>> The tail change looks good to me.
>>
>> For foldLeft, I agree with you that the old way is more readable (although
>> less idiomatic scala).
>>
>>
>>
>>
>> On Thu, Apr 10, 2014 at 1:48 PM, Ignacio Zendejas <
>> ignacio.zendejas.cs@gmail.com> wrote:
>>
>> > Hi, all -
>> >
>> > First off, I want to say that I love spark and am very excited about
>> > MLBase. I'd love to contribute now that I have some time, but before I do
>> > that I'd like to familiarize myself with the process.
>> >
>> > In looking for a few projects and settling on one which I'll discuss in
>> > another thread, I found some very minor optimizations I could contribute,
>> > again, as part of this first step.
>> >
>> > Before I initiate a PR, I've gone ahead and tested style, ran tests, etc
>> > per the instructions, but I'd still like to have someone quickly glance
>> > over it and ensure that these are JIRA worthy.
>> >
>> > Commit:
>> >
>> >
>> https://github.com/izendejas/spark/commit/81065aed9987c1b08cd5784b7a6153e26f3f7402
>> >
>> > To summarize:
>> >
>> > * I got rid of some SeqLike.reverse calls when sorting by descending
>> order
>> > * replaced slice(1, length) calls with the much safer (avoids IOOBEs) and
>> > more readable .tail calls
>> > * used a foldleft to avoid using mutable variables in NaiveBayes code
>> >
>> > This last one is meant to understand what's valued more between idiomatic
>> > Scala development or readability. I'm personally a fan of foldLefts where
>> > applicable, but do think they're a bit less readable.
>> >
>> > Thanks,
>> > Ignacio
>> >
>>

From dev-return-7314-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 11 00:17:07 2014
Return-Path: <dev-return-7314-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 45E00107F1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Apr 2014 00:17:07 +0000 (UTC)
Received: (qmail 82234 invoked by uid 500); 11 Apr 2014 00:17:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82103 invoked by uid 500); 11 Apr 2014 00:17:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81719 invoked by uid 99); 11 Apr 2014 00:17:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 00:17:04 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vanzin@cloudera.com designates 209.85.192.53 as permitted sender)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 00:16:59 +0000
Received: by mail-qg0-f53.google.com with SMTP id f51so3888705qge.26
        for <dev@spark.apache.org>; Thu, 10 Apr 2014 17:16:37 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=XjhORdirXQSm/HwJPVxWjxrybQ0EuFISwrfFyYEvj2Q=;
        b=OM+lNJ6rRIG1dVY0pUejPWWVVU/TpaVjzwye9EnZYI7KWtGcwJMo46J2fOFg+6MM41
         s8zrAAzrABIAl3DyPLrTC+pED9iLoh+m0FYgU/m90UBnOgEagGcTtZQeq/9r1GiRNprt
         3KCxGoHFgO6dlg249j2C7KdiawyTvPiXVX6Kb917BKd5AjSw8gJ/HPLsme+GBS7mIOD9
         f+JtnnM7K6mBhBrrynPe0UO+77ueWpKoIOhhWV1Ik96BrV9AdzF/spc4b+gTn+bBDnSZ
         eQRCzrsVhVwvV3cclCQI3jaWTJJ94al0nL85LfbnDjyOMuNwa8e7gkC/S+kZMafeLuXk
         I3Xg==
X-Gm-Message-State: ALoCoQnXtj9L0wqtJ0bxlZJDBQorkqo8p78cS/td4RfWDEd2aPIIw2YAkVqqKnxecGJXops7T6fi
MIME-Version: 1.0
X-Received: by 10.140.40.47 with SMTP id w44mr42116qgw.112.1397175396967; Thu,
 10 Apr 2014 17:16:36 -0700 (PDT)
Received: by 10.229.211.5 with HTTP; Thu, 10 Apr 2014 17:16:36 -0700 (PDT)
Date: Thu, 10 Apr 2014 17:16:36 -0700
Message-ID: <CAAOnQ7vvpbuPHgpyf139rYS=m-AVgNpE0FQovRBc0JZgSmOHJg@mail.gmail.com>
Subject: RFC: varargs in Logging.scala?
From: Marcelo Vanzin <vanzin@cloudera.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hey there,

While going through the try to get the hang of things, I've noticed
several different styles of logging. They all have some downside
(readability being one of them in certain cases), but all of the
suffer from the fact that the log message needs to be built even
though it might not be used.

I spent some time trying to add varargs support to Logging.scala (also
to learn more about Scala itself), and came up with this:
https://github.com/vanzin/spark/commit/a15c284d4aac3d645b13c0ef157787ba014840e4

The change may look large, but the only interesting changes are in
Logging.scala, I promise.

What do you guys think of this approach?

It should, at worst, be just as fast (or slow) as before for the
majority of cases (i.e., any case where variables were used in the log
message). Personally, I think it reads better.

It might be possible to have something similar using string
interpolation, but I'm not familiar enough with Scala yet to try my
hand at that. Also, I believe that would still require some kind of
formatting when you want to do calculations (e.g. turn a variable
holding milliseconds into seconds in the log message).

If people like it, I'll submit a proper pull request. I've run a few
things using this code, and also the tests (which caught a few type
mismatches in the format strings), and everything looks ok so far.

-- 
Marcelo

From dev-return-7315-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 11 00:47:07 2014
Return-Path: <dev-return-7315-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 375BE108CF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Apr 2014 00:47:07 +0000 (UTC)
Received: (qmail 24827 invoked by uid 500); 11 Apr 2014 00:47:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24729 invoked by uid 500); 11 Apr 2014 00:47:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24720 invoked by uid 99); 11 Apr 2014 00:47:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 00:47:05 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.51] (HELO mail-qg0-f51.google.com) (209.85.192.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 00:47:00 +0000
Received: by mail-qg0-f51.google.com with SMTP id q108so4802012qgd.10
        for <dev@spark.apache.org>; Thu, 10 Apr 2014 17:46:39 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=S1AzTrx7hal8T97hyHyl+DfNsUQrQyrO+x4M4BhXSs0=;
        b=C8q54tVO9NXlW7+SEJzAauIS1UDjFagW5LzHUFLw3tU+x4s1gPjZYg4KEdX1xN/lUa
         ygY6ed3Gs4IgX0fej5jjdnyNhOr+Hh/68uY/v8qNQ8QIuigEtL0b9JmSMOUNUu6owfo5
         Uuzfj4/K5jq8NID8Tyn3ThqGOf6WwEl2Fjod1t8lM3mt09kJpMVTLiVgstLFdfS+enlt
         W1J2omlaYUErqgaqSzg3+dFYaxurGx2x19THv6b491s9RsCdGrcUwrkK1wVe+j6BA/C5
         jwnf7a09AVREI6mYTvPz3I5p2gpvO0aAobGq9ygvGzgCxGcRPCFc0L3tEU0f0IetE9E4
         bIsQ==
X-Gm-Message-State: ALoCoQkTwQnJYRrM1zJNO87LOa7PbmjcdDfNppFQXuYhUUINcHMcyXnvdzpif2gkZ5VQ+R9oQihv
X-Received: by 10.140.51.14 with SMTP id t14mr23203105qga.50.1397177199540;
 Thu, 10 Apr 2014 17:46:39 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.125.5 with HTTP; Thu, 10 Apr 2014 17:46:19 -0700 (PDT)
In-Reply-To: <CAAOnQ7vvpbuPHgpyf139rYS=m-AVgNpE0FQovRBc0JZgSmOHJg@mail.gmail.com>
References: <CAAOnQ7vvpbuPHgpyf139rYS=m-AVgNpE0FQovRBc0JZgSmOHJg@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Thu, 10 Apr 2014 17:46:19 -0700
Message-ID: <CAAswR-4owXBP+orY4jR8zHdww5jTHgfZoMNrDx+sBdQdi9jDqw@mail.gmail.com>
Subject: Re: RFC: varargs in Logging.scala?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1135343eaae6cc04f6b9aa90
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1135343eaae6cc04f6b9aa90
Content-Type: text/plain; charset=ISO-8859-1

Hi Marcelo,

Thanks for bringing this up here, as this has been a topic of debate
recently.  Some thoughts below.

... all of the suffer from the fact that the log message needs to be built
> even
> though it might not be used.
>

This is not true of the current implementation (and this is actually why
Spark has a logging trait instead of just using a logger directly.)

If you look at the original function signatures:

protected def logDebug(msg: => String) ...


The => implies that we are passing the msg by name instead of by value.
Under the covers, scala is creating a closure that can be used to calculate
the log message, only if its actually required.  This does result is a
significant performance improvement, but still requires allocating an
object for the closure.  The bytecode is really something like this:

val logMessage = new Function0() { def call() =  "Log message" +
someExpensiveComputation() }
log.debug(logMessage)


In Catalyst and Spark SQL we are using the scala-logging package, which
uses macros to automatically rewrite all of your log statements.

You write: logger.debug(s"Log message $someExpensiveComputation")

You get:

if(logger.debugEnabled) {
  val logMsg = "Log message" + someExpensiveComputation()
  logger.debug(logMsg)
}

IMHO, this is the cleanest option (and is supported by Typesafe).  Based on
a micro-benchmark, it is also the fastest:

std logging: 19885.48ms
spark logging 914.408ms
scala logging 729.779ms

Once the dust settles from the 1.0 release, I'd be in favor of
standardizing on scala-logging.

Michael

--001a1135343eaae6cc04f6b9aa90--

From dev-return-7316-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 11 00:56:17 2014
Return-Path: <dev-return-7316-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B12BB10916
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Apr 2014 00:56:17 +0000 (UTC)
Received: (qmail 46207 invoked by uid 500); 11 Apr 2014 00:56:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46152 invoked by uid 500); 11 Apr 2014 00:56:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46140 invoked by uid 99); 11 Apr 2014 00:56:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 00:56:15 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.54] (HELO mail-qg0-f54.google.com) (209.85.192.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 00:56:09 +0000
Received: by mail-qg0-f54.google.com with SMTP id a108so4641901qge.41
        for <dev@spark.apache.org>; Thu, 10 Apr 2014 17:55:46 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=i4IPGLKeLQyVlbaikFOYXdui55uGXXuDvfWIKtISc6U=;
        b=mHSs7Q748LqUMA3rhjhY3UCGBEVAlqlFtimLhHrT3FNHi2Uc8z4KAQmhSAX+1fhGRL
         dMLZau3HxtjAwy7m0hdB55heeVfVyvr9WdSCeo8lV1IkTvoDLzwNX08CSaiuwzjLNXM/
         ANrsGrcSRoQTvvhJAaSDhcdcb/AFkuJjA2EBiKmwbiCumPUPiKJRvu5ToWCfzqRlN500
         TFqtvYIkrAO09HZy1HpSnmBozOT13m0aHj7TqPJCYSdcgDYkFErwHdNB33w/kLB6i9NA
         boC3cVfngwZoKuZNSZRrM5TA5thdRWdrPvZScvy6SbkwburRAPw6XL5VDGcxmjviyv7e
         U+Gg==
X-Gm-Message-State: ALoCoQnOFnoeuoOthGpyHfjTuMjCNhkVYoYI4DjBIqs7FskatIqagwnpbYhDoN8e2jnAgw0NZmuG
X-Received: by 10.224.131.67 with SMTP id w3mr24823370qas.32.1397177746857;
 Thu, 10 Apr 2014 17:55:46 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.125.5 with HTTP; Thu, 10 Apr 2014 17:55:24 -0700 (PDT)
In-Reply-To: <CAAswR-4owXBP+orY4jR8zHdww5jTHgfZoMNrDx+sBdQdi9jDqw@mail.gmail.com>
References: <CAAOnQ7vvpbuPHgpyf139rYS=m-AVgNpE0FQovRBc0JZgSmOHJg@mail.gmail.com>
 <CAAswR-4owXBP+orY4jR8zHdww5jTHgfZoMNrDx+sBdQdi9jDqw@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Thu, 10 Apr 2014 17:55:24 -0700
Message-ID: <CAAswR-5z0Hqomk56wh57akmqrjvA1dAmNv9qnJJ7hMs6r+vFwA@mail.gmail.com>
Subject: Re: RFC: varargs in Logging.scala?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2c2624a361804f6b9cb41
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2c2624a361804f6b9cb41
Content-Type: text/plain; charset=ISO-8859-1

BTW...

You can do calculations in string interpolation:
s"Time: ${timeMillis / 1000}s"

Or use format strings.
f"Float with two decimal places: $floatValue%.2f"

More info:
http://docs.scala-lang.org/overviews/core/string-interpolation.html


On Thu, Apr 10, 2014 at 5:46 PM, Michael Armbrust <michael@databricks.com>wrote:

> Hi Marcelo,
>
> Thanks for bringing this up here, as this has been a topic of debate
> recently.  Some thoughts below.
>
> ... all of the suffer from the fact that the log message needs to be built
>> even
>>
>> though it might not be used.
>>
>
> This is not true of the current implementation (and this is actually why
> Spark has a logging trait instead of just using a logger directly.)
>
> If you look at the original function signatures:
>
> protected def logDebug(msg: => String) ...
>
>
> The => implies that we are passing the msg by name instead of by value.
> Under the covers, scala is creating a closure that can be used to calculate
> the log message, only if its actually required.  This does result is a
> significant performance improvement, but still requires allocating an
> object for the closure.  The bytecode is really something like this:
>
> val logMessage = new Function0() { def call() =  "Log message" + someExpensiveComputation() }
> log.debug(logMessage)
>
>
> In Catalyst and Spark SQL we are using the scala-logging package, which
> uses macros to automatically rewrite all of your log statements.
>
> You write: logger.debug(s"Log message $someExpensiveComputation")
>
> You get:
>
> if(logger.debugEnabled) {
>   val logMsg = "Log message" + someExpensiveComputation()
>   logger.debug(logMsg)
> }
>
> IMHO, this is the cleanest option (and is supported by Typesafe).  Based
> on a micro-benchmark, it is also the fastest:
>
> std logging: 19885.48ms
> spark logging 914.408ms
> scala logging 729.779ms
>
> Once the dust settles from the 1.0 release, I'd be in favor of
> standardizing on scala-logging.
>
> Michael
>

--001a11c2c2624a361804f6b9cb41--

From dev-return-7317-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 11 01:58:03 2014
Return-Path: <dev-return-7317-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 494A410B57
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Apr 2014 01:58:03 +0000 (UTC)
Received: (qmail 53925 invoked by uid 500); 11 Apr 2014 01:57:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53815 invoked by uid 500); 11 Apr 2014 01:57:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53788 invoked by uid 99); 11 Apr 2014 01:57:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 01:57:54 +0000
X-ASF-Spam-Status: No, hits=-0.6 required=10.0
	tests=FROM_12LTRDOM,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.214.177] (HELO mail-ob0-f177.google.com) (209.85.214.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 01:57:49 +0000
Received: by mail-ob0-f177.google.com with SMTP id wo20so5106401obc.8
        for <dev@spark.apache.org>; Thu, 10 Apr 2014 18:57:28 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=anconafamily.com; s=google;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=agwEGRpP3dxH9GKCDhegjkxVkuaLZK/XgFD9817wtIo=;
        b=gPS0QYk8l0xdhXyJwZugd2OoRClLbf7Si4q646DgntrNK+wpLvgo/1pf3QgIcqtc1d
         +6Lmg78pNetINSpzcprnbyzEX15PHpbFArgnTxndzJ1weEPktE5whZAKNNHZfFYIMDDk
         I1JFL07me4znQu8/DjWI7IQCpc0JMkLryDOhY=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=agwEGRpP3dxH9GKCDhegjkxVkuaLZK/XgFD9817wtIo=;
        b=XlKgT7Wd9UMy2qXkd8ztJ84EynVHULjxbW+OB2b1djgP+qnlfBDDSPZIHPKFCTefH2
         eaRSROPqALhw4zHyIqomyfnG7sbVyNcWuAucDfxJgd+wD5XqJeWXpU/KZPjwTQRnXljH
         YPkOowM2a8cKZOf77uNrHTvbEDKP3ZbI1oBgEyussKWaqvh9X0/RlUHyNJVwcmNdt0rz
         MiJ2e8nvuP/UWHtRahgiBFajM1fJjn2ILw5bxsqW6uZ5MX+PkLuIbQ+BLYZi8ItE2GQW
         AjdRRN/w2D8EIJ1zKhgvi+MJs5BzknEl4xq6+/dq7OPHpvIL9JNs5+q9N+NvkEU1ZB8p
         J+KQ==
X-Gm-Message-State: ALoCoQkk2Ln0+bYiDHgQ4mQ4zA9EErmZPcEAZ2wogljOgdBGSvr7jmuAfg9NF5Qdz1q/YrnnNPv8
MIME-Version: 1.0
X-Received: by 10.182.250.200 with SMTP id ze8mr53000obc.72.1397181448183;
 Thu, 10 Apr 2014 18:57:28 -0700 (PDT)
Received: by 10.182.56.170 with HTTP; Thu, 10 Apr 2014 18:57:28 -0700 (PDT)
X-Originating-IP: [68.118.231.30]
Date: Thu, 10 Apr 2014 21:57:28 -0400
Message-ID: <CAKYY9AKJNFonyWw5UeqsnKvze10DauEHgq3nAEtm--yBUbxbCw@mail.gmail.com>
Subject: Building Spark AMI
From: Jim Ancona <jim@anconafamily.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Are there scripts to build the AMI used by the spark-ec2 script?

Alternatively, is there a place to download the AMI. I'm interested in
using it to deploy into an internal Openstack cloud.

Thanks,

Jim

From dev-return-7318-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 11 02:22:19 2014
Return-Path: <dev-return-7318-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 695A010BC2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Apr 2014 02:22:19 +0000 (UTC)
Received: (qmail 80884 invoked by uid 500); 11 Apr 2014 02:22:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80803 invoked by uid 500); 11 Apr 2014 02:22:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80793 invoked by uid 99); 11 Apr 2014 02:22:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 02:22:14 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=FROM_EXCESS_BASE64,HTML_MESSAGE,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of witgo@qq.com designates 58.250.132.207 as permitted sender)
Received: from [58.250.132.207] (HELO smtpbg344.qq.com) (58.250.132.207)
    by apache.org (qpsmtpd/0.29) with SMTP; Fri, 11 Apr 2014 02:22:09 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=qq.com; s=s201307;
	t=1397182853; bh=+VMDRb9bvk0Bs5tDohimgWitxXgwZhCH+yCjFyy+ieM=;
	h=X-QQ-FEAT:X-QQ-SSF:X-HAS-ATTACH:X-QQ-BUSINESS-ORIGIN:
	 X-Originating-IP:In-Reply-To:References:X-QQ-STYLE:X-QQ-mid:From:To:Subject:Mime-Version:Content-Type:Content-Transfer-Encoding:Date:
	 X-Priority:Message-ID:X-QQ-MIME:X-Mailer:X-QQ-Mailer:
	 X-QQ-ReplyHash:X-QQ-SENDSIZE;
	b=WqZCJvqbxByZ+I6XYh6Fp5yNCHSH1NyV4PCVpffLGdWXf5gzc/WZN8Alc9yo/DaK1
	 yl8amkfG3oIfy5zdMat+7G3vdv0Wxih2zuDIbsXjdo4txm6dQYn9kIMagZfp0QDFMq
	 i6KgJFz/4+LSug97bzlyVdQ+7UkP+Ij/UB59GWzA=
X-QQ-FEAT: UniauF+gXs0opT7seCbKwf/s6nib2QucybR8ptuzzToISB+0BIcB7MYDC0OlX
	lbUEDeRKWLnbk8WIT6sc2u1G4FJPZjjp78QPMWN83Wn9JyMqdD52H6G4W5+LfDbyLNoPXX/
	UtolKCGGnu3FEsvhNC3kVhekjK0J/BtkLmGe8aw=
X-QQ-SSF: 000000000000001000000000000000M
X-HAS-ATTACH: no
X-QQ-BUSINESS-ORIGIN: 2
X-Originating-IP: 219.142.170.212
In-Reply-To: <CAAOnQ7vvpbuPHgpyf139rYS=m-AVgNpE0FQovRBc0JZgSmOHJg@mail.gmail.com>
References: <CAAOnQ7vvpbuPHgpyf139rYS=m-AVgNpE0FQovRBc0JZgSmOHJg@mail.gmail.com>
X-QQ-STYLE: 
X-QQ-mid: webmail421t1397182851t1438438
From: "=?ISO-8859-1?B?d2l0Z28=?=" <witgo@qq.com>
To: "=?ISO-8859-1?B?ZGV2?=" <dev@spark.apache.org>
Subject: Re:RFC: varargs in Logging.scala?
Mime-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_53475183_08E8DB00_35F52C0E"
Content-Transfer-Encoding: 8Bit
Date: Fri, 11 Apr 2014 10:20:51 +0800
X-Priority: 3
Message-ID: <tencent_385BC0560D759D4F1307C078@qq.com>
X-QQ-MIME: TCMime 1.0 by Tencent
X-Mailer: QQMail 2.x
X-QQ-Mailer: QQMail 2.x
X-QQ-ReplyHash: 123276007
X-QQ-SENDSIZE: 520
X-QQ-Bgrelay: 1
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_53475183_08E8DB00_35F52C0E
Content-Type: text/plain;
	charset="ISO-8859-1"
Content-Transfer-Encoding: base64
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==

------=_NextPart_53475183_08E8DB00_35F52C0E--




From dev-return-7319-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 11 03:21:15 2014
Return-Path: <dev-return-7319-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E896710D1E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Apr 2014 03:21:15 +0000 (UTC)
Received: (qmail 48057 invoked by uid 500); 11 Apr 2014 03:21:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47559 invoked by uid 500); 11 Apr 2014 03:21:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47547 invoked by uid 99); 11 Apr 2014 03:21:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 03:21:10 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 209.85.216.181 as permitted sender)
Received: from [209.85.216.181] (HELO mail-qc0-f181.google.com) (209.85.216.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 03:21:05 +0000
Received: by mail-qc0-f181.google.com with SMTP id x3so5412026qcv.12
        for <dev@spark.apache.org>; Thu, 10 Apr 2014 20:20:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=stqch5h0AcsRqQzE/yzrv9Vx6zmRTPyGHQwz4zS9TCc=;
        b=FOz1o+FVp9B1+uymcUht4y+w3lbPo6+0njPAuHWmQmvKaoc2bJ1sNKhuPraywTG4+C
         lSZb6WdKiqi9G292vaQ7o4N7AbYYp5buRwMH/xwUBEDJ8VGB0if/ITsQvOmVPA0sl+Uy
         EXFoszpRsPpainwFgcqW+pBwUX+fqmk2UX+rzDYR8H3Z3Qwthh/tfz8J9lCflI9kG60/
         7hlRB8lncBvtEl01lTtw7P+dk/kfVAsBp1YJgbDAJ2zlpKdeeIYjMGKNVtSOrueH0bFs
         lflwurhHxnjrU+yq0b8V0KitNL0xpz4C3WXC5U4ECfqr5+HIOegQhpclIXChPz46nSps
         SzFw==
MIME-Version: 1.0
X-Received: by 10.224.36.129 with SMTP id t1mr28964qad.88.1397186444253; Thu,
 10 Apr 2014 20:20:44 -0700 (PDT)
Received: by 10.229.220.134 with HTTP; Thu, 10 Apr 2014 20:20:44 -0700 (PDT)
In-Reply-To: <CANJrAvBtDTMs9YHE=EPZ6_HR8m5QDHUjcAyK2vpVWYB2GOfcHA@mail.gmail.com>
References: <CANJrAvBtDTMs9YHE=EPZ6_HR8m5QDHUjcAyK2vpVWYB2GOfcHA@mail.gmail.com>
Date: Thu, 10 Apr 2014 23:20:44 -0400
Message-ID: <CAJgQjQ-X6giAEp7M+v2wRsSQk4kX10BM+ptuQt_SF7b0NGnTjg@mail.gmail.com>
Subject: Re: feature selection and sparse vector support
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Ignacio,

Please create a JIRA and send a PR for the information gain
computation, so it is easy to track the progress.

The sparse vector support for NaiveBayes is already implemented in
branch-1.0 and master. You only need to provide an RDD of sparse
vectors (created from Vectors.sparse).

MLUtils.loadLibSVMData reads sparse features in LIBSVM format.

Best,
Xiangrui

On Thu, Apr 10, 2014 at 5:18 PM, Ignacio Zendejas
<ignacio.zendejas.cs@gmail.com> wrote:
> Hi, again -
>
> As part of the next step, I'd like to make a more substantive contribution
> and propose some initial work on feature selection, primarily as it relates
> to text classification.
>
> Specifically, I'd like to contribute very straightforward code to perform
> information gain feature evaluation. Below's a good primer that shows that
> Information Gain is a very good option in many cases. If successful, BNS
> (introduced in the paper), would be another approach worth looking into as
> it actually improves the f score with a smaller feature space.
>
> http://machinelearning.wustl.edu/mlpapers/paper_files/Forman03.pdf
>
> And here's my first cut:
> https://github.com/izendejas/spark/commit/e5a0620838841c99865ffa4fb0d2b449751236a8
>
> I don't like that I do two passes to compute the class priors and joint
> distributions, so I'll look into using combineByKey as in the NaiveBayes
> implementation.  Also, this is still untested code, but it gets my ideas
> out there and think it'd be best to define a FeatureEval trait or whatnot
> that helps with ranking and selecting.
>
> I also realize the above methods are probably more suitable for MLI than
> MLlib, but there doesn't seem to be much activity on the former.
>
> Second, is there a plan to support sparse vector representations for
> NaiveBayes. This will probably be more efficient in, for example, text
> classification tasks with lots of features (consider the case where n-grams
> with n > 1 are used).
>
> And on a related note, MLUtils.loadLabeledData doesn't support loading
> sparse data. Any plans here to do so? There also doesn't seem to be a
> defined file format for MLlib. Has there been any consideration to support
> multiple standard formats, rather than defining one: eg, csv, tsv, Weka's
> arff, etc?
>
> Thanks for your time,
> Ignacio

From dev-return-7320-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 11 12:45:11 2014
Return-Path: <dev-return-7320-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7925911890
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Apr 2014 12:45:11 +0000 (UTC)
Received: (qmail 96602 invoked by uid 500); 11 Apr 2014 12:45:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96341 invoked by uid 500); 11 Apr 2014 12:45:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96332 invoked by uid 99); 11 Apr 2014 12:45:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 12:45:06 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of hmourit@gmail.com designates 209.85.215.49 as permitted sender)
Received: from [209.85.215.49] (HELO mail-la0-f49.google.com) (209.85.215.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 12:45:02 +0000
Received: by mail-la0-f49.google.com with SMTP id mc6so3436375lab.36
        for <dev@spark.apache.org>; Fri, 11 Apr 2014 05:44:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=m6S6LufvBRLvbIemY6SQEWKZBPvYDY2KjFI63D5whIc=;
        b=iN0fCAhbkK60AMQkHmvxx8A4cjJvC7hCfAFa6oyrHaMvkeYbRxRqDs1mJ3XIvbdeah
         IX9j20N0O4ZVBe2j2zHHVyZQCRYplyrAMhAOOpU4vnXcx0/Tv9wse96wmdVczpDRdDT9
         jYQ7tkWBsjfLS8sr4EYSs/g3gI8n1NpcAb4MuyH/Ib3SIHvizct+czEiv9xIL9UaCpLT
         nqMInIJSVzZuKFVV6D1fdyEVvhB8a+TsgGyykf3GttWejLqrpMc8Pzjf6nNgSqU+wQWL
         3AeLdTdtdeFAIFloRmgznoM86odzqLZuLoq+fXVFQrZ53PKfXGMx0H2kvdqh9OJ6A+HP
         p6jw==
X-Received: by 10.152.42.144 with SMTP id o16mr16880952lal.9.1397220280787;
 Fri, 11 Apr 2014 05:44:40 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.218.68 with HTTP; Fri, 11 Apr 2014 05:44:00 -0700 (PDT)
In-Reply-To: <CAJgQjQ-X6giAEp7M+v2wRsSQk4kX10BM+ptuQt_SF7b0NGnTjg@mail.gmail.com>
References: <CANJrAvBtDTMs9YHE=EPZ6_HR8m5QDHUjcAyK2vpVWYB2GOfcHA@mail.gmail.com>
 <CAJgQjQ-X6giAEp7M+v2wRsSQk4kX10BM+ptuQt_SF7b0NGnTjg@mail.gmail.com>
From: =?UTF-8?B?SMOpY3RvciBNb3VyacOxby1UYWzDrW4=?= <hmourit@gmail.com>
Date: Fri, 11 Apr 2014 14:44:00 +0200
Message-ID: <CAEXZ939NxMosfQA4TMqFcGma6Gi1Xb00bwAX9dqKPfMx1ar3Gw@mail.gmail.com>
Subject: Re: feature selection and sparse vector support
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c33ef282666e04f6c3b208
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c33ef282666e04f6c3b208
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi,

Regarding the implementation of feature selection techniques, I'm
implementing some iterative algorithms based on a paper by Gavin Brown et
al. [1]. In this paper, he proposes a common framework for many Information
Theory-based criteria, namely those that use relevancy (mutual information
between one feature and the label; Information Gain), redundancy, and
conditional redundancy. The latter two are differently interpreted
depending on the criteria, but all of them play with the mutual information
between the feature being analyzed and the already selected ones and the
same mutual information conditioned to the label.

I think we should have a common interface to plug different Feature
Selection techniques. I already have the algorithm implemented, but still
have to do tests on it. Right now I'm working on the design. Next week I
can share with you a proposal, so we can work together to bring Feature
Selection to Spark.

[1] Brown, G., Pocock, A., Zhao, M. J., & Luj=C3=A1n, M. (2012). Conditiona=
l
likelihood maximisation: a unifying framework for information theoretic
feature selection.*The Journal of Machine Learning Research*, *13*, 27-66.

---
H=C3=A9ctor


On Fri, Apr 11, 2014 at 5:20 AM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Hi Ignacio,
>
> Please create a JIRA and send a PR for the information gain
> computation, so it is easy to track the progress.
>
> The sparse vector support for NaiveBayes is already implemented in
> branch-1.0 and master. You only need to provide an RDD of sparse
> vectors (created from Vectors.sparse).
>
> MLUtils.loadLibSVMData reads sparse features in LIBSVM format.
>
> Best,
> Xiangrui
>
> On Thu, Apr 10, 2014 at 5:18 PM, Ignacio Zendejas
> <ignacio.zendejas.cs@gmail.com> wrote:
> > Hi, again -
> >
> > As part of the next step, I'd like to make a more substantive
> contribution
> > and propose some initial work on feature selection, primarily as it
> relates
> > to text classification.
> >
> > Specifically, I'd like to contribute very straightforward code to perfo=
rm
> > information gain feature evaluation. Below's a good primer that shows
> that
> > Information Gain is a very good option in many cases. If successful, BN=
S
> > (introduced in the paper), would be another approach worth looking into
> as
> > it actually improves the f score with a smaller feature space.
> >
> > http://machinelearning.wustl.edu/mlpapers/paper_files/Forman03.pdf
> >
> > And here's my first cut:
> >
> https://github.com/izendejas/spark/commit/e5a0620838841c99865ffa4fb0d2b44=
9751236a8
> >
> > I don't like that I do two passes to compute the class priors and joint
> > distributions, so I'll look into using combineByKey as in the NaiveBaye=
s
> > implementation.  Also, this is still untested code, but it gets my idea=
s
> > out there and think it'd be best to define a FeatureEval trait or whatn=
ot
> > that helps with ranking and selecting.
> >
> > I also realize the above methods are probably more suitable for MLI tha=
n
> > MLlib, but there doesn't seem to be much activity on the former.
> >
> > Second, is there a plan to support sparse vector representations for
> > NaiveBayes. This will probably be more efficient in, for example, text
> > classification tasks with lots of features (consider the case where
> n-grams
> > with n > 1 are used).
> >
> > And on a related note, MLUtils.loadLabeledData doesn't support loading
> > sparse data. Any plans here to do so? There also doesn't seem to be a
> > defined file format for MLlib. Has there been any consideration to
> support
> > multiple standard formats, rather than defining one: eg, csv, tsv, Weka=
's
> > arff, etc?
> >
> > Thanks for your time,
> > Ignacio
>

--001a11c33ef282666e04f6c3b208--

From dev-return-7321-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 11 16:25:11 2014
Return-Path: <dev-return-7321-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DD586102AB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Apr 2014 16:25:10 +0000 (UTC)
Received: (qmail 18352 invoked by uid 500); 11 Apr 2014 16:25:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18302 invoked by uid 500); 11 Apr 2014 16:25:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18289 invoked by uid 99); 11 Apr 2014 16:25:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 16:25:06 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vanzin@cloudera.com designates 209.85.216.181 as permitted sender)
Received: from [209.85.216.181] (HELO mail-qc0-f181.google.com) (209.85.216.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 16:25:02 +0000
Received: by mail-qc0-f181.google.com with SMTP id x3so6197434qcv.12
        for <dev@spark.apache.org>; Fri, 11 Apr 2014 09:24:40 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=VM7rnZo84Ey/AINb4q3x9zCIH+O+CillX4d4WDyTKKE=;
        b=NcKFHAtV3OJqp+HvIH7meCkZgkOGTFf88m/rSKdxj8phjSqvjnNCu+okYgIRadQKMD
         fOocUV3MtpHzcb5bJEKSE6vJlP4OYxVBBUUv+NV+U8e+AlTI3Zj1Diyg3U1XVBCtw13Y
         Tu5v2es4w6+/6Lvvo9RTZUEERt7QckLr+w0rlJZwZSqcNg//ywubmfnwoL9qSkmN/Miv
         i4tF03zMAx4n6uJIwSa32/wrOTSOBXDcu0G0xyUpuy25SrUyDWr8FSGJZaj0jPSkHZAm
         oCSJOhK3o++TBHnqyK0kOPxYNJfl1J/p8uRkXi6zF5HThYtAHHOhr6BNVtRqXFenh5Gi
         NIVA==
X-Gm-Message-State: ALoCoQmiy5R5g7agXXFnglr9EqsrH5f70dqAqT273BdlykNY/QaYGREvM7XIdMOKJcB80kmvQk9i
MIME-Version: 1.0
X-Received: by 10.140.98.116 with SMTP id n107mr4162747qge.94.1397233480153;
 Fri, 11 Apr 2014 09:24:40 -0700 (PDT)
Received: by 10.229.211.5 with HTTP; Fri, 11 Apr 2014 09:24:40 -0700 (PDT)
In-Reply-To: <CAAswR-4owXBP+orY4jR8zHdww5jTHgfZoMNrDx+sBdQdi9jDqw@mail.gmail.com>
References: <CAAOnQ7vvpbuPHgpyf139rYS=m-AVgNpE0FQovRBc0JZgSmOHJg@mail.gmail.com>
	<CAAswR-4owXBP+orY4jR8zHdww5jTHgfZoMNrDx+sBdQdi9jDqw@mail.gmail.com>
Date: Fri, 11 Apr 2014 09:24:40 -0700
Message-ID: <CAAOnQ7vTAnwWG_UzBRMpSwwQXQ=cb8_K0=O_Hr707BQf3Cezkw@mail.gmail.com>
Subject: Re: RFC: varargs in Logging.scala?
From: Marcelo Vanzin <vanzin@cloudera.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Thu, Apr 10, 2014 at 5:46 PM, Michael Armbrust
<michael@databricks.com> wrote:
> ... all of the suffer from the fact that the log message needs to be built
>> even
>> though it might not be used.
>
> This is not true of the current implementation (and this is actually why
> Spark has a logging trait instead of just using a logger directly.)
>
> If you look at the original function signatures:
>
> protected def logDebug(msg: => String) ...
>
>
> The => implies that we are passing the msg by name instead of by value.
> Under the covers, scala is creating a closure that can be used to calculate
> the log message, only if its actually required.

Hah. Interesting. Guess it's my noob Scala hat showing off.

I saw the PR about using scala-logging before, but didn't pay too
close attention to it.

Thanks for the info guys!

-- 
Marcelo

From dev-return-7322-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 11 18:02:23 2014
Return-Path: <dev-return-7322-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 02B5C107DC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Apr 2014 18:02:23 +0000 (UTC)
Received: (qmail 41260 invoked by uid 500); 11 Apr 2014 18:02:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41140 invoked by uid 500); 11 Apr 2014 18:02:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41084 invoked by uid 99); 11 Apr 2014 18:02:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 18:02:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of david.lw.hall@gmail.com designates 209.85.212.172 as permitted sender)
Received: from [209.85.212.172] (HELO mail-wi0-f172.google.com) (209.85.212.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 18:02:09 +0000
Received: by mail-wi0-f172.google.com with SMTP id hi2so1448423wib.5
        for <dev@spark.apache.org>; Fri, 11 Apr 2014 11:01:48 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=ItyGwWDKE4XeBL6SusW7+FbidlNR62eOPa85melHLvY=;
        b=mMbkrATwBxkxG07Mc7ze/TrHF5rVgAA5nauCJCqecOUATk2ViAuvdDxELms4pT6XGO
         Eir15gXJIjKQAgBKD7evVt6LbSfYmosfkILMNZRKgQzthFGc/UHrrOlGTi8fEZfk6XnR
         UdfJ7hhgmMRDBNutb+nO/4W7SUeiTzDvDNhDgo//F+R18y1u4FZzBZTlAK0zB4G4fdXd
         lZJXOl3zogS/HYslxpvyRDZTphcFChUqu0zBAPPJ96gBJ+FmV9JhLjKusKn/ce2DKrt1
         R1P716YSczTojyMp3DvRgwYiLhxr9a6ypK5rMdIMVqJhDoW9XcPgKEUEPpFfdE+PiNWl
         VHCA==
MIME-Version: 1.0
X-Received: by 10.195.18.8 with SMTP id gi8mr3213273wjd.75.1397239308646; Fri,
 11 Apr 2014 11:01:48 -0700 (PDT)
Sender: david.lw.hall@gmail.com
Received: by 10.227.232.207 with HTTP; Fri, 11 Apr 2014 11:01:48 -0700 (PDT)
In-Reply-To: <CAAswR-5z0Hqomk56wh57akmqrjvA1dAmNv9qnJJ7hMs6r+vFwA@mail.gmail.com>
References: <CAAOnQ7vvpbuPHgpyf139rYS=m-AVgNpE0FQovRBc0JZgSmOHJg@mail.gmail.com>
	<CAAswR-4owXBP+orY4jR8zHdww5jTHgfZoMNrDx+sBdQdi9jDqw@mail.gmail.com>
	<CAAswR-5z0Hqomk56wh57akmqrjvA1dAmNv9qnJJ7hMs6r+vFwA@mail.gmail.com>
Date: Fri, 11 Apr 2014 11:01:48 -0700
X-Google-Sender-Auth: 6tAAErpvvu-AS4VVYRk_K-8B5xs
Message-ID: <CALW2ey1ikvR-0qLox4Jf7PcBMv60RfB2xfp9HKHZfa_=uXMiDA@mail.gmail.com>
Subject: Re: RFC: varargs in Logging.scala?
From: David Hall <dlwh@cs.berkeley.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1130cc46a881bf04f6c820b0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1130cc46a881bf04f6c820b0
Content-Type: text/plain; charset=UTF-8

Another usage that's nice is:

logDebug {
   val timeS = timeMillis/1000.0
   s"Time: $timeS"
}

which can be useful for more complicated expressions.


On Thu, Apr 10, 2014 at 5:55 PM, Michael Armbrust <michael@databricks.com>wrote:

> BTW...
>
> You can do calculations in string interpolation:
> s"Time: ${timeMillis / 1000}s"
>
> Or use format strings.
> f"Float with two decimal places: $floatValue%.2f"
>
> More info:
> http://docs.scala-lang.org/overviews/core/string-interpolation.html
>
>
> On Thu, Apr 10, 2014 at 5:46 PM, Michael Armbrust <michael@databricks.com
> >wrote:
>
> > Hi Marcelo,
> >
> > Thanks for bringing this up here, as this has been a topic of debate
> > recently.  Some thoughts below.
> >
> > ... all of the suffer from the fact that the log message needs to be
> built
> >> even
> >>
> >> though it might not be used.
> >>
> >
> > This is not true of the current implementation (and this is actually why
> > Spark has a logging trait instead of just using a logger directly.)
> >
> > If you look at the original function signatures:
> >
> > protected def logDebug(msg: => String) ...
> >
> >
> > The => implies that we are passing the msg by name instead of by value.
> > Under the covers, scala is creating a closure that can be used to
> calculate
> > the log message, only if its actually required.  This does result is a
> > significant performance improvement, but still requires allocating an
> > object for the closure.  The bytecode is really something like this:
> >
> > val logMessage = new Function0() { def call() =  "Log message" +
> someExpensiveComputation() }
> > log.debug(logMessage)
> >
> >
> > In Catalyst and Spark SQL we are using the scala-logging package, which
> > uses macros to automatically rewrite all of your log statements.
> >
> > You write: logger.debug(s"Log message $someExpensiveComputation")
> >
> > You get:
> >
> > if(logger.debugEnabled) {
> >   val logMsg = "Log message" + someExpensiveComputation()
> >   logger.debug(logMsg)
> > }
> >
> > IMHO, this is the cleanest option (and is supported by Typesafe).  Based
> > on a micro-benchmark, it is also the fastest:
> >
> > std logging: 19885.48ms
> > spark logging 914.408ms
> > scala logging 729.779ms
> >
> > Once the dust settles from the 1.0 release, I'd be in favor of
> > standardizing on scala-logging.
> >
> > Michael
> >
>

--001a1130cc46a881bf04f6c820b0--

From dev-return-7323-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 11 18:27:27 2014
Return-Path: <dev-return-7323-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C4C9A10932
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Apr 2014 18:27:27 +0000 (UTC)
Received: (qmail 11755 invoked by uid 500); 11 Apr 2014 18:27:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11726 invoked by uid 500); 11 Apr 2014 18:27:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11716 invoked by uid 99); 11 Apr 2014 18:27:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 18:27:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ignacio.zendejas.cs@gmail.com designates 209.85.160.50 as permitted sender)
Received: from [209.85.160.50] (HELO mail-pb0-f50.google.com) (209.85.160.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 18:27:19 +0000
Received: by mail-pb0-f50.google.com with SMTP id md12so5722209pbc.23
        for <dev@spark.apache.org>; Fri, 11 Apr 2014 11:26:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=2TWUwbk4dSABohoDPde2mKUGylieL5QocUOsWHH+OhY=;
        b=0dFezE6EBT8xx+T5CPdvwnjjfI5tqcb7Lp6I/Bdb/KrxQUaJfUOp2DyyIJS1N3AcU1
         qcpFmhMC+7CzZjc8yzb4p+y0PFasnUCkDH4j4QSk0fGQQXtHSeX+RvPsrCKD5mtbXfj6
         EjACILuNgibYHDVy8ZHWRH7oEB+vWZ5gx34NPivjIZXcVoRwcn5Gu1u5udU7iHptCtWd
         zJK6Fowb2A6ip1WfJPi7UXx05Hl32pVPAZQvyAVRdTjPKZuXDfgAM51nbPzCxoqseK/b
         ZxUZUWjbFP9EnQuCthxdGtwpz8TN3BmsimuDYAmmjm77zxDcbeB1s9cdu7AR1jMq3nZI
         zmSQ==
MIME-Version: 1.0
X-Received: by 10.68.181.165 with SMTP id dx5mr29385082pbc.38.1397240818991;
 Fri, 11 Apr 2014 11:26:58 -0700 (PDT)
Received: by 10.70.51.225 with HTTP; Fri, 11 Apr 2014 11:26:58 -0700 (PDT)
In-Reply-To: <CAEXZ939NxMosfQA4TMqFcGma6Gi1Xb00bwAX9dqKPfMx1ar3Gw@mail.gmail.com>
References: <CANJrAvBtDTMs9YHE=EPZ6_HR8m5QDHUjcAyK2vpVWYB2GOfcHA@mail.gmail.com>
	<CAJgQjQ-X6giAEp7M+v2wRsSQk4kX10BM+ptuQt_SF7b0NGnTjg@mail.gmail.com>
	<CAEXZ939NxMosfQA4TMqFcGma6Gi1Xb00bwAX9dqKPfMx1ar3Gw@mail.gmail.com>
Date: Fri, 11 Apr 2014 11:26:58 -0700
Message-ID: <CANJrAvDwmJ6LXpRgTwdZZboWy2MUN1WJ5mt_0i3S2mo9mXgPFQ@mail.gmail.com>
Subject: Re: feature selection and sparse vector support
From: Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b86d8acae87d404f6c87ad4
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b86d8acae87d404f6c87ad4
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable

Thanks for the response, Xiangrui.

And sounds good, H=E9ctor. Look forward to working on this together.

A common interface is definitely required.  I'll create a JIRA shortly and
will explore design options myself to bring ideas to the table.

cheers.



On Fri, Apr 11, 2014 at 5:44 AM, H=E9ctor Mouri=F1o-Tal=EDn <hmourit@gmail.=
com>wrote:

> Hi,
>
> Regarding the implementation of feature selection techniques, I'm
> implementing some iterative algorithms based on a paper by Gavin Brown et
> al. [1]. In this paper, he proposes a common framework for many Informati=
on
> Theory-based criteria, namely those that use relevancy (mutual informatio=
n
> between one feature and the label; Information Gain), redundancy, and
> conditional redundancy. The latter two are differently interpreted
> depending on the criteria, but all of them play with the mutual informati=
on
> between the feature being analyzed and the already selected ones and the
> same mutual information conditioned to the label.
>
> I think we should have a common interface to plug different Feature
> Selection techniques. I already have the algorithm implemented, but still
> have to do tests on it. Right now I'm working on the design. Next week I
> can share with you a proposal, so we can work together to bring Feature
> Selection to Spark.
>
> [1] Brown, G., Pocock, A., Zhao, M. J., & Luj=E1n, M. (2012). Conditional
> likelihood maximisation: a unifying framework for information theoretic
> feature selection.*The Journal of Machine Learning Research*, *13*, 27-66=
.
>
> ---
> H=E9ctor
>
>
> On Fri, Apr 11, 2014 at 5:20 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
> > Hi Ignacio,
> >
> > Please create a JIRA and send a PR for the information gain
> > computation, so it is easy to track the progress.
> >
> > The sparse vector support for NaiveBayes is already implemented in
> > branch-1.0 and master. You only need to provide an RDD of sparse
> > vectors (created from Vectors.sparse).
> >
> > MLUtils.loadLibSVMData reads sparse features in LIBSVM format.
> >
> > Best,
> > Xiangrui
> >
> > On Thu, Apr 10, 2014 at 5:18 PM, Ignacio Zendejas
> > <ignacio.zendejas.cs@gmail.com> wrote:
> > > Hi, again -
> > >
> > > As part of the next step, I'd like to make a more substantive
> > contribution
> > > and propose some initial work on feature selection, primarily as it
> > relates
> > > to text classification.
> > >
> > > Specifically, I'd like to contribute very straightforward code to
> perform
> > > information gain feature evaluation. Below's a good primer that shows
> > that
> > > Information Gain is a very good option in many cases. If successful,
> BNS
> > > (introduced in the paper), would be another approach worth looking in=
to
> > as
> > > it actually improves the f score with a smaller feature space.
> > >
> > > http://machinelearning.wustl.edu/mlpapers/paper_files/Forman03.pdf
> > >
> > > And here's my first cut:
> > >
> >
> https://github.com/izendejas/spark/commit/e5a0620838841c99865ffa4fb0d2b44=
9751236a8
> > >
> > > I don't like that I do two passes to compute the class priors and joi=
nt
> > > distributions, so I'll look into using combineByKey as in the
> NaiveBayes
> > > implementation.  Also, this is still untested code, but it gets my
> ideas
> > > out there and think it'd be best to define a FeatureEval trait or
> whatnot
> > > that helps with ranking and selecting.
> > >
> > > I also realize the above methods are probably more suitable for MLI
> than
> > > MLlib, but there doesn't seem to be much activity on the former.
> > >
> > > Second, is there a plan to support sparse vector representations for
> > > NaiveBayes. This will probably be more efficient in, for example, tex=
t
> > > classification tasks with lots of features (consider the case where
> > n-grams
> > > with n > 1 are used).
> > >
> > > And on a related note, MLUtils.loadLabeledData doesn't support loadin=
g
> > > sparse data. Any plans here to do so? There also doesn't seem to be a
> > > defined file format for MLlib. Has there been any consideration to
> > support
> > > multiple standard formats, rather than defining one: eg, csv, tsv,
> Weka's
> > > arff, etc?
> > >
> > > Thanks for your time,
> > > Ignacio
> >
>

--047d7b86d8acae87d404f6c87ad4--

From dev-return-7324-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 11 19:09:01 2014
Return-Path: <dev-return-7324-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7F94410B29
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Apr 2014 19:09:01 +0000 (UTC)
Received: (qmail 4329 invoked by uid 500); 11 Apr 2014 19:09:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3800 invoked by uid 500); 11 Apr 2014 19:08:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 98634 invoked by uid 99); 11 Apr 2014 19:05:31 -0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of arora.priya4172@gmail.com designates 209.85.216.171 as permitted sender)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=3B2aTIOBUSyJ5LR591xz72MHn6iNwmZSwrJ43XZUL/c=;
        b=PJyIn9J5yGKVtbKTWigyOG5MNsXRrvZZWAxiMivp859/15TEWp81IrbpAVa2T1MW8n
         uFY1Lf5imOZ822m49rdymAIwLR8tq6t5/09ONlWKDhPQOP7+0CgZnpN1eGYCl5lxBgBR
         AYgg1Ia9xOWu4qJfYXrPIXx/6nk24Dnda7+zT0ubT6hT60eY+WXqlLDSULU2TMBjShiX
         BGO+CY1FOdjb7OBwtKIpRe79DBOzUWBoGOs3zBohbtq/KZMg+EQV8TfmYCvUjYOVTk87
         eTRgHqj85aHd2cTipxbPS9uh7uQQcK+LhahMU08DiTVpCFCo5OdarZvbRIkUrzbQvCsv
         zJtQ==
MIME-Version: 1.0
X-Received: by 10.140.87.5 with SMTP id q5mr30609363qgd.31.1397243103605; Fri,
 11 Apr 2014 12:05:03 -0700 (PDT)
Date: Fri, 11 Apr 2014 15:05:03 -0400
Message-ID: <CA+abfm8y7en7tfD8Rfa7JB3YXs130z3UTR6PbfjiGjZ1n=fYVA@mail.gmail.com>
Subject: Suggestion
From: priya arora <arora.priya4172@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113ab2ecdafe6204f6c90265
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ab2ecdafe6204f6c90265
Content-Type: text/plain; charset=ISO-8859-1

Hi,

May I know how one can contribute in this project
http://spark.apache.org/mllib/ or in any other project. I am very eager to
contribute. Do let me know.

Thanks & Regards,
Priya Arora

--001a113ab2ecdafe6204f6c90265--

From dev-return-7325-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 11 19:17:14 2014
Return-Path: <dev-return-7325-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B5A6910BA1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Apr 2014 19:17:14 +0000 (UTC)
Received: (qmail 23614 invoked by uid 500); 11 Apr 2014 19:17:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23369 invoked by uid 500); 11 Apr 2014 19:17:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23266 invoked by uid 99); 11 Apr 2014 19:17:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 19:17:09 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.53 as permitted sender)
Received: from [209.85.216.53] (HELO mail-qa0-f53.google.com) (209.85.216.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 19:17:04 +0000
Received: by mail-qa0-f53.google.com with SMTP id w8so5744627qac.12
        for <dev@spark.apache.org>; Fri, 11 Apr 2014 12:16:43 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=cmCjoy+MRKPGafSHWRrUkUbKm7wltrIPEMIig7gj9n0=;
        b=EdFtlLUl/NmsvEXlWKo/IcJYg294v8pDpFYpXU19jlga8pVtqkUp6ZQAF85mDM9K8F
         FpvUZXO6K4wjAjV+05HR6xBsF85qRBZ8XTrRyZd7K888M0Z2COETtXJL6zt61WgKPm8i
         BG/2nritURRS5XwoZVrZjypBmtyAQVwMpo1jl7x62OOAi6WUw613K+JElyUe27tl6Yr8
         pRGBGcW/n/vnhJ8YeFDRzqdaTIQC9G7CLUnRKW/XqU01V1MkSaIl4DPa7csyjeQR5CAL
         2OV+6HqoNedKOJUFCbVnHEiBigeMf8UWJGZbaxcsy5fHzWnuuOiuDWsTWXKVNeab6ipa
         gI3Q==
X-Gm-Message-State: ALoCoQm+yJjdwrx+1ijb5vOLBMI5FcwdgeqWkv9p+c+tTTQClDE80KQJuzWpQpeq03LIjGyqdcji
MIME-Version: 1.0
X-Received: by 10.224.57.72 with SMTP id b8mr1324181qah.41.1397243803785; Fri,
 11 Apr 2014 12:16:43 -0700 (PDT)
Received: by 10.140.98.69 with HTTP; Fri, 11 Apr 2014 12:16:43 -0700 (PDT)
In-Reply-To: <CA+abfm8y7en7tfD8Rfa7JB3YXs130z3UTR6PbfjiGjZ1n=fYVA@mail.gmail.com>
References: <CA+abfm8y7en7tfD8Rfa7JB3YXs130z3UTR6PbfjiGjZ1n=fYVA@mail.gmail.com>
Date: Fri, 11 Apr 2014 12:16:43 -0700
Message-ID: <CACBYxK+i+ROPngdiTUS5od-SG5pD=XbLsrMBG2_kC-v9oxDmiA@mail.gmail.com>
Subject: Re: Suggestion
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0149398296ee1304f6c92cfe
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149398296ee1304f6c92cfe
Content-Type: text/plain; charset=ISO-8859-1

Hi Priya,

Here's a good place to start:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

-Sandy


On Fri, Apr 11, 2014 at 12:05 PM, priya arora <arora.priya4172@gmail.com>wrote:

> Hi,
>
> May I know how one can contribute in this project
> http://spark.apache.org/mllib/ or in any other project. I am very eager to
> contribute. Do let me know.
>
> Thanks & Regards,
> Priya Arora
>

--089e0149398296ee1304f6c92cfe--

From dev-return-7326-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 11 21:22:03 2014
Return-Path: <dev-return-7326-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7F4F011073
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Apr 2014 21:22:03 +0000 (UTC)
Received: (qmail 50991 invoked by uid 500); 11 Apr 2014 21:11:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49515 invoked by uid 500); 11 Apr 2014 21:09:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42882 invoked by uid 99); 11 Apr 2014 21:04:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 21:04:39 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ignacio.zendejas.cs@gmail.com designates 209.85.160.46 as permitted sender)
Received: from [209.85.160.46] (HELO mail-pb0-f46.google.com) (209.85.160.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 21:04:35 +0000
Received: by mail-pb0-f46.google.com with SMTP id rq2so5895202pbb.33
        for <dev@spark.apache.org>; Fri, 11 Apr 2014 14:04:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=eOfs3j9492oxvW068X8gH4HMOuRGHgYd8ewcj7rXVng=;
        b=nLq4BUiyIlwh9gNr1kl34R95BSpj0UqP7DaWEeVSZJZSukFd2uZrxvdfp+EDxes1x8
         7q0h1uE9kdGRZsunfsL+2B8slLTQaDY9JpbGJYVBNvRpnWZdTNMY80UdZV9txfQYGrJm
         Q9Ky0706wzIbZtBrAh890KrpgiBl3dr4iSODHKozaMCb5fADDg+tgXq8kXk9TtbKDFsl
         FDqNpuwv2SDGLN0XUFaTmXl1G0SChK5OkaHzgNSdIA6oVIZCBbv+2L/DIArzRHWfz7/a
         lywGpoukrC2qP4GHcHtKBPOOL1W2Hx7kcIRW9mT9NZme9aeWG9zxmxmqDZA6XEtP27GD
         bZMQ==
MIME-Version: 1.0
X-Received: by 10.68.181.165 with SMTP id dx5mr30180423pbc.38.1397250255023;
 Fri, 11 Apr 2014 14:04:15 -0700 (PDT)
Received: by 10.70.51.225 with HTTP; Fri, 11 Apr 2014 14:04:14 -0700 (PDT)
In-Reply-To: <CANJrAvDwmJ6LXpRgTwdZZboWy2MUN1WJ5mt_0i3S2mo9mXgPFQ@mail.gmail.com>
References: <CANJrAvBtDTMs9YHE=EPZ6_HR8m5QDHUjcAyK2vpVWYB2GOfcHA@mail.gmail.com>
	<CAJgQjQ-X6giAEp7M+v2wRsSQk4kX10BM+ptuQt_SF7b0NGnTjg@mail.gmail.com>
	<CAEXZ939NxMosfQA4TMqFcGma6Gi1Xb00bwAX9dqKPfMx1ar3Gw@mail.gmail.com>
	<CANJrAvDwmJ6LXpRgTwdZZboWy2MUN1WJ5mt_0i3S2mo9mXgPFQ@mail.gmail.com>
Date: Fri, 11 Apr 2014 14:04:14 -0700
Message-ID: <CANJrAvCH3UxJ4x8XLXdTYDmx8kapmAhs69NVgTTQhQrbo3xrwg@mail.gmail.com>
Subject: Re: feature selection and sparse vector support
From: Ignacio Zendejas <ignacio.zendejas.cs@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b86d8ac1cf50604f6caad26
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b86d8ac1cf50604f6caad26
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable

Here's the JIRA:
https://issues.apache.org/jira/browse/SPARK-1473

Future discussions should take place in its comments section.

Thanks.




On Fri, Apr 11, 2014 at 11:26 AM, Ignacio Zendejas <
ignacio.zendejas.cs@gmail.com> wrote:

> Thanks for the response, Xiangrui.
>
> And sounds good, H=E9ctor. Look forward to working on this together.
>
> A common interface is definitely required.  I'll create a JIRA shortly an=
d
> will explore design options myself to bring ideas to the table.
>
> cheers.
>
>
>
> On Fri, Apr 11, 2014 at 5:44 AM, H=E9ctor Mouri=F1o-Tal=EDn <hmourit@gmai=
l.com>wrote:
>
>> Hi,
>>
>> Regarding the implementation of feature selection techniques, I'm
>> implementing some iterative algorithms based on a paper by Gavin Brown e=
t
>> al. [1]. In this paper, he proposes a common framework for many
>> Information
>> Theory-based criteria, namely those that use relevancy (mutual informati=
on
>> between one feature and the label; Information Gain), redundancy, and
>> conditional redundancy. The latter two are differently interpreted
>> depending on the criteria, but all of them play with the mutual
>> information
>> between the feature being analyzed and the already selected ones and the
>> same mutual information conditioned to the label.
>>
>> I think we should have a common interface to plug different Feature
>> Selection techniques. I already have the algorithm implemented, but stil=
l
>> have to do tests on it. Right now I'm working on the design. Next week I
>> can share with you a proposal, so we can work together to bring Feature
>> Selection to Spark.
>>
>> [1] Brown, G., Pocock, A., Zhao, M. J., & Luj=E1n, M. (2012). Conditiona=
l
>> likelihood maximisation: a unifying framework for information theoretic
>> feature selection.*The Journal of Machine Learning Research*, *13*, 27-6=
6.
>>
>> ---
>> H=E9ctor
>>
>>
>> On Fri, Apr 11, 2014 at 5:20 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>
>> > Hi Ignacio,
>> >
>> > Please create a JIRA and send a PR for the information gain
>> > computation, so it is easy to track the progress.
>> >
>> > The sparse vector support for NaiveBayes is already implemented in
>> > branch-1.0 and master. You only need to provide an RDD of sparse
>> > vectors (created from Vectors.sparse).
>> >
>> > MLUtils.loadLibSVMData reads sparse features in LIBSVM format.
>> >
>> > Best,
>> > Xiangrui
>> >
>> > On Thu, Apr 10, 2014 at 5:18 PM, Ignacio Zendejas
>> > <ignacio.zendejas.cs@gmail.com> wrote:
>> > > Hi, again -
>> > >
>> > > As part of the next step, I'd like to make a more substantive
>> > contribution
>> > > and propose some initial work on feature selection, primarily as it
>> > relates
>> > > to text classification.
>> > >
>> > > Specifically, I'd like to contribute very straightforward code to
>> perform
>> > > information gain feature evaluation. Below's a good primer that show=
s
>> > that
>> > > Information Gain is a very good option in many cases. If successful,
>> BNS
>> > > (introduced in the paper), would be another approach worth looking
>> into
>> > as
>> > > it actually improves the f score with a smaller feature space.
>> > >
>> > > http://machinelearning.wustl.edu/mlpapers/paper_files/Forman03.pdf
>> > >
>> > > And here's my first cut:
>> > >
>> >
>> https://github.com/izendejas/spark/commit/e5a0620838841c99865ffa4fb0d2b4=
49751236a8
>> > >
>> > > I don't like that I do two passes to compute the class priors and
>> joint
>> > > distributions, so I'll look into using combineByKey as in the
>> NaiveBayes
>> > > implementation.  Also, this is still untested code, but it gets my
>> ideas
>> > > out there and think it'd be best to define a FeatureEval trait or
>> whatnot
>> > > that helps with ranking and selecting.
>> > >
>> > > I also realize the above methods are probably more suitable for MLI
>> than
>> > > MLlib, but there doesn't seem to be much activity on the former.
>> > >
>> > > Second, is there a plan to support sparse vector representations for
>> > > NaiveBayes. This will probably be more efficient in, for example, te=
xt
>> > > classification tasks with lots of features (consider the case where
>> > n-grams
>> > > with n > 1 are used).
>> > >
>> > > And on a related note, MLUtils.loadLabeledData doesn't support loadi=
ng
>> > > sparse data. Any plans here to do so? There also doesn't seem to be =
a
>> > > defined file format for MLlib. Has there been any consideration to
>> > support
>> > > multiple standard formats, rather than defining one: eg, csv, tsv,
>> Weka's
>> > > arff, etc?
>> > >
>> > > Thanks for your time,
>> > > Ignacio
>> >
>>
>
>

--047d7b86d8ac1cf50604f6caad26--

From dev-return-7327-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 11 21:35:43 2014
Return-Path: <dev-return-7327-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9B9A411148
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Apr 2014 21:35:43 +0000 (UTC)
Received: (qmail 22522 invoked by uid 500); 11 Apr 2014 21:35:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22474 invoked by uid 500); 11 Apr 2014 21:35:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22465 invoked by uid 99); 11 Apr 2014 21:35:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 21:35:41 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.82 as permitted sender)
Received: from [171.67.219.82] (HELO smtp.stanford.edu) (171.67.219.82)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 21:35:37 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 452DB34341C
	for <dev@spark.apache.org>; Fri, 11 Apr 2014 14:35:14 -0700 (PDT)
Received: from mail-qa0-f43.google.com (mail-qa0-f43.google.com [209.85.216.43])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id EB107342FD9
	for <dev@spark.apache.org>; Fri, 11 Apr 2014 14:35:09 -0700 (PDT)
Received: by mail-qa0-f43.google.com with SMTP id j15so5944518qaq.30
        for <dev@spark.apache.org>; Fri, 11 Apr 2014 14:35:08 -0700 (PDT)
X-Gm-Message-State: ALoCoQmJKel4LtCr1HJ8cVG9e9RqsWvgFhmzzJ4f9CrMD54AFmrmYgSzxpUzus1n4DITGPLQr1ha
MIME-Version: 1.0
X-Received: by 10.224.160.206 with SMTP id o14mr2183590qax.44.1397252108103;
 Fri, 11 Apr 2014 14:35:08 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Fri, 11 Apr 2014 14:35:08 -0700 (PDT)
Date: Fri, 11 Apr 2014 14:35:08 -0700
Message-ID: <CAEYYnxb5nwQ7TUBgeXjzApsTPsDSVucgRAJHG2rDV5ABD1-ajA@mail.gmail.com>
Subject: It seems that jenkins for PR is not working
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I always got
=========================================================================

Could not find Apache license headers in the following files:
 !????? /root/workspace/SparkPullRequestBuilder/python/metastore/db.lck
 !????? /root/workspace/SparkPullRequestBuilder/python/metastore/service.properties


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai

From dev-return-7328-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 11 23:59:12 2014
Return-Path: <dev-return-7328-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5814E114A8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Apr 2014 23:59:12 +0000 (UTC)
Received: (qmail 22735 invoked by uid 500); 11 Apr 2014 23:59:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22626 invoked by uid 500); 11 Apr 2014 23:59:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22618 invoked by uid 99); 11 Apr 2014 23:59:10 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 23:59:10 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mayur.rustagi@gmail.com designates 74.125.82.49 as permitted sender)
Received: from [74.125.82.49] (HELO mail-wg0-f49.google.com) (74.125.82.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Apr 2014 23:59:06 +0000
Received: by mail-wg0-f49.google.com with SMTP id a1so6167306wgh.32
        for <dev@spark.apache.org>; Fri, 11 Apr 2014 16:58:45 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=6oiwKik8h/IdA5QnMppzFkr1s2H2y9I2K1YRZxI6fII=;
        b=P8waAwOj66bUvR63k41UmLA/HQfqcYruGg5NsY3YDMDvDPG33q0ahhHALNvvTYeyQY
         N/RJMhzEB5BnOAKtfjCQrY8CGbHI1CTEVnHHZ1s+i5wyC53OglrOyrmB7rZbKsyn70ZB
         lYw/88EihRWfjszXx6ACxNdjBOc5s8WjmIAawCiHc1HiahRBWWD5Rv/kOiP0AfmsV1t7
         ToTXJL8XgIz7ywK6EocWxWtp+3XKr65AstoexcV1Dcr2ekke2R0kCVew5F3AvX25oFN8
         CkarY2o6nB+E10DiL+ELA7KUqkzoDaHpKGSIZFJVgq4tcXezLyTu9iej2t93u36B5nSL
         /9fw==
MIME-Version: 1.0
X-Received: by 10.180.74.132 with SMTP id t4mr384665wiv.30.1397260725183; Fri,
 11 Apr 2014 16:58:45 -0700 (PDT)
Received: by 10.194.7.225 with HTTP; Fri, 11 Apr 2014 16:58:45 -0700 (PDT)
Received: by 10.194.7.225 with HTTP; Fri, 11 Apr 2014 16:58:45 -0700 (PDT)
In-Reply-To: <CAKYY9AKJNFonyWw5UeqsnKvze10DauEHgq3nAEtm--yBUbxbCw@mail.gmail.com>
References: <CAKYY9AKJNFonyWw5UeqsnKvze10DauEHgq3nAEtm--yBUbxbCw@mail.gmail.com>
Date: Fri, 11 Apr 2014 16:58:45 -0700
Message-ID: <CAAqHKj5u79OOZ2nhFLY30EKSAEWVitbOkXp7B+J7jBZzFfn+4Q@mail.gmail.com>
Subject: Re: Building Spark AMI
From: Mayur Rustagi <mayur.rustagi@gmail.com>
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d043be06c2eebd904f6cd1d4d
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043be06c2eebd904f6cd1d4d
Content-Type: text/plain; charset=UTF-8

I am creating one fully configured & synced one. But you still need to send
over configuration. Do you plan to use chef for that ?
 On Apr 10, 2014 6:58 PM, "Jim Ancona" <jim@anconafamily.com> wrote:

> Are there scripts to build the AMI used by the spark-ec2 script?
>
> Alternatively, is there a place to download the AMI. I'm interested in
> using it to deploy into an internal Openstack cloud.
>
> Thanks,
>
> Jim
>

--f46d043be06c2eebd904f6cd1d4d--

From dev-return-7329-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Apr 12 02:06:28 2014
Return-Path: <dev-return-7329-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0654B11736
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 12 Apr 2014 02:06:28 +0000 (UTC)
Received: (qmail 43651 invoked by uid 500); 12 Apr 2014 02:06:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43474 invoked by uid 500); 12 Apr 2014 02:06:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43466 invoked by uid 99); 12 Apr 2014 02:06:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 12 Apr 2014 02:06:26 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.219.49] (HELO mail-oa0-f49.google.com) (209.85.219.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 12 Apr 2014 02:06:19 +0000
Received: by mail-oa0-f49.google.com with SMTP id o6so7030139oag.36
        for <dev@spark.apache.org>; Fri, 11 Apr 2014 19:05:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=anconafamily.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=tbTot0nDgn2UpcQJ2lzcJy94fLEDyL99Z3ONtU/4gDs=;
        b=GuH/8NDdN9BuzbFfRiMtdM8jirCtm7ZMXF7XVG3fS8gDyqVUrZePWwvwIPZ6IyTgFf
         MEAoNHLNMImAGrEGLKws9/Gsefes4M+I6+NkgSzNmK6SrAdk6+m3ks9RR+m8/KVfmC5E
         c0ACYsdytpdi/QHaUPsOtxDlwDq3KvaIvmby8=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=tbTot0nDgn2UpcQJ2lzcJy94fLEDyL99Z3ONtU/4gDs=;
        b=caRGBjBtDPTODQoJGQpt19qSyo1ZK9qXGNxXg4aNP0+hnwZ29Qqxwlt9mABqHo6qUE
         W71U8R2lyc30p6o2l7vcXuaEgXsz/B+DW+u/trHdRgVpKvDCqGjUj9PC2u4kuJNSK3+K
         mQp9yRGCEBaSMIoU1ipYjsyPdr6uCwjSOuHsv0D/Cp9glBDftyxNrcJzQvcV8NOWY+SD
         lIc3lcc865Ch1shQuHgzHNHEs8Via93DDzf62a1m4hdxm83a99lGL3kj5K14SgKj3RJk
         YblRFCS4s1e0ox0bvZ2Q11z41kcSx7Tt+u7bQQQhiZTm74eVTUQMlCzGEVNoYJK6mqgV
         9N6Q==
X-Gm-Message-State: ALoCoQn5mGdgq05swA5CKWD+AdIdZIDsKv026fivYiE6skvdRx7aTluRKvBYg6Ynz2mzflQbN4F6
MIME-Version: 1.0
X-Received: by 10.182.117.195 with SMTP id kg3mr21904885obb.17.1397268356805;
 Fri, 11 Apr 2014 19:05:56 -0700 (PDT)
Received: by 10.182.56.170 with HTTP; Fri, 11 Apr 2014 19:05:56 -0700 (PDT)
X-Originating-IP: [68.118.231.30]
In-Reply-To: <CAAqHKj5u79OOZ2nhFLY30EKSAEWVitbOkXp7B+J7jBZzFfn+4Q@mail.gmail.com>
References: <CAKYY9AKJNFonyWw5UeqsnKvze10DauEHgq3nAEtm--yBUbxbCw@mail.gmail.com>
	<CAAqHKj5u79OOZ2nhFLY30EKSAEWVitbOkXp7B+J7jBZzFfn+4Q@mail.gmail.com>
Date: Fri, 11 Apr 2014 22:05:56 -0400
Message-ID: <CAKYY9AJaGv35Nh_Yd_sGhwVn=+kpam9GoxEyLdFTMtdP2LbFTg@mail.gmail.com>
Subject: Re: Building Spark AMI
From: Jim Ancona <jim@anconafamily.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

Right now my use case is setting up a small cluster for
prototyping/evaluation. My hope was that I could use the scripts that
come with Spark to get things up and running quickly. For a production
deploy we would probably roll our own using Puppet.

Jim

On Fri, Apr 11, 2014 at 7:58 PM, Mayur Rustagi <mayur.rustagi@gmail.com> wrote:
> I am creating one fully configured & synced one. But you still need to send
> over configuration. Do you plan to use chef for that ?
>  On Apr 10, 2014 6:58 PM, "Jim Ancona" <jim@anconafamily.com> wrote:
>
>> Are there scripts to build the AMI used by the spark-ec2 script?
>>
>> Alternatively, is there a place to download the AMI. I'm interested in
>> using it to deploy into an internal Openstack cloud.
>>
>> Thanks,
>>
>> Jim
>>

From dev-return-7330-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Apr 12 12:37:31 2014
Return-Path: <dev-return-7330-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D1ABF11F0F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 12 Apr 2014 12:37:31 +0000 (UTC)
Received: (qmail 88964 invoked by uid 500); 12 Apr 2014 12:37:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88527 invoked by uid 500); 12 Apr 2014 12:37:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88480 invoked by uid 99); 12 Apr 2014 12:37:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 12 Apr 2014 12:37:23 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nick.pentreath@gmail.com designates 209.85.219.51 as permitted sender)
Received: from [209.85.219.51] (HELO mail-oa0-f51.google.com) (209.85.219.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 12 Apr 2014 12:37:19 +0000
Received: by mail-oa0-f51.google.com with SMTP id i4so7333057oah.24
        for <dev@spark.apache.org>; Sat, 12 Apr 2014 05:36:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=ePdurAsYnjS7lp3TLupHX2tUC9zdfytYcmOK4UT/WIw=;
        b=PrV1gdECyF8V8eWQh58WV2PpYB8dBwCPGCs7ShPhNQa7LK0CQot1pNt7Sx/8gPeecH
         Jf7GeQOzE57XQ4OkSVpUsLgRXoEmcoS7K81LmCS1Qo8W5Th+g/UjkCbzNkkwQwkL8Z5B
         oXJggD54SZoXKcIG+y4jWHosk5Pj0FGx6myfixXntDkUxUoyxTcM/wJmJrfldz2kgyml
         vIbO/d9tfjaBai3AV+vmt6dVXURvpCtRjh6qN6iSpOTYBO38XIGbkQ/5MSm7fF7yQBw0
         CgKWjqKYKg7EmmMd1t3LX6D3S6Oi8WGZD9RCmlI330UnlLuZllV3J9DKmTtF1f/3noiM
         n3gw==
MIME-Version: 1.0
X-Received: by 10.182.66.202 with SMTP id h10mr24741519obt.38.1397306218939;
 Sat, 12 Apr 2014 05:36:58 -0700 (PDT)
Received: by 10.182.95.103 with HTTP; Sat, 12 Apr 2014 05:36:58 -0700 (PDT)
In-Reply-To: <CA+B-+fxOXH4mdJM+e1d9D=2bxn3HzCvKuDeW8oqEeveg+55X2Q@mail.gmail.com>
References: <7841A4E0A9C8784C9D8B07DF5E48F0A811656DB6@SHSMSX104.ccr.corp.intel.com>
	<CADWPM3jwcqhmDsNNspBn0YP6Bzvmxe22P442o=0b3+xcyh8G7g@mail.gmail.com>
	<51F0296D-3996-4DAD-9548-AD6444DA435D@gmail.com>
	<CA+B-+fxfghzWOeiAvcdOyHmP0hUenAZ7Xk+uuE3MStV+8r3uNw@mail.gmail.com>
	<CA+B-+fxQqzMDwt1FpBUK7zCp3mz10LQmN2p2X-JA4q_PtA3N2g@mail.gmail.com>
	<CAJgQjQ8QU02qKe22-VSzEjjAX+=NFy=U=Ew4EZ8XZPH2UgDBtA@mail.gmail.com>
	<CAJgQjQ_1O-DfmhCUHTJy+=9K0mCXswN3PK_jKryXCjqveDwv6g@mail.gmail.com>
	<CA+B-+fxOXH4mdJM+e1d9D=2bxn3HzCvKuDeW8oqEeveg+55X2Q@mail.gmail.com>
Date: Sat, 12 Apr 2014 14:36:58 +0200
Message-ID: <CALD+6GM2UWthqG9jCGkedf0FpzYpC+p3pU_782fMatpC49SJ4g@mail.gmail.com>
Subject: Re: Any suggestion about JIRA 1006 "MLlib ALS gets stack overflow
 with too many iterations"?
From: Nick Pentreath <nick.pentreath@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0160c34ed28ac404f6d7b4c1
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160c34ed28ac404f6d7b4c1
Content-Type: text/plain; charset=ISO-8859-1

I think having the option of seeding the factors from HDFS rather than
random is a good one (well, actually providing additional optional
arguments initialUserFactors and initialItemFactors as RDD[(Int,
Array[Double])])


On Mon, Apr 7, 2014 at 8:09 AM, Debasish Das <debasish.das83@gmail.com>wrote:

> Sorry not persist...I meant adding a user parameter k which does checkpoint
> after every k iterations...out of N ALS iterations...We have hdfs installed
> so not a big deal...is there an issue of adding this user parameter in
> ALS.scala ? If it is then I can add it to our internal branch...
>
> For me tipping k seems like 4...With 4 iterations I can write out the
> factors...if I run with 10 iterations, after 4 I can see that it restarts
> the sparse matrix partition...tries to run all the iterations over again
> and fails due to array index out of bound which does not seems like a real
> bug...
>
> Not sure if it can be reproduced in movielens as the dataset I have is 25M
> x 3M (and counting)...whille movielens is tall and thin....
>
> Another idea would be to give an option to restart ALS with previous
> factors...that way ALS core algorithm does not need to change and it might
> be more useful...and that way we can point to a location from where the old
> factors can be load...I think @sean used similar idea in Oryx
> generations...
>
> Let me know which way you guys prefer....I can add it in...
>
>
>
>
> On Sun, Apr 6, 2014 at 9:15 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
> > Btw, explicit ALS doesn't need persist because each intermediate
> > factor is only used once. -Xiangrui
> >
> > On Sun, Apr 6, 2014 at 9:13 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
> > > The persist used in implicit ALS doesn't help StackOverflow problem.
> > > Persist doesn't cut lineage. We need to call count() and then
> > > checkpoint() to cut the lineage. Did you try the workaround mentioned
> > > in https://issues.apache.org/jira/browse/SPARK-958:
> > >
> > > "I tune JVM thread stack size to 512k via option -Xss512k and it
> works."
> > >
> > > Best,
> > > Xiangrui
> > >
> > > On Sun, Apr 6, 2014 at 10:21 AM, Debasish Das <
> debasish.das83@gmail.com>
> > wrote:
> > >> At the head I see persist option in implicitPrefs but more cases like
> > the
> > >> ones mentioned above why don't we use similar technique and take an
> > input
> > >> that which iteration should we persist in explicit runs as well ?
> > >>
> > >> for (iter <- 1 to iterations) {
> > >>         // perform ALS update
> > >>         logInfo("Re-computing I given U (Iteration
> %d/%d)".format(iter,
> > >> iterations))
> > >>         products = updateFeatures(users, userOutLinks, productInLinks,
> > >> partitioner, rank, lambda,
> > >>           alpha, YtY = None)
> > >>         logInfo("Re-computing U given I (Iteration
> %d/%d)".format(iter,
> > >> iterations))
> > >>         users = updateFeatures(products, productOutLinks, userInLinks,
> > >> partitioner, rank, lambda,
> > >>           alpha, YtY = None)
> > >>       }
> > >>
> > >> Say if I want to persist at every k iterations out of N iterations of
> > ALS
> > >> explicit, there shoud be an option to do that...implicit right now
> uses
> > >> persist at each iteration...
> > >>
> > >> Does this option make sense or you guys want this issue to be fixed
> in a
> > >> different way...
> > >>
> > >> I definitely see that for my 25M x 3M run, with 64 gb executor memory,
> > >> something is going wrong after 5-th iteration and I wanted to run for
> 10
> > >> iterations...
> > >>
> > >> So my k is 4/5 for this particular problem...
> > >>
> > >> I can ask for the PR after testing the fix on the dataset I have...I
> > will
> > >> also try to see if we can make such datasets public for more
> research...
> > >>
> > >> For the LDA problem mentioned earlier in this email chain, k is
> 10...NMF
> > >> can generate topics similar to LDA as well...Carrot2 project uses
> it...
> > >>
> > >>
> > >>
> > >> On Thu, Mar 27, 2014 at 3:20 PM, Debasish Das <
> debasish.das83@gmail.com
> > >wrote:
> > >>
> > >>> Hi Matei,
> > >>>
> > >>> I am hitting similar problems with 10 ALS iterations...I am running
> > with
> > >>> 24 gb executor memory on 10 nodes for 20M x 3 M matrix with rank =50
> > >>>
> > >>> The first iteration of flatMaps run fine which means that the memory
> > >>> requirements are good per iteration...
> > >>>
> > >>> If I do check-pointing on RDD, most likely rest 9 iterations will
> also
> > run
> > >>> fine and I will get the results...
> > >>>
> > >>> Is there a plan to add checkpoint option to ALS for such large
> > >>> factorization jobs ?
> > >>>
> > >>> Thanks.
> > >>> Deb
> > >>>
> > >>>
> > >>>
> > >>>
> > >>>
> > >>> On Tue, Jan 28, 2014 at 11:10 PM, Matei Zaharia <
> > matei.zaharia@gmail.com>wrote:
> > >>>
> > >>>> That would be great to add. Right now it would be easy to change it
> to
> > >>>> use another Hadoop FileSystem implementation at the very least (I
> > think you
> > >>>> can just pass the URL for that), but for Cassandra you'd have to
> use a
> > >>>> different InputFormat or some direct Cassandra access API.
> > >>>>
> > >>>> Matei
> > >>>>
> > >>>> On Jan 28, 2014, at 5:02 PM, Evan Chan <ev@ooyala.com> wrote:
> > >>>>
> > >>>> > By the way, is there any plan to make a pluggable backend for
> > >>>> > checkpointing?   We might be interested in writing a, for example,
> > >>>> > Cassandra backend.
> > >>>> >
> > >>>> > On Sat, Jan 25, 2014 at 9:49 PM, Xia, Junluan <
> > junluan.xia@intel.com>
> > >>>> wrote:
> > >>>> >> Hi all
> > >>>> >>
> > >>>> >> The description about this Bug submitted by Matei is as following
> > >>>> >>
> > >>>> >>
> > >>>> >> The tipping point seems to be around 50. We should fix this by
> > >>>> checkpointing the RDDs every 10-20 iterations to break the lineage
> > chain,
> > >>>> but checkpointing currently requires HDFS installed, which not all
> > users
> > >>>> will have.
> > >>>> >>
> > >>>> >> We might also be able to fix DAGScheduler to not be recursive.
> > >>>> >>
> > >>>> >>
> > >>>> >> regards,
> > >>>> >> Andrew
> > >>>> >>
> > >>>> >
> > >>>> >
> > >>>> >
> > >>>> > --
> > >>>> > --
> > >>>> > Evan Chan
> > >>>> > Staff Engineer
> > >>>> > ev@ooyala.com  |
> > >>>>
> > >>>>
> > >>>
> >
>

--089e0160c34ed28ac404f6d7b4c1--

From dev-return-7331-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 14 10:15:25 2014
Return-Path: <dev-return-7331-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1EAD711558
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Apr 2014 10:15:25 +0000 (UTC)
Received: (qmail 33386 invoked by uid 500); 14 Apr 2014 10:15:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32967 invoked by uid 500); 14 Apr 2014 10:15:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32724 invoked by uid 99); 14 Apr 2014 10:15:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Apr 2014 10:15:18 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of advancedxy@gmail.com designates 209.85.192.178 as permitted sender)
Received: from [209.85.192.178] (HELO mail-pd0-f178.google.com) (209.85.192.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Apr 2014 10:15:12 +0000
Received: by mail-pd0-f178.google.com with SMTP id x10so7938407pdj.9
        for <dev@spark.apache.org>; Mon, 14 Apr 2014 03:14:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:subject:mime-version:content-type;
        bh=qk6OQPCrGb8yFySvdR5S2EVnGofse+0KpnVlkcF1yBo=;
        b=dorW2ocT7GI2jU/F1d2s6/1XvvzcLu4GbQFHMjZLfxhSZiyzZsU449cG3mFy/U44eN
         7fkEiMIFYJrHOAWRsq/JoEtKinbV2NSE18oDKh/o8N1p5KoOWoL+65irdqbA3ny0J8jQ
         vtR74ddKWblVIez9gB1jUT0f48zK6HxsIOTc4IgPeK3JoREoCK+E4AzMGoBxusIj3wsk
         XgwqmnqQ7hB/DKKgkxtTe14uOFMsxYn4zT2LDWYbTdteyMXWnNFCjmpbZCO52Oo899va
         yZkWTOO4Q/ezm9K27E1O+mBMoPL1RvJi0S29Vp0MmfOS3HqBKQbHOvebz5N7uD2ISwJn
         IvcA==
X-Received: by 10.68.129.34 with SMTP id nt2mr43225719pbb.18.1397470489845;
        Mon, 14 Apr 2014 03:14:49 -0700 (PDT)
Received: from [10.240.138.131] ([123.58.191.68])
        by mx.google.com with ESMTPSA id nc1sm32873613pbc.32.2014.04.14.03.14.45
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Mon, 14 Apr 2014 03:14:48 -0700 (PDT)
Date: Mon, 14 Apr 2014 18:14:45 +0800
From: Ye Xianjin <advancedxy@gmail.com>
To: dev@spark.apache.org
Message-ID: <5C3428E1B84C4E09BC5A79CAF0A10FDA@gmail.com>
Subject: Tests failed after assembling the latest code from github
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="534bb515_1a27709e_139"
X-Virus-Checked: Checked by ClamAV on apache.org

--534bb515_1a27709e_139
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

Hi, everyone: 
I am new to Spark development. I download spark's latest code from github. After running sbt/sbt assembly,
I began running  sbt/sbt test in the spark source code dir. But it failed running the repl module test.

Here are some output details.

command:
sbt/sbt "test-only org.apache.spark.repl.*"
output:

[info] Loading project definition from /Volumes/MacintoshHD/github/spark/project/project
[info] Loading project definition from /Volumes/MacintoshHD/github/spark/project
[info] Set current project to root (in build file:/Volumes/MacintoshHD/github/spark/)
[info] Passed: Total 0, Failed 0, Errors 0, Passed 0
[info] No tests to run for graphx/test:testOnly
[info] Passed: Total 0, Failed 0, Errors 0, Passed 0
[info] No tests to run for bagel/test:testOnly
[info] Passed: Total 0, Failed 0, Errors 0, Passed 0
[info] No tests to run for streaming/test:testOnly
[info] Passed: Total 0, Failed 0, Errors 0, Passed 0
[info] No tests to run for mllib/test:testOnly
[info] Passed: Total 0, Failed 0, Errors 0, Passed 0
[info] No tests to run for catalyst/test:testOnly
[info] Passed: Total 0, Failed 0, Errors 0, Passed 0
[info] No tests to run for core/test:testOnly
[info] Passed: Total 0, Failed 0, Errors 0, Passed 0
[info] No tests to run for assembly/test:testOnly
[info] Passed: Total 0, Failed 0, Errors 0, Passed 0
[info] No tests to run for sql/test:testOnly
[info] ExecutorClassLoaderSuite:
2014-04-14 16:59:31.247 java[8393:1003] Unable to load realm info from SCDynamicStore
[info] - child first *** FAILED *** (440 milliseconds)
[info]   java.lang.ClassNotFoundException: ReplFakeClass2
[info]   at java.lang.ClassLoader.findClass(ClassLoader.java:364)
[info]   at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26)
[info]   at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
[info]   at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
[info]   at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
[info]   at org.apache.spark.repl.ExecutorClassLoader$$anonfun$findClass$1.apply(ExecutorClassLoader.scala:57)
[info]   at org.apache.spark.repl.ExecutorClassLoader$$anonfun$findClass$1.apply(ExecutorClassLoader.scala:57)
[info]   at scala.Option.getOrElse(Option.scala:120)
[info]   at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:57)
[info]   at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
[info]   at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
[info]   at org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply$mcV$sp(ExecutorClassLoaderSuite.scala:47)
[info]   at org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply(ExecutorClassLoaderSuite.scala:44)
[info]   at org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply(ExecutorClassLoaderSuite.scala:44)
[info]   at org.scalatest.FunSuite$$anon$1.apply(FunSuite.scala:1265)
[info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1974)
[info]   at org.apache.spark.repl.ExecutorClassLoaderSuite.withFixture(ExecutorClassLoaderSuite.scala:30)
[info]   at org.scalatest.FunSuite$class.invokeWithFixture$1(FunSuite.scala:1262)
[info]   at org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)
[info]   at org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:198)
[info]   at org.scalatest.FunSuite$class.runTest(FunSuite.scala:1271)
[info]   at org.apache.spark.repl.ExecutorClassLoaderSuite.runTest(ExecutorClassLoaderSuite.scala:30)
[info]   at org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)
[info]   at org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)
[info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:260)
[info]   at org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:249)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:249)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:326)
[info]   at org.scalatest.FunSuite$class.runTests(FunSuite.scala:1304)
[info]   at org.apache.spark.repl.ExecutorClassLoaderSuite.runTests(ExecutorClassLoaderSuite.scala:30)
[info]   at org.scalatest.Suite$class.run(Suite.scala:2303)
[info]   at org.apache.spark.repl.ExecutorClassLoaderSuite.org$scalatest$FunSuite$$super$run(ExecutorClassLoaderSuite.scala:30)
[info]   at org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)
[info]   at org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:362)
[info]   at org.scalatest.FunSuite$class.run(FunSuite.scala:1310)
[info]   at org.apache.spark.repl.ExecutorClassLoaderSuite.org$scalatest$BeforeAndAfterAll$$super$run(ExecutorClassLoaderSuite.scala:30)
[info]   at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:213)
[info]   at org.apache.spark.repl.ExecutorClassLoaderSuite.run(ExecutorClassLoaderSuite.scala:30)
[info]   at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:214)
[info]   at sbt.RunnerWrapper$1.runRunner2(FrameworkWrapper.java:220)
[info]   at sbt.RunnerWrapper$1.execute(FrameworkWrapper.java:233)
[info]   at sbt.ForkMain$Run.runTest(ForkMain.java:243)
[info]   at sbt.ForkMain$Run.runTestSafe(ForkMain.java:214)
[info]   at sbt.ForkMain$Run.runTests(ForkMain.java:190)
[info]   at sbt.ForkMain$Run.run(ForkMain.java:257)
[info]   at sbt.ForkMain.main(ForkMain.java:99)
[info] - parent first *** FAILED *** (59 milliseconds)
[info]   java.lang.ClassNotFoundException: ReplFakeClass1
...
[info]   Cause: java.lang.ClassNotFoundException: ReplFakeClass1
...
[info] - child first can fall back *** FAILED *** (39 milliseconds)
[info]   java.lang.ClassNotFoundException: ReplFakeClass3
...
[info] - child first can fail (46 milliseconds)
[info] ReplSuite:
[info] - propagation of local properties (9 seconds, 353 milliseconds)
[info] - simple foreach with accumulator (7 seconds, 608 milliseconds)
[info] - external vars (5 seconds, 783 milliseconds)
[info] - external classes (4 seconds, 341 milliseconds)
[info] - external functions (4 seconds, 106 milliseconds)
[info] - external functions that access vars (4 seconds, 538 milliseconds)
[info] - broadcast vars (4 seconds, 155 milliseconds)
[info] - interacting with files (3 seconds, 376 milliseconds)
Exception in thread "Connection manager future execution context-0"


Some output is omitted.

Here are some more information:
ReplFakeClass1.class is in the {spark_source_dir}/repl/ReplFakeClass1.class, same as ReplFakeClass2 and 3.
ReplSuite failed in running test("local-cluster mode"). The first time running this test throws OOM error. The exception shown in above is a second try
The test("local-cluster mode") jvm options are '-Xms512M -Xmx512M' which I see from the corresponding stderr log
I have .sbtconfig file in my home dir.  The content is 
export SBT_OPTS="-XX:+CMSClassUnloadingEnabled -XX:PermSize=5120M -XX:MaxPermSize=10240M"


The test hung after the test failed in the ReplSuite. I have to Ctr-c to close the test.

Thank you for you advice.



-- 
Ye Xianjin
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)


--534bb515_1a27709e_139--


From dev-return-7332-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 14 15:15:12 2014
Return-Path: <dev-return-7332-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6D3F211F16
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Apr 2014 15:15:12 +0000 (UTC)
Received: (qmail 69184 invoked by uid 500); 14 Apr 2014 15:13:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68671 invoked by uid 500); 14 Apr 2014 15:13:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68404 invoked by uid 99); 14 Apr 2014 15:12:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Apr 2014 15:12:53 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of malouf.gary@gmail.com designates 209.85.216.52 as permitted sender)
Received: from [209.85.216.52] (HELO mail-qa0-f52.google.com) (209.85.216.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Apr 2014 15:12:49 +0000
Received: by mail-qa0-f52.google.com with SMTP id s7so6180863qap.11
        for <dev@spark.incubator.apache.org>; Mon, 14 Apr 2014 08:12:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=8U0/HqNNf3MwWBupQwdsUsU7KxKN2NW/iGnUuJwf3b8=;
        b=kooqQInnKA5OVm7oonFoU6mD/m+dF/c0Z9Qs0aKKSlNzFdiKtmqZuzzL0Xf///tzDo
         ux4K9OtsHBmBPwbE1tfhv37IOuCrtWd1oo7TSnNHxCpghTebXlMVf99N7sVVvHfRqZs9
         jNCb/CtEp8YggF9TG4DDW89Wjh+pkdQbeTcjLsxk1LS1ZRpPl4Byco+qbSltX36NeFJq
         0o+FvLS8ToTV9x739qjCvvWl3/3OdGlo/MP59hEhlwXHSnpsYJOg95ZxT8RY4xpffr49
         Hb5ZPnK+DBa4omE0HnDXFCtxjm50pyq4+Qd1SrZV02QNNFLhxcBhBby6h1Vn2HEj5Gyu
         Qqcw==
MIME-Version: 1.0
X-Received: by 10.224.16.69 with SMTP id n5mr47049374qaa.7.1397488340652; Mon,
 14 Apr 2014 08:12:20 -0700 (PDT)
Received: by 10.140.25.174 with HTTP; Mon, 14 Apr 2014 08:12:20 -0700 (PDT)
In-Reply-To: <CAM6vJxHaVocAZ4oceOtHg5k35CoOHpkbsyGj_zz=NH7daMgUKA@mail.gmail.com>
References: <CAPh_B=ass2NcrN41t7KTSoF1SFGce=N57YMVyukX4hPcO5YN2Q@mail.gmail.com>
	<CADWPM3hLv_76=K=nJFmLnDLv9UCUZpYpf+6f6WLd=5dru5ssNQ@mail.gmail.com>
	<CAM6vJxHaVocAZ4oceOtHg5k35CoOHpkbsyGj_zz=NH7daMgUKA@mail.gmail.com>
Date: Mon, 14 Apr 2014 11:12:20 -0400
Message-ID: <CAGOvqipiVjS9sd9sN=zVd8LV_1V2AcD=NANo15-Bo06iV_mCFA@mail.gmail.com>
Subject: Re: Akka problem when using scala command to launch Spark
 applications in the current 0.9.0-SNAPSHOT
From: Gary Malouf <malouf.gary@gmail.com>
To: dev@spark.incubator.apache.org
Cc: Evan Chan <ev@ooyala.com>, Tathagata Das <tdas@eecs.berkeley.edu>
Content-Type: multipart/alternative; boundary=047d7bea39861f585604f7021c74
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bea39861f585604f7021c74
Content-Type: text/plain; charset=UTF-8

Sorry to dig up an old issue.

We build an assembly against spark-0.9.0-RC3 to run on our Spark cluster on
top of Mesos.  When we upgraded to 0.9.0-RC3 from an earlier master cut
from November, we ran into Akka issues described above.

Is it supported to be able to deploy this jar using the Spark classpath
script and Java?  Putting Scala 2.10.3's libraries on the path seems to
break it at runtime.


On Tue, Dec 24, 2013 at 3:49 PM, Patrick Wendell <patrick@databricks.com>wrote:

> Even,
>
> This problem also exists for people who write their own applications that
> depend on/include Spark. E.g. they bundle up their app and then launch the
> driver with "scala -cp my-budle.jar"... I've seen this cause an issue in
> that setting.
>
> - Patrick
>
>
> On Tue, Dec 24, 2013 at 10:50 AM, Evan Chan <ev@ooyala.com> wrote:
>
> > Hi Reynold,
> >
> > The default, documented methods of starting Spark all use the assembly
> > jar, and thus java, right?
> >
> > -Evan
> >
> >
> >
> > On Fri, Dec 20, 2013 at 11:36 PM, Reynold Xin <rxin@databricks.com>
> wrote:
> >
> >> It took me hours to debug a problem yesterday on the latest master
> branch
> >> (0.9.0-SNAPSHOT), and I would like to share with the dev list in case
> >> anybody runs into this Akka problem.
> >>
> >> A little background for those of you who haven't followed closely the
> >> development of Spark and YARN 2.2: YARN 2.2 uses protobuf 2.5, and Akka
> >> uses an older version of protobuf that is not binary compatible. In
> order
> >> to have a single build that is compatible for both YARN 2.2 and pre-2.2
> >> YARN/Hadoop, we published a special version of Akka that builds with
> >> protobuf shaded (i.e. using a different package name for the protobuf
> >> stuff).
> >>
> >> However, it turned out Scala 2.10 includes a version of Akka jar in its
> >> default classpath (look at the lib folder in Scala 2.10 binary
> >> distribution). If you use the scala command to launch any Spark
> >> application
> >> on the current master branch, there is a pretty high chance that you
> >> wouldn't be able to create the SparkContext (stack trace at the end of
> the
> >> email). The problem is that the Akka packaged with Scala 2.10 takes
> >> precedence in the classloader over the special Akka version Spark
> >> includes.
> >>
> >> Before we have a good solution for this, the workaround is to use java
> to
> >> launch the application instead of scala. All you need to do is to
> include
> >> the right Scala jars (scala-library and scala-compiler) in the
> classpath.
> >> Note that the scala command is really just a simple script that calls
> java
> >> with the right classpath.
> >>
> >>
> >> Stack trace:
> >>
> >> java.lang.NoSuchMethodException:
> >> akka.remote.RemoteActorRefProvider.<init>(java.lang.String,
> >> akka.actor.ActorSystem$Settings, akka.event.EventStream,
> >> akka.actor.Scheduler, akka.actor.DynamicAccess)
> >> at java.lang.Class.getConstructor0(Class.java:2763)
> >> at java.lang.Class.getDeclaredConstructor(Class.java:2021)
> >> at
> >>
> >>
> akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$2.apply(DynamicAccess.scala:77)
> >> at scala.util.Try$.apply(Try.scala:161)
> >> at
> >>
> >>
> akka.actor.ReflectiveDynamicAccess.createInstanceFor(DynamicAccess.scala:74)
> >> at
> >>
> >>
> akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply(DynamicAccess.scala:85)
> >> at
> >>
> >>
> akka.actor.ReflectiveDynamicAccess$$anonfun$createInstanceFor$3.apply(DynamicAccess.scala:85)
> >> at scala.util.Success.flatMap(Try.scala:200)
> >> at
> >>
> >>
> akka.actor.ReflectiveDynamicAccess.createInstanceFor(DynamicAccess.scala:85)
> >> at akka.actor.ActorSystemImpl.<init>(ActorSystem.scala:546)
> >> at akka.actor.ActorSystem$.apply(ActorSystem.scala:111)
> >> at akka.actor.ActorSystem$.apply(ActorSystem.scala:104)
> >> at
> org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:79)
> >> at
> >>
> org.apache.spark.SparkEnv$.createFromSystemProperties(SparkEnv.scala:120)
> >> at org.apache.spark.SparkContext.<init>(SparkContext.scala:106)
> >>
> >
> >
> >
> > --
> > --
> > Evan Chan
> > Staff Engineer
> > ev@ooyala.com  |
> >
> > <http://www.ooyala.com/> <http://www.facebook.com/ooyala><
> http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>
> >
> >
>

--047d7bea39861f585604f7021c74--

From dev-return-7333-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 14 17:40:00 2014
Return-Path: <dev-return-7333-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1B3E810CF3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Apr 2014 17:40:00 +0000 (UTC)
Received: (qmail 90767 invoked by uid 500); 14 Apr 2014 17:39:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90685 invoked by uid 500); 14 Apr 2014 17:39:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90626 invoked by uid 99); 14 Apr 2014 17:39:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Apr 2014 17:39:53 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.50] (HELO mail-qa0-f50.google.com) (209.85.216.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Apr 2014 17:39:49 +0000
Received: by mail-qa0-f50.google.com with SMTP id ih12so8241321qab.37
        for <dev@spark.apache.org>; Mon, 14 Apr 2014 10:39:26 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=iMFurcDNDI3CKIHz5VGh6O/nTTJqdqewMWFMWMrsktg=;
        b=E4MtYZbOOPhI9s7Bft8ns9dsswTh65ke5ZLen0nWwaosPLP3HVv8GC5d3OW6M30opd
         dpR6QGE7CrCP8f+rrWqO4g3xxHCuulLrSU/f1HF5BJaLYkcmpitR7jBZBhJq82CiSWBc
         PIwgL9G6342M75Vg1MXM49H6i6oqyif1v79hnJGzyBHSRQxnEU0y8F/jilZOMqSrrJNI
         JuEb2RRQdjivBEBh3dyD3T0basrjg2ZbZGr2cuCVf1V1UUUyr2ms5eTOHg6T1sPQTgVb
         7TSOpHgAHZWcmVwzVHW8h+//OFFFkZitHeub++unyY3qIaNuVAaZNH5seViLq4l5e+Tg
         p/AA==
X-Gm-Message-State: ALoCoQnvyAgo2iMUBNDF6X5/o3pPIV5hL7zS8uHky/oRGwvr6ka88c3dGfQKKvx5XzTyKZ3naGig
X-Received: by 10.224.46.197 with SMTP id k5mr21664937qaf.15.1397497166288;
 Mon, 14 Apr 2014 10:39:26 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.68.2 with HTTP; Mon, 14 Apr 2014 10:39:06 -0700 (PDT)
In-Reply-To: <5C3428E1B84C4E09BC5A79CAF0A10FDA@gmail.com>
References: <5C3428E1B84C4E09BC5A79CAF0A10FDA@gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Mon, 14 Apr 2014 10:39:06 -0700
Message-ID: <CAAswR-4Wrbh82xPGXM3BoQmDfExBBro7U9w8hqTMpMDAyWnSNQ@mail.gmail.com>
Subject: Re: Tests failed after assembling the latest code from github
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c1f07e2beb3904f7042a2c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1f07e2beb3904f7042a2c
Content-Type: text/plain; charset=ISO-8859-1

I believe you may need an assembly jar to run the ReplSuite. "sbt/sbt
assembly/assembly".

Michael


On Mon, Apr 14, 2014 at 3:14 AM, Ye Xianjin <advancedxy@gmail.com> wrote:

> Hi, everyone:
> I am new to Spark development. I download spark's latest code from github.
> After running sbt/sbt assembly,
> I began running  sbt/sbt test in the spark source code dir. But it failed
> running the repl module test.
>
> Here are some output details.
>
> command:
> sbt/sbt "test-only org.apache.spark.repl.*"
> output:
>
> [info] Loading project definition from
> /Volumes/MacintoshHD/github/spark/project/project
> [info] Loading project definition from
> /Volumes/MacintoshHD/github/spark/project
> [info] Set current project to root (in build
> file:/Volumes/MacintoshHD/github/spark/)
> [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> [info] No tests to run for graphx/test:testOnly
> [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> [info] No tests to run for bagel/test:testOnly
> [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> [info] No tests to run for streaming/test:testOnly
> [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> [info] No tests to run for mllib/test:testOnly
> [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> [info] No tests to run for catalyst/test:testOnly
> [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> [info] No tests to run for core/test:testOnly
> [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> [info] No tests to run for assembly/test:testOnly
> [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> [info] No tests to run for sql/test:testOnly
> [info] ExecutorClassLoaderSuite:
> 2014-04-14 16:59:31.247 java[8393:1003] Unable to load realm info from
> SCDynamicStore
> [info] - child first *** FAILED *** (440 milliseconds)
> [info]   java.lang.ClassNotFoundException: ReplFakeClass2
> [info]   at java.lang.ClassLoader.findClass(ClassLoader.java:364)
> [info]   at
> org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26)
> [info]   at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
> [info]   at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
> [info]   at
> org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
> [info]   at
> org.apache.spark.repl.ExecutorClassLoader$$anonfun$findClass$1.apply(ExecutorClassLoader.scala:57)
> [info]   at
> org.apache.spark.repl.ExecutorClassLoader$$anonfun$findClass$1.apply(ExecutorClassLoader.scala:57)
> [info]   at scala.Option.getOrElse(Option.scala:120)
> [info]   at
> org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:57)
> [info]   at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
> [info]   at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
> [info]   at
> org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply$mcV$sp(ExecutorClassLoaderSuite.scala:47)
> [info]   at
> org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply(ExecutorClassLoaderSuite.scala:44)
> [info]   at
> org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply(ExecutorClassLoaderSuite.scala:44)
> [info]   at org.scalatest.FunSuite$$anon$1.apply(FunSuite.scala:1265)
> [info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1974)
> [info]   at
> org.apache.spark.repl.ExecutorClassLoaderSuite.withFixture(ExecutorClassLoaderSuite.scala:30)
> [info]   at
> org.scalatest.FunSuite$class.invokeWithFixture$1(FunSuite.scala:1262)
> [info]   at
> org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)
> [info]   at
> org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)
> [info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:198)
> [info]   at org.scalatest.FunSuite$class.runTest(FunSuite.scala:1271)
> [info]   at
> org.apache.spark.repl.ExecutorClassLoaderSuite.runTest(ExecutorClassLoaderSuite.scala:30)
> [info]   at
> org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)
> [info]   at
> org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)
> [info]   at
> org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:260)
> [info]   at
> org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:249)
> [info]   at scala.collection.immutable.List.foreach(List.scala:318)
> [info]   at org.scalatest.SuperEngine.org
> $scalatest$SuperEngine$$runTestsInBranch(Engine.scala:249)
> [info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:326)
> [info]   at org.scalatest.FunSuite$class.runTests(FunSuite.scala:1304)
> [info]   at
> org.apache.spark.repl.ExecutorClassLoaderSuite.runTests(ExecutorClassLoaderSuite.scala:30)
> [info]   at org.scalatest.Suite$class.run(Suite.scala:2303)
> [info]   at org.apache.spark.repl.ExecutorClassLoaderSuite.org
> $scalatest$FunSuite$$super$run(ExecutorClassLoaderSuite.scala:30)
> [info]   at
> org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)
> [info]   at
> org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)
> [info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:362)
> [info]   at org.scalatest.FunSuite$class.run(FunSuite.scala:1310)
> [info]   at org.apache.spark.repl.ExecutorClassLoaderSuite.org
> $scalatest$BeforeAndAfterAll$$super$run(ExecutorClassLoaderSuite.scala:30)
> [info]   at
> org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:213)
> [info]   at
> org.apache.spark.repl.ExecutorClassLoaderSuite.run(ExecutorClassLoaderSuite.scala:30)
> [info]   at
> org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:214)
> [info]   at sbt.RunnerWrapper$1.runRunner2(FrameworkWrapper.java:220)
> [info]   at sbt.RunnerWrapper$1.execute(FrameworkWrapper.java:233)
> [info]   at sbt.ForkMain$Run.runTest(ForkMain.java:243)
> [info]   at sbt.ForkMain$Run.runTestSafe(ForkMain.java:214)
> [info]   at sbt.ForkMain$Run.runTests(ForkMain.java:190)
> [info]   at sbt.ForkMain$Run.run(ForkMain.java:257)
> [info]   at sbt.ForkMain.main(ForkMain.java:99)
> [info] - parent first *** FAILED *** (59 milliseconds)
> [info]   java.lang.ClassNotFoundException: ReplFakeClass1
> ...
> [info]   Cause: java.lang.ClassNotFoundException: ReplFakeClass1
> ...
> [info] - child first can fall back *** FAILED *** (39 milliseconds)
> [info]   java.lang.ClassNotFoundException: ReplFakeClass3
> ...
> [info] - child first can fail (46 milliseconds)
> [info] ReplSuite:
> [info] - propagation of local properties (9 seconds, 353 milliseconds)
> [info] - simple foreach with accumulator (7 seconds, 608 milliseconds)
> [info] - external vars (5 seconds, 783 milliseconds)
> [info] - external classes (4 seconds, 341 milliseconds)
> [info] - external functions (4 seconds, 106 milliseconds)
> [info] - external functions that access vars (4 seconds, 538 milliseconds)
> [info] - broadcast vars (4 seconds, 155 milliseconds)
> [info] - interacting with files (3 seconds, 376 milliseconds)
> Exception in thread "Connection manager future execution context-0"
>
>
> Some output is omitted.
>
> Here are some more information:
> ReplFakeClass1.class is in the
> {spark_source_dir}/repl/ReplFakeClass1.class, same as ReplFakeClass2 and 3.
> ReplSuite failed in running test("local-cluster mode"). The first time
> running this test throws OOM error. The exception shown in above is a
> second try
> The test("local-cluster mode") jvm options are '-Xms512M -Xmx512M' which I
> see from the corresponding stderr log
> I have .sbtconfig file in my home dir.  The content is
> export SBT_OPTS="-XX:+CMSClassUnloadingEnabled -XX:PermSize=5120M
> -XX:MaxPermSize=10240M"
>
>
> The test hung after the test failed in the ReplSuite. I have to Ctr-c to
> close the test.
>
> Thank you for you advice.
>
>
>
> --
> Ye Xianjin
> Sent with Sparrow (http://www.sparrowmailapp.com/?sig)
>
>

--001a11c1f07e2beb3904f7042a2c--

From dev-return-7334-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 14 19:04:51 2014
Return-Path: <dev-return-7334-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 28A1211413
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Apr 2014 19:04:51 +0000 (UTC)
Received: (qmail 93438 invoked by uid 500); 14 Apr 2014 19:04:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93382 invoked by uid 500); 14 Apr 2014 19:04:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93372 invoked by uid 99); 14 Apr 2014 19:04:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Apr 2014 19:04:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of advancedxy@gmail.com designates 209.85.160.46 as permitted sender)
Received: from [209.85.160.46] (HELO mail-pb0-f46.google.com) (209.85.160.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Apr 2014 19:04:44 +0000
Received: by mail-pb0-f46.google.com with SMTP id rq2so8617081pbb.33
        for <dev@spark.apache.org>; Mon, 14 Apr 2014 12:04:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=citJ6UAefnLH6dowrsK1S6SqKLjjeEY85Jdmt0A/LIQ=;
        b=tbm7uFPfUa94RdGzUklxZXesst9mc8kM2me0uRJTEmWMZ2DGN7YcvzyNIBzAfrE7yn
         T0Ml9OX1SrLvMrF6hx3gTAhO0g6PYbQce2YpKPTE+lLEvv1LAOuwuMm6LlAOxczF6KEe
         JgnDO+SDCmjptTZ06zOX4Jzgtd+HpvOKbM0DyWIb0xWRMrbT+63qlMYNmzZtBX0HWEOS
         EXV/KOWCpJuf25U/NIl+bPGfiByb2cQgmMWsuliMOq2ZCJcuPLhs1Vw9+IXBKEZkNlJM
         janZrvTWsZgeS7mj2AdjmGeITe0saZz0REwIG377erJrVyWbefPKkOhNv22zsvmXifyD
         YWeA==
X-Received: by 10.67.30.168 with SMTP id kf8mr46177369pad.84.1397502261380;
        Mon, 14 Apr 2014 12:04:21 -0700 (PDT)
Received: from [192.168.1.102] ([115.195.177.211])
        by mx.google.com with ESMTPSA id ha11sm35392652pbd.17.2014.04.14.12.04.17
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Mon, 14 Apr 2014 12:04:20 -0700 (PDT)
Date: Tue, 15 Apr 2014 03:04:15 +0800
From: Ye Xianjin <advancedxy@gmail.com>
To: dev@spark.apache.org
Message-ID: <E9FE091E16764429B3F1D61D4D77E35F@gmail.com>
In-Reply-To: <CAAswR-4Wrbh82xPGXM3BoQmDfExBBro7U9w8hqTMpMDAyWnSNQ@mail.gmail.com>
References: <5C3428E1B84C4E09BC5A79CAF0A10FDA@gmail.com>
 <CAAswR-4Wrbh82xPGXM3BoQmDfExBBro7U9w8hqTMpMDAyWnSNQ@mail.gmail.com>
Subject: Re: Tests failed after assembling the latest code from github
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="534c312f_7fb7e0aa_139"
X-Virus-Checked: Checked by ClamAV on apache.org

--534c312f_7fb7e0aa_139
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

Thank you for your reply. 

After building the assembly jar, the repl test still failed. The error output is same as I post before. 

-- 
Ye Xianjin
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)


On Tuesday, April 15, 2014 at 1:39 AM, Michael Armbrust wrote:

> I believe you may need an assembly jar to run the ReplSuite. "sbt/sbt
> assembly/assembly".
> 
> Michael
> 
> 
> On Mon, Apr 14, 2014 at 3:14 AM, Ye Xianjin <advancedxy@gmail.com (mailto:advancedxy@gmail.com)> wrote:
> 
> > Hi, everyone:
> > I am new to Spark development. I download spark's latest code from github.
> > After running sbt/sbt assembly,
> > I began running sbt/sbt test in the spark source code dir. But it failed
> > running the repl module test.
> > 
> > Here are some output details.
> > 
> > command:
> > sbt/sbt "test-only org.apache.spark.repl.*"
> > output:
> > 
> > [info] Loading project definition from
> > /Volumes/MacintoshHD/github/spark/project/project
> > [info] Loading project definition from
> > /Volumes/MacintoshHD/github/spark/project
> > [info] Set current project to root (in build
> > file:/Volumes/MacintoshHD/github/spark/)
> > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > [info] No tests to run for graphx/test:testOnly
> > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > [info] No tests to run for bagel/test:testOnly
> > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > [info] No tests to run for streaming/test:testOnly
> > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > [info] No tests to run for mllib/test:testOnly
> > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > [info] No tests to run for catalyst/test:testOnly
> > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > [info] No tests to run for core/test:testOnly
> > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > [info] No tests to run for assembly/test:testOnly
> > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > [info] No tests to run for sql/test:testOnly
> > [info] ExecutorClassLoaderSuite:
> > 2014-04-14 16:59:31.247 java[8393:1003] Unable to load realm info from
> > SCDynamicStore
> > [info] - child first *** FAILED *** (440 milliseconds)
> > [info] java.lang.ClassNotFoundException: ReplFakeClass2
> > [info] at java.lang.ClassLoader.findClass(ClassLoader.java:364)
> > [info] at
> > org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26)
> > [info] at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
> > [info] at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
> > [info] at
> > org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
> > [info] at
> > org.apache.spark.repl.ExecutorClassLoader$$anonfun$findClass$1.apply(ExecutorClassLoader.scala:57)
> > [info] at
> > org.apache.spark.repl.ExecutorClassLoader$$anonfun$findClass$1.apply(ExecutorClassLoader.scala:57)
> > [info] at scala.Option.getOrElse(Option.scala:120)
> > [info] at
> > org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:57)
> > [info] at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
> > [info] at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
> > [info] at
> > org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply$mcV$sp(ExecutorClassLoaderSuite.scala:47)
> > [info] at
> > org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply(ExecutorClassLoaderSuite.scala:44)
> > [info] at
> > org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply(ExecutorClassLoaderSuite.scala:44)
> > [info] at org.scalatest.FunSuite$$anon$1.apply(FunSuite.scala:1265)
> > [info] at org.scalatest.Suite$class.withFixture(Suite.scala:1974)
> > [info] at
> > org.apache.spark.repl.ExecutorClassLoaderSuite.withFixture(ExecutorClassLoaderSuite.scala:30)
> > [info] at
> > org.scalatest.FunSuite$class.invokeWithFixture$1(FunSuite.scala:1262)
> > [info] at
> > org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)
> > [info] at
> > org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)
> > [info] at org.scalatest.SuperEngine.runTestImpl(Engine.scala:198)
> > [info] at org.scalatest.FunSuite$class.runTest(FunSuite.scala:1271)
> > [info] at
> > org.apache.spark.repl.ExecutorClassLoaderSuite.runTest(ExecutorClassLoaderSuite.scala:30)
> > [info] at
> > org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)
> > [info] at
> > org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)
> > [info] at
> > org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:260)
> > [info] at
> > org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:249)
> > [info] at scala.collection.immutable.List.foreach(List.scala:318)
> > [info] at org.scalatest.SuperEngine.org (http://org.scalatest.SuperEngine.org)
> > $scalatest$SuperEngine$$runTestsInBranch(Engine.scala:249)
> > [info] at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:326)
> > [info] at org.scalatest.FunSuite$class.runTests(FunSuite.scala:1304)
> > [info] at
> > org.apache.spark.repl.ExecutorClassLoaderSuite.runTests(ExecutorClassLoaderSuite.scala:30)
> > [info] at org.scalatest.Suite$class.run(Suite.scala:2303)
> > [info] at org.apache.spark.repl.ExecutorClassLoaderSuite.org (http://org.apache.spark.repl.ExecutorClassLoaderSuite.org)
> > $scalatest$FunSuite$$super$run(ExecutorClassLoaderSuite.scala:30)
> > [info] at
> > org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)
> > [info] at
> > org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)
> > [info] at org.scalatest.SuperEngine.runImpl(Engine.scala:362)
> > [info] at org.scalatest.FunSuite$class.run(FunSuite.scala:1310)
> > [info] at org.apache.spark.repl.ExecutorClassLoaderSuite.org (http://org.apache.spark.repl.ExecutorClassLoaderSuite.org)
> > $scalatest$BeforeAndAfterAll$$super$run(ExecutorClassLoaderSuite.scala:30)
> > [info] at
> > org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:213)
> > [info] at
> > org.apache.spark.repl.ExecutorClassLoaderSuite.run(ExecutorClassLoaderSuite.scala:30)
> > [info] at
> > org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:214)
> > [info] at sbt.RunnerWrapper$1.runRunner2(FrameworkWrapper.java:220)
> > [info] at sbt.RunnerWrapper$1.execute(FrameworkWrapper.java:233)
> > [info] at sbt.ForkMain$Run.runTest(ForkMain.java:243)
> > [info] at sbt.ForkMain$Run.runTestSafe(ForkMain.java:214)
> > [info] at sbt.ForkMain$Run.runTests(ForkMain.java:190)
> > [info] at sbt.ForkMain$Run.run(ForkMain.java:257)
> > [info] at sbt.ForkMain.main(ForkMain.java:99)
> > [info] - parent first *** FAILED *** (59 milliseconds)
> > [info] java.lang.ClassNotFoundException: ReplFakeClass1
> > ...
> > [info] Cause: java.lang.ClassNotFoundException: ReplFakeClass1
> > ...
> > [info] - child first can fall back *** FAILED *** (39 milliseconds)
> > [info] java.lang.ClassNotFoundException: ReplFakeClass3
> > ...
> > [info] - child first can fail (46 milliseconds)
> > [info] ReplSuite:
> > [info] - propagation of local properties (9 seconds, 353 milliseconds)
> > [info] - simple foreach with accumulator (7 seconds, 608 milliseconds)
> > [info] - external vars (5 seconds, 783 milliseconds)
> > [info] - external classes (4 seconds, 341 milliseconds)
> > [info] - external functions (4 seconds, 106 milliseconds)
> > [info] - external functions that access vars (4 seconds, 538 milliseconds)
> > [info] - broadcast vars (4 seconds, 155 milliseconds)
> > [info] - interacting with files (3 seconds, 376 milliseconds)
> > Exception in thread "Connection manager future execution context-0"
> > 
> > 
> > Some output is omitted.
> > 
> > Here are some more information:
> > ReplFakeClass1.class is in the
> > {spark_source_dir}/repl/ReplFakeClass1.class, same as ReplFakeClass2 and 3.
> > ReplSuite failed in running test("local-cluster mode"). The first time
> > running this test throws OOM error. The exception shown in above is a
> > second try
> > The test("local-cluster mode") jvm options are '-Xms512M -Xmx512M' which I
> > see from the corresponding stderr log
> > I have .sbtconfig file in my home dir. The content is
> > export SBT_OPTS="-XX:+CMSClassUnloadingEnabled -XX:PermSize=5120M
> > -XX:MaxPermSize=10240M"
> > 
> > 
> > The test hung after the test failed in the ReplSuite. I have to Ctr-c to
> > close the test.
> > 
> > Thank you for you advice.
> > 
> > 
> > 
> > --
> > Ye Xianjin
> > Sent with Sparrow (http://www.sparrowmailapp.com/?sig)
> > 
> 
> 
> 



--534c312f_7fb7e0aa_139--


From dev-return-7335-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 14 19:14:58 2014
Return-Path: <dev-return-7335-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B2A141148A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Apr 2014 19:14:58 +0000 (UTC)
Received: (qmail 25726 invoked by uid 500); 14 Apr 2014 19:14:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25623 invoked by uid 500); 14 Apr 2014 19:14:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25602 invoked by uid 99); 14 Apr 2014 19:14:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Apr 2014 19:14:56 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ilikerps@gmail.com designates 209.85.216.174 as permitted sender)
Received: from [209.85.216.174] (HELO mail-qc0-f174.google.com) (209.85.216.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Apr 2014 19:14:51 +0000
Received: by mail-qc0-f174.google.com with SMTP id c9so9179189qcz.19
        for <dev@spark.apache.org>; Mon, 14 Apr 2014 12:14:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=fqC0gYexAXsqZmItzYeBaDuCKzvzyM3N6ntgRndLXPQ=;
        b=mAtktwftwTYw/vN0ItIKlrlG/wlDVJNfcLInnQfnubrn5qHgXtKL/Axt2EyFFnG5Ge
         n2Dp/I2ttwufvYR6Ly7JPn3XQymIcX2jvheKetyL01kIQMZZPpIOpOyJ5ECuoTNBcQm/
         WJp7tLAvSJhp+dtN31hMWbPjpb//NVWZIoosVok4NEslCGP1qac72mLqxhz5W1zlJEjx
         r8C2fVvFZlJ5151R34+Uq1DgDqiH7Tw2ct4rN6KTiJjw7JwOKuz7ltU70kzNze74+Iqg
         io7RyWgb9xUK7sIpnXkG6NluS7ZEakZWNFk4XWVekS/lJIWMzj4v4RgDFmhxb2H2HwMy
         1irQ==
X-Received: by 10.224.57.72 with SMTP id b8mr21923013qah.41.1397502869211;
 Mon, 14 Apr 2014 12:14:29 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.37.170 with HTTP; Mon, 14 Apr 2014 12:14:09 -0700 (PDT)
In-Reply-To: <E9FE091E16764429B3F1D61D4D77E35F@gmail.com>
References: <5C3428E1B84C4E09BC5A79CAF0A10FDA@gmail.com> <CAAswR-4Wrbh82xPGXM3BoQmDfExBBro7U9w8hqTMpMDAyWnSNQ@mail.gmail.com>
 <E9FE091E16764429B3F1D61D4D77E35F@gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Mon, 14 Apr 2014 12:14:09 -0700
Message-ID: <CANGvG8rTgNNWMb5fZ1JrTywnTTuU7X+Jbt6uuj9k7mt9Z4mogw@mail.gmail.com>
Subject: Re: Tests failed after assembling the latest code from github
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0149398217c1b004f7057e45
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149398217c1b004f7057e45
Content-Type: text/plain; charset=UTF-8

This may have something to do with running the tests on a Mac, as there is
a lot of File/URI/URL stuff going on in that test which may just have
happened to work if run on a Linux system (like Jenkins). Note that this
suite was added relatively recently:
https://github.com/apache/spark/pull/217


On Mon, Apr 14, 2014 at 12:04 PM, Ye Xianjin <advancedxy@gmail.com> wrote:

> Thank you for your reply.
>
> After building the assembly jar, the repl test still failed. The error
> output is same as I post before.
>
> --
> Ye Xianjin
> Sent with Sparrow (http://www.sparrowmailapp.com/?sig)
>
>
> On Tuesday, April 15, 2014 at 1:39 AM, Michael Armbrust wrote:
>
> > I believe you may need an assembly jar to run the ReplSuite. "sbt/sbt
> > assembly/assembly".
> >
> > Michael
> >
> >
> > On Mon, Apr 14, 2014 at 3:14 AM, Ye Xianjin <advancedxy@gmail.com(mailto:
> advancedxy@gmail.com)> wrote:
> >
> > > Hi, everyone:
> > > I am new to Spark development. I download spark's latest code from
> github.
> > > After running sbt/sbt assembly,
> > > I began running sbt/sbt test in the spark source code dir. But it
> failed
> > > running the repl module test.
> > >
> > > Here are some output details.
> > >
> > > command:
> > > sbt/sbt "test-only org.apache.spark.repl.*"
> > > output:
> > >
> > > [info] Loading project definition from
> > > /Volumes/MacintoshHD/github/spark/project/project
> > > [info] Loading project definition from
> > > /Volumes/MacintoshHD/github/spark/project
> > > [info] Set current project to root (in build
> > > file:/Volumes/MacintoshHD/github/spark/)
> > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > [info] No tests to run for graphx/test:testOnly
> > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > [info] No tests to run for bagel/test:testOnly
> > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > [info] No tests to run for streaming/test:testOnly
> > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > [info] No tests to run for mllib/test:testOnly
> > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > [info] No tests to run for catalyst/test:testOnly
> > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > [info] No tests to run for core/test:testOnly
> > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > [info] No tests to run for assembly/test:testOnly
> > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > [info] No tests to run for sql/test:testOnly
> > > [info] ExecutorClassLoaderSuite:
> > > 2014-04-14 16:59:31.247 java[8393:1003] Unable to load realm info from
> > > SCDynamicStore
> > > [info] - child first *** FAILED *** (440 milliseconds)
> > > [info] java.lang.ClassNotFoundException: ReplFakeClass2
> > > [info] at java.lang.ClassLoader.findClass(ClassLoader.java:364)
> > > [info] at
> > >
> org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26)
> > > [info] at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
> > > [info] at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
> > > [info] at
> > >
> org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
> > > [info] at
> > >
> org.apache.spark.repl.ExecutorClassLoader$$anonfun$findClass$1.apply(ExecutorClassLoader.scala:57)
> > > [info] at
> > >
> org.apache.spark.repl.ExecutorClassLoader$$anonfun$findClass$1.apply(ExecutorClassLoader.scala:57)
> > > [info] at scala.Option.getOrElse(Option.scala:120)
> > > [info] at
> > >
> org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:57)
> > > [info] at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
> > > [info] at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
> > > [info] at
> > >
> org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply$mcV$sp(ExecutorClassLoaderSuite.scala:47)
> > > [info] at
> > >
> org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply(ExecutorClassLoaderSuite.scala:44)
> > > [info] at
> > >
> org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply(ExecutorClassLoaderSuite.scala:44)
> > > [info] at org.scalatest.FunSuite$$anon$1.apply(FunSuite.scala:1265)
> > > [info] at org.scalatest.Suite$class.withFixture(Suite.scala:1974)
> > > [info] at
> > >
> org.apache.spark.repl.ExecutorClassLoaderSuite.withFixture(ExecutorClassLoaderSuite.scala:30)
> > > [info] at
> > > org.scalatest.FunSuite$class.invokeWithFixture$1(FunSuite.scala:1262)
> > > [info] at
> > > org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)
> > > [info] at
> > > org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)
> > > [info] at org.scalatest.SuperEngine.runTestImpl(Engine.scala:198)
> > > [info] at org.scalatest.FunSuite$class.runTest(FunSuite.scala:1271)
> > > [info] at
> > >
> org.apache.spark.repl.ExecutorClassLoaderSuite.runTest(ExecutorClassLoaderSuite.scala:30)
> > > [info] at
> > > org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)
> > > [info] at
> > > org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)
> > > [info] at
> > >
> org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:260)
> > > [info] at
> > >
> org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:249)
> > > [info] at scala.collection.immutable.List.foreach(List.scala:318)
> > > [info] at org.scalatest.SuperEngine.org (
> http://org.scalatest.SuperEngine.org)
> > > $scalatest$SuperEngine$$runTestsInBranch(Engine.scala:249)
> > > [info] at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:326)
> > > [info] at org.scalatest.FunSuite$class.runTests(FunSuite.scala:1304)
> > > [info] at
> > >
> org.apache.spark.repl.ExecutorClassLoaderSuite.runTests(ExecutorClassLoaderSuite.scala:30)
> > > [info] at org.scalatest.Suite$class.run(Suite.scala:2303)
> > > [info] at org.apache.spark.repl.ExecutorClassLoaderSuite.org (
> http://org.apache.spark.repl.ExecutorClassLoaderSuite.org)
> > > $scalatest$FunSuite$$super$run(ExecutorClassLoaderSuite.scala:30)
> > > [info] at
> > > org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)
> > > [info] at
> > > org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)
> > > [info] at org.scalatest.SuperEngine.runImpl(Engine.scala:362)
> > > [info] at org.scalatest.FunSuite$class.run(FunSuite.scala:1310)
> > > [info] at org.apache.spark.repl.ExecutorClassLoaderSuite.org (
> http://org.apache.spark.repl.ExecutorClassLoaderSuite.org)
> > >
> $scalatest$BeforeAndAfterAll$$super$run(ExecutorClassLoaderSuite.scala:30)
> > > [info] at
> > > org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:213)
> > > [info] at
> > >
> org.apache.spark.repl.ExecutorClassLoaderSuite.run(ExecutorClassLoaderSuite.scala:30)
> > > [info] at
> > >
> org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:214)
> > > [info] at sbt.RunnerWrapper$1.runRunner2(FrameworkWrapper.java:220)
> > > [info] at sbt.RunnerWrapper$1.execute(FrameworkWrapper.java:233)
> > > [info] at sbt.ForkMain$Run.runTest(ForkMain.java:243)
> > > [info] at sbt.ForkMain$Run.runTestSafe(ForkMain.java:214)
> > > [info] at sbt.ForkMain$Run.runTests(ForkMain.java:190)
> > > [info] at sbt.ForkMain$Run.run(ForkMain.java:257)
> > > [info] at sbt.ForkMain.main(ForkMain.java:99)
> > > [info] - parent first *** FAILED *** (59 milliseconds)
> > > [info] java.lang.ClassNotFoundException: ReplFakeClass1
> > > ...
> > > [info] Cause: java.lang.ClassNotFoundException: ReplFakeClass1
> > > ...
> > > [info] - child first can fall back *** FAILED *** (39 milliseconds)
> > > [info] java.lang.ClassNotFoundException: ReplFakeClass3
> > > ...
> > > [info] - child first can fail (46 milliseconds)
> > > [info] ReplSuite:
> > > [info] - propagation of local properties (9 seconds, 353 milliseconds)
> > > [info] - simple foreach with accumulator (7 seconds, 608 milliseconds)
> > > [info] - external vars (5 seconds, 783 milliseconds)
> > > [info] - external classes (4 seconds, 341 milliseconds)
> > > [info] - external functions (4 seconds, 106 milliseconds)
> > > [info] - external functions that access vars (4 seconds, 538
> milliseconds)
> > > [info] - broadcast vars (4 seconds, 155 milliseconds)
> > > [info] - interacting with files (3 seconds, 376 milliseconds)
> > > Exception in thread "Connection manager future execution context-0"
> > >
> > >
> > > Some output is omitted.
> > >
> > > Here are some more information:
> > > ReplFakeClass1.class is in the
> > > {spark_source_dir}/repl/ReplFakeClass1.class, same as ReplFakeClass2
> and 3.
> > > ReplSuite failed in running test("local-cluster mode"). The first time
> > > running this test throws OOM error. The exception shown in above is a
> > > second try
> > > The test("local-cluster mode") jvm options are '-Xms512M -Xmx512M'
> which I
> > > see from the corresponding stderr log
> > > I have .sbtconfig file in my home dir. The content is
> > > export SBT_OPTS="-XX:+CMSClassUnloadingEnabled -XX:PermSize=5120M
> > > -XX:MaxPermSize=10240M"
> > >
> > >
> > > The test hung after the test failed in the ReplSuite. I have to Ctr-c
> to
> > > close the test.
> > >
> > > Thank you for you advice.
> > >
> > >
> > >
> > > --
> > > Ye Xianjin
> > > Sent with Sparrow (http://www.sparrowmailapp.com/?sig)
> > >
> >
> >
> >
>
>
>

--089e0149398217c1b004f7057e45--

From dev-return-7336-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 14 19:44:03 2014
Return-Path: <dev-return-7336-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7EECD1162E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Apr 2014 19:44:03 +0000 (UTC)
Received: (qmail 12177 invoked by uid 500); 14 Apr 2014 19:44:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12123 invoked by uid 500); 14 Apr 2014 19:44:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12112 invoked by uid 99); 14 Apr 2014 19:44:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Apr 2014 19:44:01 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of advancedxy@gmail.com designates 209.85.160.49 as permitted sender)
Received: from [209.85.160.49] (HELO mail-pb0-f49.google.com) (209.85.160.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Apr 2014 19:43:56 +0000
Received: by mail-pb0-f49.google.com with SMTP id jt11so8602640pbb.22
        for <dev@spark.apache.org>; Mon, 14 Apr 2014 12:43:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=XVMjNloHfevsiZfcCStuL7L6EcAiKkKsk28AipXftw4=;
        b=Zz4vHfuBz8rd9abdx7D3+VBz2UVV1gS7BIrIWtjBRNKbvseZaK7P6XZxO5CI3vNLc6
         WYf4h2JWM2mZti6XZYaG2gNXaCLsoFm/YCoH/zZWgM+UWZc4VUGxLu7wCKnwBqiyCkKH
         7A6BiaoBPeShkVDD7tQ0QRI8p4PxIXiqT35iUrJIEd+vk4gZsQ8SsdECDhuzL8wRJwnU
         Q36wFEUEduVoywWe1rDcBmxiiFHDUpRpFnVclVJbwV9rIJgaHaSZeuWO/VCBX1KG0+Q3
         zClgx2Bs3/atTFYYWREeLjwg78mnvLVOrR48oAcnVAxRPPXDhBCenn8WqT4M79okgfav
         wE5Q==
X-Received: by 10.68.242.68 with SMTP id wo4mr46564167pbc.32.1397504613224;
        Mon, 14 Apr 2014 12:43:33 -0700 (PDT)
Received: from [192.168.1.102] ([115.195.177.211])
        by mx.google.com with ESMTPSA id qx11sm84344599pab.35.2014.04.14.12.43.28
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Mon, 14 Apr 2014 12:43:32 -0700 (PDT)
Date: Tue, 15 Apr 2014 03:43:27 +0800
From: Ye Xianjin <advancedxy@gmail.com>
To: dev@spark.apache.org
Message-ID: <1E3C4CA4A28D4CCBA401A3BA63503779@gmail.com>
In-Reply-To: <CANGvG8rTgNNWMb5fZ1JrTywnTTuU7X+Jbt6uuj9k7mt9Z4mogw@mail.gmail.com>
References: <5C3428E1B84C4E09BC5A79CAF0A10FDA@gmail.com>
 <CAAswR-4Wrbh82xPGXM3BoQmDfExBBro7U9w8hqTMpMDAyWnSNQ@mail.gmail.com>
 <E9FE091E16764429B3F1D61D4D77E35F@gmail.com>
 <CANGvG8rTgNNWMb5fZ1JrTywnTTuU7X+Jbt6uuj9k7mt9Z4mogw@mail.gmail.com>
Subject: Re: Tests failed after assembling the latest code from github
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="534c3a5f_60ef0119_139"
X-Virus-Checked: Checked by ClamAV on apache.org

--534c3a5f_60ef0119_139
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

well. This is very strange. 
I looked into ExecutorClassLoaderSuite.scala and ReplSuite.scala and made small changes to ExecutorClassLoaderSuite.scala (mostly output some internal variables). After that, when running repl test, I noticed the ReplSuite  
was tested first and the test result is ok. But the ExecutorClassLoaderSuite test was weird.
Here is the output:
[info] ExecutorClassLoaderSuite:
[error] Uncaught exception when running org.apache.spark.repl.ExecutorClassLoaderSuite: java.lang.OutOfMemoryError: PermGen space
[error] Uncaught exception when running org.apache.spark.repl.ExecutorClassLoaderSuite: java.lang.OutOfMemoryError: PermGen space
Internal error when running tests: java.lang.OutOfMemoryError: PermGen space
Exception in thread "Thread-3" java.io.EOFException
at java.io.ObjectInputStream$BlockDataInputStream.peekByte(ObjectInputStream.java:2577)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1297)
at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1685)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1323)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)
at sbt.React.react(ForkTests.scala:116)
at sbt.ForkTests$$anonfun$mainTestTask$1$Acceptor$2$.run(ForkTests.scala:75)
at java.lang.Thread.run(Thread.java:695)


I revert my changes. The test result is same.

 I touched the ReplSuite.scala file (use touch command), the test order is reversed, same as the very beginning. And the output is also the same.(The result in my first post).


-- 
Ye Xianjin
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)


On Tuesday, April 15, 2014 at 3:14 AM, Aaron Davidson wrote:

> This may have something to do with running the tests on a Mac, as there is
> a lot of File/URI/URL stuff going on in that test which may just have
> happened to work if run on a Linux system (like Jenkins). Note that this
> suite was added relatively recently:
> https://github.com/apache/spark/pull/217
> 
> 
> On Mon, Apr 14, 2014 at 12:04 PM, Ye Xianjin <advancedxy@gmail.com (mailto:advancedxy@gmail.com)> wrote:
> 
> > Thank you for your reply.
> > 
> > After building the assembly jar, the repl test still failed. The error
> > output is same as I post before.
> > 
> > --
> > Ye Xianjin
> > Sent with Sparrow (http://www.sparrowmailapp.com/?sig)
> > 
> > 
> > On Tuesday, April 15, 2014 at 1:39 AM, Michael Armbrust wrote:
> > 
> > > I believe you may need an assembly jar to run the ReplSuite. "sbt/sbt
> > > assembly/assembly".
> > > 
> > > Michael
> > > 
> > > 
> > > On Mon, Apr 14, 2014 at 3:14 AM, Ye Xianjin <advancedxy@gmail.com (mailto:advancedxy@gmail.com)(mailto:
> > advancedxy@gmail.com (mailto:advancedxy@gmail.com))> wrote:
> > > 
> > > > Hi, everyone:
> > > > I am new to Spark development. I download spark's latest code from
> > > > 
> > > 
> > > 
> > 
> > github.
> > > > After running sbt/sbt assembly,
> > > > I began running sbt/sbt test in the spark source code dir. But it
> > > > 
> > > 
> > 
> > failed
> > > > running the repl module test.
> > > > 
> > > > Here are some output details.
> > > > 
> > > > command:
> > > > sbt/sbt "test-only org.apache.spark.repl.*"
> > > > output:
> > > > 
> > > > [info] Loading project definition from
> > > > /Volumes/MacintoshHD/github/spark/project/project
> > > > [info] Loading project definition from
> > > > /Volumes/MacintoshHD/github/spark/project
> > > > [info] Set current project to root (in build
> > > > file:/Volumes/MacintoshHD/github/spark/)
> > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > [info] No tests to run for graphx/test:testOnly
> > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > [info] No tests to run for bagel/test:testOnly
> > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > [info] No tests to run for streaming/test:testOnly
> > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > [info] No tests to run for mllib/test:testOnly
> > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > [info] No tests to run for catalyst/test:testOnly
> > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > [info] No tests to run for core/test:testOnly
> > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > [info] No tests to run for assembly/test:testOnly
> > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > [info] No tests to run for sql/test:testOnly
> > > > [info] ExecutorClassLoaderSuite:
> > > > 2014-04-14 16:59:31.247 java[8393:1003] Unable to load realm info from
> > > > SCDynamicStore
> > > > [info] - child first *** FAILED *** (440 milliseconds)
> > > > [info] java.lang.ClassNotFoundException: ReplFakeClass2
> > > > [info] at java.lang.ClassLoader.findClass(ClassLoader.java:364)
> > > > [info] at
> > > > 
> > > 
> > 
> > org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26)
> > > > [info] at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
> > > > [info] at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
> > > > [info] at
> > > > 
> > > 
> > 
> > org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
> > > > [info] at
> > > 
> > 
> > org.apache.spark.repl.ExecutorClassLoader$$anonfun$findClass$1.apply(ExecutorClassLoader.scala:57)
> > > > [info] at
> > > 
> > 
> > org.apache.spark.repl.ExecutorClassLoader$$anonfun$findClass$1.apply(ExecutorClassLoader.scala:57)
> > > > [info] at scala.Option.getOrElse(Option.scala:120)
> > > > [info] at
> > > > 
> > > 
> > 
> > org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:57)
> > > > [info] at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
> > > > [info] at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
> > > > [info] at
> > > > 
> > > 
> > 
> > org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply$mcV$sp(ExecutorClassLoaderSuite.scala:47)
> > > > [info] at
> > > 
> > 
> > org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply(ExecutorClassLoaderSuite.scala:44)
> > > > [info] at
> > > 
> > 
> > org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply(ExecutorClassLoaderSuite.scala:44)
> > > > [info] at org.scalatest.FunSuite$$anon$1.apply(FunSuite.scala:1265)
> > > > [info] at org.scalatest.Suite$class.withFixture(Suite.scala:1974)
> > > > [info] at
> > > > 
> > > 
> > 
> > org.apache.spark.repl.ExecutorClassLoaderSuite.withFixture(ExecutorClassLoaderSuite.scala:30)
> > > > [info] at
> > > > org.scalatest.FunSuite$class.invokeWithFixture$1(FunSuite.scala:1262)
> > > > [info] at
> > > > org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)
> > > > [info] at
> > > > org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)
> > > > [info] at org.scalatest.SuperEngine.runTestImpl(Engine.scala:198)
> > > > [info] at org.scalatest.FunSuite$class.runTest(FunSuite.scala:1271)
> > > > [info] at
> > > > 
> > > 
> > 
> > org.apache.spark.repl.ExecutorClassLoaderSuite.runTest(ExecutorClassLoaderSuite.scala:30)
> > > > [info] at
> > > > org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)
> > > > [info] at
> > > > org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)
> > > > [info] at
> > > > 
> > > 
> > 
> > org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:260)
> > > > [info] at
> > > 
> > 
> > org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:249)
> > > > [info] at scala.collection.immutable.List.foreach(List.scala:318)
> > > > [info] at org.scalatest.SuperEngine.org (http://org.scalatest.SuperEngine.org) (
> > > > 
> > > 
> > 
> > http://org.scalatest.SuperEngine.org)
> > > > $scalatest$SuperEngine$$runTestsInBranch(Engine.scala:249)
> > > > [info] at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:326)
> > > > [info] at org.scalatest.FunSuite$class.runTests(FunSuite.scala:1304)
> > > > [info] at
> > > > 
> > > 
> > 
> > org.apache.spark.repl.ExecutorClassLoaderSuite.runTests(ExecutorClassLoaderSuite.scala:30)
> > > > [info] at org.scalatest.Suite$class.run(Suite.scala:2303)
> > > > [info] at org.apache.spark.repl.ExecutorClassLoaderSuite.org (http://org.apache.spark.repl.ExecutorClassLoaderSuite.org) (
> > > > 
> > > 
> > 
> > http://org.apache.spark.repl.ExecutorClassLoaderSuite.org)
> > > > $scalatest$FunSuite$$super$run(ExecutorClassLoaderSuite.scala:30)
> > > > [info] at
> > > > org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)
> > > > [info] at
> > > > org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)
> > > > [info] at org.scalatest.SuperEngine.runImpl(Engine.scala:362)
> > > > [info] at org.scalatest.FunSuite$class.run(FunSuite.scala:1310)
> > > > [info] at org.apache.spark.repl.ExecutorClassLoaderSuite.org (http://org.apache.spark.repl.ExecutorClassLoaderSuite.org) (
> > > > 
> > > 
> > 
> > http://org.apache.spark.repl.ExecutorClassLoaderSuite.org)
> > > > 
> > > 
> > 
> > $scalatest$BeforeAndAfterAll$$super$run(ExecutorClassLoaderSuite.scala:30)
> > > > [info] at
> > > > org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:213)
> > > > [info] at
> > > > 
> > > 
> > 
> > org.apache.spark.repl.ExecutorClassLoaderSuite.run(ExecutorClassLoaderSuite.scala:30)
> > > > [info] at
> > > 
> > 
> > org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:214)
> > > > [info] at sbt.RunnerWrapper$1.runRunner2(FrameworkWrapper.java:220)
> > > > [info] at sbt.RunnerWrapper$1.execute(FrameworkWrapper.java:233)
> > > > [info] at sbt.ForkMain$Run.runTest(ForkMain.java:243)
> > > > [info] at sbt.ForkMain$Run.runTestSafe(ForkMain.java:214)
> > > > [info] at sbt.ForkMain$Run.runTests(ForkMain.java:190)
> > > > [info] at sbt.ForkMain$Run.run(ForkMain.java:257)
> > > > [info] at sbt.ForkMain.main(ForkMain.java:99)
> > > > [info] - parent first *** FAILED *** (59 milliseconds)
> > > > [info] java.lang.ClassNotFoundException: ReplFakeClass1
> > > > ...
> > > > [info] Cause: java.lang.ClassNotFoundException: ReplFakeClass1
> > > > ...
> > > > [info] - child first can fall back *** FAILED *** (39 milliseconds)
> > > > [info] java.lang.ClassNotFoundException: ReplFakeClass3
> > > > ...
> > > > [info] - child first can fail (46 milliseconds)
> > > > [info] ReplSuite:
> > > > [info] - propagation of local properties (9 seconds, 353 milliseconds)
> > > > [info] - simple foreach with accumulator (7 seconds, 608 milliseconds)
> > > > [info] - external vars (5 seconds, 783 milliseconds)
> > > > [info] - external classes (4 seconds, 341 milliseconds)
> > > > [info] - external functions (4 seconds, 106 milliseconds)
> > > > [info] - external functions that access vars (4 seconds, 538
> > > > 
> > > 
> > 
> > milliseconds)
> > > > [info] - broadcast vars (4 seconds, 155 milliseconds)
> > > > [info] - interacting with files (3 seconds, 376 milliseconds)
> > > > Exception in thread "Connection manager future execution context-0"
> > > > 
> > > > 
> > > > Some output is omitted.
> > > > 
> > > > Here are some more information:
> > > > ReplFakeClass1.class is in the
> > > > {spark_source_dir}/repl/ReplFakeClass1.class, same as ReplFakeClass2
> > > > 
> > > 
> > 
> > and 3.
> > > > ReplSuite failed in running test("local-cluster mode"). The first time
> > > > running this test throws OOM error. The exception shown in above is a
> > > > second try
> > > > The test("local-cluster mode") jvm options are '-Xms512M -Xmx512M'
> > > > 
> > > 
> > 
> > which I
> > > > see from the corresponding stderr log
> > > > I have .sbtconfig file in my home dir. The content is
> > > > export SBT_OPTS="-XX:+CMSClassUnloadingEnabled -XX:PermSize=5120M
> > > > -XX:MaxPermSize=10240M"
> > > > 
> > > > 
> > > > The test hung after the test failed in the ReplSuite. I have to Ctr-c
> > to
> > > > close the test.
> > > > 
> > > > Thank you for you advice.
> > > > 
> > > > 
> > > > 
> > > > --
> > > > Ye Xianjin
> > > > Sent with Sparrow (http://www.sparrowmailapp.com/?sig)
> > > > 
> > > 
> > 
> > 
> 
> 
> 



--534c3a5f_60ef0119_139--


From dev-return-7337-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 14 20:26:28 2014
Return-Path: <dev-return-7337-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 81CC711863
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Apr 2014 20:26:28 +0000 (UTC)
Received: (qmail 7647 invoked by uid 500); 14 Apr 2014 20:26:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7423 invoked by uid 500); 14 Apr 2014 20:26:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7246 invoked by uid 99); 14 Apr 2014 20:26:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Apr 2014 20:26:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.223.180 as permitted sender)
Received: from [209.85.223.180] (HELO mail-ie0-f180.google.com) (209.85.223.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Apr 2014 20:26:21 +0000
Received: by mail-ie0-f180.google.com with SMTP id as1so8659102iec.11
        for <dev@spark.apache.org>; Mon, 14 Apr 2014 13:25:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=cpNB/hV544ocDKROejBsFh+BzRoKccU1mJ4yt1UwTWE=;
        b=FbGivp+rPaeahE6lcVHCdJu7DRDpw05DqPmEmPzHMWNTKBDxFuGgk8qsVTSn12BnLt
         SOpYpXXeRseuB2a7b0tDszlXaekyNbI20YlOxZWcZPilpi9MgcCTUQK9OnHlvtC+dG4H
         d70SCXC30qDxERxLhmtA3Cew/Z8W2k2ST+q77fH9PULaeSz4l0brJ90KaG0CtSqbuQ/e
         Ztsy5jUNoME/h45+IKa1yIYEA0+D5Hf0/K9HQDKG0MSn5aqlmSM4vUTEcOVIEn7T/vao
         FRSeXtVNT4n50T5YtOeEQ9kubsNbMUVoJ+d5jO7ZFiUPwzuhe3uXkojZYxFujRrGtVuI
         JyPw==
X-Received: by 10.50.50.66 with SMTP id a2mr18668188igo.8.1397507159206;
        Mon, 14 Apr 2014 13:25:59 -0700 (PDT)
Received: from [142.157.43.22] (wpa043022.Wireless.McGill.CA. [142.157.43.22])
        by mx.google.com with ESMTPSA id s9sm32000307igw.16.2014.04.14.13.25.58
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Mon, 14 Apr 2014 13:25:58 -0700 (PDT)
Date: Mon, 14 Apr 2014 16:32:08 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Message-ID: <C197363CC38348CA9ED47E0BEE6950C0@gmail.com>
In-Reply-To: <CAEYYnxb5nwQ7TUBgeXjzApsTPsDSVucgRAJHG2rDV5ABD1-ajA@mail.gmail.com>
References: <CAEYYnxb5nwQ7TUBgeXjzApsTPsDSVucgRAJHG2rDV5ABD1-ajA@mail.gmail.com>
Subject: Re: It seems that jenkins for PR is not working
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="534c45c8_216231b_1a7"
X-Virus-Checked: Checked by ClamAV on apache.org

--534c45c8_216231b_1a7
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

+1=E2=80=A6. =20

-- =20
Nan Zhu


On =46riday, April 11, 2014 at 5:35 PM, DB Tsai wrote:

> I always got
> =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D
> =20
> Could not find Apache license headers in the following files:
> =21=3F=3F=3F=3F=3F /root/workspace/SparkPullRequestBuilder/python/metas=
tore/db.lck
> =21=3F=3F=3F=3F=3F /root/workspace/SparkPullRequestBuilder/python/metas=
tore/service.properties
> =20
> =20
> Sincerely,
> =20
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
> =20
> =20



--534c45c8_216231b_1a7--


From dev-return-7338-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 15 05:35:41 2014
Return-Path: <dev-return-7338-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3686A10C40
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Apr 2014 05:35:41 +0000 (UTC)
Received: (qmail 41093 invoked by uid 500); 15 Apr 2014 05:35:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40692 invoked by uid 500); 15 Apr 2014 05:35:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40684 invoked by uid 99); 15 Apr 2014 05:35:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Apr 2014 05:35:30 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of advancedxy@gmail.com designates 209.85.220.54 as permitted sender)
Received: from [209.85.220.54] (HELO mail-pa0-f54.google.com) (209.85.220.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Apr 2014 05:35:25 +0000
Received: by mail-pa0-f54.google.com with SMTP id lf10so9237551pab.27
        for <dev@spark.apache.org>; Mon, 14 Apr 2014 22:35:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=Bw6Vxej9VPvYTTazx07uR+tg1a/AZ5a9cykhSiRnEhQ=;
        b=dEJfDSqFVV2n5JGDFRTeYQR/TXwV31rMXk/FDJ2ElbjhxiIj/zdKWHfuzQaoFlHQXb
         wVzvmjJGo5Q6bjJNgfxjvPXZu3qbiYZzzLd/ibiLxuJEbVoLO1WoPLDbPJ2Axfdnr5I2
         udMKx4Mfqa7q+zthIac3ThWxr1NVqvg/hYlxwbt5VseLyNhVWpUIg+3KMC6YItX2cpZa
         EVfXiWghDLalIepEOHErJUm5dLMQdVxt9Am/JTuzBLTGQOYg6wKMSa5lzFjLiSRL9BGe
         85LIfHn/BvB5OrCANy6CXLwksbn2e5tAPnTrnVeyI0QE76xP+33F+r9++oiAaTxlIZQY
         0KAw==
X-Received: by 10.68.134.101 with SMTP id pj5mr48720268pbb.62.1397540105442;
        Mon, 14 Apr 2014 22:35:05 -0700 (PDT)
Received: from [10.240.138.131] ([123.58.191.68])
        by mx.google.com with ESMTPSA id qq5sm37652938pbb.24.2014.04.14.22.35.00
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Mon, 14 Apr 2014 22:35:04 -0700 (PDT)
Date: Tue, 15 Apr 2014 13:34:53 +0800
From: Ye Xianjin <advancedxy@gmail.com>
To: dev@spark.apache.org
Message-ID: <524DDF2A01BF4A58A9614BC73F999760@gmail.com>
In-Reply-To: <1E3C4CA4A28D4CCBA401A3BA63503779@gmail.com>
References: <5C3428E1B84C4E09BC5A79CAF0A10FDA@gmail.com>
 <CAAswR-4Wrbh82xPGXM3BoQmDfExBBro7U9w8hqTMpMDAyWnSNQ@mail.gmail.com>
 <E9FE091E16764429B3F1D61D4D77E35F@gmail.com>
 <CANGvG8rTgNNWMb5fZ1JrTywnTTuU7X+Jbt6uuj9k7mt9Z4mogw@mail.gmail.com>
 <1E3C4CA4A28D4CCBA401A3BA63503779@gmail.com>
Subject: Re: Tests failed after assembling the latest code from github
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="534cc4fd_5ec6afd4_139"
X-Virus-Checked: Checked by ClamAV on apache.org

--534cc4fd_5ec6afd4_139
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

Hi, I think I have found the cause of the tests failing. 

I have two disks on my laptop. The spark project dir is on an HDD disk while the tempdir created by google.io.Files.createTempDir is the /var/folders/5q/.... ,which is on the system disk, an SSD.
The ExecutorLoaderSuite test uses org.apache.spark.TestUtils.createdCompiledClass methods.
The createCompiledClass method first generates the compiled class in the pwd(spark/repl), thens use renameTo to move
the file. The renameTo method fails because the dest file is in a different filesystem than the source file.

I modify the TestUtils.scala to first copy the file to dest then delete the original file. The tests go smoothly.
Should I issue an jira about this problem? Then I can send a pr on Github.

-- 
Ye Xianjin
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)


On Tuesday, April 15, 2014 at 3:43 AM, Ye Xianjin wrote:

> well. This is very strange. 
> I looked into ExecutorClassLoaderSuite.scala and ReplSuite.scala and made small changes to ExecutorClassLoaderSuite.scala (mostly output some internal variables). After that, when running repl test, I noticed the ReplSuite  
> was tested first and the test result is ok. But the ExecutorClassLoaderSuite test was weird.
> Here is the output:
> [info] ExecutorClassLoaderSuite:
> [error] Uncaught exception when running org.apache.spark.repl.ExecutorClassLoaderSuite: java.lang.OutOfMemoryError: PermGen space
> [error] Uncaught exception when running org.apache.spark.repl.ExecutorClassLoaderSuite: java.lang.OutOfMemoryError: PermGen space
> Internal error when running tests: java.lang.OutOfMemoryError: PermGen space
> Exception in thread "Thread-3" java.io.EOFException
> at java.io.ObjectInputStream$BlockDataInputStream.peekByte(ObjectInputStream.java:2577)
> at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1297)
> at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1685)
> at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1323)
> at java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)
> at sbt.React.react(ForkTests.scala:116)
> at sbt.ForkTests$$anonfun$mainTestTask$1$Acceptor$2$.run(ForkTests.scala:75)
> at java.lang.Thread.run(Thread.java:695)
> 
> 
> I revert my changes. The test result is same.
> 
>  I touched the ReplSuite.scala file (use touch command), the test order is reversed, same as the very beginning. And the output is also the same.(The result in my first post).
> 
> 
> -- 
> Ye Xianjin
> Sent with Sparrow (http://www.sparrowmailapp.com/?sig)
> 
> 
> On Tuesday, April 15, 2014 at 3:14 AM, Aaron Davidson wrote:
> 
> > This may have something to do with running the tests on a Mac, as there is
> > a lot of File/URI/URL stuff going on in that test which may just have
> > happened to work if run on a Linux system (like Jenkins). Note that this
> > suite was added relatively recently:
> > https://github.com/apache/spark/pull/217
> > 
> > 
> > On Mon, Apr 14, 2014 at 12:04 PM, Ye Xianjin <advancedxy@gmail.com (mailto:advancedxy@gmail.com)> wrote:
> > 
> > > Thank you for your reply.
> > > 
> > > After building the assembly jar, the repl test still failed. The error
> > > output is same as I post before.
> > > 
> > > --
> > > Ye Xianjin
> > > Sent with Sparrow (http://www.sparrowmailapp.com/?sig)
> > > 
> > > 
> > > On Tuesday, April 15, 2014 at 1:39 AM, Michael Armbrust wrote:
> > > 
> > > > I believe you may need an assembly jar to run the ReplSuite. "sbt/sbt
> > > > assembly/assembly".
> > > > 
> > > > Michael
> > > > 
> > > > 
> > > > On Mon, Apr 14, 2014 at 3:14 AM, Ye Xianjin <advancedxy@gmail.com (mailto:advancedxy@gmail.com)(mailto:
> > > advancedxy@gmail.com (mailto:advancedxy@gmail.com))> wrote:
> > > > 
> > > > > Hi, everyone:
> > > > > I am new to Spark development. I download spark's latest code from
> > > > > 
> > > > 
> > > > 
> > > 
> > > github.
> > > > > After running sbt/sbt assembly,
> > > > > I began running sbt/sbt test in the spark source code dir. But it
> > > > > 
> > > > 
> > > 
> > > failed
> > > > > running the repl module test.
> > > > > 
> > > > > Here are some output details.
> > > > > 
> > > > > command:
> > > > > sbt/sbt "test-only org.apache.spark.repl.*"
> > > > > output:
> > > > > 
> > > > > [info] Loading project definition from
> > > > > /Volumes/MacintoshHD/github/spark/project/project
> > > > > [info] Loading project definition from
> > > > > /Volumes/MacintoshHD/github/spark/project
> > > > > [info] Set current project to root (in build
> > > > > file:/Volumes/MacintoshHD/github/spark/)
> > > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > > [info] No tests to run for graphx/test:testOnly
> > > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > > [info] No tests to run for bagel/test:testOnly
> > > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > > [info] No tests to run for streaming/test:testOnly
> > > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > > [info] No tests to run for mllib/test:testOnly
> > > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > > [info] No tests to run for catalyst/test:testOnly
> > > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > > [info] No tests to run for core/test:testOnly
> > > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > > [info] No tests to run for assembly/test:testOnly
> > > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > > [info] No tests to run for sql/test:testOnly
> > > > > [info] ExecutorClassLoaderSuite:
> > > > > 2014-04-14 16:59:31.247 java[8393:1003] Unable to load realm info from
> > > > > SCDynamicStore
> > > > > [info] - child first *** FAILED *** (440 milliseconds)
> > > > > [info] java.lang.ClassNotFoundException: ReplFakeClass2
> > > > > [info] at java.lang.ClassLoader.findClass(ClassLoader.java:364)
> > > > > [info] at
> > > > > 
> > > > 
> > > 
> > > org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26)
> > > > > [info] at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
> > > > > [info] at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
> > > > > [info] at
> > > > > 
> > > > 
> > > 
> > > org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
> > > > > [info] at
> > > > 
> > > 
> > > org.apache.spark.repl.ExecutorClassLoader$$anonfun$findClass$1.apply(ExecutorClassLoader.scala:57)
> > > > > [info] at
> > > > 
> > > 
> > > org.apache.spark.repl.ExecutorClassLoader$$anonfun$findClass$1.apply(ExecutorClassLoader.scala:57)
> > > > > [info] at scala.Option.getOrElse(Option.scala:120)
> > > > > [info] at
> > > > > 
> > > > 
> > > 
> > > org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:57)
> > > > > [info] at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
> > > > > [info] at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
> > > > > [info] at
> > > > > 
> > > > 
> > > 
> > > org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply$mcV$sp(ExecutorClassLoaderSuite.scala:47)
> > > > > [info] at
> > > > 
> > > 
> > > org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply(ExecutorClassLoaderSuite.scala:44)
> > > > > [info] at
> > > > 
> > > 
> > > org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply(ExecutorClassLoaderSuite.scala:44)
> > > > > [info] at org.scalatest.FunSuite$$anon$1.apply(FunSuite.scala:1265)
> > > > > [info] at org.scalatest.Suite$class.withFixture(Suite.scala:1974)
> > > > > [info] at
> > > > > 
> > > > 
> > > 
> > > org.apache.spark.repl.ExecutorClassLoaderSuite.withFixture(ExecutorClassLoaderSuite.scala:30)
> > > > > [info] at
> > > > > org.scalatest.FunSuite$class.invokeWithFixture$1(FunSuite.scala:1262)
> > > > > [info] at
> > > > > org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)
> > > > > [info] at
> > > > > org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)
> > > > > [info] at org.scalatest.SuperEngine.runTestImpl(Engine.scala:198)
> > > > > [info] at org.scalatest.FunSuite$class.runTest(FunSuite.scala:1271)
> > > > > [info] at
> > > > > 
> > > > 
> > > 
> > > org.apache.spark.repl.ExecutorClassLoaderSuite.runTest(ExecutorClassLoaderSuite.scala:30)
> > > > > [info] at
> > > > > org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)
> > > > > [info] at
> > > > > org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)
> > > > > [info] at
> > > > > 
> > > > 
> > > 
> > > org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:260)
> > > > > [info] at
> > > > 
> > > 
> > > org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:249)
> > > > > [info] at scala.collection.immutable.List.foreach(List.scala:318)
> > > > > [info] at org.scalatest.SuperEngine.org (http://org.scalatest.SuperEngine.org) (
> > > > > 
> > > > 
> > > 
> > > http://org.scalatest.SuperEngine.org)
> > > > > $scalatest$SuperEngine$$runTestsInBranch(Engine.scala:249)
> > > > > [info] at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:326)
> > > > > [info] at org.scalatest.FunSuite$class.runTests(FunSuite.scala:1304)
> > > > > [info] at
> > > > > 
> > > > 
> > > 
> > > org.apache.spark.repl.ExecutorClassLoaderSuite.runTests(ExecutorClassLoaderSuite.scala:30)
> > > > > [info] at org.scalatest.Suite$class.run(Suite.scala:2303)
> > > > > [info] at org.apache.spark.repl.ExecutorClassLoaderSuite.org (http://org.apache.spark.repl.ExecutorClassLoaderSuite.org) (
> > > > > 
> > > > 
> > > 
> > > http://org.apache.spark.repl.ExecutorClassLoaderSuite.org)
> > > > > $scalatest$FunSuite$$super$run(ExecutorClassLoaderSuite.scala:30)
> > > > > [info] at
> > > > > org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)
> > > > > [info] at
> > > > > org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)
> > > > > [info] at org.scalatest.SuperEngine.runImpl(Engine.scala:362)
> > > > > [info] at org.scalatest.FunSuite$class.run(FunSuite.scala:1310)
> > > > > [info] at org.apache.spark.repl.ExecutorClassLoaderSuite.org (http://org.apache.spark.repl.ExecutorClassLoaderSuite.org) (
> > > > > 
> > > > 
> > > 
> > > http://org.apache.spark.repl.ExecutorClassLoaderSuite.org)
> > > > > 
> > > > 
> > > 
> > > $scalatest$BeforeAndAfterAll$$super$run(ExecutorClassLoaderSuite.scala:30)
> > > > > [info] at
> > > > > org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:213)
> > > > > [info] at
> > > > > 
> > > > 
> > > 
> > > org.apache.spark.repl.ExecutorClassLoaderSuite.run(ExecutorClassLoaderSuite.scala:30)
> > > > > [info] at
> > > > 
> > > 
> > > org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:214)
> > > > > [info] at sbt.RunnerWrapper$1.runRunner2(FrameworkWrapper.java:220)
> > > > > [info] at sbt.RunnerWrapper$1.execute(FrameworkWrapper.java:233)
> > > > > [info] at sbt.ForkMain$Run.runTest(ForkMain.java:243)
> > > > > [info] at sbt.ForkMain$Run.runTestSafe(ForkMain.java:214)
> > > > > [info] at sbt.ForkMain$Run.runTests(ForkMain.java:190)
> > > > > [info] at sbt.ForkMain$Run.run(ForkMain.java:257)
> > > > > [info] at sbt.ForkMain.main(ForkMain.java:99)
> > > > > [info] - parent first *** FAILED *** (59 milliseconds)
> > > > > [info] java.lang.ClassNotFoundException: ReplFakeClass1
> > > > > ...
> > > > > [info] Cause: java.lang.ClassNotFoundException: ReplFakeClass1
> > > > > ...
> > > > > [info] - child first can fall back *** FAILED *** (39 milliseconds)
> > > > > [info] java.lang.ClassNotFoundException: ReplFakeClass3
> > > > > ...
> > > > > [info] - child first can fail (46 milliseconds)
> > > > > [info] ReplSuite:
> > > > > [info] - propagation of local properties (9 seconds, 353 milliseconds)
> > > > > [info] - simple foreach with accumulator (7 seconds, 608 milliseconds)
> > > > > [info] - external vars (5 seconds, 783 milliseconds)
> > > > > [info] - external classes (4 seconds, 341 milliseconds)
> > > > > [info] - external functions (4 seconds, 106 milliseconds)
> > > > > [info] - external functions that access vars (4 seconds, 538
> > > > > 
> > > > 
> > > 
> > > milliseconds)
> > > > > [info] - broadcast vars (4 seconds, 155 milliseconds)
> > > > > [info] - interacting with files (3 seconds, 376 milliseconds)
> > > > > Exception in thread "Connection manager future execution context-0"
> > > > > 
> > > > > 
> > > > > Some output is omitted.
> > > > > 
> > > > > Here are some more information:
> > > > > ReplFakeClass1.class is in the
> > > > > {spark_source_dir}/repl/ReplFakeClass1.class, same as ReplFakeClass2
> > > > > 
> > > > 
> > > 
> > > and 3.
> > > > > ReplSuite failed in running test("local-cluster mode"). The first time
> > > > > running this test throws OOM error. The exception shown in above is a
> > > > > second try
> > > > > The test("local-cluster mode") jvm options are '-Xms512M -Xmx512M'
> > > > > 
> > > > 
> > > 
> > > which I
> > > > > see from the corresponding stderr log
> > > > > I have .sbtconfig file in my home dir. The content is
> > > > > export SBT_OPTS="-XX:+CMSClassUnloadingEnabled -XX:PermSize=5120M
> > > > > -XX:MaxPermSize=10240M"
> > > > > 
> > > > > 
> > > > > The test hung after the test failed in the ReplSuite. I have to Ctr-c
> > > to
> > > > > close the test.
> > > > > 
> > > > > Thank you for you advice.
> > > > > 
> > > > > 
> > > > > 
> > > > > --
> > > > > Ye Xianjin
> > > > > Sent with Sparrow (http://www.sparrowmailapp.com/?sig)
> > > > > 
> > > > 
> > > 
> > > 
> > 
> > 
> > 
> > 
> 
> 


--534cc4fd_5ec6afd4_139--


From dev-return-7339-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 15 06:36:56 2014
Return-Path: <dev-return-7339-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 803AA10E62
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Apr 2014 06:36:56 +0000 (UTC)
Received: (qmail 32285 invoked by uid 500); 15 Apr 2014 06:36:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31650 invoked by uid 500); 15 Apr 2014 06:36:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31642 invoked by uid 99); 15 Apr 2014 06:36:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Apr 2014 06:36:49 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ilikerps@gmail.com designates 209.85.216.172 as permitted sender)
Received: from [209.85.216.172] (HELO mail-qc0-f172.google.com) (209.85.216.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Apr 2014 06:36:44 +0000
Received: by mail-qc0-f172.google.com with SMTP id i8so9979408qcq.3
        for <dev@spark.apache.org>; Mon, 14 Apr 2014 23:36:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=plCW1AqdaR+s2GrntJZXdWEECLrUjXqkY0QNGaiAqXk=;
        b=yZXz74YuH4Na6/ur5gto2I2Wq9Hrwi5BWxQaN0225bYHKnUhElB5YEs73Cw5bEX546
         FY47wj+sIewliUkSmsCocWkI7W3pYy5qJxBhRo8gIQvPUs01+g0Qu+hzhTDirnyfVN/n
         RGlK7FmFVyW3GbYwc3rnz/gFFFJTsTNwibPtn0viBIxJ1mXufQmUHGgJSMwaZCWyIgJG
         k/5bvx0m0gjPA7eV5PQ/NCjRfyP7iTCTcLEnMVJIfnLpUkIbb8uKbvFvwCk2NmqH/Hhr
         HJs5kShhe6s1CbQQUKCe24ULSd7yRmvR+WaLT2RIimoQ+oQro4jzJndtGofeeQWedrI/
         yuwg==
X-Received: by 10.140.104.103 with SMTP id z94mr14275249qge.91.1397543782001;
 Mon, 14 Apr 2014 23:36:22 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.37.170 with HTTP; Mon, 14 Apr 2014 23:36:00 -0700 (PDT)
In-Reply-To: <524DDF2A01BF4A58A9614BC73F999760@gmail.com>
References: <5C3428E1B84C4E09BC5A79CAF0A10FDA@gmail.com> <CAAswR-4Wrbh82xPGXM3BoQmDfExBBro7U9w8hqTMpMDAyWnSNQ@mail.gmail.com>
 <E9FE091E16764429B3F1D61D4D77E35F@gmail.com> <CANGvG8rTgNNWMb5fZ1JrTywnTTuU7X+Jbt6uuj9k7mt9Z4mogw@mail.gmail.com>
 <1E3C4CA4A28D4CCBA401A3BA63503779@gmail.com> <524DDF2A01BF4A58A9614BC73F999760@gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Mon, 14 Apr 2014 23:36:00 -0700
Message-ID: <CANGvG8orRHUzPf9Krd+cV1aCEUk3ABwjKO9aOcJrP72ys8UBGw@mail.gmail.com>
Subject: Re: Tests failed after assembling the latest code from github
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134f2beaf300f04f70f047e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134f2beaf300f04f70f047e
Content-Type: text/plain; charset=UTF-8

By all means, it would be greatly appreciated!


On Mon, Apr 14, 2014 at 10:34 PM, Ye Xianjin <advancedxy@gmail.com> wrote:

> Hi, I think I have found the cause of the tests failing.
>
> I have two disks on my laptop. The spark project dir is on an HDD disk
> while the tempdir created by google.io.Files.createTempDir is the
> /var/folders/5q/.... ,which is on the system disk, an SSD.
> The ExecutorLoaderSuite test uses
> org.apache.spark.TestUtils.createdCompiledClass methods.
> The createCompiledClass method first generates the compiled class in the
> pwd(spark/repl), thens use renameTo to move
> the file. The renameTo method fails because the dest file is in a
> different filesystem than the source file.
>
> I modify the TestUtils.scala to first copy the file to dest then delete
> the original file. The tests go smoothly.
> Should I issue an jira about this problem? Then I can send a pr on Github.
>
> --
> Ye Xianjin
> Sent with Sparrow (http://www.sparrowmailapp.com/?sig)
>
>
> On Tuesday, April 15, 2014 at 3:43 AM, Ye Xianjin wrote:
>
> > well. This is very strange.
> > I looked into ExecutorClassLoaderSuite.scala and ReplSuite.scala and
> made small changes to ExecutorClassLoaderSuite.scala (mostly output some
> internal variables). After that, when running repl test, I noticed the
> ReplSuite
> > was tested first and the test result is ok. But the
> ExecutorClassLoaderSuite test was weird.
> > Here is the output:
> > [info] ExecutorClassLoaderSuite:
> > [error] Uncaught exception when running
> org.apache.spark.repl.ExecutorClassLoaderSuite: java.lang.OutOfMemoryError:
> PermGen space
> > [error] Uncaught exception when running
> org.apache.spark.repl.ExecutorClassLoaderSuite: java.lang.OutOfMemoryError:
> PermGen space
> > Internal error when running tests: java.lang.OutOfMemoryError: PermGen
> space
> > Exception in thread "Thread-3" java.io.EOFException
> > at
> java.io.ObjectInputStream$BlockDataInputStream.peekByte(ObjectInputStream.java:2577)
> > at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1297)
> > at java.io.ObjectInputStream.readArray(ObjectInputStream.java:1685)
> > at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1323)
> > at java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)
> > at sbt.React.react(ForkTests.scala:116)
> > at
> sbt.ForkTests$$anonfun$mainTestTask$1$Acceptor$2$.run(ForkTests.scala:75)
> > at java.lang.Thread.run(Thread.java:695)
> >
> >
> > I revert my changes. The test result is same.
> >
> >  I touched the ReplSuite.scala file (use touch command), the test order
> is reversed, same as the very beginning. And the output is also the
> same.(The result in my first post).
> >
> >
> > --
> > Ye Xianjin
> > Sent with Sparrow (http://www.sparrowmailapp.com/?sig)
> >
> >
> > On Tuesday, April 15, 2014 at 3:14 AM, Aaron Davidson wrote:
> >
> > > This may have something to do with running the tests on a Mac, as
> there is
> > > a lot of File/URI/URL stuff going on in that test which may just have
> > > happened to work if run on a Linux system (like Jenkins). Note that
> this
> > > suite was added relatively recently:
> > > https://github.com/apache/spark/pull/217
> > >
> > >
> > > On Mon, Apr 14, 2014 at 12:04 PM, Ye Xianjin <advancedxy@gmail.com(mailto:
> advancedxy@gmail.com)> wrote:
> > >
> > > > Thank you for your reply.
> > > >
> > > > After building the assembly jar, the repl test still failed. The
> error
> > > > output is same as I post before.
> > > >
> > > > --
> > > > Ye Xianjin
> > > > Sent with Sparrow (http://www.sparrowmailapp.com/?sig)
> > > >
> > > >
> > > > On Tuesday, April 15, 2014 at 1:39 AM, Michael Armbrust wrote:
> > > >
> > > > > I believe you may need an assembly jar to run the ReplSuite.
> "sbt/sbt
> > > > > assembly/assembly".
> > > > >
> > > > > Michael
> > > > >
> > > > >
> > > > > On Mon, Apr 14, 2014 at 3:14 AM, Ye Xianjin <advancedxy@gmail.com(mailto:
> advancedxy@gmail.com)(mailto:
> > > > advancedxy@gmail.com (mailto:advancedxy@gmail.com))> wrote:
> > > > >
> > > > > > Hi, everyone:
> > > > > > I am new to Spark development. I download spark's latest code
> from
> > > > > >
> > > > >
> > > > >
> > > >
> > > > github.
> > > > > > After running sbt/sbt assembly,
> > > > > > I began running sbt/sbt test in the spark source code dir. But it
> > > > > >
> > > > >
> > > >
> > > > failed
> > > > > > running the repl module test.
> > > > > >
> > > > > > Here are some output details.
> > > > > >
> > > > > > command:
> > > > > > sbt/sbt "test-only org.apache.spark.repl.*"
> > > > > > output:
> > > > > >
> > > > > > [info] Loading project definition from
> > > > > > /Volumes/MacintoshHD/github/spark/project/project
> > > > > > [info] Loading project definition from
> > > > > > /Volumes/MacintoshHD/github/spark/project
> > > > > > [info] Set current project to root (in build
> > > > > > file:/Volumes/MacintoshHD/github/spark/)
> > > > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > > > [info] No tests to run for graphx/test:testOnly
> > > > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > > > [info] No tests to run for bagel/test:testOnly
> > > > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > > > [info] No tests to run for streaming/test:testOnly
> > > > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > > > [info] No tests to run for mllib/test:testOnly
> > > > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > > > [info] No tests to run for catalyst/test:testOnly
> > > > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > > > [info] No tests to run for core/test:testOnly
> > > > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > > > [info] No tests to run for assembly/test:testOnly
> > > > > > [info] Passed: Total 0, Failed 0, Errors 0, Passed 0
> > > > > > [info] No tests to run for sql/test:testOnly
> > > > > > [info] ExecutorClassLoaderSuite:
> > > > > > 2014-04-14 16:59:31.247 java[8393:1003] Unable to load realm
> info from
> > > > > > SCDynamicStore
> > > > > > [info] - child first *** FAILED *** (440 milliseconds)
> > > > > > [info] java.lang.ClassNotFoundException: ReplFakeClass2
> > > > > > [info] at java.lang.ClassLoader.findClass(ClassLoader.java:364)
> > > > > > [info] at
> > > > > >
> > > > >
> > > >
> > > >
> org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26)
> > > > > > [info] at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
> > > > > > [info] at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
> > > > > > [info] at
> > > > > >
> > > > >
> > > >
> > > >
> org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
> > > > > > [info] at
> > > > >
> > > >
> > > >
> org.apache.spark.repl.ExecutorClassLoader$$anonfun$findClass$1.apply(ExecutorClassLoader.scala:57)
> > > > > > [info] at
> > > > >
> > > >
> > > >
> org.apache.spark.repl.ExecutorClassLoader$$anonfun$findClass$1.apply(ExecutorClassLoader.scala:57)
> > > > > > [info] at scala.Option.getOrElse(Option.scala:120)
> > > > > > [info] at
> > > > > >
> > > > >
> > > >
> > > >
> org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:57)
> > > > > > [info] at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
> > > > > > [info] at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
> > > > > > [info] at
> > > > > >
> > > > >
> > > >
> > > >
> org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply$mcV$sp(ExecutorClassLoaderSuite.scala:47)
> > > > > > [info] at
> > > > >
> > > >
> > > >
> org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply(ExecutorClassLoaderSuite.scala:44)
> > > > > > [info] at
> > > > >
> > > >
> > > >
> org.apache.spark.repl.ExecutorClassLoaderSuite$$anonfun$1.apply(ExecutorClassLoaderSuite.scala:44)
> > > > > > [info] at
> org.scalatest.FunSuite$$anon$1.apply(FunSuite.scala:1265)
> > > > > > [info] at org.scalatest.Suite$class.withFixture(Suite.scala:1974)
> > > > > > [info] at
> > > > > >
> > > > >
> > > >
> > > >
> org.apache.spark.repl.ExecutorClassLoaderSuite.withFixture(ExecutorClassLoaderSuite.scala:30)
> > > > > > [info] at
> > > > > >
> org.scalatest.FunSuite$class.invokeWithFixture$1(FunSuite.scala:1262)
> > > > > > [info] at
> > > > > >
> org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)
> > > > > > [info] at
> > > > > >
> org.scalatest.FunSuite$$anonfun$runTest$1.apply(FunSuite.scala:1271)
> > > > > > [info] at org.scalatest.SuperEngine.runTestImpl(Engine.scala:198)
> > > > > > [info] at
> org.scalatest.FunSuite$class.runTest(FunSuite.scala:1271)
> > > > > > [info] at
> > > > > >
> > > > >
> > > >
> > > >
> org.apache.spark.repl.ExecutorClassLoaderSuite.runTest(ExecutorClassLoaderSuite.scala:30)
> > > > > > [info] at
> > > > > >
> org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)
> > > > > > [info] at
> > > > > >
> org.scalatest.FunSuite$$anonfun$runTests$1.apply(FunSuite.scala:1304)
> > > > > > [info] at
> > > > > >
> > > > >
> > > >
> > > >
> org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:260)
> > > > > > [info] at
> > > > >
> > > >
> > > >
> org.scalatest.SuperEngine$$anonfun$org$scalatest$SuperEngine$$runTestsInBranch$1.apply(Engine.scala:249)
> > > > > > [info] at scala.collection.immutable.List.foreach(List.scala:318)
> > > > > > [info] at org.scalatest.SuperEngine.org (
> http://org.scalatest.SuperEngine.org) (
> > > > > >
> > > > >
> > > >
> > > > http://org.scalatest.SuperEngine.org)
> > > > > > $scalatest$SuperEngine$$runTestsInBranch(Engine.scala:249)
> > > > > > [info] at
> org.scalatest.SuperEngine.runTestsImpl(Engine.scala:326)
> > > > > > [info] at
> org.scalatest.FunSuite$class.runTests(FunSuite.scala:1304)
> > > > > > [info] at
> > > > > >
> > > > >
> > > >
> > > >
> org.apache.spark.repl.ExecutorClassLoaderSuite.runTests(ExecutorClassLoaderSuite.scala:30)
> > > > > > [info] at org.scalatest.Suite$class.run(Suite.scala:2303)
> > > > > > [info] at org.apache.spark.repl.ExecutorClassLoaderSuite.org (
> http://org.apache.spark.repl.ExecutorClassLoaderSuite.org) (
> > > > > >
> > > > >
> > > >
> > > > http://org.apache.spark.repl.ExecutorClassLoaderSuite.org)
> > > > > > $scalatest$FunSuite$$super$run(ExecutorClassLoaderSuite.scala:30)
> > > > > > [info] at
> > > > > > org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)
> > > > > > [info] at
> > > > > > org.scalatest.FunSuite$$anonfun$run$1.apply(FunSuite.scala:1310)
> > > > > > [info] at org.scalatest.SuperEngine.runImpl(Engine.scala:362)
> > > > > > [info] at org.scalatest.FunSuite$class.run(FunSuite.scala:1310)
> > > > > > [info] at org.apache.spark.repl.ExecutorClassLoaderSuite.org (
> http://org.apache.spark.repl.ExecutorClassLoaderSuite.org) (
> > > > > >
> > > > >
> > > >
> > > > http://org.apache.spark.repl.ExecutorClassLoaderSuite.org)
> > > > > >
> > > > >
> > > >
> > > >
> $scalatest$BeforeAndAfterAll$$super$run(ExecutorClassLoaderSuite.scala:30)
> > > > > > [info] at
> > > > > >
> org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:213)
> > > > > > [info] at
> > > > > >
> > > > >
> > > >
> > > >
> org.apache.spark.repl.ExecutorClassLoaderSuite.run(ExecutorClassLoaderSuite.scala:30)
> > > > > > [info] at
> > > > >
> > > >
> > > >
> org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:214)
> > > > > > [info] at
> sbt.RunnerWrapper$1.runRunner2(FrameworkWrapper.java:220)
> > > > > > [info] at sbt.RunnerWrapper$1.execute(FrameworkWrapper.java:233)
> > > > > > [info] at sbt.ForkMain$Run.runTest(ForkMain.java:243)
> > > > > > [info] at sbt.ForkMain$Run.runTestSafe(ForkMain.java:214)
> > > > > > [info] at sbt.ForkMain$Run.runTests(ForkMain.java:190)
> > > > > > [info] at sbt.ForkMain$Run.run(ForkMain.java:257)
> > > > > > [info] at sbt.ForkMain.main(ForkMain.java:99)
> > > > > > [info] - parent first *** FAILED *** (59 milliseconds)
> > > > > > [info] java.lang.ClassNotFoundException: ReplFakeClass1
> > > > > > ...
> > > > > > [info] Cause: java.lang.ClassNotFoundException: ReplFakeClass1
> > > > > > ...
> > > > > > [info] - child first can fall back *** FAILED *** (39
> milliseconds)
> > > > > > [info] java.lang.ClassNotFoundException: ReplFakeClass3
> > > > > > ...
> > > > > > [info] - child first can fail (46 milliseconds)
> > > > > > [info] ReplSuite:
> > > > > > [info] - propagation of local properties (9 seconds, 353
> milliseconds)
> > > > > > [info] - simple foreach with accumulator (7 seconds, 608
> milliseconds)
> > > > > > [info] - external vars (5 seconds, 783 milliseconds)
> > > > > > [info] - external classes (4 seconds, 341 milliseconds)
> > > > > > [info] - external functions (4 seconds, 106 milliseconds)
> > > > > > [info] - external functions that access vars (4 seconds, 538
> > > > > >
> > > > >
> > > >
> > > > milliseconds)
> > > > > > [info] - broadcast vars (4 seconds, 155 milliseconds)
> > > > > > [info] - interacting with files (3 seconds, 376 milliseconds)
> > > > > > Exception in thread "Connection manager future execution
> context-0"
> > > > > >
> > > > > >
> > > > > > Some output is omitted.
> > > > > >
> > > > > > Here are some more information:
> > > > > > ReplFakeClass1.class is in the
> > > > > > {spark_source_dir}/repl/ReplFakeClass1.class, same as
> ReplFakeClass2
> > > > > >
> > > > >
> > > >
> > > > and 3.
> > > > > > ReplSuite failed in running test("local-cluster mode"). The
> first time
> > > > > > running this test throws OOM error. The exception shown in above
> is a
> > > > > > second try
> > > > > > The test("local-cluster mode") jvm options are '-Xms512M
> -Xmx512M'
> > > > > >
> > > > >
> > > >
> > > > which I
> > > > > > see from the corresponding stderr log
> > > > > > I have .sbtconfig file in my home dir. The content is
> > > > > > export SBT_OPTS="-XX:+CMSClassUnloadingEnabled -XX:PermSize=5120M
> > > > > > -XX:MaxPermSize=10240M"
> > > > > >
> > > > > >
> > > > > > The test hung after the test failed in the ReplSuite. I have to
> Ctr-c
> > > > to
> > > > > > close the test.
> > > > > >
> > > > > > Thank you for you advice.
> > > > > >
> > > > > >
> > > > > >
> > > > > > --
> > > > > > Ye Xianjin
> > > > > > Sent with Sparrow (http://www.sparrowmailapp.com/?sig)
> > > > > >
> > > > >
> > > >
> > > >
> > >
> > >
> > >
> > >
> >
> >
>
>

--001a1134f2beaf300f04f70f047e--

From dev-return-7340-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 15 06:42:24 2014
Return-Path: <dev-return-7340-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5E07D10EA4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Apr 2014 06:42:24 +0000 (UTC)
Received: (qmail 40206 invoked by uid 500); 15 Apr 2014 06:42:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39571 invoked by uid 500); 15 Apr 2014 06:42:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39362 invoked by uid 99); 15 Apr 2014 06:42:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Apr 2014 06:42:21 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.128.181 as permitted sender)
Received: from [209.85.128.181] (HELO mail-ve0-f181.google.com) (209.85.128.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Apr 2014 06:42:15 +0000
Received: by mail-ve0-f181.google.com with SMTP id oy12so8859361veb.12
        for <dev@spark.apache.org>; Mon, 14 Apr 2014 23:41:55 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=TIO6MlnVehno1sDquos+SXlvK6tTyuRVbjB85X3fSOw=;
        b=eVuFAqOaZtgngQmxH9iXly6pNuVNKo73JXwDpb2B2bywwXyqr4z/WTQBqs4EAlxQAK
         DC5OrDZ9ZzbqaPcSsGvsdRKcE/6r1vPldoy7S7ZH4973/kfknez29p3SHU9wT5be/gPB
         NnDoqwhcs9n//PwnPcj67o0I0zV5aP3iYCNiwFgOt3D+2QgF4KjGcaj+3BNIB5GWUp0r
         mawXotY6ReLsuyjh1kv76uG63jw3XiixIzpO19jzUzubMfQNXcFhHAfrpRxWe7RV3QR5
         zZFIXsEAREOyFfGeTylJkvf3Jawo4RCotacRvCt1p0WQzARAw3oJO5NqT6mPtbIDVS/B
         hmqw==
X-Gm-Message-State: ALoCoQm4rZFCSJ2a5+xJV9w826jTJA9T2hXSvMdkGylPog9zt+xrOxwZy+CjvRqY5hVuWWGhUMNE
X-Received: by 10.58.112.98 with SMTP id ip2mr51133veb.35.1397544114900; Mon,
 14 Apr 2014 23:41:54 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.111.69 with HTTP; Mon, 14 Apr 2014 23:41:34 -0700 (PDT)
In-Reply-To: <524DDF2A01BF4A58A9614BC73F999760@gmail.com>
References: <5C3428E1B84C4E09BC5A79CAF0A10FDA@gmail.com> <CAAswR-4Wrbh82xPGXM3BoQmDfExBBro7U9w8hqTMpMDAyWnSNQ@mail.gmail.com>
 <E9FE091E16764429B3F1D61D4D77E35F@gmail.com> <CANGvG8rTgNNWMb5fZ1JrTywnTTuU7X+Jbt6uuj9k7mt9Z4mogw@mail.gmail.com>
 <1E3C4CA4A28D4CCBA401A3BA63503779@gmail.com> <524DDF2A01BF4A58A9614BC73F999760@gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Tue, 15 Apr 2014 07:41:34 +0100
Message-ID: <CAMAsSdJGMCgoZcP8J6BMCtBFB8bUCY-dNjmSyBMrvCOT-h7KoA@mail.gmail.com>
Subject: Re: Tests failed after assembling the latest code from github
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Good call -- indeed that same Files class has a move() method that
will try to use renameTo() and then fall back to copy() and delete()
if needed for this very reason.


On Tue, Apr 15, 2014 at 6:34 AM, Ye Xianjin <advancedxy@gmail.com> wrote:
> Hi, I think I have found the cause of the tests failing.
>
> I have two disks on my laptop. The spark project dir is on an HDD disk while the tempdir created by google.io.Files.createTempDir is the /var/folders/5q/.... ,which is on the system disk, an SSD.
> The ExecutorLoaderSuite test uses org.apache.spark.TestUtils.createdCompiledClass methods.
> The createCompiledClass method first generates the compiled class in the pwd(spark/repl), thens use renameTo to move
> the file. The renameTo method fails because the dest file is in a different filesystem than the source file.
>
> I modify the TestUtils.scala to first copy the file to dest then delete the original file. The tests go smoothly.
> Should I issue an jira about this problem? Then I can send a pr on Github.

From dev-return-7341-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 15 06:52:03 2014
Return-Path: <dev-return-7341-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B25A310EF3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Apr 2014 06:52:03 +0000 (UTC)
Received: (qmail 54668 invoked by uid 500); 15 Apr 2014 06:52:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54211 invoked by uid 500); 15 Apr 2014 06:52:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54199 invoked by uid 99); 15 Apr 2014 06:52:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Apr 2014 06:52:00 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of advancedxy@gmail.com designates 209.85.160.46 as permitted sender)
Received: from [209.85.160.46] (HELO mail-pb0-f46.google.com) (209.85.160.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Apr 2014 06:51:54 +0000
Received: by mail-pb0-f46.google.com with SMTP id rq2so9176913pbb.33
        for <dev@spark.apache.org>; Mon, 14 Apr 2014 23:51:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=MUEzH59puPkk2FtHrdsDU8CSLYJi4PN4yk4s1g6tA4g=;
        b=kJ0W88Y/vuSkjiwr/H6zAW/zHB11dlctcAe2hyBqNmkwgb/DEylXs5XVxoHp4Lfekq
         of+P2xd/XCEYOjORn2yvugw4G0wfSgbbMzmpM28yi5hiC0cqfp/cy6aqC93Fhv/v97oZ
         tVOQIIEN+H8YNYlfJlOSjzl+Iza5IAroQOCaV/voxXR7dcCdhiD3HIHwqGM9V6tmzS+m
         /Wa2TgMa0sfYW1WOFzIo+bkDs3q2EJXxtYOrRNCW2X1F6/V6imkf9u9XhGkfCj6K35F/
         6t0Qas/rCird9c21NNT73JnSptIUdrhpsoWUw1XLTqrYg/5e4ctAaFGs0A+lQZobgS72
         jJCw==
X-Received: by 10.66.136.17 with SMTP id pw17mr110050pab.86.1397544691103;
        Mon, 14 Apr 2014 23:51:31 -0700 (PDT)
Received: from [10.240.138.131] ([123.58.191.68])
        by mx.google.com with ESMTPSA id de5sm38072461pbc.66.2014.04.14.23.51.27
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Mon, 14 Apr 2014 23:51:30 -0700 (PDT)
Date: Tue, 15 Apr 2014 14:51:21 +0800
From: Ye Xianjin <advancedxy@gmail.com>
To: dev@spark.apache.org
Message-ID: <89E840C73B1C45C1ACA3637ACBD675F3@gmail.com>
In-Reply-To: <CAMAsSdJGMCgoZcP8J6BMCtBFB8bUCY-dNjmSyBMrvCOT-h7KoA@mail.gmail.com>
References: <5C3428E1B84C4E09BC5A79CAF0A10FDA@gmail.com>
 <CAAswR-4Wrbh82xPGXM3BoQmDfExBBro7U9w8hqTMpMDAyWnSNQ@mail.gmail.com>
 <E9FE091E16764429B3F1D61D4D77E35F@gmail.com>
 <CANGvG8rTgNNWMb5fZ1JrTywnTTuU7X+Jbt6uuj9k7mt9Z4mogw@mail.gmail.com>
 <1E3C4CA4A28D4CCBA401A3BA63503779@gmail.com>
 <524DDF2A01BF4A58A9614BC73F999760@gmail.com>
 <CAMAsSdJGMCgoZcP8J6BMCtBFB8bUCY-dNjmSyBMrvCOT-h7KoA@mail.gmail.com>
Subject: Re: Tests failed after assembling the latest code from github
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="534cd6e9_57a61a29_139"
X-Virus-Checked: Checked by ClamAV on apache.org

--534cd6e9_57a61a29_139
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

@Sean Owen, Thanks for your advice.
 There are still some failing tests on my laptop. I will work on this issue(file move) as soon as I figure out other test related issues.


-- 
Ye Xianjin
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)


On Tuesday, April 15, 2014 at 2:41 PM, Sean Owen wrote:

> Good call -- indeed that same Files class has a move() method that
> will try to use renameTo() and then fall back to copy() and delete()
> if needed for this very reason.
> 
> 
> On Tue, Apr 15, 2014 at 6:34 AM, Ye Xianjin <advancedxy@gmail.com (mailto:advancedxy@gmail.com)> wrote:
> > Hi, I think I have found the cause of the tests failing.
> > 
> > I have two disks on my laptop. The spark project dir is on an HDD disk while the tempdir created by google.io.Files.createTempDir is the /var/folders/5q/.... ,which is on the system disk, an SSD.
> > The ExecutorLoaderSuite test uses org.apache.spark.TestUtils.createdCompiledClass methods.
> > The createCompiledClass method first generates the compiled class in the pwd(spark/repl), thens use renameTo to move
> > the file. The renameTo method fails because the dest file is in a different filesystem than the source file.
> > 
> > I modify the TestUtils.scala to first copy the file to dest then delete the original file. The tests go smoothly.
> > Should I issue an jira about this problem? Then I can send a pr on Github.
> > 
> 
> 
> 



--534cd6e9_57a61a29_139--


From dev-return-7342-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 15 16:35:12 2014
Return-Path: <dev-return-7342-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3E54B1107B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Apr 2014 16:35:12 +0000 (UTC)
Received: (qmail 17008 invoked by uid 500); 15 Apr 2014 16:35:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16372 invoked by uid 500); 15 Apr 2014 16:35:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16360 invoked by uid 99); 15 Apr 2014 16:35:08 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Apr 2014 16:35:08 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mark@clearstorydata.com designates 74.125.82.46 as permitted sender)
Received: from [74.125.82.46] (HELO mail-wg0-f46.google.com) (74.125.82.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Apr 2014 16:35:04 +0000
Received: by mail-wg0-f46.google.com with SMTP id b13so9863914wgh.5
        for <dev@spark.apache.org>; Tue, 15 Apr 2014 09:34:42 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=EW3g+K+jHr2pMTN3LbrIofqbhffMfs4Af+wA5AxQsIc=;
        b=Y2KEJvy18Fc1f2ME3+rAAH+YP2IXaXOJOJ37z3XNSZKKBB785i1y5maeMvFVPISKVR
         ggYICU4h3D73ZkJkVpHFPSVCzkK58TQWCtqFZq45ogmUjk945SiVqFXj0+JJF5LsEYRi
         ln6I3TvRGmWVysntvFN+VWZOBBiUPVJoEpyMJRRrto3d6+iN/SmAT1MqHM5CJtdpqUl/
         wzOMkD5Gu8vL/AzcpjbOzAL9imawzHs+jPLql275YG0jDQpz08TFxLxv/d06S58/uRTZ
         5iaZjKE7WUGpwAX4UlQG/DuGJk3hhPaEDF2Zf/2/oveHqZytG5RIHHFcJjQihzkKa8RH
         N0Ng==
X-Gm-Message-State: ALoCoQlNh5lCURZQcEUVfcG8t+VdoPppPtBk3/DVZh6VKTVqeWPTp9NoUeXQmPvoG6UrcPwI1e1b
MIME-Version: 1.0
X-Received: by 10.181.12.103 with SMTP id ep7mr3260159wid.43.1397579681981;
 Tue, 15 Apr 2014 09:34:41 -0700 (PDT)
Received: by 10.216.232.69 with HTTP; Tue, 15 Apr 2014 09:34:41 -0700 (PDT)
In-Reply-To: <C197363CC38348CA9ED47E0BEE6950C0@gmail.com>
References: <CAEYYnxb5nwQ7TUBgeXjzApsTPsDSVucgRAJHG2rDV5ABD1-ajA@mail.gmail.com>
	<C197363CC38348CA9ED47E0BEE6950C0@gmail.com>
Date: Tue, 15 Apr 2014 09:34:41 -0700
Message-ID: <CAAsvFPk_3O_gevx-Kou5BMBpVfvT6K_T-HXiABF2avOtdK39cQ@mail.gmail.com>
Subject: Re: It seems that jenkins for PR is not working
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d0438eddb7d7f9a04f717602a
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0438eddb7d7f9a04f717602a
Content-Type: text/plain; charset=ISO-8859-1

The RAT path issue is now fixed, but it appears to me that some recent
change has dramatically altered the behavior of the testing framework, so
that I am now seeing many individual tests taking more than a minute to run
and the complete test run taking a very, very long time.  I expect that
this is what is causing Jenkins to now timeout repeatedly.


On Mon, Apr 14, 2014 at 1:32 PM, Nan Zhu <zhunanmcgill@gmail.com> wrote:

> +1....
>
> --
> Nan Zhu
>
>
> On Friday, April 11, 2014 at 5:35 PM, DB Tsai wrote:
>
> > I always got
> > =========================================================================
> >
> > Could not find Apache license headers in the following files:
> > !????? /root/workspace/SparkPullRequestBuilder/python/metastore/db.lck
> > !?????
> /root/workspace/SparkPullRequestBuilder/python/metastore/service.properties
> >
> >
> > Sincerely,
> >
> > DB Tsai
> > -------------------------------------------------------
> > My Blog: https://www.dbtsai.com
> > LinkedIn: https://www.linkedin.com/in/dbtsai
> >
> >
>
>
>

--f46d0438eddb7d7f9a04f717602a--

From dev-return-7343-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 16 05:15:09 2014
Return-Path: <dev-return-7343-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3840511953
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 16 Apr 2014 05:15:09 +0000 (UTC)
Received: (qmail 84043 invoked by uid 500); 16 Apr 2014 05:15:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83971 invoked by uid 500); 16 Apr 2014 05:15:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83945 invoked by uid 99); 16 Apr 2014 05:15:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Apr 2014 05:15:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.52 as permitted sender)
Received: from [209.85.219.52] (HELO mail-oa0-f52.google.com) (209.85.219.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Apr 2014 05:14:59 +0000
Received: by mail-oa0-f52.google.com with SMTP id l6so11726386oag.25
        for <dev@spark.apache.org>; Tue, 15 Apr 2014 22:14:37 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=w+pM7MBB4EUwLKUim5BUf/mBiXQ1IyjpL4prLtw1QGo=;
        b=RytEC3bV6d9xzzRCeumlMmCDI95HQCpBK/z2cNDBWEU2dCphitFTLaMjxlNhdaA5U1
         eyjp6/I2DBWCRtUN9m2/5JPEHkO2dxdTvaMkSDhxQsyWLLWsoqO2KESR+aipPvjckKmR
         wbSQvUnygoyQ+CUCGIqd8ElJam35ukG/hHwsM5jD3aauKg+0Al/8HdtWHE/IXPHAAKTh
         UTntDEyHXBXFxypZcu2rvmBekyyo5OiqZNnYrLwqECVHojDaaK5vf2W391Zm0+xBO1Du
         IJBOwmaAc43G8i3fpDtNLWfQQw8WY8pCv8LzAZQvmp8eQhA+4MP0p9C9f15iOo0wMxPg
         tfjg==
MIME-Version: 1.0
X-Received: by 10.182.22.18 with SMTP id z18mr244204obe.42.1397625277294; Tue,
 15 Apr 2014 22:14:37 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Tue, 15 Apr 2014 22:14:37 -0700 (PDT)
In-Reply-To: <CAAsvFPk_3O_gevx-Kou5BMBpVfvT6K_T-HXiABF2avOtdK39cQ@mail.gmail.com>
References: <CAEYYnxb5nwQ7TUBgeXjzApsTPsDSVucgRAJHG2rDV5ABD1-ajA@mail.gmail.com>
	<C197363CC38348CA9ED47E0BEE6950C0@gmail.com>
	<CAAsvFPk_3O_gevx-Kou5BMBpVfvT6K_T-HXiABF2avOtdK39cQ@mail.gmail.com>
Date: Tue, 15 Apr 2014 22:14:37 -0700
Message-ID: <CABPQxstWBL==dXPCNjGZ8ot1TVWyvn1QJUNv229VJCdOKxu9_g@mail.gmail.com>
Subject: Re: It seems that jenkins for PR is not working
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11332d162eae1304f721fe74
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11332d162eae1304f721fe74
Content-Type: text/plain; charset=ISO-8859-1

There are a few things going on here wrt tests.

1. I fixed up the RAT issues with a hotfix.

2. The Hive tests were actually disabled for a while accidentally. A recent
fix correctly re-enabled them. Without Hive Spark tests run in about 40
minutes and with Hive it runs in 1 hour and 15 minutes, so it's a big
difference.

To ease things I committed a patch today that only runs the Hive tests if
the change touches Spark SQL. So this should make it simpler for normal
tests.

We can actually generalize this to do much finer grained testing, e.g. if
something in MLLib changes we don't need to re-run the streaming tests.
I've added this JIRA to track it:
https://issues.apache.org/jira/browse/SPARK-1455

3. Overall we've experienced more race conditions with tests recently. I
noticed a few zombie test processes on Jenkins hogging up 100% of CPU so I
think this has triggered several previously unseen races due to CPU
contention on the test cluster. I killed them and we'll see if they crop up
again.

4. Please try to keep an eye on the length of new tests that get committed.
It's common to see people commit tests that e.g. sleep for several seconds
or do things that take a long time. Almost always this can be avoided and
usually avoiding it makes the test cleaner anyways (e.g. use proper
synchronization instead of sleeping).

- Patrick


On Tue, Apr 15, 2014 at 9:34 AM, Mark Hamstra <mark@clearstorydata.com>wrote:

> The RAT path issue is now fixed, but it appears to me that some recent
> change has dramatically altered the behavior of the testing framework, so
> that I am now seeing many individual tests taking more than a minute to run
> and the complete test run taking a very, very long time.  I expect that
> this is what is causing Jenkins to now timeout repeatedly.
>
>
> On Mon, Apr 14, 2014 at 1:32 PM, Nan Zhu <zhunanmcgill@gmail.com> wrote:
>
> > +1....
> >
> > --
> > Nan Zhu
> >
> >
> > On Friday, April 11, 2014 at 5:35 PM, DB Tsai wrote:
> >
> > > I always got
> > >
> =========================================================================
> > >
> > > Could not find Apache license headers in the following files:
> > > !????? /root/workspace/SparkPullRequestBuilder/python/metastore/db.lck
> > > !?????
> >
> /root/workspace/SparkPullRequestBuilder/python/metastore/service.properties
> > >
> > >
> > > Sincerely,
> > >
> > > DB Tsai
> > > -------------------------------------------------------
> > > My Blog: https://www.dbtsai.com
> > > LinkedIn: https://www.linkedin.com/in/dbtsai
> > >
> > >
> >
> >
> >
>

--001a11332d162eae1304f721fe74--

From dev-return-7344-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 16 17:28:46 2014
Return-Path: <dev-return-7344-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4F51A11BF1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 16 Apr 2014 17:28:46 +0000 (UTC)
Received: (qmail 46738 invoked by uid 500); 16 Apr 2014 17:28:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46698 invoked by uid 500); 16 Apr 2014 17:28:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46685 invoked by uid 99); 16 Apr 2014 17:28:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Apr 2014 17:28:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.182 as permitted sender)
Received: from [209.85.214.182] (HELO mail-ob0-f182.google.com) (209.85.214.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Apr 2014 17:28:39 +0000
Received: by mail-ob0-f182.google.com with SMTP id uz6so12792037obc.13
        for <multiple recipients>; Wed, 16 Apr 2014 10:28:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=gagTV/RQHkEfc1LtD01aBZcyTKCOf5/rJkPU0l9X1oY=;
        b=QvouoYB7sQuiZYScagFc0HXBfjZHoCnMB3lj9OoCSnIhkaydlQ5WdyTkpv0gM3fm4g
         CcOtzKn30lBVuYHMNVKONFe+Een2o2+O9RI4t9/cJsgFAin1bO4XmzR2dwDhAihN5qB2
         IsPbNRtIW21Ym7fjgmlQb6lSRrV+zZ3X3Ty5uHBfy0RpxXMNbnToQVpx/bOUWLrYAQ2W
         hmmcaoFckLLxFOHRdmTba/AIgpGz0/CzGfP5prTNjImwc/8z6XujCGcXmZH2S5WYJz91
         xDeMX04PlckA+e8kVBkVSIXOV6fHEIPHsRl6rEy3SI8AXKorv1Z6iT2APt6mnJzH+oZ+
         A3lw==
MIME-Version: 1.0
X-Received: by 10.60.77.35 with SMTP id p3mr3010195oew.46.1397669298916; Wed,
 16 Apr 2014 10:28:18 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Wed, 16 Apr 2014 10:28:18 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Wed, 16 Apr 2014 10:28:18 -0700 (PDT)
In-Reply-To: <JIRA.12708432.1397530390606.126308.1397668578795@arcas>
References: <JIRA.12708432.1397530390606@arcas>
	<JIRA.12708432.1397530390606.126308.1397668578795@arcas>
Date: Wed, 16 Apr 2014 10:28:18 -0700
Message-ID: <CABPQxstJJVLS=VNMtoCTNNeOV6br94MaYRfUv02hmo9pr=U5Cw@mail.gmail.com>
Subject: Re: [jira] [Commented] (SPARK-1496) SparkContext.jarOfClass should
 return Option instead of a sequence
From: Patrick Wendell <pwendell@gmail.com>
To: dev@spark.apache.org
Cc: issues@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b33d2441353c904f72c3e48
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b33d2441353c904f72c3e48
Content-Type: text/plain; charset=ISO-8859-1

Just option [string] is fine. Happy to accept a fix but I'll probably
submit one tonight if no one else has.

---
sent from my phone
On Apr 16, 2014 10:16 AM, "haosdent (JIRA)" <jira@apache.org> wrote:

>
>     [
> https://issues.apache.org/jira/browse/SPARK-1496?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13971671#comment-13971671]
>
> haosdent commented on SPARK-1496:
> ---------------------------------
>
> Is it should return Option[Seq[String]]? Maybe I could help you fix this
> issue. :-)
>
> > SparkContext.jarOfClass should return Option instead of a sequence
> > ------------------------------------------------------------------
> >
> >                 Key: SPARK-1496
> >                 URL: https://issues.apache.org/jira/browse/SPARK-1496
> >             Project: Spark
> >          Issue Type: Improvement
> >          Components: Spark Core
> >            Reporter: Patrick Wendell
> >            Assignee: Patrick Wendell
> >             Fix For: 1.0.0
> >
> >
> > This is pretty confusing, especially since addJar expects to take a
> single jar.
>
>
>
> --
> This message was sent by Atlassian JIRA
> (v6.2#6252)
>

--047d7b33d2441353c904f72c3e48--

From dev-return-7345-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 17 19:04:32 2014
Return-Path: <dev-return-7345-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A9786108C4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Apr 2014 19:04:32 +0000 (UTC)
Received: (qmail 74327 invoked by uid 500); 17 Apr 2014 19:04:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74279 invoked by uid 500); 17 Apr 2014 19:04:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74265 invoked by uid 99); 17 Apr 2014 19:04:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 19:04:23 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dlieu.7@gmail.com designates 209.85.219.42 as permitted sender)
Received: from [209.85.219.42] (HELO mail-oa0-f42.google.com) (209.85.219.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 19:04:18 +0000
Received: by mail-oa0-f42.google.com with SMTP id i4so896731oah.29
        for <dev@spark.incubator.apache.org>; Thu, 17 Apr 2014 12:03:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=Qh2Q5FNwztQ9/nIWVdAEz+IIg2ebh5khuTfm6tK1FZo=;
        b=y2axZMBK3tEgQobluctHBcKAAPOICsm4e+WNFqdVrM+HlA1tH2tMopsV1OKZXHaQwL
         +B1QFBKlghIQT3XxBzcAAxPfIhNX3DePtOHQW7BA0R7cnxnIuyayJWTV8/VVnmaa30C+
         OR72+4JxTXYIzIDHPihiBUT3GzrtEh9jLsH5DKLIExT7YxUwPGqfIoMDH3QKvFC2+dLZ
         5LbsqebWeUzL5th7Hsh3CqvILPlAMiKq5BIdGHsQ7zi+fixXMcYF+obon1GvtR51BehY
         DNg9+NWR1t6mFbQTW6ehhEf7yiOSDGwN2LUFIcFKbXXJtqSipSGvfIXgmIt1ryXTftJ9
         idRQ==
MIME-Version: 1.0
X-Received: by 10.182.66.202 with SMTP id h10mr8694005obt.38.1397761437306;
 Thu, 17 Apr 2014 12:03:57 -0700 (PDT)
Received: by 10.76.77.3 with HTTP; Thu, 17 Apr 2014 12:03:57 -0700 (PDT)
Date: Thu, 17 Apr 2014 12:03:57 -0700
Message-ID: <CAPud8ToO-NyJMbn_0DBS2u-iEYktDK7CXW8JCJyu_3G6sifwCw@mail.gmail.com>
Subject: Double lhbase dependency in spark 0.9.1
From: Dmitriy Lyubimov <dlieu.7@gmail.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e0160c34ef3933004f741b150
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160c34ef3933004f741b150
Content-Type: text/plain; charset=UTF-8

Not sure if I am seeing double.

SparkBuild.scala for 0.9.1 has dobule hbase declaration

      "org.apache.hbase"     %  "hbase"           % "0.94.6"
excludeAll(excludeNetty, excludeAsm),
      "org.apache.hbase" % "hbase" % HBASE_VERSION excludeAll(excludeNetty,
excludeAsm),


as a result i am not getting the right version of hbase here. Perhaps the
old declaration crept in during a merge at some point?

-d

--089e0160c34ef3933004f741b150--

From dev-return-7346-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 17 19:09:45 2014
Return-Path: <dev-return-7346-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DF8E7108EC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Apr 2014 19:09:44 +0000 (UTC)
Received: (qmail 93720 invoked by uid 500); 17 Apr 2014 19:09:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93674 invoked by uid 500); 17 Apr 2014 19:09:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93666 invoked by uid 99); 17 Apr 2014 19:09:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 19:09:43 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.128.176 as permitted sender)
Received: from [209.85.128.176] (HELO mail-ve0-f176.google.com) (209.85.128.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 19:09:39 +0000
Received: by mail-ve0-f176.google.com with SMTP id db11so1017396veb.35
        for <dev@spark.apache.org>; Thu, 17 Apr 2014 12:09:19 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=902E89FYWyVTnSLZZ1+AS4TwF4OxV5ESvWScTet59/k=;
        b=gJNGGFgGIYr+gv+gK796R3SqIS7fmfcH2OlZeYrlk0EtkNXLebYxCn06ba/Yo4gkGP
         sZT+/Jao+1i9bmxjSnT9ctC2LQ4Ki8hHMAZ02UtpFL5vOPgY+b7nUaUSCMFQIVtC0On6
         E20frYDbLkiH8MUXPTx8cTGcZYwu0Th5j1W7wtFmI2pqvSWpZSO0vrSJwH0CECfc6tVM
         251tWqsXrIXmQw0lk0qYYjxNMq/MkVaolVhCd+12LLBRGP+PJiPCcBQZJtDB7V8FnW0Z
         xnygS6uPk+SUK6VJeF8ND9E8mrxJzFo1+fkio+OnDeoiH/Y21QVdFY/pH20uglK66DOL
         UGkA==
X-Gm-Message-State: ALoCoQl35yFhBvMH2oLV0vDq5ibwy6x2IiQ+afcuuSrylL9QFWpES7qHo+jMLyPAm+SMvd3Ddq9c
X-Received: by 10.52.26.161 with SMTP id m1mr7503075vdg.24.1397761758914; Thu,
 17 Apr 2014 12:09:18 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.111.69 with HTTP; Thu, 17 Apr 2014 12:08:58 -0700 (PDT)
In-Reply-To: <CAPud8ToO-NyJMbn_0DBS2u-iEYktDK7CXW8JCJyu_3G6sifwCw@mail.gmail.com>
References: <CAPud8ToO-NyJMbn_0DBS2u-iEYktDK7CXW8JCJyu_3G6sifwCw@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Thu, 17 Apr 2014 20:08:58 +0100
Message-ID: <CAMAsSdKcKPd-FWrPH2xVjjSrdGL5LX1v=00f_Smp3CKbB4rN1g@mail.gmail.com>
Subject: Re: Double lhbase dependency in spark 0.9.1
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I remember that too, and it has been fixed already in master, but
maybe it was not included in 0.9.1:

https://github.com/apache/spark/blob/master/project/SparkBuild.scala#L367
--
Sean Owen | Director, Data Science | London


On Thu, Apr 17, 2014 at 8:03 PM, Dmitriy Lyubimov <dlieu.7@gmail.com> wrote:
> Not sure if I am seeing double.
>
> SparkBuild.scala for 0.9.1 has dobule hbase declaration
>
>       "org.apache.hbase"     %  "hbase"           % "0.94.6"
> excludeAll(excludeNetty, excludeAsm),
>       "org.apache.hbase" % "hbase" % HBASE_VERSION excludeAll(excludeNetty,
> excludeAsm),
>
>
> as a result i am not getting the right version of hbase here. Perhaps the
> old declaration crept in during a merge at some point?
>
> -d

From dev-return-7347-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 17 19:09:48 2014
Return-Path: <dev-return-7347-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 736DE108ED
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Apr 2014 19:09:48 +0000 (UTC)
Received: (qmail 94379 invoked by uid 500); 17 Apr 2014 19:09:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94333 invoked by uid 500); 17 Apr 2014 19:09:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94316 invoked by uid 99); 17 Apr 2014 19:09:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 19:09:45 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.128.174 as permitted sender)
Received: from [209.85.128.174] (HELO mail-ve0-f174.google.com) (209.85.128.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 19:09:41 +0000
Received: by mail-ve0-f174.google.com with SMTP id oz11so996578veb.5
        for <dev@spark.incubator.apache.org>; Thu, 17 Apr 2014 12:09:19 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=902E89FYWyVTnSLZZ1+AS4TwF4OxV5ESvWScTet59/k=;
        b=VdV00GSG6LSVPdXL23J/N4ynw3UklcvkD4f0+bxxkAyXi+pr4IB/SlbACZxlrjBck9
         fFwKZxRh94r2B4UrnU9UEhRf+OGXMjUBFaYHkZbcvkwtpLohs7QAteiGaVDhC/hr/eIJ
         gd+rD/OSyfkZh5F3geXaBCYozT+QPPVbEZSSmLkaGFivitrciRgIuGE6GvhmRVBMt7pI
         /je46vT8opwvIA/a3Psj0zW4kueCjKJLYxMJfJ4DKnaCiwS5swX92L4mIrF61b3TeGQ5
         1L8+qJF3JHfbvmTm30z5tleosnV6XX5E7/XhpKIg7k4X+tV2dH9KIRzOULKJUxbETIxV
         1OFA==
X-Gm-Message-State: ALoCoQkIJTJUMvL4TaoAZzUqCe4lcTd/wurtcvBlAG4O9wc4iJlPyDLwobch14PW4ULvHDSMLxom
X-Received: by 10.52.26.161 with SMTP id m1mr7503075vdg.24.1397761758914; Thu,
 17 Apr 2014 12:09:18 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.111.69 with HTTP; Thu, 17 Apr 2014 12:08:58 -0700 (PDT)
In-Reply-To: <CAPud8ToO-NyJMbn_0DBS2u-iEYktDK7CXW8JCJyu_3G6sifwCw@mail.gmail.com>
References: <CAPud8ToO-NyJMbn_0DBS2u-iEYktDK7CXW8JCJyu_3G6sifwCw@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Thu, 17 Apr 2014 20:08:58 +0100
Message-ID: <CAMAsSdKcKPd-FWrPH2xVjjSrdGL5LX1v=00f_Smp3CKbB4rN1g@mail.gmail.com>
Subject: Re: Double lhbase dependency in spark 0.9.1
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I remember that too, and it has been fixed already in master, but
maybe it was not included in 0.9.1:

https://github.com/apache/spark/blob/master/project/SparkBuild.scala#L367
--
Sean Owen | Director, Data Science | London


On Thu, Apr 17, 2014 at 8:03 PM, Dmitriy Lyubimov <dlieu.7@gmail.com> wrote:
> Not sure if I am seeing double.
>
> SparkBuild.scala for 0.9.1 has dobule hbase declaration
>
>       "org.apache.hbase"     %  "hbase"           % "0.94.6"
> excludeAll(excludeNetty, excludeAsm),
>       "org.apache.hbase" % "hbase" % HBASE_VERSION excludeAll(excludeNetty,
> excludeAsm),
>
>
> as a result i am not getting the right version of hbase here. Perhaps the
> old declaration crept in during a merge at some point?
>
> -d

From dev-return-7348-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 17 20:59:00 2014
Return-Path: <dev-return-7348-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B999B10E3A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Apr 2014 20:59:00 +0000 (UTC)
Received: (qmail 54457 invoked by uid 500); 17 Apr 2014 20:59:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54418 invoked by uid 500); 17 Apr 2014 20:58:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54410 invoked by uid 99); 17 Apr 2014 20:58:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 20:58:59 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of zhazhan@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 20:58:55 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <zhazhan@gmail.com>)
	id 1WatOE-0000U6-PO
	for dev@spark.incubator.apache.org; Thu, 17 Apr 2014 13:58:34 -0700
Date: Thu, 17 Apr 2014 13:58:34 -0700 (PDT)
From: Zhan Zhang <zhazhan@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1397768314748-6331.post@n3.nabble.com>
Subject: Spark REPL question
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Please help, I am knew to both Spark and scala. 

I am trying to figure out how spark distribute the task to workers in REPL.
I only found the place where task is serialized and sent, and workers
deserialize and load the task with the class name by ExecutorClassLoader.
But I didn't find how the driver uploaded the REPL generated .class/jar file
by REPL to file server/hdfs. My understanding is that the worker has to know
the class as well to instantiate the task.

Does anybody know where the code is (file or function name) or my
undertanding is wrong?

Thanks.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-REPL-question-tp6331.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7350-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 17 21:34:47 2014
Return-Path: <dev-return-7350-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 96E261000A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Apr 2014 21:34:47 +0000 (UTC)
Received: (qmail 34673 invoked by uid 500); 17 Apr 2014 21:34:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34626 invoked by uid 500); 17 Apr 2014 21:34:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34611 invoked by uid 99); 17 Apr 2014 21:34:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 21:34:36 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.173] (HELO mail-qc0-f173.google.com) (209.85.216.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 21:34:32 +0000
Received: by mail-qc0-f173.google.com with SMTP id r5so1011925qcx.4
        for <dev@spark.incubator.apache.org>; Thu, 17 Apr 2014 14:34:09 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=g06m6Xn8lKJ601ZhKo7J48LtJYfC4ZDenHu3tyBLftY=;
        b=lToIaa7C+EX5b6f0Jw6sQ8jAipq0m8jQHfUgwdNwh0MRge90SyFzXxPIlxPrbRXsb4
         p2hawvKwNHbQe0N9I/E6F+WtrsxA8cs5gT48HuNAhGSX/q6kYgdjqszhUhWm+a4SyCTD
         F65naZRRLcXgDC9AKHpV4lM8x+g/inp9YgP6Ihyfy/nSy8XT9SznGaqF0e2SabFmoEFp
         QcKFSb7R80cpXp0iO67pFGppHUCGIZLb8HM9+PNPoE03gHwEuFONGLBfDZ8SQxdrG1Yx
         DqnjkiM4YuIIOKHfctc1032MHfGOYLgti2hgUnHuUmtVigCHB3CdNK7zM84w2kulKSJe
         O9kQ==
X-Gm-Message-State: ALoCoQku3sVwSfiifcI2Eyjbgy2/wORdI0W0u3q+iw6sYHXadpqkpx6GOo5K8Ak/zXcx915EPDOr
X-Received: by 10.140.26.242 with SMTP id 105mr13297396qgv.92.1397770449562;
 Thu, 17 Apr 2014 14:34:09 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.68.2 with HTTP; Thu, 17 Apr 2014 14:33:49 -0700 (PDT)
In-Reply-To: <1397768314748-6331.post@n3.nabble.com>
References: <1397768314748-6331.post@n3.nabble.com>
From: Michael Armbrust <michael@databricks.com>
Date: Thu, 17 Apr 2014 14:33:49 -0700
Message-ID: <CAAswR-7N0S7TODJ7gZNtYbLKJUEh2kMYHNsh+EJXuULbWcjPog@mail.gmail.com>
Subject: Re: Spark REPL question
To: dev@spark.apache.org
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11c00a001fd88904f743cbc4
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c00a001fd88904f743cbc4
Content-Type: text/plain; charset=ISO-8859-1

The REPL spins up an org.apache.spark.HttpServer, which provides classes
that are generated by the REPL as well as jars from addJar.

Michael


On Thu, Apr 17, 2014 at 1:58 PM, Zhan Zhang <zhazhan@gmail.com> wrote:

> Please help, I am knew to both Spark and scala.
>
> I am trying to figure out how spark distribute the task to workers in REPL.
> I only found the place where task is serialized and sent, and workers
> deserialize and load the task with the class name by ExecutorClassLoader.
> But I didn't find how the driver uploaded the REPL generated .class/jar
> file
> by REPL to file server/hdfs. My understanding is that the worker has to
> know
> the class as well to instantiate the task.
>
> Does anybody know where the code is (file or function name) or my
> undertanding is wrong?
>
> Thanks.
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-REPL-question-tp6331.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c00a001fd88904f743cbc4--

From dev-return-7349-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 17 21:34:47 2014
Return-Path: <dev-return-7349-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9BDCF1000B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Apr 2014 21:34:47 +0000 (UTC)
Received: (qmail 34701 invoked by uid 500); 17 Apr 2014 21:34:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34597 invoked by uid 500); 17 Apr 2014 21:34:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34586 invoked by uid 99); 17 Apr 2014 21:34:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 21:34:35 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.175] (HELO mail-qc0-f175.google.com) (209.85.216.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 21:34:30 +0000
Received: by mail-qc0-f175.google.com with SMTP id e16so1020612qcx.6
        for <dev@spark.apache.org>; Thu, 17 Apr 2014 14:34:09 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=g06m6Xn8lKJ601ZhKo7J48LtJYfC4ZDenHu3tyBLftY=;
        b=Ysyqgckn0xGYWIzUQXGaR9arBSLGmftTvlWBGZ0CZMNPbaALqEL1sPDgFLBMga0irQ
         T3VbEec7+madbOCZ2DYhnNC1N3W9QwwuFPPuQLoomLFgmz8aUOvEQWj1e5eDZVLNsNIW
         JHH3yqZD6royd8Ws216zYjJhuZzwk6gTDNx4b+19zRpF6nsd0/3iOyF6J4T+xPYzX96a
         zZ0lFkbc0C0nGhWHyGH0rCGYT6ZrBqjP3ebcTWEHbtqNb1ay2npIr5S+rn0bX9PnQY3e
         gVGK5LoxRgAtE6Y8fMu40NY/nlvoU14SR3pEVHSN3DiDfrrWLH/xBgWPlKg9zB+OO+Hn
         zieA==
X-Gm-Message-State: ALoCoQnnLsAs3bd6/Wvz2MWGgmUu7OUZNhyaLC746gqqJ9bMvl4HBv3mH04zFjJy8J+ygD98C2SV
X-Received: by 10.140.26.242 with SMTP id 105mr13297396qgv.92.1397770449562;
 Thu, 17 Apr 2014 14:34:09 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.68.2 with HTTP; Thu, 17 Apr 2014 14:33:49 -0700 (PDT)
In-Reply-To: <1397768314748-6331.post@n3.nabble.com>
References: <1397768314748-6331.post@n3.nabble.com>
From: Michael Armbrust <michael@databricks.com>
Date: Thu, 17 Apr 2014 14:33:49 -0700
Message-ID: <CAAswR-7N0S7TODJ7gZNtYbLKJUEh2kMYHNsh+EJXuULbWcjPog@mail.gmail.com>
Subject: Re: Spark REPL question
To: dev@spark.apache.org
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11c00a001fd88904f743cbc4
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c00a001fd88904f743cbc4
Content-Type: text/plain; charset=ISO-8859-1

The REPL spins up an org.apache.spark.HttpServer, which provides classes
that are generated by the REPL as well as jars from addJar.

Michael


On Thu, Apr 17, 2014 at 1:58 PM, Zhan Zhang <zhazhan@gmail.com> wrote:

> Please help, I am knew to both Spark and scala.
>
> I am trying to figure out how spark distribute the task to workers in REPL.
> I only found the place where task is serialized and sent, and workers
> deserialize and load the task with the class name by ExecutorClassLoader.
> But I didn't find how the driver uploaded the REPL generated .class/jar
> file
> by REPL to file server/hdfs. My understanding is that the worker has to
> know
> the class as well to instantiate the task.
>
> Does anybody know where the code is (file or function name) or my
> undertanding is wrong?
>
> Thanks.
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-REPL-question-tp6331.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c00a001fd88904f743cbc4--

From dev-return-7351-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 17 21:47:49 2014
Return-Path: <dev-return-7351-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 99507100BA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Apr 2014 21:47:49 +0000 (UTC)
Received: (qmail 64723 invoked by uid 500); 17 Apr 2014 21:47:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64608 invoked by uid 500); 17 Apr 2014 21:47:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64599 invoked by uid 99); 17 Apr 2014 21:47:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 21:47:48 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of zhazhan@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 21:47:44 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <zhazhan@gmail.com>)
	id 1Wau9R-0004RL-MI
	for dev@spark.incubator.apache.org; Thu, 17 Apr 2014 14:47:21 -0700
Date: Thu, 17 Apr 2014 14:47:21 -0700 (PDT)
From: Zhan Zhang <zhazhan@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1397771241682-6333.post@n3.nabble.com>
In-Reply-To: <CAAswR-7N0S7TODJ7gZNtYbLKJUEh2kMYHNsh+EJXuULbWcjPog@mail.gmail.com>
References: <1397768314748-6331.post@n3.nabble.com> <CAAswR-7N0S7TODJ7gZNtYbLKJUEh2kMYHNsh+EJXuULbWcjPog@mail.gmail.com>
Subject: Re: Spark REPL question
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks a lot.

By "spins up", do you mean using the same directory, specified by following?

      /** Local directory to save .class files too */
      val outputDir = {
        val tmp = System.getProperty("java.io.tmpdir")
        val rootDir = new SparkConf().get("spark.repl.classdir",  tmp)
        Utils.createTempDir(rootDir)
      }
    val virtualDirectory                              = new
PlainFile(outputDir) // "directory" for classfiles
    val classServer                                   = new
HttpServer(outputDir)     /** Jetty server that will serve our classes to
worker nodes */



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-REPL-question-tp6331p6333.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7353-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 17 21:53:55 2014
Return-Path: <dev-return-7353-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D169F100F2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Apr 2014 21:53:55 +0000 (UTC)
Received: (qmail 81224 invoked by uid 500); 17 Apr 2014 21:53:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81087 invoked by uid 500); 17 Apr 2014 21:53:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81067 invoked by uid 99); 17 Apr 2014 21:53:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 21:53:52 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.43] (HELO mail-qg0-f43.google.com) (209.85.192.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 21:53:47 +0000
Received: by mail-qg0-f43.google.com with SMTP id a108so1025331qge.2
        for <dev@spark.incubator.apache.org>; Thu, 17 Apr 2014 14:53:25 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=HQT8mMupz25ZWmYaWIlEPw/oOSWNzpuDUjjv2TVLKCg=;
        b=kZGQ6r7sjXG7X3FUbNOifeOvlyOjdRNLkTTqOI/hiX8NEnhAkOY8yl15O3/0i/x71v
         iNiDCFIZ3+4zriDcEwtDJP4KnswckaxhOQisBC9P7HUxxN2BV8ORPoUOl/eEhePWiirM
         jbiYbQNb40AMs+77F3jAZeWSgOl0jnI0Cn2HHhOpNF849IqdryQmu/j95+5cd8G1cxp9
         vt7OAJI5p9Ro7z9umFBGwzZJL0h8xrkokJ0b0u4d+8SxVl8qOnUHx5cdsLNFK+ySNyeS
         +T8EDLIYPXFbdBgkG95j0qtjab25v79PZO5YGG+brnqPUTg9DSHR1Y5YCJEv4cvcWvC7
         yE/w==
X-Gm-Message-State: ALoCoQmuLlPdkISUBob6fd0cWi9bYUOz64+wRzhZUyQq2s5Y/upD9uvo1o08ZmYEGTvodlimpD3q
X-Received: by 10.224.20.72 with SMTP id e8mr13910587qab.86.1397771605342;
 Thu, 17 Apr 2014 14:53:25 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.68.2 with HTTP; Thu, 17 Apr 2014 14:53:05 -0700 (PDT)
In-Reply-To: <1397771241682-6333.post@n3.nabble.com>
References: <1397768314748-6331.post@n3.nabble.com> <CAAswR-7N0S7TODJ7gZNtYbLKJUEh2kMYHNsh+EJXuULbWcjPog@mail.gmail.com>
 <1397771241682-6333.post@n3.nabble.com>
From: Michael Armbrust <michael@databricks.com>
Date: Thu, 17 Apr 2014 14:53:05 -0700
Message-ID: <CAAswR-7f5y0zvSQhasF7NarPzpCvO0+NAf6-JSezF_1X3P-ftA@mail.gmail.com>
Subject: Re: Spark REPL question
To: dev@spark.apache.org
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11c1bfea0392a004f74410c6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1bfea0392a004f74410c6
Content-Type: text/plain; charset=ISO-8859-1

Yeah, I think that is correct.


On Thu, Apr 17, 2014 at 2:47 PM, Zhan Zhang <zhazhan@gmail.com> wrote:

> Thanks a lot.
>
> By "spins up", do you mean using the same directory, specified by
> following?
>
>       /** Local directory to save .class files too */
>       val outputDir = {
>         val tmp = System.getProperty("java.io.tmpdir")
>         val rootDir = new SparkConf().get("spark.repl.classdir",  tmp)
>         Utils.createTempDir(rootDir)
>       }
>     val virtualDirectory                              = new
> PlainFile(outputDir) // "directory" for classfiles
>     val classServer                                   = new
> HttpServer(outputDir)     /** Jetty server that will serve our classes to
> worker nodes */
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-REPL-question-tp6331p6333.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c1bfea0392a004f74410c6--

From dev-return-7352-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 17 21:53:56 2014
Return-Path: <dev-return-7352-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EB30B100F4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Apr 2014 21:53:55 +0000 (UTC)
Received: (qmail 81164 invoked by uid 500); 17 Apr 2014 21:53:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81076 invoked by uid 500); 17 Apr 2014 21:53:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81066 invoked by uid 99); 17 Apr 2014 21:53:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 21:53:52 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.54] (HELO mail-qa0-f54.google.com) (209.85.216.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 21:53:47 +0000
Received: by mail-qa0-f54.google.com with SMTP id w8so942580qac.27
        for <dev@spark.apache.org>; Thu, 17 Apr 2014 14:53:25 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=HQT8mMupz25ZWmYaWIlEPw/oOSWNzpuDUjjv2TVLKCg=;
        b=Pmwo4mkLWynObtzvQYjY9v4B6SvMzAVetN5dKe2+OK7eqFd5r7EShjEo2UKj5xFfRe
         tMNeBvuKJMGyNRte84BH2D96TwhlWGdhicyk5kQYJ6wnqmE/YvcG6ENVjrOQxzgOb0td
         AFmF0ambKr+zGC2WuCKloQe7iIst3cIaOD0f9XI0eDiki68DNBAOj6nGXQIt3qz8oDcb
         GpR++bHhI6osM95ee6kpGgNTrKweWHwHyrh8PcBQlSkAG5rsL1FTep/uXgSWEJNQ0GEA
         nfOjza3sQRi4uljfGqzPKV9YoznsJKTbw9Kwa3UBHbbvulkvFWqyZVeQ0x9FkorUGZNL
         5slA==
X-Gm-Message-State: ALoCoQmSV48fsAjmAET7yAgn/SjwLsFQehRlj8aJre5TerI0tWgTBhb1exlp38nO1OqHK0fub8pw
X-Received: by 10.224.20.72 with SMTP id e8mr13910587qab.86.1397771605342;
 Thu, 17 Apr 2014 14:53:25 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.68.2 with HTTP; Thu, 17 Apr 2014 14:53:05 -0700 (PDT)
In-Reply-To: <1397771241682-6333.post@n3.nabble.com>
References: <1397768314748-6331.post@n3.nabble.com> <CAAswR-7N0S7TODJ7gZNtYbLKJUEh2kMYHNsh+EJXuULbWcjPog@mail.gmail.com>
 <1397771241682-6333.post@n3.nabble.com>
From: Michael Armbrust <michael@databricks.com>
Date: Thu, 17 Apr 2014 14:53:05 -0700
Message-ID: <CAAswR-7f5y0zvSQhasF7NarPzpCvO0+NAf6-JSezF_1X3P-ftA@mail.gmail.com>
Subject: Re: Spark REPL question
To: dev@spark.apache.org
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11c1bfea0392a004f74410c6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1bfea0392a004f74410c6
Content-Type: text/plain; charset=ISO-8859-1

Yeah, I think that is correct.


On Thu, Apr 17, 2014 at 2:47 PM, Zhan Zhang <zhazhan@gmail.com> wrote:

> Thanks a lot.
>
> By "spins up", do you mean using the same directory, specified by
> following?
>
>       /** Local directory to save .class files too */
>       val outputDir = {
>         val tmp = System.getProperty("java.io.tmpdir")
>         val rootDir = new SparkConf().get("spark.repl.classdir",  tmp)
>         Utils.createTempDir(rootDir)
>       }
>     val virtualDirectory                              = new
> PlainFile(outputDir) // "directory" for classfiles
>     val classServer                                   = new
> HttpServer(outputDir)     /** Jetty server that will serve our classes to
> worker nodes */
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-REPL-question-tp6331p6333.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c1bfea0392a004f74410c6--

From dev-return-7354-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 17 22:04:40 2014
Return-Path: <dev-return-7354-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C4FBE10168
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Apr 2014 22:04:40 +0000 (UTC)
Received: (qmail 4126 invoked by uid 500); 17 Apr 2014 22:04:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4095 invoked by uid 500); 17 Apr 2014 22:04:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4086 invoked by uid 99); 17 Apr 2014 22:04:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 22:04:38 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of zhazhan@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 22:04:34 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <zhazhan@gmail.com>)
	id 1WauPk-0005Ui-9i
	for dev@spark.incubator.apache.org; Thu, 17 Apr 2014 15:04:12 -0700
Date: Thu, 17 Apr 2014 15:04:12 -0700 (PDT)
From: Zhan Zhang <zhazhan@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1397772252292-6335.post@n3.nabble.com>
In-Reply-To: <CAAswR-7f5y0zvSQhasF7NarPzpCvO0+NAf6-JSezF_1X3P-ftA@mail.gmail.com>
References: <1397768314748-6331.post@n3.nabble.com> <CAAswR-7N0S7TODJ7gZNtYbLKJUEh2kMYHNsh+EJXuULbWcjPog@mail.gmail.com> <1397771241682-6333.post@n3.nabble.com> <CAAswR-7f5y0zvSQhasF7NarPzpCvO0+NAf6-JSezF_1X3P-ftA@mail.gmail.com>
Subject: Re: Spark REPL question
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Clear to me now.

Thanks.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-REPL-question-tp6331p6335.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7355-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 17 23:35:14 2014
Return-Path: <dev-return-7355-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B8C22104B7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Apr 2014 23:35:14 +0000 (UTC)
Received: (qmail 35337 invoked by uid 500); 17 Apr 2014 23:35:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35249 invoked by uid 500); 17 Apr 2014 23:35:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35229 invoked by uid 99); 17 Apr 2014 23:35:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 23:35:10 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.128.176 as permitted sender)
Received: from [209.85.128.176] (HELO mail-ve0-f176.google.com) (209.85.128.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 23:35:06 +0000
Received: by mail-ve0-f176.google.com with SMTP id db11so1526220veb.21
        for <dev@spark.apache.org>; Thu, 17 Apr 2014 16:34:45 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=7pUOs0NfB/Pvj38x/qh0RZ76x9hXSFLlSYp5qF74zZ0=;
        b=RCDJH5kMul/ZWq/YNejPq8lwpgGQa0DiRYlqeCDWIUTgNjipBP+O8TrwUa7T4075g9
         /kx1AYJ4lbMyVoPXayDtJBTuiyuMiSYn8vy8PxJNniXfxpIbZMTOL8gNQgCy8NrRxBbx
         RXSaMKHt+b79jzypsskGS9UPE5xBA8jasRUk33Wp7/oJUJGo2jcV6+5djjcQD1Vl7cHD
         nCf90GLzg/zGDL/6eh+CCI7iaVfOM6gpLZvfVtAU8eCymleqist6kQ208IPQQLWgvSKd
         VOv6skUSyp0kdvnUan2wjKnFmSZBDzLUGE3WBw7MtnDPv4RSaceM6eima01c6XlvKP/q
         ueDQ==
X-Received: by 10.58.107.65 with SMTP id ha1mr14242677veb.1.1397777684984;
 Thu, 17 Apr 2014 16:34:44 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.191.72 with HTTP; Thu, 17 Apr 2014 16:34:14 -0700 (PDT)
In-Reply-To: <CAMAsSdKcKPd-FWrPH2xVjjSrdGL5LX1v=00f_Smp3CKbB4rN1g@mail.gmail.com>
References: <CAPud8ToO-NyJMbn_0DBS2u-iEYktDK7CXW8JCJyu_3G6sifwCw@mail.gmail.com>
 <CAMAsSdKcKPd-FWrPH2xVjjSrdGL5LX1v=00f_Smp3CKbB4rN1g@mail.gmail.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Thu, 17 Apr 2014 16:34:14 -0700
Message-ID: <CAMwrk0mXF+7+6qcLCb+VL69Gc2B6Tkc_ZGEgWz=eT+TL5qy9Ag@mail.gmail.com>
Subject: Re: Double lhbase dependency in spark 0.9.1
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e01229dfe637bf504f7457a99
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01229dfe637bf504f7457a99
Content-Type: text/plain; charset=UTF-8

Aaah, this should have been ported to Spark 0.9.1!

TD


On Thu, Apr 17, 2014 at 12:08 PM, Sean Owen <sowen@cloudera.com> wrote:

> I remember that too, and it has been fixed already in master, but
> maybe it was not included in 0.9.1:
>
> https://github.com/apache/spark/blob/master/project/SparkBuild.scala#L367
> --
> Sean Owen | Director, Data Science | London
>
>
> On Thu, Apr 17, 2014 at 8:03 PM, Dmitriy Lyubimov <dlieu.7@gmail.com>
> wrote:
> > Not sure if I am seeing double.
> >
> > SparkBuild.scala for 0.9.1 has dobule hbase declaration
> >
> >       "org.apache.hbase"     %  "hbase"           % "0.94.6"
> > excludeAll(excludeNetty, excludeAsm),
> >       "org.apache.hbase" % "hbase" % HBASE_VERSION
> excludeAll(excludeNetty,
> > excludeAsm),
> >
> >
> > as a result i am not getting the right version of hbase here. Perhaps the
> > old declaration crept in during a merge at some point?
> >
> > -d
>

--089e01229dfe637bf504f7457a99--

From dev-return-7356-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 17 23:35:15 2014
Return-Path: <dev-return-7356-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 446F9104B8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Apr 2014 23:35:15 +0000 (UTC)
Received: (qmail 35540 invoked by uid 500); 17 Apr 2014 23:35:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35421 invoked by uid 500); 17 Apr 2014 23:35:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35354 invoked by uid 99); 17 Apr 2014 23:35:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 23:35:11 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.128.175 as permitted sender)
Received: from [209.85.128.175] (HELO mail-ve0-f175.google.com) (209.85.128.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Apr 2014 23:35:07 +0000
Received: by mail-ve0-f175.google.com with SMTP id oz11so1520199veb.6
        for <dev@spark.incubator.apache.org>; Thu, 17 Apr 2014 16:34:45 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=7pUOs0NfB/Pvj38x/qh0RZ76x9hXSFLlSYp5qF74zZ0=;
        b=RCDJH5kMul/ZWq/YNejPq8lwpgGQa0DiRYlqeCDWIUTgNjipBP+O8TrwUa7T4075g9
         /kx1AYJ4lbMyVoPXayDtJBTuiyuMiSYn8vy8PxJNniXfxpIbZMTOL8gNQgCy8NrRxBbx
         RXSaMKHt+b79jzypsskGS9UPE5xBA8jasRUk33Wp7/oJUJGo2jcV6+5djjcQD1Vl7cHD
         nCf90GLzg/zGDL/6eh+CCI7iaVfOM6gpLZvfVtAU8eCymleqist6kQ208IPQQLWgvSKd
         VOv6skUSyp0kdvnUan2wjKnFmSZBDzLUGE3WBw7MtnDPv4RSaceM6eima01c6XlvKP/q
         ueDQ==
X-Received: by 10.58.107.65 with SMTP id ha1mr14242677veb.1.1397777684984;
 Thu, 17 Apr 2014 16:34:44 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.191.72 with HTTP; Thu, 17 Apr 2014 16:34:14 -0700 (PDT)
In-Reply-To: <CAMAsSdKcKPd-FWrPH2xVjjSrdGL5LX1v=00f_Smp3CKbB4rN1g@mail.gmail.com>
References: <CAPud8ToO-NyJMbn_0DBS2u-iEYktDK7CXW8JCJyu_3G6sifwCw@mail.gmail.com>
 <CAMAsSdKcKPd-FWrPH2xVjjSrdGL5LX1v=00f_Smp3CKbB4rN1g@mail.gmail.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Thu, 17 Apr 2014 16:34:14 -0700
Message-ID: <CAMwrk0mXF+7+6qcLCb+VL69Gc2B6Tkc_ZGEgWz=eT+TL5qy9Ag@mail.gmail.com>
Subject: Re: Double lhbase dependency in spark 0.9.1
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e01229dfe637bf504f7457a99
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01229dfe637bf504f7457a99
Content-Type: text/plain; charset=UTF-8

Aaah, this should have been ported to Spark 0.9.1!

TD


On Thu, Apr 17, 2014 at 12:08 PM, Sean Owen <sowen@cloudera.com> wrote:

> I remember that too, and it has been fixed already in master, but
> maybe it was not included in 0.9.1:
>
> https://github.com/apache/spark/blob/master/project/SparkBuild.scala#L367
> --
> Sean Owen | Director, Data Science | London
>
>
> On Thu, Apr 17, 2014 at 8:03 PM, Dmitriy Lyubimov <dlieu.7@gmail.com>
> wrote:
> > Not sure if I am seeing double.
> >
> > SparkBuild.scala for 0.9.1 has dobule hbase declaration
> >
> >       "org.apache.hbase"     %  "hbase"           % "0.94.6"
> > excludeAll(excludeNetty, excludeAsm),
> >       "org.apache.hbase" % "hbase" % HBASE_VERSION
> excludeAll(excludeNetty,
> > excludeAsm),
> >
> >
> > as a result i am not getting the right version of hbase here. Perhaps the
> > old declaration crept in during a merge at some point?
> >
> > -d
>

--089e01229dfe637bf504f7457a99--

From dev-return-7357-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 18 23:50:10 2014
Return-Path: <dev-return-7357-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E1DA0115C1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Apr 2014 23:50:10 +0000 (UTC)
Received: (qmail 17857 invoked by uid 500); 18 Apr 2014 23:50:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17741 invoked by uid 500); 18 Apr 2014 23:50:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17733 invoked by uid 99); 18 Apr 2014 23:50:08 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Apr 2014 23:50:08 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vanzin@cloudera.com designates 209.85.216.180 as permitted sender)
Received: from [209.85.216.180] (HELO mail-qc0-f180.google.com) (209.85.216.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Apr 2014 23:50:03 +0000
Received: by mail-qc0-f180.google.com with SMTP id w7so2170472qcr.25
        for <dev@spark.apache.org>; Fri, 18 Apr 2014 16:49:40 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=7Xa16KieuK694dW0ohzZEUgt4lFf+TbG5z8noVcIgn8=;
        b=IyoDrbbXXOB9iK0Bg0e7uHJKtjvPFbXX6ilA7iC4GFguVEaMUCANJqnew1NyOesEbz
         n0c159OhWfqlvBj4pQcqncjaB61BfMqzK/K9oeFoA22o2m9MGOMfTG5lQs/I5C0z1avL
         uBVvCnbjT8n2n00pwechDGFs6xownkqHR8CRWTCc0ouOeBTsA0vlaA4hGyAcFiknoelf
         iT14iqbAWGsdtYnbaOm36F3x9Mz7D0nIBpn/fYMV0ujiIvmNtLW5ekb0CzPAz2zz9pbA
         5TINppkf+U2UokxXhsD1j6KwKwf/POYgXW8fgdCPng33hZjUmGoDXGt3mktavPADHDi6
         6FJw==
X-Gm-Message-State: ALoCoQlTH7eDxz+fwGoNZRmmDTUzcKx4SH8Q56FJIkocWy3Fwr77vpJciWBuRjrKYWCt/gxI99Ny
MIME-Version: 1.0
X-Received: by 10.140.98.116 with SMTP id n107mr20381971qge.94.1397864980832;
 Fri, 18 Apr 2014 16:49:40 -0700 (PDT)
Received: by 10.229.211.5 with HTTP; Fri, 18 Apr 2014 16:49:40 -0700 (PDT)
Date: Fri, 18 Apr 2014 16:49:40 -0700
Message-ID: <CAAOnQ7sFEUsWXjPmxQKQ-3_GY+cQsCYQ1hbjTJrKxgJRBOPhuA@mail.gmail.com>
Subject: SparkListener questions
From: Marcelo Vanzin <vanzin@cloudera.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hello all,

I'm currently taking a look at how to hook Spark with the Application
Timeline Server (ATS) work going on in Yarn (YARN-1530). I've got a
reasonable idea of how the Yarn part works, and the basic idea of what
needs to be done in Spark, but I've run into a couple of issues with
the current listener framework and I'd like to ask for some guidance.

(i) SparkContext.addSparkListener() may miss events

Notably, it's not possible to add a listener to catch the initial
SparkListenerEnvironmentUpdate and SparkListenerApplicationStart
events; the second one is particularly important for something like an
event logger. The current EventLoggingListener gets around that by
being initialized by SparkContext itself, but for other listeners,
that option isn't available (and that doesn't seem like a scalable
solution either).

My initial idea was to add a "listeners" argument to the "big"
SparkContext constructor, but I'm open to different suggestions, since
I kinda dislike when constructors start growing too much. (It current
has 6 arguments, some of which have default values.) A builder-like
pattern could be an option (e.g.
SparkContext.newBuilder().setConf(...).addJars(...).addListener(...).build()).


(ii) Posting things to the ATS requires an ID.

If you look at the TimelineEntity class in Yarn, it requires both a
type (which would be something like "SparkApplication" for Spark) and
an ID. A SparkContext currently has no concept of an application ID (I
don't count name as an ID).

Using a random UUID is possible, but I think is ugly.

EventLoggingListener uses app name + System.currentTimeMillis, which
is better from a user-friendliness p.o.v. (if you ignore the
possibility of clashes).

But really my preferred solution here would be to use the Yarn
application id. That would make it easy to correlate this data with
more generic data kept by Yarn (see YARN-321).

The problem here is that we don't know this ID until way after the
SparkContext is created. I can see two different ways to solve this
issue.

A more hackish way would be expose the listeners to the Yarn code, so
that it can then find the Yarn-specific listener and trigger some
action to update the application id.

A more generic way would be to allow arbitrary events to be posted to
the bus, not just the ones declared in SparkListener.scala. This way,
the Yarn code could publish a "SparkYarnApplicationStarted" event that
has the information the listener wants, and other listeners could
potentially use that information too.

How do you guys feel about the latter?


Feedback here is greatly appreciated! :-)

-- 
Marcelo

From dev-return-7358-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Apr 19 23:45:52 2014
Return-Path: <dev-return-7358-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EBE6D109B9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 19 Apr 2014 23:45:51 +0000 (UTC)
Received: (qmail 11147 invoked by uid 500); 19 Apr 2014 23:45:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11108 invoked by uid 500); 19 Apr 2014 23:45:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11100 invoked by uid 99); 19 Apr 2014 23:45:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 19 Apr 2014 23:45:49 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.219.52 as permitted sender)
Received: from [209.85.219.52] (HELO mail-oa0-f52.google.com) (209.85.219.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 19 Apr 2014 23:45:46 +0000
Received: by mail-oa0-f52.google.com with SMTP id l6so2967025oag.25
        for <dev@spark.apache.org>; Sat, 19 Apr 2014 16:45:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=7Sah2y6v0Yu6dOlFdRwyeB8Tpo/Svd1FNjxavU9CMIM=;
        b=boKxXPfBOEu7IGfNJcRiFj2XSLedOPEajn42nMJdj3La5vlE9x4FPZZcG51prTYEuH
         fbP3NK72l7zgpBsTzQePd+4vbzyVc/7+U5I1DZljXLPKNKGuE4n8flKK4dyev1dYqzwm
         2agU8I+eNPbKOieh9ymGeoPzJlVrkivdXi6r+wSWb/k6UUK+jH8AdzYYqj2HNvpLiB2Q
         WcShUFWPTsR/ldExDI51V4odGxA6PvGQN2ILxvG+4EicmwvZs8//dH9JQmOY55P2bTrC
         nn3Vf2aQj8Tcv4LML1n1lW9Tm4zkeTfkBmUZCBEqaQQZKUGCLVmYqkst2FI89d4fVfws
         SSVQ==
MIME-Version: 1.0
X-Received: by 10.60.93.168 with SMTP id cv8mr24391712oeb.21.1397951123278;
 Sat, 19 Apr 2014 16:45:23 -0700 (PDT)
Received: by 10.182.246.164 with HTTP; Sat, 19 Apr 2014 16:45:23 -0700 (PDT)
Date: Sat, 19 Apr 2014 16:45:23 -0700
Message-ID: <CA+B-+fwuMM7-kwqzdR3+dyCMtFq6gKkGfYMZ4GcAFu_3LnKOQg@mail.gmail.com>
Subject: Publish spark jars to artifactory
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b33d3c01dd07204f76ddca7
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b33d3c01dd07204f76ddca7
Content-Type: text/plain; charset=UTF-8

Hi,

I saw in the code that spark jars are published on sonatype but I was
wondering if you guys have published spark jars to artifactory
as...Cloudera uses artifactory...

Somehow I can publish maven projects to artifactory but after following the
sbt link:

http://www.scala-sbt.org/release/docs/Detailed-Topics/Publishing.html

And trying various things, still I have not been able to publish the jars
on artifactory...

Any github example that publishes sbt project to artifactory would be
really helpful...

Thanks.
Deb

--047d7b33d3c01dd07204f76ddca7--

From dev-return-7359-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Apr 20 10:45:39 2014
Return-Path: <dev-return-7359-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B5081111FC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 20 Apr 2014 10:45:39 +0000 (UTC)
Received: (qmail 9340 invoked by uid 500); 20 Apr 2014 10:45:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8842 invoked by uid 500); 20 Apr 2014 10:45:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8834 invoked by uid 99); 20 Apr 2014 10:45:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Apr 2014 10:45:35 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of liqingyang1985@gmail.com designates 209.85.212.177 as permitted sender)
Received: from [209.85.212.177] (HELO mail-wi0-f177.google.com) (209.85.212.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Apr 2014 10:45:30 +0000
Received: by mail-wi0-f177.google.com with SMTP id cc10so944677wib.16
        for <dev@spark.apache.org>; Sun, 20 Apr 2014 03:45:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=ey81TfdO8A3uq5CmyGp7qKs8VIo+iykpO9wT+PiHSt8=;
        b=MAHSBe9YAdxXVUEoNiv3bCvg+4aWleg0Zg0HU1tZM+/ndkGq9HEDNy5Q4i35HOO66D
         42J3ldMLqPhuFX+ygvFZYkOsiTyElCefCgPcOJ6RFl2NJEGqjnKiG4jnupQdLfdHr9t/
         XCfLUN78EixJeCK+B1UZm/KK1z9wrYFp9XLEbVBWQswbDGdyO118pcsxl7Dww4b6F8Vo
         XcUzKJtbJhru044sT/+8qJCvN/In9XTzdcLeqtEJs5NMQc2n9Q4777RU+fd0PXlS7YMK
         dKDPKCd0zvq5bKA9nBs3bafzlQBzFzrQqdGwPeN1pi+YUb7R2xQ02VwefRfakHBLLp0B
         QsPA==
MIME-Version: 1.0
X-Received: by 10.180.105.72 with SMTP id gk8mr9629472wib.32.1397990708654;
 Sun, 20 Apr 2014 03:45:08 -0700 (PDT)
Received: by 10.194.61.39 with HTTP; Sun, 20 Apr 2014 03:45:08 -0700 (PDT)
Date: Sun, 20 Apr 2014 18:45:08 +0800
Message-ID: <CABDsqqZx+o7=Nwn3H_vU-dEcavs_OLDU5XXnTAnsEof9cX6emw@mail.gmail.com>
Subject: does shark0.9.1 work well with hadoop2.2.0 ?
From: qingyang li <liqingyang1985@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d0442685a96b67b04f77713b2
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0442685a96b67b04f77713b2
Content-Type: text/plain; charset=UTF-8

shark 0.9.1 is using protobuf 2.4.1 , but hadoop2.2.0 is using
protobuf2.5.0,
how can we make them work together?
I have tried replace protobuf2.4.1 in shark with protobuf2.5.0, it does not
work.
I have also tried replacing protobuf2.5.0 in hadoop with shark's 2.4.1, it
does not work too.

--f46d0442685a96b67b04f77713b2--

From dev-return-7360-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Apr 20 15:03:41 2014
Return-Path: <dev-return-7360-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 35BA311484
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 20 Apr 2014 15:03:41 +0000 (UTC)
Received: (qmail 58538 invoked by uid 500); 20 Apr 2014 15:03:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58505 invoked by uid 500); 20 Apr 2014 15:03:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58497 invoked by uid 99); 20 Apr 2014 15:03:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Apr 2014 15:03:39 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yuzhihong@gmail.com designates 209.85.160.171 as permitted sender)
Received: from [209.85.160.171] (HELO mail-yk0-f171.google.com) (209.85.160.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Apr 2014 15:03:34 +0000
Received: by mail-yk0-f171.google.com with SMTP id q9so2730550ykb.16
        for <dev@spark.apache.org>; Sun, 20 Apr 2014 08:03:12 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=ZPwBumRYAvaWaBh7jlKaw56KwiXmfOCMCWTMP0fKk0c=;
        b=qGSTDgA4VqeUp9sFz0XoIpVusmW8fn4wL2B+6WpiEY9zEP/X3BWRPt/BnHU3MrDgb1
         duj7YekCpRqzgufhLtXflicIp63B12Mf7+ADS1rFZ6py4LkzrrhAdydlAt7NxviMqJBp
         zCKbW19aPQzBbg8Aoy47K856S+Zc8QYzY0KuceaZN3xoNkLvXRgwgHP7Cu99gtRddsYH
         iFfx1EhJWfjQafQJdE8CqYhQQ2NIYTNUihLyaLduK4fqxXbEpUZO4sYLBfrAN/vyOf2n
         NpAfP77oK2JGaV2+3zotxFBMVn/ucOxzge+sTs8Sj/nPNDRVcl1MTkaUv7QyvYkFTAZF
         9khQ==
MIME-Version: 1.0
X-Received: by 10.236.87.139 with SMTP id y11mr45815949yhe.27.1398006191935;
 Sun, 20 Apr 2014 08:03:11 -0700 (PDT)
Received: by 10.170.79.130 with HTTP; Sun, 20 Apr 2014 08:03:11 -0700 (PDT)
In-Reply-To: <CABDsqqZx+o7=Nwn3H_vU-dEcavs_OLDU5XXnTAnsEof9cX6emw@mail.gmail.com>
References: <CABDsqqZx+o7=Nwn3H_vU-dEcavs_OLDU5XXnTAnsEof9cX6emw@mail.gmail.com>
Date: Sun, 20 Apr 2014 08:03:11 -0700
Message-ID: <CALte62yA11ofnrez0wmQVFC-OrYEMsnTJFFdNJkkFedwqatLZw@mail.gmail.com>
Subject: Re: does shark0.9.1 work well with hadoop2.2.0 ?
From: Ted Yu <yuzhihong@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=20cf301b63e976d59b04f77aae1d
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf301b63e976d59b04f77aae1d
Content-Type: text/plain; charset=UTF-8

bq. I have tried replace protobuf2.4.1 in shark with protobuf2.5.0

Did you replace the jar file or did you change the following in pom.xml and
rebuild ?
    <protobuf.version>2.4.1</protobuf.version>

Cheers


On Sun, Apr 20, 2014 at 3:45 AM, qingyang li <liqingyang1985@gmail.com>wrote:

> shark 0.9.1 is using protobuf 2.4.1 , but hadoop2.2.0 is using
> protobuf2.5.0,
> how can we make them work together?
> I have tried replace protobuf2.4.1 in shark with protobuf2.5.0, it does not
> work.
> I have also tried replacing protobuf2.5.0 in hadoop with shark's 2.4.1, it
> does not work too.
>

--20cf301b63e976d59b04f77aae1d--

From dev-return-7361-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Apr 20 15:53:56 2014
Return-Path: <dev-return-7361-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B44AA11517
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 20 Apr 2014 15:53:56 +0000 (UTC)
Received: (qmail 92358 invoked by uid 500); 20 Apr 2014 15:53:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92263 invoked by uid 500); 20 Apr 2014 15:53:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92254 invoked by uid 99); 20 Apr 2014 15:53:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Apr 2014 15:53:55 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of gwang@gopivotal.com designates 209.85.192.52 as permitted sender)
Received: from [209.85.192.52] (HELO mail-qg0-f52.google.com) (209.85.192.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Apr 2014 15:53:50 +0000
Received: by mail-qg0-f52.google.com with SMTP id q107so3196943qgd.25
        for <dev@spark.apache.org>; Sun, 20 Apr 2014 08:53:29 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=XSSgLKdiCR8kDqRwaXc7A0coFAUVGfWrgGkq+R5ULXE=;
        b=WSgvaYLhsgnSpFlGWlr7Fl5vp49wYmnrpgyt0npSHvRxEfctrgOd5vJgyiPdcyb66w
         eGXNmiGVELxtxoq0qTPbTpjaj34pLTtI9F5gv4bAqU3ABo6hrw2s3IHdWQCW4WRKQyRs
         Li2gQaFstGyGoSqXkA6QKblCZLtWjZZnRB1fNdks98gmToXqFgxDyzL8XkGClIqv+K4t
         hf2MXYJI3IBPbBWu3O5k2KKk4LtdTESJ/eY1lcrCemRl80z374mqYU0TE0JvpfuPvSFS
         gmwNnQEvuIQRRLmZZKzJVD188R5rVatdDWUF0H4jA/k62izF5I19dCz48HRk2OxuqGqv
         4Qtg==
X-Gm-Message-State: ALoCoQkyN+3EzZ4xkDr6VzM0NlLbr7Oo6TnNkArhwiCZ/2dXt8CmQ1gVFGYnefW/ol8QRvVS1S1O
X-Received: by 10.140.51.74 with SMTP id t68mr14311125qga.50.1398009209413;
 Sun, 20 Apr 2014 08:53:29 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.229.106.132 with HTTP; Sun, 20 Apr 2014 08:53:09 -0700 (PDT)
In-Reply-To: <CABDsqqZx+o7=Nwn3H_vU-dEcavs_OLDU5XXnTAnsEof9cX6emw@mail.gmail.com>
References: <CABDsqqZx+o7=Nwn3H_vU-dEcavs_OLDU5XXnTAnsEof9cX6emw@mail.gmail.com>
From: Gordon Wang <gwang@gopivotal.com>
Date: Sun, 20 Apr 2014 23:53:09 +0800
Message-ID: <CAOwyor0CAmEbg5pjqNJGZmsC9x0VKWssBLmR3=vajZ5ROLTnbQ@mail.gmail.com>
Subject: Re: does shark0.9.1 work well with hadoop2.2.0 ?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1135235e52349704f77b6215
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1135235e52349704f77b6215
Content-Type: text/plain; charset=ISO-8859-1

replacing the jar is not enough.
You have to change protobuf dependency in shark's build script. and
recompile the source.

Protobuf 2.4.1 and 2.5.0 is not binary compatible.


On Sun, Apr 20, 2014 at 6:45 PM, qingyang li <liqingyang1985@gmail.com>wrote:

> shark 0.9.1 is using protobuf 2.4.1 , but hadoop2.2.0 is using
> protobuf2.5.0,
> how can we make them work together?
> I have tried replace protobuf2.4.1 in shark with protobuf2.5.0, it does not
> work.
> I have also tried replacing protobuf2.5.0 in hadoop with shark's 2.4.1, it
> does not work too.
>



-- 
Regards
Gordon Wang

--001a1135235e52349704f77b6215--

From dev-return-7362-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Apr 20 18:17:17 2014
Return-Path: <dev-return-7362-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C081D1169B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 20 Apr 2014 18:17:17 +0000 (UTC)
Received: (qmail 193 invoked by uid 500); 20 Apr 2014 18:17:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99974 invoked by uid 500); 20 Apr 2014 18:17:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99966 invoked by uid 99); 20 Apr 2014 18:17:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Apr 2014 18:17:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.178 as permitted sender)
Received: from [209.85.216.178] (HELO mail-qc0-f178.google.com) (209.85.216.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Apr 2014 18:17:10 +0000
Received: by mail-qc0-f178.google.com with SMTP id i8so3371644qcq.9
        for <dev@spark.apache.org>; Sun, 20 Apr 2014 11:16:48 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=mMbkYWDPjUtB4JIwmZZprOkrmqrAPvN5lORhUsI1+L8=;
        b=B1FS/IDD36GnxdlSkt4yu60lLX59l6lzfo+072g+9a8QCUY/c+Qj/LCzxi/PeL+lrN
         yHmluFup2JlnCs72L1CDEnjig/lpEEv7+80KW9Urp6rE0u6FncI5BF8fe8pVUdcTiq3B
         sy14BY+2bXyzihNSUpsQccNx2Lqv6OtLpzQARHthCyS+alwYq/ZvOsa285HoG4ifpw2D
         b3ERYD1rkUlCNKRsssLWJbE1/8Pkkd80lqt7xd7zMSNbN+xEe4Sz67GgrPsmp8QvqvDf
         HRq1/0KOm07oVrAW8lvWl20Gi9h2ttxmZZLQ916+kMxFZ9zbSG2FR+DV5KKVudf0KXvX
         7APw==
X-Gm-Message-State: ALoCoQnEY3SWzsi1PbR5v5NIc1CFcFOifDOATaPLRJgyDdb03R61EF4AvdDYwyQEtmjaD0kdx45R
MIME-Version: 1.0
X-Received: by 10.224.44.17 with SMTP id y17mr36291123qae.36.1398017808090;
 Sun, 20 Apr 2014 11:16:48 -0700 (PDT)
Received: by 10.140.94.2 with HTTP; Sun, 20 Apr 2014 11:16:47 -0700 (PDT)
Date: Sun, 20 Apr 2014 11:16:47 -0700
Message-ID: <CACBYxKKWvS5FpVyWSDJG=o3xgDm2XabwNHYuzxRg8CmOV7+K8A@mail.gmail.com>
Subject: all values for a key must fit in memory
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdc88e4d75c6f04f77d626e
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc88e4d75c6f04f77d626e
Content-Type: text/plain; charset=ISO-8859-1

Hey all,

After a shuffle / groupByKey, Hadoop MapReduce allows the values for a key
to not all fit in memory.  The current ShuffleFetcher.fetch API, which
doesn't distinguish between keys and values, only returning an Iterator[P],
seems incompatible with this.

Any thoughts on how we could achieve parity here?

-Sandy

--047d7bdc88e4d75c6f04f77d626e--

From dev-return-7363-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Apr 20 18:37:20 2014
Return-Path: <dev-return-7363-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D6021116C7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 20 Apr 2014 18:37:20 +0000 (UTC)
Received: (qmail 10482 invoked by uid 500); 20 Apr 2014 18:37:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10367 invoked by uid 500); 20 Apr 2014 18:37:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10359 invoked by uid 99); 20 Apr 2014 18:37:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Apr 2014 18:37:19 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.216.47 as permitted sender)
Received: from [209.85.216.47] (HELO mail-qa0-f47.google.com) (209.85.216.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Apr 2014 18:37:16 +0000
Received: by mail-qa0-f47.google.com with SMTP id m5so3190715qaj.34
        for <dev@spark.apache.org>; Sun, 20 Apr 2014 11:36:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=Qgo0ujEOSuTy/f6ZDjA+kxZWaFfTtTS5GkUz6qcsVlU=;
        b=ksCgKdM5T+LFiS5eWVyn6yx87/RgCwfI9SAuDTUOdXtwkWjCc8wuil8C2YP5sP+B3Y
         db4gmQgIgCsOYNLw6g4j6wuQ6SMe7OKgk2OlSjoVIgfIwDGDQ9gC/5PgWyEiP1aVtJpT
         JN/2SSWl4FaSxi4kFbC5eb/jcGFsJCP19cRuDk0TS2ATCLh+MlBy8KBfRl5IUl9gCT8f
         vcxs7BYFnOjtZazXmxdCDT9qD8AyWyhNgyvMiC+IkSqRsehxWzAm5vF5FjKNK7B6dMC8
         reyicb3phBWUlMldLMCxV4qjgEiIsyjMxVFlY7aWQ2vztTZLQ7XoVCYsNpcOGxsqPMYI
         U1rQ==
MIME-Version: 1.0
X-Received: by 10.224.38.209 with SMTP id c17mr35956325qae.11.1398019013420;
 Sun, 20 Apr 2014 11:36:53 -0700 (PDT)
Received: by 10.140.51.195 with HTTP; Sun, 20 Apr 2014 11:36:53 -0700 (PDT)
In-Reply-To: <CACBYxKKWvS5FpVyWSDJG=o3xgDm2XabwNHYuzxRg8CmOV7+K8A@mail.gmail.com>
References: <CACBYxKKWvS5FpVyWSDJG=o3xgDm2XabwNHYuzxRg8CmOV7+K8A@mail.gmail.com>
Date: Mon, 21 Apr 2014 00:06:53 +0530
Message-ID: <CAJiQeY+=QUjcF4vKckRnvv=G_k-QTmZ_0CCUHF=amyVjUfjNrw@mail.gmail.com>
Subject: Re: all values for a key must fit in memory
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

An iterator does not imply data has to be memory resident.
Think merge sort output as an iterator (disk backed).

Tom is actually planning to work on something similar with me on this
hopefully this or next month.

Regards,
Mridul


On Sun, Apr 20, 2014 at 11:46 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
> Hey all,
>
> After a shuffle / groupByKey, Hadoop MapReduce allows the values for a key
> to not all fit in memory.  The current ShuffleFetcher.fetch API, which
> doesn't distinguish between keys and values, only returning an Iterator[P],
> seems incompatible with this.
>
> Any thoughts on how we could achieve parity here?
>
> -Sandy

From dev-return-7364-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 21 00:56:14 2014
Return-Path: <dev-return-7364-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0691911B5D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Apr 2014 00:56:14 +0000 (UTC)
Received: (qmail 35205 invoked by uid 500); 21 Apr 2014 00:56:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35168 invoked by uid 500); 21 Apr 2014 00:56:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35160 invoked by uid 99); 21 Apr 2014 00:56:10 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 00:56:10 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.192.52 as permitted sender)
Received: from [209.85.192.52] (HELO mail-qg0-f52.google.com) (209.85.192.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 00:56:06 +0000
Received: by mail-qg0-f52.google.com with SMTP id q107so3550515qgd.11
        for <dev@spark.apache.org>; Sun, 20 Apr 2014 17:55:44 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=OKWApQIZAd6ystIU5PsGK6oNG3F8GY0hzHissiRuacE=;
        b=DYIf7yWkdnxD7cXBr6C/NgeS4ao+RH5+JQyUrccuWdeOpPCztJRq+n2k89bq2j7BR0
         hfcVUoQkHOmcsLGWQlJw/w4aZokWnusTOnskmI28BipeyeBsYcdfCwjLyb0/0aUwUZFF
         psl49rjdk3H9DGOZnv3N0DqQwikmedUzhpx4wkis1z4PVGR763sf8MCDTBtHrV7l5HVP
         iYw1esJCqTS/B981diezwWSFbdZIp+WsuFFQsocAXqehA8x2QYzFUGPTYdegz3uZDHKX
         9LMjpcGPb6+tAamq57jbT3ZjclYdtDV2FqDiT8YzbxzhQnFU8Hxx3+nbWSC0qEBQRTlF
         /GWg==
X-Gm-Message-State: ALoCoQlVyRD9znemb77eADLHhS8p4TlncZiPhEE6RNT5ciFH+qC9xkkZyAMN7zyeFvtaNAINAxKc
MIME-Version: 1.0
X-Received: by 10.224.167.80 with SMTP id p16mr35760044qay.62.1398041743984;
 Sun, 20 Apr 2014 17:55:43 -0700 (PDT)
Received: by 10.140.94.2 with HTTP; Sun, 20 Apr 2014 17:55:43 -0700 (PDT)
In-Reply-To: <CAJiQeY+=QUjcF4vKckRnvv=G_k-QTmZ_0CCUHF=amyVjUfjNrw@mail.gmail.com>
References: <CACBYxKKWvS5FpVyWSDJG=o3xgDm2XabwNHYuzxRg8CmOV7+K8A@mail.gmail.com>
	<CAJiQeY+=QUjcF4vKckRnvv=G_k-QTmZ_0CCUHF=amyVjUfjNrw@mail.gmail.com>
Date: Sun, 20 Apr 2014 17:55:43 -0700
Message-ID: <CACBYxK+b_JyBt-D20+UUz2Sdwift0w6NaJ0_yQM75KRHDsVPEQ@mail.gmail.com>
Subject: Re: all values for a key must fit in memory
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0149cf8a88393f04f782f561
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149cf8a88393f04f782f561
Content-Type: text/plain; charset=ISO-8859-1

The issue isn't that the Iterator[P] can't be disk-backed.  It's that, with
a groupBy, each P is a (Key, Values) tuple, and the entire tuple is read
into memory at once.  The ShuffledRDD is agnostic to what goes inside P.

On Sun, Apr 20, 2014 at 11:36 AM, Mridul Muralidharan <mridul@gmail.com>wrote:

> An iterator does not imply data has to be memory resident.
> Think merge sort output as an iterator (disk backed).
>
> Tom is actually planning to work on something similar with me on this
> hopefully this or next month.
>
> Regards,
> Mridul
>
>
> On Sun, Apr 20, 2014 at 11:46 PM, Sandy Ryza <sandy.ryza@cloudera.com>
> wrote:
> > Hey all,
> >
> > After a shuffle / groupByKey, Hadoop MapReduce allows the values for a
> key
> > to not all fit in memory.  The current ShuffleFetcher.fetch API, which
> > doesn't distinguish between keys and values, only returning an
> Iterator[P],
> > seems incompatible with this.
> >
> > Any thoughts on how we could achieve parity here?
> >
> > -Sandy
>

--089e0149cf8a88393f04f782f561--

From dev-return-7365-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 21 03:14:08 2014
Return-Path: <dev-return-7365-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A738911CD7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Apr 2014 03:14:08 +0000 (UTC)
Received: (qmail 96437 invoked by uid 500); 21 Apr 2014 03:14:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95792 invoked by uid 500); 21 Apr 2014 03:14:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95783 invoked by uid 99); 21 Apr 2014 03:14:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 03:14:02 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.52 as permitted sender)
Received: from [209.85.220.52] (HELO mail-pa0-f52.google.com) (209.85.220.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 03:13:57 +0000
Received: by mail-pa0-f52.google.com with SMTP id rd3so3259671pab.25
        for <dev@spark.apache.org>; Sun, 20 Apr 2014 20:13:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=2G9vwgDDrUfhPfTtbw6pINbdNASHpsLxleJvioyKnVI=;
        b=EQxXjmTqXbzuw2C3PQHTlqllI92B/2cvJYWkRfwaA7JlH0wLRSse7BvBbsonYW7Orw
         eFk6aPxmkOOinPybx5K3vqFOxqtwqrQTSb47fntLN55CNt7/iRPfZeaburB3mmAXo6MV
         S4Y2greM4YTKzZ/5UrPjkzFz4ZakJygRNdVoJ34L6A9AAvO20JuvMX8v1iN29cdDoo+Y
         JGzgdXGsP/zjLxFjLlQbsYE8dypiSkS1dH5+2TV0/5FadXLxz2W5/ICAfXMuR+RDg5TE
         salSGvdYCfd/7p//b+dEkXcm3UeZ5RCRTGvmXO0vprk7gMHdE8tHRBK+aLeo1BVfsCe3
         TsHw==
X-Received: by 10.66.254.166 with SMTP id aj6mr35686706pad.11.1398050014566;
        Sun, 20 Apr 2014 20:13:34 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id xo9sm178033044pab.18.2014.04.20.20.13.31
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 20 Apr 2014 20:13:32 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: all values for a key must fit in memory
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CACBYxK+b_JyBt-D20+UUz2Sdwift0w6NaJ0_yQM75KRHDsVPEQ@mail.gmail.com>
Date: Sun, 20 Apr 2014 20:13:29 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <FBFD63BC-23CC-44D2-A114-5BE07B641A82@gmail.com>
References: <CACBYxKKWvS5FpVyWSDJG=o3xgDm2XabwNHYuzxRg8CmOV7+K8A@mail.gmail.com> <CAJiQeY+=QUjcF4vKckRnvv=G_k-QTmZ_0CCUHF=amyVjUfjNrw@mail.gmail.com> <CACBYxK+b_JyBt-D20+UUz2Sdwift0w6NaJ0_yQM75KRHDsVPEQ@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

We=92ve updated the user-facing API of groupBy in 1.0 to allow this: =
https://issues.apache.org/jira/browse/SPARK-1271. The ShuffleFetcher API =
is internal to Spark, it doesn=92t really matter what it is because we =
can change it. But the problem before was that groupBy and cogroup were =
defined as returning (Key, Seq[Value]). Now they return (Key, =
Iterable[Value]), which will allow us to make the internal changes to =
allow spilling to disk within a key. This will happen after 1.0 though, =
but it will be doable without any changes to user programs.

Matei

On Apr 20, 2014, at 5:55 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:

> The issue isn't that the Iterator[P] can't be disk-backed.  It's that, =
with
> a groupBy, each P is a (Key, Values) tuple, and the entire tuple is =
read
> into memory at once.  The ShuffledRDD is agnostic to what goes inside =
P.
>=20
> On Sun, Apr 20, 2014 at 11:36 AM, Mridul Muralidharan =
<mridul@gmail.com>wrote:
>=20
>> An iterator does not imply data has to be memory resident.
>> Think merge sort output as an iterator (disk backed).
>>=20
>> Tom is actually planning to work on something similar with me on this
>> hopefully this or next month.
>>=20
>> Regards,
>> Mridul
>>=20
>>=20
>> On Sun, Apr 20, 2014 at 11:46 PM, Sandy Ryza =
<sandy.ryza@cloudera.com>
>> wrote:
>>> Hey all,
>>>=20
>>> After a shuffle / groupByKey, Hadoop MapReduce allows the values for =
a
>> key
>>> to not all fit in memory.  The current ShuffleFetcher.fetch API, =
which
>>> doesn't distinguish between keys and values, only returning an
>> Iterator[P],
>>> seems incompatible with this.
>>>=20
>>> Any thoughts on how we could achieve parity here?
>>>=20
>>> -Sandy
>>=20


From dev-return-7366-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 21 03:38:42 2014
Return-Path: <dev-return-7366-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 923EA11D19
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Apr 2014 03:38:42 +0000 (UTC)
Received: (qmail 11012 invoked by uid 500); 21 Apr 2014 03:38:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10760 invoked by uid 500); 21 Apr 2014 03:38:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10752 invoked by uid 99); 21 Apr 2014 03:38:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 03:38:40 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.41 as permitted sender)
Received: from [209.85.219.41] (HELO mail-oa0-f41.google.com) (209.85.219.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 03:38:36 +0000
Received: by mail-oa0-f41.google.com with SMTP id j17so3785495oag.14
        for <dev@spark.apache.org>; Sun, 20 Apr 2014 20:38:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=PgDPLxfJKm8NpQE7jxalMKmfvHh2fO1df7wQmN+mS9s=;
        b=Ch1z2ZT6n1qGZsVrLS41idtth6BZqCwuyeBhFkJQ+InyMAz6/VxKLoABsgrkSuil3V
         zEjSJkrz48QhggWW5lU3yzZG4CdAPGCjc1orYduEPf6137uO5aGncPok+/YV7Bq/GTlt
         EtYoUDoJ+xhm8jrvSfQtPSFQHFon5y0EEceHdCJ0N709RbyEyaYopRhJiPOOgOKAJaN6
         jGdyNJXZaBgBQSfeh1JBDlpBhQt6GkEwiQFNZlFfulaUBJA+qjWRs6TbQajGUkrlu+J1
         v5LFedWw394vlSIUTmtfs5Xn/NdyMpfNsvRUcVoHTXVU1YBYmIR8+G8Rk107bRpa1PAp
         ywhQ==
MIME-Version: 1.0
X-Received: by 10.60.63.12 with SMTP id c12mr29557755oes.23.1398051495341;
 Sun, 20 Apr 2014 20:38:15 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Sun, 20 Apr 2014 20:38:15 -0700 (PDT)
In-Reply-To: <FBFD63BC-23CC-44D2-A114-5BE07B641A82@gmail.com>
References: <CACBYxKKWvS5FpVyWSDJG=o3xgDm2XabwNHYuzxRg8CmOV7+K8A@mail.gmail.com>
	<CAJiQeY+=QUjcF4vKckRnvv=G_k-QTmZ_0CCUHF=amyVjUfjNrw@mail.gmail.com>
	<CACBYxK+b_JyBt-D20+UUz2Sdwift0w6NaJ0_yQM75KRHDsVPEQ@mail.gmail.com>
	<FBFD63BC-23CC-44D2-A114-5BE07B641A82@gmail.com>
Date: Sun, 20 Apr 2014 20:38:15 -0700
Message-ID: <CABPQxstgJG+BTVAvh9P=s_fVKdiVWszQ4oa4WEywzq4mnX=xPQ@mail.gmail.com>
Subject: Re: all values for a key must fit in memory
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c20ce6c1f3ea04f7853afc
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c20ce6c1f3ea04f7853afc
Content-Type: text/plain; charset=ISO-8859-1

Just wanted to mention - one common thing I've seen users do is use
groupByKey, then do something that is commutitive and associative once the
values are grouped. Really users here should be doing reduceByKey.

rdd.groupByKey().map{ case (key, values) => (key, values.sum))
rdd.reduceByKey(_ + _)

I've seen this happen particularly for users coming from MapReduce where
they are used to having to write their own combiners and it's not intuitive
that these functions are very different.

Sandy - have you heard from users who have a specific problems they can't
solve using an associative function? I'm sure they exist, but I wonder how
often it's this vs. they just don't understand they API.

I wonder if we should actually warn about this in the groupByKey
documentation.

- Patrick


On Sun, Apr 20, 2014 at 8:13 PM, Matei Zaharia <matei.zaharia@gmail.com>wrote:

> We've updated the user-facing API of groupBy in 1.0 to allow this:
> https://issues.apache.org/jira/browse/SPARK-1271. The ShuffleFetcher API
> is internal to Spark, it doesn't really matter what it is because we can
> change it. But the problem before was that groupBy and cogroup were defined
> as returning (Key, Seq[Value]). Now they return (Key, Iterable[Value]),
> which will allow us to make the internal changes to allow spilling to disk
> within a key. This will happen after 1.0 though, but it will be doable
> without any changes to user programs.
>
> Matei
>
> On Apr 20, 2014, at 5:55 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
>
> > The issue isn't that the Iterator[P] can't be disk-backed.  It's that,
> with
> > a groupBy, each P is a (Key, Values) tuple, and the entire tuple is read
> > into memory at once.  The ShuffledRDD is agnostic to what goes inside P.
> >
> > On Sun, Apr 20, 2014 at 11:36 AM, Mridul Muralidharan <mridul@gmail.com
> >wrote:
> >
> >> An iterator does not imply data has to be memory resident.
> >> Think merge sort output as an iterator (disk backed).
> >>
> >> Tom is actually planning to work on something similar with me on this
> >> hopefully this or next month.
> >>
> >> Regards,
> >> Mridul
> >>
> >>
> >> On Sun, Apr 20, 2014 at 11:46 PM, Sandy Ryza <sandy.ryza@cloudera.com>
> >> wrote:
> >>> Hey all,
> >>>
> >>> After a shuffle / groupByKey, Hadoop MapReduce allows the values for a
> >> key
> >>> to not all fit in memory.  The current ShuffleFetcher.fetch API, which
> >>> doesn't distinguish between keys and values, only returning an
> >> Iterator[P],
> >>> seems incompatible with this.
> >>>
> >>> Any thoughts on how we could achieve parity here?
> >>>
> >>> -Sandy
> >>
>
>

--001a11c20ce6c1f3ea04f7853afc--

From dev-return-7367-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 21 07:31:58 2014
Return-Path: <dev-return-7367-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 007141105F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Apr 2014 07:31:58 +0000 (UTC)
Received: (qmail 18302 invoked by uid 500); 21 Apr 2014 07:31:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17807 invoked by uid 500); 21 Apr 2014 07:31:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17799 invoked by uid 99); 21 Apr 2014 07:31:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 07:31:54 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.216.52 as permitted sender)
Received: from [209.85.216.52] (HELO mail-qa0-f52.google.com) (209.85.216.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 07:31:50 +0000
Received: by mail-qa0-f52.google.com with SMTP id s7so3598890qap.11
        for <dev@spark.apache.org>; Mon, 21 Apr 2014 00:31:28 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=PWftDVShr/yMQF2sjQIqX/ogLLZCGnFwzh09yJT4Oh4=;
        b=0vWrTeBdwH31qzCdzmEUXlg/dgk43As90BlGKLDfcMrqfCmon8q1QBd6BX0bpf6GMI
         ATLfW+N11J8kfrVwg3nd7Hu6qZJdRLboWkzACzhfBgAsrH9/3WbUBUNqq17CVAuxBU5y
         a2qslJfrVF0t7EcEG8nT0xL3Mf2o7fggi3tZnl/E/HPj6t7anjxTRjj6hFIOAtXH/+7R
         WqiUrnTVAi9triZwEuy3FTeny69wACpiJIIhELt6TQQ+F2abFqKBODK3DBS/RRcu4PrD
         L2X9F7mFcA4r9XgrWlrSQauG3k5p2iR42/TSVuC0IqfdLY51iLrCUi+aGyM8Qy/n9Lyn
         lZlg==
MIME-Version: 1.0
X-Received: by 10.140.41.80 with SMTP id y74mr613017qgy.104.1398065487941;
 Mon, 21 Apr 2014 00:31:27 -0700 (PDT)
Received: by 10.140.51.195 with HTTP; Mon, 21 Apr 2014 00:31:27 -0700 (PDT)
In-Reply-To: <CACBYxK+b_JyBt-D20+UUz2Sdwift0w6NaJ0_yQM75KRHDsVPEQ@mail.gmail.com>
References: <CACBYxKKWvS5FpVyWSDJG=o3xgDm2XabwNHYuzxRg8CmOV7+K8A@mail.gmail.com>
	<CAJiQeY+=QUjcF4vKckRnvv=G_k-QTmZ_0CCUHF=amyVjUfjNrw@mail.gmail.com>
	<CACBYxK+b_JyBt-D20+UUz2Sdwift0w6NaJ0_yQM75KRHDsVPEQ@mail.gmail.com>
Date: Mon, 21 Apr 2014 13:01:27 +0530
Message-ID: <CAJiQeYKtjcyzkvRrDhPzGk3DQ3DMSXNCFkPQCT_cxExVZfcLag@mail.gmail.com>
Subject: Re: all values for a key must fit in memory
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

As Matei mentioned, the Values is now an Iterable : which can be disk backed.
Does that not address the concern ?

@Patrick - we do have cases where the length of the sequence is large
and size per value is also non trivial : so we do need this :-)
Note that join is a trivial example where this is required (in our
current implementation).

Regards,
Mridul

On Mon, Apr 21, 2014 at 6:25 AM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
> The issue isn't that the Iterator[P] can't be disk-backed.  It's that, with
> a groupBy, each P is a (Key, Values) tuple, and the entire tuple is read
> into memory at once.  The ShuffledRDD is agnostic to what goes inside P.
>
> On Sun, Apr 20, 2014 at 11:36 AM, Mridul Muralidharan <mridul@gmail.com>wrote:
>
>> An iterator does not imply data has to be memory resident.
>> Think merge sort output as an iterator (disk backed).
>>
>> Tom is actually planning to work on something similar with me on this
>> hopefully this or next month.
>>
>> Regards,
>> Mridul
>>
>>
>> On Sun, Apr 20, 2014 at 11:46 PM, Sandy Ryza <sandy.ryza@cloudera.com>
>> wrote:
>> > Hey all,
>> >
>> > After a shuffle / groupByKey, Hadoop MapReduce allows the values for a
>> key
>> > to not all fit in memory.  The current ShuffleFetcher.fetch API, which
>> > doesn't distinguish between keys and values, only returning an
>> Iterator[P],
>> > seems incompatible with this.
>> >
>> > Any thoughts on how we could achieve parity here?
>> >
>> > -Sandy
>>

From dev-return-7368-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 21 09:50:21 2014
Return-Path: <dev-return-7368-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 42C6D1126C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Apr 2014 09:50:21 +0000 (UTC)
Received: (qmail 44317 invoked by uid 500); 21 Apr 2014 09:50:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44055 invoked by uid 500); 21 Apr 2014 09:50:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44047 invoked by uid 99); 21 Apr 2014 09:50:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 09:50:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.171 as permitted sender)
Received: from [209.85.216.171] (HELO mail-qc0-f171.google.com) (209.85.216.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 09:50:15 +0000
Received: by mail-qc0-f171.google.com with SMTP id c9so3785604qcz.2
        for <dev@spark.apache.org>; Mon, 21 Apr 2014 02:49:52 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=guZVtmifZxd87uykgCq4/5jB93O1Q2hmtfH6kqC0J2E=;
        b=QS1jWUXk9uIvLXeu1dLbMjIad+fvnhMTgiHZGqG2Bk1s01tsrUoEHS6+DULigTHLeO
         6B6Ul1z/CyjDSDJJxTubpnbnYpxsGavKpdWTg+IxzYtanSX7y20DmUUTk9D6JuSIlxSe
         Ouw2cAQuIdzxHtcmNWRXUtPeph8xNPUawuvEq2rTTjY/8YJ+4zckowIxDtVnY1/k3HcB
         V21k+TVqHlUJVfGuuu5bFLe+T2v5XoHclPCkqCzJB+IE0RRmpcySgXM9U91Cmi5ZU+LF
         05iLNbmLc/S5vJhx2HhV/zmL4AW2yXr9fhM6HkfCcFFeQlhy2LKnE4paNKkrCMC0Javx
         LPlg==
X-Gm-Message-State: ALoCoQkdx+WH+YV97ZORPgA/Bofbodo0oNfvO0rp3nvW1VawW/5mxis/huDHsPZqqnKCV5j7kn5O
MIME-Version: 1.0
X-Received: by 10.140.94.179 with SMTP id g48mr36474319qge.58.1398073792519;
 Mon, 21 Apr 2014 02:49:52 -0700 (PDT)
Received: by 10.140.94.2 with HTTP; Mon, 21 Apr 2014 02:49:52 -0700 (PDT)
In-Reply-To: <CAJiQeYKtjcyzkvRrDhPzGk3DQ3DMSXNCFkPQCT_cxExVZfcLag@mail.gmail.com>
References: <CACBYxKKWvS5FpVyWSDJG=o3xgDm2XabwNHYuzxRg8CmOV7+K8A@mail.gmail.com>
	<CAJiQeY+=QUjcF4vKckRnvv=G_k-QTmZ_0CCUHF=amyVjUfjNrw@mail.gmail.com>
	<CACBYxK+b_JyBt-D20+UUz2Sdwift0w6NaJ0_yQM75KRHDsVPEQ@mail.gmail.com>
	<CAJiQeYKtjcyzkvRrDhPzGk3DQ3DMSXNCFkPQCT_cxExVZfcLag@mail.gmail.com>
Date: Mon, 21 Apr 2014 02:49:52 -0700
Message-ID: <CACBYxKKmZ8ap6TSD3jAromsWF+g3DOq5e9=eGGoRiQVzb5117A@mail.gmail.com>
Subject: Re: all values for a key must fit in memory
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ab3ecc5f50304f78a6b0d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ab3ecc5f50304f78a6b0d
Content-Type: text/plain; charset=ISO-8859-1

Thanks Matei and Mridul - was basically wondering whether we would be able
to change the shuffle to accommodate this after 1.0, and from your answers
it sounds like we can.


On Mon, Apr 21, 2014 at 12:31 AM, Mridul Muralidharan <mridul@gmail.com>wrote:

> As Matei mentioned, the Values is now an Iterable : which can be disk
> backed.
> Does that not address the concern ?
>
> @Patrick - we do have cases where the length of the sequence is large
> and size per value is also non trivial : so we do need this :-)
> Note that join is a trivial example where this is required (in our
> current implementation).
>
> Regards,
> Mridul
>
> On Mon, Apr 21, 2014 at 6:25 AM, Sandy Ryza <sandy.ryza@cloudera.com>
> wrote:
> > The issue isn't that the Iterator[P] can't be disk-backed.  It's that,
> with
> > a groupBy, each P is a (Key, Values) tuple, and the entire tuple is read
> > into memory at once.  The ShuffledRDD is agnostic to what goes inside P.
> >
> > On Sun, Apr 20, 2014 at 11:36 AM, Mridul Muralidharan <mridul@gmail.com
> >wrote:
> >
> >> An iterator does not imply data has to be memory resident.
> >> Think merge sort output as an iterator (disk backed).
> >>
> >> Tom is actually planning to work on something similar with me on this
> >> hopefully this or next month.
> >>
> >> Regards,
> >> Mridul
> >>
> >>
> >> On Sun, Apr 20, 2014 at 11:46 PM, Sandy Ryza <sandy.ryza@cloudera.com>
> >> wrote:
> >> > Hey all,
> >> >
> >> > After a shuffle / groupByKey, Hadoop MapReduce allows the values for a
> >> key
> >> > to not all fit in memory.  The current ShuffleFetcher.fetch API, which
> >> > doesn't distinguish between keys and values, only returning an
> >> Iterator[P],
> >> > seems incompatible with this.
> >> >
> >> > Any thoughts on how we could achieve parity here?
> >> >
> >> > -Sandy
> >>
>

--001a113ab3ecc5f50304f78a6b0d--

From dev-return-7369-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 21 15:40:02 2014
Return-Path: <dev-return-7369-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3E97C1182F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Apr 2014 15:40:02 +0000 (UTC)
Received: (qmail 34484 invoked by uid 500); 21 Apr 2014 15:40:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34445 invoked by uid 500); 21 Apr 2014 15:40:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34436 invoked by uid 99); 21 Apr 2014 15:40:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 15:40:00 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of aliaksei.litouka@gmail.com designates 209.85.160.51 as permitted sender)
Received: from [209.85.160.51] (HELO mail-pb0-f51.google.com) (209.85.160.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 15:39:56 +0000
Received: by mail-pb0-f51.google.com with SMTP id uo5so3821647pbc.24
        for <dev@spark.apache.org>; Mon, 21 Apr 2014 08:39:35 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=e4JUH6vkIZWOlrExJsYPBeNliZwsF8GmIY5R61iZGqw=;
        b=0RobfsXGyjtO5V2HBPhhDxYiJSyU+2YxhUilmgx39dx8Lum3Xx3NZB20KTm5voRGZG
         QzZH9B76AfAgxyntngHGH0S8Mw10dFnk4yKJw+oMOpujFlKyN23Y2ynyhduTuzMPKGif
         NXpCLikSV0mNDDQxWhQ7qMq7znlQ+vSjiE6fbqDoYteJFStPvZoqD+/IFhC3CZ6gpeuc
         h5KLvLPZTWFXi29WKGSTtj8q+XnpNCEy6nzpsY03CvB+yohLnNSD52D8gGCZs2QgZU8z
         AwfLxiw5NnEWHUtMZrUBJH2fxC7iB7hLHjRmDbZuXLHASODRiPLNmIfsh3xtdV47mQR7
         uDHA==
MIME-Version: 1.0
X-Received: by 10.68.103.165 with SMTP id fx5mr8631416pbb.118.1398094775673;
 Mon, 21 Apr 2014 08:39:35 -0700 (PDT)
Received: by 10.70.94.3 with HTTP; Mon, 21 Apr 2014 08:39:35 -0700 (PDT)
Date: Mon, 21 Apr 2014 10:39:35 -0500
Message-ID: <CADSNrJHOT035G1xda8p9eZz-90uiWOb4CwgiY1r-d8m9V9LM+g@mail.gmail.com>
Subject: Any plans for new clustering algorithms?
From: Aliaksei Litouka <aliaksei.litouka@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b676062776c1704f78f4ec6
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b676062776c1704f78f4ec6
Content-Type: text/plain; charset=UTF-8

Hi, Spark developers.
Are there any plans for implementing new clustering algorithms in MLLib? As
far as I understand, current version of Spark ships with only one
clustering algorithm - K-Means. I want to contribute to Spark and I'm
thinking of adding more clustering algorithms - maybe
DBSCAN<http://en.wikipedia.org/wiki/DBSCAN>.
I can start working on it. Does anyone want to join me?

--047d7b676062776c1704f78f4ec6--

From dev-return-7370-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 21 15:59:45 2014
Return-Path: <dev-return-7370-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5D7811189D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Apr 2014 15:59:45 +0000 (UTC)
Received: (qmail 66695 invoked by uid 500); 21 Apr 2014 15:59:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66643 invoked by uid 500); 21 Apr 2014 15:59:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66635 invoked by uid 99); 21 Apr 2014 15:59:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 15:59:44 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.220.177 as permitted sender)
Received: from [209.85.220.177] (HELO mail-vc0-f177.google.com) (209.85.220.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 15:59:39 +0000
Received: by mail-vc0-f177.google.com with SMTP id if17so1164394vcb.36
        for <dev@spark.apache.org>; Mon, 21 Apr 2014 08:59:19 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=YDckIlOVdlFa+rToJ+1hFlnowsJu6thaDAf0w0Uw9ck=;
        b=bvSpuKMBWhYHWKFx5nxYJerZHL8Vb+zvQ1TdoVVKDRvmoGChvL4V158sL7pjdzgLKL
         otfVO52McUTXKBW2WpOZIxPnDCqUYT9k1Q5sR1Cq3ly8bJARISWWmTGq/nf1mmcWoUvT
         fVP+6iMkmqKQON/qxHf3RBdhKxxV7Ki8A+rttGRKCRbdS9uWRIdYbKU5VJSzBEv87O2e
         uBk9iSY2gbKNwUPX5+QdC3Mfs2Mk8M6qogcY9n/t3kr7HHJ7NrzWE2fJAGFundd67XKt
         1pdn21TPwHnIEX17i+oBXriS9XentqMVxQASPpPyYXWSG+SD6xjozpYrQY33uLmyUAcJ
         Kv0g==
X-Gm-Message-State: ALoCoQnnZkrGUK7jX9RbZnBHRgzlvWzEkR446niObWo0Gv8BlhlDgjgtVlQQWeJUz9U+3MfS3Pz0
X-Received: by 10.58.187.78 with SMTP id fq14mr34261954vec.9.1398095959077;
 Mon, 21 Apr 2014 08:59:19 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.111.69 with HTTP; Mon, 21 Apr 2014 08:58:59 -0700 (PDT)
In-Reply-To: <CADSNrJHOT035G1xda8p9eZz-90uiWOb4CwgiY1r-d8m9V9LM+g@mail.gmail.com>
References: <CADSNrJHOT035G1xda8p9eZz-90uiWOb4CwgiY1r-d8m9V9LM+g@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 21 Apr 2014 16:58:59 +0100
Message-ID: <CAMAsSdJ5em5SgjszP-NiA58u_NR_tgT2PC6Wwxi4W8VX0cvVgA@mail.gmail.com>
Subject: Re: Any plans for new clustering algorithms?
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Nobody asked me, and this is a comment on a broader question, not this
one, but:

In light of a number of recent items about adding more algorithms,
I'll say that I personally think an explosion of algorithms should
come after the MLlib "core" is more fully baked. I'm thinking of
finishing out the changes to vectors and matrices, for example. Things
are going to change significantly in the short term as people use the
algorithms and see how well the abstractions do or don't work. I've
seen another similar project suffer mightily from too many algorithms
too early, so maybe I'm just paranoid.

Anyway, long-term, I think lots of good algorithms is a right and
proper goal for MLlib, myself. Consistent approaches, representations
and APIs will make or break MLlib much more than having or not having
a particular algorithm. With the plumbing in place, writing the algo
is the fun easy part.
--
Sean Owen | Director, Data Science | London


On Mon, Apr 21, 2014 at 4:39 PM, Aliaksei Litouka
<aliaksei.litouka@gmail.com> wrote:
> Hi, Spark developers.
> Are there any plans for implementing new clustering algorithms in MLLib? As
> far as I understand, current version of Spark ships with only one
> clustering algorithm - K-Means. I want to contribute to Spark and I'm
> thinking of adding more clustering algorithms - maybe
> DBSCAN<http://en.wikipedia.org/wiki/DBSCAN>.
> I can start working on it. Does anyone want to join me?

From dev-return-7371-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 21 16:08:31 2014
Return-Path: <dev-return-7371-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AC758118CB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Apr 2014 16:08:31 +0000 (UTC)
Received: (qmail 82118 invoked by uid 500); 21 Apr 2014 16:08:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82008 invoked by uid 500); 21 Apr 2014 16:08:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82000 invoked by uid 99); 21 Apr 2014 16:08:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 16:08:30 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of evan.sparks@gmail.com designates 209.85.128.174 as permitted sender)
Received: from [209.85.128.174] (HELO mail-ve0-f174.google.com) (209.85.128.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 16:08:26 +0000
Received: by mail-ve0-f174.google.com with SMTP id oz11so8005779veb.19
        for <dev@spark.apache.org>; Mon, 21 Apr 2014 09:08:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=lnRu/4LfAmx2ckK8SqAeqYppP3l1MTiu9LatCs8ZgKU=;
        b=B+G90FkmjuLYpVx7IbzK5q50Qrqsdn+/aeWBkMbo9oq9RKucF8oJ7E/U3PJ1YIHyTt
         shrC7jhzS34J1WkfdYabf43Ofa2Z/xe4LBZX0ie8oWAqnrgloXglYvekTfcSH9wLOv7l
         hw6ZhJ6n/GMbpie6VFwr47+Vrhu2B18y7YN3oQi3rNYDGz4+EZa05dAhNvrz1aEMQ8u0
         52whgQj1S3RFwzUpPGHxFqvldryuU+FERMdJIOz6O+qwaSLLyFUexgqWGwQyzGGYgbZ7
         xqlOFX6My77D8za0OT2jBrSDMoSowsXP9Lqk4CH/2l1td8/lvLI3PB11MRPPtz12oAzU
         RzuQ==
X-Received: by 10.220.167.2 with SMTP id o2mr31750283vcy.8.1398096485454; Mon,
 21 Apr 2014 09:08:05 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.52.146.9 with HTTP; Mon, 21 Apr 2014 09:07:45 -0700 (PDT)
In-Reply-To: <CAMAsSdJ5em5SgjszP-NiA58u_NR_tgT2PC6Wwxi4W8VX0cvVgA@mail.gmail.com>
References: <CADSNrJHOT035G1xda8p9eZz-90uiWOb4CwgiY1r-d8m9V9LM+g@mail.gmail.com>
 <CAMAsSdJ5em5SgjszP-NiA58u_NR_tgT2PC6Wwxi4W8VX0cvVgA@mail.gmail.com>
From: "Evan R. Sparks" <evan.sparks@gmail.com>
Date: Mon, 21 Apr 2014 09:07:45 -0700
Message-ID: <CABjXkq5PnvkM7PUo-uif6kS4EVEobf+NJapGJgGtAOad23r_-g@mail.gmail.com>
Subject: Re: Any plans for new clustering algorithms?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e011618ce60985004f78fb42a
X-Virus-Checked: Checked by ClamAV on apache.org

--089e011618ce60985004f78fb42a
Content-Type: text/plain; charset=UTF-8

While DBSCAN and others would be welcome contributions, I couldn't agree
more with Sean.




On Mon, Apr 21, 2014 at 8:58 AM, Sean Owen <sowen@cloudera.com> wrote:

> Nobody asked me, and this is a comment on a broader question, not this
> one, but:
>
> In light of a number of recent items about adding more algorithms,
> I'll say that I personally think an explosion of algorithms should
> come after the MLlib "core" is more fully baked. I'm thinking of
> finishing out the changes to vectors and matrices, for example. Things
> are going to change significantly in the short term as people use the
> algorithms and see how well the abstractions do or don't work. I've
> seen another similar project suffer mightily from too many algorithms
> too early, so maybe I'm just paranoid.
>
> Anyway, long-term, I think lots of good algorithms is a right and
> proper goal for MLlib, myself. Consistent approaches, representations
> and APIs will make or break MLlib much more than having or not having
> a particular algorithm. With the plumbing in place, writing the algo
> is the fun easy part.
> --
> Sean Owen | Director, Data Science | London
>
>
> On Mon, Apr 21, 2014 at 4:39 PM, Aliaksei Litouka
> <aliaksei.litouka@gmail.com> wrote:
> > Hi, Spark developers.
> > Are there any plans for implementing new clustering algorithms in MLLib?
> As
> > far as I understand, current version of Spark ships with only one
> > clustering algorithm - K-Means. I want to contribute to Spark and I'm
> > thinking of adding more clustering algorithms - maybe
> > DBSCAN<http://en.wikipedia.org/wiki/DBSCAN>.
> > I can start working on it. Does anyone want to join me?
>

--089e011618ce60985004f78fb42a--

From dev-return-7372-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 21 16:33:20 2014
Return-Path: <dev-return-7372-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DD9B611988
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Apr 2014 16:33:20 +0000 (UTC)
Received: (qmail 24007 invoked by uid 500); 21 Apr 2014 16:33:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23954 invoked by uid 500); 21 Apr 2014 16:33:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23941 invoked by uid 99); 21 Apr 2014 16:33:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 16:33:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sang.venkatraman@gmail.com designates 209.85.220.171 as permitted sender)
Received: from [209.85.220.171] (HELO mail-vc0-f171.google.com) (209.85.220.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 16:33:13 +0000
Received: by mail-vc0-f171.google.com with SMTP id lg15so1231960vcb.16
        for <dev@spark.apache.org>; Mon, 21 Apr 2014 09:32:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=jeWL7HLvrbZyZDieRJPYkCeF9UZ17/RKHFQkcta8KRM=;
        b=spcRDEvL6aZLESxKeTYetoZtSjcmoomXHPMDZJSFxXFhe4CR1n6r6qgmhZn/o6mU/c
         F51LDf9Kzl2MIwInrjA+qmQAH4wbDe0sw9ISFKiaIEpL1epvLPOS0c6VqY8Ye2uL00OQ
         r+wkxlpbiImIgxsxLvD9pgKvrAmt4X7fwyYeXhUUGSpdfKXyfRjux+segqN9hR6cdxYQ
         heQDvuOQhywdxnTpN/dqbSlsM4SWIRTjYchsQlDwWiz26g3LYF+ZQMgVVa4g6swVopw4
         2Np1zjl2FQl+nJ5lHSLXvon7TiuyyS6HYa1wR4PNeE9Qh6tFVeGSivtX1uaZG/yt2Mbw
         AIDw==
X-Received: by 10.58.146.5 with SMTP id sy5mr85382veb.43.1398097970378; Mon,
 21 Apr 2014 09:32:50 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.142.71 with HTTP; Mon, 21 Apr 2014 09:32:30 -0700 (PDT)
In-Reply-To: <CABjXkq5PnvkM7PUo-uif6kS4EVEobf+NJapGJgGtAOad23r_-g@mail.gmail.com>
References: <CADSNrJHOT035G1xda8p9eZz-90uiWOb4CwgiY1r-d8m9V9LM+g@mail.gmail.com>
 <CAMAsSdJ5em5SgjszP-NiA58u_NR_tgT2PC6Wwxi4W8VX0cvVgA@mail.gmail.com> <CABjXkq5PnvkM7PUo-uif6kS4EVEobf+NJapGJgGtAOad23r_-g@mail.gmail.com>
From: Sang Venkatraman <sang.venkatraman@gmail.com>
Date: Mon, 21 Apr 2014 12:32:30 -0400
Message-ID: <CAAPGksBJDPTB24BMZ3OKKNQLYDB8-2sAFGazKhZRkb4r6pR_SA@mail.gmail.com>
Subject: Re: Any plans for new clustering algorithms?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b6743a2e2be9404f7900ce8
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6743a2e2be9404f7900ce8
Content-Type: text/plain; charset=UTF-8

Hi,

On a related note, I have not looked at the the MLlib library in detail but
are there plans on reusing or porting over parts of apache mahout.

Thanks,
Sang


On Mon, Apr 21, 2014 at 12:07 PM, Evan R. Sparks <evan.sparks@gmail.com>wrote:

> While DBSCAN and others would be welcome contributions, I couldn't agree
> more with Sean.
>
>
>
>
> On Mon, Apr 21, 2014 at 8:58 AM, Sean Owen <sowen@cloudera.com> wrote:
>
> > Nobody asked me, and this is a comment on a broader question, not this
> > one, but:
> >
> > In light of a number of recent items about adding more algorithms,
> > I'll say that I personally think an explosion of algorithms should
> > come after the MLlib "core" is more fully baked. I'm thinking of
> > finishing out the changes to vectors and matrices, for example. Things
> > are going to change significantly in the short term as people use the
> > algorithms and see how well the abstractions do or don't work. I've
> > seen another similar project suffer mightily from too many algorithms
> > too early, so maybe I'm just paranoid.
> >
> > Anyway, long-term, I think lots of good algorithms is a right and
> > proper goal for MLlib, myself. Consistent approaches, representations
> > and APIs will make or break MLlib much more than having or not having
> > a particular algorithm. With the plumbing in place, writing the algo
> > is the fun easy part.
> > --
> > Sean Owen | Director, Data Science | London
> >
> >
> > On Mon, Apr 21, 2014 at 4:39 PM, Aliaksei Litouka
> > <aliaksei.litouka@gmail.com> wrote:
> > > Hi, Spark developers.
> > > Are there any plans for implementing new clustering algorithms in
> MLLib?
> > As
> > > far as I understand, current version of Spark ships with only one
> > > clustering algorithm - K-Means. I want to contribute to Spark and I'm
> > > thinking of adding more clustering algorithms - maybe
> > > DBSCAN<http://en.wikipedia.org/wiki/DBSCAN>.
> > > I can start working on it. Does anyone want to join me?
> >
>

--047d7b6743a2e2be9404f7900ce8--

From dev-return-7373-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 21 16:40:31 2014
Return-Path: <dev-return-7373-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5ABCD119CA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Apr 2014 16:40:31 +0000 (UTC)
Received: (qmail 35482 invoked by uid 500); 21 Apr 2014 16:40:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35409 invoked by uid 500); 21 Apr 2014 16:40:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35401 invoked by uid 99); 21 Apr 2014 16:40:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 16:40:30 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nick.pentreath@gmail.com designates 209.85.219.43 as permitted sender)
Received: from [209.85.219.43] (HELO mail-oa0-f43.google.com) (209.85.219.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 16:40:26 +0000
Received: by mail-oa0-f43.google.com with SMTP id eb12so4434290oac.2
        for <dev@spark.apache.org>; Mon, 21 Apr 2014 09:40:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=2cl9n+HbFAZIjMlZPk3RxmZb9lIFGvezfncJLXuOUyg=;
        b=k9aJCJ8utemvq7Vul8LcsZNxlurijHOKpKvJOR1G/OyGWdJB1BbxGaWWPN50dwEXv8
         lyEpInAcQxiG8KzsYSLdBiL2amEPgxqWqDCE98BgsZaTA9/aVJFiAfy2KHHNEdIuWK3A
         vsQP3Oj2ONmoYIoFeLd9E/hlgX1H0TpdsCmI8ovz4Me8PJjzVZqOeDhp3cgXqIjZkB/Y
         93q9YGKYj/ACRuZNQb4DZGxiqqFoD9YARXqTuo4oVybjvUJMKG0JNCdpzWRN9RmCfac4
         Fg4FvWhCJsvO82M1+CUsYDC/uwFajlt7kES4NyI925Zsi9J7uAXGcSebzzaQxeHrXNeI
         uHIg==
MIME-Version: 1.0
X-Received: by 10.60.92.132 with SMTP id cm4mr2809936oeb.49.1398098405059;
 Mon, 21 Apr 2014 09:40:05 -0700 (PDT)
Received: by 10.182.95.103 with HTTP; Mon, 21 Apr 2014 09:40:04 -0700 (PDT)
In-Reply-To: <CABjXkq5PnvkM7PUo-uif6kS4EVEobf+NJapGJgGtAOad23r_-g@mail.gmail.com>
References: <CADSNrJHOT035G1xda8p9eZz-90uiWOb4CwgiY1r-d8m9V9LM+g@mail.gmail.com>
	<CAMAsSdJ5em5SgjszP-NiA58u_NR_tgT2PC6Wwxi4W8VX0cvVgA@mail.gmail.com>
	<CABjXkq5PnvkM7PUo-uif6kS4EVEobf+NJapGJgGtAOad23r_-g@mail.gmail.com>
Date: Mon, 21 Apr 2014 18:40:04 +0200
Message-ID: <CALD+6GNPSjbRwbwGPkZ8ZxQHr+Ed0VR4RNO16eUzHKjdGQo-BA@mail.gmail.com>
Subject: Re: Any plans for new clustering algorithms?
From: Nick Pentreath <nick.pentreath@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b33d3c6cb701104f79026ca
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b33d3c6cb701104f79026ca
Content-Type: text/plain; charset=UTF-8

I am very much +1 on Sean's comment.

I think the correct abstractions and API for Vectors, Matrices and
distributed matrices (distributed row matrix etc) will, once bedded down
and battle tested in the wild, allow a whole lot of flexibility for
developers of algorithms on top of MLlib core.

This is true whether the algorithm finds itself in MLlib, MLBase, or
resides in a separate contrib project. Just like Spark core sometimes risks
becoming "trying to please everybody" by having the kitchen sink in terms
of Hadoop integration aspects or RDD operations, and thus a spark-contrib
project may make a lot of sense. So too could ml-contrib hold a lot of
algorithms that are not core but still of wide interest. This can include,
for example, models that are still cutting edge and perhaps not as widely
used in production yet, or specialist models that are of interest to a more
niche group.

scikit-learn is very tough about this, requiring a very high bar for
including a new algorithm (many citations, dev support, proof of strong
performance and wide demand). And this leads to a very high quality code
base in general.

I'd say we should (if it hasn't been done already, I may have missed such a
discussion), decide precisely what does constitute MLlib's "1.0.0" goals
for algorithms. I'd say what we have in terms of clustering (K-Means||),
linear models, decision trees and collaborative filtering is pretty much a
good goal. Potentially the Random Forest implementation on top of the DT,
and perhaps another form of recommendation model (such as the co-occurrence
models cf. Mahout's) could be potential candidates for inclusion. I'd also
say any other optimization methods/procedures in addition to SGD and LBFGS
that are very strong and widely used for a variety of (distributed) ML
problems, could be candidates. And finally things like useful utils,
cross-validation and evaluation methods, etc.

So I'd say by all means, please work on a new model such as DBSCAN. Put it
in a new GitHub project, post some detailed performance comparisons vs
MLlib K-Means, and then in future if it gets included in MLlib core it's a
pretty easy to do.


On Mon, Apr 21, 2014 at 6:07 PM, Evan R. Sparks <evan.sparks@gmail.com>wrote:

> While DBSCAN and others would be welcome contributions, I couldn't agree
> more with Sean.
>
>
>
>
> On Mon, Apr 21, 2014 at 8:58 AM, Sean Owen <sowen@cloudera.com> wrote:
>
> > Nobody asked me, and this is a comment on a broader question, not this
> > one, but:
> >
> > In light of a number of recent items about adding more algorithms,
> > I'll say that I personally think an explosion of algorithms should
> > come after the MLlib "core" is more fully baked. I'm thinking of
> > finishing out the changes to vectors and matrices, for example. Things
> > are going to change significantly in the short term as people use the
> > algorithms and see how well the abstractions do or don't work. I've
> > seen another similar project suffer mightily from too many algorithms
> > too early, so maybe I'm just paranoid.
> >
> > Anyway, long-term, I think lots of good algorithms is a right and
> > proper goal for MLlib, myself. Consistent approaches, representations
> > and APIs will make or break MLlib much more than having or not having
> > a particular algorithm. With the plumbing in place, writing the algo
> > is the fun easy part.
> > --
> > Sean Owen | Director, Data Science | London
> >
> >
> > On Mon, Apr 21, 2014 at 4:39 PM, Aliaksei Litouka
> > <aliaksei.litouka@gmail.com> wrote:
> > > Hi, Spark developers.
> > > Are there any plans for implementing new clustering algorithms in
> MLLib?
> > As
> > > far as I understand, current version of Spark ships with only one
> > > clustering algorithm - K-Means. I want to contribute to Spark and I'm
> > > thinking of adding more clustering algorithms - maybe
> > > DBSCAN<http://en.wikipedia.org/wiki/DBSCAN>.
> > > I can start working on it. Does anyone want to join me?
> >
>

--047d7b33d3c6cb701104f79026ca--

From dev-return-7374-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 21 16:56:53 2014
Return-Path: <dev-return-7374-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6022A11A4D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Apr 2014 16:56:53 +0000 (UTC)
Received: (qmail 68893 invoked by uid 500); 21 Apr 2014 16:56:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68808 invoked by uid 500); 21 Apr 2014 16:56:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68800 invoked by uid 99); 21 Apr 2014 16:56:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 16:56:52 +0000
X-ASF-Spam-Status: No, hits=3.2 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of aliaksei.litouka@gmail.com designates 209.85.192.175 as permitted sender)
Received: from [209.85.192.175] (HELO mail-pd0-f175.google.com) (209.85.192.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 16:56:48 +0000
Received: by mail-pd0-f175.google.com with SMTP id x10so3855763pdj.20
        for <dev@spark.apache.org>; Mon, 21 Apr 2014 09:56:25 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=F4ouS9RB9cDRuQP0ydMzQNMd9ZXnBpdavaRIYsiDxuY=;
        b=UaKLAxA36WCKKOvMiVlOTSs6J0kufdw7UuPLieTAnyU9hfoWVh/ozk6ze96n5Vayfr
         Z8mlVz8xPRsgErGBJHxtEs+mhAEk/dHeZIDTjENpZ3Ny+ss6u8E1Q7ImdJj7vGsjTShb
         spcyR0ULMUaN1D3dqyWPJiNy0lzCIenU1eX0mx+ma0s+BytLOeM4CLTsito6R7BYHv5R
         dJV7kgnWkKjQXC01Cnik0uI0Za3CKRo53CaGUrWoU1i4eIktJg0eBIWNN8+fOeVYhlP0
         ew9olFo1wXEd8+r/yPA77nqDBASkz9RhFElBBPQNp9TWL1g2+oWIZ03xhpKDb3tBjDE7
         YDdw==
MIME-Version: 1.0
X-Received: by 10.68.198.36 with SMTP id iz4mr39475162pbc.109.1398099385389;
 Mon, 21 Apr 2014 09:56:25 -0700 (PDT)
Received: by 10.70.94.3 with HTTP; Mon, 21 Apr 2014 09:56:25 -0700 (PDT)
In-Reply-To: <CALD+6GNPSjbRwbwGPkZ8ZxQHr+Ed0VR4RNO16eUzHKjdGQo-BA@mail.gmail.com>
References: <CADSNrJHOT035G1xda8p9eZz-90uiWOb4CwgiY1r-d8m9V9LM+g@mail.gmail.com>
	<CAMAsSdJ5em5SgjszP-NiA58u_NR_tgT2PC6Wwxi4W8VX0cvVgA@mail.gmail.com>
	<CABjXkq5PnvkM7PUo-uif6kS4EVEobf+NJapGJgGtAOad23r_-g@mail.gmail.com>
	<CALD+6GNPSjbRwbwGPkZ8ZxQHr+Ed0VR4RNO16eUzHKjdGQo-BA@mail.gmail.com>
Date: Mon, 21 Apr 2014 11:56:25 -0500
Message-ID: <CADSNrJG-spRvNUiHCS-Z4agZnWhQLnx_cUMrOmmH-XKWKYxAPQ@mail.gmail.com>
Subject: Re: Any plans for new clustering algorithms?
From: Aliaksei Litouka <aliaksei.litouka@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bd756a03a18b104f7906108
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd756a03a18b104f7906108
Content-Type: text/plain; charset=UTF-8

Thank you very much for detailed answers.
I can't but agree that a good MLLib core is a higher priority than
algorithms built on top of it. I'll check if I can contribute anything to
the core. I will also follow Nick Pentreath's recommendation to start a new
GitHub project. Actually, here is a link to repository:
https://github.com/alitouka/spark_dbscan . Currently it is empty - I've
just created it :)


2014-04-21 11:40 GMT-05:00 Nick Pentreath <nick.pentreath@gmail.com>:

> I am very much +1 on Sean's comment.
>
> I think the correct abstractions and API for Vectors, Matrices and
> distributed matrices (distributed row matrix etc) will, once bedded down
> and battle tested in the wild, allow a whole lot of flexibility for
> developers of algorithms on top of MLlib core.
>
> This is true whether the algorithm finds itself in MLlib, MLBase, or
> resides in a separate contrib project. Just like Spark core sometimes risks
> becoming "trying to please everybody" by having the kitchen sink in terms
> of Hadoop integration aspects or RDD operations, and thus a spark-contrib
> project may make a lot of sense. So too could ml-contrib hold a lot of
> algorithms that are not core but still of wide interest. This can include,
> for example, models that are still cutting edge and perhaps not as widely
> used in production yet, or specialist models that are of interest to a more
> niche group.
>
> scikit-learn is very tough about this, requiring a very high bar for
> including a new algorithm (many citations, dev support, proof of strong
> performance and wide demand). And this leads to a very high quality code
> base in general.
>
> I'd say we should (if it hasn't been done already, I may have missed such a
> discussion), decide precisely what does constitute MLlib's "1.0.0" goals
> for algorithms. I'd say what we have in terms of clustering (K-Means||),
> linear models, decision trees and collaborative filtering is pretty much a
> good goal. Potentially the Random Forest implementation on top of the DT,
> and perhaps another form of recommendation model (such as the co-occurrence
> models cf. Mahout's) could be potential candidates for inclusion. I'd also
> say any other optimization methods/procedures in addition to SGD and LBFGS
> that are very strong and widely used for a variety of (distributed) ML
> problems, could be candidates. And finally things like useful utils,
> cross-validation and evaluation methods, etc.
>
> So I'd say by all means, please work on a new model such as DBSCAN. Put it
> in a new GitHub project, post some detailed performance comparisons vs
> MLlib K-Means, and then in future if it gets included in MLlib core it's a
> pretty easy to do.
>
>
> On Mon, Apr 21, 2014 at 6:07 PM, Evan R. Sparks <evan.sparks@gmail.com
> >wrote:
>
> > While DBSCAN and others would be welcome contributions, I couldn't agree
> > more with Sean.
> >
> >
> >
> >
> > On Mon, Apr 21, 2014 at 8:58 AM, Sean Owen <sowen@cloudera.com> wrote:
> >
> > > Nobody asked me, and this is a comment on a broader question, not this
> > > one, but:
> > >
> > > In light of a number of recent items about adding more algorithms,
> > > I'll say that I personally think an explosion of algorithms should
> > > come after the MLlib "core" is more fully baked. I'm thinking of
> > > finishing out the changes to vectors and matrices, for example. Things
> > > are going to change significantly in the short term as people use the
> > > algorithms and see how well the abstractions do or don't work. I've
> > > seen another similar project suffer mightily from too many algorithms
> > > too early, so maybe I'm just paranoid.
> > >
> > > Anyway, long-term, I think lots of good algorithms is a right and
> > > proper goal for MLlib, myself. Consistent approaches, representations
> > > and APIs will make or break MLlib much more than having or not having
> > > a particular algorithm. With the plumbing in place, writing the algo
> > > is the fun easy part.
> > > --
> > > Sean Owen | Director, Data Science | London
> > >
> > >
> > > On Mon, Apr 21, 2014 at 4:39 PM, Aliaksei Litouka
> > > <aliaksei.litouka@gmail.com> wrote:
> > > > Hi, Spark developers.
> > > > Are there any plans for implementing new clustering algorithms in
> > MLLib?
> > > As
> > > > far as I understand, current version of Spark ships with only one
> > > > clustering algorithm - K-Means. I want to contribute to Spark and I'm
> > > > thinking of adding more clustering algorithms - maybe
> > > > DBSCAN<http://en.wikipedia.org/wiki/DBSCAN>.
> > > > I can start working on it. Does anyone want to join me?
> > >
> >
>

--047d7bd756a03a18b104f7906108--

From dev-return-7375-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 21 17:04:46 2014
Return-Path: <dev-return-7375-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3C43311A94
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Apr 2014 17:04:46 +0000 (UTC)
Received: (qmail 87959 invoked by uid 500); 21 Apr 2014 17:04:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87890 invoked by uid 500); 21 Apr 2014 17:04:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87878 invoked by uid 99); 21 Apr 2014 17:04:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 17:04:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.219.51] (HELO mail-oa0-f51.google.com) (209.85.219.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 17:04:40 +0000
Received: by mail-oa0-f51.google.com with SMTP id i4so4417795oah.24
        for <dev@spark.apache.org>; Mon, 21 Apr 2014 10:04:16 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=l6b8FIMWYCcJn6S5uSlXDnUIF/7ZLYdWvd2U7aOcZE8=;
        b=mnTQUKbKOO+kGZOhR1YAS0kYgJvvWc0MbVUnvtdbwuy+2/ywwR0Y+rhGXJg+igoqYB
         W+V6xfGv3ZWQ5HBLRKhBwqolGFjH6kzBBCgRzdthoH733QWilcOt41jLeBfCRnpp70vs
         sRAIPfXgoomMJMU3pYtPaa5DZdX6wGGs9YBGq/VRYAudsuDG//OVmwvhPMxwcj6+Vu4v
         kftCP++yIttu2viIhLTQhwBVSjZfWPl3BRJwfWUBv1woCkScYyE+ahcTiKMdFdt0tGwF
         VXB9tR8WiWJow+q0U5pWOQoklhgJSFtuMpwt0vEuBrpfFK04o8PafmPLHyZgnVHgNGKJ
         znkA==
X-Gm-Message-State: ALoCoQlMactd63mqWQq80dT6JABZVfTNzoZZszGaTLYuNKSO2queOrgphvSvmAEPnH2pGN0I3v/f
X-Received: by 10.60.51.227 with SMTP id n3mr33091390oeo.33.1398099855965;
 Mon, 21 Apr 2014 10:04:15 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.76.152.163 with HTTP; Mon, 21 Apr 2014 10:03:55 -0700 (PDT)
In-Reply-To: <CAMAsSdJ5em5SgjszP-NiA58u_NR_tgT2PC6Wwxi4W8VX0cvVgA@mail.gmail.com>
References: <CADSNrJHOT035G1xda8p9eZz-90uiWOb4CwgiY1r-d8m9V9LM+g@mail.gmail.com>
 <CAMAsSdJ5em5SgjszP-NiA58u_NR_tgT2PC6Wwxi4W8VX0cvVgA@mail.gmail.com>
From: Paul Brown <prb@mult.ifario.us>
Date: Mon, 21 Apr 2014 10:03:55 -0700
Message-ID: <CACArsZ8w=mRKMwPyG9+9Kyyn-kVtOj1E2ujC6p_Uv_X-86m1Tg@mail.gmail.com>
Subject: Re: Any plans for new clustering algorithms?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2fdf64694fe04f7907dfd
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2fdf64694fe04f7907dfd
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I agree that it will be good to see more algorithms added to the MLlib
universe, although this does bring to mind a couple of comments:

- MLlib as Mahout.next would be a unfortunate.  There are some gems in
Mahout, but there are also lots of rocks.  Setting a minimal bar of
working, correctly implemented, and documented requires a surprising amount
of work.

- Not getting any signal out of your data with an algorithm like K-means
implies one of the following: (1) there is no signal in your data, (2) you
should try tuning the algorithm differently, (3) you're using K-means
wrong, (4) you should try preparing the data differently, (5) all of the
above, or (6) none of the above.

My $0.02.
-- Paul


=E2=80=94
prb@mult.ifario.us | Multifarious, Inc. | http://mult.ifario.us/


On Mon, Apr 21, 2014 at 8:58 AM, Sean Owen <sowen@cloudera.com> wrote:

> Nobody asked me, and this is a comment on a broader question, not this
> one, but:
>
> In light of a number of recent items about adding more algorithms,
> I'll say that I personally think an explosion of algorithms should
> come after the MLlib "core" is more fully baked. I'm thinking of
> finishing out the changes to vectors and matrices, for example. Things
> are going to change significantly in the short term as people use the
> algorithms and see how well the abstractions do or don't work. I've
> seen another similar project suffer mightily from too many algorithms
> too early, so maybe I'm just paranoid.
>
> Anyway, long-term, I think lots of good algorithms is a right and
> proper goal for MLlib, myself. Consistent approaches, representations
> and APIs will make or break MLlib much more than having or not having
> a particular algorithm. With the plumbing in place, writing the algo
> is the fun easy part.
> --
> Sean Owen | Director, Data Science | London
>
>
> On Mon, Apr 21, 2014 at 4:39 PM, Aliaksei Litouka
> <aliaksei.litouka@gmail.com> wrote:
> > Hi, Spark developers.
> > Are there any plans for implementing new clustering algorithms in MLLib=
?
> As
> > far as I understand, current version of Spark ships with only one
> > clustering algorithm - K-Means. I want to contribute to Spark and I'm
> > thinking of adding more clustering algorithms - maybe
> > DBSCAN<http://en.wikipedia.org/wiki/DBSCAN>.
> > I can start working on it. Does anyone want to join me?
>

--001a11c2fdf64694fe04f7907dfd--

From dev-return-7376-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 21 17:23:53 2014
Return-Path: <dev-return-7376-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2A76711B57
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Apr 2014 17:23:53 +0000 (UTC)
Received: (qmail 24221 invoked by uid 500); 21 Apr 2014 17:23:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24136 invoked by uid 500); 21 Apr 2014 17:23:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24128 invoked by uid 99); 21 Apr 2014 17:23:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 17:23:51 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.128.176 as permitted sender)
Received: from [209.85.128.176] (HELO mail-ve0-f176.google.com) (209.85.128.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 17:23:47 +0000
Received: by mail-ve0-f176.google.com with SMTP id db11so8090029veb.21
        for <dev@spark.apache.org>; Mon, 21 Apr 2014 10:23:24 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=+MqsMA1+giMnSNauVpg4qDzPV6i/LuNi62veVXbPoPQ=;
        b=JUzcljj11SY+JMyDyT8eQzQMhvwZv3nG8vQIFUjxw9cEMkAaPU+HU9pJWfOAzJV1xh
         YkjcZh9Z5XRxfsRRip2nOq+gTJu78TxQzYnkn1WhENcVaAAR7Rd1G0Ll4V+sHXJgTtVt
         ZhCpNppJKYRYmEZJoW5WO8Sdx1XFLTVUrx9Q22aJcZz7KA7Qt/u/4dSI/N+/l6dVT3jK
         YmR8x1hi2ARz3DyfzL4wEVOmLkmmMDkoCvmvngXEehW4oBMs9JiaRZCzuG0iHiPZrBDh
         r8kig8IUI0rno1jpK89WOQWD6aBnnvKUNVwJuHLXzROWlho1OZ8Egi+lGZiKmtBq6lMF
         Bm9w==
X-Gm-Message-State: ALoCoQmfjdK4rRylXrPIZPEiebCe1pKwQGmYNV5G1KPFUx9FuVoCcKqcfBvSajZVR+h61BUYUMYU
X-Received: by 10.220.191.134 with SMTP id dm6mr31562132vcb.16.1398101004806;
 Mon, 21 Apr 2014 10:23:24 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.111.69 with HTTP; Mon, 21 Apr 2014 10:23:04 -0700 (PDT)
In-Reply-To: <CACArsZ8w=mRKMwPyG9+9Kyyn-kVtOj1E2ujC6p_Uv_X-86m1Tg@mail.gmail.com>
References: <CADSNrJHOT035G1xda8p9eZz-90uiWOb4CwgiY1r-d8m9V9LM+g@mail.gmail.com>
 <CAMAsSdJ5em5SgjszP-NiA58u_NR_tgT2PC6Wwxi4W8VX0cvVgA@mail.gmail.com> <CACArsZ8w=mRKMwPyG9+9Kyyn-kVtOj1E2ujC6p_Uv_X-86m1Tg@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 21 Apr 2014 18:23:04 +0100
Message-ID: <CAMAsSdJX=2UiV2CmJWwNX4Uiud8JuvcFoCkbnOrWJBouJMMjkg@mail.gmail.com>
Subject: Re: Any plans for new clustering algorithms?
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Mon, Apr 21, 2014 at 6:03 PM, Paul Brown <prb@mult.ifario.us> wrote:
> - MLlib as Mahout.next would be a unfortunate.  There are some gems in
> Mahout, but there are also lots of rocks.  Setting a minimal bar of
> working, correctly implemented, and documented requires a surprising amount
> of work.

As someone with first-hand knowledge, this is correct. To Sang's
question, I can't see value in 'porting' Mahout since it is based on a
quite different paradigm. About the only part that translates is the
algorithm concept itself.

This is also the cautionary tale. The contents of the project have
ended up being a number of "drive-by" contributions of implementations
that, while individually perhaps brilliant (perhaps), didn't
necessarily match any other implementation in structure, input/output,
libraries used. The implementations were often a touch academic. The
result was hard to document, maintain, evolve or use.

Far more of the structure of the MLlib implementations are consistent
by virtue of being built around Spark core already. That's great.

One can't wait to completely build the foundation before building any
implementations. To me, the existing implementations are almost
exactly the basics I would choose. They cover the bases and will
exercise the abstractions and structure. So that's also great IMHO.

From dev-return-7377-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 21 17:54:54 2014
Return-Path: <dev-return-7377-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 18DD111C59
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Apr 2014 17:54:54 +0000 (UTC)
Received: (qmail 86295 invoked by uid 500); 21 Apr 2014 17:54:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86113 invoked by uid 500); 21 Apr 2014 17:54:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85847 invoked by uid 99); 21 Apr 2014 17:54:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 17:54:50 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.212.171 as permitted sender)
Received: from [209.85.212.171] (HELO mail-wi0-f171.google.com) (209.85.212.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 17:54:47 +0000
Received: by mail-wi0-f171.google.com with SMTP id q5so2110420wiv.10
        for <dev@spark.apache.org>; Mon, 21 Apr 2014 10:54:25 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=TK7x+ZhVLxDMLCJ08edQ0wtKOBaLFXlrYgI6zZK83uE=;
        b=cXpAMOdR7WigzO1eGG9s8g4k+P5TnnZBN5cgJZ36/egJXOOGDXkMmYJuSj6auW9qFo
         RZyKddzsfoxCDjdSlJ0Bjj+uOI/42RYUe+WvYMYKvu0Hxlvr/LEMeLcrK823zunGk/gk
         qZwPQi/SBCQkP/j7gh4SfNzPtoujqtN08NLy5nV2lOwWSevMOG6MuioxmhkhMI8Ja2pf
         dn5e5Yehh+sUlb5hNEOjmQQOKqsFtOGLK+3EnFu2PxXpdr6lZW6uD0ZPv6fFsI/UGBul
         0fUAbF4Uip7xr3CuDDmmUGLZgHXBGqh1q+0zTebjcIO5txYJGneYVbB2R0dXW5K3K852
         9Kig==
MIME-Version: 1.0
X-Received: by 10.180.211.116 with SMTP id nb20mr14835611wic.5.1398102865150;
 Mon, 21 Apr 2014 10:54:25 -0700 (PDT)
Received: by 10.194.82.105 with HTTP; Mon, 21 Apr 2014 10:54:25 -0700 (PDT)
In-Reply-To: <CAMAsSdJX=2UiV2CmJWwNX4Uiud8JuvcFoCkbnOrWJBouJMMjkg@mail.gmail.com>
References: <CADSNrJHOT035G1xda8p9eZz-90uiWOb4CwgiY1r-d8m9V9LM+g@mail.gmail.com>
	<CAMAsSdJ5em5SgjszP-NiA58u_NR_tgT2PC6Wwxi4W8VX0cvVgA@mail.gmail.com>
	<CACArsZ8w=mRKMwPyG9+9Kyyn-kVtOj1E2ujC6p_Uv_X-86m1Tg@mail.gmail.com>
	<CAMAsSdJX=2UiV2CmJWwNX4Uiud8JuvcFoCkbnOrWJBouJMMjkg@mail.gmail.com>
Date: Mon, 21 Apr 2014 10:54:25 -0700
Message-ID: <CAJgQjQ8KzOUGH-ChXy5QiLuK_WEmj5FS31ucHtRftZd5Aj46eA@mail.gmail.com>
Subject: Re: Any plans for new clustering algorithms?
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

+1 on Sean's comment. MLlib covers the basic algorithms but we
definitely need to spend more time on how to make the design scalable.
For example, think about current "ProblemWithAlgorithm" naming scheme.
That being said, new algorithms are welcomed. I wish they are
well-established and well-understood by users. They shouldn't be
research algorithms tuned to work well with a particular dataset but
not tested widely. You see the change log from Mahout:

===
The following algorithms that were marked deprecated in 0.8 have been
removed in 0.9:

>From Clustering:
  Switched LDA implementation from using Gibbs Sampling to Collapsed
Variational Bayes (CVB)
Meanshift
MinHash - removed due to poor performance, lack of support and lack of usage

>From Classification (both are sequential implementations)
Winnow - lack of actual usage and support
Perceptron - lack of actual usage and support

Collaborative Filtering
    SlopeOne implementations in
org.apache.mahout.cf.taste.hadoop.slopeone and
org.apache.mahout.cf.taste.impl.recommender.slopeone
    Distributed pseudo recommender in org.apache.mahout.cf.taste.hadoop.pseudo
    TreeClusteringRecommender in org.apache.mahout.cf.taste.impl.recommender

Mahout Math
    Hadoop entropy stuff in org.apache.mahout.math.stats.entropy
===

In MLlib, we should include the algorithms users know how to use and
we can provide support rather than letting algorithms come and go.

My $0.02,
Xiangrui

On Mon, Apr 21, 2014 at 10:23 AM, Sean Owen <sowen@cloudera.com> wrote:
> On Mon, Apr 21, 2014 at 6:03 PM, Paul Brown <prb@mult.ifario.us> wrote:
>> - MLlib as Mahout.next would be a unfortunate.  There are some gems in
>> Mahout, but there are also lots of rocks.  Setting a minimal bar of
>> working, correctly implemented, and documented requires a surprising amount
>> of work.
>
> As someone with first-hand knowledge, this is correct. To Sang's
> question, I can't see value in 'porting' Mahout since it is based on a
> quite different paradigm. About the only part that translates is the
> algorithm concept itself.
>
> This is also the cautionary tale. The contents of the project have
> ended up being a number of "drive-by" contributions of implementations
> that, while individually perhaps brilliant (perhaps), didn't
> necessarily match any other implementation in structure, input/output,
> libraries used. The implementations were often a touch academic. The
> result was hard to document, maintain, evolve or use.
>
> Far more of the structure of the MLlib implementations are consistent
> by virtue of being built around Spark core already. That's great.
>
> One can't wait to completely build the foundation before building any
> implementations. To me, the existing implementations are almost
> exactly the basics I would choose. They cover the bases and will
> exercise the abstractions and structure. So that's also great IMHO.

From dev-return-7378-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 21 17:59:53 2014
Return-Path: <dev-return-7378-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5682411C87
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Apr 2014 17:59:53 +0000 (UTC)
Received: (qmail 94209 invoked by uid 500); 21 Apr 2014 17:59:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94103 invoked by uid 500); 21 Apr 2014 17:59:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94095 invoked by uid 99); 21 Apr 2014 17:59:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 17:59:51 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.44 as permitted sender)
Received: from [209.85.216.44] (HELO mail-qa0-f44.google.com) (209.85.216.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 17:59:47 +0000
Received: by mail-qa0-f44.google.com with SMTP id hw13so4123161qab.31
        for <dev@spark.apache.org>; Mon, 21 Apr 2014 10:59:25 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=LFrUswT2n5VmRt4FPI757v4KgmgULoTGBLPPJcCEkH8=;
        b=EseT8m5fvvpRx/LnhpssDM6pc9ZNsqeH44wGE879p4gq9zyfLgRO2syLJUman5zT5a
         ucLxRoyRePgzoDwVRGBMcD4Ly2w11ElEQT+lh7uOcwWH8ZAR9obelMDvZpq2Pe6u3s1M
         Su1TPVUEYA8eFZl7ZaRgqMiB9OVjxx/xgaBUW/Wmj7sUY0xIh1EeqpCrLgyOptT6+d3z
         vNX5nkkGMfHIp8y5farnscYBDJbb1PI34DbVidB99DcI57FvLCmBRP7PLVEe6+ugAaQR
         +QlD3keQL2qDLHaABM/Sw+bj2x/lpJ6dA+WjPj+nWbTXkHPX7c/viBW6XM18NzzAtDWi
         LRhA==
X-Gm-Message-State: ALoCoQkPCfndeurwiL0hbZzta2GFH3hxjyEL6ydfHT1Scn5PaUumdO3EgF5tvw9Qir1rj9GbRhy+
MIME-Version: 1.0
X-Received: by 10.224.98.207 with SMTP id r15mr29619759qan.41.1398103165062;
 Mon, 21 Apr 2014 10:59:25 -0700 (PDT)
Received: by 10.140.94.2 with HTTP; Mon, 21 Apr 2014 10:59:24 -0700 (PDT)
In-Reply-To: <CAJgQjQ8KzOUGH-ChXy5QiLuK_WEmj5FS31ucHtRftZd5Aj46eA@mail.gmail.com>
References: <CADSNrJHOT035G1xda8p9eZz-90uiWOb4CwgiY1r-d8m9V9LM+g@mail.gmail.com>
	<CAMAsSdJ5em5SgjszP-NiA58u_NR_tgT2PC6Wwxi4W8VX0cvVgA@mail.gmail.com>
	<CACArsZ8w=mRKMwPyG9+9Kyyn-kVtOj1E2ujC6p_Uv_X-86m1Tg@mail.gmail.com>
	<CAMAsSdJX=2UiV2CmJWwNX4Uiud8JuvcFoCkbnOrWJBouJMMjkg@mail.gmail.com>
	<CAJgQjQ8KzOUGH-ChXy5QiLuK_WEmj5FS31ucHtRftZd5Aj46eA@mail.gmail.com>
Date: Mon, 21 Apr 2014 10:59:24 -0700
Message-ID: <CACBYxK+-NCgGHKSNt=T+TCCqofqA+-HQzC00FB2qJdD-c1hNGw@mail.gmail.com>
Subject: Re: Any plans for new clustering algorithms?
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0149c5fa8360ed04f7914239
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149c5fa8360ed04f7914239
Content-Type: text/plain; charset=ISO-8859-1

If it's not done already, would it make sense to codify this philosophy
somewhere?  I imagine this won't be the first time this discussion comes
up, and it would be nice to have a doc to point to.  I'd be happy to take a
stab at this.


On Mon, Apr 21, 2014 at 10:54 AM, Xiangrui Meng <mengxr@gmail.com> wrote:

> +1 on Sean's comment. MLlib covers the basic algorithms but we
> definitely need to spend more time on how to make the design scalable.
> For example, think about current "ProblemWithAlgorithm" naming scheme.
> That being said, new algorithms are welcomed. I wish they are
> well-established and well-understood by users. They shouldn't be
> research algorithms tuned to work well with a particular dataset but
> not tested widely. You see the change log from Mahout:
>
> ===
> The following algorithms that were marked deprecated in 0.8 have been
> removed in 0.9:
>
> From Clustering:
>   Switched LDA implementation from using Gibbs Sampling to Collapsed
> Variational Bayes (CVB)
> Meanshift
> MinHash - removed due to poor performance, lack of support and lack of
> usage
>
> From Classification (both are sequential implementations)
> Winnow - lack of actual usage and support
> Perceptron - lack of actual usage and support
>
> Collaborative Filtering
>     SlopeOne implementations in
> org.apache.mahout.cf.taste.hadoop.slopeone and
> org.apache.mahout.cf.taste.impl.recommender.slopeone
>     Distributed pseudo recommender in
> org.apache.mahout.cf.taste.hadoop.pseudo
>     TreeClusteringRecommender in
> org.apache.mahout.cf.taste.impl.recommender
>
> Mahout Math
>     Hadoop entropy stuff in org.apache.mahout.math.stats.entropy
> ===
>
> In MLlib, we should include the algorithms users know how to use and
> we can provide support rather than letting algorithms come and go.
>
> My $0.02,
> Xiangrui
>
> On Mon, Apr 21, 2014 at 10:23 AM, Sean Owen <sowen@cloudera.com> wrote:
> > On Mon, Apr 21, 2014 at 6:03 PM, Paul Brown <prb@mult.ifario.us> wrote:
> >> - MLlib as Mahout.next would be a unfortunate.  There are some gems in
> >> Mahout, but there are also lots of rocks.  Setting a minimal bar of
> >> working, correctly implemented, and documented requires a surprising
> amount
> >> of work.
> >
> > As someone with first-hand knowledge, this is correct. To Sang's
> > question, I can't see value in 'porting' Mahout since it is based on a
> > quite different paradigm. About the only part that translates is the
> > algorithm concept itself.
> >
> > This is also the cautionary tale. The contents of the project have
> > ended up being a number of "drive-by" contributions of implementations
> > that, while individually perhaps brilliant (perhaps), didn't
> > necessarily match any other implementation in structure, input/output,
> > libraries used. The implementations were often a touch academic. The
> > result was hard to document, maintain, evolve or use.
> >
> > Far more of the structure of the MLlib implementations are consistent
> > by virtue of being built around Spark core already. That's great.
> >
> > One can't wait to completely build the foundation before building any
> > implementations. To me, the existing implementations are almost
> > exactly the basics I would choose. They cover the bases and will
> > exercise the abstractions and structure. So that's also great IMHO.
>

--089e0149c5fa8360ed04f7914239--

From dev-return-7379-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 21 20:42:09 2014
Return-Path: <dev-return-7379-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 91F401122A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Apr 2014 20:42:09 +0000 (UTC)
Received: (qmail 26423 invoked by uid 500); 21 Apr 2014 20:42:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26325 invoked by uid 500); 21 Apr 2014 20:42:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26317 invoked by uid 99); 21 Apr 2014 20:42:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 20:42:07 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nick.pentreath@gmail.com designates 74.125.82.47 as permitted sender)
Received: from [74.125.82.47] (HELO mail-wg0-f47.google.com) (74.125.82.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 20:42:03 +0000
Received: by mail-wg0-f47.google.com with SMTP id x12so3020744wgg.18
        for <dev@spark.apache.org>; Mon, 21 Apr 2014 13:41:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=subject:references:from:content-type:in-reply-to:message-id:date:to
         :content-transfer-encoding:mime-version;
        bh=woVOh2taHucPuJgtjhHHYlQ6z1Qgawtb7ZjuXenFrio=;
        b=ZtuBmHV1jmIgt5TGnyeJ2bItM7/ZfCHrChVOieBOB6MJLfvBf4SpJjKvCitpX7jVSU
         SD0qNoEjFHPdQbLl2D/yPJcRVaElK6AO15NaWP+wuwCGFiM0TqRxc8btom+F8VuwGMJA
         TA9ymqA3czql7GFbY/MWMAhQsRXXwrhJrlLBnxEGgGs6eKHoesFlBw3b5j/HzWDMNGW3
         b7YcNVtS4pf6SHXQLqg+UxHSbe6Y8ODX0UZhhVR48NJ2fE2kcAubF1BTGIRSnedSYcfY
         B0cL5BAwB0mWXJld4jn2AXzJwfrkKXjf6hSjscdXJRrw5ysRdBlmFiLj3LN32ZiAyTGZ
         dUwg==
X-Received: by 10.195.18.8 with SMTP id gi8mr43059wjd.75.1398112902031;
        Mon, 21 Apr 2014 13:41:42 -0700 (PDT)
Received: from [10.0.0.9] (196-210-179-224.dynamic.isadsl.co.za. [196.210.179.224])
        by mx.google.com with ESMTPSA id ea8sm18137950wib.11.2014.04.21.13.41.39
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 21 Apr 2014 13:41:40 -0700 (PDT)
Subject: Re: Any plans for new clustering algorithms?
References: <CADSNrJHOT035G1xda8p9eZz-90uiWOb4CwgiY1r-d8m9V9LM+g@mail.gmail.com> <CAMAsSdJ5em5SgjszP-NiA58u_NR_tgT2PC6Wwxi4W8VX0cvVgA@mail.gmail.com> <CACArsZ8w=mRKMwPyG9+9Kyyn-kVtOj1E2ujC6p_Uv_X-86m1Tg@mail.gmail.com> <CAMAsSdJX=2UiV2CmJWwNX4Uiud8JuvcFoCkbnOrWJBouJMMjkg@mail.gmail.com> <CAJgQjQ8KzOUGH-ChXy5QiLuK_WEmj5FS31ucHtRftZd5Aj46eA@mail.gmail.com> <CACBYxK+-NCgGHKSNt=T+TCCqofqA+-HQzC00FB2qJdD-c1hNGw@mail.gmail.com>
From: Nick Pentreath <nick.pentreath@gmail.com>
Content-Type: text/plain;
	charset=us-ascii
X-Mailer: iPhone Mail (11D167)
In-Reply-To: <CACBYxK+-NCgGHKSNt=T+TCCqofqA+-HQzC00FB2qJdD-c1hNGw@mail.gmail.com>
Message-Id: <A1C0F7CE-1A54-4486-89EA-63EA9DAB65C9@googlemail.com>
Date: Mon, 21 Apr 2014 22:41:36 +0200
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Mime-Version: 1.0 (1.0)
X-Virus-Checked: Checked by ClamAV on apache.org

I'd say a section in the "how to contribute" page would be a good place to p=
ut this.

In general I'd say that the criteria for inclusion of an algorithm is it sho=
uld be high quality, widely known, used and accepted (citations and concrete=
 use cases as examples of this), scalable and parallelizable, well documente=
d and with reasonable expectation of dev support

Sent from my iPhone

> On 21 Apr 2014, at 19:59, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
>=20
> If it's not done already, would it make sense to codify this philosophy
> somewhere?  I imagine this won't be the first time this discussion comes
> up, and it would be nice to have a doc to point to.  I'd be happy to take a=

> stab at this.
>=20
>=20
>> On Mon, Apr 21, 2014 at 10:54 AM, Xiangrui Meng <mengxr@gmail.com> wrote:=

>>=20
>> +1 on Sean's comment. MLlib covers the basic algorithms but we
>> definitely need to spend more time on how to make the design scalable.
>> For example, think about current "ProblemWithAlgorithm" naming scheme.
>> That being said, new algorithms are welcomed. I wish they are
>> well-established and well-understood by users. They shouldn't be
>> research algorithms tuned to work well with a particular dataset but
>> not tested widely. You see the change log from Mahout:
>>=20
>> =3D=3D=3D
>> The following algorithms that were marked deprecated in 0.8 have been
>> removed in 0.9:
>>=20
>> =46rom Clustering:
>>  Switched LDA implementation from using Gibbs Sampling to Collapsed
>> Variational Bayes (CVB)
>> Meanshift
>> MinHash - removed due to poor performance, lack of support and lack of
>> usage
>>=20
>> =46rom Classification (both are sequential implementations)
>> Winnow - lack of actual usage and support
>> Perceptron - lack of actual usage and support
>>=20
>> Collaborative Filtering
>>    SlopeOne implementations in
>> org.apache.mahout.cf.taste.hadoop.slopeone and
>> org.apache.mahout.cf.taste.impl.recommender.slopeone
>>    Distributed pseudo recommender in
>> org.apache.mahout.cf.taste.hadoop.pseudo
>>    TreeClusteringRecommender in
>> org.apache.mahout.cf.taste.impl.recommender
>>=20
>> Mahout Math
>>    Hadoop entropy stuff in org.apache.mahout.math.stats.entropy
>> =3D=3D=3D
>>=20
>> In MLlib, we should include the algorithms users know how to use and
>> we can provide support rather than letting algorithms come and go.
>>=20
>> My $0.02,
>> Xiangrui
>>=20
>>> On Mon, Apr 21, 2014 at 10:23 AM, Sean Owen <sowen@cloudera.com> wrote:
>>>> On Mon, Apr 21, 2014 at 6:03 PM, Paul Brown <prb@mult.ifario.us> wrote:=

>>>> - MLlib as Mahout.next would be a unfortunate.  There are some gems in
>>>> Mahout, but there are also lots of rocks.  Setting a minimal bar of
>>>> working, correctly implemented, and documented requires a surprising
>> amount
>>>> of work.
>>>=20
>>> As someone with first-hand knowledge, this is correct. To Sang's
>>> question, I can't see value in 'porting' Mahout since it is based on a
>>> quite different paradigm. About the only part that translates is the
>>> algorithm concept itself.
>>>=20
>>> This is also the cautionary tale. The contents of the project have
>>> ended up being a number of "drive-by" contributions of implementations
>>> that, while individually perhaps brilliant (perhaps), didn't
>>> necessarily match any other implementation in structure, input/output,
>>> libraries used. The implementations were often a touch academic. The
>>> result was hard to document, maintain, evolve or use.
>>>=20
>>> Far more of the structure of the MLlib implementations are consistent
>>> by virtue of being built around Spark core already. That's great.
>>>=20
>>> One can't wait to completely build the foundation before building any
>>> implementations. To me, the existing implementations are almost
>>> exactly the basics I would choose. They cover the bases and will
>>> exercise the abstractions and structure. So that's also great IMHO.
>>=20

From dev-return-7380-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 21 22:19:58 2014
Return-Path: <dev-return-7380-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C6C661156B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Apr 2014 22:19:58 +0000 (UTC)
Received: (qmail 89270 invoked by uid 500); 21 Apr 2014 22:19:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89134 invoked by uid 500); 21 Apr 2014 22:19:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89126 invoked by uid 99); 21 Apr 2014 22:19:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 22:19:57 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 74.125.82.47 as permitted sender)
Received: from [74.125.82.47] (HELO mail-wg0-f47.google.com) (74.125.82.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Apr 2014 22:19:53 +0000
Received: by mail-wg0-f47.google.com with SMTP id x12so3100819wgg.18
        for <dev@spark.apache.org>; Mon, 21 Apr 2014 15:19:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type:content-transfer-encoding;
        bh=Msgqr7N+b0vsb4Ue7Wu+Xn83aKP0xWbdpMiJv5+faEE=;
        b=BPFy95TuoNex8sPzdiPHRfzorLCYxhHKKuObcNceErKtJSsOCg2IVhtjXKxlH6XSZd
         jMgHzEKlIdC6OvQ2LY7cXoq8G8dnOQLJXLxGbrW6O7V1mEeMiEw1sLUjfTYYQBkRZBzr
         zCeGMjo4MLIjnXQyk7xGELuySnENqzyMoPntmDNN5o9IXXWBxyBYgZGiMF4ZKhBi9OVr
         mqBzGkS0M206MJv+vZeo7uQR/mposmFq53HXSB7XLzlHk7VHXNhw7yTRyaRXRsnlo5VY
         6/koo8KMSc29BmKSllheSP4je66Vt6/9LtR4TWlHkd/IiGfLCwRUNUqKMa6mjvqVJNX9
         jO+w==
MIME-Version: 1.0
X-Received: by 10.180.76.146 with SMTP id k18mr15550223wiw.5.1398118771363;
 Mon, 21 Apr 2014 15:19:31 -0700 (PDT)
Received: by 10.194.82.105 with HTTP; Mon, 21 Apr 2014 15:19:31 -0700 (PDT)
In-Reply-To: <A1C0F7CE-1A54-4486-89EA-63EA9DAB65C9@googlemail.com>
References: <CADSNrJHOT035G1xda8p9eZz-90uiWOb4CwgiY1r-d8m9V9LM+g@mail.gmail.com>
	<CAMAsSdJ5em5SgjszP-NiA58u_NR_tgT2PC6Wwxi4W8VX0cvVgA@mail.gmail.com>
	<CACArsZ8w=mRKMwPyG9+9Kyyn-kVtOj1E2ujC6p_Uv_X-86m1Tg@mail.gmail.com>
	<CAMAsSdJX=2UiV2CmJWwNX4Uiud8JuvcFoCkbnOrWJBouJMMjkg@mail.gmail.com>
	<CAJgQjQ8KzOUGH-ChXy5QiLuK_WEmj5FS31ucHtRftZd5Aj46eA@mail.gmail.com>
	<CACBYxK+-NCgGHKSNt=T+TCCqofqA+-HQzC00FB2qJdD-c1hNGw@mail.gmail.com>
	<A1C0F7CE-1A54-4486-89EA-63EA9DAB65C9@googlemail.com>
Date: Mon, 21 Apr 2014 15:19:31 -0700
Message-ID: <CAJgQjQ9wmVtwCtW21sxQ+OCsN3v__1MrTVsFKuJ8Q_WX-018ig@mail.gmail.com>
Subject: Re: Any plans for new clustering algorithms?
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Cannot agree more with your words. Could you add one section about
"how and what to contribute" to MLlib's guide? -Xiangrui

On Mon, Apr 21, 2014 at 1:41 PM, Nick Pentreath
<nick.pentreath@gmail.com> wrote:
> I'd say a section in the "how to contribute" page would be a good place t=
o put this.
>
> In general I'd say that the criteria for inclusion of an algorithm is it =
should be high quality, widely known, used and accepted (citations and conc=
rete use cases as examples of this), scalable and parallelizable, well docu=
mented and with reasonable expectation of dev support
>
> Sent from my iPhone
>
>> On 21 Apr 2014, at 19:59, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
>>
>> If it's not done already, would it make sense to codify this philosophy
>> somewhere?  I imagine this won't be the first time this discussion comes
>> up, and it would be nice to have a doc to point to.  I'd be happy to tak=
e a
>> stab at this.
>>
>>
>>> On Mon, Apr 21, 2014 at 10:54 AM, Xiangrui Meng <mengxr@gmail.com> wrot=
e:
>>>
>>> +1 on Sean's comment. MLlib covers the basic algorithms but we
>>> definitely need to spend more time on how to make the design scalable.
>>> For example, think about current "ProblemWithAlgorithm" naming scheme.
>>> That being said, new algorithms are welcomed. I wish they are
>>> well-established and well-understood by users. They shouldn't be
>>> research algorithms tuned to work well with a particular dataset but
>>> not tested widely. You see the change log from Mahout:
>>>
>>> =3D=3D=3D
>>> The following algorithms that were marked deprecated in 0.8 have been
>>> removed in 0.9:
>>>
>>> From Clustering:
>>>  Switched LDA implementation from using Gibbs Sampling to Collapsed
>>> Variational Bayes (CVB)
>>> Meanshift
>>> MinHash - removed due to poor performance, lack of support and lack of
>>> usage
>>>
>>> From Classification (both are sequential implementations)
>>> Winnow - lack of actual usage and support
>>> Perceptron - lack of actual usage and support
>>>
>>> Collaborative Filtering
>>>    SlopeOne implementations in
>>> org.apache.mahout.cf.taste.hadoop.slopeone and
>>> org.apache.mahout.cf.taste.impl.recommender.slopeone
>>>    Distributed pseudo recommender in
>>> org.apache.mahout.cf.taste.hadoop.pseudo
>>>    TreeClusteringRecommender in
>>> org.apache.mahout.cf.taste.impl.recommender
>>>
>>> Mahout Math
>>>    Hadoop entropy stuff in org.apache.mahout.math.stats.entropy
>>> =3D=3D=3D
>>>
>>> In MLlib, we should include the algorithms users know how to use and
>>> we can provide support rather than letting algorithms come and go.
>>>
>>> My $0.02,
>>> Xiangrui
>>>
>>>> On Mon, Apr 21, 2014 at 10:23 AM, Sean Owen <sowen@cloudera.com> wrote=
:
>>>>> On Mon, Apr 21, 2014 at 6:03 PM, Paul Brown <prb@mult.ifario.us> wrot=
e:
>>>>> - MLlib as Mahout.next would be a unfortunate.  There are some gems i=
n
>>>>> Mahout, but there are also lots of rocks.  Setting a minimal bar of
>>>>> working, correctly implemented, and documented requires a surprising
>>> amount
>>>>> of work.
>>>>
>>>> As someone with first-hand knowledge, this is correct. To Sang's
>>>> question, I can't see value in 'porting' Mahout since it is based on a
>>>> quite different paradigm. About the only part that translates is the
>>>> algorithm concept itself.
>>>>
>>>> This is also the cautionary tale. The contents of the project have
>>>> ended up being a number of "drive-by" contributions of implementations
>>>> that, while individually perhaps brilliant (perhaps), didn't
>>>> necessarily match any other implementation in structure, input/output,
>>>> libraries used. The implementations were often a touch academic. The
>>>> result was hard to document, maintain, evolve or use.
>>>>
>>>> Far more of the structure of the MLlib implementations are consistent
>>>> by virtue of being built around Spark core already. That's great.
>>>>
>>>> One can't wait to completely build the foundation before building any
>>>> implementations. To me, the existing implementations are almost
>>>> exactly the basics I would choose. They cover the bases and will
>>>> exercise the abstractions and structure. So that's also great IMHO.
>>>

From dev-return-7381-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 22 01:02:17 2014
Return-Path: <dev-return-7381-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AADA31199F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Apr 2014 01:02:17 +0000 (UTC)
Received: (qmail 97716 invoked by uid 500); 22 Apr 2014 01:02:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97620 invoked by uid 500); 22 Apr 2014 01:02:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 97612 invoked by uid 99); 22 Apr 2014 01:02:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Apr 2014 01:02:16 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.173 as permitted sender)
Received: from [209.85.216.173] (HELO mail-qc0-f173.google.com) (209.85.216.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Apr 2014 01:02:12 +0000
Received: by mail-qc0-f173.google.com with SMTP id r5so4777014qcx.32
        for <dev@spark.apache.org>; Mon, 21 Apr 2014 18:01:51 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=y6wxXrvJc/1o3nDxckW2jV/VxG5juDOVke+gnrnh5IM=;
        b=OEv182gQkc48Zdy5S2UBkr9+9kmm8gp/Lpxkp2mieccVRa2uY/dnqxHafATXX7X/Hz
         KheYO8hGdp5Ssuru18OclGd/G0xIM9gte4ipo3Pr2MSEmQi7rq266HjOcBafqyPy4zgn
         ehAS9i+iHzqP6aiF5kZhAKDLpYE+lB00ULlO68yoNPf+WJHKNYh0FA7W3ptrDH94Wd5b
         RBDdQ79JGS22KKM8BG7hRHyaL8Mbdi2V7qs92m6//Hf4reUwyhjwmNOoDUlVJWNGZFrp
         IoC2BPYTb1A9pNoYY8MtVKJI8W3tdBBGOhHf4/KoI2FSA3s2AawMuTUsjaPpjrdWQybl
         a7tw==
X-Gm-Message-State: ALoCoQnFLPj0FVEOI2QUjIPQ2ceypEooBTcAtsrPKKsYDkBV8szJ0IMmir5NHIX4s+sSQDb1pyQP
MIME-Version: 1.0
X-Received: by 10.140.91.228 with SMTP id z91mr47767891qgd.38.1398128511557;
 Mon, 21 Apr 2014 18:01:51 -0700 (PDT)
Received: by 10.140.94.2 with HTTP; Mon, 21 Apr 2014 18:01:51 -0700 (PDT)
In-Reply-To: <CAJgQjQ9wmVtwCtW21sxQ+OCsN3v__1MrTVsFKuJ8Q_WX-018ig@mail.gmail.com>
References: <CADSNrJHOT035G1xda8p9eZz-90uiWOb4CwgiY1r-d8m9V9LM+g@mail.gmail.com>
	<CAMAsSdJ5em5SgjszP-NiA58u_NR_tgT2PC6Wwxi4W8VX0cvVgA@mail.gmail.com>
	<CACArsZ8w=mRKMwPyG9+9Kyyn-kVtOj1E2ujC6p_Uv_X-86m1Tg@mail.gmail.com>
	<CAMAsSdJX=2UiV2CmJWwNX4Uiud8JuvcFoCkbnOrWJBouJMMjkg@mail.gmail.com>
	<CAJgQjQ8KzOUGH-ChXy5QiLuK_WEmj5FS31ucHtRftZd5Aj46eA@mail.gmail.com>
	<CACBYxK+-NCgGHKSNt=T+TCCqofqA+-HQzC00FB2qJdD-c1hNGw@mail.gmail.com>
	<A1C0F7CE-1A54-4486-89EA-63EA9DAB65C9@googlemail.com>
	<CAJgQjQ9wmVtwCtW21sxQ+OCsN3v__1MrTVsFKuJ8Q_WX-018ig@mail.gmail.com>
Date: Mon, 21 Apr 2014 18:01:51 -0700
Message-ID: <CACBYxKJBeZ2Hu=nupm0u3ZPqWRu=4KGmppQ6OVqNBnqzP4HR+A@mail.gmail.com>
Subject: Re: Any plans for new clustering algorithms?
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113a7b1e48318904f7972960
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a7b1e48318904f7972960
Content-Type: text/plain; charset=ISO-8859-1

How do I get permissions to edit the wiki?


On Mon, Apr 21, 2014 at 3:19 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Cannot agree more with your words. Could you add one section about
> "how and what to contribute" to MLlib's guide? -Xiangrui
>
> On Mon, Apr 21, 2014 at 1:41 PM, Nick Pentreath
> <nick.pentreath@gmail.com> wrote:
> > I'd say a section in the "how to contribute" page would be a good place
> to put this.
> >
> > In general I'd say that the criteria for inclusion of an algorithm is it
> should be high quality, widely known, used and accepted (citations and
> concrete use cases as examples of this), scalable and parallelizable, well
> documented and with reasonable expectation of dev support
> >
> > Sent from my iPhone
> >
> >> On 21 Apr 2014, at 19:59, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
> >>
> >> If it's not done already, would it make sense to codify this philosophy
> >> somewhere?  I imagine this won't be the first time this discussion comes
> >> up, and it would be nice to have a doc to point to.  I'd be happy to
> take a
> >> stab at this.
> >>
> >>
> >>> On Mon, Apr 21, 2014 at 10:54 AM, Xiangrui Meng <mengxr@gmail.com>
> wrote:
> >>>
> >>> +1 on Sean's comment. MLlib covers the basic algorithms but we
> >>> definitely need to spend more time on how to make the design scalable.
> >>> For example, think about current "ProblemWithAlgorithm" naming scheme.
> >>> That being said, new algorithms are welcomed. I wish they are
> >>> well-established and well-understood by users. They shouldn't be
> >>> research algorithms tuned to work well with a particular dataset but
> >>> not tested widely. You see the change log from Mahout:
> >>>
> >>> ===
> >>> The following algorithms that were marked deprecated in 0.8 have been
> >>> removed in 0.9:
> >>>
> >>> From Clustering:
> >>>  Switched LDA implementation from using Gibbs Sampling to Collapsed
> >>> Variational Bayes (CVB)
> >>> Meanshift
> >>> MinHash - removed due to poor performance, lack of support and lack of
> >>> usage
> >>>
> >>> From Classification (both are sequential implementations)
> >>> Winnow - lack of actual usage and support
> >>> Perceptron - lack of actual usage and support
> >>>
> >>> Collaborative Filtering
> >>>    SlopeOne implementations in
> >>> org.apache.mahout.cf.taste.hadoop.slopeone and
> >>> org.apache.mahout.cf.taste.impl.recommender.slopeone
> >>>    Distributed pseudo recommender in
> >>> org.apache.mahout.cf.taste.hadoop.pseudo
> >>>    TreeClusteringRecommender in
> >>> org.apache.mahout.cf.taste.impl.recommender
> >>>
> >>> Mahout Math
> >>>    Hadoop entropy stuff in org.apache.mahout.math.stats.entropy
> >>> ===
> >>>
> >>> In MLlib, we should include the algorithms users know how to use and
> >>> we can provide support rather than letting algorithms come and go.
> >>>
> >>> My $0.02,
> >>> Xiangrui
> >>>
> >>>> On Mon, Apr 21, 2014 at 10:23 AM, Sean Owen <sowen@cloudera.com>
> wrote:
> >>>>> On Mon, Apr 21, 2014 at 6:03 PM, Paul Brown <prb@mult.ifario.us>
> wrote:
> >>>>> - MLlib as Mahout.next would be a unfortunate.  There are some gems
> in
> >>>>> Mahout, but there are also lots of rocks.  Setting a minimal bar of
> >>>>> working, correctly implemented, and documented requires a surprising
> >>> amount
> >>>>> of work.
> >>>>
> >>>> As someone with first-hand knowledge, this is correct. To Sang's
> >>>> question, I can't see value in 'porting' Mahout since it is based on a
> >>>> quite different paradigm. About the only part that translates is the
> >>>> algorithm concept itself.
> >>>>
> >>>> This is also the cautionary tale. The contents of the project have
> >>>> ended up being a number of "drive-by" contributions of implementations
> >>>> that, while individually perhaps brilliant (perhaps), didn't
> >>>> necessarily match any other implementation in structure, input/output,
> >>>> libraries used. The implementations were often a touch academic. The
> >>>> result was hard to document, maintain, evolve or use.
> >>>>
> >>>> Far more of the structure of the MLlib implementations are consistent
> >>>> by virtue of being built around Spark core already. That's great.
> >>>>
> >>>> One can't wait to completely build the foundation before building any
> >>>> implementations. To me, the existing implementations are almost
> >>>> exactly the basics I would choose. They cover the bases and will
> >>>> exercise the abstractions and structure. So that's also great IMHO.
> >>>
>

--001a113a7b1e48318904f7972960--

From dev-return-7382-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 22 01:10:21 2014
Return-Path: <dev-return-7382-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 77463119D5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Apr 2014 01:10:21 +0000 (UTC)
Received: (qmail 11554 invoked by uid 500); 22 Apr 2014 01:10:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11444 invoked by uid 500); 22 Apr 2014 01:10:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11436 invoked by uid 99); 22 Apr 2014 01:10:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Apr 2014 01:10:20 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 74.125.82.170 as permitted sender)
Received: from [74.125.82.170] (HELO mail-we0-f170.google.com) (74.125.82.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Apr 2014 01:10:16 +0000
Received: by mail-we0-f170.google.com with SMTP id w61so4356653wes.15
        for <dev@spark.apache.org>; Mon, 21 Apr 2014 18:09:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=euiwWIJSKgoguA+gzYAQJqP+VHmhfQ/dut9c3fmMqhA=;
        b=satH+Ep1LSvU9nTUqO++FBzaVhVNKHE74n1YT6OEaQ9+qRgtsOM3E878dRYSkq/qLZ
         WWs32W1Yhbfdz26A+90h2cc21usLYFToQnY7S/MVCV8TnVlodAZqPyOE0b/Vgy/oov87
         jUzXaTCFhNp3aN3F48s1FZOdVSoVgfrs/YlorNpzNK8W/FtLkXFDDOuQSOUPtvQDRlA6
         xkcchGvtgDCTrSUQQDR0//e3OaVoJxJXj0EDL/0nSN0t11EF1FwGO0NjzbRN5BlUtJJu
         6tUty6IDZyMTxjwkMAp2yqKXaq3QgUF5ppzwC9ZauHAgytYQj9F666Szcz0p/doUG/Ff
         FXmw==
MIME-Version: 1.0
X-Received: by 10.194.6.106 with SMTP id z10mr30489570wjz.1.1398128994996;
 Mon, 21 Apr 2014 18:09:54 -0700 (PDT)
Received: by 10.194.82.105 with HTTP; Mon, 21 Apr 2014 18:09:54 -0700 (PDT)
In-Reply-To: <CACBYxKJBeZ2Hu=nupm0u3ZPqWRu=4KGmppQ6OVqNBnqzP4HR+A@mail.gmail.com>
References: <CADSNrJHOT035G1xda8p9eZz-90uiWOb4CwgiY1r-d8m9V9LM+g@mail.gmail.com>
	<CAMAsSdJ5em5SgjszP-NiA58u_NR_tgT2PC6Wwxi4W8VX0cvVgA@mail.gmail.com>
	<CACArsZ8w=mRKMwPyG9+9Kyyn-kVtOj1E2ujC6p_Uv_X-86m1Tg@mail.gmail.com>
	<CAMAsSdJX=2UiV2CmJWwNX4Uiud8JuvcFoCkbnOrWJBouJMMjkg@mail.gmail.com>
	<CAJgQjQ8KzOUGH-ChXy5QiLuK_WEmj5FS31ucHtRftZd5Aj46eA@mail.gmail.com>
	<CACBYxK+-NCgGHKSNt=T+TCCqofqA+-HQzC00FB2qJdD-c1hNGw@mail.gmail.com>
	<A1C0F7CE-1A54-4486-89EA-63EA9DAB65C9@googlemail.com>
	<CAJgQjQ9wmVtwCtW21sxQ+OCsN3v__1MrTVsFKuJ8Q_WX-018ig@mail.gmail.com>
	<CACBYxKJBeZ2Hu=nupm0u3ZPqWRu=4KGmppQ6OVqNBnqzP4HR+A@mail.gmail.com>
Date: Mon, 21 Apr 2014 18:09:54 -0700
Message-ID: <CAJgQjQ_oK0Q65uLBtGfzq6qW3rWrXvEojUcPj-4NOrO7cuUn6g@mail.gmail.com>
Subject: Re: Any plans for new clustering algorithms?
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

The markdown files are under spark/docs. You can submit a PR for
changes. -Xiangrui

On Mon, Apr 21, 2014 at 6:01 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
> How do I get permissions to edit the wiki?
>
>
> On Mon, Apr 21, 2014 at 3:19 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
>> Cannot agree more with your words. Could you add one section about
>> "how and what to contribute" to MLlib's guide? -Xiangrui
>>
>> On Mon, Apr 21, 2014 at 1:41 PM, Nick Pentreath
>> <nick.pentreath@gmail.com> wrote:
>> > I'd say a section in the "how to contribute" page would be a good place
>> to put this.
>> >
>> > In general I'd say that the criteria for inclusion of an algorithm is it
>> should be high quality, widely known, used and accepted (citations and
>> concrete use cases as examples of this), scalable and parallelizable, well
>> documented and with reasonable expectation of dev support
>> >
>> > Sent from my iPhone
>> >
>> >> On 21 Apr 2014, at 19:59, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
>> >>
>> >> If it's not done already, would it make sense to codify this philosophy
>> >> somewhere?  I imagine this won't be the first time this discussion comes
>> >> up, and it would be nice to have a doc to point to.  I'd be happy to
>> take a
>> >> stab at this.
>> >>
>> >>
>> >>> On Mon, Apr 21, 2014 at 10:54 AM, Xiangrui Meng <mengxr@gmail.com>
>> wrote:
>> >>>
>> >>> +1 on Sean's comment. MLlib covers the basic algorithms but we
>> >>> definitely need to spend more time on how to make the design scalable.
>> >>> For example, think about current "ProblemWithAlgorithm" naming scheme.
>> >>> That being said, new algorithms are welcomed. I wish they are
>> >>> well-established and well-understood by users. They shouldn't be
>> >>> research algorithms tuned to work well with a particular dataset but
>> >>> not tested widely. You see the change log from Mahout:
>> >>>
>> >>> ===
>> >>> The following algorithms that were marked deprecated in 0.8 have been
>> >>> removed in 0.9:
>> >>>
>> >>> From Clustering:
>> >>>  Switched LDA implementation from using Gibbs Sampling to Collapsed
>> >>> Variational Bayes (CVB)
>> >>> Meanshift
>> >>> MinHash - removed due to poor performance, lack of support and lack of
>> >>> usage
>> >>>
>> >>> From Classification (both are sequential implementations)
>> >>> Winnow - lack of actual usage and support
>> >>> Perceptron - lack of actual usage and support
>> >>>
>> >>> Collaborative Filtering
>> >>>    SlopeOne implementations in
>> >>> org.apache.mahout.cf.taste.hadoop.slopeone and
>> >>> org.apache.mahout.cf.taste.impl.recommender.slopeone
>> >>>    Distributed pseudo recommender in
>> >>> org.apache.mahout.cf.taste.hadoop.pseudo
>> >>>    TreeClusteringRecommender in
>> >>> org.apache.mahout.cf.taste.impl.recommender
>> >>>
>> >>> Mahout Math
>> >>>    Hadoop entropy stuff in org.apache.mahout.math.stats.entropy
>> >>> ===
>> >>>
>> >>> In MLlib, we should include the algorithms users know how to use and
>> >>> we can provide support rather than letting algorithms come and go.
>> >>>
>> >>> My $0.02,
>> >>> Xiangrui
>> >>>
>> >>>> On Mon, Apr 21, 2014 at 10:23 AM, Sean Owen <sowen@cloudera.com>
>> wrote:
>> >>>>> On Mon, Apr 21, 2014 at 6:03 PM, Paul Brown <prb@mult.ifario.us>
>> wrote:
>> >>>>> - MLlib as Mahout.next would be a unfortunate.  There are some gems
>> in
>> >>>>> Mahout, but there are also lots of rocks.  Setting a minimal bar of
>> >>>>> working, correctly implemented, and documented requires a surprising
>> >>> amount
>> >>>>> of work.
>> >>>>
>> >>>> As someone with first-hand knowledge, this is correct. To Sang's
>> >>>> question, I can't see value in 'porting' Mahout since it is based on a
>> >>>> quite different paradigm. About the only part that translates is the
>> >>>> algorithm concept itself.
>> >>>>
>> >>>> This is also the cautionary tale. The contents of the project have
>> >>>> ended up being a number of "drive-by" contributions of implementations
>> >>>> that, while individually perhaps brilliant (perhaps), didn't
>> >>>> necessarily match any other implementation in structure, input/output,
>> >>>> libraries used. The implementations were often a touch academic. The
>> >>>> result was hard to document, maintain, evolve or use.
>> >>>>
>> >>>> Far more of the structure of the MLlib implementations are consistent
>> >>>> by virtue of being built around Spark core already. That's great.
>> >>>>
>> >>>> One can't wait to completely build the foundation before building any
>> >>>> implementations. To me, the existing implementations are almost
>> >>>> exactly the basics I would choose. They cover the bases and will
>> >>>> exercise the abstractions and structure. So that's also great IMHO.
>> >>>
>>

From dev-return-7383-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 22 01:15:29 2014
Return-Path: <dev-return-7383-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B50CF11A01
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Apr 2014 01:15:29 +0000 (UTC)
Received: (qmail 16721 invoked by uid 500); 22 Apr 2014 01:15:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16524 invoked by uid 500); 22 Apr 2014 01:15:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16505 invoked by uid 99); 22 Apr 2014 01:15:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Apr 2014 01:15:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.44 as permitted sender)
Received: from [209.85.216.44] (HELO mail-qa0-f44.google.com) (209.85.216.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Apr 2014 01:15:13 +0000
Received: by mail-qa0-f44.google.com with SMTP id hw13so4510468qab.31
        for <dev@spark.apache.org>; Mon, 21 Apr 2014 18:14:52 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=RafmMhyW5B+iS8ITsUdYYlON+60WPdprR7gCErTVGKs=;
        b=LeQABChF2m9NoVV58z+Az3fU+wM3Dsl9vGmpdQjC0q0Tl0tneQhOBqu0d/HMpzSCEL
         1+K6lKek5xPzIetoaRL/zMebJI7fGo3URVAp8AJuH31y3cymx8ZBk7d8V0DL/zBxIx3D
         FG0c9I+QNOJRUWEKu+ZsUp+mrtK8vTzzNLlhLhIdyAMA5+yjggYwlpL7w/QN8UubE54N
         Xv4pk+FssAoVDjwjdlPMy895NY7ANOeAw1Jb96ly58k+ThvD1ySaD7BPY81Y/48Atg3c
         iyHSxUFDYY3e7K3oDkDYEPgnC7aRnLDk18naw5xuOe6V5VNrB39m/M9te/YLZocP8GqG
         Do5A==
X-Gm-Message-State: ALoCoQlHImzJ+RrP+LLJoNiqDTl6MHzwgse6tEKiqB0HVcwz1sX/zvgt3TH++3ZuEjW7fHA4Yqv5
MIME-Version: 1.0
X-Received: by 10.229.136.135 with SMTP id r7mr43528593qct.17.1398129292227;
 Mon, 21 Apr 2014 18:14:52 -0700 (PDT)
Received: by 10.140.94.2 with HTTP; Mon, 21 Apr 2014 18:14:52 -0700 (PDT)
In-Reply-To: <CAJgQjQ_oK0Q65uLBtGfzq6qW3rWrXvEojUcPj-4NOrO7cuUn6g@mail.gmail.com>
References: <CADSNrJHOT035G1xda8p9eZz-90uiWOb4CwgiY1r-d8m9V9LM+g@mail.gmail.com>
	<CAMAsSdJ5em5SgjszP-NiA58u_NR_tgT2PC6Wwxi4W8VX0cvVgA@mail.gmail.com>
	<CACArsZ8w=mRKMwPyG9+9Kyyn-kVtOj1E2ujC6p_Uv_X-86m1Tg@mail.gmail.com>
	<CAMAsSdJX=2UiV2CmJWwNX4Uiud8JuvcFoCkbnOrWJBouJMMjkg@mail.gmail.com>
	<CAJgQjQ8KzOUGH-ChXy5QiLuK_WEmj5FS31ucHtRftZd5Aj46eA@mail.gmail.com>
	<CACBYxK+-NCgGHKSNt=T+TCCqofqA+-HQzC00FB2qJdD-c1hNGw@mail.gmail.com>
	<A1C0F7CE-1A54-4486-89EA-63EA9DAB65C9@googlemail.com>
	<CAJgQjQ9wmVtwCtW21sxQ+OCsN3v__1MrTVsFKuJ8Q_WX-018ig@mail.gmail.com>
	<CACBYxKJBeZ2Hu=nupm0u3ZPqWRu=4KGmppQ6OVqNBnqzP4HR+A@mail.gmail.com>
	<CAJgQjQ_oK0Q65uLBtGfzq6qW3rWrXvEojUcPj-4NOrO7cuUn6g@mail.gmail.com>
Date: Mon, 21 Apr 2014 18:14:52 -0700
Message-ID: <CACBYxKJB63gPZ_M0RH-B-V7msc1s6roxzLBjJ883BHmjuK6CsA@mail.gmail.com>
Subject: Re: Any plans for new clustering algorithms?
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11360368d053b304f79757a1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11360368d053b304f79757a1
Content-Type: text/plain; charset=ISO-8859-1

I thought this might be a good thing to add to the wiki's "How to
contribute" page<https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark>,
as it's not tied to a release.


On Mon, Apr 21, 2014 at 6:09 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> The markdown files are under spark/docs. You can submit a PR for
> changes. -Xiangrui
>
> On Mon, Apr 21, 2014 at 6:01 PM, Sandy Ryza <sandy.ryza@cloudera.com>
> wrote:
> > How do I get permissions to edit the wiki?
> >
> >
> > On Mon, Apr 21, 2014 at 3:19 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
> >
> >> Cannot agree more with your words. Could you add one section about
> >> "how and what to contribute" to MLlib's guide? -Xiangrui
> >>
> >> On Mon, Apr 21, 2014 at 1:41 PM, Nick Pentreath
> >> <nick.pentreath@gmail.com> wrote:
> >> > I'd say a section in the "how to contribute" page would be a good
> place
> >> to put this.
> >> >
> >> > In general I'd say that the criteria for inclusion of an algorithm is
> it
> >> should be high quality, widely known, used and accepted (citations and
> >> concrete use cases as examples of this), scalable and parallelizable,
> well
> >> documented and with reasonable expectation of dev support
> >> >
> >> > Sent from my iPhone
> >> >
> >> >> On 21 Apr 2014, at 19:59, Sandy Ryza <sandy.ryza@cloudera.com>
> wrote:
> >> >>
> >> >> If it's not done already, would it make sense to codify this
> philosophy
> >> >> somewhere?  I imagine this won't be the first time this discussion
> comes
> >> >> up, and it would be nice to have a doc to point to.  I'd be happy to
> >> take a
> >> >> stab at this.
> >> >>
> >> >>
> >> >>> On Mon, Apr 21, 2014 at 10:54 AM, Xiangrui Meng <mengxr@gmail.com>
> >> wrote:
> >> >>>
> >> >>> +1 on Sean's comment. MLlib covers the basic algorithms but we
> >> >>> definitely need to spend more time on how to make the design
> scalable.
> >> >>> For example, think about current "ProblemWithAlgorithm" naming
> scheme.
> >> >>> That being said, new algorithms are welcomed. I wish they are
> >> >>> well-established and well-understood by users. They shouldn't be
> >> >>> research algorithms tuned to work well with a particular dataset but
> >> >>> not tested widely. You see the change log from Mahout:
> >> >>>
> >> >>> ===
> >> >>> The following algorithms that were marked deprecated in 0.8 have
> been
> >> >>> removed in 0.9:
> >> >>>
> >> >>> From Clustering:
> >> >>>  Switched LDA implementation from using Gibbs Sampling to Collapsed
> >> >>> Variational Bayes (CVB)
> >> >>> Meanshift
> >> >>> MinHash - removed due to poor performance, lack of support and lack
> of
> >> >>> usage
> >> >>>
> >> >>> From Classification (both are sequential implementations)
> >> >>> Winnow - lack of actual usage and support
> >> >>> Perceptron - lack of actual usage and support
> >> >>>
> >> >>> Collaborative Filtering
> >> >>>    SlopeOne implementations in
> >> >>> org.apache.mahout.cf.taste.hadoop.slopeone and
> >> >>> org.apache.mahout.cf.taste.impl.recommender.slopeone
> >> >>>    Distributed pseudo recommender in
> >> >>> org.apache.mahout.cf.taste.hadoop.pseudo
> >> >>>    TreeClusteringRecommender in
> >> >>> org.apache.mahout.cf.taste.impl.recommender
> >> >>>
> >> >>> Mahout Math
> >> >>>    Hadoop entropy stuff in org.apache.mahout.math.stats.entropy
> >> >>> ===
> >> >>>
> >> >>> In MLlib, we should include the algorithms users know how to use and
> >> >>> we can provide support rather than letting algorithms come and go.
> >> >>>
> >> >>> My $0.02,
> >> >>> Xiangrui
> >> >>>
> >> >>>> On Mon, Apr 21, 2014 at 10:23 AM, Sean Owen <sowen@cloudera.com>
> >> wrote:
> >> >>>>> On Mon, Apr 21, 2014 at 6:03 PM, Paul Brown <prb@mult.ifario.us>
> >> wrote:
> >> >>>>> - MLlib as Mahout.next would be a unfortunate.  There are some
> gems
> >> in
> >> >>>>> Mahout, but there are also lots of rocks.  Setting a minimal bar
> of
> >> >>>>> working, correctly implemented, and documented requires a
> surprising
> >> >>> amount
> >> >>>>> of work.
> >> >>>>
> >> >>>> As someone with first-hand knowledge, this is correct. To Sang's
> >> >>>> question, I can't see value in 'porting' Mahout since it is based
> on a
> >> >>>> quite different paradigm. About the only part that translates is
> the
> >> >>>> algorithm concept itself.
> >> >>>>
> >> >>>> This is also the cautionary tale. The contents of the project have
> >> >>>> ended up being a number of "drive-by" contributions of
> implementations
> >> >>>> that, while individually perhaps brilliant (perhaps), didn't
> >> >>>> necessarily match any other implementation in structure,
> input/output,
> >> >>>> libraries used. The implementations were often a touch academic.
> The
> >> >>>> result was hard to document, maintain, evolve or use.
> >> >>>>
> >> >>>> Far more of the structure of the MLlib implementations are
> consistent
> >> >>>> by virtue of being built around Spark core already. That's great.
> >> >>>>
> >> >>>> One can't wait to completely build the foundation before building
> any
> >> >>>> implementations. To me, the existing implementations are almost
> >> >>>> exactly the basics I would choose. They cover the bases and will
> >> >>>> exercise the abstractions and structure. So that's also great IMHO.
> >> >>>
> >>
>

--001a11360368d053b304f79757a1--

From dev-return-7384-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 22 01:17:01 2014
Return-Path: <dev-return-7384-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8ADDF11A19
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Apr 2014 01:17:01 +0000 (UTC)
Received: (qmail 22365 invoked by uid 500); 22 Apr 2014 01:16:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22321 invoked by uid 500); 22 Apr 2014 01:16:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22230 invoked by uid 99); 22 Apr 2014 01:16:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Apr 2014 01:16:54 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.216.50 as permitted sender)
Received: from [209.85.216.50] (HELO mail-qa0-f50.google.com) (209.85.216.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Apr 2014 01:16:50 +0000
Received: by mail-qa0-f50.google.com with SMTP id ih12so4378545qab.23
        for <dev@spark.apache.org>; Mon, 21 Apr 2014 18:16:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=tDUnhw8rF4OmToQpyuVCsltREJnmy1mqTZ5PB1lPnNk=;
        b=uWB0Qf/X44jot2/YJmfL+36uJOWDrp0lnZQ74wCh84DBapYA0V5ix3H/8gkjPCnnhn
         WfO0TtdPfFt8uhXjYvbEymZwLEphmvQ4mSLH+gNq6TRronWq18lAqCHI0z17NFpJrXd6
         yO+OHfdErXygaoBkSm9z5tmKbs88rWjiljMCvhzWwSjWuyLE/O3jeF6rXwxXRVoiPGJZ
         Zs+/Zt7LLKksY6BrRB+RKfb2K7DxI96CIhSHLTnaperWLZT4J701flFD4EWYr3b+sToi
         OJtnGknsD8ZGgFcxoEj5lb0jnwHCzvUr/wvaiuLdzO+wpPLFmyn4XfEeusjgPu1IcKrL
         IUNA==
X-Received: by 10.224.36.194 with SMTP id u2mr42786271qad.73.1398129387083;
        Mon, 21 Apr 2014 18:16:27 -0700 (PDT)
Received: from [192.168.2.17] ([69.157.95.72])
        by mx.google.com with ESMTPSA id 11sm47372670qgv.20.2014.04.21.18.16.26
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Mon, 21 Apr 2014 18:16:26 -0700 (PDT)
Date: Mon, 21 Apr 2014 21:22:58 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Message-ID: <DAD72F07091F4CD7A99CDD4E3222EC5C@gmail.com>
In-Reply-To: <CAJgQjQ_oK0Q65uLBtGfzq6qW3rWrXvEojUcPj-4NOrO7cuUn6g@mail.gmail.com>
References: <CADSNrJHOT035G1xda8p9eZz-90uiWOb4CwgiY1r-d8m9V9LM+g@mail.gmail.com>
 <CAMAsSdJ5em5SgjszP-NiA58u_NR_tgT2PC6Wwxi4W8VX0cvVgA@mail.gmail.com>
 <CACArsZ8w=mRKMwPyG9+9Kyyn-kVtOj1E2ujC6p_Uv_X-86m1Tg@mail.gmail.com>
 <CAMAsSdJX=2UiV2CmJWwNX4Uiud8JuvcFoCkbnOrWJBouJMMjkg@mail.gmail.com>
 <CAJgQjQ8KzOUGH-ChXy5QiLuK_WEmj5FS31ucHtRftZd5Aj46eA@mail.gmail.com>
 <CACBYxK+-NCgGHKSNt=T+TCCqofqA+-HQzC00FB2qJdD-c1hNGw@mail.gmail.com>
 <A1C0F7CE-1A54-4486-89EA-63EA9DAB65C9@googlemail.com>
 <CAJgQjQ9wmVtwCtW21sxQ+OCsN3v__1MrTVsFKuJ8Q_WX-018ig@mail.gmail.com>
 <CACBYxKJBeZ2Hu=nupm0u3ZPqWRu=4KGmppQ6OVqNBnqzP4HR+A@mail.gmail.com>
 <CAJgQjQ_oK0Q65uLBtGfzq6qW3rWrXvEojUcPj-4NOrO7cuUn6g@mail.gmail.com>
Subject: Re: Any plans for new clustering algorithms?
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="5355c472_4ad084e9_206"
X-Virus-Checked: Checked by ClamAV on apache.org

--5355c472_4ad084e9_206
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

I thought those are files of spark.apache.org? 

-- 
Nan Zhu


On Monday, April 21, 2014 at 9:09 PM, Xiangrui Meng wrote:

> The markdown files are under spark/docs. You can submit a PR for
> changes. -Xiangrui
> 
> On Mon, Apr 21, 2014 at 6:01 PM, Sandy Ryza <sandy.ryza@cloudera.com (mailto:sandy.ryza@cloudera.com)> wrote:
> > How do I get permissions to edit the wiki?
> > 
> > 
> > On Mon, Apr 21, 2014 at 3:19 PM, Xiangrui Meng <mengxr@gmail.com (mailto:mengxr@gmail.com)> wrote:
> > 
> > > Cannot agree more with your words. Could you add one section about
> > > "how and what to contribute" to MLlib's guide? -Xiangrui
> > > 
> > > On Mon, Apr 21, 2014 at 1:41 PM, Nick Pentreath
> > > <nick.pentreath@gmail.com (mailto:nick.pentreath@gmail.com)> wrote:
> > > > I'd say a section in the "how to contribute" page would be a good place
> > > 
> > > to put this.
> > > > 
> > > > In general I'd say that the criteria for inclusion of an algorithm is it
> > > should be high quality, widely known, used and accepted (citations and
> > > concrete use cases as examples of this), scalable and parallelizable, well
> > > documented and with reasonable expectation of dev support
> > > > 
> > > > Sent from my iPhone
> > > > 
> > > > > On 21 Apr 2014, at 19:59, Sandy Ryza <sandy.ryza@cloudera.com (mailto:sandy.ryza@cloudera.com)> wrote:
> > > > > 
> > > > > If it's not done already, would it make sense to codify this philosophy
> > > > > somewhere? I imagine this won't be the first time this discussion comes
> > > > > up, and it would be nice to have a doc to point to. I'd be happy to
> > > > > 
> > > > 
> > > > 
> > > 
> > > take a
> > > > > stab at this.
> > > > > 
> > > > > 
> > > > > > On Mon, Apr 21, 2014 at 10:54 AM, Xiangrui Meng <mengxr@gmail.com (mailto:mengxr@gmail.com)>
> > > wrote:
> > > > > > 
> > > > > > +1 on Sean's comment. MLlib covers the basic algorithms but we
> > > > > > definitely need to spend more time on how to make the design scalable.
> > > > > > For example, think about current "ProblemWithAlgorithm" naming scheme.
> > > > > > That being said, new algorithms are welcomed. I wish they are
> > > > > > well-established and well-understood by users. They shouldn't be
> > > > > > research algorithms tuned to work well with a particular dataset but
> > > > > > not tested widely. You see the change log from Mahout:
> > > > > > 
> > > > > > ===
> > > > > > The following algorithms that were marked deprecated in 0.8 have been
> > > > > > removed in 0.9:
> > > > > > 
> > > > > > From Clustering:
> > > > > > Switched LDA implementation from using Gibbs Sampling to Collapsed
> > > > > > Variational Bayes (CVB)
> > > > > > Meanshift
> > > > > > MinHash - removed due to poor performance, lack of support and lack of
> > > > > > usage
> > > > > > 
> > > > > > From Classification (both are sequential implementations)
> > > > > > Winnow - lack of actual usage and support
> > > > > > Perceptron - lack of actual usage and support
> > > > > > 
> > > > > > Collaborative Filtering
> > > > > > SlopeOne implementations in
> > > > > > org.apache.mahout.cf.taste.hadoop.slopeone and
> > > > > > org.apache.mahout.cf.taste.impl.recommender.slopeone
> > > > > > Distributed pseudo recommender in
> > > > > > org.apache.mahout.cf.taste.hadoop.pseudo
> > > > > > TreeClusteringRecommender in
> > > > > > org.apache.mahout.cf.taste.impl.recommender
> > > > > > 
> > > > > > Mahout Math
> > > > > > Hadoop entropy stuff in org.apache.mahout.math.stats.entropy
> > > > > > ===
> > > > > > 
> > > > > > In MLlib, we should include the algorithms users know how to use and
> > > > > > we can provide support rather than letting algorithms come and go.
> > > > > > 
> > > > > > My $0.02,
> > > > > > Xiangrui
> > > > > > 
> > > > > > > On Mon, Apr 21, 2014 at 10:23 AM, Sean Owen <sowen@cloudera.com (mailto:sowen@cloudera.com)>
> > > wrote:
> > > > > > > > On Mon, Apr 21, 2014 at 6:03 PM, Paul Brown <prb@mult.ifario.us (mailto:prb@mult.ifario.us)>
> > > > > > > 
> > > > > > 
> > > > > 
> > > > 
> > > 
> > > wrote:
> > > > > > > > - MLlib as Mahout.next would be a unfortunate. There are some gems
> > > > > > > 
> > > > > > 
> > > > > 
> > > > 
> > > 
> > > in
> > > > > > > > Mahout, but there are also lots of rocks. Setting a minimal bar of
> > > > > > > > working, correctly implemented, and documented requires a surprising
> > > > > > > > 
> > > > > > > 
> > > > > > 
> > > > > > amount
> > > > > > > > of work.
> > > > > > > 
> > > > > > > 
> > > > > > > As someone with first-hand knowledge, this is correct. To Sang's
> > > > > > > question, I can't see value in 'porting' Mahout since it is based on a
> > > > > > > quite different paradigm. About the only part that translates is the
> > > > > > > algorithm concept itself.
> > > > > > > 
> > > > > > > This is also the cautionary tale. The contents of the project have
> > > > > > > ended up being a number of "drive-by" contributions of implementations
> > > > > > > that, while individually perhaps brilliant (perhaps), didn't
> > > > > > > necessarily match any other implementation in structure, input/output,
> > > > > > > libraries used. The implementations were often a touch academic. The
> > > > > > > result was hard to document, maintain, evolve or use.
> > > > > > > 
> > > > > > > Far more of the structure of the MLlib implementations are consistent
> > > > > > > by virtue of being built around Spark core already. That's great.
> > > > > > > 
> > > > > > > One can't wait to completely build the foundation before building any
> > > > > > > implementations. To me, the existing implementations are almost
> > > > > > > exactly the basics I would choose. They cover the bases and will
> > > > > > > exercise the abstractions and structure. So that's also great IMHO.
> > > > > > > 
> > > > > > 
> > > > > > 
> > > > > 
> > > > 
> > > 
> > > 
> > 
> > 
> 
> 
> 



--5355c472_4ad084e9_206--


From dev-return-7385-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 22 02:25:44 2014
Return-Path: <dev-return-7385-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0BCDC11B81
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Apr 2014 02:25:44 +0000 (UTC)
Received: (qmail 1631 invoked by uid 500); 22 Apr 2014 02:25:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1539 invoked by uid 500); 22 Apr 2014 02:25:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1531 invoked by uid 99); 22 Apr 2014 02:25:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Apr 2014 02:25:42 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.45 as permitted sender)
Received: from [209.85.220.45] (HELO mail-pa0-f45.google.com) (209.85.220.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Apr 2014 02:25:38 +0000
Received: by mail-pa0-f45.google.com with SMTP id kl14so4378032pab.18
        for <dev@spark.apache.org>; Mon, 21 Apr 2014 19:25:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=00gCIlUBbVxNmL9lUYsjPVC68q3l7t9UTbRX6fVkbNY=;
        b=gLP5Yn3z3BDbthL0q4U1SGVfgvGQuHVYVg5fjOUUvo+l7PksnU032qCGlELNjIP9uc
         JzZ9DzE+nQmzd5ML49S6iUb2L3Pch5EHj2jRXYCqfmzkcHlcadzONy4OAq8tGkez2ETA
         oBJ1cBoNPFZT0Ob/YJwYZxwnNUQ/FX895UjoGRCMtDQ9vVW76thok+jAYdU9kWGbVNYT
         rasxEmozFLVog8Bfaj0QKW5wZrbwvuWsdYMeIil6HbteYPfk/TTPmhNc/XazZfZRrGHE
         HL/ruUtJ3yDdXTseJ74YaN3icafVd11rTOEsf5Azob0yyT3dh4b6DC2ciQpDeRkftk/x
         FywA==
X-Received: by 10.68.189.33 with SMTP id gf1mr41288310pbc.111.1398133514932;
        Mon, 21 Apr 2014 19:25:14 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id tk5sm81087227pbc.63.2014.04.21.19.25.13
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 21 Apr 2014 19:25:13 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: Any plans for new clustering algorithms?
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <DAD72F07091F4CD7A99CDD4E3222EC5C@gmail.com>
Date: Mon, 21 Apr 2014 19:25:10 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <6C66459B-CAC2-45C5-A8EA-23ACB711F2DC@gmail.com>
References: <CADSNrJHOT035G1xda8p9eZz-90uiWOb4CwgiY1r-d8m9V9LM+g@mail.gmail.com> <CAMAsSdJ5em5SgjszP-NiA58u_NR_tgT2PC6Wwxi4W8VX0cvVgA@mail.gmail.com> <CACArsZ8w=mRKMwPyG9+9Kyyn-kVtOj1E2ujC6p_Uv_X-86m1Tg@mail.gmail.com> <CAMAsSdJX=2UiV2CmJWwNX4Uiud8JuvcFoCkbnOrWJBouJMMjkg@mail.gmail.com> <CAJgQjQ8KzOUGH-ChXy5QiLuK_WEmj5FS31ucHtRftZd5Aj46eA@mail.gmail.com> <CACBYxK+-NCgGHKSNt=T+TCCqofqA+-HQzC00FB2qJdD-c1hNGw@mail.gmail.com> <A1C0F7CE-1A54-4486-89EA-63EA9DAB65C9@googlemail.com> <CAJgQjQ9wmVtwCtW21sxQ+OCsN3v__1MrTVsFKuJ8Q_WX-018ig@mail.gmail.com> <CACBYxKJBeZ2Hu=nupm0u3ZPqWRu=4KGmppQ6OVqNBnqzP4HR+A@mail.gmail.com> <CAJgQjQ_oK0Q65uLBtGfzq6qW3rWrXvEojUcPj-4NOrO7cuUn6g@mail.gmail.com> <DAD72F07091F4CD7A99CDD4E3222EC5C@gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

The wiki is actually maintained separately in =
https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage. We =
restricted editing of the wiki because bots would automatically add =
stuff. I=92ve given you permissions now.

Matei

On Apr 21, 2014, at 6:22 PM, Nan Zhu <zhunanmcgill@gmail.com> wrote:

> I thought those are files of spark.apache.org?=20
>=20
> --=20
> Nan Zhu
>=20
>=20
> On Monday, April 21, 2014 at 9:09 PM, Xiangrui Meng wrote:
>=20
>> The markdown files are under spark/docs. You can submit a PR for
>> changes. -Xiangrui
>>=20
>> On Mon, Apr 21, 2014 at 6:01 PM, Sandy Ryza <sandy.ryza@cloudera.com =
(mailto:sandy.ryza@cloudera.com)> wrote:
>>> How do I get permissions to edit the wiki?
>>>=20
>>>=20
>>> On Mon, Apr 21, 2014 at 3:19 PM, Xiangrui Meng <mengxr@gmail.com =
(mailto:mengxr@gmail.com)> wrote:
>>>=20
>>>> Cannot agree more with your words. Could you add one section about
>>>> "how and what to contribute" to MLlib's guide? -Xiangrui
>>>>=20
>>>> On Mon, Apr 21, 2014 at 1:41 PM, Nick Pentreath
>>>> <nick.pentreath@gmail.com (mailto:nick.pentreath@gmail.com)> wrote:
>>>>> I'd say a section in the "how to contribute" page would be a good =
place
>>>>=20
>>>> to put this.
>>>>>=20
>>>>> In general I'd say that the criteria for inclusion of an algorithm =
is it
>>>> should be high quality, widely known, used and accepted (citations =
and
>>>> concrete use cases as examples of this), scalable and =
parallelizable, well
>>>> documented and with reasonable expectation of dev support
>>>>>=20
>>>>> Sent from my iPhone
>>>>>=20
>>>>>> On 21 Apr 2014, at 19:59, Sandy Ryza <sandy.ryza@cloudera.com =
(mailto:sandy.ryza@cloudera.com)> wrote:
>>>>>>=20
>>>>>> If it's not done already, would it make sense to codify this =
philosophy
>>>>>> somewhere? I imagine this won't be the first time this discussion =
comes
>>>>>> up, and it would be nice to have a doc to point to. I'd be happy =
to
>>>>>>=20
>>>>>=20
>>>>>=20
>>>>=20
>>>> take a
>>>>>> stab at this.
>>>>>>=20
>>>>>>=20
>>>>>>> On Mon, Apr 21, 2014 at 10:54 AM, Xiangrui Meng =
<mengxr@gmail.com (mailto:mengxr@gmail.com)>
>>>> wrote:
>>>>>>>=20
>>>>>>> +1 on Sean's comment. MLlib covers the basic algorithms but we
>>>>>>> definitely need to spend more time on how to make the design =
scalable.
>>>>>>> For example, think about current "ProblemWithAlgorithm" naming =
scheme.
>>>>>>> That being said, new algorithms are welcomed. I wish they are
>>>>>>> well-established and well-understood by users. They shouldn't be
>>>>>>> research algorithms tuned to work well with a particular dataset =
but
>>>>>>> not tested widely. You see the change log from Mahout:
>>>>>>>=20
>>>>>>> =3D=3D=3D
>>>>>>> The following algorithms that were marked deprecated in 0.8 have =
been
>>>>>>> removed in 0.9:
>>>>>>>=20
>>>>>>> =46rom Clustering:
>>>>>>> Switched LDA implementation from using Gibbs Sampling to =
Collapsed
>>>>>>> Variational Bayes (CVB)
>>>>>>> Meanshift
>>>>>>> MinHash - removed due to poor performance, lack of support and =
lack of
>>>>>>> usage
>>>>>>>=20
>>>>>>> =46rom Classification (both are sequential implementations)
>>>>>>> Winnow - lack of actual usage and support
>>>>>>> Perceptron - lack of actual usage and support
>>>>>>>=20
>>>>>>> Collaborative Filtering
>>>>>>> SlopeOne implementations in
>>>>>>> org.apache.mahout.cf.taste.hadoop.slopeone and
>>>>>>> org.apache.mahout.cf.taste.impl.recommender.slopeone
>>>>>>> Distributed pseudo recommender in
>>>>>>> org.apache.mahout.cf.taste.hadoop.pseudo
>>>>>>> TreeClusteringRecommender in
>>>>>>> org.apache.mahout.cf.taste.impl.recommender
>>>>>>>=20
>>>>>>> Mahout Math
>>>>>>> Hadoop entropy stuff in org.apache.mahout.math.stats.entropy
>>>>>>> =3D=3D=3D
>>>>>>>=20
>>>>>>> In MLlib, we should include the algorithms users know how to use =
and
>>>>>>> we can provide support rather than letting algorithms come and =
go.
>>>>>>>=20
>>>>>>> My $0.02,
>>>>>>> Xiangrui
>>>>>>>=20
>>>>>>>> On Mon, Apr 21, 2014 at 10:23 AM, Sean Owen <sowen@cloudera.com =
(mailto:sowen@cloudera.com)>
>>>> wrote:
>>>>>>>>> On Mon, Apr 21, 2014 at 6:03 PM, Paul Brown =
<prb@mult.ifario.us (mailto:prb@mult.ifario.us)>
>>>>>>>>=20
>>>>>>>=20
>>>>>>=20
>>>>>=20
>>>>=20
>>>> wrote:
>>>>>>>>> - MLlib as Mahout.next would be a unfortunate. There are some =
gems
>>>>>>>>=20
>>>>>>>=20
>>>>>>=20
>>>>>=20
>>>>=20
>>>> in
>>>>>>>>> Mahout, but there are also lots of rocks. Setting a minimal =
bar of
>>>>>>>>> working, correctly implemented, and documented requires a =
surprising
>>>>>>>>>=20
>>>>>>>>=20
>>>>>>>=20
>>>>>>> amount
>>>>>>>>> of work.
>>>>>>>>=20
>>>>>>>>=20
>>>>>>>> As someone with first-hand knowledge, this is correct. To =
Sang's
>>>>>>>> question, I can't see value in 'porting' Mahout since it is =
based on a
>>>>>>>> quite different paradigm. About the only part that translates =
is the
>>>>>>>> algorithm concept itself.
>>>>>>>>=20
>>>>>>>> This is also the cautionary tale. The contents of the project =
have
>>>>>>>> ended up being a number of "drive-by" contributions of =
implementations
>>>>>>>> that, while individually perhaps brilliant (perhaps), didn't
>>>>>>>> necessarily match any other implementation in structure, =
input/output,
>>>>>>>> libraries used. The implementations were often a touch =
academic. The
>>>>>>>> result was hard to document, maintain, evolve or use.
>>>>>>>>=20
>>>>>>>> Far more of the structure of the MLlib implementations are =
consistent
>>>>>>>> by virtue of being built around Spark core already. That's =
great.
>>>>>>>>=20
>>>>>>>> One can't wait to completely build the foundation before =
building any
>>>>>>>> implementations. To me, the existing implementations are almost
>>>>>>>> exactly the basics I would choose. They cover the bases and =
will
>>>>>>>> exercise the abstractions and structure. So that's also great =
IMHO.
>>>>>>>>=20
>>>>>>>=20
>>>>>>>=20
>>>>>>=20
>>>>>=20
>>>>=20
>>>>=20
>>>=20
>>>=20
>>=20
>>=20
>>=20
>=20
>=20


From dev-return-7386-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 22 06:00:16 2014
Return-Path: <dev-return-7386-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BFE6611FE1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Apr 2014 06:00:16 +0000 (UTC)
Received: (qmail 57300 invoked by uid 500); 22 Apr 2014 06:00:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 56970 invoked by uid 500); 22 Apr 2014 06:00:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56961 invoked by uid 99); 22 Apr 2014 06:00:12 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Apr 2014 06:00:12 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of prabsmails@gmail.com designates 209.85.215.50 as permitted sender)
Received: from [209.85.215.50] (HELO mail-la0-f50.google.com) (209.85.215.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Apr 2014 06:00:02 +0000
Received: by mail-la0-f50.google.com with SMTP id pv20so3992522lab.9
        for <dev@spark.apache.org>; Mon, 21 Apr 2014 22:59:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=DviKjPhC42GEF415aVYOZHlyTnyCvoVY9dUpboV9hDY=;
        b=uTcfI676meODXcud0cgp68K1trFOaefhrWbkrFIuvkR4jFM7WBnilMbIjITTCX2jzk
         DIKZmOKP3tEclda1XQ9N60fcd4XSR+4a9gxSAGRb3olOdebnklSWTcMX4vcp94yLJ5uD
         r+YLVzZnfEvcaauxRByzOdyo0rUaMPWemfCNeQe7nNSHGbLma61GyxjbZxgtaDSPRu2C
         Og+0X/Jt6PbVbZKgUd8KkhOJ0jpRAKG84ZEqukOYqNJHmt4HST6fNF6eGiw77Pqbz18n
         DgFdciHUdZ2pT8AdSyWZiQXZJjTn0VBuoGpqFCyTNX74owwFr+YJlI6jIokyF9lDGovo
         9ySg==
X-Received: by 10.152.43.70 with SMTP id u6mr29731097lal.3.1398146380966; Mon,
 21 Apr 2014 22:59:40 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.152.207.12 with HTTP; Mon, 21 Apr 2014 22:59:00 -0700 (PDT)
From: prabeesh k <prabsmails@gmail.com>
Date: Tue, 22 Apr 2014 11:29:00 +0530
Message-ID: <CAPdPcW0rxVuBwAJj4-PHSvZN0uVEbm5K23b5nYojeFF7yvzwnA@mail.gmail.com>
Subject: Link not working
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c352a461b0cf04f79b5297
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c352a461b0cf04f79b5297
Content-Type: text/plain; charset=UTF-8

For Spark-0.8.0, the download links are not working.

Please update the same

Regarding,
prabeesh

--001a11c352a461b0cf04f79b5297--

From dev-return-7387-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 22 13:59:48 2014
Return-Path: <dev-return-7387-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AF20211227
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Apr 2014 13:59:48 +0000 (UTC)
Received: (qmail 53572 invoked by uid 500); 22 Apr 2014 13:59:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53520 invoked by uid 500); 22 Apr 2014 13:59:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53512 invoked by uid 99); 22 Apr 2014 13:59:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Apr 2014 13:59:46 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of liqingyang1985@gmail.com designates 74.125.82.50 as permitted sender)
Received: from [74.125.82.50] (HELO mail-wg0-f50.google.com) (74.125.82.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Apr 2014 13:59:42 +0000
Received: by mail-wg0-f50.google.com with SMTP id x13so4042243wgg.9
        for <dev@spark.apache.org>; Tue, 22 Apr 2014 06:59:20 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=el/7K/iD0ONt5NDOSRC32BZ9v7FM129S97uBS6FA0N0=;
        b=epohlTUWXZ223E2Nf53GUbnUJ092yap6XVWV8swTPUQwrHe9iHaMkej6qBrO4em6Wf
         zpVsdjMOvLhfQtLFkQbLVSsc5iZFf/55N+QXxM50GHegleKQOQAhU1ieYx+fYAMW2vFJ
         oVh4C8iw3R0fX1Ab4U8ELg6+3c07D9Y7iX1nWEl9GTxAEIkeD6Ru7Lz51mvFafn0rW6T
         8vQORih6JVdyUdELNs8ZYHVWC/ScHRbY2aTrIs8Wr06wNkOSVsdDoVKBa+AdHqlizOsE
         sPuIDLFSlnsiIa8fjqtLUMJeJ/kGzFV6xJF7zWU2Fiz/UeyheNHb4VLvF0m5g7KIE7hl
         ks4g==
MIME-Version: 1.0
X-Received: by 10.181.12.103 with SMTP id ep7mr19026061wid.43.1398175159949;
 Tue, 22 Apr 2014 06:59:19 -0700 (PDT)
Received: by 10.194.61.39 with HTTP; Tue, 22 Apr 2014 06:59:19 -0700 (PDT)
In-Reply-To: <CAOwyor0CAmEbg5pjqNJGZmsC9x0VKWssBLmR3=vajZ5ROLTnbQ@mail.gmail.com>
References: <CABDsqqZx+o7=Nwn3H_vU-dEcavs_OLDU5XXnTAnsEof9cX6emw@mail.gmail.com>
	<CAOwyor0CAmEbg5pjqNJGZmsC9x0VKWssBLmR3=vajZ5ROLTnbQ@mail.gmail.com>
Date: Tue, 22 Apr 2014 21:59:19 +0800
Message-ID: <CABDsqqbFpr1aGJV=YOMX6C0d5Ww2MQ0ary8h9=74w904xEpnnw@mail.gmail.com>
Subject: Re: does shark0.9.1 work well with hadoop2.2.0 ?
From: qingyang li <liqingyang1985@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d0438eddbbe1be104f7a20585
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0438eddbbe1be104f7a20585
Content-Type: text/plain; charset=UTF-8

this can help resolve protobuf version problem, too.
https://groups.google.com/forum/#!msg/shark-users/0pGIVQvaYfo/-43oaK8scNAJ


2014-04-20 23:53 GMT+08:00 Gordon Wang <gwang@gopivotal.com>:

> replacing the jar is not enough.
> You have to change protobuf dependency in shark's build script. and
> recompile the source.
>
> Protobuf 2.4.1 and 2.5.0 is not binary compatible.
>
>
> On Sun, Apr 20, 2014 at 6:45 PM, qingyang li <liqingyang1985@gmail.com
> >wrote:
>
> > shark 0.9.1 is using protobuf 2.4.1 , but hadoop2.2.0 is using
> > protobuf2.5.0,
> > how can we make them work together?
> > I have tried replace protobuf2.4.1 in shark with protobuf2.5.0, it does
> not
> > work.
> > I have also tried replacing protobuf2.5.0 in hadoop with shark's 2.4.1,
> it
> > does not work too.
> >
>
>
>
> --
> Regards
> Gordon Wang
>

--f46d0438eddbbe1be104f7a20585--

From dev-return-7388-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 22 17:12:24 2014
Return-Path: <dev-return-7388-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 787C811D4E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Apr 2014 17:12:24 +0000 (UTC)
Received: (qmail 46636 invoked by uid 500); 22 Apr 2014 17:12:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46592 invoked by uid 500); 22 Apr 2014 17:12:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46583 invoked by uid 99); 22 Apr 2014 17:12:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Apr 2014 17:12:22 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.172 as permitted sender)
Received: from [209.85.216.172] (HELO mail-qc0-f172.google.com) (209.85.216.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Apr 2014 17:12:16 +0000
Received: by mail-qc0-f172.google.com with SMTP id i8so5764254qcq.3
        for <dev@spark.apache.org>; Tue, 22 Apr 2014 10:11:53 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=tqQbsYoMzqKZF2peEvTH1q+PrurDs1+Gbx50Fj0dvf0=;
        b=Xgh167myOQH0s/WuQvxcTqVIJR7YW2aXp92NocFTsdaYJVjf0rv03nIYF9BraZisLy
         QtUP4PqHxfJk+HBOCVzvgEdA+6D3+SHLiVDwpk92zh/ykYXIPJ0GVMBaBxvUNTNeucSi
         xKSnZJ5hkRdF4BcL6CCHazpHvcOoUR9KMOXqeWJZkTLN6+t9qygdEsNz1Tv0r68ymG0U
         ZdAXnMZdc6IrfNTNMiKXa2SHC1TGG5kDQOUAC+OLDnLKRU3aKKT9k87KJTdQeDBA3UUn
         OtT6rEFliIM/rSoLBfK8akIe83ediW1ZRShhRO81iXO37wJkuJ3IJjwTjnV7mL3z1ADW
         LhbA==
X-Gm-Message-State: ALoCoQmO52EjQBumsO1rTV1AQKr6hOdso5aiSu69FXuSbRPd7WUmHbNgjLDp8xMDLDeKGHheJBsK
MIME-Version: 1.0
X-Received: by 10.229.176.72 with SMTP id bd8mr49386063qcb.12.1398186713384;
 Tue, 22 Apr 2014 10:11:53 -0700 (PDT)
Received: by 10.140.94.2 with HTTP; Tue, 22 Apr 2014 10:11:53 -0700 (PDT)
In-Reply-To: <6C66459B-CAC2-45C5-A8EA-23ACB711F2DC@gmail.com>
References: <CADSNrJHOT035G1xda8p9eZz-90uiWOb4CwgiY1r-d8m9V9LM+g@mail.gmail.com>
	<CAMAsSdJ5em5SgjszP-NiA58u_NR_tgT2PC6Wwxi4W8VX0cvVgA@mail.gmail.com>
	<CACArsZ8w=mRKMwPyG9+9Kyyn-kVtOj1E2ujC6p_Uv_X-86m1Tg@mail.gmail.com>
	<CAMAsSdJX=2UiV2CmJWwNX4Uiud8JuvcFoCkbnOrWJBouJMMjkg@mail.gmail.com>
	<CAJgQjQ8KzOUGH-ChXy5QiLuK_WEmj5FS31ucHtRftZd5Aj46eA@mail.gmail.com>
	<CACBYxK+-NCgGHKSNt=T+TCCqofqA+-HQzC00FB2qJdD-c1hNGw@mail.gmail.com>
	<A1C0F7CE-1A54-4486-89EA-63EA9DAB65C9@googlemail.com>
	<CAJgQjQ9wmVtwCtW21sxQ+OCsN3v__1MrTVsFKuJ8Q_WX-018ig@mail.gmail.com>
	<CACBYxKJBeZ2Hu=nupm0u3ZPqWRu=4KGmppQ6OVqNBnqzP4HR+A@mail.gmail.com>
	<CAJgQjQ_oK0Q65uLBtGfzq6qW3rWrXvEojUcPj-4NOrO7cuUn6g@mail.gmail.com>
	<DAD72F07091F4CD7A99CDD4E3222EC5C@gmail.com>
	<6C66459B-CAC2-45C5-A8EA-23ACB711F2DC@gmail.com>
Date: Tue, 22 Apr 2014 10:11:53 -0700
Message-ID: <CACBYxK+yBV=YrGta0DJSuuN3+4o9QqvXMP=jap+pWv1hXrrshw@mail.gmail.com>
Subject: Re: Any plans for new clustering algorithms?
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c303ec61d8e604f7a4b60f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c303ec61d8e604f7a4b60f
Content-Type: text/plain; charset=ISO-8859-1

Thanks Matei.  I added a section "How to contribute" page.


On Mon, Apr 21, 2014 at 7:25 PM, Matei Zaharia <matei.zaharia@gmail.com>wrote:

> The wiki is actually maintained separately in
> https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage. We
> restricted editing of the wiki because bots would automatically add stuff.
> I've given you permissions now.
>
> Matei
>
> On Apr 21, 2014, at 6:22 PM, Nan Zhu <zhunanmcgill@gmail.com> wrote:
>
> > I thought those are files of spark.apache.org?
> >
> > --
> > Nan Zhu
> >
> >
> > On Monday, April 21, 2014 at 9:09 PM, Xiangrui Meng wrote:
> >
> >> The markdown files are under spark/docs. You can submit a PR for
> >> changes. -Xiangrui
> >>
> >> On Mon, Apr 21, 2014 at 6:01 PM, Sandy Ryza <sandy.ryza@cloudera.com(mailto:
> sandy.ryza@cloudera.com)> wrote:
> >>> How do I get permissions to edit the wiki?
> >>>
> >>>
> >>> On Mon, Apr 21, 2014 at 3:19 PM, Xiangrui Meng <mengxr@gmail.com(mailto:
> mengxr@gmail.com)> wrote:
> >>>
> >>>> Cannot agree more with your words. Could you add one section about
> >>>> "how and what to contribute" to MLlib's guide? -Xiangrui
> >>>>
> >>>> On Mon, Apr 21, 2014 at 1:41 PM, Nick Pentreath
> >>>> <nick.pentreath@gmail.com (mailto:nick.pentreath@gmail.com)> wrote:
> >>>>> I'd say a section in the "how to contribute" page would be a good
> place
> >>>>
> >>>> to put this.
> >>>>>
> >>>>> In general I'd say that the criteria for inclusion of an algorithm
> is it
> >>>> should be high quality, widely known, used and accepted (citations and
> >>>> concrete use cases as examples of this), scalable and parallelizable,
> well
> >>>> documented and with reasonable expectation of dev support
> >>>>>
> >>>>> Sent from my iPhone
> >>>>>
> >>>>>> On 21 Apr 2014, at 19:59, Sandy Ryza <sandy.ryza@cloudera.com(mailto:
> sandy.ryza@cloudera.com)> wrote:
> >>>>>>
> >>>>>> If it's not done already, would it make sense to codify this
> philosophy
> >>>>>> somewhere? I imagine this won't be the first time this discussion
> comes
> >>>>>> up, and it would be nice to have a doc to point to. I'd be happy to
> >>>>>>
> >>>>>
> >>>>>
> >>>>
> >>>> take a
> >>>>>> stab at this.
> >>>>>>
> >>>>>>
> >>>>>>> On Mon, Apr 21, 2014 at 10:54 AM, Xiangrui Meng <mengxr@gmail.com(mailto:
> mengxr@gmail.com)>
> >>>> wrote:
> >>>>>>>
> >>>>>>> +1 on Sean's comment. MLlib covers the basic algorithms but we
> >>>>>>> definitely need to spend more time on how to make the design
> scalable.
> >>>>>>> For example, think about current "ProblemWithAlgorithm" naming
> scheme.
> >>>>>>> That being said, new algorithms are welcomed. I wish they are
> >>>>>>> well-established and well-understood by users. They shouldn't be
> >>>>>>> research algorithms tuned to work well with a particular dataset
> but
> >>>>>>> not tested widely. You see the change log from Mahout:
> >>>>>>>
> >>>>>>> ===
> >>>>>>> The following algorithms that were marked deprecated in 0.8 have
> been
> >>>>>>> removed in 0.9:
> >>>>>>>
> >>>>>>> From Clustering:
> >>>>>>> Switched LDA implementation from using Gibbs Sampling to Collapsed
> >>>>>>> Variational Bayes (CVB)
> >>>>>>> Meanshift
> >>>>>>> MinHash - removed due to poor performance, lack of support and
> lack of
> >>>>>>> usage
> >>>>>>>
> >>>>>>> From Classification (both are sequential implementations)
> >>>>>>> Winnow - lack of actual usage and support
> >>>>>>> Perceptron - lack of actual usage and support
> >>>>>>>
> >>>>>>> Collaborative Filtering
> >>>>>>> SlopeOne implementations in
> >>>>>>> org.apache.mahout.cf.taste.hadoop.slopeone and
> >>>>>>> org.apache.mahout.cf.taste.impl.recommender.slopeone
> >>>>>>> Distributed pseudo recommender in
> >>>>>>> org.apache.mahout.cf.taste.hadoop.pseudo
> >>>>>>> TreeClusteringRecommender in
> >>>>>>> org.apache.mahout.cf.taste.impl.recommender
> >>>>>>>
> >>>>>>> Mahout Math
> >>>>>>> Hadoop entropy stuff in org.apache.mahout.math.stats.entropy
> >>>>>>> ===
> >>>>>>>
> >>>>>>> In MLlib, we should include the algorithms users know how to use
> and
> >>>>>>> we can provide support rather than letting algorithms come and go.
> >>>>>>>
> >>>>>>> My $0.02,
> >>>>>>> Xiangrui
> >>>>>>>
> >>>>>>>> On Mon, Apr 21, 2014 at 10:23 AM, Sean Owen <sowen@cloudera.com(mailto:
> sowen@cloudera.com)>
> >>>> wrote:
> >>>>>>>>> On Mon, Apr 21, 2014 at 6:03 PM, Paul Brown <prb@mult.ifario.us(mailto:
> prb@mult.ifario.us)>
> >>>>>>>>
> >>>>>>>
> >>>>>>
> >>>>>
> >>>>
> >>>> wrote:
> >>>>>>>>> - MLlib as Mahout.next would be a unfortunate. There are some
> gems
> >>>>>>>>
> >>>>>>>
> >>>>>>
> >>>>>
> >>>>
> >>>> in
> >>>>>>>>> Mahout, but there are also lots of rocks. Setting a minimal bar
> of
> >>>>>>>>> working, correctly implemented, and documented requires a
> surprising
> >>>>>>>>>
> >>>>>>>>
> >>>>>>>
> >>>>>>> amount
> >>>>>>>>> of work.
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> As someone with first-hand knowledge, this is correct. To Sang's
> >>>>>>>> question, I can't see value in 'porting' Mahout since it is based
> on a
> >>>>>>>> quite different paradigm. About the only part that translates is
> the
> >>>>>>>> algorithm concept itself.
> >>>>>>>>
> >>>>>>>> This is also the cautionary tale. The contents of the project have
> >>>>>>>> ended up being a number of "drive-by" contributions of
> implementations
> >>>>>>>> that, while individually perhaps brilliant (perhaps), didn't
> >>>>>>>> necessarily match any other implementation in structure,
> input/output,
> >>>>>>>> libraries used. The implementations were often a touch academic.
> The
> >>>>>>>> result was hard to document, maintain, evolve or use.
> >>>>>>>>
> >>>>>>>> Far more of the structure of the MLlib implementations are
> consistent
> >>>>>>>> by virtue of being built around Spark core already. That's great.
> >>>>>>>>
> >>>>>>>> One can't wait to completely build the foundation before building
> any
> >>>>>>>> implementations. To me, the existing implementations are almost
> >>>>>>>> exactly the basics I would choose. They cover the bases and will
> >>>>>>>> exercise the abstractions and structure. So that's also great
> IMHO.
> >>>>>>>>
> >>>>>>>
> >>>>>>>
> >>>>>>
> >>>>>
> >>>>
> >>>>
> >>>
> >>>
> >>
> >>
> >>
> >
> >
>
>

--001a11c303ec61d8e604f7a4b60f--

From dev-return-7389-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 23 05:04:01 2014
Return-Path: <dev-return-7389-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5A1E5105D4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 23 Apr 2014 05:04:01 +0000 (UTC)
Received: (qmail 11745 invoked by uid 500); 23 Apr 2014 05:04:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11521 invoked by uid 500); 23 Apr 2014 05:03:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11512 invoked by uid 99); 23 Apr 2014 05:03:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 05:03:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of prodigyaj@gmail.com designates 209.85.220.51 as permitted sender)
Received: from [209.85.220.51] (HELO mail-pa0-f51.google.com) (209.85.220.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 05:03:54 +0000
Received: by mail-pa0-f51.google.com with SMTP id kl14so373544pab.24
        for <dev@spark.apache.org>; Tue, 22 Apr 2014 22:03:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=5hH5PAaqowqdQknsP7+0/gHnXUIERwR7tdhUnl60EYw=;
        b=dTK3oR/9LIe6HcfNGv0a/WY4Y4rDGdnXrW3a/2EVJM3yDGYG4ICPTtCr9WPH+GAPd3
         RToMAXnZQiv0x/N82zjuegAzDy4WTyCyHO/5D2bkqF/dGt7MhxPIHOuPaAzeSsdd614y
         qz5rWLDETTbRkCHvCHbjd0fPXMEaQHvQVD3IOojQ3sbV5ooxdsxly4U1jGahnBUicbqf
         FiO2m0+7apbC4Hdj97RuU1TnLWnbFK6bv0YlaENG/q/eIl/GHzl8GnJ0RRM0cZiGFcIM
         hanDcAwozK1ZJsFoAviZPg06PA1eMU8aC1SdyQ4StAMhVDzZqE08Tk3wBtoue2FMgHyO
         17XA==
MIME-Version: 1.0
X-Received: by 10.68.231.196 with SMTP id ti4mr51215198pbc.48.1398229411218;
 Tue, 22 Apr 2014 22:03:31 -0700 (PDT)
Received: by 10.70.94.97 with HTTP; Tue, 22 Apr 2014 22:03:31 -0700 (PDT)
Date: Wed, 23 Apr 2014 00:03:31 -0500
Message-ID: <CAHVi7zJg-P959-WL8n2CzMe0Mpbu2c4FWS2uxqKwJ=1U5TKAFA@mail.gmail.com>
Subject: Spark on wikipedia dataset
From: Ajay Nair <prodigyaj@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b339d435ecb5504f7aea7ce
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b339d435ecb5504f7aea7ce
Content-Type: text/plain; charset=UTF-8

I am going to perform some test experiments on the wikipedia dataset using
the spark framework. I know wikipedia data set might already have been
analyzed, but what are the potential explored/unexplored aspects of spark
that can be tested and benchmarked on wikipedia dataset?

Thanks
AJ

--047d7b339d435ecb5504f7aea7ce--

From dev-return-7390-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 23 05:37:50 2014
Return-Path: <dev-return-7390-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AF8E510680
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 23 Apr 2014 05:37:50 +0000 (UTC)
Received: (qmail 46724 invoked by uid 500); 23 Apr 2014 05:37:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46684 invoked by uid 500); 23 Apr 2014 05:37:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46676 invoked by uid 99); 23 Apr 2014 05:37:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 05:37:48 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of andykonwinski@gmail.com designates 209.85.215.42 as permitted sender)
Received: from [209.85.215.42] (HELO mail-la0-f42.google.com) (209.85.215.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 05:37:43 +0000
Received: by mail-la0-f42.google.com with SMTP id el20so363611lab.1
        for <dev@spark.apache.org>; Tue, 22 Apr 2014 22:37:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=4aEDGKA+bKx/lp0EmUUzhYDDM7zS0oG0G3lX2RsKApo=;
        b=Z+TKcaVBFcvNYMXYalRBjLwrKLhsF0pulIE94BtQazv848C2BAcd84AyAqDuH0ab77
         0cYCkHCFXl59lxH4GE1h1wGluLKpxdJIeIwVJbkZUSmtGMNDlT+Pk7uBdZfX1p4qLoYU
         JLwXaB9Zp1L2kg5OgWqyP2aJwlV8xxb42pNLtlC3/w1bU8jvmGIyhQuI0YZ8k8KeOaLy
         tt+8L+MXu7XhCQZ2D8LWs2oK69wovjB9nDVqbtupEcZf/YjzkTCKlcN9W9X/iY2jYAF0
         sb3Qvi8PhQq9vD5ythvMjmzqOCNghg2v1Eqy6+V0/ZNKPPjSPUvK4j7DxDn7eUHeW13a
         QpRA==
MIME-Version: 1.0
X-Received: by 10.152.235.3 with SMTP id ui3mr34348988lac.2.1398231441290;
 Tue, 22 Apr 2014 22:37:21 -0700 (PDT)
Received: by 10.113.3.163 with HTTP; Tue, 22 Apr 2014 22:37:21 -0700 (PDT)
In-Reply-To: <CAPdPcW0rxVuBwAJj4-PHSvZN0uVEbm5K23b5nYojeFF7yvzwnA@mail.gmail.com>
References: <CAPdPcW0rxVuBwAJj4-PHSvZN0uVEbm5K23b5nYojeFF7yvzwnA@mail.gmail.com>
Date: Tue, 22 Apr 2014 22:37:21 -0700
Message-ID: <CALEZFQw56wTs6poTTEfmJ0g9Ee_zJ4BCDQT-ko07mcTYx=eqMg@mail.gmail.com>
Subject: Re: Link not working
From: Andy Konwinski <andykonwinski@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134630a5f3b5b04f7af201d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134630a5f3b5b04f7af201d
Content-Type: text/plain; charset=UTF-8

Should be fixed now, thanks for reporting this!

Andy


On Mon, Apr 21, 2014 at 10:59 PM, prabeesh k <prabsmails@gmail.com> wrote:

> For Spark-0.8.0, the download links are not working.
>
> Please update the same
>
> Regarding,
> prabeesh
>

--001a1134630a5f3b5b04f7af201d--

From dev-return-7391-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 23 06:33:21 2014
Return-Path: <dev-return-7391-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EA1CE1083F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 23 Apr 2014 06:33:20 +0000 (UTC)
Received: (qmail 25943 invoked by uid 500); 23 Apr 2014 06:33:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25901 invoked by uid 500); 23 Apr 2014 06:33:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 18784 invoked by uid 99); 23 Apr 2014 06:26:27 -0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:sender:date:message-id
         :subject:from:to:content-type;
        bh=Tw9ml3QECoSfPI6/Vg5wuQPxLOEyuRccbAIwcKILQSM=;
        b=cFz3mM5Pu9pT2hnHSzSuwfy21m0m6SdtOb7A6He70oIW7hAnt+ZA1O7dWnOZqvrkNZ
         otDqaw9jNsOb3cLg3FDzPkK53vl1YuOI1MpSGyVE8YxYPpZJy3Q0li60QMWVwaYSazKy
         aW6CdpdSk16ykMWFG2v/F930I4EH0RoFDJ2Wv7HWWkId/T6I9QcEapIZQIrVsE3mQRHq
         w6QL+vzYONMa1PVMrCeeM+t0P9sa44lh/EelzDc9HkD8OvFEnPNb3aWgnydhVjS3IWP8
         uMS8E4TUWSHtYVzFkZRSjtCrmMj20rTT7Z5lpYfRf9gptW7o62C6J5Nhq9LwyLZH+n1m
         lGug==
X-Gm-Message-State: ALoCoQnjuv9/pwYE2PUeVPpw/NoCQty1JnsVD7Jasn1J4M4cEAYZlW2rrIhvgciDCW29boa2BpRe
MIME-Version: 1.0
X-Received: by 10.229.58.68 with SMTP id f4mr53660426qch.18.1398234361228;
 Tue, 22 Apr 2014 23:26:01 -0700 (PDT)
Reply-To: dbtsai@dbtsai.com
Sender: dbtsai@dbtsai.org
Date: Tue, 22 Apr 2014 23:26:01 -0700
X-Google-Sender-Auth: G-7flqc2z8RcnhI858X8pyEGShA
Message-ID: <CAEYYnxYq7fi0AWx3HJWFKADL_JMfRqbbRPWBuRj_uWZT5q6W-Q@mail.gmail.com>
Subject: Jekyll documentation generation error
From: DB Tsai <dbtsai@dbtsai.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113306b06a04df04f7afce08
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113306b06a04df04f7afce08
Content-Type: text/plain; charset=UTF-8

Hi guys,

I'm trying to update LBFGS documentation so I need to generate html
document to see if everything looks great. However, mv I get the following
error.

Conversion error: There was an error converting 'docs/cluster-overview.md'.
error: MaRuKu encountered problem(s) while converting your markup.. Use
--trace to view backtrace


I'm using ruby 2.1.1p76 and jekyll 1.5.1 installed by gem as instruction.
Do I miss anything?

Also, in the instruction, if we want to skip the api docs, we can do the
following.

# Skip generating API docs (which takes a while)
$ SKIP_SCALADOC=1 jekyll build

But what does"SKIP_SCALADOC=1" mean? export SKIP_SCALADOC=1?

Thanks.




Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai

--001a113306b06a04df04f7afce08--

From dev-return-7392-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 23 06:33:41 2014
Return-Path: <dev-return-7392-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C411910843
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 23 Apr 2014 06:33:41 +0000 (UTC)
Received: (qmail 27003 invoked by uid 500); 23 Apr 2014 06:33:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26963 invoked by uid 500); 23 Apr 2014 06:33:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 24686 invoked by uid 99); 23 Apr 2014 06:32:22 -0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:sender:in-reply-to
         :references:date:message-id:subject:from:to:content-type;
        bh=sLn4nQVwqUGtat8PFun3RFH8COyyDSz3VetpH4H35b4=;
        b=YwQdD7H0IgZWWBlxQV2Bs9B9R6j1NH//qF1jPgiYB1G7AHCzrx96AfIWEDL8xOR9rl
         pSDRBAE7N7CTBAT8DZJ8+FrKPFv4/ISI1yNDFD1aQVNlKp8stU4BGvYeQgCLGwRPVxpE
         JNjPEyJCWBVSUlLJ9KD9X2CttWOo5XZ2v1RULCpj0ajGyL9F6X0GW37wEJeDLRVObbjo
         6Hbim1gIKYvupnBTRBOJz2zfIHVU+MdSzsI9yy/xy9t1I+yFszcOqd0iqjFvCLw9LfZ3
         42f2nIF+P4PdgLqFz0MS8JSj/2NyCBCO8wWgWk1dkmIiTCbfymIEe1y7a6f1PBBOf2yt
         GE1g==
X-Gm-Message-State: ALoCoQmoXwNKKjcUMgOrJofgeUE1b6NWTIP59wf/UqeZZguGP68Ci7e/ElRmWW2ymE5sgJJa59bF
MIME-Version: 1.0
X-Received: by 10.140.105.118 with SMTP id b109mr58480937qgf.28.1398234716311;
 Tue, 22 Apr 2014 23:31:56 -0700 (PDT)
Reply-To: dbtsai@dbtsai.com
Sender: dbtsai@dbtsai.org
In-Reply-To: <CAEYYnxYq7fi0AWx3HJWFKADL_JMfRqbbRPWBuRj_uWZT5q6W-Q@mail.gmail.com>
References: <CAEYYnxYq7fi0AWx3HJWFKADL_JMfRqbbRPWBuRj_uWZT5q6W-Q@mail.gmail.com>
Date: Tue, 22 Apr 2014 23:31:56 -0700
X-Google-Sender-Auth: App8cJlXH78hKZvMPfDQVkB_3MY
Message-ID: <CAEYYnxbhUVbvSupEQAY4RFVnSAD59rvNTm=f_Fwpq3K+eMLbog@mail.gmail.com>
Subject: Re: Jekyll documentation generation error
From: DB Tsai <dbtsai@dbtsai.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1137d4a494274604f7afe339
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1137d4a494274604f7afe339
Content-Type: text/plain; charset=UTF-8

This is the trace.

  Conversion error: There was an error converting 'docs/cluster-overview.md
'.

/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/converters/markdown/maruku_parser.rb:45:in
`print_errors_and_fail': MaRuKu encountered problem(s) while converting
your markup. (MaRuKu::Exception)

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/converters/markdown/maruku_parser.rb:50:in
`convert'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/converters/markdown.rb:39:in
`convert'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/convertible.rb:57:in
`transform'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/convertible.rb:153:in
`do_layout'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/page.rb:115:in
`render'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/site.rb:239:in
`block in render'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/site.rb:238:in
`each'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/site.rb:238:in
`render'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/site.rb:39:in
`process'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/command.rb:18:in
`process_site'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/commands/build.rb:23:in
`build'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/lib/jekyll/commands/build.rb:7:in
`process'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/bin/jekyll:77:in
`block (2 levels) in <top (required)>'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1.6/lib/commander/command.rb:180:in
`call'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1.6/lib/commander/command.rb:180:in
`call'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1.6/lib/commander/command.rb:155:in
`run'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1.6/lib/commander/runner.rb:422:in
`run_active_command'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1.6/lib/commander/runner.rb:82:in
`run!'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1.6/lib/commander/delegates.rb:8:in
`run!'

from
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1.6/lib/commander/import.rb:10:in
`block in <top (required)>'


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Tue, Apr 22, 2014 at 11:26 PM, DB Tsai <dbtsai@dbtsai.com> wrote:

> Hi guys,
>
> I'm trying to update LBFGS documentation so I need to generate html
> document to see if everything looks great. However, mv I get the following
> error.
>
> Conversion error: There was an error converting 'docs/cluster-overview.md
> '.
> error: MaRuKu encountered problem(s) while converting your markup.. Use
> --trace to view backtrace
>
>
> I'm using ruby 2.1.1p76 and jekyll 1.5.1 installed by gem as instruction.
> Do I miss anything?
>
> Also, in the instruction, if we want to skip the api docs, we can do the
> following.
>
> # Skip generating API docs (which takes a while)
> $ SKIP_SCALADOC=1 jekyll build
>
> But what does"SKIP_SCALADOC=1" mean? export SKIP_SCALADOC=1?
>
> Thanks.
>
>
>
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>

--001a1137d4a494274604f7afe339--

From dev-return-7393-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 23 06:39:09 2014
Return-Path: <dev-return-7393-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D11641086A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 23 Apr 2014 06:39:09 +0000 (UTC)
Received: (qmail 34039 invoked by uid 500); 23 Apr 2014 06:39:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33898 invoked by uid 500); 23 Apr 2014 06:39:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33890 invoked by uid 99); 23 Apr 2014 06:39:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 06:39:07 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.42 as permitted sender)
Received: from [209.85.160.42] (HELO mail-pb0-f42.google.com) (209.85.160.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 06:39:02 +0000
Received: by mail-pb0-f42.google.com with SMTP id un15so465122pbc.1
        for <dev@spark.apache.org>; Tue, 22 Apr 2014 23:38:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=1GnWa8TsiBhVAECI8Rz/KEE6ORcaGxGEMdnCdwwU2mI=;
        b=lkg4ttE/z/0lXkyyHOEtNfw3Kp8lOHgZgNi5pb/h8ZUUSmjEyAIedpzfhuCjbJPMZH
         7YAY2MFThkaSkPgrFVD6TvOzK62/RUIa8fHrA42JoMoJf+8BcRIhnNPnpDA98GnvedpK
         9JZPdtBaAmIqn2gFHyc+UZwVUHliEOkXyrHeGtYgs4Le1jWrPWY8Ovop6YtYIiqFgQ/s
         d7EP3etTp7Uc+IS7u82ppnJOtlUi/br4U1NYMKtDzgQ4fXoTMl7kJIGxsg3eXw9oW50r
         FFkoN/2v4HfOePWZftBLEudj0Nk01/1FKzLx/jIxj8KWS95ge6dgrz0xN/8zB3EvQmt/
         CvUw==
X-Received: by 10.68.202.194 with SMTP id kk2mr10407101pbc.156.1398235119604;
        Tue, 22 Apr 2014 23:38:39 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id qh2sm122470pab.13.2014.04.22.23.38.37
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 22 Apr 2014 23:38:38 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: Jekyll documentation generation error
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAEYYnxbhUVbvSupEQAY4RFVnSAD59rvNTm=f_Fwpq3K+eMLbog@mail.gmail.com>
Date: Tue, 22 Apr 2014 23:38:34 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <2DE7C3E8-5A8C-4735-B23B-36141E808F0C@gmail.com>
References: <CAEYYnxYq7fi0AWx3HJWFKADL_JMfRqbbRPWBuRj_uWZT5q6W-Q@mail.gmail.com> <CAEYYnxbhUVbvSupEQAY4RFVnSAD59rvNTm=f_Fwpq3K+eMLbog@mail.gmail.com>
To: dev@spark.apache.org,
 dbtsai@dbtsai.com
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

Try doing =93gem install kramdown=94. The maruku gem for Markdown throws =
these errors, but Kramdown doesn=92t.

Matei

On Apr 22, 2014, at 11:31 PM, DB Tsai <dbtsai@dbtsai.com> wrote:

> This is the trace.
>=20
>  Conversion error: There was an error converting =
'docs/cluster-overview.md
> '.
>=20
> =
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/=
lib/jekyll/converters/markdown/maruku_parser.rb:45:in
> `print_errors_and_fail': MaRuKu encountered problem(s) while =
converting
> your markup. (MaRuKu::Exception)
>=20
> from
> =
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/=
lib/jekyll/converters/markdown/maruku_parser.rb:50:in
> `convert'
>=20
> from
> =
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/=
lib/jekyll/converters/markdown.rb:39:in
> `convert'
>=20
> from
> =
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/=
lib/jekyll/convertible.rb:57:in
> `transform'
>=20
> from
> =
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/=
lib/jekyll/convertible.rb:153:in
> `do_layout'
>=20
> from
> =
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/=
lib/jekyll/page.rb:115:in
> `render'
>=20
> from
> =
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/=
lib/jekyll/site.rb:239:in
> `block in render'
>=20
> from
> =
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/=
lib/jekyll/site.rb:238:in
> `each'
>=20
> from
> =
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/=
lib/jekyll/site.rb:238:in
> `render'
>=20
> from
> =
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/=
lib/jekyll/site.rb:39:in
> `process'
>=20
> from
> =
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/=
lib/jekyll/command.rb:18:in
> `process_site'
>=20
> from
> =
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/=
lib/jekyll/commands/build.rb:23:in
> `build'
>=20
> from
> =
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/=
lib/jekyll/commands/build.rb:7:in
> `process'
>=20
> from
> =
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1/=
bin/jekyll:77:in
> `block (2 levels) in <top (required)>'
>=20
> from
> =
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1=
.6/lib/commander/command.rb:180:in
> `call'
>=20
> from
> =
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1=
.6/lib/commander/command.rb:180:in
> `call'
>=20
> from
> =
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1=
.6/lib/commander/command.rb:155:in
> `run'
>=20
> from
> =
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1=
.6/lib/commander/runner.rb:422:in
> `run_active_command'
>=20
> from
> =
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1=
.6/lib/commander/runner.rb:82:in
> `run!'
>=20
> from
> =
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1=
.6/lib/commander/delegates.rb:8:in
> `run!'
>=20
> from
> =
/Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.1=
.6/lib/commander/import.rb:10:in
> `block in <top (required)>'
>=20
>=20
> Sincerely,
>=20
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>=20
>=20
> On Tue, Apr 22, 2014 at 11:26 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
>=20
>> Hi guys,
>>=20
>> I'm trying to update LBFGS documentation so I need to generate html
>> document to see if everything looks great. However, mv I get the =
following
>> error.
>>=20
>> Conversion error: There was an error converting =
'docs/cluster-overview.md
>> '.
>> error: MaRuKu encountered problem(s) while converting your markup.. =
Use
>> --trace to view backtrace
>>=20
>>=20
>> I'm using ruby 2.1.1p76 and jekyll 1.5.1 installed by gem as =
instruction.
>> Do I miss anything?
>>=20
>> Also, in the instruction, if we want to skip the api docs, we can do =
the
>> following.
>>=20
>> # Skip generating API docs (which takes a while)
>> $ SKIP_SCALADOC=3D1 jekyll build
>>=20
>> But what does"SKIP_SCALADOC=3D1" mean? export SKIP_SCALADOC=3D1?
>>=20
>> Thanks.
>>=20
>>=20
>>=20
>>=20
>> Sincerely,
>>=20
>> DB Tsai
>> -------------------------------------------------------
>> My Blog: https://www.dbtsai.com
>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>=20


From dev-return-7394-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 23 06:44:45 2014
Return-Path: <dev-return-7394-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9083A1088B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 23 Apr 2014 06:44:45 +0000 (UTC)
Received: (qmail 40703 invoked by uid 500); 23 Apr 2014 06:44:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40431 invoked by uid 500); 23 Apr 2014 06:44:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40423 invoked by uid 99); 23 Apr 2014 06:44:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 06:44:42 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.217.169] (HELO mail-lb0-f169.google.com) (209.85.217.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 06:44:38 +0000
Received: by mail-lb0-f169.google.com with SMTP id n15so414298lbi.28
        for <dev@spark.apache.org>; Tue, 22 Apr 2014 23:44:15 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:sender:in-reply-to
         :references:date:message-id:subject:from:to:content-type;
        bh=RrDguqgymCOCqre/Nx4Mew9oirRXA1+zTuPHDeO7BkU=;
        b=ldLjpGLZ5uuimhAkl+XWtJyEUMiEJzzPqGzdWXvmbiupPYGA5R90eOnElSP+1bFHh9
         xlDJxkxuRIVIKG14nYyNLNeGsMglN94ZGYVHI8RaCIo2x221OzMUAUaWcppEk5JQbbht
         GSKFGj7PyOdI2WHpP1MIVH2/37FzZkrOwrTbo02q8jzVNVs89In4bbBow2ZhGAeQ5JEg
         HM1OCun5sx7PFmAtG490WBoJBFBtZoBvGTm8skW7nzl8zhTlOZep6HzXzWKK2IE5I6kX
         WN8HbnhllOHwdLaYg3juWK7t27+6QxWqyYJveopHDJDNa3FhY8iFSZs3+5eccLHBKtsj
         5nMw==
X-Gm-Message-State: ALoCoQmUg/AjUgHrS0SpRnOD3yuK0xBsKMdee5IiUmX2bNeCn0edPuZUjSrGV0T2xX+TjAyMHMbp
MIME-Version: 1.0
X-Received: by 10.112.41.101 with SMTP id e5mr323805lbl.46.1398235455045; Tue,
 22 Apr 2014 23:44:15 -0700 (PDT)
Reply-To: dbtsai@dbtsai.com
Sender: dbtsai@dbtsai.org
Received: by 10.152.131.138 with HTTP; Tue, 22 Apr 2014 23:44:14 -0700 (PDT)
In-Reply-To: <2DE7C3E8-5A8C-4735-B23B-36141E808F0C@gmail.com>
References: <CAEYYnxYq7fi0AWx3HJWFKADL_JMfRqbbRPWBuRj_uWZT5q6W-Q@mail.gmail.com>
	<CAEYYnxbhUVbvSupEQAY4RFVnSAD59rvNTm=f_Fwpq3K+eMLbog@mail.gmail.com>
	<2DE7C3E8-5A8C-4735-B23B-36141E808F0C@gmail.com>
Date: Tue, 22 Apr 2014 23:44:14 -0700
X-Google-Sender-Auth: Ll4xY4wIa0TpvMPWeNu4nQYBakg
Message-ID: <CAEYYnxbsMJWmzL1cPh=HTiLt_z3rrP6-_pzgTFU0XPc2OqvhTQ@mail.gmail.com>
Subject: Re: Jekyll documentation generation error
From: DB Tsai <dbtsai@dbtsai.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134dce49c503604f7b00f0a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134dce49c503604f7b00f0a
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Matei, thanks. It works with kramdown.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Tue, Apr 22, 2014 at 11:38 PM, Matei Zaharia <matei.zaharia@gmail.com>wr=
ote:

> Try doing =E2=80=9Cgem install kramdown=E2=80=9D. The maruku gem for Mark=
down throws these
> errors, but Kramdown doesn=E2=80=99t.
>
> Matei
>
> On Apr 22, 2014, at 11:31 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
>
> > This is the trace.
> >
> >  Conversion error: There was an error converting 'docs/
> cluster-overview.md
> > '.
> >
> >
> /Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1=
/lib/jekyll/converters/markdown/maruku_parser.rb:45:in
> > `print_errors_and_fail': MaRuKu encountered problem(s) while converting
> > your markup. (MaRuKu::Exception)
> >
> > from
> >
> /Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1=
/lib/jekyll/converters/markdown/maruku_parser.rb:50:in
> > `convert'
> >
> > from
> >
> /Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1=
/lib/jekyll/converters/markdown.rb:39:in
> > `convert'
> >
> > from
> >
> /Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1=
/lib/jekyll/convertible.rb:57:in
> > `transform'
> >
> > from
> >
> /Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1=
/lib/jekyll/convertible.rb:153:in
> > `do_layout'
> >
> > from
> >
> /Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1=
/lib/jekyll/page.rb:115:in
> > `render'
> >
> > from
> >
> /Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1=
/lib/jekyll/site.rb:239:in
> > `block in render'
> >
> > from
> >
> /Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1=
/lib/jekyll/site.rb:238:in
> > `each'
> >
> > from
> >
> /Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1=
/lib/jekyll/site.rb:238:in
> > `render'
> >
> > from
> >
> /Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1=
/lib/jekyll/site.rb:39:in
> > `process'
> >
> > from
> >
> /Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1=
/lib/jekyll/command.rb:18:in
> > `process_site'
> >
> > from
> >
> /Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1=
/lib/jekyll/commands/build.rb:23:in
> > `build'
> >
> > from
> >
> /Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1=
/lib/jekyll/commands/build.rb:7:in
> > `process'
> >
> > from
> >
> /Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/jekyll-1.5.1=
/bin/jekyll:77:in
> > `block (2 levels) in <top (required)>'
> >
> > from
> >
> /Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.=
1.6/lib/commander/command.rb:180:in
> > `call'
> >
> > from
> >
> /Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.=
1.6/lib/commander/command.rb:180:in
> > `call'
> >
> > from
> >
> /Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.=
1.6/lib/commander/command.rb:155:in
> > `run'
> >
> > from
> >
> /Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.=
1.6/lib/commander/runner.rb:422:in
> > `run_active_command'
> >
> > from
> >
> /Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.=
1.6/lib/commander/runner.rb:82:in
> > `run!'
> >
> > from
> >
> /Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.=
1.6/lib/commander/delegates.rb:8:in
> > `run!'
> >
> > from
> >
> /Users/dbtsai/.rbenv/versions/2.1.1/lib/ruby/gems/2.1.0/gems/commander-4.=
1.6/lib/commander/import.rb:10:in
> > `block in <top (required)>'
> >
> >
> > Sincerely,
> >
> > DB Tsai
> > -------------------------------------------------------
> > My Blog: https://www.dbtsai.com
> > LinkedIn: https://www.linkedin.com/in/dbtsai
> >
> >
> > On Tue, Apr 22, 2014 at 11:26 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
> >
> >> Hi guys,
> >>
> >> I'm trying to update LBFGS documentation so I need to generate html
> >> document to see if everything looks great. However, mv I get the
> following
> >> error.
> >>
> >> Conversion error: There was an error converting 'docs/
> cluster-overview.md
> >> '.
> >> error: MaRuKu encountered problem(s) while converting your markup.. Us=
e
> >> --trace to view backtrace
> >>
> >>
> >> I'm using ruby 2.1.1p76 and jekyll 1.5.1 installed by gem as
> instruction.
> >> Do I miss anything?
> >>
> >> Also, in the instruction, if we want to skip the api docs, we can do t=
he
> >> following.
> >>
> >> # Skip generating API docs (which takes a while)
> >> $ SKIP_SCALADOC=3D1 jekyll build
> >>
> >> But what does"SKIP_SCALADOC=3D1" mean? export SKIP_SCALADOC=3D1?
> >>
> >> Thanks.
> >>
> >>
> >>
> >>
> >> Sincerely,
> >>
> >> DB Tsai
> >> -------------------------------------------------------
> >> My Blog: https://www.dbtsai.com
> >> LinkedIn: https://www.linkedin.com/in/dbtsai
> >>
>
>

--001a1134dce49c503604f7b00f0a--

From dev-return-7395-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 23 07:07:05 2014
Return-Path: <dev-return-7395-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2EF3610930
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 23 Apr 2014 07:07:05 +0000 (UTC)
Received: (qmail 75942 invoked by uid 500); 23 Apr 2014 07:07:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75855 invoked by uid 500); 23 Apr 2014 07:07:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75831 invoked by uid 99); 23 Apr 2014 07:07:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 07:07:02 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.212.169] (HELO mail-wi0-f169.google.com) (209.85.212.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 07:06:58 +0000
Received: by mail-wi0-f169.google.com with SMTP id hm4so4094459wib.2
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 00:06:35 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=KU9MvMn3W+nuEsmptt7sfEVluhmg+o0KJUlfRLHSq5Q=;
        b=kbrulPYUgkolhqrIPrcbyXwuXdwUbeg0saLTaFqcbl13JKOa0kN1VJq/bUkI0bgiPK
         hPPzPf+jVUAD9IrObzgVwbFmZCUAP4JjliyyKT0072QlUx+h3jJNdEdHEDCvnamaKfaS
         i9PTkYvLGUMhsxY1ygyrv0BPEyHu4LePegl088wynVoetNbe253OQJ1naIGu5Y+GcVd3
         RYXQudOHZyRMti2DWddsv3DZWWD8xlZYUTJD3CmNns2xJfT8nRqoTm1Gt2xuyfwWI5tG
         7l52XuP13sAkCKw87bn5jyu0ZWmSc1LDzFYWkAwlYYf78pFhectpr6whigAZgloqRdH4
         nFLQ==
X-Gm-Message-State: ALoCoQnDpVyopc8LG7JdEh4yL3y/0YWtOn/51zL4goYrNc5xhIINaxJGmcGS2RO9GCBgb/1IGdWx
X-Received: by 10.180.37.178 with SMTP id z18mr456161wij.46.1398236795703;
 Wed, 23 Apr 2014 00:06:35 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.216.64.194 with HTTP; Wed, 23 Apr 2014 00:06:05 -0700 (PDT)
From: "Saumitra Shahapure (Vizury)" <saumitra.shahapure@vizury.com>
Date: Wed, 23 Apr 2014 12:36:05 +0530
Message-ID: <CAGP031sGz8UKrFpU07Vq7vz_3qWxhoBGcXWAi8Di2HQaExx1+Q@mail.gmail.com>
Subject: Sharing RDDs
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=e89a8f642e1e8525eb04f7b05f36
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f642e1e8525eb04f7b05f36
Content-Type: text/plain; charset=ISO-8859-1

Hello,

Is it possible in spark to reuse cached RDDs generated in earlier run?

Specifically, I am trying to have a setup where first scala script
generates cached RDDs. If another scala script tries to perform same
operations on same dataset, it should be able to get results from cache
generated in earlier run.

Is there any direct/indirect way to do this?

--
Regards,
Saumitra Shahapure

--e89a8f642e1e8525eb04f7b05f36--

From dev-return-7396-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 23 08:04:16 2014
Return-Path: <dev-return-7396-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5D9BA10B46
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 23 Apr 2014 08:04:16 +0000 (UTC)
Received: (qmail 79577 invoked by uid 500); 23 Apr 2014 08:04:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79476 invoked by uid 500); 23 Apr 2014 08:04:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79468 invoked by uid 99); 23 Apr 2014 08:04:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 08:04:12 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mayur.rustagi@gmail.com designates 74.125.82.41 as permitted sender)
Received: from [74.125.82.41] (HELO mail-wg0-f41.google.com) (74.125.82.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 08:04:07 +0000
Received: by mail-wg0-f41.google.com with SMTP id n12so503138wgh.24
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 01:03:45 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=oEP79c76NucoO2GZZ13bGpjBM31P+t5Ej3GjaSVRs1Y=;
        b=nirz1FrZSwSlzgpKQb/QnGgBKEksR+wy0K5r2jlxdVoxAMmpqAhAH7sPY71bzNK23T
         ii2SoZYR/XVRBL2qg1WKueEEET4jqvzKTZb3lJuLYwcy5hm6TXhEb+iwKRDWmR9ZEPbE
         cFdLhWjLq/lMa5aJSoFgO4+CIXy65iFyxfjJSw2HtnbJXbskWxUrNzpV8s8Qi44KXfOC
         EqwlNHzk3e8c9V8+++SjPv2rpALFeQY6ucgzR0cHoq+iquauza/QdnK7nqtoYgCqqHSG
         bo2Ofe0XslLS3z+oDdKjgWMgdp/LzZVKyQpDVp0eKnT9zJodmjiPKsLM6FrkLASafqfP
         GHJA==
X-Received: by 10.194.189.80 with SMTP id gg16mr8211wjc.84.1398240225748; Wed,
 23 Apr 2014 01:03:45 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.7.225 with HTTP; Wed, 23 Apr 2014 01:03:25 -0700 (PDT)
In-Reply-To: <CAHVi7zJg-P959-WL8n2CzMe0Mpbu2c4FWS2uxqKwJ=1U5TKAFA@mail.gmail.com>
References: <CAHVi7zJg-P959-WL8n2CzMe0Mpbu2c4FWS2uxqKwJ=1U5TKAFA@mail.gmail.com>
From: Mayur Rustagi <mayur.rustagi@gmail.com>
Date: Wed, 23 Apr 2014 13:33:25 +0530
Message-ID: <CAAqHKj4-iAp+J56-iuVmKx=hcubvfNEn4aD_zNJkjFxyAObCzA@mail.gmail.com>
Subject: Re: Spark on wikipedia dataset
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bb03f9cf7738304f7b12b25
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bb03f9cf7738304f7b12b25
Content-Type: text/plain; charset=UTF-8

Huge joins would be interesting. I do all my demos on wikipedia dataset for
Shark. Joins are typical pain to showcase & show off :)

Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>



On Wed, Apr 23, 2014 at 10:33 AM, Ajay Nair <prodigyaj@gmail.com> wrote:

> I am going to perform some test experiments on the wikipedia dataset using
> the spark framework. I know wikipedia data set might already have been
> analyzed, but what are the potential explored/unexplored aspects of spark
> that can be tested and benchmarked on wikipedia dataset?
>
> Thanks
> AJ
>

--047d7bb03f9cf7738304f7b12b25--

From dev-return-7397-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 23 09:19:50 2014
Return-Path: <dev-return-7397-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0223F10D76
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 23 Apr 2014 09:19:50 +0000 (UTC)
Received: (qmail 5081 invoked by uid 500); 23 Apr 2014 09:19:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4877 invoked by uid 500); 23 Apr 2014 09:19:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4866 invoked by uid 99); 23 Apr 2014 09:19:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 09:19:47 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of liqingyang1985@gmail.com designates 209.85.212.181 as permitted sender)
Received: from [209.85.212.181] (HELO mail-wi0-f181.google.com) (209.85.212.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 09:19:42 +0000
Received: by mail-wi0-f181.google.com with SMTP id hm4so801621wib.8
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 02:19:20 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=kmm7xmh3YU+qcoqhU/0RyzsSuJy00b9x0Cng/YZr6Uc=;
        b=XOharLCjYPc4CjWsf42+C0IlItzVhfkNHBGjF8PhRPfCaK1H9fS4HlN17hXV8EMcQr
         8JTc/e8kLnPaJbGOtFLPriP0lhyf0v4Afjb5yQXHbu/cPZtPGkp8CxQYPVpBFcXrkEkl
         ipx1PO3/uyl+1JQwZhijpUrVtZhms3XBnNhfa/89vKzyHMfEEzMGP6kLWo7xvxDe2SAY
         vzxAqQGDl62lG0YDxX4tlzTM4p7lK+de8quXBYPo8OT2elqsFVg7U0ymoRvjKZEejWvC
         LH0yPNcIgc/w1OrZ8THzeYIW8IDZ0KH+B1nilDG7+mtBNPP0Px3uG73us3U3ZfRIACEu
         9oFw==
MIME-Version: 1.0
X-Received: by 10.194.81.98 with SMTP id z2mr38078479wjx.12.1398244760135;
 Wed, 23 Apr 2014 02:19:20 -0700 (PDT)
Received: by 10.194.61.39 with HTTP; Wed, 23 Apr 2014 02:19:20 -0700 (PDT)
Date: Wed, 23 Apr 2014 17:19:20 +0800
Message-ID: <CABDsqqanKD04qPcdFpUgEy1ybU=5xgO37DfQFAoBmg=wzKtGbw@mail.gmail.com>
Subject: get -101 error code when running select query
From: qingyang li <liqingyang1985@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bd6ba703cac4b04f7b23aeb
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd6ba703cac4b04f7b23aeb
Content-Type: text/plain; charset=UTF-8

hi,  i have started one sharkserver2 ,  and using java code to send query
to this server by hive jdbc,  but i got such error:
------
FAILED: Execution Error, return code -101 from shark.execution.SparkTask
org.apache.hive.service.cli.HiveSQLException: Error while processing
statement: FAILED: Execution Error, return code -101 from
shark.execution.SparkTask
        at shark.server.SharkSQLOperation.run(SharkSQLOperation.scala:45)
        at
org.apache.hive.service.cli.session.HiveSessionImpl.executeStatement(HiveSessionImpl.java:180)
        at
org.apache.hive.service.cli.CLIService.executeStatement(CLIService.java:152)
        at
org.apache.hive.service.cli.thrift.ThriftCLIService.ExecuteStatement(ThriftCLIService.java:203)
        at
org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1133)
        at
org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement.getResult(TCLIService.java:1118)
        at
org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
        at
org.apache.hive.service.auth.TUGIContainingProcessor$2.run(TUGIContainingProcessor.java:64)
        at
org.apache.hive.service.auth.TUGIContainingProcessor$2.run(TUGIContainingProcessor.java:61)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)
        at
org.apache.hadoop.hive.shims.HadoopShimsSecure.doAs(HadoopShimsSecure.java:524)
        at
org.apache.hive.service.auth.TUGIContainingProcessor.process(TUGIContainingProcessor.java:61)
        at
org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:206)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)

-------
do anyone encounter this problem?

--047d7bd6ba703cac4b04f7b23aeb--

From dev-return-7398-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 23 13:08:03 2014
Return-Path: <dev-return-7398-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7A43411411
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 23 Apr 2014 13:08:03 +0000 (UTC)
Received: (qmail 42491 invoked by uid 500); 23 Apr 2014 13:08:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41703 invoked by uid 500); 23 Apr 2014 13:07:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41683 invoked by uid 99); 23 Apr 2014 13:07:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 13:07:49 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.192.49 as permitted sender)
Received: from [209.85.192.49] (HELO mail-qg0-f49.google.com) (209.85.192.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 13:07:44 +0000
Received: by mail-qg0-f49.google.com with SMTP id j5so863937qga.22
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 06:07:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=vIIL1PMirCa684znYDKp4DvTpUcaCKGTciJd9kKUX6Q=;
        b=A/Ueu39+R5B1cZ0d6hqCDJy3+QJbw70w6VZ4ghP5EO+qTbXN3EcSYhOl0q0VM6cykc
         tBhv4kYeTqI3tUqO7p9gnEHmTfzZoRlg/WAFr4w+8VTeYvTroDJpy4Vp2rur+VrXyTV+
         y6IExJsNy7OnpHW4A7W0CuXXjR6znI4wCJicQ0fn3k8iBfxHj13sZqLS+oPK35Ksr22K
         2KZ2slPUNN287maOLS3GEMI8WrhIneaSDKIXCAAmrGK7g+rl68enXYpsNjfisHNNt5RT
         fE1M7BUsdQWgDplIdfe6w7lJCXYzrNqXDlwdBjYngARcqDc7RJqRfjhob7t49fp75eb1
         XPbw==
MIME-Version: 1.0
X-Received: by 10.224.13.7 with SMTP id z7mr58810297qaz.4.1398258441764; Wed,
 23 Apr 2014 06:07:21 -0700 (PDT)
Received: by 10.140.51.195 with HTTP; Wed, 23 Apr 2014 06:07:21 -0700 (PDT)
Received: by 10.140.51.195 with HTTP; Wed, 23 Apr 2014 06:07:21 -0700 (PDT)
In-Reply-To: <JIRA.12709883.1398202237930.162781.1398258138077@arcas>
References: <JIRA.12709883.1398202237930@arcas>
	<JIRA.12709883.1398202237930.162781.1398258138077@arcas>
Date: Wed, 23 Apr 2014 18:37:21 +0530
Message-ID: <CAJiQeYLHhKjvmwVrEtjdT1jJ5HDp4yXO7hmWr2QR0eAgo59bVw@mail.gmail.com>
Subject: Re: [jira] [Commented] (SPARK-1576) Passing of JAVA_OPTS to YARN on
 command line
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0149c6dcb9c79d04f7b5699a
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149c6dcb9c79d04f7b5699a
Content-Type: text/plain; charset=UTF-8

This breaks all existing jobs which are not using spark-submit.
The consensus was not to break compatibility unless there was an overriding
reason to do so
On Apr 23, 2014 6:32 PM, "Thomas Graves (JIRA)" <jira@apache.org> wrote:

>
>     [
> https://issues.apache.org/jira/browse/SPARK-1576?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13978164#comment-13978164]
>
> Thomas Graves commented on SPARK-1576:
> --------------------------------------
>
> Is this meant for the driver or the executors?  The spark-submit script
> has a command line option for the driver:  --driver-java-options.
> I believe the intent of https://github.com/apache/spark/pull/299 was to
> not expose SPARK_JAVA_OPTS to the user anymore.
>
> > Passing of JAVA_OPTS to YARN on command line
> > --------------------------------------------
> >
> >                 Key: SPARK-1576
> >                 URL: https://issues.apache.org/jira/browse/SPARK-1576
> >             Project: Spark
> >          Issue Type: Improvement
> >    Affects Versions: 0.9.0, 1.0.0, 0.9.1
> >            Reporter: Nishkam Ravi
> >             Fix For: 0.9.0, 1.0.0, 0.9.1
> >
> >         Attachments: SPARK-1576.patch
> >
> >
> > JAVA_OPTS can be passed by using either env variables (i.e.,
> SPARK_JAVA_OPTS) or as config vars (after Patrick's recent change). It
> would be good to allow the user to pass them on command line as well to
> restrict scope to single application invocation.
>
>
>
> --
> This message was sent by Atlassian JIRA
> (v6.2#6252)
>

--089e0149c6dcb9c79d04f7b5699a--

From dev-return-7399-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 23 15:51:37 2014
Return-Path: <dev-return-7399-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6F22411A4C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 23 Apr 2014 15:51:37 +0000 (UTC)
Received: (qmail 32134 invoked by uid 500); 23 Apr 2014 15:51:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32042 invoked by uid 500); 23 Apr 2014 15:51:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32031 invoked by uid 99); 23 Apr 2014 15:51:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 15:51:34 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tgraves_cs@yahoo.com designates 72.30.239.15 as permitted sender)
Received: from [72.30.239.15] (HELO nm31-vm7.bullet.mail.bf1.yahoo.com) (72.30.239.15)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 15:51:28 +0000
Received: from [98.139.215.141] by nm31.bullet.mail.bf1.yahoo.com with NNFMP; 23 Apr 2014 15:51:07 -0000
Received: from [98.139.212.217] by tm12.bullet.mail.bf1.yahoo.com with NNFMP; 23 Apr 2014 15:51:07 -0000
Received: from [127.0.0.1] by omp1026.mail.bf1.yahoo.com with NNFMP; 23 Apr 2014 15:51:07 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 314221.14783.bm@omp1026.mail.bf1.yahoo.com
Received: (qmail 24895 invoked by uid 60001); 23 Apr 2014 15:51:07 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.com; s=s1024; t=1398268267; bh=JT7it2xlpybed6H3v2dFMtp+OboOiugyhd3Zq3c0jBY=; h=X-YMail-OSG:Received:X-Rocket-MIMEInfo:X-Mailer:References:Message-ID:Date:From:Reply-To:Subject:To:In-Reply-To:MIME-Version:Content-Type; b=vA1fpSC+mOY5f/ciN6Bbwi4cbgns8OutWSrloEmtbgoaAMPrs4bsDLQm/TJKmvgE0BemnuHOMpQUPb0pDY863Z6sKILN7S/v073Eksk6Wj86AFElix0Wf9QT1rwIg54pZ0b98hwTGfd/WO8tAnMolzkxPKXaImsn9ayeWSHDOKY=
DomainKey-Signature:a=rsa-sha1; q=dns; c=nofws;
  s=s1024; d=yahoo.com;
  h=X-YMail-OSG:Received:X-Rocket-MIMEInfo:X-Mailer:References:Message-ID:Date:From:Reply-To:Subject:To:In-Reply-To:MIME-Version:Content-Type;
  b=B38zuYWSLqVtH2IvStwf10OfuYXR+/UG3RpeFQWLPe1i2sJYl5QPhxuIZBuCCa8upgrP2BlfUNvwuamIgWrNyN/zETYzOWwSX0LQB+c8EyI7skIwMty5WDKENlywY0xkg6+8u43tkrazVmhv/1myzftghWgN+D7oJIrF7wiyWhI=;
X-YMail-OSG: 3BG96cEVM1l4KHWkuUVry6ocTouHko1oU5P0Q2LKZ5C3R8G
 iuT5hC_5C3qgzb3ZAOK54GrCLHu7.HBHOFcMkyIsj7SfYmL6YcpEr_5oB7Ne
 thbd.HHr7eVjeFYxHYWXt8KawNJ.31T9qc0XgbvjSOovowR_tgY8dAnHI06P
 xI2IAbQ9Q0u3v.BoabXSlCGEfomDUhoEY1f2VI69XBIzTEiM_OpBHIJLLkq3
 LVpmK5C1JO1GMb0GmHB7Z5PhWtzqgi.QV.bJRRJ8iQXu2PUslZhhp0pwgLZ0
 vg3nCH2UncKHhTRPX8unyJZkZLRO.p8x.gzXNeJazhKdmJGXTM66PvhJfv2b
 t85UoDZMDI.2Jywp1v1Lp_JzCV18QsLm5bIuTnuvoOb_u2z.2EixrKCxroBm
 OXELo.eBqjMIeP2zvLV3dyZLlIsqspR5QYSWxQJma9enebTe4MWTZNTJ7YQs
 XfWH40_q9rlH4fa5zsMxZvBPNv7HhmRYkefB_dzR7Od_scju6s6mdbEQGw2v
 sm6g84QAzz38GJr9eNpS_VFsDWVWqVoE88D7yNcUw4tpgKDRU4Mgju9SGt7J
 _YoxMFlDrXEsbJzXqEr2Xeh5ygMFsRzCuAn5t1Xw3VD2dOtOmqcokf8ZcxwW
 z3az1DxbCjqsSRyJr8IWV
Received: from [204.11.79.50] by web140101.mail.bf1.yahoo.com via HTTP; Wed, 23 Apr 2014 08:51:07 PDT
X-Rocket-MIMEInfo: 002.001,Y2FuIHlvdSBiZSBtb3JlIHNwZWNpZmljPyDCoFdoYXQgYnJlYWtzIGV4aXN0aW5nIGpvYnM_IMKgSWYgeW91IGFyZSByZWZlcnJpbmcgdG8gbXkgY29tbWVudCwgwqBTUEFSS19KQVZBX09QVFMgc3RpbGwgd29ya3MgYnV0IEkgdGhpbmsgdGhlIGludGVudCBpcyB0byBtb3ZlIGF3YXkgZnJvbSBpdC4KClRvbQpPbiBXZWRuZXNkYXksIEFwcmlsIDIzLCAyMDE0IDg6MDcgQU0sIE1yaWR1bCBNdXJhbGlkaGFyYW4gPG1yaWR1bEBnbWFpbC5jb20.IHdyb3RlOgogClRoaXMgYnJlYWtzIGFsbCBleGlzdGluZyBqb2IBMAEBAQE-
X-Mailer: YahooMailWebService/0.8.185.657
References: <JIRA.12709883.1398202237930@arcas>	<JIRA.12709883.1398202237930.162781.1398258138077@arcas> <CAJiQeYLHhKjvmwVrEtjdT1jJ5HDp4yXO7hmWr2QR0eAgo59bVw@mail.gmail.com>
Message-ID: <1398268267.41571.YahooMailNeo@web140101.mail.bf1.yahoo.com>
Date: Wed, 23 Apr 2014 08:51:07 -0700 (PDT)
From: Tom Graves <tgraves_cs@yahoo.com>
Reply-To: Tom Graves <tgraves_cs@yahoo.com>
Subject: Re: [jira] [Commented] (SPARK-1576) Passing of JAVA_OPTS to YARN on command line
To: "dev@spark.apache.org" <dev@spark.apache.org>
In-Reply-To: <CAJiQeYLHhKjvmwVrEtjdT1jJ5HDp4yXO7hmWr2QR0eAgo59bVw@mail.gmail.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="584511794-736474254-1398268267=:41571"
X-Virus-Checked: Checked by ClamAV on apache.org

--584511794-736474254-1398268267=:41571
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

can you be more specific? =A0What breaks existing jobs? =A0If you are refer=
ring to my comment, =A0SPARK_JAVA_OPTS still works but I think the intent i=
s to move away from it.=0A=0ATom=0AOn Wednesday, April 23, 2014 8:07 AM, Mr=
idul Muralidharan <mridul@gmail.com> wrote:=0A =0AThis breaks all existing =
jobs which are not using spark-submit.=0AThe consensus was not to break com=
patibility unless there was an overriding=0Areason to do so=0A=0AOn Apr 23,=
 2014 6:32 PM, "Thomas Graves (JIRA)" <jira@apache.org> wrote:=0A=0A>=0A>=
=A0 =A0  [=0A> https://issues.apache.org/jira/browse/SPARK-1576?page=3Dcom.=
atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedComment=
Id=3D13978164#comment-13978164]=0A>=0A> Thomas Graves commented on SPARK-15=
76:=0A> --------------------------------------=0A>=0A> Is this meant for th=
e driver or the executors?=A0 The spark-submit script=0A> has a command lin=
e option for the driver:=A0 --driver-java-options.=0A> I believe the intent=
 of https://github.com/apache/spark/pull/299 was to=0A> not expose SPARK_JA=
VA_OPTS to the user anymore.=0A>=0A> > Passing of JAVA_OPTS to YARN on comm=
and line=0A> > --------------------------------------------=0A> >=0A> >=A0 =
=A0 =A0 =A0 =A0 =A0 =A0 =A0  Key: SPARK-1576=0A> >=A0 =A0 =A0 =A0 =A0 =A0 =
=A0 =A0  URL: https://issues.apache.org/jira/browse/SPARK-1576=0A> >=A0 =A0=
 =A0 =A0 =A0 =A0  Project: Spark=0A> >=A0 =A0 =A0 =A0 =A0 Issue Type: Impro=
vement=0A> >=A0 =A0 Affects Versions: 0.9.0, 1.0.0, 0.9.1=0A> >=A0 =A0 =A0 =
=A0 =A0 =A0 Reporter: Nishkam Ravi=0A> >=A0 =A0 =A0 =A0 =A0 =A0  Fix For: 0=
.9.0, 1.0.0, 0.9.1=0A> >=0A> >=A0 =A0 =A0 =A0  Attachments: SPARK-1576.patc=
h=0A> >=0A> >=0A> > JAVA_OPTS can be passed by using either env variables (=
i.e.,=0A> SPARK_JAVA_OPTS) or as config vars (after Patrick's recent change=
). It=0A> would be good to allow the user to pass them on command line as w=
ell to=0A> restrict scope to single application invocation.=0A>=0A>=0A>=0A>=
 --=0A> This message was sent by Atlassian JIRA=0A> (v6.2#6252)=0A>
--584511794-736474254-1398268267=:41571--

From dev-return-7400-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 23 17:54:47 2014
Return-Path: <dev-return-7400-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 670F411FA4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 23 Apr 2014 17:54:47 +0000 (UTC)
Received: (qmail 68636 invoked by uid 500); 23 Apr 2014 17:54:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68580 invoked by uid 500); 23 Apr 2014 17:54:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68572 invoked by uid 99); 23 Apr 2014 17:54:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 17:54:45 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.216.178 as permitted sender)
Received: from [209.85.216.178] (HELO mail-qc0-f178.google.com) (209.85.216.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 17:54:41 +0000
Received: by mail-qc0-f178.google.com with SMTP id i8so1325349qcq.9
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 10:54:20 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=1Vr/mtwHY3tKc9xwRNWRNajUYDXVPo0MJXIDK+gHTSg=;
        b=HIp21u1n8QBzwF4kZ83qfZkvlGqc4FBFyy/LJzBu8mGc7lfBuc6OMZMfQzGf5Mg9eZ
         dGolt0aWtvtfWmz1Y/j44h9gFCPHMF40s83l8/P9PwxamBU2Gqn0uYBgRB3USPnJvE81
         PJx8Q7uod7yty6F5ZQehul3k5ZqLzkhf0GAc8X3PrJxViLMZiBW51OeO4ssPcOvPAhkr
         lYq1Fqy9UYk3gmzi5ZnsHlliO0AIx5FNyoIeoMazfdG6Iw5HeT96l8UiWun6GlF8EwJD
         309wNPskZk9fxGMYGOPEmF07i3XjwwmK9F6jvyyzlCuXmEzBc4uzxUz7yNE7ZGEsbx51
         aNpA==
MIME-Version: 1.0
X-Received: by 10.140.20.165 with SMTP id 34mr55350388qgj.77.1398275660818;
 Wed, 23 Apr 2014 10:54:20 -0700 (PDT)
Received: by 10.140.51.195 with HTTP; Wed, 23 Apr 2014 10:54:20 -0700 (PDT)
In-Reply-To: <CAJiQeYLHhKjvmwVrEtjdT1jJ5HDp4yXO7hmWr2QR0eAgo59bVw@mail.gmail.com>
References: <JIRA.12709883.1398202237930@arcas>
	<JIRA.12709883.1398202237930.162781.1398258138077@arcas>
	<CAJiQeYLHhKjvmwVrEtjdT1jJ5HDp4yXO7hmWr2QR0eAgo59bVw@mail.gmail.com>
Date: Wed, 23 Apr 2014 23:24:20 +0530
Message-ID: <CAJiQeYJmZVvqcOqzsEzDoDKA3Tm4k6YYNEZZXU+1Fzotpgvm5Q@mail.gmail.com>
Subject: Re: [jira] [Commented] (SPARK-1576) Passing of JAVA_OPTS to YARN on
 command line
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Sorry, I misread - I meant SPARK_JAVA_OPTS - not JAVA_OPTS.
See here : https://issues.apache.org/jira/browse/SPARK-1588

Regards,
Mridul

On Wed, Apr 23, 2014 at 6:37 PM, Mridul Muralidharan <mridul@gmail.com> wrote:
> This breaks all existing jobs which are not using spark-submit.
> The consensus was not to break compatibility unless there was an overriding
> reason to do so
>
> On Apr 23, 2014 6:32 PM, "Thomas Graves (JIRA)" <jira@apache.org> wrote:
>>
>>
>>     [
>> https://issues.apache.org/jira/browse/SPARK-1576?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13978164#comment-13978164
>> ]
>>
>> Thomas Graves commented on SPARK-1576:
>> --------------------------------------
>>
>> Is this meant for the driver or the executors?  The spark-submit script
>> has a command line option for the driver:  --driver-java-options.
>> I believe the intent of https://github.com/apache/spark/pull/299 was to
>> not expose SPARK_JAVA_OPTS to the user anymore.
>>
>> > Passing of JAVA_OPTS to YARN on command line
>> > --------------------------------------------
>> >
>> >                 Key: SPARK-1576
>> >                 URL: https://issues.apache.org/jira/browse/SPARK-1576
>> >             Project: Spark
>> >          Issue Type: Improvement
>> >    Affects Versions: 0.9.0, 1.0.0, 0.9.1
>> >            Reporter: Nishkam Ravi
>> >             Fix For: 0.9.0, 1.0.0, 0.9.1
>> >
>> >         Attachments: SPARK-1576.patch
>> >
>> >
>> > JAVA_OPTS can be passed by using either env variables (i.e.,
>> > SPARK_JAVA_OPTS) or as config vars (after Patrick's recent change). It would
>> > be good to allow the user to pass them on command line as well to restrict
>> > scope to single application invocation.
>>
>>
>>
>> --
>> This message was sent by Atlassian JIRA
>> (v6.2#6252)

From dev-return-7401-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 23 19:53:13 2014
Return-Path: <dev-return-7401-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 49A7E114F0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 23 Apr 2014 19:53:13 +0000 (UTC)
Received: (qmail 57520 invoked by uid 500); 23 Apr 2014 19:53:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57330 invoked by uid 500); 23 Apr 2014 19:53:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57313 invoked by uid 99); 23 Apr 2014 19:53:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 19:53:05 +0000
X-ASF-Spam-Status: No, hits=0.6 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 74.125.82.46 as permitted sender)
Received: from [74.125.82.46] (HELO mail-wg0-f46.google.com) (74.125.82.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Apr 2014 19:53:01 +0000
Received: by mail-wg0-f46.google.com with SMTP id b13so1336848wgh.29
        for <multiple recipients>; Wed, 23 Apr 2014 12:52:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=O/WoYK/YxVsVB7260FQN0gLtj7tkZ2bwPheQGuT2YMA=;
        b=FELI/SkxuWH+DDv9dDbRu9Jmh/hcGmx87n+P992xhhiLCXXh4muxY4ezhRtcigZ4GI
         NaSTmXAz4dGRxv9d4yLnb416h4lP6JGhO75OyzSgpZWVtgnpJ/NxgtbHeifnxbNaAqOk
         MZR426LBOGefa4mQ86LXIM0MjThHRpEo2VozP7bVxd5mc2aQLdYSyqSjydMKCf+QPwLE
         Gpxq3xX6dGdGblFG9fzqBN4AXnsXUNJxxstaqcITFMv7zON23U7SogwrCvtYx7bbwbm4
         lpKuqH1ZdtZaKabkKwkX6ABlURrPwX8rHVi+Sd23qPp2+5bz1ooR4s9oXgsYh2MO7db0
         29Eg==
MIME-Version: 1.0
X-Received: by 10.180.72.205 with SMTP id f13mr3139043wiv.45.1398282759735;
 Wed, 23 Apr 2014 12:52:39 -0700 (PDT)
Received: by 10.194.82.105 with HTTP; Wed, 23 Apr 2014 12:52:39 -0700 (PDT)
In-Reply-To: <CAJgQjQ-n5g6fSFKmyt4ELE_UNRM8K-jk=dvi4b2ErG4CRUrvMw@mail.gmail.com>
References: <1395977927144-3400.post@n3.nabble.com>
	<CAJgQjQ-n5g6fSFKmyt4ELE_UNRM8K-jk=dvi4b2ErG4CRUrvMw@mail.gmail.com>
Date: Wed, 23 Apr 2014 12:52:39 -0700
Message-ID: <CAJgQjQ_T+Dc1vEjvsD7N56A+f5Vcg7cjNpvYN7-HTBgPNbipgQ@mail.gmail.com>
Subject: Re: ArrayIndexOutOfBoundsException in ALS.implicit
From: Xiangrui Meng <mengxr@gmail.com>
To: user@spark.apache.org, j.barrett.strausser@gmail.com
Cc: user <user@spark.incubator.apache.org>, dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi bearrito, this issue was fixed by Tor in
https://github.com/apache/spark/pull/407. You can either try the
master branch or wait for the 1.0 release. -Xiangrui

On Fri, Mar 28, 2014 at 12:19 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
> Hi bearrito,
>
> This is a known issue
> (https://spark-project.atlassian.net/browse/SPARK-1281) and it should
> be easy to fix by switching to a hash partitioner.
>
> CC'ed dev list in case someone volunteers to work on it.
>
> Best,
> Xiangrui
>
> On Thu, Mar 27, 2014 at 8:38 PM, bearrito <j.barrett.strausser@gmail.com> wrote:
>> Usage of negative product id's causes the above exception.
>>
>> The cause is the use of the product id's as a mechanism to index into the
>> the in and out block structures.
>>
>> Specifically on 9.0 it occurs at
>> org.apache.spark.mllib.recommendation.ALS$$anonfun$org$apache$spark$mllib$recommendation$ALS$$makeInLinkBlock$2.apply(ALS.scala:262)
>>
>> It seems reasonable to expect that product id's are positive, if a bit
>> opinionated.  I ran across this because the hash function I was using on my
>> product id's includes the negatives in it's range.
>>
>>
>>
>>
>>
>> --
>> View this message in context: http://apache-spark-user-list.1001560.n3.nabble.com/ArrayIndexOutOfBoundsException-in-ALS-implicit-tp3400.html
>> Sent from the Apache Spark User List mailing list archive at Nabble.com.

From dev-return-7402-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 01:27:09 2014
Return-Path: <dev-return-7402-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2CA8C11F90
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 01:27:09 +0000 (UTC)
Received: (qmail 76197 invoked by uid 500); 24 Apr 2014 01:27:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76103 invoked by uid 500); 24 Apr 2014 01:27:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76093 invoked by uid 99); 24 Apr 2014 01:27:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 01:27:07 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 01:27:02 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <madhu@madhu.com>)
	id 1Wd8Qz-0002L6-4g
	for dev@spark.incubator.apache.org; Wed, 23 Apr 2014 18:26:41 -0700
Date: Wed, 23 Apr 2014 18:26:41 -0700 (PDT)
From: Madhu <madhu@madhu.com>
To: dev@spark.incubator.apache.org
Message-ID: <1398302801119-6382.post@n3.nabble.com>
In-Reply-To: <CABDsqqanKD04qPcdFpUgEy1ybU=5xgO37DfQFAoBmg=wzKtGbw@mail.gmail.com>
References: <CABDsqqanKD04qPcdFpUgEy1ybU=5xgO37DfQFAoBmg=wzKtGbw@mail.gmail.com>
Subject: Re: get -101 error code when running select query
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I have seen a similar error message when connecting to Hive through JDBC.
This is just a guess on my part, but check your query. The error occurs if
you have a select that includes a null literal with an alias like this:

select a, b, null as c, d from foo

In my case, rewriting the query to use an empty string or other literal
instead of null worked:

select a, b, '' as c, d from foo

I think the problem is the lack of type information when supplying a null
literal.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/get-101-error-code-when-running-select-query-tp6377p6382.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7403-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 01:27:21 2014
Return-Path: <dev-return-7403-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 437F911F92
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 01:27:21 +0000 (UTC)
Received: (qmail 77347 invoked by uid 500); 24 Apr 2014 01:27:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77300 invoked by uid 500); 24 Apr 2014 01:27:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77284 invoked by uid 99); 24 Apr 2014 01:27:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 01:27:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nravi@cloudera.com designates 209.85.214.176 as permitted sender)
Received: from [209.85.214.176] (HELO mail-ob0-f176.google.com) (209.85.214.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 01:27:11 +0000
Received: by mail-ob0-f176.google.com with SMTP id wp4so1936115obc.35
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 18:26:50 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=zK/2bpt3X4kF6PVZiSBVONjTVdHKTsOiIZ1lBQ8M3yY=;
        b=E2hhPcplW110nlC2m2pOQtyBdZW9JuEHV3AEEZGNhVtMRVWZYeULp3BRb5VDOaP+Fs
         ZNvqUkBPUHxZFVvy8F/FAn5bQ/r9cNJZztCrjiKHdYJrSrWev6SVX3r5Yg/wEVFuEknJ
         5hi/rG8wVUO/LkMpBuOtv0UBHfVb90GLf10s1ZCozlAAce4AvxzZ2nnE1nuJRsZSLSWx
         yNbbwp4ScTqYCz1VioskPjekF02iHM6sxyf5IeREf9nOFN49s5mWu5q3EO7IVnZuBScm
         0Tr4FWPeP8439P039YdZoAKlUpqa2kUoYuL+OGvLg+Zj2BTJoQgJu8X5f6r1tGncWw9r
         YuPg==
X-Gm-Message-State: ALoCoQkYKffUXY5HkYvIt0KIQNwLzvfApeXWwH+iDlkmscBmbgpvm12s5WwsOtVmfmhGYmgxO/HZ
X-Received: by 10.60.58.7 with SMTP id m7mr78065oeq.59.1398302810451; Wed, 23
 Apr 2014 18:26:50 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.182.210.193 with HTTP; Wed, 23 Apr 2014 18:26:30 -0700 (PDT)
In-Reply-To: <CAJiQeYJmZVvqcOqzsEzDoDKA3Tm4k6YYNEZZXU+1Fzotpgvm5Q@mail.gmail.com>
References: <JIRA.12709883.1398202237930@arcas> <JIRA.12709883.1398202237930.162781.1398258138077@arcas>
 <CAJiQeYLHhKjvmwVrEtjdT1jJ5HDp4yXO7hmWr2QR0eAgo59bVw@mail.gmail.com> <CAJiQeYJmZVvqcOqzsEzDoDKA3Tm4k6YYNEZZXU+1Fzotpgvm5Q@mail.gmail.com>
From: Nishkam Ravi <nravi@cloudera.com>
Date: Wed, 23 Apr 2014 18:26:30 -0700
Message-ID: <CACfA1zX9XvpdT+Y7Of6WS_7ryifJd2YWm-Noqc40hz6GWSVoMQ@mail.gmail.com>
Subject: Re: [jira] [Commented] (SPARK-1576) Passing of JAVA_OPTS to YARN on
 command line
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01538c564e43b004f7bfbe1c
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01538c564e43b004f7bfbe1c
Content-Type: text/plain; charset=ISO-8859-1

Bit of a race condition here it seems. Patrick made a few changes yesterday
around the same time as I did (in ClientBase.scala):

for ((k, v) <- sys.props.filterKeys(_.startsWith("spark")))
{ JAVA_OPTS += "-D" + k + "=" + "\\\"" + v + "\\\"" }

This would allow JAVA_OPTS to be passed on the command line to the
ApplicationMaster, and accomplishes the same things as creation of a new
command line flag --spark-java-opts.


Mridul, the use of SPARK_JAVA_OPTS has been intentionally suppressed.


On Wed, Apr 23, 2014 at 10:54 AM, Mridul Muralidharan <mridul@gmail.com>wrote:

> Sorry, I misread - I meant SPARK_JAVA_OPTS - not JAVA_OPTS.
> See here : https://issues.apache.org/jira/browse/SPARK-1588
>
> Regards,
> Mridul
>
> On Wed, Apr 23, 2014 at 6:37 PM, Mridul Muralidharan <mridul@gmail.com>
> wrote:
> > This breaks all existing jobs which are not using spark-submit.
> > The consensus was not to break compatibility unless there was an
> overriding
> > reason to do so
> >
> > On Apr 23, 2014 6:32 PM, "Thomas Graves (JIRA)" <jira@apache.org> wrote:
> >>
> >>
> >>     [
> >>
> https://issues.apache.org/jira/browse/SPARK-1576?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13978164#comment-13978164
> >> ]
> >>
> >> Thomas Graves commented on SPARK-1576:
> >> --------------------------------------
> >>
> >> Is this meant for the driver or the executors?  The spark-submit script
> >> has a command line option for the driver:  --driver-java-options.
> >> I believe the intent of https://github.com/apache/spark/pull/299 was to
> >> not expose SPARK_JAVA_OPTS to the user anymore.
> >>
> >> > Passing of JAVA_OPTS to YARN on command line
> >> > --------------------------------------------
> >> >
> >> >                 Key: SPARK-1576
> >> >                 URL: https://issues.apache.org/jira/browse/SPARK-1576
> >> >             Project: Spark
> >> >          Issue Type: Improvement
> >> >    Affects Versions: 0.9.0, 1.0.0, 0.9.1
> >> >            Reporter: Nishkam Ravi
> >> >             Fix For: 0.9.0, 1.0.0, 0.9.1
> >> >
> >> >         Attachments: SPARK-1576.patch
> >> >
> >> >
> >> > JAVA_OPTS can be passed by using either env variables (i.e.,
> >> > SPARK_JAVA_OPTS) or as config vars (after Patrick's recent change).
> It would
> >> > be good to allow the user to pass them on command line as well to
> restrict
> >> > scope to single application invocation.
> >>
> >>
> >>
> >> --
> >> This message was sent by Atlassian JIRA
> >> (v6.2#6252)
>

--089e01538c564e43b004f7bfbe1c--

From dev-return-7404-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 01:30:46 2014
Return-Path: <dev-return-7404-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 686E911FA8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 01:30:46 +0000 (UTC)
Received: (qmail 80837 invoked by uid 500); 24 Apr 2014 01:30:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80795 invoked by uid 500); 24 Apr 2014 01:30:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80587 invoked by uid 99); 24 Apr 2014 01:30:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 01:30:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nravi@cloudera.com designates 209.85.219.54 as permitted sender)
Received: from [209.85.219.54] (HELO mail-oa0-f54.google.com) (209.85.219.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 01:30:25 +0000
Received: by mail-oa0-f54.google.com with SMTP id i7so1953626oag.27
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 18:30:03 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=7QPuTrEUPIVJ3DxRtjfzpUtn5m1rkn4PJls0FEHRuDo=;
        b=dCTjYQLcZE/XbzMn8a7I4QlxLhhqLadhNEMqf13MWhSUFHpDPlHEGGvdb38VHEr1VD
         Ox6PdD8VqzeGpwlGJy3eNyv1nG9GzwScxhCui1oLFe6/uauZz/bHzgriE7Sa3IaN8MW2
         NACI27S4IzkEKrfxApooKnYFFcklSLAjfiYPjIeN4t/Zm9WLO5gnaze6lWnzGs5X4lBs
         +sGOrsBg+omumYY+sBCHFa1dHWxbm1nlrFEWsPeVlKO3UOt6aW/I1APjarVPRFLG6/7V
         1DJEdnDknsmq9d9W+eOkNkIicKR1K2iCOAFMP7U67emameR/zNskyJcifObDv7qF9fq5
         2KNA==
X-Gm-Message-State: ALoCoQmPyUYbeN4INar2yQYqGgT6YgNcffrcBKdUxa5jNpzqA2YIVT+TuDfb0OPwYMM9TPQffq7G
X-Received: by 10.182.105.1 with SMTP id gi1mr45902724obb.9.1398303003076;
 Wed, 23 Apr 2014 18:30:03 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.182.210.193 with HTTP; Wed, 23 Apr 2014 18:29:42 -0700 (PDT)
In-Reply-To: <CACfA1zX9XvpdT+Y7Of6WS_7ryifJd2YWm-Noqc40hz6GWSVoMQ@mail.gmail.com>
References: <JIRA.12709883.1398202237930@arcas> <JIRA.12709883.1398202237930.162781.1398258138077@arcas>
 <CAJiQeYLHhKjvmwVrEtjdT1jJ5HDp4yXO7hmWr2QR0eAgo59bVw@mail.gmail.com>
 <CAJiQeYJmZVvqcOqzsEzDoDKA3Tm4k6YYNEZZXU+1Fzotpgvm5Q@mail.gmail.com> <CACfA1zX9XvpdT+Y7Of6WS_7ryifJd2YWm-Noqc40hz6GWSVoMQ@mail.gmail.com>
From: Nishkam Ravi <nravi@cloudera.com>
Date: Wed, 23 Apr 2014 18:29:42 -0700
Message-ID: <CACfA1zVoCB_5XvD7Tf1RfC3+SyNn5PYEFbB6JDKNmOV_Fex9Hw@mail.gmail.com>
Subject: Re: [jira] [Commented] (SPARK-1576) Passing of JAVA_OPTS to YARN on
 command line
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=e89a8ff1cf66c97e7304f7bfc932
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8ff1cf66c97e7304f7bfc932
Content-Type: text/plain; charset=ISO-8859-1

It would probably be best to retain support for SPARK_JAVA_OPTS in
ClientBase though..for developers that may have been using it.


On Wed, Apr 23, 2014 at 6:26 PM, Nishkam Ravi <nravi@cloudera.com> wrote:

> Bit of a race condition here it seems. Patrick made a few changes
> yesterday around the same time as I did (in ClientBase.scala):
>
> for ((k, v) <- sys.props.filterKeys(_.startsWith("spark")))
> { JAVA_OPTS += "-D" + k + "=" + "\\\"" + v + "\\\"" }
>
> This would allow JAVA_OPTS to be passed on the command line to the
> ApplicationMaster, and accomplishes the same things as creation of a new
> command line flag --spark-java-opts.
>
>
> Mridul, the use of SPARK_JAVA_OPTS has been intentionally suppressed.
>
>
> On Wed, Apr 23, 2014 at 10:54 AM, Mridul Muralidharan <mridul@gmail.com>wrote:
>
>> Sorry, I misread - I meant SPARK_JAVA_OPTS - not JAVA_OPTS.
>> See here : https://issues.apache.org/jira/browse/SPARK-1588
>>
>> Regards,
>> Mridul
>>
>> On Wed, Apr 23, 2014 at 6:37 PM, Mridul Muralidharan <mridul@gmail.com>
>> wrote:
>> > This breaks all existing jobs which are not using spark-submit.
>> > The consensus was not to break compatibility unless there was an
>> overriding
>> > reason to do so
>> >
>> > On Apr 23, 2014 6:32 PM, "Thomas Graves (JIRA)" <jira@apache.org>
>> wrote:
>> >>
>> >>
>> >>     [
>> >>
>> https://issues.apache.org/jira/browse/SPARK-1576?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13978164#comment-13978164
>> >> ]
>> >>
>> >> Thomas Graves commented on SPARK-1576:
>> >> --------------------------------------
>> >>
>> >> Is this meant for the driver or the executors?  The spark-submit script
>> >> has a command line option for the driver:  --driver-java-options.
>> >> I believe the intent of https://github.com/apache/spark/pull/299 was
>> to
>> >> not expose SPARK_JAVA_OPTS to the user anymore.
>> >>
>> >> > Passing of JAVA_OPTS to YARN on command line
>> >> > --------------------------------------------
>> >> >
>> >> >                 Key: SPARK-1576
>> >> >                 URL:
>> https://issues.apache.org/jira/browse/SPARK-1576
>> >> >             Project: Spark
>> >> >          Issue Type: Improvement
>> >> >    Affects Versions: 0.9.0, 1.0.0, 0.9.1
>> >> >            Reporter: Nishkam Ravi
>> >> >             Fix For: 0.9.0, 1.0.0, 0.9.1
>> >> >
>> >> >         Attachments: SPARK-1576.patch
>> >> >
>> >> >
>> >> > JAVA_OPTS can be passed by using either env variables (i.e.,
>> >> > SPARK_JAVA_OPTS) or as config vars (after Patrick's recent change).
>> It would
>> >> > be good to allow the user to pass them on command line as well to
>> restrict
>> >> > scope to single application invocation.
>> >>
>> >>
>> >>
>> >> --
>> >> This message was sent by Atlassian JIRA
>> >> (v6.2#6252)
>>
>
>

--e89a8ff1cf66c97e7304f7bfc932--

From dev-return-7405-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 01:45:55 2014
Return-Path: <dev-return-7405-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CD5A411FF1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 01:45:55 +0000 (UTC)
Received: (qmail 95841 invoked by uid 500); 24 Apr 2014 01:45:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95727 invoked by uid 500); 24 Apr 2014 01:45:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95719 invoked by uid 99); 24 Apr 2014 01:45:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 01:45:54 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.216.45 as permitted sender)
Received: from [209.85.216.45] (HELO mail-qa0-f45.google.com) (209.85.216.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 01:45:49 +0000
Received: by mail-qa0-f45.google.com with SMTP id cm18so1637063qab.18
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 18:45:28 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=PmqNtw+tTnVfhaajE57Ga/0FSU4cjgDt7bJS+enXQpU=;
        b=Vem+0od2QQ7eHS/cHgjpTyLC5e16Bqo614vkr9evE3TPldghFTIB1lGIG5xxPEiowL
         VK0ab4Gdn27vECkXiZuwXd50b5vv8bIRLwz9bERw6/GsZzTkbjSECOOdj5Q7JOyZArsD
         xX0xbnqmkDXFnleVj1PxyOJkKET4KDdJTuXFNTPb+u5muZPFzFiO7SfX96iUoxHV0Y4N
         KmXHzhdgKNUruUZNVRWdusvicIuluRj9d8FHtlwgFckyEfFGz1v4WHffl6Niho1SGtCQ
         2tssY7pRYRgwwh6yk1wbgny6dBc14PSilmqqFjIQLBXOrAzRZjTc0F1mEfcSmdfWMIPM
         v6Ng==
X-Received: by 10.224.164.197 with SMTP id f5mr32731708qay.32.1398303928590;
        Wed, 23 Apr 2014 18:45:28 -0700 (PDT)
Received: from [192.168.2.13] ([69.157.95.72])
        by mx.google.com with ESMTPSA id f106sm3428786qge.8.2014.04.23.18.45.28
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Wed, 23 Apr 2014 18:45:28 -0700 (PDT)
Date: Wed, 23 Apr 2014 21:52:05 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Message-ID: <8852B2350EE348D9A58E043BAD67DF79@gmail.com>
In-Reply-To: <CAPh_B=YvSCiwWJUqXOfT6aPPT7MX1tNtdYmRS3C-aiLWmx-HoA@mail.gmail.com>
References: <2B39597F59C94379B2A2498D2A7A9A9B@gmail.com>
 <CAPh_B=YvSCiwWJUqXOfT6aPPT7MX1tNtdYmRS3C-aiLWmx-HoA@mail.gmail.com>
Subject: Fw: Is there any way to make a quick test on some pre-commit
 code?
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53586e45_109cf92e_1a2"
X-Virus-Checked: Checked by ClamAV on apache.org

--53586e45_109cf92e_1a2
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

I=E2=80=99m just asked by others for the same question =20

I think Reynold gave a pretty helpful tip on this, =20

Shall we put this on Contribute-to-Spark wiki=3F =20

-- =20
Nan Zhu


=46orwarded message:

> =46rom: Reynold Xin <rxin=40databricks.com>
> Reply To: dev=40spark.incubator.apache.org
> To: dev=40spark.incubator.apache.org <dev=40spark.incubator.apache.org>=

> Date: Thursday, =46ebruary 6, 2014 at 7:50:57 PM
> Subject: Re: Is there any way to make a quick test on some pre-commit c=
ode=3F
> =20
> You can do
> =20
> sbt/sbt assemble-deps
> =20
> =20
> and then just run
> =20
> sbt/sbt package
> =20
> each time.
> =20
> =20
> You can even do
> =20
> sbt/sbt =7Epackage
> =20
> for automatic incremental compilation.
> =20
> =20
> =20
> On Thu, =46eb 6, 2014 at 4:46 PM, Nan Zhu <zhunanmcgill=40gmail.com (ma=
ilto:zhunanmcgill=40gmail.com)> wrote:
> =20
> > Hi, all
> > =20
> > Is it always necessary to run sbt assembly when you want to test some=
 code,
> > =20
> > Sometimes you just repeatedly change one or two lines for some failed=
 test
> > case, it is really time-consuming to sbt assembly every time
> > =20
> > any faster way=3F
> > =20
> > Best,
> > =20
> > --
> > Nan Zhu
> > =20
> =20
> =20
> =20
> =20



--53586e45_109cf92e_1a2--


From dev-return-7406-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 04:22:05 2014
Return-Path: <dev-return-7406-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 44CB71128F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 04:22:05 +0000 (UTC)
Received: (qmail 55090 invoked by uid 500); 24 Apr 2014 04:22:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54353 invoked by uid 500); 24 Apr 2014 04:22:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54319 invoked by uid 99); 24 Apr 2014 04:21:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 04:21:58 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 04:21:54 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 2F086101AD4
	for <dev@spark.apache.org>; Wed, 23 Apr 2014 21:21:34 -0700 (PDT)
Received: from mail-qa0-f49.google.com (mail-qa0-f49.google.com [209.85.216.49])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 84C6F101AB2
	for <dev@spark.apache.org>; Wed, 23 Apr 2014 21:21:33 -0700 (PDT)
Received: by mail-qa0-f49.google.com with SMTP id j7so1727529qaq.8
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 21:21:32 -0700 (PDT)
X-Gm-Message-State: ALoCoQk3LBBfrAxyJhP/1tv0kLV2550or7k/YeiDr7bnYN9BN4FSIBQVF2lR8fqC9JTSXacov6iE
MIME-Version: 1.0
X-Received: by 10.140.51.172 with SMTP id u41mr10476696qga.69.1398313292573;
 Wed, 23 Apr 2014 21:21:32 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Wed, 23 Apr 2014 21:21:32 -0700 (PDT)
Date: Wed, 23 Apr 2014 21:21:32 -0700
Message-ID: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
Subject: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org, mengxr@gmail.com
Content-Type: multipart/mixed; boundary=001a113517d21709bc04f7c22f1d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113517d21709bc04f7c22f1d
Content-Type: multipart/alternative; boundary=001a113517d21709ad04f7c22f1b

--001a113517d21709ad04f7c22f1b
Content-Type: text/plain; charset=UTF-8

Hi all,

I'm benchmarking Logistic Regression in MLlib using the newly added
optimizer LBFGS and GD. I'm using the same dataset and the same methodology
in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf

I want to know how Spark scale while adding workers, and how optimizers and
input format (sparse or dense) impact performance.

The benchmark code can be found here,
https://github.com/dbtsai/spark-lbfgs-benchmark

The first dataset I benchmarked is a9a which only has 2.2MB. I duplicated
the dataset, and made it 762MB to have 11M rows. This dataset has 123
features and 11% of the data are non-zero elements.

In this benchmark, all the dataset is cached in memory.

As we expect, LBFGS converges faster than GD, and at some point, no matter
how we push GD, it will converge slower and slower.

However, it's surprising that sparse format runs slower than dense format.
I did see that sparse format takes significantly smaller amount of memory
in caching RDD, but sparse is 40% slower than dense. I think sparse should
be fast since when we compute x wT, since x is sparse, we can do it faster.
I wonder if there is anything I'm doing wrong.

The attachment is the benchmark result.

Thanks.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai

--001a113517d21709ad04f7c22f1b
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi all,<div><br></div><div>I&#39;m benchmarking Logistic R=
egression in MLlib using the newly added optimizer LBFGS and GD. I&#39;m us=
ing the same dataset and the same methodology in this paper, <a href=3D"htt=
p://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf">http://www.csie.ntu.edu.tw/~c=
jlin/papers/l1.pdf</a><div>
<br></div><div>I want to know how Spark scale while adding workers, and how=
 optimizers and input format (sparse or dense) impact performance.=C2=A0</d=
iv><div><br></div><div>The benchmark code can be found here, <a href=3D"htt=
ps://github.com/dbtsai/spark-lbfgs-benchmark">https://github.com/dbtsai/spa=
rk-lbfgs-benchmark</a></div>
<div><br></div><div>The first dataset I benchmarked is a9a which only has 2=
.2MB. I duplicated the dataset, and made it 762MB to have 11M rows. This da=
taset has 123 features and 11% of the data are non-zero elements.=C2=A0</di=
v>
<div><br></div><div>In this benchmark, all the dataset is cached in memory.=
</div><div><br></div><div>As we expect, LBFGS converges faster than GD, and=
 at some point, no matter how we push GD, it will converge slower and slowe=
r.=C2=A0</div>
<div><br></div><div>However, it&#39;s surprising that sparse format runs sl=
ower than dense format. I did see that sparse format takes significantly sm=
aller amount of memory in caching RDD, but sparse is 40% slower than dense.=
 I think sparse should be fast since when we compute x wT, since x is spars=
e, we can do it faster. I wonder if there is anything I&#39;m doing wrong.=
=C2=A0</div>
<div><br></div><div>The attachment is the benchmark result.</div><div><br><=
/div><div>Thanks. =C2=A0</div><div><br>Sincerely,<br><br>DB Tsai<br>-------=
------------------------------------------------<br>My Blog: <a href=3D"htt=
ps://www.dbtsai.com" target=3D"_blank">https://www.dbtsai.com</a><br>
LinkedIn: <a href=3D"https://www.linkedin.com/in/dbtsai" target=3D"_blank">=
https://www.linkedin.com/in/dbtsai</a></div>

</div></div>

--001a113517d21709ad04f7c22f1b--
--001a113517d21709bc04f7c22f1d--

From dev-return-7407-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 04:31:01 2014
Return-Path: <dev-return-7407-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 89536112BB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 04:31:01 +0000 (UTC)
Received: (qmail 61526 invoked by uid 500); 24 Apr 2014 04:31:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61153 invoked by uid 500); 24 Apr 2014 04:30:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61136 invoked by uid 99); 24 Apr 2014 04:30:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 04:30:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of evan.sparks@gmail.com designates 209.85.220.41 as permitted sender)
Received: from [209.85.220.41] (HELO mail-pa0-f41.google.com) (209.85.220.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 04:30:55 +0000
Received: by mail-pa0-f41.google.com with SMTP id fa1so1506131pad.0
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 21:30:35 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=dReoa1JGh4973uK9J8rZAMCo7ATKmlEMJgCfGYartv4=;
        b=dnANtxWmrC3pGmdnfR8TJJc4EXVq4JgwMmUI7V2vLvhFYtcWf8jhE0ACwzrstF39c3
         4GxWbm9CeCzMil4Wrws2T/J78dqadY4sj0S8wCZZ+5YS2ZlFNxC+3Pst7Fl981Szy/lj
         VPmGS517CeBdWQwqPAWoimZKgrGhBTP+GkhqhE9Z1QfX1GWkF7hZeZacPt0z49oV3QCA
         ciJWYcKslMFQo0adGnJ00kxfCxMIJFM9yGc+fVLJlMflhHIeUSfCGLfJ1lFbaNY9dNlB
         sSn4OInm+8HzRMuFzn/nkqtS6DLycLO1/fxkopDnOmT5yTjnMwf3875E4HojpooSmXT1
         8JvQ==
X-Received: by 10.68.245.100 with SMTP id xn4mr25744029pbc.152.1398313835226;
        Wed, 23 Apr 2014 21:30:35 -0700 (PDT)
Received: from [10.0.1.116] (c-24-7-80-227.hsd1.ca.comcast.net. [24.7.80.227])
        by mx.google.com with ESMTPSA id ry10sm14144031pab.38.2014.04.23.21.30.33
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 23 Apr 2014 21:30:34 -0700 (PDT)
Content-Type: multipart/alternative;
	boundary=Apple-Mail-68F3BB07-64DB-4C6A-A859-F2094904AA9C
Mime-Version: 1.0 (1.0)
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense benchmark result
From: Evan Sparks <evan.sparks@gmail.com>
X-Mailer: iPhone Mail (11D167)
In-Reply-To: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
Date: Wed, 23 Apr 2014 21:30:33 -0700
Cc: "mengxr@gmail.com" <mengxr@gmail.com>
Content-Transfer-Encoding: 7bit
Message-Id: <FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail-68F3BB07-64DB-4C6A-A859-F2094904AA9C
Content-Type: text/plain;
	charset=us-ascii
Content-Transfer-Encoding: quoted-printable

What is the number of non zeroes per row (and number of features) in the spa=
rse case? We've hit some issues with breeze sparse support in the past but f=
or sufficiently sparse data it's still pretty good.=20

> On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>=20
> Hi all,
>=20
> I'm benchmarking Logistic Regression in MLlib using the newly added optimi=
zer LBFGS and GD. I'm using the same dataset and the same methodology in thi=
s paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>=20
> I want to know how Spark scale while adding workers, and how optimizers an=
d input format (sparse or dense) impact performance.=20
>=20
> The benchmark code can be found here, https://github.com/dbtsai/spark-lbfg=
s-benchmark
>=20
> The first dataset I benchmarked is a9a which only has 2.2MB. I duplicated t=
he dataset, and made it 762MB to have 11M rows. This dataset has 123 feature=
s and 11% of the data are non-zero elements.=20
>=20
> In this benchmark, all the dataset is cached in memory.
>=20
> As we expect, LBFGS converges faster than GD, and at some point, no matter=
 how we push GD, it will converge slower and slower.=20
>=20
> However, it's surprising that sparse format runs slower than dense format.=
 I did see that sparse format takes significantly smaller amount of memory i=
n caching RDD, but sparse is 40% slower than dense. I think sparse should be=
 fast since when we compute x wT, since x is sparse, we can do it faster. I w=
onder if there is anything I'm doing wrong.=20
>=20
> The attachment is the benchmark result.
>=20
> Thanks. =20
>=20
> Sincerely,
>=20
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai

--Apple-Mail-68F3BB07-64DB-4C6A-A859-F2094904AA9C--

From dev-return-7408-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 04:33:21 2014
Return-Path: <dev-return-7408-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A7D1E112C3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 04:33:21 +0000 (UTC)
Received: (qmail 62694 invoked by uid 500); 24 Apr 2014 04:33:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62641 invoked by uid 500); 24 Apr 2014 04:33:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62632 invoked by uid 99); 24 Apr 2014 04:33:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 04:33:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of evan.sparks@gmail.com designates 209.85.220.41 as permitted sender)
Received: from [209.85.220.41] (HELO mail-pa0-f41.google.com) (209.85.220.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 04:33:15 +0000
Received: by mail-pa0-f41.google.com with SMTP id fa1so1490302pad.14
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 21:32:52 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=GgWANL9ACnlktwca4HtjmBIFRQLYspJF3KMm9ucTMRs=;
        b=sgOw6fnExT/8VpMk5b1hIUUh+Khg4SjGVgnlfpzdP+QPCfIy/V95T92uJU98FgEjXw
         7eOLoZtyYTIuZ4QNKdQPZddJnOntHqhQnsxADqYWohELtJbRqc2fYlSbn+Wa9ZA4qCdc
         4/DW0lASX+7N6JJs7I5WUrj3M6DG04LDo3JrQ3TH1HjSsZC1o0TodXKX0ajmrjKyQMMT
         oDj3X7H+LvXYI8jkHQvP/iYgosYo+mXfmTqrqnmr2yfWbhp3dtp3L2uyaihOGU7C0wlL
         yM1GkySoZ/yN5xmWwQ9bb2IZ2lcn2r/eeGumLz2Yx+6cUv4lfDf2yRHvFPX/6sedoOQW
         j0Sw==
X-Received: by 10.68.201.10 with SMTP id jw10mr61224733pbc.25.1398313971991;
        Wed, 23 Apr 2014 21:32:51 -0700 (PDT)
Received: from [10.0.1.116] (c-24-7-80-227.hsd1.ca.comcast.net. [24.7.80.227])
        by mx.google.com with ESMTPSA id ko10sm6192805pbd.0.2014.04.23.21.32.50
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 23 Apr 2014 21:32:50 -0700 (PDT)
Content-Type: multipart/alternative;
	boundary=Apple-Mail-3048C2FC-7182-4257-B4ED-31DC37614EE8
Mime-Version: 1.0 (1.0)
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense benchmark result
From: Evan Sparks <evan.sparks@gmail.com>
X-Mailer: iPhone Mail (11D167)
In-Reply-To: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
Date: Wed, 23 Apr 2014 21:32:50 -0700
Cc: "mengxr@gmail.com" <mengxr@gmail.com>
Content-Transfer-Encoding: 7bit
Message-Id: <0643BFEE-B45A-4EFD-A95E-C792E807618E@gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail-3048C2FC-7182-4257-B4ED-31DC37614EE8
Content-Type: text/plain;
	charset=us-ascii
Content-Transfer-Encoding: quoted-printable

Sorry - just saw the 11% number. That is around the spot where dense data is=
 usually faster (blocking, cache coherence, etc) is there any chance you hav=
e a 1% (or so) sparse dataset to experiment with?

> On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>=20
> Hi all,
>=20
> I'm benchmarking Logistic Regression in MLlib using the newly added optimi=
zer LBFGS and GD. I'm using the same dataset and the same methodology in thi=
s paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>=20
> I want to know how Spark scale while adding workers, and how optimizers an=
d input format (sparse or dense) impact performance.=20
>=20
> The benchmark code can be found here, https://github.com/dbtsai/spark-lbfg=
s-benchmark
>=20
> The first dataset I benchmarked is a9a which only has 2.2MB. I duplicated t=
he dataset, and made it 762MB to have 11M rows. This dataset has 123 feature=
s and 11% of the data are non-zero elements.=20
>=20
> In this benchmark, all the dataset is cached in memory.
>=20
> As we expect, LBFGS converges faster than GD, and at some point, no matter=
 how we push GD, it will converge slower and slower.=20
>=20
> However, it's surprising that sparse format runs slower than dense format.=
 I did see that sparse format takes significantly smaller amount of memory i=
n caching RDD, but sparse is 40% slower than dense. I think sparse should be=
 fast since when we compute x wT, since x is sparse, we can do it faster. I w=
onder if there is anything I'm doing wrong.=20
>=20
> The attachment is the benchmark result.
>=20
> Thanks. =20
>=20
> Sincerely,
>=20
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai

--Apple-Mail-3048C2FC-7182-4257-B4ED-31DC37614EE8--

From dev-return-7409-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 04:33:25 2014
Return-Path: <dev-return-7409-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 836E4112C4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 04:33:25 +0000 (UTC)
Received: (qmail 63391 invoked by uid 500); 24 Apr 2014 04:33:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63339 invoked by uid 500); 24 Apr 2014 04:33:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63330 invoked by uid 99); 24 Apr 2014 04:33:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 04:33:23 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 04:33:19 +0000
Received: by mail-qg0-f53.google.com with SMTP id f51so1948841qge.26
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 21:32:56 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:sender:in-reply-to
         :references:date:message-id:subject:from:to:cc:content-type;
        bh=bIV+5Rc8+IYh3WC9yz9w6VdTaMymBu4XKw9DmobUcvU=;
        b=K+SEpQ1z6rj3/7xEH7Ref+ajHbMLbcyh81HHEoGAWhs56dAJdSOKYb7+p0g3dgZDHd
         Rk3iE1bq05bteGFR9cfhUeJn/f4vDoM98dYVQBVpDdYnXLCscLuING0uiCLHkU/d1JF9
         hqw3tjMQFcRfpYRSlbCsr9Kr06Xa6WcBVYbtdEuy8vaXcApzUYJ4JW5LRybFYQRUsXuN
         bOjxkzyXYgOQOeNvDo84qwgw+4DoJpTxzIxflAuXqON5wJvMU/rK1vDoVkcDico/hqUd
         iSfguoQSk2kqqdxN5dCXivvelCYjif+7hbdttERoD3fSVrQaUgSqhG6BZSxYUaELYwjS
         Gn+A==
X-Gm-Message-State: ALoCoQlywlsmIgIWz2avuOkinBlHSDDWI1aKmcJYZdwF1z0dUiYNPv8yibPC7/M3/oxHW3EsGahy
MIME-Version: 1.0
X-Received: by 10.224.166.210 with SMTP id n18mr64584055qay.6.1398313976765;
 Wed, 23 Apr 2014 21:32:56 -0700 (PDT)
Reply-To: dbtsai@dbtsai.com
Sender: dbtsai@dbtsai.org
Received: by 10.229.96.201 with HTTP; Wed, 23 Apr 2014 21:32:56 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Wed, 23 Apr 2014 21:32:56 -0700 (PDT)
In-Reply-To: <FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
Date: Wed, 23 Apr 2014 21:32:56 -0700
X-Google-Sender-Auth: 6lqmED_dLBXGpCXTvnIFb19tNDQ
Message-ID: <CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: DB Tsai <dbtsai@dbtsai.com>
To: dev@spark.apache.org
Cc: Xiangrui Meng <mengxr@gmail.com>
Content-Type: multipart/alternative; boundary=089e0149c39cdeb0ee04f7c25793
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149c39cdeb0ee04f7c25793
Content-Type: text/plain; charset=UTF-8

123 features per rows, and in average, 89% are zeros.
On Apr 23, 2014 9:31 PM, "Evan Sparks" <evan.sparks@gmail.com> wrote:

> What is the number of non zeroes per row (and number of features) in the
> sparse case? We've hit some issues with breeze sparse support in the past
> but for sufficiently sparse data it's still pretty good.
>
> > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> >
> > Hi all,
> >
> > I'm benchmarking Logistic Regression in MLlib using the newly added
> optimizer LBFGS and GD. I'm using the same dataset and the same methodology
> in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
> >
> > I want to know how Spark scale while adding workers, and how optimizers
> and input format (sparse or dense) impact performance.
> >
> > The benchmark code can be found here,
> https://github.com/dbtsai/spark-lbfgs-benchmark
> >
> > The first dataset I benchmarked is a9a which only has 2.2MB. I
> duplicated the dataset, and made it 762MB to have 11M rows. This dataset
> has 123 features and 11% of the data are non-zero elements.
> >
> > In this benchmark, all the dataset is cached in memory.
> >
> > As we expect, LBFGS converges faster than GD, and at some point, no
> matter how we push GD, it will converge slower and slower.
> >
> > However, it's surprising that sparse format runs slower than dense
> format. I did see that sparse format takes significantly smaller amount of
> memory in caching RDD, but sparse is 40% slower than dense. I think sparse
> should be fast since when we compute x wT, since x is sparse, we can do it
> faster. I wonder if there is anything I'm doing wrong.
> >
> > The attachment is the benchmark result.
> >
> > Thanks.
> >
> > Sincerely,
> >
> > DB Tsai
> > -------------------------------------------------------
> > My Blog: https://www.dbtsai.com
> > LinkedIn: https://www.linkedin.com/in/dbtsai
>

--089e0149c39cdeb0ee04f7c25793--

From dev-return-7410-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 04:34:58 2014
Return-Path: <dev-return-7410-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 77185112C6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 04:34:58 +0000 (UTC)
Received: (qmail 64168 invoked by uid 500); 24 Apr 2014 04:34:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64135 invoked by uid 500); 24 Apr 2014 04:34:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64127 invoked by uid 99); 24 Apr 2014 04:34:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 04:34:55 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of shivaram@berkeley.edu designates 74.125.82.180 as permitted sender)
Received: from [74.125.82.180] (HELO mail-we0-f180.google.com) (74.125.82.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 04:34:52 +0000
Received: by mail-we0-f180.google.com with SMTP id k48so1733314wev.11
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 21:34:29 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:in-reply-to:references
         :date:message-id:subject:from:to:cc:content-type;
        bh=ysFmisddrUzJsKiDlptMXwNI4URKOJbhcYxKGFnsbmU=;
        b=i49FbIaXCqnKSKd3A11sQaE7ndrU0Q9FI+cvnsWkGm1w2WwjhJp0ln/cgfmZiZsmti
         Y8vX21sT7G8A20bv6xkXNKTg1LVmjyL6qlGHOrZ30DUA6HYJ7CLryrK88Hc/GEQ520SR
         tvS2mc2U3pg8MMBXU/61eQUlV6Q+Gyhl+yHzc6ZJc3U94e9RaqT6PgLkIp77BPBokUyG
         5q0Z+Zg74tCYeobk2VmzWao9LKZPGP9gZ77/IZFQ6u/3FC9ydNV75qe9ktL1P7zOrjW3
         qUjkmcmVfj6bXaHaGfns1TYHfSNcZM0oqUvP0AOq7Il0LL8J5VvpHAX6CtjfbHs7ZGRC
         SoUA==
X-Gm-Message-State: ALoCoQlAKKD9n7kFrfNfSUS9TgNAMR1l/e/NDL8vX0mBc4ZJkJ2OB+2gJdyDK4ALVBlzVdp6noeA
MIME-Version: 1.0
X-Received: by 10.194.185.148 with SMTP id fc20mr42136461wjc.27.1398314068598;
 Wed, 23 Apr 2014 21:34:28 -0700 (PDT)
Reply-To: shivaram@eecs.berkeley.edu
Received: by 10.216.92.11 with HTTP; Wed, 23 Apr 2014 21:34:28 -0700 (PDT)
In-Reply-To: <CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
Date: Wed, 23 Apr 2014 21:34:28 -0700
Message-ID: <CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
To: dev@spark.apache.org, dbtsai@dbtsai.com
Cc: Xiangrui Meng <mengxr@gmail.com>
Content-Type: multipart/alternative; boundary=047d7bdcab7c57fd7104f7c25de0
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdcab7c57fd7104f7c25de0
Content-Type: text/plain; charset=ISO-8859-1

I don't think the attachment came through in the list. Could you upload the
results somewhere and link to them ?


On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com> wrote:

> 123 features per rows, and in average, 89% are zeros.
> On Apr 23, 2014 9:31 PM, "Evan Sparks" <evan.sparks@gmail.com> wrote:
>
> > What is the number of non zeroes per row (and number of features) in the
> > sparse case? We've hit some issues with breeze sparse support in the past
> > but for sufficiently sparse data it's still pretty good.
> >
> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> > >
> > > Hi all,
> > >
> > > I'm benchmarking Logistic Regression in MLlib using the newly added
> > optimizer LBFGS and GD. I'm using the same dataset and the same
> methodology
> > in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
> > >
> > > I want to know how Spark scale while adding workers, and how optimizers
> > and input format (sparse or dense) impact performance.
> > >
> > > The benchmark code can be found here,
> > https://github.com/dbtsai/spark-lbfgs-benchmark
> > >
> > > The first dataset I benchmarked is a9a which only has 2.2MB. I
> > duplicated the dataset, and made it 762MB to have 11M rows. This dataset
> > has 123 features and 11% of the data are non-zero elements.
> > >
> > > In this benchmark, all the dataset is cached in memory.
> > >
> > > As we expect, LBFGS converges faster than GD, and at some point, no
> > matter how we push GD, it will converge slower and slower.
> > >
> > > However, it's surprising that sparse format runs slower than dense
> > format. I did see that sparse format takes significantly smaller amount
> of
> > memory in caching RDD, but sparse is 40% slower than dense. I think
> sparse
> > should be fast since when we compute x wT, since x is sparse, we can do
> it
> > faster. I wonder if there is anything I'm doing wrong.
> > >
> > > The attachment is the benchmark result.
> > >
> > > Thanks.
> > >
> > > Sincerely,
> > >
> > > DB Tsai
> > > -------------------------------------------------------
> > > My Blog: https://www.dbtsai.com
> > > LinkedIn: https://www.linkedin.com/in/dbtsai
> >
>

--047d7bdcab7c57fd7104f7c25de0--

From dev-return-7411-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 04:36:44 2014
Return-Path: <dev-return-7411-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4933D112CD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 04:36:44 +0000 (UTC)
Received: (qmail 65700 invoked by uid 500); 24 Apr 2014 04:36:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65639 invoked by uid 500); 24 Apr 2014 04:36:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65616 invoked by uid 99); 24 Apr 2014 04:36:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 04:36:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.169] (HELO mail-qc0-f169.google.com) (209.85.216.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 04:36:36 +0000
Received: by mail-qc0-f169.google.com with SMTP id i17so2001799qcy.28
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 21:36:16 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:sender:in-reply-to
         :references:date:message-id:subject:from:to:cc:content-type;
        bh=PmiinIueSdfYMvRkt4HircQKFbU2weh7g4BeZUlk860=;
        b=XyJy94WCKsyDK/MPJI/alYyw/qYjo8fZzfGn8XSxOKhcVB6++juYjYUR/KLUiI2OXz
         m9QLO0D2tqrOroWlSK9DHbuGYQlI85wbYCKWN5cSN5JwBSCJMNGeUEjE3wu0cD6tjt9W
         k1TJ+qTyrmEUL7mOlKue4Ywb9GO/XDo3zAoZSDC0Uz1qhXxM/7Ejjxibl8zZNCISrvW8
         b9McfJD2i01du1TdL61U6zbtR9h74HVxrAI7COwcYPaUlei/TThXwvyX4TuKR77LoYqw
         RkVJnsvx26QDC+GtKNZ6EA5zL2Za+TKVfhh07CxyMUE9+6Fldjz4i1rf8KFtt8d9Ja4F
         /sXw==
X-Gm-Message-State: ALoCoQk7OebgUwzNOB9UhYNby05YLTFX8HBLDKj96V3LaR5CCg1PoaIAMKZt9nxdEIIQRpsH1PJZ
MIME-Version: 1.0
X-Received: by 10.224.160.206 with SMTP id o14mr65179147qax.44.1398314175899;
 Wed, 23 Apr 2014 21:36:15 -0700 (PDT)
Reply-To: dbtsai@dbtsai.com
Sender: dbtsai@dbtsai.org
Received: by 10.229.96.201 with HTTP; Wed, 23 Apr 2014 21:36:15 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Wed, 23 Apr 2014 21:36:15 -0700 (PDT)
In-Reply-To: <0643BFEE-B45A-4EFD-A95E-C792E807618E@gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<0643BFEE-B45A-4EFD-A95E-C792E807618E@gmail.com>
Date: Wed, 23 Apr 2014 21:36:15 -0700
X-Google-Sender-Auth: N_b8SVVYDavEg5ktEqk_UGV2-ms
Message-ID: <CAEYYnxaK0SOBAUvQ17av6PxjdSbamaGq0Ht9_iBk7mZuvn=_mQ@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: DB Tsai <dbtsai@dbtsai.com>
To: dev@spark.apache.org
Cc: Xiangrui Meng <mengxr@gmail.com>
Content-Type: multipart/alternative; boundary=089e0149d01abd413904f7c2631d
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149d01abd413904f7c2631d
Content-Type: text/plain; charset=UTF-8

Any suggestion for sparser dataset? Will test more tomorrow in the office.
On Apr 23, 2014 9:33 PM, "Evan Sparks" <evan.sparks@gmail.com> wrote:

> Sorry - just saw the 11% number. That is around the spot where dense data
> is usually faster (blocking, cache coherence, etc) is there any chance you
> have a 1% (or so) sparse dataset to experiment with?
>
> > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> >
> > Hi all,
> >
> > I'm benchmarking Logistic Regression in MLlib using the newly added
> optimizer LBFGS and GD. I'm using the same dataset and the same methodology
> in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
> >
> > I want to know how Spark scale while adding workers, and how optimizers
> and input format (sparse or dense) impact performance.
> >
> > The benchmark code can be found here,
> https://github.com/dbtsai/spark-lbfgs-benchmark
> >
> > The first dataset I benchmarked is a9a which only has 2.2MB. I
> duplicated the dataset, and made it 762MB to have 11M rows. This dataset
> has 123 features and 11% of the data are non-zero elements.
> >
> > In this benchmark, all the dataset is cached in memory.
> >
> > As we expect, LBFGS converges faster than GD, and at some point, no
> matter how we push GD, it will converge slower and slower.
> >
> > However, it's surprising that sparse format runs slower than dense
> format. I did see that sparse format takes significantly smaller amount of
> memory in caching RDD, but sparse is 40% slower than dense. I think sparse
> should be fast since when we compute x wT, since x is sparse, we can do it
> faster. I wonder if there is anything I'm doing wrong.
> >
> > The attachment is the benchmark result.
> >
> > Thanks.
> >
> > Sincerely,
> >
> > DB Tsai
> > -------------------------------------------------------
> > My Blog: https://www.dbtsai.com
> > LinkedIn: https://www.linkedin.com/in/dbtsai
>

--089e0149d01abd413904f7c2631d--

From dev-return-7412-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 04:38:48 2014
Return-Path: <dev-return-7412-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 25278112D3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 04:38:48 +0000 (UTC)
Received: (qmail 67025 invoked by uid 500); 24 Apr 2014 04:38:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66986 invoked by uid 500); 24 Apr 2014 04:38:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66978 invoked by uid 99); 24 Apr 2014 04:38:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 04:38:46 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of david.lw.hall@gmail.com designates 209.85.212.181 as permitted sender)
Received: from [209.85.212.181] (HELO mail-wi0-f181.google.com) (209.85.212.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 04:38:42 +0000
Received: by mail-wi0-f181.google.com with SMTP id hm4so439417wib.2
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 21:38:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:cc:content-type;
        bh=BDdNRPBfv3M8q+LYwqUtg7lS6BYDzP0+xCnWBpYjE5g=;
        b=y+T9uderS+6DQYTrPUykz5pSrmUhSBEVe+C/03yA0hYZRaBrbMNU7nYb8VlXnNpCQd
         2lutr89y9JbLSwDnJyCz7DnyQeSleUaEx/ywQVt2EaQ8ZIosHH+AXznfl3TqBlRQBe2F
         d8j2Nysik/p26dLl15Xv95Mb70tqCR6dWXUaCHqKUrq2wXN2hQauoiTp6jC+Np09jtr8
         kQeDEbzHpp0wdRDQ9fcUaSkS/N6gXVJQoyvo4p79/5UTbp8Eha26981OEBQUnA2aGGfZ
         w9YH1lCMEUvoO9D0mvVjjl1w22R7gTFuJp8h2JxevHv9d8th9mnuf3ydHIfQf5FCGzJZ
         HuLQ==
MIME-Version: 1.0
X-Received: by 10.180.228.42 with SMTP id sf10mr911733wic.48.1398314301440;
 Wed, 23 Apr 2014 21:38:21 -0700 (PDT)
Sender: david.lw.hall@gmail.com
Received: by 10.227.232.207 with HTTP; Wed, 23 Apr 2014 21:38:21 -0700 (PDT)
In-Reply-To: <FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
Date: Wed, 23 Apr 2014 21:38:21 -0700
X-Google-Sender-Auth: tqQjUNjD_Ie1dE1FDz-vL4Vz9JY
Message-ID: <CALW2ey1NRhuLxHzQwixVP+FYr0_9EEkGSKWZp+2n+GQ3vR8Gcg@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: David Hall <dlwh@cs.berkeley.edu>
To: dev@spark.apache.org
Cc: "mengxr@gmail.com" <mengxr@gmail.com>
Content-Type: multipart/alternative; boundary=001a1135e8f238c88c04f7c26b6d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1135e8f238c88c04f7c26b6d
Content-Type: text/plain; charset=UTF-8

On Wed, Apr 23, 2014 at 9:30 PM, Evan Sparks <evan.sparks@gmail.com> wrote:

> What is the number of non zeroes per row (and number of features) in the
> sparse case? We've hit some issues with breeze sparse support in the past
> but for sufficiently sparse data it's still pretty good.
>

Any chance you remember what the problems were? I'm sure it could be
better, but it's good to know where improvements need to happen.

-- David


>
> > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> >
> > Hi all,
> >
> > I'm benchmarking Logistic Regression in MLlib using the newly added
> optimizer LBFGS and GD. I'm using the same dataset and the same methodology
> in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
> >
> > I want to know how Spark scale while adding workers, and how optimizers
> and input format (sparse or dense) impact performance.
> >
> > The benchmark code can be found here,
> https://github.com/dbtsai/spark-lbfgs-benchmark
> >
> > The first dataset I benchmarked is a9a which only has 2.2MB. I
> duplicated the dataset, and made it 762MB to have 11M rows. This dataset
> has 123 features and 11% of the data are non-zero elements.
> >
> > In this benchmark, all the dataset is cached in memory.
> >
> > As we expect, LBFGS converges faster than GD, and at some point, no
> matter how we push GD, it will converge slower and slower.
> >
> > However, it's surprising that sparse format runs slower than dense
> format. I did see that sparse format takes significantly smaller amount of
> memory in caching RDD, but sparse is 40% slower than dense. I think sparse
> should be fast since when we compute x wT, since x is sparse, we can do it
> faster. I wonder if there is anything I'm doing wrong.
> >
> > The attachment is the benchmark result.
> >
> > Thanks.
> >
> > Sincerely,
> >
> > DB Tsai
> > -------------------------------------------------------
> > My Blog: https://www.dbtsai.com
> > LinkedIn: https://www.linkedin.com/in/dbtsai
>

--001a1135e8f238c88c04f7c26b6d--

From dev-return-7413-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 05:08:52 2014
Return-Path: <dev-return-7413-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A4C0811469
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 05:08:52 +0000 (UTC)
Received: (qmail 1426 invoked by uid 500); 24 Apr 2014 05:08:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 946 invoked by uid 500); 24 Apr 2014 05:08:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 916 invoked by uid 99); 24 Apr 2014 05:08:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 05:08:44 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.177] (HELO mail-qc0-f177.google.com) (209.85.216.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 05:08:41 +0000
Received: by mail-qc0-f177.google.com with SMTP id w7so1975532qcr.36
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 22:08:18 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:sender:in-reply-to
         :references:date:message-id:subject:from:to:cc:content-type;
        bh=hp5gIjWvG1qi2d1ArnoQLwPtoKqwW+HFXfCAjFkaRyo=;
        b=DR7PRNpDjCAH75QENDADpM4E1X52JbabfMzvymwGAG3Ebc1hwk47m2HYnVMa23KjsW
         UHUkDSi7iOGHUNiL4/7z9+z39+8oLnRSlp0RWeTIXn9X4C0mt5Amrm9P1N51wAYIMw3m
         nqkSyRLXJc3QWKii1O/6FOJXG+LiNG3180/HZsDu5kxiKK7VmArpUF+I38lBTVmnDrPf
         RoPwrSluj0bU9xrfGRb/zB/ULwtAd00L7aCaYuoAZ04+zi7EppwRhLFVQZ1mVl4cJUIB
         ddusO01tEhC0vVQQ7EyPXL7xtw0+7c3u3oWku508Z89D81GtljQEqRM4fGmxjTF8SXwu
         ntRA==
X-Gm-Message-State: ALoCoQlQaQPhkglSWUlmDl1F0Vp54HZxFEYVSa3cEEiNtUdvxG73klhONisM1xzTPmY62MNxvm9d
MIME-Version: 1.0
X-Received: by 10.140.32.139 with SMTP id h11mr66154618qgh.49.1398316098408;
 Wed, 23 Apr 2014 22:08:18 -0700 (PDT)
Reply-To: dbtsai@dbtsai.com
Sender: dbtsai@dbtsai.org
Received: by 10.229.96.201 with HTTP; Wed, 23 Apr 2014 22:08:18 -0700 (PDT)
In-Reply-To: <CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
Date: Wed, 23 Apr 2014 22:08:18 -0700
X-Google-Sender-Auth: NNPTeuY_RQlr_NF8JzZzPaD9UAg
Message-ID: <CAEYYnxa8=TP4YYM=H=H+e23qnW-edn7Xae+8Nb+=erFg6esBiQ@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: DB Tsai <dbtsai@dbtsai.com>
To: shivaram@eecs.berkeley.edu
Cc: dev@spark.apache.org, Xiangrui Meng <mengxr@gmail.com>
Content-Type: multipart/alternative; boundary=001a113b599054637f04f7c2d686
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113b599054637f04f7c2d686
Content-Type: text/plain; charset=UTF-8

The figure showing the Log-Likelihood vs Time can be found here.

https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf

Let me know if you can not open it.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman <
shivaram@eecs.berkeley.edu> wrote:

> I don't think the attachment came through in the list. Could you upload
> the results somewhere and link to them ?
>
>
> On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
>
>> 123 features per rows, and in average, 89% are zeros.
>> On Apr 23, 2014 9:31 PM, "Evan Sparks" <evan.sparks@gmail.com> wrote:
>>
>> > What is the number of non zeroes per row (and number of features) in the
>> > sparse case? We've hit some issues with breeze sparse support in the
>> past
>> > but for sufficiently sparse data it's still pretty good.
>> >
>> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>> > >
>> > > Hi all,
>> > >
>> > > I'm benchmarking Logistic Regression in MLlib using the newly added
>> > optimizer LBFGS and GD. I'm using the same dataset and the same
>> methodology
>> > in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>> > >
>> > > I want to know how Spark scale while adding workers, and how
>> optimizers
>> > and input format (sparse or dense) impact performance.
>> > >
>> > > The benchmark code can be found here,
>> > https://github.com/dbtsai/spark-lbfgs-benchmark
>> > >
>> > > The first dataset I benchmarked is a9a which only has 2.2MB. I
>> > duplicated the dataset, and made it 762MB to have 11M rows. This dataset
>> > has 123 features and 11% of the data are non-zero elements.
>> > >
>> > > In this benchmark, all the dataset is cached in memory.
>> > >
>> > > As we expect, LBFGS converges faster than GD, and at some point, no
>> > matter how we push GD, it will converge slower and slower.
>> > >
>> > > However, it's surprising that sparse format runs slower than dense
>> > format. I did see that sparse format takes significantly smaller amount
>> of
>> > memory in caching RDD, but sparse is 40% slower than dense. I think
>> sparse
>> > should be fast since when we compute x wT, since x is sparse, we can do
>> it
>> > faster. I wonder if there is anything I'm doing wrong.
>> > >
>> > > The attachment is the benchmark result.
>> > >
>> > > Thanks.
>> > >
>> > > Sincerely,
>> > >
>> > > DB Tsai
>> > > -------------------------------------------------------
>> > > My Blog: https://www.dbtsai.com
>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>> >
>>
>
>

--001a113b599054637f04f7c2d686--

From dev-return-7414-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 05:16:40 2014
Return-Path: <dev-return-7414-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2B1931148B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 05:16:40 +0000 (UTC)
Received: (qmail 14391 invoked by uid 500); 24 Apr 2014 05:16:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14260 invoked by uid 500); 24 Apr 2014 05:16:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14252 invoked by uid 99); 24 Apr 2014 05:16:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 05:16:37 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of david.lw.hall@gmail.com designates 74.125.82.178 as permitted sender)
Received: from [74.125.82.178] (HELO mail-we0-f178.google.com) (74.125.82.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 05:16:33 +0000
Received: by mail-we0-f178.google.com with SMTP id u56so1759370wes.9
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 22:16:12 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:cc:content-type;
        bh=Y751xO8rIzMWl/BdlJBWluON/+nmtuUigyxH8ReD+/w=;
        b=KZk569/oPoa6TopJ1IcsBf6yGsbJxou816oEffCCvk26xWxMC1FJzlk3w0JxvfR+AT
         /B9+wVB3QB0KmC5e25K01LCLFZuTto/yF836bRCduuLteHafLCYiXp/YRQ6Y3vzXVWLu
         z5ZnmOqME1AntB5yyqZs0WaoaeXOOid9R/Z1yoAfwWXZiI3ePRWsL5LbdGwe25NJJZWn
         netq8DcL8JcsAcoPfTZ3GuyoPJSIbtFtOTXxK+xFGbl9rO8zeaqh7vZP4oNFKXz3kDwf
         dNa3PK1v3vzIlA/O66xinpTwdHEBtsD0yVIllIhKo8XOZ6ktu5iPZJddo5hezi045IYw
         AOhA==
MIME-Version: 1.0
X-Received: by 10.180.7.198 with SMTP id l6mr4685277wia.52.1398316571869; Wed,
 23 Apr 2014 22:16:11 -0700 (PDT)
Sender: david.lw.hall@gmail.com
Received: by 10.227.232.207 with HTTP; Wed, 23 Apr 2014 22:16:11 -0700 (PDT)
In-Reply-To: <CAEYYnxa8=TP4YYM=H=H+e23qnW-edn7Xae+8Nb+=erFg6esBiQ@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxa8=TP4YYM=H=H+e23qnW-edn7Xae+8Nb+=erFg6esBiQ@mail.gmail.com>
Date: Wed, 23 Apr 2014 22:16:11 -0700
X-Google-Sender-Auth: mChiu8ShHclV0COzML8v5WYTd2k
Message-ID: <CALW2ey2iu44y5UjuumFoH0rqCqh90+zF=pZK4h-1XqV-=k2wDA@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: David Hall <dlwh@cs.berkeley.edu>
To: dev@spark.apache.org, dbtsai@dbtsai.com
Cc: shivaram@eecs.berkeley.edu, Xiangrui Meng <mengxr@gmail.com>
Content-Type: multipart/alternative; boundary=f46d044287c08cc6fd04f7c2f25f
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d044287c08cc6fd04f7c2f25f
Content-Type: text/plain; charset=UTF-8

Was the weight vector sparse? The gradients? Or just the feature vectors?


On Wed, Apr 23, 2014 at 10:08 PM, DB Tsai <dbtsai@dbtsai.com> wrote:

> The figure showing the Log-Likelihood vs Time can be found here.
>
>
> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
>
> Let me know if you can not open it.
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman <
> shivaram@eecs.berkeley.edu> wrote:
>
> > I don't think the attachment came through in the list. Could you upload
> > the results somewhere and link to them ?
> >
> >
> > On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
> >
> >> 123 features per rows, and in average, 89% are zeros.
> >> On Apr 23, 2014 9:31 PM, "Evan Sparks" <evan.sparks@gmail.com> wrote:
> >>
> >> > What is the number of non zeroes per row (and number of features) in
> the
> >> > sparse case? We've hit some issues with breeze sparse support in the
> >> past
> >> > but for sufficiently sparse data it's still pretty good.
> >> >
> >> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> >> > >
> >> > > Hi all,
> >> > >
> >> > > I'm benchmarking Logistic Regression in MLlib using the newly added
> >> > optimizer LBFGS and GD. I'm using the same dataset and the same
> >> methodology
> >> > in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
> >> > >
> >> > > I want to know how Spark scale while adding workers, and how
> >> optimizers
> >> > and input format (sparse or dense) impact performance.
> >> > >
> >> > > The benchmark code can be found here,
> >> > https://github.com/dbtsai/spark-lbfgs-benchmark
> >> > >
> >> > > The first dataset I benchmarked is a9a which only has 2.2MB. I
> >> > duplicated the dataset, and made it 762MB to have 11M rows. This
> dataset
> >> > has 123 features and 11% of the data are non-zero elements.
> >> > >
> >> > > In this benchmark, all the dataset is cached in memory.
> >> > >
> >> > > As we expect, LBFGS converges faster than GD, and at some point, no
> >> > matter how we push GD, it will converge slower and slower.
> >> > >
> >> > > However, it's surprising that sparse format runs slower than dense
> >> > format. I did see that sparse format takes significantly smaller
> amount
> >> of
> >> > memory in caching RDD, but sparse is 40% slower than dense. I think
> >> sparse
> >> > should be fast since when we compute x wT, since x is sparse, we can
> do
> >> it
> >> > faster. I wonder if there is anything I'm doing wrong.
> >> > >
> >> > > The attachment is the benchmark result.
> >> > >
> >> > > Thanks.
> >> > >
> >> > > Sincerely,
> >> > >
> >> > > DB Tsai
> >> > > -------------------------------------------------------
> >> > > My Blog: https://www.dbtsai.com
> >> > > LinkedIn: https://www.linkedin.com/in/dbtsai
> >> >
> >>
> >
> >
>

--f46d044287c08cc6fd04f7c2f25f--

From dev-return-7415-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 05:18:12 2014
Return-Path: <dev-return-7415-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1596D11492
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 05:18:12 +0000 (UTC)
Received: (qmail 16879 invoked by uid 500); 24 Apr 2014 05:18:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16842 invoked by uid 500); 24 Apr 2014 05:18:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16834 invoked by uid 99); 24 Apr 2014 05:18:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 05:18:09 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.171] (HELO mail-qc0-f171.google.com) (209.85.216.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 05:18:05 +0000
Received: by mail-qc0-f171.google.com with SMTP id c9so2022164qcz.30
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 22:17:44 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:sender:in-reply-to
         :references:date:message-id:subject:from:to:cc:content-type;
        bh=59J9XWUB97huaLTYUzkWIHAbzJMShW4hN8zLJK2BfQw=;
        b=m78q+pydHtfDdNMe8QDBYTOt6zpsMov/mbhHX1Kw4tl/OYt6SKXy8j4s0RxjFjC9u/
         Ur7Qn+LH+OhdaTl8j9oeWegQumT3ifEJMfUKwwSidD0s3WlNWjhsS7QHfqt4CKOtoxfB
         W5dk6PRwxxBpZBoIAISmMbCRtaNvk/ABB4XJ6/11MCWUqwX0j+vZJBDONFNshA5b56P3
         P//6FlwL/cbnoH9lWqBPG4lAoUwhuFbLY/YgyfyZciWyav62QmcUpuYg1lI3e4SDlSSX
         UhMPmJeUmf4N0GdqCgPEIqflajTJMFzeu6QKjXTxNOlyjz4BtpR8tZx7G48mMVoinCgz
         IY4A==
X-Gm-Message-State: ALoCoQl8dMiKbGHhd2lpT+KSMe0PkmxXO1Wfi8GdPJcldUWqHe5kqU8yQQzOP1V9X49Biq3jKUTH
MIME-Version: 1.0
X-Received: by 10.224.160.206 with SMTP id o14mr65350342qax.44.1398316664641;
 Wed, 23 Apr 2014 22:17:44 -0700 (PDT)
Reply-To: dbtsai@dbtsai.com
Sender: dbtsai@dbtsai.org
Received: by 10.229.96.201 with HTTP; Wed, 23 Apr 2014 22:17:44 -0700 (PDT)
In-Reply-To: <CALW2ey2iu44y5UjuumFoH0rqCqh90+zF=pZK4h-1XqV-=k2wDA@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxa8=TP4YYM=H=H+e23qnW-edn7Xae+8Nb+=erFg6esBiQ@mail.gmail.com>
	<CALW2ey2iu44y5UjuumFoH0rqCqh90+zF=pZK4h-1XqV-=k2wDA@mail.gmail.com>
Date: Wed, 23 Apr 2014 22:17:44 -0700
X-Google-Sender-Auth: dBbqJ8TiFQK9E_UAIPnNhgG81E8
Message-ID: <CAEYYnxZLe2DRox8zrCDDJmSW0RpT0d8xV5BB3tJ42DJ1Ht68xw@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: DB Tsai <dbtsai@dbtsai.com>
To: David Hall <dlwh@cs.berkeley.edu>
Cc: dev@spark.apache.org, shivaram@eecs.berkeley.edu, 
	Xiangrui Meng <mengxr@gmail.com>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

In mllib, the weight, and gradient are dense. Only feature is sparse.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Wed, Apr 23, 2014 at 10:16 PM, David Hall <dlwh@cs.berkeley.edu> wrote:
> Was the weight vector sparse? The gradients? Or just the feature vectors?
>
>
> On Wed, Apr 23, 2014 at 10:08 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
>>
>> The figure showing the Log-Likelihood vs Time can be found here.
>>
>>
>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
>>
>> Let me know if you can not open it.
>>
>> Sincerely,
>>
>> DB Tsai
>> -------------------------------------------------------
>> My Blog: https://www.dbtsai.com
>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>
>>
>> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman <
>> shivaram@eecs.berkeley.edu> wrote:
>>
>> > I don't think the attachment came through in the list. Could you upload
>> > the results somewhere and link to them ?
>> >
>> >
>> > On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
>> >
>> >> 123 features per rows, and in average, 89% are zeros.
>> >> On Apr 23, 2014 9:31 PM, "Evan Sparks" <evan.sparks@gmail.com> wrote:
>> >>
>> >> > What is the number of non zeroes per row (and number of features) in
>> >> > the
>> >> > sparse case? We've hit some issues with breeze sparse support in the
>> >> past
>> >> > but for sufficiently sparse data it's still pretty good.
>> >> >
>> >> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>> >> > >
>> >> > > Hi all,
>> >> > >
>> >> > > I'm benchmarking Logistic Regression in MLlib using the newly added
>> >> > optimizer LBFGS and GD. I'm using the same dataset and the same
>> >> methodology
>> >> > in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>> >> > >
>> >> > > I want to know how Spark scale while adding workers, and how
>> >> optimizers
>> >> > and input format (sparse or dense) impact performance.
>> >> > >
>> >> > > The benchmark code can be found here,
>> >> > https://github.com/dbtsai/spark-lbfgs-benchmark
>> >> > >
>> >> > > The first dataset I benchmarked is a9a which only has 2.2MB. I
>> >> > duplicated the dataset, and made it 762MB to have 11M rows. This
>> >> > dataset
>> >> > has 123 features and 11% of the data are non-zero elements.
>> >> > >
>> >> > > In this benchmark, all the dataset is cached in memory.
>> >> > >
>> >> > > As we expect, LBFGS converges faster than GD, and at some point, no
>> >> > matter how we push GD, it will converge slower and slower.
>> >> > >
>> >> > > However, it's surprising that sparse format runs slower than dense
>> >> > format. I did see that sparse format takes significantly smaller
>> >> > amount
>> >> of
>> >> > memory in caching RDD, but sparse is 40% slower than dense. I think
>> >> sparse
>> >> > should be fast since when we compute x wT, since x is sparse, we can
>> >> > do
>> >> it
>> >> > faster. I wonder if there is anything I'm doing wrong.
>> >> > >
>> >> > > The attachment is the benchmark result.
>> >> > >
>> >> > > Thanks.
>> >> > >
>> >> > > Sincerely,
>> >> > >
>> >> > > DB Tsai
>> >> > > -------------------------------------------------------
>> >> > > My Blog: https://www.dbtsai.com
>> >> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>> >> >
>> >>
>> >
>> >
>
>

From dev-return-7416-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 05:19:22 2014
Return-Path: <dev-return-7416-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6AACD11496
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 05:19:22 +0000 (UTC)
Received: (qmail 17957 invoked by uid 500); 24 Apr 2014 05:19:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17929 invoked by uid 500); 24 Apr 2014 05:19:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17921 invoked by uid 99); 24 Apr 2014 05:19:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 05:19:21 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.177] (HELO mail-qc0-f177.google.com) (209.85.216.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 05:19:17 +0000
Received: by mail-qc0-f177.google.com with SMTP id w7so1982763qcr.36
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 22:18:57 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:sender:in-reply-to
         :references:date:message-id:subject:from:to:cc:content-type;
        bh=0+fgiXleyp3fMJK4Q2jw6LC6JBHuPa/r4FqcLaWhKFc=;
        b=II05712x8eV4S6yfpG+tYHVqCvgmuMa/t7+FsRUtAZeiEr+dYBAIrM4gOn88RGC+61
         2gQa5VGwbdHeZH1rdo5XRmEBab7vnJoNpfmPk6H0SMG1GSwDbeIyoSdNl1uSIFKK5j4k
         kpS2rPACf9oGCfOD5ORKJ2qFSTSNiZ232yQc8BUTGYD42G/R5kKPHrjSFivl7dQPXBQ0
         w93ZehJro1cx3Vfc5qFJzU09RZscyrSls8Jm2SgAJblUZapH023DKrZpughIRBpT2kCF
         M39W0wJ3zXqnEEoxsDwTT9zuQjEu7v4RbsaLNqwXaGwKd6B1jLii1PfQNX8KGmWyYR4H
         fZRg==
X-Gm-Message-State: ALoCoQm/SjdJJkpsdfMxDruSQjANnR/sTafrhtYKMd+7NXs8LmPINjmJoUU+Urwz0/yvHMcYumBd
MIME-Version: 1.0
X-Received: by 10.140.90.84 with SMTP id w78mr66301377qgd.52.1398316737003;
 Wed, 23 Apr 2014 22:18:57 -0700 (PDT)
Reply-To: dbtsai@dbtsai.com
Sender: dbtsai@dbtsai.org
Received: by 10.229.96.201 with HTTP; Wed, 23 Apr 2014 22:18:56 -0700 (PDT)
In-Reply-To: <CAEYYnxZLe2DRox8zrCDDJmSW0RpT0d8xV5BB3tJ42DJ1Ht68xw@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxa8=TP4YYM=H=H+e23qnW-edn7Xae+8Nb+=erFg6esBiQ@mail.gmail.com>
	<CALW2ey2iu44y5UjuumFoH0rqCqh90+zF=pZK4h-1XqV-=k2wDA@mail.gmail.com>
	<CAEYYnxZLe2DRox8zrCDDJmSW0RpT0d8xV5BB3tJ42DJ1Ht68xw@mail.gmail.com>
Date: Wed, 23 Apr 2014 22:18:56 -0700
X-Google-Sender-Auth: RFAn5v1yiPaEnHiAU5IVPz0G1U8
Message-ID: <CAEYYnxYF_qpoJwNHf5w+0ZFG4X+=KmFVCF57P_Vu3wgeu-H5Fw@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: DB Tsai <dbtsai@dbtsai.com>
To: David Hall <dlwh@cs.berkeley.edu>
Cc: dev@spark.apache.org, shivaram@eecs.berkeley.edu, 
	Xiangrui Meng <mengxr@gmail.com>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

ps, it doesn't make sense to have weight and gradient sparse unless
with strong L1 penalty.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Wed, Apr 23, 2014 at 10:17 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
> In mllib, the weight, and gradient are dense. Only feature is sparse.
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Wed, Apr 23, 2014 at 10:16 PM, David Hall <dlwh@cs.berkeley.edu> wrote:
>> Was the weight vector sparse? The gradients? Or just the feature vectors?
>>
>>
>> On Wed, Apr 23, 2014 at 10:08 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
>>>
>>> The figure showing the Log-Likelihood vs Time can be found here.
>>>
>>>
>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
>>>
>>> Let me know if you can not open it.
>>>
>>> Sincerely,
>>>
>>> DB Tsai
>>> -------------------------------------------------------
>>> My Blog: https://www.dbtsai.com
>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>
>>>
>>> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman <
>>> shivaram@eecs.berkeley.edu> wrote:
>>>
>>> > I don't think the attachment came through in the list. Could you upload
>>> > the results somewhere and link to them ?
>>> >
>>> >
>>> > On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
>>> >
>>> >> 123 features per rows, and in average, 89% are zeros.
>>> >> On Apr 23, 2014 9:31 PM, "Evan Sparks" <evan.sparks@gmail.com> wrote:
>>> >>
>>> >> > What is the number of non zeroes per row (and number of features) in
>>> >> > the
>>> >> > sparse case? We've hit some issues with breeze sparse support in the
>>> >> past
>>> >> > but for sufficiently sparse data it's still pretty good.
>>> >> >
>>> >> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>> >> > >
>>> >> > > Hi all,
>>> >> > >
>>> >> > > I'm benchmarking Logistic Regression in MLlib using the newly added
>>> >> > optimizer LBFGS and GD. I'm using the same dataset and the same
>>> >> methodology
>>> >> > in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>>> >> > >
>>> >> > > I want to know how Spark scale while adding workers, and how
>>> >> optimizers
>>> >> > and input format (sparse or dense) impact performance.
>>> >> > >
>>> >> > > The benchmark code can be found here,
>>> >> > https://github.com/dbtsai/spark-lbfgs-benchmark
>>> >> > >
>>> >> > > The first dataset I benchmarked is a9a which only has 2.2MB. I
>>> >> > duplicated the dataset, and made it 762MB to have 11M rows. This
>>> >> > dataset
>>> >> > has 123 features and 11% of the data are non-zero elements.
>>> >> > >
>>> >> > > In this benchmark, all the dataset is cached in memory.
>>> >> > >
>>> >> > > As we expect, LBFGS converges faster than GD, and at some point, no
>>> >> > matter how we push GD, it will converge slower and slower.
>>> >> > >
>>> >> > > However, it's surprising that sparse format runs slower than dense
>>> >> > format. I did see that sparse format takes significantly smaller
>>> >> > amount
>>> >> of
>>> >> > memory in caching RDD, but sparse is 40% slower than dense. I think
>>> >> sparse
>>> >> > should be fast since when we compute x wT, since x is sparse, we can
>>> >> > do
>>> >> it
>>> >> > faster. I wonder if there is anything I'm doing wrong.
>>> >> > >
>>> >> > > The attachment is the benchmark result.
>>> >> > >
>>> >> > > Thanks.
>>> >> > >
>>> >> > > Sincerely,
>>> >> > >
>>> >> > > DB Tsai
>>> >> > > -------------------------------------------------------
>>> >> > > My Blog: https://www.dbtsai.com
>>> >> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>> >> >
>>> >>
>>> >
>>> >
>>
>>

From dev-return-7417-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 05:23:02 2014
Return-Path: <dev-return-7417-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EC330114A5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 05:23:02 +0000 (UTC)
Received: (qmail 22591 invoked by uid 500); 24 Apr 2014 05:23:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22546 invoked by uid 500); 24 Apr 2014 05:23:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22538 invoked by uid 99); 24 Apr 2014 05:23:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 05:23:01 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of david.lw.hall@gmail.com designates 209.85.212.182 as permitted sender)
Received: from [209.85.212.182] (HELO mail-wi0-f182.google.com) (209.85.212.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 05:22:57 +0000
Received: by mail-wi0-f182.google.com with SMTP id d1so462237wiv.15
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 22:22:35 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:cc:content-type;
        bh=tY9VhpKaddHPOdIKxA/FTRW9g+5+ab7rBSbIKB1wxr0=;
        b=HHLlajmwZAJDFsx5xakOMJ06IEX95hhhN5fdkl4+GJRjRob7baEsBY11kZw5Vfmmj6
         pKFHtCDdtly0ucNEaA0f9WqAoO0pvXWZwCckbt2SpmjTtO0h2k/ga8SgTrvGZQgolHKI
         Hy3AkX4c8BqNEffuR35G8V0pAu9X+/d5CpKT0a1WKS3XqPMj9ocLfL4U/H+ZOJj3q9iT
         aV1QOwtDwOSohLHPN7BUdJzBhWdofxPHlvoRexiOHZr63tl13WbFrzLbC5Bt9B0ydDas
         tiNVCgEW9nHk6dWENXNPSSSu5FO0dzaQZttaa3dcNL1R383eHhv4nuJ9veBW44oDWv77
         jcdQ==
MIME-Version: 1.0
X-Received: by 10.194.80.7 with SMTP id n7mr42057346wjx.8.1398316955279; Wed,
 23 Apr 2014 22:22:35 -0700 (PDT)
Sender: david.lw.hall@gmail.com
Received: by 10.227.232.207 with HTTP; Wed, 23 Apr 2014 22:22:35 -0700 (PDT)
In-Reply-To: <CAEYYnxYF_qpoJwNHf5w+0ZFG4X+=KmFVCF57P_Vu3wgeu-H5Fw@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxa8=TP4YYM=H=H+e23qnW-edn7Xae+8Nb+=erFg6esBiQ@mail.gmail.com>
	<CALW2ey2iu44y5UjuumFoH0rqCqh90+zF=pZK4h-1XqV-=k2wDA@mail.gmail.com>
	<CAEYYnxZLe2DRox8zrCDDJmSW0RpT0d8xV5BB3tJ42DJ1Ht68xw@mail.gmail.com>
	<CAEYYnxYF_qpoJwNHf5w+0ZFG4X+=KmFVCF57P_Vu3wgeu-H5Fw@mail.gmail.com>
Date: Wed, 23 Apr 2014 22:22:35 -0700
X-Google-Sender-Auth: ZI5xu1ezAvQYTamhJLZhj4Sp-4c
Message-ID: <CALW2ey1sU7aU-H8k4T1T2oXSjwvkUQwVRNcCMmsFB6MLjWn4XQ@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: David Hall <dlwh@cs.berkeley.edu>
To: dbtsai@dbtsai.com
Cc: dev@spark.apache.org, shivaram@eecs.berkeley.edu, 
	Xiangrui Meng <mengxr@gmail.com>
Content-Type: multipart/alternative; boundary=047d7beb9c80672ef404f7c309fa
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7beb9c80672ef404f7c309fa
Content-Type: text/plain; charset=UTF-8

On Wed, Apr 23, 2014 at 10:18 PM, DB Tsai <dbtsai@dbtsai.com> wrote:

> ps, it doesn't make sense to have weight and gradient sparse unless
> with strong L1 penalty.
>

Sure, I was just checking the obvious things. Have you run it through it a
profiler to see where the problem is?



>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Wed, Apr 23, 2014 at 10:17 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
> > In mllib, the weight, and gradient are dense. Only feature is sparse.
> >
> > Sincerely,
> >
> > DB Tsai
> > -------------------------------------------------------
> > My Blog: https://www.dbtsai.com
> > LinkedIn: https://www.linkedin.com/in/dbtsai
> >
> >
> > On Wed, Apr 23, 2014 at 10:16 PM, David Hall <dlwh@cs.berkeley.edu>
> wrote:
> >> Was the weight vector sparse? The gradients? Or just the feature
> vectors?
> >>
> >>
> >> On Wed, Apr 23, 2014 at 10:08 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
> >>>
> >>> The figure showing the Log-Likelihood vs Time can be found here.
> >>>
> >>>
> >>>
> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
> >>>
> >>> Let me know if you can not open it.
> >>>
> >>> Sincerely,
> >>>
> >>> DB Tsai
> >>> -------------------------------------------------------
> >>> My Blog: https://www.dbtsai.com
> >>> LinkedIn: https://www.linkedin.com/in/dbtsai
> >>>
> >>>
> >>> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman <
> >>> shivaram@eecs.berkeley.edu> wrote:
> >>>
> >>> > I don't think the attachment came through in the list. Could you
> upload
> >>> > the results somewhere and link to them ?
> >>> >
> >>> >
> >>> > On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
> >>> >
> >>> >> 123 features per rows, and in average, 89% are zeros.
> >>> >> On Apr 23, 2014 9:31 PM, "Evan Sparks" <evan.sparks@gmail.com>
> wrote:
> >>> >>
> >>> >> > What is the number of non zeroes per row (and number of features)
> in
> >>> >> > the
> >>> >> > sparse case? We've hit some issues with breeze sparse support in
> the
> >>> >> past
> >>> >> > but for sufficiently sparse data it's still pretty good.
> >>> >> >
> >>> >> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu>
> wrote:
> >>> >> > >
> >>> >> > > Hi all,
> >>> >> > >
> >>> >> > > I'm benchmarking Logistic Regression in MLlib using the newly
> added
> >>> >> > optimizer LBFGS and GD. I'm using the same dataset and the same
> >>> >> methodology
> >>> >> > in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
> >>> >> > >
> >>> >> > > I want to know how Spark scale while adding workers, and how
> >>> >> optimizers
> >>> >> > and input format (sparse or dense) impact performance.
> >>> >> > >
> >>> >> > > The benchmark code can be found here,
> >>> >> > https://github.com/dbtsai/spark-lbfgs-benchmark
> >>> >> > >
> >>> >> > > The first dataset I benchmarked is a9a which only has 2.2MB. I
> >>> >> > duplicated the dataset, and made it 762MB to have 11M rows. This
> >>> >> > dataset
> >>> >> > has 123 features and 11% of the data are non-zero elements.
> >>> >> > >
> >>> >> > > In this benchmark, all the dataset is cached in memory.
> >>> >> > >
> >>> >> > > As we expect, LBFGS converges faster than GD, and at some
> point, no
> >>> >> > matter how we push GD, it will converge slower and slower.
> >>> >> > >
> >>> >> > > However, it's surprising that sparse format runs slower than
> dense
> >>> >> > format. I did see that sparse format takes significantly smaller
> >>> >> > amount
> >>> >> of
> >>> >> > memory in caching RDD, but sparse is 40% slower than dense. I
> think
> >>> >> sparse
> >>> >> > should be fast since when we compute x wT, since x is sparse, we
> can
> >>> >> > do
> >>> >> it
> >>> >> > faster. I wonder if there is anything I'm doing wrong.
> >>> >> > >
> >>> >> > > The attachment is the benchmark result.
> >>> >> > >
> >>> >> > > Thanks.
> >>> >> > >
> >>> >> > > Sincerely,
> >>> >> > >
> >>> >> > > DB Tsai
> >>> >> > > -------------------------------------------------------
> >>> >> > > My Blog: https://www.dbtsai.com
> >>> >> > > LinkedIn: https://www.linkedin.com/in/dbtsai
> >>> >> >
> >>> >>
> >>> >
> >>> >
> >>
> >>
>

--047d7beb9c80672ef404f7c309fa--

From dev-return-7418-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 05:34:11 2014
Return-Path: <dev-return-7418-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EF77511500
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 05:34:11 +0000 (UTC)
Received: (qmail 36952 invoked by uid 500); 24 Apr 2014 05:34:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36917 invoked by uid 500); 24 Apr 2014 05:34:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36909 invoked by uid 99); 24 Apr 2014 05:34:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 05:34:09 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.45] (HELO mail-qg0-f45.google.com) (209.85.192.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 05:34:06 +0000
Received: by mail-qg0-f45.google.com with SMTP id a108so2012469qge.32
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 22:33:45 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:sender:in-reply-to
         :references:date:message-id:subject:from:to:cc:content-type;
        bh=5iQvcxRwQiLbYDAllwpDWukbscO5UOtHs/Udcqww6yo=;
        b=l5eb3UGzRvfCOUXSNFeC6qr++hwtXt03hC34YmOhjPcmIxmULpf5b1Vh7FEPiIeFww
         IetvcpVsMFESUMsjIYUaRBD11J/bkB8Utl+mgP4V0cFtnqFmQpFg2+uS5b69PvCY4Yf7
         dFqUzAjN+YvfdFR/kVqEpEJ+oBxPY51bzG8CPBwymb/UvYbLn5hj0yKvYtCtfJCLfVQV
         KyHIf4520SgPGEqwd7j/+sGWpGIZ2c7h3DQJcDD9BirNLWidBb+X8ZLSCeAR87v+jV/k
         goR5Ycy/b2SF0/HDCPxbRkzVWd9m83N9VcOxBb+JQaIPYo0oedPIbX3pXFSA6XLINQ0G
         O0mA==
X-Gm-Message-State: ALoCoQmAAv/ONXJfCiNKR9TW/i/7guDOPf7cDsYIgQ6sSS/Et+W5VUU/ltbEgmIkIvb8y+5OvFJf
MIME-Version: 1.0
X-Received: by 10.224.30.131 with SMTP id u3mr63648828qac.50.1398317625200;
 Wed, 23 Apr 2014 22:33:45 -0700 (PDT)
Reply-To: dbtsai@dbtsai.com
Sender: dbtsai@dbtsai.org
Received: by 10.229.96.201 with HTTP; Wed, 23 Apr 2014 22:33:45 -0700 (PDT)
In-Reply-To: <CALW2ey1sU7aU-H8k4T1T2oXSjwvkUQwVRNcCMmsFB6MLjWn4XQ@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxa8=TP4YYM=H=H+e23qnW-edn7Xae+8Nb+=erFg6esBiQ@mail.gmail.com>
	<CALW2ey2iu44y5UjuumFoH0rqCqh90+zF=pZK4h-1XqV-=k2wDA@mail.gmail.com>
	<CAEYYnxZLe2DRox8zrCDDJmSW0RpT0d8xV5BB3tJ42DJ1Ht68xw@mail.gmail.com>
	<CAEYYnxYF_qpoJwNHf5w+0ZFG4X+=KmFVCF57P_Vu3wgeu-H5Fw@mail.gmail.com>
	<CALW2ey1sU7aU-H8k4T1T2oXSjwvkUQwVRNcCMmsFB6MLjWn4XQ@mail.gmail.com>
Date: Wed, 23 Apr 2014 22:33:45 -0700
X-Google-Sender-Auth: jgPt_gD0Fb38a-hE1862WyQBSUQ
Message-ID: <CAEYYnxbFrsfzhyzCeSmJvv-9S0XPpPQaNBFj4Ei2cnUnDugHgg@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: DB Tsai <dbtsai@dbtsai.com>
To: David Hall <dlwh@cs.berkeley.edu>
Cc: dev@spark.apache.org, shivaram@eecs.berkeley.edu, 
	Xiangrui Meng <mengxr@gmail.com>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Not yet since it's running in the cluster. Will run locally with
profiler. Thanks for help.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Wed, Apr 23, 2014 at 10:22 PM, David Hall <dlwh@cs.berkeley.edu> wrote:
> On Wed, Apr 23, 2014 at 10:18 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
>>
>> ps, it doesn't make sense to have weight and gradient sparse unless
>> with strong L1 penalty.
>
>
> Sure, I was just checking the obvious things. Have you run it through it a
> profiler to see where the problem is?
>
>
>>
>>
>> Sincerely,
>>
>> DB Tsai
>> -------------------------------------------------------
>> My Blog: https://www.dbtsai.com
>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>
>>
>> On Wed, Apr 23, 2014 at 10:17 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
>> > In mllib, the weight, and gradient are dense. Only feature is sparse.
>> >
>> > Sincerely,
>> >
>> > DB Tsai
>> > -------------------------------------------------------
>> > My Blog: https://www.dbtsai.com
>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>> >
>> >
>> > On Wed, Apr 23, 2014 at 10:16 PM, David Hall <dlwh@cs.berkeley.edu>
>> > wrote:
>> >> Was the weight vector sparse? The gradients? Or just the feature
>> >> vectors?
>> >>
>> >>
>> >> On Wed, Apr 23, 2014 at 10:08 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
>> >>>
>> >>> The figure showing the Log-Likelihood vs Time can be found here.
>> >>>
>> >>>
>> >>>
>> >>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
>> >>>
>> >>> Let me know if you can not open it.
>> >>>
>> >>> Sincerely,
>> >>>
>> >>> DB Tsai
>> >>> -------------------------------------------------------
>> >>> My Blog: https://www.dbtsai.com
>> >>> LinkedIn: https://www.linkedin.com/in/dbtsai
>> >>>
>> >>>
>> >>> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman <
>> >>> shivaram@eecs.berkeley.edu> wrote:
>> >>>
>> >>> > I don't think the attachment came through in the list. Could you
>> >>> > upload
>> >>> > the results somewhere and link to them ?
>> >>> >
>> >>> >
>> >>> > On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
>> >>> >
>> >>> >> 123 features per rows, and in average, 89% are zeros.
>> >>> >> On Apr 23, 2014 9:31 PM, "Evan Sparks" <evan.sparks@gmail.com>
>> >>> >> wrote:
>> >>> >>
>> >>> >> > What is the number of non zeroes per row (and number of features)
>> >>> >> > in
>> >>> >> > the
>> >>> >> > sparse case? We've hit some issues with breeze sparse support in
>> >>> >> > the
>> >>> >> past
>> >>> >> > but for sufficiently sparse data it's still pretty good.
>> >>> >> >
>> >>> >> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu>
>> >>> >> > > wrote:
>> >>> >> > >
>> >>> >> > > Hi all,
>> >>> >> > >
>> >>> >> > > I'm benchmarking Logistic Regression in MLlib using the newly
>> >>> >> > > added
>> >>> >> > optimizer LBFGS and GD. I'm using the same dataset and the same
>> >>> >> methodology
>> >>> >> > in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>> >>> >> > >
>> >>> >> > > I want to know how Spark scale while adding workers, and how
>> >>> >> optimizers
>> >>> >> > and input format (sparse or dense) impact performance.
>> >>> >> > >
>> >>> >> > > The benchmark code can be found here,
>> >>> >> > https://github.com/dbtsai/spark-lbfgs-benchmark
>> >>> >> > >
>> >>> >> > > The first dataset I benchmarked is a9a which only has 2.2MB. I
>> >>> >> > duplicated the dataset, and made it 762MB to have 11M rows. This
>> >>> >> > dataset
>> >>> >> > has 123 features and 11% of the data are non-zero elements.
>> >>> >> > >
>> >>> >> > > In this benchmark, all the dataset is cached in memory.
>> >>> >> > >
>> >>> >> > > As we expect, LBFGS converges faster than GD, and at some
>> >>> >> > > point, no
>> >>> >> > matter how we push GD, it will converge slower and slower.
>> >>> >> > >
>> >>> >> > > However, it's surprising that sparse format runs slower than
>> >>> >> > > dense
>> >>> >> > format. I did see that sparse format takes significantly smaller
>> >>> >> > amount
>> >>> >> of
>> >>> >> > memory in caching RDD, but sparse is 40% slower than dense. I
>> >>> >> > think
>> >>> >> sparse
>> >>> >> > should be fast since when we compute x wT, since x is sparse, we
>> >>> >> > can
>> >>> >> > do
>> >>> >> it
>> >>> >> > faster. I wonder if there is anything I'm doing wrong.
>> >>> >> > >
>> >>> >> > > The attachment is the benchmark result.
>> >>> >> > >
>> >>> >> > > Thanks.
>> >>> >> > >
>> >>> >> > > Sincerely,
>> >>> >> > >
>> >>> >> > > DB Tsai
>> >>> >> > > -------------------------------------------------------
>> >>> >> > > My Blog: https://www.dbtsai.com
>> >>> >> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>> >>> >> >
>> >>> >>
>> >>> >
>> >>> >
>> >>
>> >>
>
>

From dev-return-7419-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 05:35:44 2014
Return-Path: <dev-return-7419-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3893611512
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 05:35:44 +0000 (UTC)
Received: (qmail 41393 invoked by uid 500); 24 Apr 2014 05:35:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41166 invoked by uid 500); 24 Apr 2014 05:35:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41158 invoked by uid 99); 24 Apr 2014 05:35:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 05:35:41 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.81 as permitted sender)
Received: from [171.67.219.81] (HELO smtp.stanford.edu) (171.67.219.81)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 05:35:37 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 67FE622781
	for <dev@spark.apache.org>; Wed, 23 Apr 2014 22:35:17 -0700 (PDT)
Received: from mail-qa0-f44.google.com (mail-qa0-f44.google.com [209.85.216.44])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 9135922780
	for <dev@spark.apache.org>; Wed, 23 Apr 2014 22:35:16 -0700 (PDT)
Received: by mail-qa0-f44.google.com with SMTP id hw13so1843119qab.3
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 22:35:14 -0700 (PDT)
X-Gm-Message-State: ALoCoQnLVmGJB5AGtUhcZRiIeEas4OJtb4jHsvTduVc/rZypPcIo866OBqQ8/ULA2z9tcw0dUUrn
MIME-Version: 1.0
X-Received: by 10.224.166.210 with SMTP id n18mr64849146qay.6.1398317714957;
 Wed, 23 Apr 2014 22:35:14 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Wed, 23 Apr 2014 22:35:14 -0700 (PDT)
In-Reply-To: <CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
Date: Wed, 23 Apr 2014 22:35:14 -0700
Message-ID: <CAEYYnxZFmxXkrNhdUNF3je7k0Eg_YjY2WnvVW5SSk__FAWNz4w@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: DB Tsai <dbtsai@stanford.edu>
To: shivaram@eecs.berkeley.edu
Cc: dev@spark.apache.org, Xiangrui Meng <mengxr@gmail.com>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

The figure showing the Log-Likelihood vs Time can be found here.

https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf

Let me know if you can not open it. Thanks.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman
<shivaram@eecs.berkeley.edu> wrote:
> I don't think the attachment came through in the list. Could you upload the
> results somewhere and link to them ?
>
>
> On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
>>
>> 123 features per rows, and in average, 89% are zeros.
>> On Apr 23, 2014 9:31 PM, "Evan Sparks" <evan.sparks@gmail.com> wrote:
>>
>> > What is the number of non zeroes per row (and number of features) in the
>> > sparse case? We've hit some issues with breeze sparse support in the
>> > past
>> > but for sufficiently sparse data it's still pretty good.
>> >
>> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>> > >
>> > > Hi all,
>> > >
>> > > I'm benchmarking Logistic Regression in MLlib using the newly added
>> > optimizer LBFGS and GD. I'm using the same dataset and the same
>> > methodology
>> > in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>> > >
>> > > I want to know how Spark scale while adding workers, and how
>> > > optimizers
>> > and input format (sparse or dense) impact performance.
>> > >
>> > > The benchmark code can be found here,
>> > https://github.com/dbtsai/spark-lbfgs-benchmark
>> > >
>> > > The first dataset I benchmarked is a9a which only has 2.2MB. I
>> > duplicated the dataset, and made it 762MB to have 11M rows. This dataset
>> > has 123 features and 11% of the data are non-zero elements.
>> > >
>> > > In this benchmark, all the dataset is cached in memory.
>> > >
>> > > As we expect, LBFGS converges faster than GD, and at some point, no
>> > matter how we push GD, it will converge slower and slower.
>> > >
>> > > However, it's surprising that sparse format runs slower than dense
>> > format. I did see that sparse format takes significantly smaller amount
>> > of
>> > memory in caching RDD, but sparse is 40% slower than dense. I think
>> > sparse
>> > should be fast since when we compute x wT, since x is sparse, we can do
>> > it
>> > faster. I wonder if there is anything I'm doing wrong.
>> > >
>> > > The attachment is the benchmark result.
>> > >
>> > > Thanks.
>> > >
>> > > Sincerely,
>> > >
>> > > DB Tsai
>> > > -------------------------------------------------------
>> > > My Blog: https://www.dbtsai.com
>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>> >
>
>

From dev-return-7420-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 05:48:51 2014
Return-Path: <dev-return-7420-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C210E11584
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 05:48:51 +0000 (UTC)
Received: (qmail 65985 invoked by uid 500); 24 Apr 2014 05:48:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65349 invoked by uid 500); 24 Apr 2014 05:48:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65341 invoked by uid 99); 24 Apr 2014 05:48:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 05:48:49 +0000
X-ASF-Spam-Status: No, hits=3.1 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of liqingyang1985@gmail.com designates 74.125.82.177 as permitted sender)
Received: from [74.125.82.177] (HELO mail-we0-f177.google.com) (74.125.82.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 05:48:46 +0000
Received: by mail-we0-f177.google.com with SMTP id u57so1731215wes.22
        for <dev@spark.apache.org>; Wed, 23 Apr 2014 22:48:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=Fs+1XwhJUPjYsIYLn0yFR8XQuoL4Ep6qW4+1V5BY4dA=;
        b=bw+uYzfHs/9JeCNQSOsWus08jMptWu5LGT9cnmb+uNdBXDMd8Mtlx/2kQ9IR3dcxpC
         TKUpCN32ksIfOMRawRyqgWMVNajS2qjYQAs2UL6Ca2vTlc6Odt5K6pprQmv5dwoVkPNO
         IWcF9PsetsO9uyhEgqOSt6JUeklT+UbMnsOVcAVgl2FsODZ6nkAxTg6tv5lmpoROjG6k
         i5XeULl2dMlh7VTZiZ/uUErDRmwckEcCaRaSmCrC93ffqWMmfxS3FtuxKUftZg49xcBk
         yZ/kkQgdribAhYIM5sHbBeQ6OhEmQ9gv75P4z3ckE0+eBYZA5ZTnPHsbpQDt/w/fwIdy
         44Eg==
MIME-Version: 1.0
X-Received: by 10.194.60.146 with SMTP id h18mr39728207wjr.26.1398318503679;
 Wed, 23 Apr 2014 22:48:23 -0700 (PDT)
Received: by 10.194.61.39 with HTTP; Wed, 23 Apr 2014 22:48:23 -0700 (PDT)
In-Reply-To: <1398302801119-6382.post@n3.nabble.com>
References: <CABDsqqanKD04qPcdFpUgEy1ybU=5xgO37DfQFAoBmg=wzKtGbw@mail.gmail.com>
	<1398302801119-6382.post@n3.nabble.com>
Date: Thu, 24 Apr 2014 13:48:23 +0800
Message-ID: <CABDsqqae3t9f5ivDWsvRx+3EKg+15HL+0GrUem+s4QAURbVAGg@mail.gmail.com>
Subject: Re: get -101 error code when running select query
From: qingyang li <liqingyang1985@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b86dc40b1db3b04f7c365a5
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b86dc40b1db3b04f7c365a5
Content-Type: text/plain; charset=UTF-8

thanks for sharing,  my case is diffrent from yours,
i have set hive.server2.enable.doAs into false in  hive-site.xml,  then
that 101 error code disappeared.



2014-04-24 9:26 GMT+08:00 Madhu <madhu@madhu.com>:

> I have seen a similar error message when connecting to Hive through JDBC.
> This is just a guess on my part, but check your query. The error occurs if
> you have a select that includes a null literal with an alias like this:
>
> select a, b, null as c, d from foo
>
> In my case, rewriting the query to use an empty string or other literal
> instead of null worked:
>
> select a, b, '' as c, d from foo
>
> I think the problem is the lack of type information when supplying a null
> literal.
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/get-101-error-code-when-running-select-query-tp6377p6382.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--047d7b86dc40b1db3b04f7c365a5--

From dev-return-7421-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 07:54:17 2014
Return-Path: <dev-return-7421-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8EEA911926
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 07:54:17 +0000 (UTC)
Received: (qmail 60876 invoked by uid 500); 24 Apr 2014 07:54:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60359 invoked by uid 500); 24 Apr 2014 07:54:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60337 invoked by uid 99); 24 Apr 2014 07:54:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 07:54:09 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 209.85.212.179 as permitted sender)
Received: from [209.85.212.179] (HELO mail-wi0-f179.google.com) (209.85.212.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 07:54:05 +0000
Received: by mail-wi0-f179.google.com with SMTP id z2so603586wiv.12
        for <dev@spark.apache.org>; Thu, 24 Apr 2014 00:53:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=dMwE1wTO65DPoAE5Y2Ke/E2LmT6IIjhnD0H8cyy8NeA=;
        b=N9MNCcUFkOCkpCmsva4mZ8vvQsgvGlhO/0nlc/CpGhMoLuUSvKFDWzxrNRIjXnIx1S
         O7N3rO+zArXoIIolk76V+LOvoRZ879cXh7jHS3Q6B5VQl6498M/vZ9c19HWxX3hbADs/
         ZoyRbRvYQ8kR52cp1yVp8UGn2mRcvemrrlSKbzbqPdxJpQGDOTMwOI1OBnezMaOIFYch
         Ea84WMlTPKsm0XBB6QqzTedAq091EeL4BB/Hc2UioZL44f2Szf7E4VY/7e1Wsp/1IE4D
         9UtX1i6+Cxl+xVs+ivb2Jclb+cl4hI6UZph+szhTyCtgtid4SCmLqtYKQFnMFgsJgW2S
         2o7A==
MIME-Version: 1.0
X-Received: by 10.180.81.40 with SMTP id w8mr1476659wix.45.1398326024540; Thu,
 24 Apr 2014 00:53:44 -0700 (PDT)
Received: by 10.194.82.105 with HTTP; Thu, 24 Apr 2014 00:53:44 -0700 (PDT)
In-Reply-To: <CAEYYnxZFmxXkrNhdUNF3je7k0Eg_YjY2WnvVW5SSk__FAWNz4w@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxZFmxXkrNhdUNF3je7k0Eg_YjY2WnvVW5SSk__FAWNz4w@mail.gmail.com>
Date: Thu, 24 Apr 2014 00:53:44 -0700
Message-ID: <CAJgQjQ-k5P3CxVFCE-pHtuH9fVwwCcPc6PBp1NgxM2uueOASsw@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: Xiangrui Meng <mengxr@gmail.com>
To: DB Tsai <dbtsai@stanford.edu>, David Hall <dlwh@cs.berkeley.edu>
Cc: shivaram@eecs.berkeley.edu, dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I don't think it is easy to make sparse faster than dense with this
sparsity and feature dimension. You can try rcv1.binary, which should
show the difference easily.

David, the breeze operators used here are

1. DenseVector dot SparseVector
2. axpy DenseVector SparseVector

However, the SparseVector is passed in as Vector[Double] instead of
SparseVector[Double]. It might use the axpy impl of [DenseVector,
Vector] and call activeIterator. I didn't check whether you used
multimethods on axpy.

Best,
Xiangrui

On Wed, Apr 23, 2014 at 10:35 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> The figure showing the Log-Likelihood vs Time can be found here.
>
> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
>
> Let me know if you can not open it. Thanks.
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman
> <shivaram@eecs.berkeley.edu> wrote:
>> I don't think the attachment came through in the list. Could you upload the
>> results somewhere and link to them ?
>>
>>
>> On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
>>>
>>> 123 features per rows, and in average, 89% are zeros.
>>> On Apr 23, 2014 9:31 PM, "Evan Sparks" <evan.sparks@gmail.com> wrote:
>>>
>>> > What is the number of non zeroes per row (and number of features) in the
>>> > sparse case? We've hit some issues with breeze sparse support in the
>>> > past
>>> > but for sufficiently sparse data it's still pretty good.
>>> >
>>> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>> > >
>>> > > Hi all,
>>> > >
>>> > > I'm benchmarking Logistic Regression in MLlib using the newly added
>>> > optimizer LBFGS and GD. I'm using the same dataset and the same
>>> > methodology
>>> > in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>>> > >
>>> > > I want to know how Spark scale while adding workers, and how
>>> > > optimizers
>>> > and input format (sparse or dense) impact performance.
>>> > >
>>> > > The benchmark code can be found here,
>>> > https://github.com/dbtsai/spark-lbfgs-benchmark
>>> > >
>>> > > The first dataset I benchmarked is a9a which only has 2.2MB. I
>>> > duplicated the dataset, and made it 762MB to have 11M rows. This dataset
>>> > has 123 features and 11% of the data are non-zero elements.
>>> > >
>>> > > In this benchmark, all the dataset is cached in memory.
>>> > >
>>> > > As we expect, LBFGS converges faster than GD, and at some point, no
>>> > matter how we push GD, it will converge slower and slower.
>>> > >
>>> > > However, it's surprising that sparse format runs slower than dense
>>> > format. I did see that sparse format takes significantly smaller amount
>>> > of
>>> > memory in caching RDD, but sparse is 40% slower than dense. I think
>>> > sparse
>>> > should be fast since when we compute x wT, since x is sparse, we can do
>>> > it
>>> > faster. I wonder if there is anything I'm doing wrong.
>>> > >
>>> > > The attachment is the benchmark result.
>>> > >
>>> > > Thanks.
>>> > >
>>> > > Sincerely,
>>> > >
>>> > > DB Tsai
>>> > > -------------------------------------------------------
>>> > > My Blog: https://www.dbtsai.com
>>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>> >
>>
>>

From dev-return-7422-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 08:03:14 2014
Return-Path: <dev-return-7422-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 372CF11965
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 08:03:14 +0000 (UTC)
Received: (qmail 73136 invoked by uid 500); 24 Apr 2014 08:03:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72911 invoked by uid 500); 24 Apr 2014 08:03:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72903 invoked by uid 99); 24 Apr 2014 08:03:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 08:03:12 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.49 as permitted sender)
Received: from [209.85.219.49] (HELO mail-oa0-f49.google.com) (209.85.219.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 08:03:08 +0000
Received: by mail-oa0-f49.google.com with SMTP id o6so2247056oag.36
        for <dev@spark.apache.org>; Thu, 24 Apr 2014 01:02:45 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=E23f16BFCkLbSxi6a+NCKIpJ8Qs0HqFZjAr+IGlBG90=;
        b=uudtec/mDsvN6YpEb1+Pncn7IpP9QyoGc3ODNO/8c3TXF8GgryJXoEKwcV6aCuS9bN
         ye9DB5IteWndJAOEbO2Hel8DwMJYMAwL7hbqAU4CarRte5fecLwjxhzDHBsSbXjVTnXh
         S4EepX9TOI1ZXJ+R6HE7FfYHK+wl6KPiPPXtiH6OvGWsz34qqQuhpDEte/M3bUqUf9sh
         neWdy/G7CLrk9YW8Js6Z602Zb+CMxspI76LUx9OKRi37Iw6WBYxifP7UQu9wzSPK2JoL
         ngZIIQQQKQ8iPATQyhZ461Svrzu5PvHO8bLlD6B1ns/bQ9j17YHRLBc/ADwedVoDYqrA
         2zmQ==
MIME-Version: 1.0
X-Received: by 10.182.96.168 with SMTP id dt8mr232929obb.43.1398326565169;
 Thu, 24 Apr 2014 01:02:45 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Thu, 24 Apr 2014 01:02:45 -0700 (PDT)
In-Reply-To: <8852B2350EE348D9A58E043BAD67DF79@gmail.com>
References: <2B39597F59C94379B2A2498D2A7A9A9B@gmail.com>
	<CAPh_B=YvSCiwWJUqXOfT6aPPT7MX1tNtdYmRS3C-aiLWmx-HoA@mail.gmail.com>
	<8852B2350EE348D9A58E043BAD67DF79@gmail.com>
Date: Thu, 24 Apr 2014 01:02:45 -0700
Message-ID: <CABPQxsstPMMhrULigesZi-vOpug6SW6Gnw=cjgJrJ9uetNnJvw@mail.gmail.com>
Subject: Re: Fw: Is there any way to make a quick test on some pre-commit code?
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b2e489a32712204f7c54628
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b2e489a32712204f7c54628
Content-Type: text/plain; charset=ISO-8859-1

This is already on the wiki:

https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools



On Wed, Apr 23, 2014 at 6:52 PM, Nan Zhu <zhunanmcgill@gmail.com> wrote:

> I'm just asked by others for the same question
>
> I think Reynold gave a pretty helpful tip on this,
>
> Shall we put this on Contribute-to-Spark wiki?
>
> --
> Nan Zhu
>
>
> Forwarded message:
>
> > From: Reynold Xin <rxin@databricks.com>
> > Reply To: dev@spark.incubator.apache.org
> > To: dev@spark.incubator.apache.org <dev@spark.incubator.apache.org>
> > Date: Thursday, February 6, 2014 at 7:50:57 PM
> > Subject: Re: Is there any way to make a quick test on some pre-commit
> code?
> >
> > You can do
> >
> > sbt/sbt assemble-deps
> >
> >
> > and then just run
> >
> > sbt/sbt package
> >
> > each time.
> >
> >
> > You can even do
> >
> > sbt/sbt ~package
> >
> > for automatic incremental compilation.
> >
> >
> >
> > On Thu, Feb 6, 2014 at 4:46 PM, Nan Zhu <zhunanmcgill@gmail.com (mailto:
> zhunanmcgill@gmail.com)> wrote:
> >
> > > Hi, all
> > >
> > > Is it always necessary to run sbt assembly when you want to test some
> code,
> > >
> > > Sometimes you just repeatedly change one or two lines for some failed
> test
> > > case, it is really time-consuming to sbt assembly every time
> > >
> > > any faster way?
> > >
> > > Best,
> > >
> > > --
> > > Nan Zhu
> > >
> >
> >
> >
> >
>
>
>

--047d7b2e489a32712204f7c54628--

From dev-return-7423-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 08:10:21 2014
Return-Path: <dev-return-7423-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E2426119B1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 08:10:21 +0000 (UTC)
Received: (qmail 84860 invoked by uid 500); 24 Apr 2014 08:10:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84621 invoked by uid 500); 24 Apr 2014 08:10:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84610 invoked by uid 99); 24 Apr 2014 08:10:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 08:10:19 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 209.85.212.177 as permitted sender)
Received: from [209.85.212.177] (HELO mail-wi0-f177.google.com) (209.85.212.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 08:10:16 +0000
Received: by mail-wi0-f177.google.com with SMTP id cc10so624457wib.10
        for <dev@spark.apache.org>; Thu, 24 Apr 2014 01:09:54 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=mu35NkmvcKqJeaQwceg4uMS2Jy3iKuT9zvNllCrgsLo=;
        b=Ji4bw8XQIhxNO79pMzGzCF3qvLBQeXldcSdU5ljQGmhr5WiousdY/kTYvwntvADIL8
         y3ysVaBAQEE2/ag+CZV+TyILejyV9EYcq4lyMkMIeZ1WVYDuLkhAm9UkilHxM+zkaXwX
         +iUlmWuB3VQdznEotiGgn6dOECo/sfRG59FnK64lxk2J3gj36SBqE7JTVzoqiYqzW7D/
         xwlEZP+uZqCmM/ZRuoUdERmxdeRrkLawvQV8nNi7Ani26Bl6KAYygdcG10NhnfmvaRFe
         KFinUzxq9FBj74N46qEZ9PqNSafQwZwRg6NfELPKvVshK/5XotK3U7FgsVnoUyh2w1rV
         L1Ag==
MIME-Version: 1.0
X-Received: by 10.180.228.42 with SMTP id sf10mr5141478wic.33.1398326994894;
 Thu, 24 Apr 2014 01:09:54 -0700 (PDT)
Received: by 10.194.82.105 with HTTP; Thu, 24 Apr 2014 01:09:54 -0700 (PDT)
In-Reply-To: <CAJgQjQ-k5P3CxVFCE-pHtuH9fVwwCcPc6PBp1NgxM2uueOASsw@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxZFmxXkrNhdUNF3je7k0Eg_YjY2WnvVW5SSk__FAWNz4w@mail.gmail.com>
	<CAJgQjQ-k5P3CxVFCE-pHtuH9fVwwCcPc6PBp1NgxM2uueOASsw@mail.gmail.com>
Date: Thu, 24 Apr 2014 01:09:54 -0700
Message-ID: <CAJgQjQ_ePvspakEfiHzxdDH30EeEzjxsz0y_G0n51VophVL1hg@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: Xiangrui Meng <mengxr@gmail.com>
To: DB Tsai <dbtsai@stanford.edu>, David Hall <dlwh@cs.berkeley.edu>
Cc: shivaram@eecs.berkeley.edu, dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi DB,

I saw you are using yarn-cluster mode for the benchmark. I tested the
yarn-cluster mode and found that YARN does not always give you the
exact number of executors requested. Just want to confirm that you've
checked the number of executors.

The second thing to check is that in the benchmark code, after you
call cache, you should also call count() to materialize the RDD. I saw
in the result, the real difference is actually at the first step.
Adding intercept is not a cheap operation for sparse vectors.

Best,
Xiangrui

On Thu, Apr 24, 2014 at 12:53 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
> I don't think it is easy to make sparse faster than dense with this
> sparsity and feature dimension. You can try rcv1.binary, which should
> show the difference easily.
>
> David, the breeze operators used here are
>
> 1. DenseVector dot SparseVector
> 2. axpy DenseVector SparseVector
>
> However, the SparseVector is passed in as Vector[Double] instead of
> SparseVector[Double]. It might use the axpy impl of [DenseVector,
> Vector] and call activeIterator. I didn't check whether you used
> multimethods on axpy.
>
> Best,
> Xiangrui
>
> On Wed, Apr 23, 2014 at 10:35 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>> The figure showing the Log-Likelihood vs Time can be found here.
>>
>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
>>
>> Let me know if you can not open it. Thanks.
>>
>> Sincerely,
>>
>> DB Tsai
>> -------------------------------------------------------
>> My Blog: https://www.dbtsai.com
>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>
>>
>> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman
>> <shivaram@eecs.berkeley.edu> wrote:
>>> I don't think the attachment came through in the list. Could you upload the
>>> results somewhere and link to them ?
>>>
>>>
>>> On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
>>>>
>>>> 123 features per rows, and in average, 89% are zeros.
>>>> On Apr 23, 2014 9:31 PM, "Evan Sparks" <evan.sparks@gmail.com> wrote:
>>>>
>>>> > What is the number of non zeroes per row (and number of features) in the
>>>> > sparse case? We've hit some issues with breeze sparse support in the
>>>> > past
>>>> > but for sufficiently sparse data it's still pretty good.
>>>> >
>>>> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>> > >
>>>> > > Hi all,
>>>> > >
>>>> > > I'm benchmarking Logistic Regression in MLlib using the newly added
>>>> > optimizer LBFGS and GD. I'm using the same dataset and the same
>>>> > methodology
>>>> > in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>>>> > >
>>>> > > I want to know how Spark scale while adding workers, and how
>>>> > > optimizers
>>>> > and input format (sparse or dense) impact performance.
>>>> > >
>>>> > > The benchmark code can be found here,
>>>> > https://github.com/dbtsai/spark-lbfgs-benchmark
>>>> > >
>>>> > > The first dataset I benchmarked is a9a which only has 2.2MB. I
>>>> > duplicated the dataset, and made it 762MB to have 11M rows. This dataset
>>>> > has 123 features and 11% of the data are non-zero elements.
>>>> > >
>>>> > > In this benchmark, all the dataset is cached in memory.
>>>> > >
>>>> > > As we expect, LBFGS converges faster than GD, and at some point, no
>>>> > matter how we push GD, it will converge slower and slower.
>>>> > >
>>>> > > However, it's surprising that sparse format runs slower than dense
>>>> > format. I did see that sparse format takes significantly smaller amount
>>>> > of
>>>> > memory in caching RDD, but sparse is 40% slower than dense. I think
>>>> > sparse
>>>> > should be fast since when we compute x wT, since x is sparse, we can do
>>>> > it
>>>> > faster. I wonder if there is anything I'm doing wrong.
>>>> > >
>>>> > > The attachment is the benchmark result.
>>>> > >
>>>> > > Thanks.
>>>> > >
>>>> > > Sincerely,
>>>> > >
>>>> > > DB Tsai
>>>> > > -------------------------------------------------------
>>>> > > My Blog: https://www.dbtsai.com
>>>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>> >
>>>
>>>

From dev-return-7424-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 08:25:56 2014
Return-Path: <dev-return-7424-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5BAA211AEF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 08:25:56 +0000 (UTC)
Received: (qmail 18619 invoked by uid 500); 24 Apr 2014 08:25:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18309 invoked by uid 500); 24 Apr 2014 08:25:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18299 invoked by uid 99); 24 Apr 2014 08:25:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 08:25:54 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of scrapcodes@gmail.com designates 209.85.128.170 as permitted sender)
Received: from [209.85.128.170] (HELO mail-ve0-f170.google.com) (209.85.128.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 08:25:49 +0000
Received: by mail-ve0-f170.google.com with SMTP id pa12so2537216veb.15
        for <dev@spark.apache.org>; Thu, 24 Apr 2014 01:25:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=/Sx5dB0aJd1KtlzR55iiA1NvhHPg+azO3S3gGF2i98E=;
        b=ZAgADTVe1k9w2OiEi72A6TAbRZQCw/2c9KMclJD9D5wZBaaZTRLOIc5H/t5YM+jTCU
         oK5zAEQBDVr7Xfh7UJpJBMkQgLCQxRJypNWSFuKDLDVO4xg4w/ZQYG0Z5ZThUMMIvcS1
         qTxPKDjElYyu+ANcDdlFHCAg+vODsy6/mriQqveT5hJp7Pdmg1ifukuZ9NGS4mej25qN
         4ZptamRQr8ZEsbvVwATjd8KFfGIE0s0yTLUixte38qqJWdOOGCoooHBLExzCn05cnnri
         OBjx52pCzkvwsPkJ+EGGx9U9K31r1lqIASAwdCkmBQyhp9BvY2rA4CHV2fQevsq10ST6
         633Q==
X-Received: by 10.58.13.104 with SMTP id g8mr247792vec.16.1398327928987; Thu,
 24 Apr 2014 01:25:28 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.19.164 with HTTP; Thu, 24 Apr 2014 01:25:08 -0700 (PDT)
In-Reply-To: <CABPQxsstPMMhrULigesZi-vOpug6SW6Gnw=cjgJrJ9uetNnJvw@mail.gmail.com>
References: <2B39597F59C94379B2A2498D2A7A9A9B@gmail.com> <CAPh_B=YvSCiwWJUqXOfT6aPPT7MX1tNtdYmRS3C-aiLWmx-HoA@mail.gmail.com>
 <8852B2350EE348D9A58E043BAD67DF79@gmail.com> <CABPQxsstPMMhrULigesZi-vOpug6SW6Gnw=cjgJrJ9uetNnJvw@mail.gmail.com>
From: Prashant Sharma <scrapcodes@gmail.com>
Date: Thu, 24 Apr 2014 13:55:08 +0530
Message-ID: <CAOYDGoCvmuUcXQJCnaQ38og_rYsNVnGZENJ-8JMUPZUbonXh+w@mail.gmail.com>
Subject: Re: Fw: Is there any way to make a quick test on some pre-commit code?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b2e73867ca4c604f7c597c1
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b2e73867ca4c604f7c597c1
Content-Type: text/plain; charset=UTF-8

Not sure but I use sbt/sbt ~compile instead of package. Any reason we use
package instead of compile(which is slightly faster ofc.)


Prashant Sharma


On Thu, Apr 24, 2014 at 1:32 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> This is already on the wiki:
>
> https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools
>
>
>
> On Wed, Apr 23, 2014 at 6:52 PM, Nan Zhu <zhunanmcgill@gmail.com> wrote:
>
> > I'm just asked by others for the same question
> >
> > I think Reynold gave a pretty helpful tip on this,
> >
> > Shall we put this on Contribute-to-Spark wiki?
> >
> > --
> > Nan Zhu
> >
> >
> > Forwarded message:
> >
> > > From: Reynold Xin <rxin@databricks.com>
> > > Reply To: dev@spark.incubator.apache.org
> > > To: dev@spark.incubator.apache.org <dev@spark.incubator.apache.org>
> > > Date: Thursday, February 6, 2014 at 7:50:57 PM
> > > Subject: Re: Is there any way to make a quick test on some pre-commit
> > code?
> > >
> > > You can do
> > >
> > > sbt/sbt assemble-deps
> > >
> > >
> > > and then just run
> > >
> > > sbt/sbt package
> > >
> > > each time.
> > >
> > >
> > > You can even do
> > >
> > > sbt/sbt ~package
> > >
> > > for automatic incremental compilation.
> > >
> > >
> > >
> > > On Thu, Feb 6, 2014 at 4:46 PM, Nan Zhu <zhunanmcgill@gmail.com(mailto:
> > zhunanmcgill@gmail.com)> wrote:
> > >
> > > > Hi, all
> > > >
> > > > Is it always necessary to run sbt assembly when you want to test some
> > code,
> > > >
> > > > Sometimes you just repeatedly change one or two lines for some failed
> > test
> > > > case, it is really time-consuming to sbt assembly every time
> > > >
> > > > any faster way?
> > > >
> > > > Best,
> > > >
> > > > --
> > > > Nan Zhu
> > > >
> > >
> > >
> > >
> > >
> >
> >
> >
>

--047d7b2e73867ca4c604f7c597c1--

From dev-return-7425-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 10:15:01 2014
Return-Path: <dev-return-7425-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B609D11120
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 10:15:01 +0000 (UTC)
Received: (qmail 36961 invoked by uid 500); 24 Apr 2014 10:15:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36724 invoked by uid 500); 24 Apr 2014 10:14:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36708 invoked by uid 99); 24 Apr 2014 10:14:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 10:14:52 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_IMAGE_ONLY_24,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pkolaczk@datastax.com designates 209.85.219.42 as permitted sender)
Received: from [209.85.219.42] (HELO mail-oa0-f42.google.com) (209.85.219.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 10:14:48 +0000
Received: by mail-oa0-f42.google.com with SMTP id i4so2388257oah.15
        for <dev@spark.apache.org>; Thu, 24 Apr 2014 03:14:25 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=xbxuNd4UuaUEb96o+qz+RPUOdRCFzrhs5X1sHXZI4WA=;
        b=LjzjN12VvrdRUpUriSTEechGTH2/7Lx94hrKjsb11MPD8WLyWYQYRYHbSnrvd/vFsy
         mNyh/1kqIoSuPyL+kDTXN4CscvtgZNfTs9OdhnpU2f2s66+v2Mt/ENPalL5K9GnuCx5C
         P410NRj96F7JwN4tiI8H6DGRFHvVt4OPg41IA6rS2hfmCN4ox4fVDkKlDM8HYEiIGLDU
         EgyEfZw48tJaWxA1WUH0WHBlr/WaY64jGEuWQYKPtUqNFaGyS090g9cNh8L9ZEyn6KTE
         yz0NkoL8SCh+L0YHO/U9QARGeexfO0DDQDpKJ7qu7dkjMcs22PuJxCkC90V3rwt2CGs8
         aHDA==
X-Gm-Message-State: ALoCoQlcfxJuDNB3F5PAvs+pI/9vREnY3+Bmpv9phZ3pJJtb0meHDFl50RCT5ZC7zwgdhAC3FJXL
X-Received: by 10.182.40.201 with SMTP id z9mr689750obk.45.1398334465031; Thu,
 24 Apr 2014 03:14:25 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.182.122.227 with HTTP; Thu, 24 Apr 2014 03:14:04 -0700 (PDT)
From: =?UTF-8?Q?Piotr_Ko=C5=82aczkowski?= <pkolaczk@datastax.com>
Date: Thu, 24 Apr 2014 12:14:04 +0200
Message-ID: <CADKBdnrrnJ-9qGAeohdq4oYAZ=hjGoYpu2Lvh9wZnFiprn9D2Q@mail.gmail.com>
Subject: Problem creating objects through reflection
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c33b0210d71b04f7c71dd1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c33b0210d71b04f7c71dd1
Content-Type: text/plain; charset=UTF-8

Hi,

I'm working on Cassandra-Spark integration and I hit a pretty severe
problem. One of the provided functionality is mapping Cassandra rows into
objects of user-defined classes. E.g. like this:

class MyRow(val key: String, val data: Int)
sc.cassandraTable("keyspace", "table").select("key", "data").as[MyRow]  //
returns CassandraRDD[MyRow]

In this example CassandraRDD creates MyRow instances by reflection, i.e.
matches selected fields from Cassandra table and passes them to the
constructor.

Unfortunately this does not work in Spark REPL.
Turns out any class declared on the REPL is an inner classes, and to be
successfully created, it needs a reference to the outer object, even though
it doesn't really use anything from the outer context.

scala> class SomeClass
defined class SomeClass

scala> classOf[SomeClass].getConstructors()(0)
res11: java.lang.reflect.Constructor[_] = public
$iwC$$iwC$SomeClass($iwC$$iwC)

I tried passing a null as a temporary workaround, and it also doesn't work
- I get NPE.
How can I get a reference to the current outer object representing the
context of the current line?

Also, plain non-spark Scala REPL doesn't exhibit this behaviour - and
classes declared on the REPL are proper top-most classes, not inner ones.
Why?

Thanks,
Piotr







-- 
Piotr Kolaczkowski, Lead Software Engineer
pkolaczk@datastax.com

777 Mariners Island Blvd., Suite 510
San Mateo, CA 94404

--001a11c33b0210d71b04f7c71dd1--

From dev-return-7426-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 18:26:41 2014
Return-Path: <dev-return-7426-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3930F118CC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 18:26:41 +0000 (UTC)
Received: (qmail 46274 invoked by uid 500); 24 Apr 2014 18:26:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46171 invoked by uid 500); 24 Apr 2014 18:26:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46163 invoked by uid 99); 24 Apr 2014 18:26:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 18:26:39 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 18:26:35 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id EA4F0101867
	for <dev@spark.apache.org>; Thu, 24 Apr 2014 11:26:14 -0700 (PDT)
Received: from mail-qg0-f51.google.com (mail-qg0-f51.google.com [209.85.192.51])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 37904101877
	for <dev@spark.apache.org>; Thu, 24 Apr 2014 11:26:07 -0700 (PDT)
Received: by mail-qg0-f51.google.com with SMTP id f51so2985426qge.10
        for <dev@spark.apache.org>; Thu, 24 Apr 2014 11:26:05 -0700 (PDT)
X-Gm-Message-State: ALoCoQlUIpI+TpTTNFF9uBGk8Uw4/TCIBQA2ArSJyraBkO1dUcJ8TmBC1TYFb72899M1zvF5+5ZP
MIME-Version: 1.0
X-Received: by 10.224.49.67 with SMTP id u3mr5183711qaf.63.1398363965188; Thu,
 24 Apr 2014 11:26:05 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Thu, 24 Apr 2014 11:26:05 -0700 (PDT)
In-Reply-To: <CAJgQjQ_ePvspakEfiHzxdDH30EeEzjxsz0y_G0n51VophVL1hg@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxZFmxXkrNhdUNF3je7k0Eg_YjY2WnvVW5SSk__FAWNz4w@mail.gmail.com>
	<CAJgQjQ-k5P3CxVFCE-pHtuH9fVwwCcPc6PBp1NgxM2uueOASsw@mail.gmail.com>
	<CAJgQjQ_ePvspakEfiHzxdDH30EeEzjxsz0y_G0n51VophVL1hg@mail.gmail.com>
Date: Thu, 24 Apr 2014 11:26:05 -0700
Message-ID: <CAEYYnxZb3GbjdVVWyF0F4c_sEikadefP88rnWuGtHrx9HwMBJQ@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: DB Tsai <dbtsai@stanford.edu>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: David Hall <dlwh@cs.berkeley.edu>, shivaram@eecs.berkeley.edu, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2ef8069a0b804f7cdfb36
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ef8069a0b804f7cdfb36
Content-Type: text/plain; charset=UTF-8

Hi Xiangrui,

Yes, I'm using yarn-cluster mode, and I did check # of executors I
specified are the same as the actual running executors.

For caching and materialization, I've the timer in optimizer after calling
count(); as a result, the time for materialization in cache isn't in the
benchmark.

The difference you saw is actually from dense feature or sparse feature
vector. For LBFGS and GD dense feature, you can see the first iteration
takes the same time. It's true for GD.

I'm going to run rcv1.binary which only has 0.15% non-zero elements to
verify the hypothesis.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Thu, Apr 24, 2014 at 1:09 AM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Hi DB,
>
> I saw you are using yarn-cluster mode for the benchmark. I tested the
> yarn-cluster mode and found that YARN does not always give you the
> exact number of executors requested. Just want to confirm that you've
> checked the number of executors.
>
> The second thing to check is that in the benchmark code, after you
> call cache, you should also call count() to materialize the RDD. I saw
> in the result, the real difference is actually at the first step.
> Adding intercept is not a cheap operation for sparse vectors.
>
> Best,
> Xiangrui
>
> On Thu, Apr 24, 2014 at 12:53 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
> > I don't think it is easy to make sparse faster than dense with this
> > sparsity and feature dimension. You can try rcv1.binary, which should
> > show the difference easily.
> >
> > David, the breeze operators used here are
> >
> > 1. DenseVector dot SparseVector
> > 2. axpy DenseVector SparseVector
> >
> > However, the SparseVector is passed in as Vector[Double] instead of
> > SparseVector[Double]. It might use the axpy impl of [DenseVector,
> > Vector] and call activeIterator. I didn't check whether you used
> > multimethods on axpy.
> >
> > Best,
> > Xiangrui
> >
> > On Wed, Apr 23, 2014 at 10:35 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> >> The figure showing the Log-Likelihood vs Time can be found here.
> >>
> >>
> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
> >>
> >> Let me know if you can not open it. Thanks.
> >>
> >> Sincerely,
> >>
> >> DB Tsai
> >> -------------------------------------------------------
> >> My Blog: https://www.dbtsai.com
> >> LinkedIn: https://www.linkedin.com/in/dbtsai
> >>
> >>
> >> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman
> >> <shivaram@eecs.berkeley.edu> wrote:
> >>> I don't think the attachment came through in the list. Could you
> upload the
> >>> results somewhere and link to them ?
> >>>
> >>>
> >>> On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
> >>>>
> >>>> 123 features per rows, and in average, 89% are zeros.
> >>>> On Apr 23, 2014 9:31 PM, "Evan Sparks" <evan.sparks@gmail.com> wrote:
> >>>>
> >>>> > What is the number of non zeroes per row (and number of features)
> in the
> >>>> > sparse case? We've hit some issues with breeze sparse support in the
> >>>> > past
> >>>> > but for sufficiently sparse data it's still pretty good.
> >>>> >
> >>>> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> >>>> > >
> >>>> > > Hi all,
> >>>> > >
> >>>> > > I'm benchmarking Logistic Regression in MLlib using the newly
> added
> >>>> > optimizer LBFGS and GD. I'm using the same dataset and the same
> >>>> > methodology
> >>>> > in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
> >>>> > >
> >>>> > > I want to know how Spark scale while adding workers, and how
> >>>> > > optimizers
> >>>> > and input format (sparse or dense) impact performance.
> >>>> > >
> >>>> > > The benchmark code can be found here,
> >>>> > https://github.com/dbtsai/spark-lbfgs-benchmark
> >>>> > >
> >>>> > > The first dataset I benchmarked is a9a which only has 2.2MB. I
> >>>> > duplicated the dataset, and made it 762MB to have 11M rows. This
> dataset
> >>>> > has 123 features and 11% of the data are non-zero elements.
> >>>> > >
> >>>> > > In this benchmark, all the dataset is cached in memory.
> >>>> > >
> >>>> > > As we expect, LBFGS converges faster than GD, and at some point,
> no
> >>>> > matter how we push GD, it will converge slower and slower.
> >>>> > >
> >>>> > > However, it's surprising that sparse format runs slower than dense
> >>>> > format. I did see that sparse format takes significantly smaller
> amount
> >>>> > of
> >>>> > memory in caching RDD, but sparse is 40% slower than dense. I think
> >>>> > sparse
> >>>> > should be fast since when we compute x wT, since x is sparse, we
> can do
> >>>> > it
> >>>> > faster. I wonder if there is anything I'm doing wrong.
> >>>> > >
> >>>> > > The attachment is the benchmark result.
> >>>> > >
> >>>> > > Thanks.
> >>>> > >
> >>>> > > Sincerely,
> >>>> > >
> >>>> > > DB Tsai
> >>>> > > -------------------------------------------------------
> >>>> > > My Blog: https://www.dbtsai.com
> >>>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
> >>>> >
> >>>
> >>>
>

--001a11c2ef8069a0b804f7cdfb36--

From dev-return-7427-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 20:44:56 2014
Return-Path: <dev-return-7427-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A275911D99
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 20:44:56 +0000 (UTC)
Received: (qmail 18327 invoked by uid 500); 24 Apr 2014 20:44:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17552 invoked by uid 500); 24 Apr 2014 20:44:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17540 invoked by uid 99); 24 Apr 2014 20:44:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 20:44:32 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 209.85.212.181 as permitted sender)
Received: from [209.85.212.181] (HELO mail-wi0-f181.google.com) (209.85.212.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 20:44:28 +0000
Received: by mail-wi0-f181.google.com with SMTP id hm4so1707467wib.14
        for <dev@spark.apache.org>; Thu, 24 Apr 2014 13:44:07 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=ZoQSWuT/GcyEX1y8fOLRFf1d1jF2UzYplyxBv0QBYzI=;
        b=GJ1jPZ9773jlZVi4ot1eUxQuwlexErJPsgZm59gsqt/AYUorn3MIqlWk8PSJ3PEut3
         V+bjBFquNyEAENOYsm2VNoS+/up8Z3AIoaDwQPIC5qUpMCKmEEFze5o5tB93a+Hz09xZ
         PZQm1Dj1XIlQONdrYReWFifwMv2KsZFuheiKr/qdWtJeIQUgZK0pg7A8xs22WoUIDo30
         gIxNvzSPkzvdaFJAwY2MKF/M1GqgcVuU25JCrQYZaT5ia7N+YXGdVagJGooi+ipwefo2
         8nyVHPxa/0d7+8y/bsiG2ve5srotEkVgL3oIdmUXfTr3tWrVyJ83UPZNHIkmeS1CAAvm
         WScw==
MIME-Version: 1.0
X-Received: by 10.194.88.230 with SMTP id bj6mr98960wjb.85.1398372247426; Thu,
 24 Apr 2014 13:44:07 -0700 (PDT)
Received: by 10.194.82.105 with HTTP; Thu, 24 Apr 2014 13:44:07 -0700 (PDT)
In-Reply-To: <CAEYYnxZb3GbjdVVWyF0F4c_sEikadefP88rnWuGtHrx9HwMBJQ@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxZFmxXkrNhdUNF3je7k0Eg_YjY2WnvVW5SSk__FAWNz4w@mail.gmail.com>
	<CAJgQjQ-k5P3CxVFCE-pHtuH9fVwwCcPc6PBp1NgxM2uueOASsw@mail.gmail.com>
	<CAJgQjQ_ePvspakEfiHzxdDH30EeEzjxsz0y_G0n51VophVL1hg@mail.gmail.com>
	<CAEYYnxZb3GbjdVVWyF0F4c_sEikadefP88rnWuGtHrx9HwMBJQ@mail.gmail.com>
Date: Thu, 24 Apr 2014 13:44:07 -0700
Message-ID: <CAJgQjQ8s7dP-rUtiiSUV2imeL9r++_WEJj1RyWCy+b3QBEcxRw@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: Xiangrui Meng <mengxr@gmail.com>
To: DB Tsai <dbtsai@stanford.edu>
Cc: David Hall <dlwh@cs.berkeley.edu>, shivaram@eecs.berkeley.edu, dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I don't understand why sparse falls behind dense so much at the very
first iteration. I didn't see count() is called in
https://github.com/dbtsai/spark-lbfgs-benchmark/blob/master/src/main/scala/org/apache/spark/mllib/benchmark/BinaryLogisticRegression.scala
. Maybe you have local uncommitted changes.

Best,
Xiangrui

On Thu, Apr 24, 2014 at 11:26 AM, DB Tsai <dbtsai@stanford.edu> wrote:
> Hi Xiangrui,
>
> Yes, I'm using yarn-cluster mode, and I did check # of executors I specified
> are the same as the actual running executors.
>
> For caching and materialization, I've the timer in optimizer after calling
> count(); as a result, the time for materialization in cache isn't in the
> benchmark.
>
> The difference you saw is actually from dense feature or sparse feature
> vector. For LBFGS and GD dense feature, you can see the first iteration
> takes the same time. It's true for GD.
>
> I'm going to run rcv1.binary which only has 0.15% non-zero elements to
> verify the hypothesis.
>
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Thu, Apr 24, 2014 at 1:09 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>
>> Hi DB,
>>
>> I saw you are using yarn-cluster mode for the benchmark. I tested the
>> yarn-cluster mode and found that YARN does not always give you the
>> exact number of executors requested. Just want to confirm that you've
>> checked the number of executors.
>>
>> The second thing to check is that in the benchmark code, after you
>> call cache, you should also call count() to materialize the RDD. I saw
>> in the result, the real difference is actually at the first step.
>> Adding intercept is not a cheap operation for sparse vectors.
>>
>> Best,
>> Xiangrui
>>
>> On Thu, Apr 24, 2014 at 12:53 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>> > I don't think it is easy to make sparse faster than dense with this
>> > sparsity and feature dimension. You can try rcv1.binary, which should
>> > show the difference easily.
>> >
>> > David, the breeze operators used here are
>> >
>> > 1. DenseVector dot SparseVector
>> > 2. axpy DenseVector SparseVector
>> >
>> > However, the SparseVector is passed in as Vector[Double] instead of
>> > SparseVector[Double]. It might use the axpy impl of [DenseVector,
>> > Vector] and call activeIterator. I didn't check whether you used
>> > multimethods on axpy.
>> >
>> > Best,
>> > Xiangrui
>> >
>> > On Wed, Apr 23, 2014 at 10:35 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>> >> The figure showing the Log-Likelihood vs Time can be found here.
>> >>
>> >>
>> >> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
>> >>
>> >> Let me know if you can not open it. Thanks.
>> >>
>> >> Sincerely,
>> >>
>> >> DB Tsai
>> >> -------------------------------------------------------
>> >> My Blog: https://www.dbtsai.com
>> >> LinkedIn: https://www.linkedin.com/in/dbtsai
>> >>
>> >>
>> >> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman
>> >> <shivaram@eecs.berkeley.edu> wrote:
>> >>> I don't think the attachment came through in the list. Could you
>> >>> upload the
>> >>> results somewhere and link to them ?
>> >>>
>> >>>
>> >>> On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
>> >>>>
>> >>>> 123 features per rows, and in average, 89% are zeros.
>> >>>> On Apr 23, 2014 9:31 PM, "Evan Sparks" <evan.sparks@gmail.com> wrote:
>> >>>>
>> >>>> > What is the number of non zeroes per row (and number of features)
>> >>>> > in the
>> >>>> > sparse case? We've hit some issues with breeze sparse support in
>> >>>> > the
>> >>>> > past
>> >>>> > but for sufficiently sparse data it's still pretty good.
>> >>>> >
>> >>>> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>> >>>> > >
>> >>>> > > Hi all,
>> >>>> > >
>> >>>> > > I'm benchmarking Logistic Regression in MLlib using the newly
>> >>>> > > added
>> >>>> > optimizer LBFGS and GD. I'm using the same dataset and the same
>> >>>> > methodology
>> >>>> > in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>> >>>> > >
>> >>>> > > I want to know how Spark scale while adding workers, and how
>> >>>> > > optimizers
>> >>>> > and input format (sparse or dense) impact performance.
>> >>>> > >
>> >>>> > > The benchmark code can be found here,
>> >>>> > https://github.com/dbtsai/spark-lbfgs-benchmark
>> >>>> > >
>> >>>> > > The first dataset I benchmarked is a9a which only has 2.2MB. I
>> >>>> > duplicated the dataset, and made it 762MB to have 11M rows. This
>> >>>> > dataset
>> >>>> > has 123 features and 11% of the data are non-zero elements.
>> >>>> > >
>> >>>> > > In this benchmark, all the dataset is cached in memory.
>> >>>> > >
>> >>>> > > As we expect, LBFGS converges faster than GD, and at some point,
>> >>>> > > no
>> >>>> > matter how we push GD, it will converge slower and slower.
>> >>>> > >
>> >>>> > > However, it's surprising that sparse format runs slower than
>> >>>> > > dense
>> >>>> > format. I did see that sparse format takes significantly smaller
>> >>>> > amount
>> >>>> > of
>> >>>> > memory in caching RDD, but sparse is 40% slower than dense. I think
>> >>>> > sparse
>> >>>> > should be fast since when we compute x wT, since x is sparse, we
>> >>>> > can do
>> >>>> > it
>> >>>> > faster. I wonder if there is anything I'm doing wrong.
>> >>>> > >
>> >>>> > > The attachment is the benchmark result.
>> >>>> > >
>> >>>> > > Thanks.
>> >>>> > >
>> >>>> > > Sincerely,
>> >>>> > >
>> >>>> > > DB Tsai
>> >>>> > > -------------------------------------------------------
>> >>>> > > My Blog: https://www.dbtsai.com
>> >>>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>> >>>> >
>> >>>
>> >>>
>
>

From dev-return-7428-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 20:54:46 2014
Return-Path: <dev-return-7428-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CAF6F11E14
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 20:54:46 +0000 (UTC)
Received: (qmail 46601 invoked by uid 500); 24 Apr 2014 20:54:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46559 invoked by uid 500); 24 Apr 2014 20:54:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46550 invoked by uid 99); 24 Apr 2014 20:54:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 20:54:44 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 20:54:40 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 0F9F61017BA
	for <dev@spark.apache.org>; Thu, 24 Apr 2014 13:54:17 -0700 (PDT)
Received: from mail-qg0-f54.google.com (mail-qg0-f54.google.com [209.85.192.54])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id A20D4101875
	for <dev@spark.apache.org>; Thu, 24 Apr 2014 13:54:15 -0700 (PDT)
Received: by mail-qg0-f54.google.com with SMTP id q107so2496050qgd.27
        for <dev@spark.apache.org>; Thu, 24 Apr 2014 13:54:14 -0700 (PDT)
X-Gm-Message-State: ALoCoQk+srX/Wn1zDBgl8GP37uNKkVs+f6ocEJxBUqDMG9gl/cDlI7Pe4VMjqXKWqmEY7vhtt56+
MIME-Version: 1.0
X-Received: by 10.224.166.210 with SMTP id n18mr6449857qay.6.1398372854058;
 Thu, 24 Apr 2014 13:54:14 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Thu, 24 Apr 2014 13:54:13 -0700 (PDT)
In-Reply-To: <CAJgQjQ8s7dP-rUtiiSUV2imeL9r++_WEJj1RyWCy+b3QBEcxRw@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxZFmxXkrNhdUNF3je7k0Eg_YjY2WnvVW5SSk__FAWNz4w@mail.gmail.com>
	<CAJgQjQ-k5P3CxVFCE-pHtuH9fVwwCcPc6PBp1NgxM2uueOASsw@mail.gmail.com>
	<CAJgQjQ_ePvspakEfiHzxdDH30EeEzjxsz0y_G0n51VophVL1hg@mail.gmail.com>
	<CAEYYnxZb3GbjdVVWyF0F4c_sEikadefP88rnWuGtHrx9HwMBJQ@mail.gmail.com>
	<CAJgQjQ8s7dP-rUtiiSUV2imeL9r++_WEJj1RyWCy+b3QBEcxRw@mail.gmail.com>
Date: Thu, 24 Apr 2014 13:54:13 -0700
Message-ID: <CAEYYnxaSh532XmO7224BiM5D2g_Q0+k49AaFbuT6LxDCmSUZBw@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: DB Tsai <dbtsai@stanford.edu>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: David Hall <dlwh@cs.berkeley.edu>, shivaram@eecs.berkeley.edu, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0149c39c3ae90504f7d00de5
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149c39c3ae90504f7d00de5
Content-Type: text/plain; charset=UTF-8

I'm doing the timer in runMiniBatchSGD after  val numExamples = data.count()

See the following. Running rcv1 dataset now, and will update soon.

    val startTime = System.nanoTime()
    for (i <- 1 to numIterations) {
      // Sample a subset (fraction miniBatchFraction) of the total data
      // compute and sum up the subgradients on this subset (this is one
map-reduce)
      val (gradientSum, lossSum) = data.sample(false, miniBatchFraction, 42
+ i)
        .aggregate((BDV.zeros[Double](weights.size), 0.0))(
          seqOp = (c, v) => (c, v) match { case ((grad, loss), (label,
features)) =>
            val l = gradient.compute(features, label, weights,
Vectors.fromBreeze(grad))
            (grad, loss + l)
          },
          combOp = (c1, c2) => (c1, c2) match { case ((grad1, loss1),
(grad2, loss2)) =>
            (grad1 += grad2, loss1 + loss2)
          })

      /**
       * NOTE(Xinghao): lossSum is computed using the weights from the
previous iteration
       * and regVal is the regularization value computed in the previous
iteration as well.
       */
      stochasticLossHistory.append(lossSum / miniBatchSize + regVal)
      val update = updater.compute(
        weights, Vectors.fromBreeze(gradientSum / miniBatchSize), stepSize,
i, regParam)
      weights = update._1
      regVal = update._2
      timeStamp.append(System.nanoTime() - startTime)
    }






Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Thu, Apr 24, 2014 at 1:44 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> I don't understand why sparse falls behind dense so much at the very
> first iteration. I didn't see count() is called in
>
> https://github.com/dbtsai/spark-lbfgs-benchmark/blob/master/src/main/scala/org/apache/spark/mllib/benchmark/BinaryLogisticRegression.scala
> . Maybe you have local uncommitted changes.
>
> Best,
> Xiangrui
>
> On Thu, Apr 24, 2014 at 11:26 AM, DB Tsai <dbtsai@stanford.edu> wrote:
> > Hi Xiangrui,
> >
> > Yes, I'm using yarn-cluster mode, and I did check # of executors I
> specified
> > are the same as the actual running executors.
> >
> > For caching and materialization, I've the timer in optimizer after
> calling
> > count(); as a result, the time for materialization in cache isn't in the
> > benchmark.
> >
> > The difference you saw is actually from dense feature or sparse feature
> > vector. For LBFGS and GD dense feature, you can see the first iteration
> > takes the same time. It's true for GD.
> >
> > I'm going to run rcv1.binary which only has 0.15% non-zero elements to
> > verify the hypothesis.
> >
> >
> > Sincerely,
> >
> > DB Tsai
> > -------------------------------------------------------
> > My Blog: https://www.dbtsai.com
> > LinkedIn: https://www.linkedin.com/in/dbtsai
> >
> >
> > On Thu, Apr 24, 2014 at 1:09 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
> >>
> >> Hi DB,
> >>
> >> I saw you are using yarn-cluster mode for the benchmark. I tested the
> >> yarn-cluster mode and found that YARN does not always give you the
> >> exact number of executors requested. Just want to confirm that you've
> >> checked the number of executors.
> >>
> >> The second thing to check is that in the benchmark code, after you
> >> call cache, you should also call count() to materialize the RDD. I saw
> >> in the result, the real difference is actually at the first step.
> >> Adding intercept is not a cheap operation for sparse vectors.
> >>
> >> Best,
> >> Xiangrui
> >>
> >> On Thu, Apr 24, 2014 at 12:53 AM, Xiangrui Meng <mengxr@gmail.com>
> wrote:
> >> > I don't think it is easy to make sparse faster than dense with this
> >> > sparsity and feature dimension. You can try rcv1.binary, which should
> >> > show the difference easily.
> >> >
> >> > David, the breeze operators used here are
> >> >
> >> > 1. DenseVector dot SparseVector
> >> > 2. axpy DenseVector SparseVector
> >> >
> >> > However, the SparseVector is passed in as Vector[Double] instead of
> >> > SparseVector[Double]. It might use the axpy impl of [DenseVector,
> >> > Vector] and call activeIterator. I didn't check whether you used
> >> > multimethods on axpy.
> >> >
> >> > Best,
> >> > Xiangrui
> >> >
> >> > On Wed, Apr 23, 2014 at 10:35 PM, DB Tsai <dbtsai@stanford.edu>
> wrote:
> >> >> The figure showing the Log-Likelihood vs Time can be found here.
> >> >>
> >> >>
> >> >>
> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
> >> >>
> >> >> Let me know if you can not open it. Thanks.
> >> >>
> >> >> Sincerely,
> >> >>
> >> >> DB Tsai
> >> >> -------------------------------------------------------
> >> >> My Blog: https://www.dbtsai.com
> >> >> LinkedIn: https://www.linkedin.com/in/dbtsai
> >> >>
> >> >>
> >> >> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman
> >> >> <shivaram@eecs.berkeley.edu> wrote:
> >> >>> I don't think the attachment came through in the list. Could you
> >> >>> upload the
> >> >>> results somewhere and link to them ?
> >> >>>
> >> >>>
> >> >>> On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
> >> >>>>
> >> >>>> 123 features per rows, and in average, 89% are zeros.
> >> >>>> On Apr 23, 2014 9:31 PM, "Evan Sparks" <evan.sparks@gmail.com>
> wrote:
> >> >>>>
> >> >>>> > What is the number of non zeroes per row (and number of features)
> >> >>>> > in the
> >> >>>> > sparse case? We've hit some issues with breeze sparse support in
> >> >>>> > the
> >> >>>> > past
> >> >>>> > but for sufficiently sparse data it's still pretty good.
> >> >>>> >
> >> >>>> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu>
> wrote:
> >> >>>> > >
> >> >>>> > > Hi all,
> >> >>>> > >
> >> >>>> > > I'm benchmarking Logistic Regression in MLlib using the newly
> >> >>>> > > added
> >> >>>> > optimizer LBFGS and GD. I'm using the same dataset and the same
> >> >>>> > methodology
> >> >>>> > in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
> >> >>>> > >
> >> >>>> > > I want to know how Spark scale while adding workers, and how
> >> >>>> > > optimizers
> >> >>>> > and input format (sparse or dense) impact performance.
> >> >>>> > >
> >> >>>> > > The benchmark code can be found here,
> >> >>>> > https://github.com/dbtsai/spark-lbfgs-benchmark
> >> >>>> > >
> >> >>>> > > The first dataset I benchmarked is a9a which only has 2.2MB. I
> >> >>>> > duplicated the dataset, and made it 762MB to have 11M rows. This
> >> >>>> > dataset
> >> >>>> > has 123 features and 11% of the data are non-zero elements.
> >> >>>> > >
> >> >>>> > > In this benchmark, all the dataset is cached in memory.
> >> >>>> > >
> >> >>>> > > As we expect, LBFGS converges faster than GD, and at some
> point,
> >> >>>> > > no
> >> >>>> > matter how we push GD, it will converge slower and slower.
> >> >>>> > >
> >> >>>> > > However, it's surprising that sparse format runs slower than
> >> >>>> > > dense
> >> >>>> > format. I did see that sparse format takes significantly smaller
> >> >>>> > amount
> >> >>>> > of
> >> >>>> > memory in caching RDD, but sparse is 40% slower than dense. I
> think
> >> >>>> > sparse
> >> >>>> > should be fast since when we compute x wT, since x is sparse, we
> >> >>>> > can do
> >> >>>> > it
> >> >>>> > faster. I wonder if there is anything I'm doing wrong.
> >> >>>> > >
> >> >>>> > > The attachment is the benchmark result.
> >> >>>> > >
> >> >>>> > > Thanks.
> >> >>>> > >
> >> >>>> > > Sincerely,
> >> >>>> > >
> >> >>>> > > DB Tsai
> >> >>>> > > -------------------------------------------------------
> >> >>>> > > My Blog: https://www.dbtsai.com
> >> >>>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
> >> >>>> >
> >> >>>
> >> >>>
> >
> >
>

--089e0149c39c3ae90504f7d00de5--

From dev-return-7429-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 21:37:33 2014
Return-Path: <dev-return-7429-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E819211003
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 21:37:33 +0000 (UTC)
Received: (qmail 63465 invoked by uid 500); 24 Apr 2014 21:37:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63400 invoked by uid 500); 24 Apr 2014 21:37:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63392 invoked by uid 99); 24 Apr 2014 21:37:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 21:37:28 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 21:37:25 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 50E5B1016BC
	for <dev@spark.apache.org>; Thu, 24 Apr 2014 14:37:03 -0700 (PDT)
Received: from mail-qg0-f52.google.com (mail-qg0-f52.google.com [209.85.192.52])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 746EC101623
	for <dev@spark.apache.org>; Thu, 24 Apr 2014 14:37:01 -0700 (PDT)
Received: by mail-qg0-f52.google.com with SMTP id j5so3175430qga.39
        for <dev@spark.apache.org>; Thu, 24 Apr 2014 14:36:59 -0700 (PDT)
X-Gm-Message-State: ALoCoQmiorjJM3100YTavhLRidcmJT1OqMrXhSr5w2+TO82LhfWlzO3OUPzHJjhnlYJU9Ee+DL1M
MIME-Version: 1.0
X-Received: by 10.140.90.84 with SMTP id w78mr6206451qgd.52.1398375419380;
 Thu, 24 Apr 2014 14:36:59 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Thu, 24 Apr 2014 14:36:59 -0700 (PDT)
In-Reply-To: <CAEYYnxaSh532XmO7224BiM5D2g_Q0+k49AaFbuT6LxDCmSUZBw@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxZFmxXkrNhdUNF3je7k0Eg_YjY2WnvVW5SSk__FAWNz4w@mail.gmail.com>
	<CAJgQjQ-k5P3CxVFCE-pHtuH9fVwwCcPc6PBp1NgxM2uueOASsw@mail.gmail.com>
	<CAJgQjQ_ePvspakEfiHzxdDH30EeEzjxsz0y_G0n51VophVL1hg@mail.gmail.com>
	<CAEYYnxZb3GbjdVVWyF0F4c_sEikadefP88rnWuGtHrx9HwMBJQ@mail.gmail.com>
	<CAJgQjQ8s7dP-rUtiiSUV2imeL9r++_WEJj1RyWCy+b3QBEcxRw@mail.gmail.com>
	<CAEYYnxaSh532XmO7224BiM5D2g_Q0+k49AaFbuT6LxDCmSUZBw@mail.gmail.com>
Date: Thu, 24 Apr 2014 14:36:59 -0700
Message-ID: <CAEYYnxaKTW-74AmKSAAg_kQkAyqePjRNhn9rS2VVbt8wp_4CsA@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: DB Tsai <dbtsai@stanford.edu>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: David Hall <dlwh@cs.berkeley.edu>, shivaram@eecs.berkeley.edu, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c117a822b2f704f7d0a6d4
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c117a822b2f704f7d0a6d4
Content-Type: text/plain; charset=UTF-8

rcv1.binary is too sparse (0.15% non-zero elements), so dense format will
not run due to out of memory. But sparse format runs really well.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Thu, Apr 24, 2014 at 1:54 PM, DB Tsai <dbtsai@stanford.edu> wrote:

> I'm doing the timer in runMiniBatchSGD after  val numExamples =
> data.count()
>
> See the following. Running rcv1 dataset now, and will update soon.
>
>     val startTime = System.nanoTime()
>     for (i <- 1 to numIterations) {
>       // Sample a subset (fraction miniBatchFraction) of the total data
>       // compute and sum up the subgradients on this subset (this is one
> map-reduce)
>       val (gradientSum, lossSum) = data.sample(false, miniBatchFraction,
> 42 + i)
>         .aggregate((BDV.zeros[Double](weights.size), 0.0))(
>           seqOp = (c, v) => (c, v) match { case ((grad, loss), (label,
> features)) =>
>             val l = gradient.compute(features, label, weights,
> Vectors.fromBreeze(grad))
>             (grad, loss + l)
>           },
>           combOp = (c1, c2) => (c1, c2) match { case ((grad1, loss1),
> (grad2, loss2)) =>
>             (grad1 += grad2, loss1 + loss2)
>           })
>
>       /**
>        * NOTE(Xinghao): lossSum is computed using the weights from the
> previous iteration
>        * and regVal is the regularization value computed in the previous
> iteration as well.
>        */
>       stochasticLossHistory.append(lossSum / miniBatchSize + regVal)
>       val update = updater.compute(
>         weights, Vectors.fromBreeze(gradientSum / miniBatchSize),
> stepSize, i, regParam)
>       weights = update._1
>       regVal = update._2
>       timeStamp.append(System.nanoTime() - startTime)
>     }
>
>
>
>
>
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Thu, Apr 24, 2014 at 1:44 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
>> I don't understand why sparse falls behind dense so much at the very
>> first iteration. I didn't see count() is called in
>>
>> https://github.com/dbtsai/spark-lbfgs-benchmark/blob/master/src/main/scala/org/apache/spark/mllib/benchmark/BinaryLogisticRegression.scala
>> . Maybe you have local uncommitted changes.
>>
>> Best,
>> Xiangrui
>>
>> On Thu, Apr 24, 2014 at 11:26 AM, DB Tsai <dbtsai@stanford.edu> wrote:
>> > Hi Xiangrui,
>> >
>> > Yes, I'm using yarn-cluster mode, and I did check # of executors I
>> specified
>> > are the same as the actual running executors.
>> >
>> > For caching and materialization, I've the timer in optimizer after
>> calling
>> > count(); as a result, the time for materialization in cache isn't in the
>> > benchmark.
>> >
>> > The difference you saw is actually from dense feature or sparse feature
>> > vector. For LBFGS and GD dense feature, you can see the first iteration
>> > takes the same time. It's true for GD.
>> >
>> > I'm going to run rcv1.binary which only has 0.15% non-zero elements to
>> > verify the hypothesis.
>> >
>> >
>> > Sincerely,
>> >
>> > DB Tsai
>> > -------------------------------------------------------
>> > My Blog: https://www.dbtsai.com
>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>> >
>> >
>> > On Thu, Apr 24, 2014 at 1:09 AM, Xiangrui Meng <mengxr@gmail.com>
>> wrote:
>> >>
>> >> Hi DB,
>> >>
>> >> I saw you are using yarn-cluster mode for the benchmark. I tested the
>> >> yarn-cluster mode and found that YARN does not always give you the
>> >> exact number of executors requested. Just want to confirm that you've
>> >> checked the number of executors.
>> >>
>> >> The second thing to check is that in the benchmark code, after you
>> >> call cache, you should also call count() to materialize the RDD. I saw
>> >> in the result, the real difference is actually at the first step.
>> >> Adding intercept is not a cheap operation for sparse vectors.
>> >>
>> >> Best,
>> >> Xiangrui
>> >>
>> >> On Thu, Apr 24, 2014 at 12:53 AM, Xiangrui Meng <mengxr@gmail.com>
>> wrote:
>> >> > I don't think it is easy to make sparse faster than dense with this
>> >> > sparsity and feature dimension. You can try rcv1.binary, which should
>> >> > show the difference easily.
>> >> >
>> >> > David, the breeze operators used here are
>> >> >
>> >> > 1. DenseVector dot SparseVector
>> >> > 2. axpy DenseVector SparseVector
>> >> >
>> >> > However, the SparseVector is passed in as Vector[Double] instead of
>> >> > SparseVector[Double]. It might use the axpy impl of [DenseVector,
>> >> > Vector] and call activeIterator. I didn't check whether you used
>> >> > multimethods on axpy.
>> >> >
>> >> > Best,
>> >> > Xiangrui
>> >> >
>> >> > On Wed, Apr 23, 2014 at 10:35 PM, DB Tsai <dbtsai@stanford.edu>
>> wrote:
>> >> >> The figure showing the Log-Likelihood vs Time can be found here.
>> >> >>
>> >> >>
>> >> >>
>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
>> >> >>
>> >> >> Let me know if you can not open it. Thanks.
>> >> >>
>> >> >> Sincerely,
>> >> >>
>> >> >> DB Tsai
>> >> >> -------------------------------------------------------
>> >> >> My Blog: https://www.dbtsai.com
>> >> >> LinkedIn: https://www.linkedin.com/in/dbtsai
>> >> >>
>> >> >>
>> >> >> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman
>> >> >> <shivaram@eecs.berkeley.edu> wrote:
>> >> >>> I don't think the attachment came through in the list. Could you
>> >> >>> upload the
>> >> >>> results somewhere and link to them ?
>> >> >>>
>> >> >>>
>> >> >>> On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com>
>> wrote:
>> >> >>>>
>> >> >>>> 123 features per rows, and in average, 89% are zeros.
>> >> >>>> On Apr 23, 2014 9:31 PM, "Evan Sparks" <evan.sparks@gmail.com>
>> wrote:
>> >> >>>>
>> >> >>>> > What is the number of non zeroes per row (and number of
>> features)
>> >> >>>> > in the
>> >> >>>> > sparse case? We've hit some issues with breeze sparse support in
>> >> >>>> > the
>> >> >>>> > past
>> >> >>>> > but for sufficiently sparse data it's still pretty good.
>> >> >>>> >
>> >> >>>> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu>
>> wrote:
>> >> >>>> > >
>> >> >>>> > > Hi all,
>> >> >>>> > >
>> >> >>>> > > I'm benchmarking Logistic Regression in MLlib using the newly
>> >> >>>> > > added
>> >> >>>> > optimizer LBFGS and GD. I'm using the same dataset and the same
>> >> >>>> > methodology
>> >> >>>> > in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>> >> >>>> > >
>> >> >>>> > > I want to know how Spark scale while adding workers, and how
>> >> >>>> > > optimizers
>> >> >>>> > and input format (sparse or dense) impact performance.
>> >> >>>> > >
>> >> >>>> > > The benchmark code can be found here,
>> >> >>>> > https://github.com/dbtsai/spark-lbfgs-benchmark
>> >> >>>> > >
>> >> >>>> > > The first dataset I benchmarked is a9a which only has 2.2MB. I
>> >> >>>> > duplicated the dataset, and made it 762MB to have 11M rows. This
>> >> >>>> > dataset
>> >> >>>> > has 123 features and 11% of the data are non-zero elements.
>> >> >>>> > >
>> >> >>>> > > In this benchmark, all the dataset is cached in memory.
>> >> >>>> > >
>> >> >>>> > > As we expect, LBFGS converges faster than GD, and at some
>> point,
>> >> >>>> > > no
>> >> >>>> > matter how we push GD, it will converge slower and slower.
>> >> >>>> > >
>> >> >>>> > > However, it's surprising that sparse format runs slower than
>> >> >>>> > > dense
>> >> >>>> > format. I did see that sparse format takes significantly smaller
>> >> >>>> > amount
>> >> >>>> > of
>> >> >>>> > memory in caching RDD, but sparse is 40% slower than dense. I
>> think
>> >> >>>> > sparse
>> >> >>>> > should be fast since when we compute x wT, since x is sparse, we
>> >> >>>> > can do
>> >> >>>> > it
>> >> >>>> > faster. I wonder if there is anything I'm doing wrong.
>> >> >>>> > >
>> >> >>>> > > The attachment is the benchmark result.
>> >> >>>> > >
>> >> >>>> > > Thanks.
>> >> >>>> > >
>> >> >>>> > > Sincerely,
>> >> >>>> > >
>> >> >>>> > > DB Tsai
>> >> >>>> > > -------------------------------------------------------
>> >> >>>> > > My Blog: https://www.dbtsai.com
>> >> >>>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>> >> >>>> >
>> >> >>>
>> >> >>>
>> >
>> >
>>
>
>

--001a11c117a822b2f704f7d0a6d4--

From dev-return-7430-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Apr 24 22:14:15 2014
Return-Path: <dev-return-7430-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6D80411134
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Apr 2014 22:14:15 +0000 (UTC)
Received: (qmail 32931 invoked by uid 500); 24 Apr 2014 22:14:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32814 invoked by uid 500); 24 Apr 2014 22:14:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32802 invoked by uid 99); 24 Apr 2014 22:14:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 22:14:13 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.178] (HELO mail-qc0-f178.google.com) (209.85.216.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Apr 2014 22:14:09 +0000
Received: by mail-qc0-f178.google.com with SMTP id i8so3155213qcq.23
        for <dev@spark.apache.org>; Thu, 24 Apr 2014 15:13:46 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=O/i+dg+wLER5tJh961KhbEwgzjeR7nEtp0UR1nIJ18s=;
        b=d9Op9u02kPG2HSyjfQC2VtxzAwtyYgegSGABpbmw13rw+gPht11Iasj64jgRR7UsBC
         V5Kubp5NZuXKdOe/2G3tGw4V4eOBkhkDBw/CUftbLWTJTIrBOXFT/hHIachFgkntVWTc
         zYt8MwMZHCTPOPd6dKQ7kiOSpQQ7p9/4Xs/imJSDZFRJyTfa7TC35r+e2qku/0dN8Jos
         mTijPlrTSTrF8ZXXIIJvxzXfpHSET28FSBkoU5ltujIVxHbf4FBvTiH1hzURS33T8yJa
         P5LUZlxObQArmlPZVsKnH96PVbo3Av99IGZMNh7wDm0g+JBxsaC19T9tkm44cP00nihq
         WiIQ==
X-Gm-Message-State: ALoCoQmKNwbGATC0YvoWfinRjMO12xXQZic+nFboNi2oajLzoiO+xLKFrTNLICeWl+PV0fUyFyiD
X-Received: by 10.224.103.66 with SMTP id j2mr6683239qao.15.1398377626527;
 Thu, 24 Apr 2014 15:13:46 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.209.73 with HTTP; Thu, 24 Apr 2014 15:13:26 -0700 (PDT)
In-Reply-To: <CADKBdnrrnJ-9qGAeohdq4oYAZ=hjGoYpu2Lvh9wZnFiprn9D2Q@mail.gmail.com>
References: <CADKBdnrrnJ-9qGAeohdq4oYAZ=hjGoYpu2Lvh9wZnFiprn9D2Q@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Thu, 24 Apr 2014 15:13:26 -0700
Message-ID: <CAAswR-5aUGtmi2KQbTPhNAuTSuabStwm9+QByi8epBXxPo7zmA@mail.gmail.com>
Subject: Re: Problem creating objects through reflection
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b67001fb1152704f7d129df
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b67001fb1152704f7d129df
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

The Spark REPL is slightly modified from the normal Scala REPL to prevent
work from being done twice when closures are deserialized on the workers.
 I'm not sure exactly why this causes your problem, but its probably worth
filing a JIRA about it.

Here is another issues with classes defined in the REPL.  Not sure if it is
related, but I'd be curious if the workaround helps you:
https://issues.apache.org/jira/browse/SPARK-1199

Michael


On Thu, Apr 24, 2014 at 3:14 AM, Piotr Ko=C5=82aczkowski
<pkolaczk@datastax.com>wrote:

> Hi,
>
> I'm working on Cassandra-Spark integration and I hit a pretty severe
> problem. One of the provided functionality is mapping Cassandra rows into
> objects of user-defined classes. E.g. like this:
>
> class MyRow(val key: String, val data: Int)
> sc.cassandraTable("keyspace", "table").select("key", "data").as[MyRow]  /=
/
> returns CassandraRDD[MyRow]
>
> In this example CassandraRDD creates MyRow instances by reflection, i.e.
> matches selected fields from Cassandra table and passes them to the
> constructor.
>
> Unfortunately this does not work in Spark REPL.
> Turns out any class declared on the REPL is an inner classes, and to be
> successfully created, it needs a reference to the outer object, even thou=
gh
> it doesn't really use anything from the outer context.
>
> scala> class SomeClass
> defined class SomeClass
>
> scala> classOf[SomeClass].getConstructors()(0)
> res11: java.lang.reflect.Constructor[_] =3D public
> $iwC$$iwC$SomeClass($iwC$$iwC)
>
> I tried passing a null as a temporary workaround, and it also doesn't wor=
k
> - I get NPE.
> How can I get a reference to the current outer object representing the
> context of the current line?
>
> Also, plain non-spark Scala REPL doesn't exhibit this behaviour - and
> classes declared on the REPL are proper top-most classes, not inner ones.
> Why?
>
> Thanks,
> Piotr
>
>
>
>
>
>
>
> --
> Piotr Kolaczkowski, Lead Software Engineer
> pkolaczk@datastax.com
>
> 777 Mariners Island Blvd., Suite 510
> San Mateo, CA 94404
>

--047d7b67001fb1152704f7d129df--

From dev-return-7431-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 25 07:29:11 2014
Return-Path: <dev-return-7431-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0865811FF5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 25 Apr 2014 07:29:11 +0000 (UTC)
Received: (qmail 31161 invoked by uid 500); 25 Apr 2014 07:29:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30715 invoked by uid 500); 25 Apr 2014 07:29:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30646 invoked by uid 99); 25 Apr 2014 07:28:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Apr 2014 07:28:58 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pkolaczk@datastax.com designates 209.85.214.175 as permitted sender)
Received: from [209.85.214.175] (HELO mail-ob0-f175.google.com) (209.85.214.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Apr 2014 07:28:54 +0000
Received: by mail-ob0-f175.google.com with SMTP id wp4so3826988obc.20
        for <dev@spark.apache.org>; Fri, 25 Apr 2014 00:28:31 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=89Eqkxp0oU6/a1kKCYSCy6ghlPJdbGhBJqEUIfgqWIw=;
        b=D1q+X06Qmt13bX+6gctOB5fq1TZBi35eJvNPG70QKxmhuFTDoAyDiICB69KZYSZrHN
         6juVi0QwYI0OgTNd/PfKKwPq14y/cVcctcwUJXCjguF22TglcZvX/ozjmMwRvjytxf0V
         5yo5hZHc1W6oEoqIfUj7FzaKNX/2qfocd5AcprriDkPlSenhJhqJEuCxlNAXxq897+bl
         ZlB/9i02YN3xRNqHYZUXXpoXCQ/qRSPz5DZgNQHp34G9sv5t0o3oTAEsMgd5CZSSQRX0
         JF+yHgSaJCXzREiyJCPrGizLxPUmjbKegPFtU4stnNXeSC0qyxcsxLngeZWMnrWvFaLS
         pjYQ==
X-Gm-Message-State: ALoCoQn7ztGFP8zeixy9Ofig4UAPE2R78BRwpVDut+lMpOE7BXfWdQAihPlG5XrejKoMJuX61lk/
X-Received: by 10.182.102.99 with SMTP id fn3mr929681obb.57.1398410911683;
 Fri, 25 Apr 2014 00:28:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.182.122.227 with HTTP; Fri, 25 Apr 2014 00:28:11 -0700 (PDT)
In-Reply-To: <CAAswR-5aUGtmi2KQbTPhNAuTSuabStwm9+QByi8epBXxPo7zmA@mail.gmail.com>
References: <CADKBdnrrnJ-9qGAeohdq4oYAZ=hjGoYpu2Lvh9wZnFiprn9D2Q@mail.gmail.com>
 <CAAswR-5aUGtmi2KQbTPhNAuTSuabStwm9+QByi8epBXxPo7zmA@mail.gmail.com>
From: =?UTF-8?Q?Piotr_Ko=C5=82aczkowski?= <pkolaczk@datastax.com>
Date: Fri, 25 Apr 2014 09:28:11 +0200
Message-ID: <CADKBdnoymVB4QS62VoXj12gKeGsRazeu+TxvMugvqHMoZR4wfQ@mail.gmail.com>
Subject: Re: Problem creating objects through reflection
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e013d0d68a4457704f7d8e993
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013d0d68a4457704f7d8e993
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Yeah, this is related.

From
https://groups.google.com/forum/#!msg/spark-users/bwAmbUgxWrA/HwP4Nv4adfEJ:
"This is a limitation that will hopefully go away in Scala 2.10 or 2.10 .1,
when we'll use macros to remove the need to do this. (Or more generally if
we get some changes in the Scala interpreter to do something smarter in
this case.) "

We're using Spark 0.9.0, Scala 2.10.3 and the limitation is there. Any
ideas when it is going to be fixed?

The workaround with embedding everything inside a singleton object does not
work for me, because nested classes defined there are still inner  and
require additional argument to the constructor (when invoked by
reflection).

If I only had some reliable way to obtain a reference to that outer object
by reflection, we could somehow workaround it. E.g. saving it in some
singleton object, etc. However, a proper fix would be to make non-inner
classes properly non-inner.

Thanks,
Piotr




2014-04-25 0:13 GMT+02:00 Michael Armbrust <michael@databricks.com>:

> The Spark REPL is slightly modified from the normal Scala REPL to prevent
> work from being done twice when closures are deserialized on the workers.
>  I'm not sure exactly why this causes your problem, but its probably wort=
h
> filing a JIRA about it.
>
> Here is another issues with classes defined in the REPL.  Not sure if it =
is
> related, but I'd be curious if the workaround helps you:
> https://issues.apache.org/jira/browse/SPARK-1199
>
> Michael
>
>
> On Thu, Apr 24, 2014 at 3:14 AM, Piotr Ko=C5=82aczkowski
> <pkolaczk@datastax.com>wrote:
>
> > Hi,
> >
> > I'm working on Cassandra-Spark integration and I hit a pretty severe
> > problem. One of the provided functionality is mapping Cassandra rows in=
to
> > objects of user-defined classes. E.g. like this:
> >
> > class MyRow(val key: String, val data: Int)
> > sc.cassandraTable("keyspace", "table").select("key", "data").as[MyRow]
>  //
> > returns CassandraRDD[MyRow]
> >
> > In this example CassandraRDD creates MyRow instances by reflection, i.e=
.
> > matches selected fields from Cassandra table and passes them to the
> > constructor.
> >
> > Unfortunately this does not work in Spark REPL.
> > Turns out any class declared on the REPL is an inner classes, and to be
> > successfully created, it needs a reference to the outer object, even
> though
> > it doesn't really use anything from the outer context.
> >
> > scala> class SomeClass
> > defined class SomeClass
> >
> > scala> classOf[SomeClass].getConstructors()(0)
> > res11: java.lang.reflect.Constructor[_] =3D public
> > $iwC$$iwC$SomeClass($iwC$$iwC)
> >
> > I tried passing a null as a temporary workaround, and it also doesn't
> work
> > - I get NPE.
> > How can I get a reference to the current outer object representing the
> > context of the current line?
> >
> > Also, plain non-spark Scala REPL doesn't exhibit this behaviour - and
> > classes declared on the REPL are proper top-most classes, not inner one=
s.
> > Why?
> >
> > Thanks,
> > Piotr
> >
> >
> >
> >
> >
> >
> >
> > --
> > Piotr Kolaczkowski, Lead Software Engineer
> > pkolaczk@datastax.com
> >
> > 777 Mariners Island Blvd., Suite 510
> > San Mateo, CA 94404
> >
>



--=20
Piotr Kolaczkowski, Lead Software Engineer
pkolaczk@datastax.com

777 Mariners Island Blvd., Suite 510
San Mateo, CA 94404

--089e013d0d68a4457704f7d8e993--

From dev-return-7432-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 25 07:52:05 2014
Return-Path: <dev-return-7432-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 53DBC1106B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 25 Apr 2014 07:52:05 +0000 (UTC)
Received: (qmail 61044 invoked by uid 500); 25 Apr 2014 07:52:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60764 invoked by uid 500); 25 Apr 2014 07:52:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60748 invoked by uid 99); 25 Apr 2014 07:52:02 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Apr 2014 07:52:02 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of foundart@gmail.com designates 209.85.160.47 as permitted sender)
Received: from [209.85.160.47] (HELO mail-pb0-f47.google.com) (209.85.160.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Apr 2014 07:51:57 +0000
Received: by mail-pb0-f47.google.com with SMTP id up15so2913963pbc.34
        for <dev@spark.apache.org>; Fri, 25 Apr 2014 00:51:37 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=3SIWrfacP3yZ/zCzLJ2m07wXQtRqwSNd0lN+XjvFKYo=;
        b=GKPo3lkt1dods6Jvy4hmkFmU8WrpJLjgHplVYYTjmZiXX1Abc6/VUkZ3RhKvqxeVMU
         KMmBWxf+v7uzUYpGjdcjzzQh9oP4v0GuDA+D3IkrQ6HT7A6vmBKUR8clxekb0WW+U17a
         sgD0tgX+stbNMv41Bbfag+OXTtlxSG1gbCGxKYUTOP1VkXuLK8R/XZaniNNy4go9Ijb5
         Po9IlMnJiMIYdvk8f+px5e/VoE7mvzpYkBNIp44vC7FTh9xALJa4tUZOhbPh+O4muH0J
         cky0LISzgnWe7+Wkmo/JN3Ovqf74VApsYd/YpyV0xBdSCAhgUNLC4mOfLFRMaxieuukG
         7luQ==
MIME-Version: 1.0
X-Received: by 10.68.99.194 with SMTP id es2mr9517369pbb.100.1398412296921;
 Fri, 25 Apr 2014 00:51:36 -0700 (PDT)
Received: by 10.66.142.196 with HTTP; Fri, 25 Apr 2014 00:51:36 -0700 (PDT)
Date: Fri, 25 Apr 2014 00:51:36 -0700
Message-ID: <CAASS4f7R7hyW1N3f6Gesj-QUWugNumQbWR9avPgJnFkPaVYqzQ@mail.gmail.com>
Subject: thoughts on spark_ec2.py?
From: Art Peel <foundart@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b86f62e35275c04f7d93c02
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b86f62e35275c04f7d93c02
Content-Type: text/plain; charset=UTF-8

I've been setting up Spark cluster on EC2 using the provided
ec2/spark_ec2.py script and am very happy I didn't have to write it from
scratch. Thanks for providing it.

There have been some issues, though, and I have had to make some additions.
 So far, they are all additions of command-line options.  For example, the
original script allows access from anywhere to the various ports.  I've
added an option to specify what net/mask should be allowed to access those
ports.

I've filed a couple of pull requests, but they are not going anywhere.
 Given what I've seen of the traffic on this list, I don't feel that a lot
of the developers are thinking about EC2 setup. I totally agree that it is
not as important as improving the guts of Spark itself; nevertheless, I
feel that being able to run Spark on EC2 smartly and easily is valuable.

So, I have 2 questions for the committers:

1. Is ec2/spark_ec2.py something the committers
a. are not thinking about?
b. are planning to replace?
c. other

2. Should I just start a new project based on ec2/spark_ec2.py but without
all the other stuff and make (and share) my changes there?

Regards,

Art

--047d7b86f62e35275c04f7d93c02--

From dev-return-7433-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 25 16:48:42 2014
Return-Path: <dev-return-7433-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7DEE4F237
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 25 Apr 2014 16:48:42 +0000 (UTC)
Received: (qmail 77001 invoked by uid 500); 25 Apr 2014 16:48:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76920 invoked by uid 500); 25 Apr 2014 16:48:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76912 invoked by uid 99); 25 Apr 2014 16:48:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Apr 2014 16:48:39 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.181] (HELO mail-pd0-f181.google.com) (209.85.192.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Apr 2014 16:48:34 +0000
Received: by mail-pd0-f181.google.com with SMTP id w10so3290311pde.26
        for <dev@spark.apache.org>; Fri, 25 Apr 2014 09:48:11 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=gNErH7CzPDejw9Ek2RYvxwF+Q3vIk/SHoReJT3/PMeA=;
        b=RFEXJtk6mMfzpA7wQq0X/27/VDZERTZQd+EauVtarITARmafrKd2aBw+ieYaxtQS5T
         XbU33ORg6pEMz1BA7ZaIcl3mLrrLWP1exRRHNEJCeAK4eWG2AQ8TlJeNARjFGPFN3bQi
         VB8BH+ZuKsc0UNkv0vGDhTmO+N0J2rXH89auJVvwtbQmBNsIwhYXDWFZIdOvZwjSgczB
         d4X+LPZgw7PwwkVD1lHwVp9xSRVDjaaucxzyf9ZvgBSXyK7K9MW9oAdPs7YnhNqsM8if
         H8nG6VVSeYUgODlJE8TWg6HGBDxjtCST3efzaGbpHLX3ECdvMN8ovJluEtKuiZ6jWqaW
         Qw3Q==
X-Gm-Message-State: ALoCoQmWU/n595kEIqFXyWf0e/cBQQ9nZ6Nw0OQgFwRjMwnS4B3mNehig3ZX5tI6euKTrAJHL2x4
MIME-Version: 1.0
X-Received: by 10.69.0.198 with SMTP id ba6mr12753727pbd.16.1398444491074;
 Fri, 25 Apr 2014 09:48:11 -0700 (PDT)
Received: by 10.70.53.8 with HTTP; Fri, 25 Apr 2014 09:48:11 -0700 (PDT)
In-Reply-To: <CAASS4f7R7hyW1N3f6Gesj-QUWugNumQbWR9avPgJnFkPaVYqzQ@mail.gmail.com>
References: <CAASS4f7R7hyW1N3f6Gesj-QUWugNumQbWR9avPgJnFkPaVYqzQ@mail.gmail.com>
Date: Fri, 25 Apr 2014 09:48:11 -0700
Message-ID: <CAMJOb8kaqKdOuuen8youLwOX1XS4ypn4KZLgZhEcUn-mfdQrFQ@mail.gmail.com>
Subject: Re: thoughts on spark_ec2.py?
From: Andrew Or <andrew@databricks.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b2e40c221035304f7e0bb4d
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b2e40c221035304f7e0bb4d
Content-Type: text/plain; charset=ISO-8859-1

Hi Art,

First of all thanks a lot for your PRs. We are currently in the middle of
all the Spark 1.0 release so most of us are swamped with the more core
features. To answer your questions:

1. Neither. We welcome changes from developers for all components of Spark,
including the EC2 scripts. Once the release is out we will have more time
to review the many PRs that we missed on the ride.

2. We prefer to keep the EC2 scripts within Spark, at least for now.

Cheers,
Andrew

On Friday, April 25, 2014, Art Peel <foundart@gmail.com> wrote:

> I've been setting up Spark cluster on EC2 using the provided
> ec2/spark_ec2.py script and am very happy I didn't have to write it from
> scratch. Thanks for providing it.
>
> There have been some issues, though, and I have had to make some additions.
>  So far, they are all additions of command-line options.  For example, the
> original script allows access from anywhere to the various ports.  I've
> added an option to specify what net/mask should be allowed to access those
> ports.
>
> I've filed a couple of pull requests, but they are not going anywhere.
>  Given what I've seen of the traffic on this list, I don't feel that a lot
> of the developers are thinking about EC2 setup. I totally agree that it is
> not as important as improving the guts of Spark itself; nevertheless, I
> feel that being able to run Spark on EC2 smartly and easily is valuable.
>
> So, I have 2 questions for the committers:
>
> 1. Is ec2/spark_ec2.py something the committers
> a. are not thinking about?
> b. are planning to replace?
> c. other
>
> 2. Should I just start a new project based on ec2/spark_ec2.py but without
> all the other stuff and make (and share) my changes there?
>
> Regards,
>
> Art
>

--047d7b2e40c221035304f7e0bb4d--

From dev-return-7434-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 25 21:58:18 2014
Return-Path: <dev-return-7434-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 47A6F1113E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 25 Apr 2014 21:58:18 +0000 (UTC)
Received: (qmail 96191 invoked by uid 500); 25 Apr 2014 21:58:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96068 invoked by uid 500); 25 Apr 2014 21:58:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96060 invoked by uid 99); 25 Apr 2014 21:58:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Apr 2014 21:58:16 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Apr 2014 21:58:12 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 6667D1020CB
	for <dev@spark.apache.org>; Fri, 25 Apr 2014 14:57:48 -0700 (PDT)
Received: from mail-qg0-f42.google.com (mail-qg0-f42.google.com [209.85.192.42])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 1C851101FD1
	for <dev@spark.apache.org>; Fri, 25 Apr 2014 14:57:47 -0700 (PDT)
Received: by mail-qg0-f42.google.com with SMTP id i50so4774863qgf.1
        for <dev@spark.apache.org>; Fri, 25 Apr 2014 14:57:46 -0700 (PDT)
X-Gm-Message-State: ALoCoQkgMgivo5oUhU422mP3Jp0qvmxSim7T8rvY5Q4vXaI9fR95se8hi3vvZxGhu8byLQQ9Wrg7
MIME-Version: 1.0
X-Received: by 10.224.160.206 with SMTP id o14mr15449386qax.44.1398463066184;
 Fri, 25 Apr 2014 14:57:46 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Fri, 25 Apr 2014 14:57:46 -0700 (PDT)
In-Reply-To: <CAEYYnxaKTW-74AmKSAAg_kQkAyqePjRNhn9rS2VVbt8wp_4CsA@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxZFmxXkrNhdUNF3je7k0Eg_YjY2WnvVW5SSk__FAWNz4w@mail.gmail.com>
	<CAJgQjQ-k5P3CxVFCE-pHtuH9fVwwCcPc6PBp1NgxM2uueOASsw@mail.gmail.com>
	<CAJgQjQ_ePvspakEfiHzxdDH30EeEzjxsz0y_G0n51VophVL1hg@mail.gmail.com>
	<CAEYYnxZb3GbjdVVWyF0F4c_sEikadefP88rnWuGtHrx9HwMBJQ@mail.gmail.com>
	<CAJgQjQ8s7dP-rUtiiSUV2imeL9r++_WEJj1RyWCy+b3QBEcxRw@mail.gmail.com>
	<CAEYYnxaSh532XmO7224BiM5D2g_Q0+k49AaFbuT6LxDCmSUZBw@mail.gmail.com>
	<CAEYYnxaKTW-74AmKSAAg_kQkAyqePjRNhn9rS2VVbt8wp_4CsA@mail.gmail.com>
Date: Fri, 25 Apr 2014 14:57:46 -0700
Message-ID: <CAEYYnxZNLk+-Hh8o7V88GBR+pTZxE0qm+mw4YxFeun7grsPEng@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: DB Tsai <dbtsai@stanford.edu>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: David Hall <dlwh@cs.berkeley.edu>, shivaram@eecs.berkeley.edu, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0149d01a4acb6c04f7e50e4c
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149d01a4acb6c04f7e50e4c
Content-Type: text/plain; charset=UTF-8

Another interesting benchmark.

*News20 dataset - 0.14M row, 1,355,191 features, 0.034% non-zero elements.*

LBFGS converges in 70 seconds, while GD seems to be not progressing.

Dense feature vector will be too big to fit in the memory, so only conduct
the sparse benchmark.

I saw the sometimes the loss bumps up, and it's weird for me. Since the
cost function of logistic regression is convex, it should be monotonically
decreasing.  David, any suggestion?

The detail figure:
https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/news20.pdf


*Rcv1 dataset - 6.8M row, 677,399 features, 0.15% non-zero elements.*

LBFGS converges in 25 seconds, while GD also seems to be not progressing.

Only conduct sparse benchmark for the same reason. I also saw the loss
bumps up for unknown reason.

The detail figure:
https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/rcv1.pdf


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Thu, Apr 24, 2014 at 2:36 PM, DB Tsai <dbtsai@stanford.edu> wrote:

> rcv1.binary is too sparse (0.15% non-zero elements), so dense format will
> not run due to out of memory. But sparse format runs really well.
>
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Thu, Apr 24, 2014 at 1:54 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>
>> I'm doing the timer in runMiniBatchSGD after  val numExamples =
>> data.count()
>>
>> See the following. Running rcv1 dataset now, and will update soon.
>>
>>     val startTime = System.nanoTime()
>>     for (i <- 1 to numIterations) {
>>       // Sample a subset (fraction miniBatchFraction) of the total data
>>       // compute and sum up the subgradients on this subset (this is one
>> map-reduce)
>>       val (gradientSum, lossSum) = data.sample(false, miniBatchFraction,
>> 42 + i)
>>         .aggregate((BDV.zeros[Double](weights.size), 0.0))(
>>           seqOp = (c, v) => (c, v) match { case ((grad, loss), (label,
>> features)) =>
>>             val l = gradient.compute(features, label, weights,
>> Vectors.fromBreeze(grad))
>>             (grad, loss + l)
>>           },
>>           combOp = (c1, c2) => (c1, c2) match { case ((grad1, loss1),
>> (grad2, loss2)) =>
>>             (grad1 += grad2, loss1 + loss2)
>>           })
>>
>>       /**
>>        * NOTE(Xinghao): lossSum is computed using the weights from the
>> previous iteration
>>        * and regVal is the regularization value computed in the previous
>> iteration as well.
>>        */
>>       stochasticLossHistory.append(lossSum / miniBatchSize + regVal)
>>       val update = updater.compute(
>>         weights, Vectors.fromBreeze(gradientSum / miniBatchSize),
>> stepSize, i, regParam)
>>       weights = update._1
>>       regVal = update._2
>>       timeStamp.append(System.nanoTime() - startTime)
>>     }
>>
>>
>>
>>
>>
>>
>> Sincerely,
>>
>> DB Tsai
>> -------------------------------------------------------
>> My Blog: https://www.dbtsai.com
>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>
>>
>> On Thu, Apr 24, 2014 at 1:44 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>
>>> I don't understand why sparse falls behind dense so much at the very
>>> first iteration. I didn't see count() is called in
>>>
>>> https://github.com/dbtsai/spark-lbfgs-benchmark/blob/master/src/main/scala/org/apache/spark/mllib/benchmark/BinaryLogisticRegression.scala
>>> . Maybe you have local uncommitted changes.
>>>
>>> Best,
>>> Xiangrui
>>>
>>> On Thu, Apr 24, 2014 at 11:26 AM, DB Tsai <dbtsai@stanford.edu> wrote:
>>> > Hi Xiangrui,
>>> >
>>> > Yes, I'm using yarn-cluster mode, and I did check # of executors I
>>> specified
>>> > are the same as the actual running executors.
>>> >
>>> > For caching and materialization, I've the timer in optimizer after
>>> calling
>>> > count(); as a result, the time for materialization in cache isn't in
>>> the
>>> > benchmark.
>>> >
>>> > The difference you saw is actually from dense feature or sparse feature
>>> > vector. For LBFGS and GD dense feature, you can see the first iteration
>>> > takes the same time. It's true for GD.
>>> >
>>> > I'm going to run rcv1.binary which only has 0.15% non-zero elements to
>>> > verify the hypothesis.
>>> >
>>> >
>>> > Sincerely,
>>> >
>>> > DB Tsai
>>> > -------------------------------------------------------
>>> > My Blog: https://www.dbtsai.com
>>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>>> >
>>> >
>>> > On Thu, Apr 24, 2014 at 1:09 AM, Xiangrui Meng <mengxr@gmail.com>
>>> wrote:
>>> >>
>>> >> Hi DB,
>>> >>
>>> >> I saw you are using yarn-cluster mode for the benchmark. I tested the
>>> >> yarn-cluster mode and found that YARN does not always give you the
>>> >> exact number of executors requested. Just want to confirm that you've
>>> >> checked the number of executors.
>>> >>
>>> >> The second thing to check is that in the benchmark code, after you
>>> >> call cache, you should also call count() to materialize the RDD. I saw
>>> >> in the result, the real difference is actually at the first step.
>>> >> Adding intercept is not a cheap operation for sparse vectors.
>>> >>
>>> >> Best,
>>> >> Xiangrui
>>> >>
>>> >> On Thu, Apr 24, 2014 at 12:53 AM, Xiangrui Meng <mengxr@gmail.com>
>>> wrote:
>>> >> > I don't think it is easy to make sparse faster than dense with this
>>> >> > sparsity and feature dimension. You can try rcv1.binary, which
>>> should
>>> >> > show the difference easily.
>>> >> >
>>> >> > David, the breeze operators used here are
>>> >> >
>>> >> > 1. DenseVector dot SparseVector
>>> >> > 2. axpy DenseVector SparseVector
>>> >> >
>>> >> > However, the SparseVector is passed in as Vector[Double] instead of
>>> >> > SparseVector[Double]. It might use the axpy impl of [DenseVector,
>>> >> > Vector] and call activeIterator. I didn't check whether you used
>>> >> > multimethods on axpy.
>>> >> >
>>> >> > Best,
>>> >> > Xiangrui
>>> >> >
>>> >> > On Wed, Apr 23, 2014 at 10:35 PM, DB Tsai <dbtsai@stanford.edu>
>>> wrote:
>>> >> >> The figure showing the Log-Likelihood vs Time can be found here.
>>> >> >>
>>> >> >>
>>> >> >>
>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
>>> >> >>
>>> >> >> Let me know if you can not open it. Thanks.
>>> >> >>
>>> >> >> Sincerely,
>>> >> >>
>>> >> >> DB Tsai
>>> >> >> -------------------------------------------------------
>>> >> >> My Blog: https://www.dbtsai.com
>>> >> >> LinkedIn: https://www.linkedin.com/in/dbtsai
>>> >> >>
>>> >> >>
>>> >> >> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman
>>> >> >> <shivaram@eecs.berkeley.edu> wrote:
>>> >> >>> I don't think the attachment came through in the list. Could you
>>> >> >>> upload the
>>> >> >>> results somewhere and link to them ?
>>> >> >>>
>>> >> >>>
>>> >> >>> On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com>
>>> wrote:
>>> >> >>>>
>>> >> >>>> 123 features per rows, and in average, 89% are zeros.
>>> >> >>>> On Apr 23, 2014 9:31 PM, "Evan Sparks" <evan.sparks@gmail.com>
>>> wrote:
>>> >> >>>>
>>> >> >>>> > What is the number of non zeroes per row (and number of
>>> features)
>>> >> >>>> > in the
>>> >> >>>> > sparse case? We've hit some issues with breeze sparse support
>>> in
>>> >> >>>> > the
>>> >> >>>> > past
>>> >> >>>> > but for sufficiently sparse data it's still pretty good.
>>> >> >>>> >
>>> >> >>>> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu>
>>> wrote:
>>> >> >>>> > >
>>> >> >>>> > > Hi all,
>>> >> >>>> > >
>>> >> >>>> > > I'm benchmarking Logistic Regression in MLlib using the newly
>>> >> >>>> > > added
>>> >> >>>> > optimizer LBFGS and GD. I'm using the same dataset and the same
>>> >> >>>> > methodology
>>> >> >>>> > in this paper, http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>>> >> >>>> > >
>>> >> >>>> > > I want to know how Spark scale while adding workers, and how
>>> >> >>>> > > optimizers
>>> >> >>>> > and input format (sparse or dense) impact performance.
>>> >> >>>> > >
>>> >> >>>> > > The benchmark code can be found here,
>>> >> >>>> > https://github.com/dbtsai/spark-lbfgs-benchmark
>>> >> >>>> > >
>>> >> >>>> > > The first dataset I benchmarked is a9a which only has 2.2MB.
>>> I
>>> >> >>>> > duplicated the dataset, and made it 762MB to have 11M rows.
>>> This
>>> >> >>>> > dataset
>>> >> >>>> > has 123 features and 11% of the data are non-zero elements.
>>> >> >>>> > >
>>> >> >>>> > > In this benchmark, all the dataset is cached in memory.
>>> >> >>>> > >
>>> >> >>>> > > As we expect, LBFGS converges faster than GD, and at some
>>> point,
>>> >> >>>> > > no
>>> >> >>>> > matter how we push GD, it will converge slower and slower.
>>> >> >>>> > >
>>> >> >>>> > > However, it's surprising that sparse format runs slower than
>>> >> >>>> > > dense
>>> >> >>>> > format. I did see that sparse format takes significantly
>>> smaller
>>> >> >>>> > amount
>>> >> >>>> > of
>>> >> >>>> > memory in caching RDD, but sparse is 40% slower than dense. I
>>> think
>>> >> >>>> > sparse
>>> >> >>>> > should be fast since when we compute x wT, since x is sparse,
>>> we
>>> >> >>>> > can do
>>> >> >>>> > it
>>> >> >>>> > faster. I wonder if there is anything I'm doing wrong.
>>> >> >>>> > >
>>> >> >>>> > > The attachment is the benchmark result.
>>> >> >>>> > >
>>> >> >>>> > > Thanks.
>>> >> >>>> > >
>>> >> >>>> > > Sincerely,
>>> >> >>>> > >
>>> >> >>>> > > DB Tsai
>>> >> >>>> > > -------------------------------------------------------
>>> >> >>>> > > My Blog: https://www.dbtsai.com
>>> >> >>>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>> >> >>>> >
>>> >> >>>
>>> >> >>>
>>> >
>>> >
>>>
>>
>>
>

--089e0149d01a4acb6c04f7e50e4c--

From dev-return-7435-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 25 22:10:46 2014
Return-Path: <dev-return-7435-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0C17D111E1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 25 Apr 2014 22:10:46 +0000 (UTC)
Received: (qmail 23933 invoked by uid 500); 25 Apr 2014 22:10:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23854 invoked by uid 500); 25 Apr 2014 22:10:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23846 invoked by uid 99); 25 Apr 2014 22:10:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Apr 2014 22:10:44 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of david.lw.hall@gmail.com designates 74.125.82.173 as permitted sender)
Received: from [74.125.82.173] (HELO mail-we0-f173.google.com) (74.125.82.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Apr 2014 22:10:40 +0000
Received: by mail-we0-f173.google.com with SMTP id w61so4138378wes.18
        for <dev@spark.apache.org>; Fri, 25 Apr 2014 15:10:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:cc:content-type;
        bh=OCtCE91BMS/0X8J8iXfVLv+KWeMbYFYKZOtZFv0rY80=;
        b=HizYN2LRu8JnIga09/bKiDJvZXR2L/M4O6YC2uTIXequxsHXPNRIhex68bq1erlXrv
         RmG/kmXV6LEiaEq91Oj81zy+TAnN4tZ1kXKoT6vF39DXQ/vkgv/I8xxpkOjCL37IwSJ8
         htwPPJ6+SNmyxG8ZF2D8Es/OWNbj8miMrvVWY3OP7GtF+Mp6u1pBl+e/kfZQqoR+drBe
         31MlDowpD/H4e2n4FWIb6ScTAG3xeaP8WWO6mAqbm97OwBOL5/SVED2aUOkxbq+58eUi
         RITz+buBu5SnwU6DJUgFzDeKFT7q7h5RdDlG0eY0WeTVZBtcXSiIUNd+A/aC177v7BmU
         KxIQ==
MIME-Version: 1.0
X-Received: by 10.194.174.100 with SMTP id br4mr392140wjc.83.1398463817779;
 Fri, 25 Apr 2014 15:10:17 -0700 (PDT)
Sender: david.lw.hall@gmail.com
Received: by 10.227.232.207 with HTTP; Fri, 25 Apr 2014 15:10:17 -0700 (PDT)
In-Reply-To: <CAEYYnxZNLk+-Hh8o7V88GBR+pTZxE0qm+mw4YxFeun7grsPEng@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxZFmxXkrNhdUNF3je7k0Eg_YjY2WnvVW5SSk__FAWNz4w@mail.gmail.com>
	<CAJgQjQ-k5P3CxVFCE-pHtuH9fVwwCcPc6PBp1NgxM2uueOASsw@mail.gmail.com>
	<CAJgQjQ_ePvspakEfiHzxdDH30EeEzjxsz0y_G0n51VophVL1hg@mail.gmail.com>
	<CAEYYnxZb3GbjdVVWyF0F4c_sEikadefP88rnWuGtHrx9HwMBJQ@mail.gmail.com>
	<CAJgQjQ8s7dP-rUtiiSUV2imeL9r++_WEJj1RyWCy+b3QBEcxRw@mail.gmail.com>
	<CAEYYnxaSh532XmO7224BiM5D2g_Q0+k49AaFbuT6LxDCmSUZBw@mail.gmail.com>
	<CAEYYnxaKTW-74AmKSAAg_kQkAyqePjRNhn9rS2VVbt8wp_4CsA@mail.gmail.com>
	<CAEYYnxZNLk+-Hh8o7V88GBR+pTZxE0qm+mw4YxFeun7grsPEng@mail.gmail.com>
Date: Fri, 25 Apr 2014 15:10:17 -0700
X-Google-Sender-Auth: w9w1Au5_QxtvZyFMCVwgvqJk1rU
Message-ID: <CALW2ey0PqNry2qK_SRMs6C=S3tZ_3KoGYHeKPHG0G8EuQ6vXag@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: David Hall <dlwh@cs.berkeley.edu>
To: DB Tsai <dbtsai@stanford.edu>
Cc: Xiangrui Meng <mengxr@gmail.com>, shivaram@eecs.berkeley.edu, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e013c60bc1711d504f7e53bfa
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013c60bc1711d504f7e53bfa
Content-Type: text/plain; charset=UTF-8

LBFGS will not take a step that sends the objective value up. It might try
a step that is "too big" and reject it, so if you're just logging
everything that gets tried by LBFGS, you could see that. The "iterations"
method of the minimizer should never return an increasing objective value.
If you're regularizing, are you including the regularizer in the objective
value computation?

GD is almost never worth your time.

-- David

On Fri, Apr 25, 2014 at 2:57 PM, DB Tsai <dbtsai@stanford.edu> wrote:

> Another interesting benchmark.
>
> *News20 dataset - 0.14M row, 1,355,191 features, 0.034% non-zero elements.*
>
> LBFGS converges in 70 seconds, while GD seems to be not progressing.
>
> Dense feature vector will be too big to fit in the memory, so only conduct
> the sparse benchmark.
>
> I saw the sometimes the loss bumps up, and it's weird for me. Since the
> cost function of logistic regression is convex, it should be monotonically
> decreasing.  David, any suggestion?
>
> The detail figure:
>
> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/news20.pdf
>
>
> *Rcv1 dataset - 6.8M row, 677,399 features, 0.15% non-zero elements.*
>
> LBFGS converges in 25 seconds, while GD also seems to be not progressing.
>
> Only conduct sparse benchmark for the same reason. I also saw the loss
> bumps up for unknown reason.
>
> The detail figure:
>
> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/rcv1.pdf
>
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Thu, Apr 24, 2014 at 2:36 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>
>> rcv1.binary is too sparse (0.15% non-zero elements), so dense format
>> will not run due to out of memory. But sparse format runs really well.
>>
>>
>> Sincerely,
>>
>> DB Tsai
>> -------------------------------------------------------
>> My Blog: https://www.dbtsai.com
>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>
>>
>> On Thu, Apr 24, 2014 at 1:54 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>
>>> I'm doing the timer in runMiniBatchSGD after  val numExamples =
>>> data.count()
>>>
>>> See the following. Running rcv1 dataset now, and will update soon.
>>>
>>>     val startTime = System.nanoTime()
>>>     for (i <- 1 to numIterations) {
>>>       // Sample a subset (fraction miniBatchFraction) of the total data
>>>       // compute and sum up the subgradients on this subset (this is one
>>> map-reduce)
>>>       val (gradientSum, lossSum) = data.sample(false, miniBatchFraction,
>>> 42 + i)
>>>         .aggregate((BDV.zeros[Double](weights.size), 0.0))(
>>>           seqOp = (c, v) => (c, v) match { case ((grad, loss), (label,
>>> features)) =>
>>>             val l = gradient.compute(features, label, weights,
>>> Vectors.fromBreeze(grad))
>>>             (grad, loss + l)
>>>           },
>>>           combOp = (c1, c2) => (c1, c2) match { case ((grad1, loss1),
>>> (grad2, loss2)) =>
>>>             (grad1 += grad2, loss1 + loss2)
>>>           })
>>>
>>>       /**
>>>        * NOTE(Xinghao): lossSum is computed using the weights from the
>>> previous iteration
>>>        * and regVal is the regularization value computed in the previous
>>> iteration as well.
>>>        */
>>>       stochasticLossHistory.append(lossSum / miniBatchSize + regVal)
>>>       val update = updater.compute(
>>>         weights, Vectors.fromBreeze(gradientSum / miniBatchSize),
>>> stepSize, i, regParam)
>>>       weights = update._1
>>>       regVal = update._2
>>>       timeStamp.append(System.nanoTime() - startTime)
>>>     }
>>>
>>>
>>>
>>>
>>>
>>>
>>> Sincerely,
>>>
>>> DB Tsai
>>> -------------------------------------------------------
>>> My Blog: https://www.dbtsai.com
>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>
>>>
>>> On Thu, Apr 24, 2014 at 1:44 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>>
>>>> I don't understand why sparse falls behind dense so much at the very
>>>> first iteration. I didn't see count() is called in
>>>>
>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/blob/master/src/main/scala/org/apache/spark/mllib/benchmark/BinaryLogisticRegression.scala
>>>> . Maybe you have local uncommitted changes.
>>>>
>>>> Best,
>>>> Xiangrui
>>>>
>>>> On Thu, Apr 24, 2014 at 11:26 AM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>> > Hi Xiangrui,
>>>> >
>>>> > Yes, I'm using yarn-cluster mode, and I did check # of executors I
>>>> specified
>>>> > are the same as the actual running executors.
>>>> >
>>>> > For caching and materialization, I've the timer in optimizer after
>>>> calling
>>>> > count(); as a result, the time for materialization in cache isn't in
>>>> the
>>>> > benchmark.
>>>> >
>>>> > The difference you saw is actually from dense feature or sparse
>>>> feature
>>>> > vector. For LBFGS and GD dense feature, you can see the first
>>>> iteration
>>>> > takes the same time. It's true for GD.
>>>> >
>>>> > I'm going to run rcv1.binary which only has 0.15% non-zero elements to
>>>> > verify the hypothesis.
>>>> >
>>>> >
>>>> > Sincerely,
>>>> >
>>>> > DB Tsai
>>>> > -------------------------------------------------------
>>>> > My Blog: https://www.dbtsai.com
>>>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>> >
>>>> >
>>>> > On Thu, Apr 24, 2014 at 1:09 AM, Xiangrui Meng <mengxr@gmail.com>
>>>> wrote:
>>>> >>
>>>> >> Hi DB,
>>>> >>
>>>> >> I saw you are using yarn-cluster mode for the benchmark. I tested the
>>>> >> yarn-cluster mode and found that YARN does not always give you the
>>>> >> exact number of executors requested. Just want to confirm that you've
>>>> >> checked the number of executors.
>>>> >>
>>>> >> The second thing to check is that in the benchmark code, after you
>>>> >> call cache, you should also call count() to materialize the RDD. I
>>>> saw
>>>> >> in the result, the real difference is actually at the first step.
>>>> >> Adding intercept is not a cheap operation for sparse vectors.
>>>> >>
>>>> >> Best,
>>>> >> Xiangrui
>>>> >>
>>>> >> On Thu, Apr 24, 2014 at 12:53 AM, Xiangrui Meng <mengxr@gmail.com>
>>>> wrote:
>>>> >> > I don't think it is easy to make sparse faster than dense with this
>>>> >> > sparsity and feature dimension. You can try rcv1.binary, which
>>>> should
>>>> >> > show the difference easily.
>>>> >> >
>>>> >> > David, the breeze operators used here are
>>>> >> >
>>>> >> > 1. DenseVector dot SparseVector
>>>> >> > 2. axpy DenseVector SparseVector
>>>> >> >
>>>> >> > However, the SparseVector is passed in as Vector[Double] instead of
>>>> >> > SparseVector[Double]. It might use the axpy impl of [DenseVector,
>>>> >> > Vector] and call activeIterator. I didn't check whether you used
>>>> >> > multimethods on axpy.
>>>> >> >
>>>> >> > Best,
>>>> >> > Xiangrui
>>>> >> >
>>>> >> > On Wed, Apr 23, 2014 at 10:35 PM, DB Tsai <dbtsai@stanford.edu>
>>>> wrote:
>>>> >> >> The figure showing the Log-Likelihood vs Time can be found here.
>>>> >> >>
>>>> >> >>
>>>> >> >>
>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
>>>> >> >>
>>>> >> >> Let me know if you can not open it. Thanks.
>>>> >> >>
>>>> >> >> Sincerely,
>>>> >> >>
>>>> >> >> DB Tsai
>>>> >> >> -------------------------------------------------------
>>>> >> >> My Blog: https://www.dbtsai.com
>>>> >> >> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>> >> >>
>>>> >> >>
>>>> >> >> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman
>>>> >> >> <shivaram@eecs.berkeley.edu> wrote:
>>>> >> >>> I don't think the attachment came through in the list. Could you
>>>> >> >>> upload the
>>>> >> >>> results somewhere and link to them ?
>>>> >> >>>
>>>> >> >>>
>>>> >> >>> On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com>
>>>> wrote:
>>>> >> >>>>
>>>> >> >>>> 123 features per rows, and in average, 89% are zeros.
>>>> >> >>>> On Apr 23, 2014 9:31 PM, "Evan Sparks" <evan.sparks@gmail.com>
>>>> wrote:
>>>> >> >>>>
>>>> >> >>>> > What is the number of non zeroes per row (and number of
>>>> features)
>>>> >> >>>> > in the
>>>> >> >>>> > sparse case? We've hit some issues with breeze sparse support
>>>> in
>>>> >> >>>> > the
>>>> >> >>>> > past
>>>> >> >>>> > but for sufficiently sparse data it's still pretty good.
>>>> >> >>>> >
>>>> >> >>>> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu>
>>>> wrote:
>>>> >> >>>> > >
>>>> >> >>>> > > Hi all,
>>>> >> >>>> > >
>>>> >> >>>> > > I'm benchmarking Logistic Regression in MLlib using the
>>>> newly
>>>> >> >>>> > > added
>>>> >> >>>> > optimizer LBFGS and GD. I'm using the same dataset and the
>>>> same
>>>> >> >>>> > methodology
>>>> >> >>>> > in this paper,
>>>> http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>>>> >> >>>> > >
>>>> >> >>>> > > I want to know how Spark scale while adding workers, and how
>>>> >> >>>> > > optimizers
>>>> >> >>>> > and input format (sparse or dense) impact performance.
>>>> >> >>>> > >
>>>> >> >>>> > > The benchmark code can be found here,
>>>> >> >>>> > https://github.com/dbtsai/spark-lbfgs-benchmark
>>>> >> >>>> > >
>>>> >> >>>> > > The first dataset I benchmarked is a9a which only has
>>>> 2.2MB. I
>>>> >> >>>> > duplicated the dataset, and made it 762MB to have 11M rows.
>>>> This
>>>> >> >>>> > dataset
>>>> >> >>>> > has 123 features and 11% of the data are non-zero elements.
>>>> >> >>>> > >
>>>> >> >>>> > > In this benchmark, all the dataset is cached in memory.
>>>> >> >>>> > >
>>>> >> >>>> > > As we expect, LBFGS converges faster than GD, and at some
>>>> point,
>>>> >> >>>> > > no
>>>> >> >>>> > matter how we push GD, it will converge slower and slower.
>>>> >> >>>> > >
>>>> >> >>>> > > However, it's surprising that sparse format runs slower than
>>>> >> >>>> > > dense
>>>> >> >>>> > format. I did see that sparse format takes significantly
>>>> smaller
>>>> >> >>>> > amount
>>>> >> >>>> > of
>>>> >> >>>> > memory in caching RDD, but sparse is 40% slower than dense. I
>>>> think
>>>> >> >>>> > sparse
>>>> >> >>>> > should be fast since when we compute x wT, since x is sparse,
>>>> we
>>>> >> >>>> > can do
>>>> >> >>>> > it
>>>> >> >>>> > faster. I wonder if there is anything I'm doing wrong.
>>>> >> >>>> > >
>>>> >> >>>> > > The attachment is the benchmark result.
>>>> >> >>>> > >
>>>> >> >>>> > > Thanks.
>>>> >> >>>> > >
>>>> >> >>>> > > Sincerely,
>>>> >> >>>> > >
>>>> >> >>>> > > DB Tsai
>>>> >> >>>> > > -------------------------------------------------------
>>>> >> >>>> > > My Blog: https://www.dbtsai.com
>>>> >> >>>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>> >> >>>> >
>>>> >> >>>
>>>> >> >>>
>>>> >
>>>> >
>>>>
>>>
>>>
>>
>

--089e013c60bc1711d504f7e53bfa--

From dev-return-7436-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Apr 25 22:11:00 2014
Return-Path: <dev-return-7436-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BA873111E3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 25 Apr 2014 22:11:00 +0000 (UTC)
Received: (qmail 24669 invoked by uid 500); 25 Apr 2014 22:10:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24582 invoked by uid 500); 25 Apr 2014 22:10:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24574 invoked by uid 99); 25 Apr 2014 22:10:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Apr 2014 22:10:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of minnesota.cs@gmail.com designates 209.85.214.182 as permitted sender)
Received: from [209.85.214.182] (HELO mail-ob0-f182.google.com) (209.85.214.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Apr 2014 22:10:55 +0000
Received: by mail-ob0-f182.google.com with SMTP id uy5so4903922obc.41
        for <dev@spark.apache.org>; Fri, 25 Apr 2014 15:10:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=Y+EReyU8z+qLsfQ5TGRYhG8t9mklYMwVThmz56Pe0Nk=;
        b=Mi2hHt+gZ6AI7b81GdNE3+vJOmsCADEwv7cf5p1Ev2mHdR1pEsGrH4gevJ/B4KW1pB
         vZCIBzH2PHSA5A06PXUCLWtM1F/i9OaEbbEwaLqZ7FtNP2R+KBFvgo7u7PjfcEqA0TKF
         C8snJaAR/a0diX9Szmy4YRvBHo9tKx6q24pXNv6WM+Bc+F4C11LyIgLGZCl6dlrr3cd1
         czpdkx5Imwcz1V1zYh5n1tE7QCrImY2agh2UyqUA3M+ZwAA5uWbloSdSaqejJPif4EV5
         pWXAQoZ+fXDT3ZE9jODHdPrTiWqeOgILNiXVloR55luclko0TpujxwZfR9O9bjWGw/sG
         o0GQ==
MIME-Version: 1.0
X-Received: by 10.182.153.226 with SMTP id vj2mr1679257obb.26.1398463834648;
 Fri, 25 Apr 2014 15:10:34 -0700 (PDT)
Received: by 10.182.97.134 with HTTP; Fri, 25 Apr 2014 15:10:34 -0700 (PDT)
In-Reply-To: <CAEYYnxZNLk+-Hh8o7V88GBR+pTZxE0qm+mw4YxFeun7grsPEng@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxZFmxXkrNhdUNF3je7k0Eg_YjY2WnvVW5SSk__FAWNz4w@mail.gmail.com>
	<CAJgQjQ-k5P3CxVFCE-pHtuH9fVwwCcPc6PBp1NgxM2uueOASsw@mail.gmail.com>
	<CAJgQjQ_ePvspakEfiHzxdDH30EeEzjxsz0y_G0n51VophVL1hg@mail.gmail.com>
	<CAEYYnxZb3GbjdVVWyF0F4c_sEikadefP88rnWuGtHrx9HwMBJQ@mail.gmail.com>
	<CAJgQjQ8s7dP-rUtiiSUV2imeL9r++_WEJj1RyWCy+b3QBEcxRw@mail.gmail.com>
	<CAEYYnxaSh532XmO7224BiM5D2g_Q0+k49AaFbuT6LxDCmSUZBw@mail.gmail.com>
	<CAEYYnxaKTW-74AmKSAAg_kQkAyqePjRNhn9rS2VVbt8wp_4CsA@mail.gmail.com>
	<CAEYYnxZNLk+-Hh8o7V88GBR+pTZxE0qm+mw4YxFeun7grsPEng@mail.gmail.com>
Date: Fri, 25 Apr 2014 17:10:34 -0500
Message-ID: <CANN3bXa4xVJYfVFjSoGaJWSOeBpp3f8FpCZXsmziKeXPUNEhUQ@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: Tom Vacek <minnesota.cs@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e013d0dc018769604f7e53c4e
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013d0dc018769604f7e53c4e
Content-Type: text/plain; charset=UTF-8

I don't know about Spark's implementation, but with LBFGS, there is a line
search step.  Since computing the line search takes roughly the same work
as one iteration, an efficient implementation will take a full step and
simultaneously compute the gradient for the next step and check if the
update satisfied the line search rules.  If it failed, then the new step is
abandoned and the previous step is revised, and the process repeats.  I
suspect the loss being reported doesn't distinguish between an update step
and a line search step, so it looks like the loss is increasing.


On Fri, Apr 25, 2014 at 4:57 PM, DB Tsai <dbtsai@stanford.edu> wrote:

> Another interesting benchmark.
>
> *News20 dataset - 0.14M row, 1,355,191 features, 0.034% non-zero elements.*
>
> LBFGS converges in 70 seconds, while GD seems to be not progressing.
>
> Dense feature vector will be too big to fit in the memory, so only conduct
> the sparse benchmark.
>
> I saw the sometimes the loss bumps up, and it's weird for me. Since the
> cost function of logistic regression is convex, it should be monotonically
> decreasing.  David, any suggestion?
>
> The detail figure:
>
> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/news20.pdf
>
>
> *Rcv1 dataset - 6.8M row, 677,399 features, 0.15% non-zero elements.*
>
> LBFGS converges in 25 seconds, while GD also seems to be not progressing.
>
> Only conduct sparse benchmark for the same reason. I also saw the loss
> bumps up for unknown reason.
>
> The detail figure:
>
> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/rcv1.pdf
>
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Thu, Apr 24, 2014 at 2:36 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>
> > rcv1.binary is too sparse (0.15% non-zero elements), so dense format will
> > not run due to out of memory. But sparse format runs really well.
> >
> >
> > Sincerely,
> >
> > DB Tsai
> > -------------------------------------------------------
> > My Blog: https://www.dbtsai.com
> > LinkedIn: https://www.linkedin.com/in/dbtsai
> >
> >
> > On Thu, Apr 24, 2014 at 1:54 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> >
> >> I'm doing the timer in runMiniBatchSGD after  val numExamples =
> >> data.count()
> >>
> >> See the following. Running rcv1 dataset now, and will update soon.
> >>
> >>     val startTime = System.nanoTime()
> >>     for (i <- 1 to numIterations) {
> >>       // Sample a subset (fraction miniBatchFraction) of the total data
> >>       // compute and sum up the subgradients on this subset (this is one
> >> map-reduce)
> >>       val (gradientSum, lossSum) = data.sample(false, miniBatchFraction,
> >> 42 + i)
> >>         .aggregate((BDV.zeros[Double](weights.size), 0.0))(
> >>           seqOp = (c, v) => (c, v) match { case ((grad, loss), (label,
> >> features)) =>
> >>             val l = gradient.compute(features, label, weights,
> >> Vectors.fromBreeze(grad))
> >>             (grad, loss + l)
> >>           },
> >>           combOp = (c1, c2) => (c1, c2) match { case ((grad1, loss1),
> >> (grad2, loss2)) =>
> >>             (grad1 += grad2, loss1 + loss2)
> >>           })
> >>
> >>       /**
> >>        * NOTE(Xinghao): lossSum is computed using the weights from the
> >> previous iteration
> >>        * and regVal is the regularization value computed in the previous
> >> iteration as well.
> >>        */
> >>       stochasticLossHistory.append(lossSum / miniBatchSize + regVal)
> >>       val update = updater.compute(
> >>         weights, Vectors.fromBreeze(gradientSum / miniBatchSize),
> >> stepSize, i, regParam)
> >>       weights = update._1
> >>       regVal = update._2
> >>       timeStamp.append(System.nanoTime() - startTime)
> >>     }
> >>
> >>
> >>
> >>
> >>
> >>
> >> Sincerely,
> >>
> >> DB Tsai
> >> -------------------------------------------------------
> >> My Blog: https://www.dbtsai.com
> >> LinkedIn: https://www.linkedin.com/in/dbtsai
> >>
> >>
> >> On Thu, Apr 24, 2014 at 1:44 PM, Xiangrui Meng <mengxr@gmail.com>
> wrote:
> >>
> >>> I don't understand why sparse falls behind dense so much at the very
> >>> first iteration. I didn't see count() is called in
> >>>
> >>>
> https://github.com/dbtsai/spark-lbfgs-benchmark/blob/master/src/main/scala/org/apache/spark/mllib/benchmark/BinaryLogisticRegression.scala
> >>> . Maybe you have local uncommitted changes.
> >>>
> >>> Best,
> >>> Xiangrui
> >>>
> >>> On Thu, Apr 24, 2014 at 11:26 AM, DB Tsai <dbtsai@stanford.edu> wrote:
> >>> > Hi Xiangrui,
> >>> >
> >>> > Yes, I'm using yarn-cluster mode, and I did check # of executors I
> >>> specified
> >>> > are the same as the actual running executors.
> >>> >
> >>> > For caching and materialization, I've the timer in optimizer after
> >>> calling
> >>> > count(); as a result, the time for materialization in cache isn't in
> >>> the
> >>> > benchmark.
> >>> >
> >>> > The difference you saw is actually from dense feature or sparse
> feature
> >>> > vector. For LBFGS and GD dense feature, you can see the first
> iteration
> >>> > takes the same time. It's true for GD.
> >>> >
> >>> > I'm going to run rcv1.binary which only has 0.15% non-zero elements
> to
> >>> > verify the hypothesis.
> >>> >
> >>> >
> >>> > Sincerely,
> >>> >
> >>> > DB Tsai
> >>> > -------------------------------------------------------
> >>> > My Blog: https://www.dbtsai.com
> >>> > LinkedIn: https://www.linkedin.com/in/dbtsai
> >>> >
> >>> >
> >>> > On Thu, Apr 24, 2014 at 1:09 AM, Xiangrui Meng <mengxr@gmail.com>
> >>> wrote:
> >>> >>
> >>> >> Hi DB,
> >>> >>
> >>> >> I saw you are using yarn-cluster mode for the benchmark. I tested
> the
> >>> >> yarn-cluster mode and found that YARN does not always give you the
> >>> >> exact number of executors requested. Just want to confirm that
> you've
> >>> >> checked the number of executors.
> >>> >>
> >>> >> The second thing to check is that in the benchmark code, after you
> >>> >> call cache, you should also call count() to materialize the RDD. I
> saw
> >>> >> in the result, the real difference is actually at the first step.
> >>> >> Adding intercept is not a cheap operation for sparse vectors.
> >>> >>
> >>> >> Best,
> >>> >> Xiangrui
> >>> >>
> >>> >> On Thu, Apr 24, 2014 at 12:53 AM, Xiangrui Meng <mengxr@gmail.com>
> >>> wrote:
> >>> >> > I don't think it is easy to make sparse faster than dense with
> this
> >>> >> > sparsity and feature dimension. You can try rcv1.binary, which
> >>> should
> >>> >> > show the difference easily.
> >>> >> >
> >>> >> > David, the breeze operators used here are
> >>> >> >
> >>> >> > 1. DenseVector dot SparseVector
> >>> >> > 2. axpy DenseVector SparseVector
> >>> >> >
> >>> >> > However, the SparseVector is passed in as Vector[Double] instead
> of
> >>> >> > SparseVector[Double]. It might use the axpy impl of [DenseVector,
> >>> >> > Vector] and call activeIterator. I didn't check whether you used
> >>> >> > multimethods on axpy.
> >>> >> >
> >>> >> > Best,
> >>> >> > Xiangrui
> >>> >> >
> >>> >> > On Wed, Apr 23, 2014 at 10:35 PM, DB Tsai <dbtsai@stanford.edu>
> >>> wrote:
> >>> >> >> The figure showing the Log-Likelihood vs Time can be found here.
> >>> >> >>
> >>> >> >>
> >>> >> >>
> >>>
> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
> >>> >> >>
> >>> >> >> Let me know if you can not open it. Thanks.
> >>> >> >>
> >>> >> >> Sincerely,
> >>> >> >>
> >>> >> >> DB Tsai
> >>> >> >> -------------------------------------------------------
> >>> >> >> My Blog: https://www.dbtsai.com
> >>> >> >> LinkedIn: https://www.linkedin.com/in/dbtsai
> >>> >> >>
> >>> >> >>
> >>> >> >> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman
> >>> >> >> <shivaram@eecs.berkeley.edu> wrote:
> >>> >> >>> I don't think the attachment came through in the list. Could you
> >>> >> >>> upload the
> >>> >> >>> results somewhere and link to them ?
> >>> >> >>>
> >>> >> >>>
> >>> >> >>> On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com>
> >>> wrote:
> >>> >> >>>>
> >>> >> >>>> 123 features per rows, and in average, 89% are zeros.
> >>> >> >>>> On Apr 23, 2014 9:31 PM, "Evan Sparks" <evan.sparks@gmail.com>
> >>> wrote:
> >>> >> >>>>
> >>> >> >>>> > What is the number of non zeroes per row (and number of
> >>> features)
> >>> >> >>>> > in the
> >>> >> >>>> > sparse case? We've hit some issues with breeze sparse support
> >>> in
> >>> >> >>>> > the
> >>> >> >>>> > past
> >>> >> >>>> > but for sufficiently sparse data it's still pretty good.
> >>> >> >>>> >
> >>> >> >>>> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu>
> >>> wrote:
> >>> >> >>>> > >
> >>> >> >>>> > > Hi all,
> >>> >> >>>> > >
> >>> >> >>>> > > I'm benchmarking Logistic Regression in MLlib using the
> newly
> >>> >> >>>> > > added
> >>> >> >>>> > optimizer LBFGS and GD. I'm using the same dataset and the
> same
> >>> >> >>>> > methodology
> >>> >> >>>> > in this paper,
> http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
> >>> >> >>>> > >
> >>> >> >>>> > > I want to know how Spark scale while adding workers, and
> how
> >>> >> >>>> > > optimizers
> >>> >> >>>> > and input format (sparse or dense) impact performance.
> >>> >> >>>> > >
> >>> >> >>>> > > The benchmark code can be found here,
> >>> >> >>>> > https://github.com/dbtsai/spark-lbfgs-benchmark
> >>> >> >>>> > >
> >>> >> >>>> > > The first dataset I benchmarked is a9a which only has
> 2.2MB.
> >>> I
> >>> >> >>>> > duplicated the dataset, and made it 762MB to have 11M rows.
> >>> This
> >>> >> >>>> > dataset
> >>> >> >>>> > has 123 features and 11% of the data are non-zero elements.
> >>> >> >>>> > >
> >>> >> >>>> > > In this benchmark, all the dataset is cached in memory.
> >>> >> >>>> > >
> >>> >> >>>> > > As we expect, LBFGS converges faster than GD, and at some
> >>> point,
> >>> >> >>>> > > no
> >>> >> >>>> > matter how we push GD, it will converge slower and slower.
> >>> >> >>>> > >
> >>> >> >>>> > > However, it's surprising that sparse format runs slower
> than
> >>> >> >>>> > > dense
> >>> >> >>>> > format. I did see that sparse format takes significantly
> >>> smaller
> >>> >> >>>> > amount
> >>> >> >>>> > of
> >>> >> >>>> > memory in caching RDD, but sparse is 40% slower than dense. I
> >>> think
> >>> >> >>>> > sparse
> >>> >> >>>> > should be fast since when we compute x wT, since x is sparse,
> >>> we
> >>> >> >>>> > can do
> >>> >> >>>> > it
> >>> >> >>>> > faster. I wonder if there is anything I'm doing wrong.
> >>> >> >>>> > >
> >>> >> >>>> > > The attachment is the benchmark result.
> >>> >> >>>> > >
> >>> >> >>>> > > Thanks.
> >>> >> >>>> > >
> >>> >> >>>> > > Sincerely,
> >>> >> >>>> > >
> >>> >> >>>> > > DB Tsai
> >>> >> >>>> > > -------------------------------------------------------
> >>> >> >>>> > > My Blog: https://www.dbtsai.com
> >>> >> >>>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
> >>> >> >>>> >
> >>> >> >>>
> >>> >> >>>
> >>> >
> >>> >
> >>>
> >>
> >>
> >
>

--089e013d0dc018769604f7e53c4e--

From dev-return-7437-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Apr 26 21:20:33 2014
Return-Path: <dev-return-7437-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7C716115E1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 26 Apr 2014 21:20:33 +0000 (UTC)
Received: (qmail 91453 invoked by uid 500); 26 Apr 2014 21:20:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91394 invoked by uid 500); 26 Apr 2014 21:20:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91386 invoked by uid 99); 26 Apr 2014 21:20:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Apr 2014 21:20:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of prodigyaj@gmail.com designates 209.85.160.50 as permitted sender)
Received: from [209.85.160.50] (HELO mail-pb0-f50.google.com) (209.85.160.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Apr 2014 21:20:27 +0000
Received: by mail-pb0-f50.google.com with SMTP id md12so4419488pbc.23
        for <dev@spark.apache.org>; Sat, 26 Apr 2014 14:20:03 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=0ZTQLEzAGOk+xX3hqBPFB6okp230veB8OplLyi6geRo=;
        b=cbbbL0L4TW1Q1Y6R2NnIpq8ju04PGOZxy4ryC7fg5DX9TtfoPoBkDFlOSySCtTXP9N
         eHY1W/u+mBpNDcPWwYA08wzxfLJCEU1OCK8v6+Zs5PtZq+702O1DeYPupsc4gBzXSHLD
         rKOB+M1/84EYegXK++4SZ/6vBaignTuddgdhps5jMhgjgp6ch7JNXa4yE+oz6LuROs8a
         9ksZ4G0SlxcYYE/w9f5qMusMjXruxqxanxMKb9ZscBqovOBTAHaWkQoDmk5N40TzBX5U
         Es532sXzGC9tdlb30t0YCyL264T6OFVwHousmHp1DPxzF4v3kQmmS3mTKFVM3mHzEVIl
         Y4QQ==
MIME-Version: 1.0
X-Received: by 10.66.231.105 with SMTP id tf9mr16213232pac.84.1398547203633;
 Sat, 26 Apr 2014 14:20:03 -0700 (PDT)
Received: by 10.70.94.97 with HTTP; Sat, 26 Apr 2014 14:20:03 -0700 (PDT)
Date: Sat, 26 Apr 2014 16:20:03 -0500
Message-ID: <CAHVi7z+C7BnG2jNwQsA=qvvzHDSVJF_auTFrJ1D05inpKzQfRA@mail.gmail.com>
Subject: Parsing wikipedia xml data in Spark
From: Ajay Nair <prodigyaj@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b15a9b746367c04f7f8a55b
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b15a9b746367c04f7f8a55b
Content-Type: text/plain; charset=UTF-8

Is there a way in spark to parse wikipedia xml dump? It seems like the
freebase dump is longer available. Also does the spark shell support the
xml load file sax parser that is present in scala.

Thanks
AJ

--047d7b15a9b746367c04f7f8a55b--

From dev-return-7438-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 28 06:29:01 2014
Return-Path: <dev-return-7438-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 11A3A1154D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Apr 2014 06:29:01 +0000 (UTC)
Received: (qmail 15802 invoked by uid 500); 28 Apr 2014 06:28:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15379 invoked by uid 500); 28 Apr 2014 06:28:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15134 invoked by uid 99); 28 Apr 2014 06:28:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Apr 2014 06:28:50 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Apr 2014 06:28:46 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 76CC01018E7
	for <dev@spark.apache.org>; Sun, 27 Apr 2014 23:28:26 -0700 (PDT)
Received: from mail-qa0-f51.google.com (mail-qa0-f51.google.com [209.85.216.51])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 7DC071018AA
	for <dev@spark.apache.org>; Sun, 27 Apr 2014 23:28:25 -0700 (PDT)
Received: by mail-qa0-f51.google.com with SMTP id ih12so2613636qab.38
        for <dev@spark.apache.org>; Sun, 27 Apr 2014 23:28:24 -0700 (PDT)
X-Gm-Message-State: ALoCoQlToh1LbV/Ip56Mpm/t5RnkVr6FGwjjEd2Xv/pkCLN3TQl8JEV9nn7YHw5kDNA+ydtt0wRw
MIME-Version: 1.0
X-Received: by 10.140.47.206 with SMTP id m72mr29070210qga.21.1398666504602;
 Sun, 27 Apr 2014 23:28:24 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Sun, 27 Apr 2014 23:28:24 -0700 (PDT)
In-Reply-To: <CALW2ey0PqNry2qK_SRMs6C=S3tZ_3KoGYHeKPHG0G8EuQ6vXag@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxZFmxXkrNhdUNF3je7k0Eg_YjY2WnvVW5SSk__FAWNz4w@mail.gmail.com>
	<CAJgQjQ-k5P3CxVFCE-pHtuH9fVwwCcPc6PBp1NgxM2uueOASsw@mail.gmail.com>
	<CAJgQjQ_ePvspakEfiHzxdDH30EeEzjxsz0y_G0n51VophVL1hg@mail.gmail.com>
	<CAEYYnxZb3GbjdVVWyF0F4c_sEikadefP88rnWuGtHrx9HwMBJQ@mail.gmail.com>
	<CAJgQjQ8s7dP-rUtiiSUV2imeL9r++_WEJj1RyWCy+b3QBEcxRw@mail.gmail.com>
	<CAEYYnxaSh532XmO7224BiM5D2g_Q0+k49AaFbuT6LxDCmSUZBw@mail.gmail.com>
	<CAEYYnxaKTW-74AmKSAAg_kQkAyqePjRNhn9rS2VVbt8wp_4CsA@mail.gmail.com>
	<CAEYYnxZNLk+-Hh8o7V88GBR+pTZxE0qm+mw4YxFeun7grsPEng@mail.gmail.com>
	<CALW2ey0PqNry2qK_SRMs6C=S3tZ_3KoGYHeKPHG0G8EuQ6vXag@mail.gmail.com>
Date: Sun, 27 Apr 2014 23:28:24 -0700
Message-ID: <CAEYYnxYEOHFEp7obAWmVq=SMXxpLp3hJavyM0NAzwQ7SsN+ixA@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: DB Tsai <dbtsai@stanford.edu>
To: David Hall <dlwh@cs.berkeley.edu>
Cc: Xiangrui Meng <mengxr@gmail.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c16a7c2ab40d04f8146c88
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c16a7c2ab40d04f8146c88
Content-Type: text/plain; charset=UTF-8

Hi David,

I'm recording the loss history in the DiffFunction implementation, and
that's why the rejected step is also recorded in my loss history.

Is there any api in Breeze LBFGS to get the history which already excludes
the reject step? Or should I just call "iterations" method and check
"iteratingShouldStop" instead?

Thanks.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Fri, Apr 25, 2014 at 3:10 PM, David Hall <dlwh@cs.berkeley.edu> wrote:

> LBFGS will not take a step that sends the objective value up. It might try
> a step that is "too big" and reject it, so if you're just logging
> everything that gets tried by LBFGS, you could see that. The "iterations"
> method of the minimizer should never return an increasing objective value.
> If you're regularizing, are you including the regularizer in the objective
> value computation?
>
> GD is almost never worth your time.
>
> -- David
>
> On Fri, Apr 25, 2014 at 2:57 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>
>> Another interesting benchmark.
>>
>> *News20 dataset - 0.14M row, 1,355,191 features, 0.034% non-zero
>> elements.*
>>
>> LBFGS converges in 70 seconds, while GD seems to be not progressing.
>>
>> Dense feature vector will be too big to fit in the memory, so only
>> conduct the sparse benchmark.
>>
>> I saw the sometimes the loss bumps up, and it's weird for me. Since the
>> cost function of logistic regression is convex, it should be monotonically
>> decreasing.  David, any suggestion?
>>
>> The detail figure:
>>
>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/news20.pdf
>>
>>
>> *Rcv1 dataset - 6.8M row, 677,399 features, 0.15% non-zero elements.*
>>
>> LBFGS converges in 25 seconds, while GD also seems to be not progressing.
>>
>> Only conduct sparse benchmark for the same reason. I also saw the loss
>> bumps up for unknown reason.
>>
>> The detail figure:
>>
>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/rcv1.pdf
>>
>>
>> Sincerely,
>>
>> DB Tsai
>> -------------------------------------------------------
>> My Blog: https://www.dbtsai.com
>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>
>>
>> On Thu, Apr 24, 2014 at 2:36 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>
>>> rcv1.binary is too sparse (0.15% non-zero elements), so dense format
>>> will not run due to out of memory. But sparse format runs really well.
>>>
>>>
>>> Sincerely,
>>>
>>> DB Tsai
>>> -------------------------------------------------------
>>> My Blog: https://www.dbtsai.com
>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>
>>>
>>> On Thu, Apr 24, 2014 at 1:54 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>
>>>> I'm doing the timer in runMiniBatchSGD after  val numExamples =
>>>> data.count()
>>>>
>>>> See the following. Running rcv1 dataset now, and will update soon.
>>>>
>>>>     val startTime = System.nanoTime()
>>>>     for (i <- 1 to numIterations) {
>>>>       // Sample a subset (fraction miniBatchFraction) of the total data
>>>>       // compute and sum up the subgradients on this subset (this is
>>>> one map-reduce)
>>>>       val (gradientSum, lossSum) = data.sample(false,
>>>> miniBatchFraction, 42 + i)
>>>>         .aggregate((BDV.zeros[Double](weights.size), 0.0))(
>>>>           seqOp = (c, v) => (c, v) match { case ((grad, loss), (label,
>>>> features)) =>
>>>>             val l = gradient.compute(features, label, weights,
>>>> Vectors.fromBreeze(grad))
>>>>             (grad, loss + l)
>>>>           },
>>>>           combOp = (c1, c2) => (c1, c2) match { case ((grad1, loss1),
>>>> (grad2, loss2)) =>
>>>>             (grad1 += grad2, loss1 + loss2)
>>>>           })
>>>>
>>>>       /**
>>>>        * NOTE(Xinghao): lossSum is computed using the weights from the
>>>> previous iteration
>>>>        * and regVal is the regularization value computed in the
>>>> previous iteration as well.
>>>>        */
>>>>       stochasticLossHistory.append(lossSum / miniBatchSize + regVal)
>>>>       val update = updater.compute(
>>>>         weights, Vectors.fromBreeze(gradientSum / miniBatchSize),
>>>> stepSize, i, regParam)
>>>>       weights = update._1
>>>>       regVal = update._2
>>>>       timeStamp.append(System.nanoTime() - startTime)
>>>>     }
>>>>
>>>>
>>>>
>>>>
>>>>
>>>>
>>>> Sincerely,
>>>>
>>>> DB Tsai
>>>> -------------------------------------------------------
>>>> My Blog: https://www.dbtsai.com
>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>
>>>>
>>>> On Thu, Apr 24, 2014 at 1:44 PM, Xiangrui Meng <mengxr@gmail.com>wrote:
>>>>
>>>>> I don't understand why sparse falls behind dense so much at the very
>>>>> first iteration. I didn't see count() is called in
>>>>>
>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/blob/master/src/main/scala/org/apache/spark/mllib/benchmark/BinaryLogisticRegression.scala
>>>>> . Maybe you have local uncommitted changes.
>>>>>
>>>>> Best,
>>>>> Xiangrui
>>>>>
>>>>> On Thu, Apr 24, 2014 at 11:26 AM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>>> > Hi Xiangrui,
>>>>> >
>>>>> > Yes, I'm using yarn-cluster mode, and I did check # of executors I
>>>>> specified
>>>>> > are the same as the actual running executors.
>>>>> >
>>>>> > For caching and materialization, I've the timer in optimizer after
>>>>> calling
>>>>> > count(); as a result, the time for materialization in cache isn't in
>>>>> the
>>>>> > benchmark.
>>>>> >
>>>>> > The difference you saw is actually from dense feature or sparse
>>>>> feature
>>>>> > vector. For LBFGS and GD dense feature, you can see the first
>>>>> iteration
>>>>> > takes the same time. It's true for GD.
>>>>> >
>>>>> > I'm going to run rcv1.binary which only has 0.15% non-zero elements
>>>>> to
>>>>> > verify the hypothesis.
>>>>> >
>>>>> >
>>>>> > Sincerely,
>>>>> >
>>>>> > DB Tsai
>>>>> > -------------------------------------------------------
>>>>> > My Blog: https://www.dbtsai.com
>>>>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>> >
>>>>> >
>>>>> > On Thu, Apr 24, 2014 at 1:09 AM, Xiangrui Meng <mengxr@gmail.com>
>>>>> wrote:
>>>>> >>
>>>>> >> Hi DB,
>>>>> >>
>>>>> >> I saw you are using yarn-cluster mode for the benchmark. I tested
>>>>> the
>>>>> >> yarn-cluster mode and found that YARN does not always give you the
>>>>> >> exact number of executors requested. Just want to confirm that
>>>>> you've
>>>>> >> checked the number of executors.
>>>>> >>
>>>>> >> The second thing to check is that in the benchmark code, after you
>>>>> >> call cache, you should also call count() to materialize the RDD. I
>>>>> saw
>>>>> >> in the result, the real difference is actually at the first step.
>>>>> >> Adding intercept is not a cheap operation for sparse vectors.
>>>>> >>
>>>>> >> Best,
>>>>> >> Xiangrui
>>>>> >>
>>>>> >> On Thu, Apr 24, 2014 at 12:53 AM, Xiangrui Meng <mengxr@gmail.com>
>>>>> wrote:
>>>>> >> > I don't think it is easy to make sparse faster than dense with
>>>>> this
>>>>> >> > sparsity and feature dimension. You can try rcv1.binary, which
>>>>> should
>>>>> >> > show the difference easily.
>>>>> >> >
>>>>> >> > David, the breeze operators used here are
>>>>> >> >
>>>>> >> > 1. DenseVector dot SparseVector
>>>>> >> > 2. axpy DenseVector SparseVector
>>>>> >> >
>>>>> >> > However, the SparseVector is passed in as Vector[Double] instead
>>>>> of
>>>>> >> > SparseVector[Double]. It might use the axpy impl of [DenseVector,
>>>>> >> > Vector] and call activeIterator. I didn't check whether you used
>>>>> >> > multimethods on axpy.
>>>>> >> >
>>>>> >> > Best,
>>>>> >> > Xiangrui
>>>>> >> >
>>>>> >> > On Wed, Apr 23, 2014 at 10:35 PM, DB Tsai <dbtsai@stanford.edu>
>>>>> wrote:
>>>>> >> >> The figure showing the Log-Likelihood vs Time can be found here.
>>>>> >> >>
>>>>> >> >>
>>>>> >> >>
>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
>>>>> >> >>
>>>>> >> >> Let me know if you can not open it. Thanks.
>>>>> >> >>
>>>>> >> >> Sincerely,
>>>>> >> >>
>>>>> >> >> DB Tsai
>>>>> >> >> -------------------------------------------------------
>>>>> >> >> My Blog: https://www.dbtsai.com
>>>>> >> >> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>> >> >>
>>>>> >> >>
>>>>> >> >> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman
>>>>> >> >> <shivaram@eecs.berkeley.edu> wrote:
>>>>> >> >>> I don't think the attachment came through in the list. Could you
>>>>> >> >>> upload the
>>>>> >> >>> results somewhere and link to them ?
>>>>> >> >>>
>>>>> >> >>>
>>>>> >> >>> On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com>
>>>>> wrote:
>>>>> >> >>>>
>>>>> >> >>>> 123 features per rows, and in average, 89% are zeros.
>>>>> >> >>>> On Apr 23, 2014 9:31 PM, "Evan Sparks" <evan.sparks@gmail.com>
>>>>> wrote:
>>>>> >> >>>>
>>>>> >> >>>> > What is the number of non zeroes per row (and number of
>>>>> features)
>>>>> >> >>>> > in the
>>>>> >> >>>> > sparse case? We've hit some issues with breeze sparse
>>>>> support in
>>>>> >> >>>> > the
>>>>> >> >>>> > past
>>>>> >> >>>> > but for sufficiently sparse data it's still pretty good.
>>>>> >> >>>> >
>>>>> >> >>>> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu>
>>>>> wrote:
>>>>> >> >>>> > >
>>>>> >> >>>> > > Hi all,
>>>>> >> >>>> > >
>>>>> >> >>>> > > I'm benchmarking Logistic Regression in MLlib using the
>>>>> newly
>>>>> >> >>>> > > added
>>>>> >> >>>> > optimizer LBFGS and GD. I'm using the same dataset and the
>>>>> same
>>>>> >> >>>> > methodology
>>>>> >> >>>> > in this paper,
>>>>> http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>>>>> >> >>>> > >
>>>>> >> >>>> > > I want to know how Spark scale while adding workers, and
>>>>> how
>>>>> >> >>>> > > optimizers
>>>>> >> >>>> > and input format (sparse or dense) impact performance.
>>>>> >> >>>> > >
>>>>> >> >>>> > > The benchmark code can be found here,
>>>>> >> >>>> > https://github.com/dbtsai/spark-lbfgs-benchmark
>>>>> >> >>>> > >
>>>>> >> >>>> > > The first dataset I benchmarked is a9a which only has
>>>>> 2.2MB. I
>>>>> >> >>>> > duplicated the dataset, and made it 762MB to have 11M rows.
>>>>> This
>>>>> >> >>>> > dataset
>>>>> >> >>>> > has 123 features and 11% of the data are non-zero elements.
>>>>> >> >>>> > >
>>>>> >> >>>> > > In this benchmark, all the dataset is cached in memory.
>>>>> >> >>>> > >
>>>>> >> >>>> > > As we expect, LBFGS converges faster than GD, and at some
>>>>> point,
>>>>> >> >>>> > > no
>>>>> >> >>>> > matter how we push GD, it will converge slower and slower.
>>>>> >> >>>> > >
>>>>> >> >>>> > > However, it's surprising that sparse format runs slower
>>>>> than
>>>>> >> >>>> > > dense
>>>>> >> >>>> > format. I did see that sparse format takes significantly
>>>>> smaller
>>>>> >> >>>> > amount
>>>>> >> >>>> > of
>>>>> >> >>>> > memory in caching RDD, but sparse is 40% slower than dense.
>>>>> I think
>>>>> >> >>>> > sparse
>>>>> >> >>>> > should be fast since when we compute x wT, since x is
>>>>> sparse, we
>>>>> >> >>>> > can do
>>>>> >> >>>> > it
>>>>> >> >>>> > faster. I wonder if there is anything I'm doing wrong.
>>>>> >> >>>> > >
>>>>> >> >>>> > > The attachment is the benchmark result.
>>>>> >> >>>> > >
>>>>> >> >>>> > > Thanks.
>>>>> >> >>>> > >
>>>>> >> >>>> > > Sincerely,
>>>>> >> >>>> > >
>>>>> >> >>>> > > DB Tsai
>>>>> >> >>>> > > -------------------------------------------------------
>>>>> >> >>>> > > My Blog: https://www.dbtsai.com
>>>>> >> >>>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>> >> >>>> >
>>>>> >> >>>
>>>>> >> >>>
>>>>> >
>>>>> >
>>>>>
>>>>
>>>>
>>>
>>
>

--001a11c16a7c2ab40d04f8146c88--

From dev-return-7439-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 28 06:32:01 2014
Return-Path: <dev-return-7439-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 816C211550
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Apr 2014 06:32:01 +0000 (UTC)
Received: (qmail 16845 invoked by uid 500); 28 Apr 2014 06:32:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16807 invoked by uid 500); 28 Apr 2014 06:32:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16798 invoked by uid 99); 28 Apr 2014 06:32:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Apr 2014 06:32:00 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Apr 2014 06:31:55 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id DF2CB1018C0
	for <dev@spark.apache.org>; Sun, 27 Apr 2014 23:31:31 -0700 (PDT)
Received: from mail-qc0-f179.google.com (mail-qc0-f179.google.com [209.85.216.179])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 137011018E4
	for <dev@spark.apache.org>; Sun, 27 Apr 2014 23:31:30 -0700 (PDT)
Received: by mail-qc0-f179.google.com with SMTP id l6so5654716qcy.10
        for <dev@spark.apache.org>; Sun, 27 Apr 2014 23:31:30 -0700 (PDT)
X-Gm-Message-State: ALoCoQlWkTIsi7XSDO/aMEBWwD5wEnAcWxjKvCHebkAALoimlPrY+NL/KIIEQmQmuHm8LQhh66qM
MIME-Version: 1.0
X-Received: by 10.140.32.139 with SMTP id h11mr29022368qgh.49.1398666690188;
 Sun, 27 Apr 2014 23:31:30 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Sun, 27 Apr 2014 23:31:30 -0700 (PDT)
In-Reply-To: <CAEYYnxYEOHFEp7obAWmVq=SMXxpLp3hJavyM0NAzwQ7SsN+ixA@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxZFmxXkrNhdUNF3je7k0Eg_YjY2WnvVW5SSk__FAWNz4w@mail.gmail.com>
	<CAJgQjQ-k5P3CxVFCE-pHtuH9fVwwCcPc6PBp1NgxM2uueOASsw@mail.gmail.com>
	<CAJgQjQ_ePvspakEfiHzxdDH30EeEzjxsz0y_G0n51VophVL1hg@mail.gmail.com>
	<CAEYYnxZb3GbjdVVWyF0F4c_sEikadefP88rnWuGtHrx9HwMBJQ@mail.gmail.com>
	<CAJgQjQ8s7dP-rUtiiSUV2imeL9r++_WEJj1RyWCy+b3QBEcxRw@mail.gmail.com>
	<CAEYYnxaSh532XmO7224BiM5D2g_Q0+k49AaFbuT6LxDCmSUZBw@mail.gmail.com>
	<CAEYYnxaKTW-74AmKSAAg_kQkAyqePjRNhn9rS2VVbt8wp_4CsA@mail.gmail.com>
	<CAEYYnxZNLk+-Hh8o7V88GBR+pTZxE0qm+mw4YxFeun7grsPEng@mail.gmail.com>
	<CALW2ey0PqNry2qK_SRMs6C=S3tZ_3KoGYHeKPHG0G8EuQ6vXag@mail.gmail.com>
	<CAEYYnxYEOHFEp7obAWmVq=SMXxpLp3hJavyM0NAzwQ7SsN+ixA@mail.gmail.com>
Date: Sun, 27 Apr 2014 23:31:30 -0700
Message-ID: <CAEYYnxYx1s+C9Qe_hHbXAvyYbeGCikMaXDHjqtcQf9tUG9cfbw@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: DB Tsai <dbtsai@stanford.edu>
To: David Hall <dlwh@cs.berkeley.edu>
Cc: Xiangrui Meng <mengxr@gmail.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113b59903ad16604f814774b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113b59903ad16604f814774b
Content-Type: text/plain; charset=UTF-8

Also, how many failure of rejection will terminate the optimization
process? How is it related to "numberOfImprovementFailures"?

Thanks.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Sun, Apr 27, 2014 at 11:28 PM, DB Tsai <dbtsai@stanford.edu> wrote:

> Hi David,
>
> I'm recording the loss history in the DiffFunction implementation, and
> that's why the rejected step is also recorded in my loss history.
>
> Is there any api in Breeze LBFGS to get the history which already excludes
> the reject step? Or should I just call "iterations" method and check
> "iteratingShouldStop" instead?
>
> Thanks.
>
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Fri, Apr 25, 2014 at 3:10 PM, David Hall <dlwh@cs.berkeley.edu> wrote:
>
>> LBFGS will not take a step that sends the objective value up. It might
>> try a step that is "too big" and reject it, so if you're just logging
>> everything that gets tried by LBFGS, you could see that. The "iterations"
>> method of the minimizer should never return an increasing objective value.
>> If you're regularizing, are you including the regularizer in the objective
>> value computation?
>>
>> GD is almost never worth your time.
>>
>> -- David
>>
>> On Fri, Apr 25, 2014 at 2:57 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>
>>> Another interesting benchmark.
>>>
>>> *News20 dataset - 0.14M row, 1,355,191 features, 0.034% non-zero
>>> elements.*
>>>
>>> LBFGS converges in 70 seconds, while GD seems to be not progressing.
>>>
>>> Dense feature vector will be too big to fit in the memory, so only
>>> conduct the sparse benchmark.
>>>
>>> I saw the sometimes the loss bumps up, and it's weird for me. Since the
>>> cost function of logistic regression is convex, it should be monotonically
>>> decreasing.  David, any suggestion?
>>>
>>> The detail figure:
>>>
>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/news20.pdf
>>>
>>>
>>> *Rcv1 dataset - 6.8M row, 677,399 features, 0.15% non-zero elements.*
>>>
>>> LBFGS converges in 25 seconds, while GD also seems to be not
>>> progressing.
>>>
>>> Only conduct sparse benchmark for the same reason. I also saw the loss
>>> bumps up for unknown reason.
>>>
>>> The detail figure:
>>>
>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/rcv1.pdf
>>>
>>>
>>> Sincerely,
>>>
>>> DB Tsai
>>> -------------------------------------------------------
>>> My Blog: https://www.dbtsai.com
>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>
>>>
>>> On Thu, Apr 24, 2014 at 2:36 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>
>>>> rcv1.binary is too sparse (0.15% non-zero elements), so dense format
>>>> will not run due to out of memory. But sparse format runs really well.
>>>>
>>>>
>>>> Sincerely,
>>>>
>>>> DB Tsai
>>>> -------------------------------------------------------
>>>> My Blog: https://www.dbtsai.com
>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>
>>>>
>>>> On Thu, Apr 24, 2014 at 1:54 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>>
>>>>> I'm doing the timer in runMiniBatchSGD after  val numExamples =
>>>>> data.count()
>>>>>
>>>>> See the following. Running rcv1 dataset now, and will update soon.
>>>>>
>>>>>     val startTime = System.nanoTime()
>>>>>     for (i <- 1 to numIterations) {
>>>>>       // Sample a subset (fraction miniBatchFraction) of the total data
>>>>>       // compute and sum up the subgradients on this subset (this is
>>>>> one map-reduce)
>>>>>       val (gradientSum, lossSum) = data.sample(false,
>>>>> miniBatchFraction, 42 + i)
>>>>>         .aggregate((BDV.zeros[Double](weights.size), 0.0))(
>>>>>           seqOp = (c, v) => (c, v) match { case ((grad, loss), (label,
>>>>> features)) =>
>>>>>             val l = gradient.compute(features, label, weights,
>>>>> Vectors.fromBreeze(grad))
>>>>>             (grad, loss + l)
>>>>>           },
>>>>>           combOp = (c1, c2) => (c1, c2) match { case ((grad1, loss1),
>>>>> (grad2, loss2)) =>
>>>>>             (grad1 += grad2, loss1 + loss2)
>>>>>           })
>>>>>
>>>>>       /**
>>>>>        * NOTE(Xinghao): lossSum is computed using the weights from the
>>>>> previous iteration
>>>>>        * and regVal is the regularization value computed in the
>>>>> previous iteration as well.
>>>>>        */
>>>>>       stochasticLossHistory.append(lossSum / miniBatchSize + regVal)
>>>>>       val update = updater.compute(
>>>>>         weights, Vectors.fromBreeze(gradientSum / miniBatchSize),
>>>>> stepSize, i, regParam)
>>>>>       weights = update._1
>>>>>       regVal = update._2
>>>>>       timeStamp.append(System.nanoTime() - startTime)
>>>>>     }
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>>
>>>>> Sincerely,
>>>>>
>>>>> DB Tsai
>>>>> -------------------------------------------------------
>>>>> My Blog: https://www.dbtsai.com
>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>
>>>>>
>>>>> On Thu, Apr 24, 2014 at 1:44 PM, Xiangrui Meng <mengxr@gmail.com>wrote:
>>>>>
>>>>>> I don't understand why sparse falls behind dense so much at the very
>>>>>> first iteration. I didn't see count() is called in
>>>>>>
>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/blob/master/src/main/scala/org/apache/spark/mllib/benchmark/BinaryLogisticRegression.scala
>>>>>> . Maybe you have local uncommitted changes.
>>>>>>
>>>>>> Best,
>>>>>> Xiangrui
>>>>>>
>>>>>> On Thu, Apr 24, 2014 at 11:26 AM, DB Tsai <dbtsai@stanford.edu>
>>>>>> wrote:
>>>>>> > Hi Xiangrui,
>>>>>> >
>>>>>> > Yes, I'm using yarn-cluster mode, and I did check # of executors I
>>>>>> specified
>>>>>> > are the same as the actual running executors.
>>>>>> >
>>>>>> > For caching and materialization, I've the timer in optimizer after
>>>>>> calling
>>>>>> > count(); as a result, the time for materialization in cache isn't
>>>>>> in the
>>>>>> > benchmark.
>>>>>> >
>>>>>> > The difference you saw is actually from dense feature or sparse
>>>>>> feature
>>>>>> > vector. For LBFGS and GD dense feature, you can see the first
>>>>>> iteration
>>>>>> > takes the same time. It's true for GD.
>>>>>> >
>>>>>> > I'm going to run rcv1.binary which only has 0.15% non-zero elements
>>>>>> to
>>>>>> > verify the hypothesis.
>>>>>> >
>>>>>> >
>>>>>> > Sincerely,
>>>>>> >
>>>>>> > DB Tsai
>>>>>> > -------------------------------------------------------
>>>>>> > My Blog: https://www.dbtsai.com
>>>>>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>> >
>>>>>> >
>>>>>> > On Thu, Apr 24, 2014 at 1:09 AM, Xiangrui Meng <mengxr@gmail.com>
>>>>>> wrote:
>>>>>> >>
>>>>>> >> Hi DB,
>>>>>> >>
>>>>>> >> I saw you are using yarn-cluster mode for the benchmark. I tested
>>>>>> the
>>>>>> >> yarn-cluster mode and found that YARN does not always give you the
>>>>>> >> exact number of executors requested. Just want to confirm that
>>>>>> you've
>>>>>> >> checked the number of executors.
>>>>>> >>
>>>>>> >> The second thing to check is that in the benchmark code, after you
>>>>>> >> call cache, you should also call count() to materialize the RDD. I
>>>>>> saw
>>>>>> >> in the result, the real difference is actually at the first step.
>>>>>> >> Adding intercept is not a cheap operation for sparse vectors.
>>>>>> >>
>>>>>> >> Best,
>>>>>> >> Xiangrui
>>>>>> >>
>>>>>> >> On Thu, Apr 24, 2014 at 12:53 AM, Xiangrui Meng <mengxr@gmail.com>
>>>>>> wrote:
>>>>>> >> > I don't think it is easy to make sparse faster than dense with
>>>>>> this
>>>>>> >> > sparsity and feature dimension. You can try rcv1.binary, which
>>>>>> should
>>>>>> >> > show the difference easily.
>>>>>> >> >
>>>>>> >> > David, the breeze operators used here are
>>>>>> >> >
>>>>>> >> > 1. DenseVector dot SparseVector
>>>>>> >> > 2. axpy DenseVector SparseVector
>>>>>> >> >
>>>>>> >> > However, the SparseVector is passed in as Vector[Double] instead
>>>>>> of
>>>>>> >> > SparseVector[Double]. It might use the axpy impl of [DenseVector,
>>>>>> >> > Vector] and call activeIterator. I didn't check whether you used
>>>>>> >> > multimethods on axpy.
>>>>>> >> >
>>>>>> >> > Best,
>>>>>> >> > Xiangrui
>>>>>> >> >
>>>>>> >> > On Wed, Apr 23, 2014 at 10:35 PM, DB Tsai <dbtsai@stanford.edu>
>>>>>> wrote:
>>>>>> >> >> The figure showing the Log-Likelihood vs Time can be found here.
>>>>>> >> >>
>>>>>> >> >>
>>>>>> >> >>
>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
>>>>>> >> >>
>>>>>> >> >> Let me know if you can not open it. Thanks.
>>>>>> >> >>
>>>>>> >> >> Sincerely,
>>>>>> >> >>
>>>>>> >> >> DB Tsai
>>>>>> >> >> -------------------------------------------------------
>>>>>> >> >> My Blog: https://www.dbtsai.com
>>>>>> >> >> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>> >> >>
>>>>>> >> >>
>>>>>> >> >> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman
>>>>>> >> >> <shivaram@eecs.berkeley.edu> wrote:
>>>>>> >> >>> I don't think the attachment came through in the list. Could
>>>>>> you
>>>>>> >> >>> upload the
>>>>>> >> >>> results somewhere and link to them ?
>>>>>> >> >>>
>>>>>> >> >>>
>>>>>> >> >>> On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com>
>>>>>> wrote:
>>>>>> >> >>>>
>>>>>> >> >>>> 123 features per rows, and in average, 89% are zeros.
>>>>>> >> >>>> On Apr 23, 2014 9:31 PM, "Evan Sparks" <evan.sparks@gmail.com>
>>>>>> wrote:
>>>>>> >> >>>>
>>>>>> >> >>>> > What is the number of non zeroes per row (and number of
>>>>>> features)
>>>>>> >> >>>> > in the
>>>>>> >> >>>> > sparse case? We've hit some issues with breeze sparse
>>>>>> support in
>>>>>> >> >>>> > the
>>>>>> >> >>>> > past
>>>>>> >> >>>> > but for sufficiently sparse data it's still pretty good.
>>>>>> >> >>>> >
>>>>>> >> >>>> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <dbtsai@stanford.edu>
>>>>>> wrote:
>>>>>> >> >>>> > >
>>>>>> >> >>>> > > Hi all,
>>>>>> >> >>>> > >
>>>>>> >> >>>> > > I'm benchmarking Logistic Regression in MLlib using the
>>>>>> newly
>>>>>> >> >>>> > > added
>>>>>> >> >>>> > optimizer LBFGS and GD. I'm using the same dataset and the
>>>>>> same
>>>>>> >> >>>> > methodology
>>>>>> >> >>>> > in this paper,
>>>>>> http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>>>>>> >> >>>> > >
>>>>>> >> >>>> > > I want to know how Spark scale while adding workers, and
>>>>>> how
>>>>>> >> >>>> > > optimizers
>>>>>> >> >>>> > and input format (sparse or dense) impact performance.
>>>>>> >> >>>> > >
>>>>>> >> >>>> > > The benchmark code can be found here,
>>>>>> >> >>>> > https://github.com/dbtsai/spark-lbfgs-benchmark
>>>>>> >> >>>> > >
>>>>>> >> >>>> > > The first dataset I benchmarked is a9a which only has
>>>>>> 2.2MB. I
>>>>>> >> >>>> > duplicated the dataset, and made it 762MB to have 11M rows.
>>>>>> This
>>>>>> >> >>>> > dataset
>>>>>> >> >>>> > has 123 features and 11% of the data are non-zero elements.
>>>>>> >> >>>> > >
>>>>>> >> >>>> > > In this benchmark, all the dataset is cached in memory.
>>>>>> >> >>>> > >
>>>>>> >> >>>> > > As we expect, LBFGS converges faster than GD, and at some
>>>>>> point,
>>>>>> >> >>>> > > no
>>>>>> >> >>>> > matter how we push GD, it will converge slower and slower.
>>>>>> >> >>>> > >
>>>>>> >> >>>> > > However, it's surprising that sparse format runs slower
>>>>>> than
>>>>>> >> >>>> > > dense
>>>>>> >> >>>> > format. I did see that sparse format takes significantly
>>>>>> smaller
>>>>>> >> >>>> > amount
>>>>>> >> >>>> > of
>>>>>> >> >>>> > memory in caching RDD, but sparse is 40% slower than dense.
>>>>>> I think
>>>>>> >> >>>> > sparse
>>>>>> >> >>>> > should be fast since when we compute x wT, since x is
>>>>>> sparse, we
>>>>>> >> >>>> > can do
>>>>>> >> >>>> > it
>>>>>> >> >>>> > faster. I wonder if there is anything I'm doing wrong.
>>>>>> >> >>>> > >
>>>>>> >> >>>> > > The attachment is the benchmark result.
>>>>>> >> >>>> > >
>>>>>> >> >>>> > > Thanks.
>>>>>> >> >>>> > >
>>>>>> >> >>>> > > Sincerely,
>>>>>> >> >>>> > >
>>>>>> >> >>>> > > DB Tsai
>>>>>> >> >>>> > > -------------------------------------------------------
>>>>>> >> >>>> > > My Blog: https://www.dbtsai.com
>>>>>> >> >>>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>> >> >>>> >
>>>>>> >> >>>
>>>>>> >> >>>
>>>>>> >
>>>>>> >
>>>>>>
>>>>>
>>>>>
>>>>
>>>
>>
>

--001a113b59903ad16604f814774b--

From dev-return-7440-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 28 06:53:58 2014
Return-Path: <dev-return-7440-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A433911637
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Apr 2014 06:53:58 +0000 (UTC)
Received: (qmail 62654 invoked by uid 500); 28 Apr 2014 06:53:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62443 invoked by uid 500); 28 Apr 2014 06:53:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62435 invoked by uid 99); 28 Apr 2014 06:53:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Apr 2014 06:53:55 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Apr 2014 06:53:51 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 22F4D101942
	for <dev@spark.apache.org>; Sun, 27 Apr 2014 23:53:28 -0700 (PDT)
Received: from mail-qg0-f51.google.com (mail-qg0-f51.google.com [209.85.192.51])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 1CCB6101956
	for <dev@spark.apache.org>; Sun, 27 Apr 2014 23:53:27 -0700 (PDT)
Received: by mail-qg0-f51.google.com with SMTP id f51so6430548qge.38
        for <dev@spark.apache.org>; Sun, 27 Apr 2014 23:53:26 -0700 (PDT)
X-Gm-Message-State: ALoCoQmG1XO8b1f5NNbv2hdRf11qSPIfyAhLaxofYptX9CgiG/Z8AWWUSpuo/AZd+Nd++h+kevW4
MIME-Version: 1.0
X-Received: by 10.140.47.206 with SMTP id m72mr29176232qga.21.1398668006276;
 Sun, 27 Apr 2014 23:53:26 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Sun, 27 Apr 2014 23:53:26 -0700 (PDT)
In-Reply-To: <CAEYYnxYx1s+C9Qe_hHbXAvyYbeGCikMaXDHjqtcQf9tUG9cfbw@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxZFmxXkrNhdUNF3je7k0Eg_YjY2WnvVW5SSk__FAWNz4w@mail.gmail.com>
	<CAJgQjQ-k5P3CxVFCE-pHtuH9fVwwCcPc6PBp1NgxM2uueOASsw@mail.gmail.com>
	<CAJgQjQ_ePvspakEfiHzxdDH30EeEzjxsz0y_G0n51VophVL1hg@mail.gmail.com>
	<CAEYYnxZb3GbjdVVWyF0F4c_sEikadefP88rnWuGtHrx9HwMBJQ@mail.gmail.com>
	<CAJgQjQ8s7dP-rUtiiSUV2imeL9r++_WEJj1RyWCy+b3QBEcxRw@mail.gmail.com>
	<CAEYYnxaSh532XmO7224BiM5D2g_Q0+k49AaFbuT6LxDCmSUZBw@mail.gmail.com>
	<CAEYYnxaKTW-74AmKSAAg_kQkAyqePjRNhn9rS2VVbt8wp_4CsA@mail.gmail.com>
	<CAEYYnxZNLk+-Hh8o7V88GBR+pTZxE0qm+mw4YxFeun7grsPEng@mail.gmail.com>
	<CALW2ey0PqNry2qK_SRMs6C=S3tZ_3KoGYHeKPHG0G8EuQ6vXag@mail.gmail.com>
	<CAEYYnxYEOHFEp7obAWmVq=SMXxpLp3hJavyM0NAzwQ7SsN+ixA@mail.gmail.com>
	<CAEYYnxYx1s+C9Qe_hHbXAvyYbeGCikMaXDHjqtcQf9tUG9cfbw@mail.gmail.com>
Date: Sun, 27 Apr 2014 23:53:26 -0700
Message-ID: <CAEYYnxb4QLneUjfj5dG80Pzae8MhEQG090YQ2G4oq7VccayM-g@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: DB Tsai <dbtsai@stanford.edu>
To: David Hall <dlwh@cs.berkeley.edu>
Cc: Xiangrui Meng <mengxr@gmail.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c16a7cac4e6d04f814c5b3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c16a7cac4e6d04f814c5b3
Content-Type: text/plain; charset=UTF-8

I think I figure it out. Instead of calling minimize, and record the loss
in the DiffFunction, I should do the following.

val states = lbfgs.iterations(new CachedDiffFunction(costFun),
initialWeights.toBreeze.toDenseVector)
states.foreach(state => lossHistory.append(state.value))

All the losses in states should be decreasing now. Am I right?



Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Sun, Apr 27, 2014 at 11:31 PM, DB Tsai <dbtsai@stanford.edu> wrote:

> Also, how many failure of rejection will terminate the optimization
> process? How is it related to "numberOfImprovementFailures"?
>
> Thanks.
>
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Sun, Apr 27, 2014 at 11:28 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>
>> Hi David,
>>
>> I'm recording the loss history in the DiffFunction implementation, and
>> that's why the rejected step is also recorded in my loss history.
>>
>> Is there any api in Breeze LBFGS to get the history which already
>> excludes the reject step? Or should I just call "iterations" method and
>> check "iteratingShouldStop" instead?
>>
>> Thanks.
>>
>>
>> Sincerely,
>>
>> DB Tsai
>> -------------------------------------------------------
>> My Blog: https://www.dbtsai.com
>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>
>>
>> On Fri, Apr 25, 2014 at 3:10 PM, David Hall <dlwh@cs.berkeley.edu> wrote:
>>
>>> LBFGS will not take a step that sends the objective value up. It might
>>> try a step that is "too big" and reject it, so if you're just logging
>>> everything that gets tried by LBFGS, you could see that. The "iterations"
>>> method of the minimizer should never return an increasing objective value.
>>> If you're regularizing, are you including the regularizer in the objective
>>> value computation?
>>>
>>> GD is almost never worth your time.
>>>
>>> -- David
>>>
>>> On Fri, Apr 25, 2014 at 2:57 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>
>>>> Another interesting benchmark.
>>>>
>>>> *News20 dataset - 0.14M row, 1,355,191 features, 0.034% non-zero
>>>> elements.*
>>>>
>>>> LBFGS converges in 70 seconds, while GD seems to be not progressing.
>>>>
>>>> Dense feature vector will be too big to fit in the memory, so only
>>>> conduct the sparse benchmark.
>>>>
>>>> I saw the sometimes the loss bumps up, and it's weird for me. Since the
>>>> cost function of logistic regression is convex, it should be monotonically
>>>> decreasing.  David, any suggestion?
>>>>
>>>> The detail figure:
>>>>
>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/news20.pdf
>>>>
>>>>
>>>> *Rcv1 dataset - 6.8M row, 677,399 features, 0.15% non-zero elements.*
>>>>
>>>> LBFGS converges in 25 seconds, while GD also seems to be not
>>>> progressing.
>>>>
>>>> Only conduct sparse benchmark for the same reason. I also saw the loss
>>>> bumps up for unknown reason.
>>>>
>>>> The detail figure:
>>>>
>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/rcv1.pdf
>>>>
>>>>
>>>> Sincerely,
>>>>
>>>> DB Tsai
>>>> -------------------------------------------------------
>>>> My Blog: https://www.dbtsai.com
>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>
>>>>
>>>> On Thu, Apr 24, 2014 at 2:36 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>>
>>>>> rcv1.binary is too sparse (0.15% non-zero elements), so dense format
>>>>> will not run due to out of memory. But sparse format runs really well.
>>>>>
>>>>>
>>>>> Sincerely,
>>>>>
>>>>> DB Tsai
>>>>> -------------------------------------------------------
>>>>> My Blog: https://www.dbtsai.com
>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>
>>>>>
>>>>> On Thu, Apr 24, 2014 at 1:54 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>>>
>>>>>> I'm doing the timer in runMiniBatchSGD after  val numExamples =
>>>>>> data.count()
>>>>>>
>>>>>> See the following. Running rcv1 dataset now, and will update soon.
>>>>>>
>>>>>>     val startTime = System.nanoTime()
>>>>>>     for (i <- 1 to numIterations) {
>>>>>>       // Sample a subset (fraction miniBatchFraction) of the total
>>>>>> data
>>>>>>       // compute and sum up the subgradients on this subset (this is
>>>>>> one map-reduce)
>>>>>>       val (gradientSum, lossSum) = data.sample(false,
>>>>>> miniBatchFraction, 42 + i)
>>>>>>         .aggregate((BDV.zeros[Double](weights.size), 0.0))(
>>>>>>           seqOp = (c, v) => (c, v) match { case ((grad, loss),
>>>>>> (label, features)) =>
>>>>>>             val l = gradient.compute(features, label, weights,
>>>>>> Vectors.fromBreeze(grad))
>>>>>>             (grad, loss + l)
>>>>>>           },
>>>>>>           combOp = (c1, c2) => (c1, c2) match { case ((grad1, loss1),
>>>>>> (grad2, loss2)) =>
>>>>>>             (grad1 += grad2, loss1 + loss2)
>>>>>>           })
>>>>>>
>>>>>>       /**
>>>>>>        * NOTE(Xinghao): lossSum is computed using the weights from
>>>>>> the previous iteration
>>>>>>        * and regVal is the regularization value computed in the
>>>>>> previous iteration as well.
>>>>>>        */
>>>>>>       stochasticLossHistory.append(lossSum / miniBatchSize + regVal)
>>>>>>       val update = updater.compute(
>>>>>>         weights, Vectors.fromBreeze(gradientSum / miniBatchSize),
>>>>>> stepSize, i, regParam)
>>>>>>       weights = update._1
>>>>>>       regVal = update._2
>>>>>>       timeStamp.append(System.nanoTime() - startTime)
>>>>>>     }
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>>
>>>>>> Sincerely,
>>>>>>
>>>>>> DB Tsai
>>>>>> -------------------------------------------------------
>>>>>> My Blog: https://www.dbtsai.com
>>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>
>>>>>>
>>>>>> On Thu, Apr 24, 2014 at 1:44 PM, Xiangrui Meng <mengxr@gmail.com>wrote:
>>>>>>
>>>>>>> I don't understand why sparse falls behind dense so much at the very
>>>>>>> first iteration. I didn't see count() is called in
>>>>>>>
>>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/blob/master/src/main/scala/org/apache/spark/mllib/benchmark/BinaryLogisticRegression.scala
>>>>>>> . Maybe you have local uncommitted changes.
>>>>>>>
>>>>>>> Best,
>>>>>>> Xiangrui
>>>>>>>
>>>>>>> On Thu, Apr 24, 2014 at 11:26 AM, DB Tsai <dbtsai@stanford.edu>
>>>>>>> wrote:
>>>>>>> > Hi Xiangrui,
>>>>>>> >
>>>>>>> > Yes, I'm using yarn-cluster mode, and I did check # of executors I
>>>>>>> specified
>>>>>>> > are the same as the actual running executors.
>>>>>>> >
>>>>>>> > For caching and materialization, I've the timer in optimizer after
>>>>>>> calling
>>>>>>> > count(); as a result, the time for materialization in cache isn't
>>>>>>> in the
>>>>>>> > benchmark.
>>>>>>> >
>>>>>>> > The difference you saw is actually from dense feature or sparse
>>>>>>> feature
>>>>>>> > vector. For LBFGS and GD dense feature, you can see the first
>>>>>>> iteration
>>>>>>> > takes the same time. It's true for GD.
>>>>>>> >
>>>>>>> > I'm going to run rcv1.binary which only has 0.15% non-zero
>>>>>>> elements to
>>>>>>> > verify the hypothesis.
>>>>>>> >
>>>>>>> >
>>>>>>> > Sincerely,
>>>>>>> >
>>>>>>> > DB Tsai
>>>>>>> > -------------------------------------------------------
>>>>>>> > My Blog: https://www.dbtsai.com
>>>>>>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>> >
>>>>>>> >
>>>>>>> > On Thu, Apr 24, 2014 at 1:09 AM, Xiangrui Meng <mengxr@gmail.com>
>>>>>>> wrote:
>>>>>>> >>
>>>>>>> >> Hi DB,
>>>>>>> >>
>>>>>>> >> I saw you are using yarn-cluster mode for the benchmark. I tested
>>>>>>> the
>>>>>>> >> yarn-cluster mode and found that YARN does not always give you the
>>>>>>> >> exact number of executors requested. Just want to confirm that
>>>>>>> you've
>>>>>>> >> checked the number of executors.
>>>>>>> >>
>>>>>>> >> The second thing to check is that in the benchmark code, after you
>>>>>>> >> call cache, you should also call count() to materialize the RDD.
>>>>>>> I saw
>>>>>>> >> in the result, the real difference is actually at the first step.
>>>>>>> >> Adding intercept is not a cheap operation for sparse vectors.
>>>>>>> >>
>>>>>>> >> Best,
>>>>>>> >> Xiangrui
>>>>>>> >>
>>>>>>> >> On Thu, Apr 24, 2014 at 12:53 AM, Xiangrui Meng <mengxr@gmail.com>
>>>>>>> wrote:
>>>>>>> >> > I don't think it is easy to make sparse faster than dense with
>>>>>>> this
>>>>>>> >> > sparsity and feature dimension. You can try rcv1.binary, which
>>>>>>> should
>>>>>>> >> > show the difference easily.
>>>>>>> >> >
>>>>>>> >> > David, the breeze operators used here are
>>>>>>> >> >
>>>>>>> >> > 1. DenseVector dot SparseVector
>>>>>>> >> > 2. axpy DenseVector SparseVector
>>>>>>> >> >
>>>>>>> >> > However, the SparseVector is passed in as Vector[Double]
>>>>>>> instead of
>>>>>>> >> > SparseVector[Double]. It might use the axpy impl of
>>>>>>> [DenseVector,
>>>>>>> >> > Vector] and call activeIterator. I didn't check whether you used
>>>>>>> >> > multimethods on axpy.
>>>>>>> >> >
>>>>>>> >> > Best,
>>>>>>> >> > Xiangrui
>>>>>>> >> >
>>>>>>> >> > On Wed, Apr 23, 2014 at 10:35 PM, DB Tsai <dbtsai@stanford.edu>
>>>>>>> wrote:
>>>>>>> >> >> The figure showing the Log-Likelihood vs Time can be found
>>>>>>> here.
>>>>>>> >> >>
>>>>>>> >> >>
>>>>>>> >> >>
>>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
>>>>>>> >> >>
>>>>>>> >> >> Let me know if you can not open it. Thanks.
>>>>>>> >> >>
>>>>>>> >> >> Sincerely,
>>>>>>> >> >>
>>>>>>> >> >> DB Tsai
>>>>>>> >> >> -------------------------------------------------------
>>>>>>> >> >> My Blog: https://www.dbtsai.com
>>>>>>> >> >> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>> >> >>
>>>>>>> >> >>
>>>>>>> >> >> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman
>>>>>>> >> >> <shivaram@eecs.berkeley.edu> wrote:
>>>>>>> >> >>> I don't think the attachment came through in the list. Could
>>>>>>> you
>>>>>>> >> >>> upload the
>>>>>>> >> >>> results somewhere and link to them ?
>>>>>>> >> >>>
>>>>>>> >> >>>
>>>>>>> >> >>> On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com>
>>>>>>> wrote:
>>>>>>> >> >>>>
>>>>>>> >> >>>> 123 features per rows, and in average, 89% are zeros.
>>>>>>> >> >>>> On Apr 23, 2014 9:31 PM, "Evan Sparks" <
>>>>>>> evan.sparks@gmail.com> wrote:
>>>>>>> >> >>>>
>>>>>>> >> >>>> > What is the number of non zeroes per row (and number of
>>>>>>> features)
>>>>>>> >> >>>> > in the
>>>>>>> >> >>>> > sparse case? We've hit some issues with breeze sparse
>>>>>>> support in
>>>>>>> >> >>>> > the
>>>>>>> >> >>>> > past
>>>>>>> >> >>>> > but for sufficiently sparse data it's still pretty good.
>>>>>>> >> >>>> >
>>>>>>> >> >>>> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <
>>>>>>> dbtsai@stanford.edu> wrote:
>>>>>>> >> >>>> > >
>>>>>>> >> >>>> > > Hi all,
>>>>>>> >> >>>> > >
>>>>>>> >> >>>> > > I'm benchmarking Logistic Regression in MLlib using the
>>>>>>> newly
>>>>>>> >> >>>> > > added
>>>>>>> >> >>>> > optimizer LBFGS and GD. I'm using the same dataset and the
>>>>>>> same
>>>>>>> >> >>>> > methodology
>>>>>>> >> >>>> > in this paper,
>>>>>>> http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>>>>>>> >> >>>> > >
>>>>>>> >> >>>> > > I want to know how Spark scale while adding workers, and
>>>>>>> how
>>>>>>> >> >>>> > > optimizers
>>>>>>> >> >>>> > and input format (sparse or dense) impact performance.
>>>>>>> >> >>>> > >
>>>>>>> >> >>>> > > The benchmark code can be found here,
>>>>>>> >> >>>> > https://github.com/dbtsai/spark-lbfgs-benchmark
>>>>>>> >> >>>> > >
>>>>>>> >> >>>> > > The first dataset I benchmarked is a9a which only has
>>>>>>> 2.2MB. I
>>>>>>> >> >>>> > duplicated the dataset, and made it 762MB to have 11M
>>>>>>> rows. This
>>>>>>> >> >>>> > dataset
>>>>>>> >> >>>> > has 123 features and 11% of the data are non-zero elements.
>>>>>>> >> >>>> > >
>>>>>>> >> >>>> > > In this benchmark, all the dataset is cached in memory.
>>>>>>> >> >>>> > >
>>>>>>> >> >>>> > > As we expect, LBFGS converges faster than GD, and at
>>>>>>> some point,
>>>>>>> >> >>>> > > no
>>>>>>> >> >>>> > matter how we push GD, it will converge slower and slower.
>>>>>>> >> >>>> > >
>>>>>>> >> >>>> > > However, it's surprising that sparse format runs slower
>>>>>>> than
>>>>>>> >> >>>> > > dense
>>>>>>> >> >>>> > format. I did see that sparse format takes significantly
>>>>>>> smaller
>>>>>>> >> >>>> > amount
>>>>>>> >> >>>> > of
>>>>>>> >> >>>> > memory in caching RDD, but sparse is 40% slower than
>>>>>>> dense. I think
>>>>>>> >> >>>> > sparse
>>>>>>> >> >>>> > should be fast since when we compute x wT, since x is
>>>>>>> sparse, we
>>>>>>> >> >>>> > can do
>>>>>>> >> >>>> > it
>>>>>>> >> >>>> > faster. I wonder if there is anything I'm doing wrong.
>>>>>>> >> >>>> > >
>>>>>>> >> >>>> > > The attachment is the benchmark result.
>>>>>>> >> >>>> > >
>>>>>>> >> >>>> > > Thanks.
>>>>>>> >> >>>> > >
>>>>>>> >> >>>> > > Sincerely,
>>>>>>> >> >>>> > >
>>>>>>> >> >>>> > > DB Tsai
>>>>>>> >> >>>> > > -------------------------------------------------------
>>>>>>> >> >>>> > > My Blog: https://www.dbtsai.com
>>>>>>> >> >>>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>> >> >>>> >
>>>>>>> >> >>>
>>>>>>> >> >>>
>>>>>>> >
>>>>>>> >
>>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>

--001a11c16a7cac4e6d04f814c5b3--

From dev-return-7441-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 28 08:37:30 2014
Return-Path: <dev-return-7441-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DDB52118BE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Apr 2014 08:37:29 +0000 (UTC)
Received: (qmail 86527 invoked by uid 500); 28 Apr 2014 08:37:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86052 invoked by uid 500); 28 Apr 2014 08:37:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86043 invoked by uid 99); 28 Apr 2014 08:37:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Apr 2014 08:37:26 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of geoffroy.fouquier@exensa.com designates 46.105.42.240 as permitted sender)
Received: from [46.105.42.240] (HELO mail.exensa.com) (46.105.42.240)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Apr 2014 08:37:21 +0000
Received: from localhost (localhost [127.0.0.1])
	by mail.exensa.com (Postfix) with ESMTP id 976F524C13B0
	for <dev@spark.apache.org>; Mon, 28 Apr 2014 08:36:59 +0000 (UTC)
Received: from mail.exensa.com ([127.0.0.1])
	by localhost (mail.exensa.com [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id AqMb8EoamaGO for <dev@spark.apache.org>;
	Mon, 28 Apr 2014 08:36:59 +0000 (UTC)
Received: from [10.1.42.7] (LPuteaux-656-01-229-158.w80-12.abo.wanadoo.fr [80.12.90.158])
	(using TLSv1 with cipher ECDHE-RSA-AES128-SHA (128/128 bits))
	(No client certificate requested)
	by mail.exensa.com (Postfix) with ESMTPSA id 5692524C075D
	for <dev@spark.apache.org>; Mon, 28 Apr 2014 08:36:59 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=exensa.com;
	s=default; t=1398674219;
	bh=qPV2S9VcFbq59yZUvOp57qwo2NaY3iZCZD4GrWxoSlQ=;
	h=Date:From:To:Subject:References:In-Reply-To;
	b=sH7I2bZhkHiPCEGe7eQhibkqqRZx2BsuVvqhSpoKNJOdJV3QhnMDcfZjlfAAwLLUH
	 wxJl9qu5x8/wxsrlkfsr9Jkj9U6rwJTcowB9B6u+r/LkA20dteHTEpZo4AweS7m
Message-ID: <535E132B.5070909@exensa.com>
Date: Mon, 28 Apr 2014 10:36:59 +0200
From: Geoffroy Fouquier <geoffroy.fouquier@exensa.com>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20100101 Thunderbird/24.4.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: Re: Parsing wikipedia xml data in Spark
References: <CAHVi7z+C7BnG2jNwQsA=qvvzHDSVJF_auTFrJ1D05inpKzQfRA@mail.gmail.com>
In-Reply-To: <CAHVi7z+C7BnG2jNwQsA=qvvzHDSVJF_auTFrJ1D05inpKzQfRA@mail.gmail.com>
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 8bit
X-Virus-Checked: Checked by ClamAV on apache.org


We did it using scala xml with spark

We start by creating a rdd containing each page is store as a single line :
   - split the xml dump with xml_split
   - process each split with a shell script which remove "xml_split" tag 
and siteinfo section, and put each page on a single line.
   - copy resulting files on hdfs

Then the dataset may be load as a text file and processed

  val rawDataset = sparkContext.textFile(input)
  val allDocuments = rawDataset.map{
     case document =>
         val page = scala.xml.XML.loadString(document)
         val pageTitle = (page \ "title").text
         [...]
  }

We create a demo using the dataset here: http://wikinsights.org

Le 26/04/2014 23:20, Ajay Nair a crit :
> Is there a way in spark to parse wikipedia xml dump? It seems like the
> freebase dump is longer available. Also does the spark shell support the
> xml load file sax parser that is present in scala.
>
> Thanks
> AJ
>

Geoffroy Fouquier
http://eXenSa.com


From dev-return-7442-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 28 14:27:21 2014
Return-Path: <dev-return-7442-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3E0FDF2E2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Apr 2014 14:27:21 +0000 (UTC)
Received: (qmail 69537 invoked by uid 500); 28 Apr 2014 14:27:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69460 invoked by uid 500); 28 Apr 2014 14:27:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69452 invoked by uid 99); 28 Apr 2014 14:27:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Apr 2014 14:27:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of foundart@gmail.com designates 209.85.160.44 as permitted sender)
Received: from [209.85.160.44] (HELO mail-pb0-f44.google.com) (209.85.160.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Apr 2014 14:27:14 +0000
Received: by mail-pb0-f44.google.com with SMTP id jt11so4178861pbb.17
        for <dev@spark.apache.org>; Mon, 28 Apr 2014 07:26:51 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=MqVEICp4qPCAceG7J0wGH8f57xcpaBYdcGVrpzjOeek=;
        b=Zh0BFYbpH3vPooISy4cfvm9CR9R8m33fvGmHfVByg5r2z0ohRLRc8IznIh0aGEvRga
         /gnJCL2WBinwaX3OA6Y9nAp55HZuqPgijMSMY2qadYr1HxKp0tr65Pyq1D1D/zLQb1rc
         IT7aLnMfnFhPSp9OCVLp322I8oSIzSD2d+wIrLq1JbajX0pMeg2hSC6G0g5O/86eG8tY
         IihsJIpTbcBJP/BeyXhmV06BC7xvDfoeZm94harPkajUZhF7Igf8b5LbG6nNSZDe2F0z
         MAMJEJya3WhOoN7eE2rx5L1OblRUJPcJDE0Cx57imtaed4lTBU0CgppprOfvNci4MsKv
         Bkrg==
MIME-Version: 1.0
X-Received: by 10.66.227.193 with SMTP id sc1mr25799485pac.102.1398695210976;
 Mon, 28 Apr 2014 07:26:50 -0700 (PDT)
Received: by 10.66.142.196 with HTTP; Mon, 28 Apr 2014 07:26:50 -0700 (PDT)
In-Reply-To: <CAMJOb8kaqKdOuuen8youLwOX1XS4ypn4KZLgZhEcUn-mfdQrFQ@mail.gmail.com>
References: <CAASS4f7R7hyW1N3f6Gesj-QUWugNumQbWR9avPgJnFkPaVYqzQ@mail.gmail.com>
	<CAMJOb8kaqKdOuuen8youLwOX1XS4ypn4KZLgZhEcUn-mfdQrFQ@mail.gmail.com>
Date: Mon, 28 Apr 2014 07:26:50 -0700
Message-ID: <CAASS4f6H4-HLY42dBy+ga+tskb3cL-QBbQBvY0w4m0F8Fr1Qpw@mail.gmail.com>
Subject: Re: thoughts on spark_ec2.py?
From: Art Peel <foundart@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b15aa4d330a3004f81b1b20
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b15aa4d330a3004f81b1b20
Content-Type: text/plain; charset=UTF-8

Thanks for the info and good luck with 1.0.

Regards,
Art



On Fri, Apr 25, 2014 at 9:48 AM, Andrew Or <andrew@databricks.com> wrote:

> Hi Art,
>
> First of all thanks a lot for your PRs. We are currently in the middle of
> all the Spark 1.0 release so most of us are swamped with the more core
> features. To answer your questions:
>
> 1. Neither. We welcome changes from developers for all components of Spark,
> including the EC2 scripts. Once the release is out we will have more time
> to review the many PRs that we missed on the ride.
>
> 2. We prefer to keep the EC2 scripts within Spark, at least for now.
>
> Cheers,
> Andrew
>
> On Friday, April 25, 2014, Art Peel <foundart@gmail.com> wrote:
>
> > I've been setting up Spark cluster on EC2 using the provided
> > ec2/spark_ec2.py script and am very happy I didn't have to write it from
> > scratch. Thanks for providing it.
> >
> > There have been some issues, though, and I have had to make some
> additions.
> >  So far, they are all additions of command-line options.  For example,
> the
> > original script allows access from anywhere to the various ports.  I've
> > added an option to specify what net/mask should be allowed to access
> those
> > ports.
> >
> > I've filed a couple of pull requests, but they are not going anywhere.
> >  Given what I've seen of the traffic on this list, I don't feel that a
> lot
> > of the developers are thinking about EC2 setup. I totally agree that it
> is
> > not as important as improving the guts of Spark itself; nevertheless, I
> > feel that being able to run Spark on EC2 smartly and easily is valuable.
> >
> > So, I have 2 questions for the committers:
> >
> > 1. Is ec2/spark_ec2.py something the committers
> > a. are not thinking about?
> > b. are planning to replace?
> > c. other
> >
> > 2. Should I just start a new project based on ec2/spark_ec2.py but
> without
> > all the other stuff and make (and share) my changes there?
> >
> > Regards,
> >
> > Art
> >
>

--047d7b15aa4d330a3004f81b1b20--

From dev-return-7443-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 28 15:56:02 2014
Return-Path: <dev-return-7443-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BAE16F680
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Apr 2014 15:56:02 +0000 (UTC)
Received: (qmail 59090 invoked by uid 500); 28 Apr 2014 15:56:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59048 invoked by uid 500); 28 Apr 2014 15:56:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59032 invoked by uid 99); 28 Apr 2014 15:56:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Apr 2014 15:56:00 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of david.lw.hall@gmail.com designates 209.85.212.174 as permitted sender)
Received: from [209.85.212.174] (HELO mail-wi0-f174.google.com) (209.85.212.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Apr 2014 15:55:56 +0000
Received: by mail-wi0-f174.google.com with SMTP id d1so5968438wiv.1
        for <dev@spark.apache.org>; Mon, 28 Apr 2014 08:55:35 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:cc:content-type;
        bh=h3ED3KzYpIr2lNPWrQoxdsWkgcDXfRMy2LF6PUpMCAs=;
        b=zZQaDEe8l4jx11ach/E1TJV7Jxt28yeoiNJUmd842nMFtXT/ByEExtce7Zv6gzMNO+
         5Jp/2MSc8szrKUPibvxJGejmpuAXZvgtVQVptmv5S84Z9wfJLqNtF8RIMVYQGJaxI+eK
         zCwumIftoVmkyeS0lOACbDKbAJ4ZTtUXKBaua5S0BGInYFHzP+s8eJ5OSIMWUCk68Loi
         itqfkQRuYZOR1bS+/r4mNz61c7WDwrhl5mrET0x2PJpgN1cqDQqJ3Vz4MchDyBzX1pXh
         VUzolQ/oUxQWl5ByEV3aDz0BgtIBNM0dkUbROEKMfAemr0bWTS8voxd1zmOcHCddobwH
         k+Qg==
MIME-Version: 1.0
X-Received: by 10.180.228.42 with SMTP id sf10mr15953539wic.33.1398700535656;
 Mon, 28 Apr 2014 08:55:35 -0700 (PDT)
Sender: david.lw.hall@gmail.com
Received: by 10.227.232.207 with HTTP; Mon, 28 Apr 2014 08:55:35 -0700 (PDT)
In-Reply-To: <CAEYYnxb4QLneUjfj5dG80Pzae8MhEQG090YQ2G4oq7VccayM-g@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxZFmxXkrNhdUNF3je7k0Eg_YjY2WnvVW5SSk__FAWNz4w@mail.gmail.com>
	<CAJgQjQ-k5P3CxVFCE-pHtuH9fVwwCcPc6PBp1NgxM2uueOASsw@mail.gmail.com>
	<CAJgQjQ_ePvspakEfiHzxdDH30EeEzjxsz0y_G0n51VophVL1hg@mail.gmail.com>
	<CAEYYnxZb3GbjdVVWyF0F4c_sEikadefP88rnWuGtHrx9HwMBJQ@mail.gmail.com>
	<CAJgQjQ8s7dP-rUtiiSUV2imeL9r++_WEJj1RyWCy+b3QBEcxRw@mail.gmail.com>
	<CAEYYnxaSh532XmO7224BiM5D2g_Q0+k49AaFbuT6LxDCmSUZBw@mail.gmail.com>
	<CAEYYnxaKTW-74AmKSAAg_kQkAyqePjRNhn9rS2VVbt8wp_4CsA@mail.gmail.com>
	<CAEYYnxZNLk+-Hh8o7V88GBR+pTZxE0qm+mw4YxFeun7grsPEng@mail.gmail.com>
	<CALW2ey0PqNry2qK_SRMs6C=S3tZ_3KoGYHeKPHG0G8EuQ6vXag@mail.gmail.com>
	<CAEYYnxYEOHFEp7obAWmVq=SMXxpLp3hJavyM0NAzwQ7SsN+ixA@mail.gmail.com>
	<CAEYYnxYx1s+C9Qe_hHbXAvyYbeGCikMaXDHjqtcQf9tUG9cfbw@mail.gmail.com>
	<CAEYYnxb4QLneUjfj5dG80Pzae8MhEQG090YQ2G4oq7VccayM-g@mail.gmail.com>
Date: Mon, 28 Apr 2014 08:55:35 -0700
X-Google-Sender-Auth: 6iqSni5a9Giq7ZDUfrstelCEvgo
Message-ID: <CALW2ey312i1iRBH9Tg_zSb1jd_hGDoXtNuy-MLC5fEGZ=Vo+Xg@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: David Hall <dlwh@cs.berkeley.edu>
To: DB Tsai <dbtsai@stanford.edu>
Cc: Xiangrui Meng <mengxr@gmail.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1135e8f293393904f81c584f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1135e8f293393904f81c584f
Content-Type: text/plain; charset=UTF-8

That's right.

FWIW, caching should be automatic now, but it might be the version of
Breeze you're using doesn't do that yet.

Also, In breeze.util._ there's an implicit that adds a tee method to
iterator, and also a last method. Both are useful for things like this.

-- David

On Sun, Apr 27, 2014 at 11:53 PM, DB Tsai <dbtsai@stanford.edu> wrote:

> I think I figure it out. Instead of calling minimize, and record the loss
> in the DiffFunction, I should do the following.
>
> val states = lbfgs.iterations(new CachedDiffFunction(costFun),
> initialWeights.toBreeze.toDenseVector)
> states.foreach(state => lossHistory.append(state.value))
>
> All the losses in states should be decreasing now. Am I right?
>
>
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Sun, Apr 27, 2014 at 11:31 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>
>> Also, how many failure of rejection will terminate the optimization
>> process? How is it related to "numberOfImprovementFailures"?
>>
>> Thanks.
>>
>>
>> Sincerely,
>>
>> DB Tsai
>> -------------------------------------------------------
>> My Blog: https://www.dbtsai.com
>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>
>>
>> On Sun, Apr 27, 2014 at 11:28 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>
>>> Hi David,
>>>
>>> I'm recording the loss history in the DiffFunction implementation, and
>>> that's why the rejected step is also recorded in my loss history.
>>>
>>> Is there any api in Breeze LBFGS to get the history which already
>>> excludes the reject step? Or should I just call "iterations" method and
>>> check "iteratingShouldStop" instead?
>>>
>>> Thanks.
>>>
>>>
>>> Sincerely,
>>>
>>> DB Tsai
>>> -------------------------------------------------------
>>> My Blog: https://www.dbtsai.com
>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>
>>>
>>> On Fri, Apr 25, 2014 at 3:10 PM, David Hall <dlwh@cs.berkeley.edu>wrote:
>>>
>>>> LBFGS will not take a step that sends the objective value up. It might
>>>> try a step that is "too big" and reject it, so if you're just logging
>>>> everything that gets tried by LBFGS, you could see that. The "iterations"
>>>> method of the minimizer should never return an increasing objective value.
>>>> If you're regularizing, are you including the regularizer in the objective
>>>> value computation?
>>>>
>>>> GD is almost never worth your time.
>>>>
>>>> -- David
>>>>
>>>> On Fri, Apr 25, 2014 at 2:57 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>>
>>>>> Another interesting benchmark.
>>>>>
>>>>> *News20 dataset - 0.14M row, 1,355,191 features, 0.034% non-zero
>>>>> elements.*
>>>>>
>>>>> LBFGS converges in 70 seconds, while GD seems to be not progressing.
>>>>>
>>>>> Dense feature vector will be too big to fit in the memory, so only
>>>>> conduct the sparse benchmark.
>>>>>
>>>>> I saw the sometimes the loss bumps up, and it's weird for me. Since
>>>>> the cost function of logistic regression is convex, it should be
>>>>> monotonically decreasing.  David, any suggestion?
>>>>>
>>>>> The detail figure:
>>>>>
>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/news20.pdf
>>>>>
>>>>>
>>>>> *Rcv1 dataset - 6.8M row, 677,399 features, 0.15% non-zero elements.*
>>>>>
>>>>> LBFGS converges in 25 seconds, while GD also seems to be not
>>>>> progressing.
>>>>>
>>>>> Only conduct sparse benchmark for the same reason. I also saw the loss
>>>>> bumps up for unknown reason.
>>>>>
>>>>> The detail figure:
>>>>>
>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/rcv1.pdf
>>>>>
>>>>>
>>>>> Sincerely,
>>>>>
>>>>> DB Tsai
>>>>> -------------------------------------------------------
>>>>> My Blog: https://www.dbtsai.com
>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>
>>>>>
>>>>> On Thu, Apr 24, 2014 at 2:36 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>>>
>>>>>> rcv1.binary is too sparse (0.15% non-zero elements), so dense format
>>>>>> will not run due to out of memory. But sparse format runs really well.
>>>>>>
>>>>>>
>>>>>> Sincerely,
>>>>>>
>>>>>> DB Tsai
>>>>>> -------------------------------------------------------
>>>>>> My Blog: https://www.dbtsai.com
>>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>
>>>>>>
>>>>>> On Thu, Apr 24, 2014 at 1:54 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>>>>
>>>>>>> I'm doing the timer in runMiniBatchSGD after  val numExamples =
>>>>>>> data.count()
>>>>>>>
>>>>>>> See the following. Running rcv1 dataset now, and will update soon.
>>>>>>>
>>>>>>>     val startTime = System.nanoTime()
>>>>>>>     for (i <- 1 to numIterations) {
>>>>>>>       // Sample a subset (fraction miniBatchFraction) of the total
>>>>>>> data
>>>>>>>       // compute and sum up the subgradients on this subset (this is
>>>>>>> one map-reduce)
>>>>>>>       val (gradientSum, lossSum) = data.sample(false,
>>>>>>> miniBatchFraction, 42 + i)
>>>>>>>         .aggregate((BDV.zeros[Double](weights.size), 0.0))(
>>>>>>>           seqOp = (c, v) => (c, v) match { case ((grad, loss),
>>>>>>> (label, features)) =>
>>>>>>>             val l = gradient.compute(features, label, weights,
>>>>>>> Vectors.fromBreeze(grad))
>>>>>>>             (grad, loss + l)
>>>>>>>           },
>>>>>>>           combOp = (c1, c2) => (c1, c2) match { case ((grad1,
>>>>>>> loss1), (grad2, loss2)) =>
>>>>>>>             (grad1 += grad2, loss1 + loss2)
>>>>>>>           })
>>>>>>>
>>>>>>>       /**
>>>>>>>        * NOTE(Xinghao): lossSum is computed using the weights from
>>>>>>> the previous iteration
>>>>>>>        * and regVal is the regularization value computed in the
>>>>>>> previous iteration as well.
>>>>>>>        */
>>>>>>>       stochasticLossHistory.append(lossSum / miniBatchSize + regVal)
>>>>>>>       val update = updater.compute(
>>>>>>>         weights, Vectors.fromBreeze(gradientSum / miniBatchSize),
>>>>>>> stepSize, i, regParam)
>>>>>>>       weights = update._1
>>>>>>>       regVal = update._2
>>>>>>>       timeStamp.append(System.nanoTime() - startTime)
>>>>>>>     }
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>> Sincerely,
>>>>>>>
>>>>>>> DB Tsai
>>>>>>> -------------------------------------------------------
>>>>>>> My Blog: https://www.dbtsai.com
>>>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>
>>>>>>>
>>>>>>> On Thu, Apr 24, 2014 at 1:44 PM, Xiangrui Meng <mengxr@gmail.com>wrote:
>>>>>>>
>>>>>>>> I don't understand why sparse falls behind dense so much at the very
>>>>>>>> first iteration. I didn't see count() is called in
>>>>>>>>
>>>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/blob/master/src/main/scala/org/apache/spark/mllib/benchmark/BinaryLogisticRegression.scala
>>>>>>>> . Maybe you have local uncommitted changes.
>>>>>>>>
>>>>>>>> Best,
>>>>>>>> Xiangrui
>>>>>>>>
>>>>>>>> On Thu, Apr 24, 2014 at 11:26 AM, DB Tsai <dbtsai@stanford.edu>
>>>>>>>> wrote:
>>>>>>>> > Hi Xiangrui,
>>>>>>>> >
>>>>>>>> > Yes, I'm using yarn-cluster mode, and I did check # of executors
>>>>>>>> I specified
>>>>>>>> > are the same as the actual running executors.
>>>>>>>> >
>>>>>>>> > For caching and materialization, I've the timer in optimizer
>>>>>>>> after calling
>>>>>>>> > count(); as a result, the time for materialization in cache isn't
>>>>>>>> in the
>>>>>>>> > benchmark.
>>>>>>>> >
>>>>>>>> > The difference you saw is actually from dense feature or sparse
>>>>>>>> feature
>>>>>>>> > vector. For LBFGS and GD dense feature, you can see the first
>>>>>>>> iteration
>>>>>>>> > takes the same time. It's true for GD.
>>>>>>>> >
>>>>>>>> > I'm going to run rcv1.binary which only has 0.15% non-zero
>>>>>>>> elements to
>>>>>>>> > verify the hypothesis.
>>>>>>>> >
>>>>>>>> >
>>>>>>>> > Sincerely,
>>>>>>>> >
>>>>>>>> > DB Tsai
>>>>>>>> > -------------------------------------------------------
>>>>>>>> > My Blog: https://www.dbtsai.com
>>>>>>>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>> >
>>>>>>>> >
>>>>>>>> > On Thu, Apr 24, 2014 at 1:09 AM, Xiangrui Meng <mengxr@gmail.com>
>>>>>>>> wrote:
>>>>>>>> >>
>>>>>>>> >> Hi DB,
>>>>>>>> >>
>>>>>>>> >> I saw you are using yarn-cluster mode for the benchmark. I
>>>>>>>> tested the
>>>>>>>> >> yarn-cluster mode and found that YARN does not always give you
>>>>>>>> the
>>>>>>>> >> exact number of executors requested. Just want to confirm that
>>>>>>>> you've
>>>>>>>> >> checked the number of executors.
>>>>>>>> >>
>>>>>>>> >> The second thing to check is that in the benchmark code, after
>>>>>>>> you
>>>>>>>> >> call cache, you should also call count() to materialize the RDD.
>>>>>>>> I saw
>>>>>>>> >> in the result, the real difference is actually at the first step.
>>>>>>>> >> Adding intercept is not a cheap operation for sparse vectors.
>>>>>>>> >>
>>>>>>>> >> Best,
>>>>>>>> >> Xiangrui
>>>>>>>> >>
>>>>>>>> >> On Thu, Apr 24, 2014 at 12:53 AM, Xiangrui Meng <
>>>>>>>> mengxr@gmail.com> wrote:
>>>>>>>> >> > I don't think it is easy to make sparse faster than dense with
>>>>>>>> this
>>>>>>>> >> > sparsity and feature dimension. You can try rcv1.binary, which
>>>>>>>> should
>>>>>>>> >> > show the difference easily.
>>>>>>>> >> >
>>>>>>>> >> > David, the breeze operators used here are
>>>>>>>> >> >
>>>>>>>> >> > 1. DenseVector dot SparseVector
>>>>>>>> >> > 2. axpy DenseVector SparseVector
>>>>>>>> >> >
>>>>>>>> >> > However, the SparseVector is passed in as Vector[Double]
>>>>>>>> instead of
>>>>>>>> >> > SparseVector[Double]. It might use the axpy impl of
>>>>>>>> [DenseVector,
>>>>>>>> >> > Vector] and call activeIterator. I didn't check whether you
>>>>>>>> used
>>>>>>>> >> > multimethods on axpy.
>>>>>>>> >> >
>>>>>>>> >> > Best,
>>>>>>>> >> > Xiangrui
>>>>>>>> >> >
>>>>>>>> >> > On Wed, Apr 23, 2014 at 10:35 PM, DB Tsai <dbtsai@stanford.edu>
>>>>>>>> wrote:
>>>>>>>> >> >> The figure showing the Log-Likelihood vs Time can be found
>>>>>>>> here.
>>>>>>>> >> >>
>>>>>>>> >> >>
>>>>>>>> >> >>
>>>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
>>>>>>>> >> >>
>>>>>>>> >> >> Let me know if you can not open it. Thanks.
>>>>>>>> >> >>
>>>>>>>> >> >> Sincerely,
>>>>>>>> >> >>
>>>>>>>> >> >> DB Tsai
>>>>>>>> >> >> -------------------------------------------------------
>>>>>>>> >> >> My Blog: https://www.dbtsai.com
>>>>>>>> >> >> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>> >> >>
>>>>>>>> >> >>
>>>>>>>> >> >> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman
>>>>>>>> >> >> <shivaram@eecs.berkeley.edu> wrote:
>>>>>>>> >> >>> I don't think the attachment came through in the list. Could
>>>>>>>> you
>>>>>>>> >> >>> upload the
>>>>>>>> >> >>> results somewhere and link to them ?
>>>>>>>> >> >>>
>>>>>>>> >> >>>
>>>>>>>> >> >>> On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com>
>>>>>>>> wrote:
>>>>>>>> >> >>>>
>>>>>>>> >> >>>> 123 features per rows, and in average, 89% are zeros.
>>>>>>>> >> >>>> On Apr 23, 2014 9:31 PM, "Evan Sparks" <
>>>>>>>> evan.sparks@gmail.com> wrote:
>>>>>>>> >> >>>>
>>>>>>>> >> >>>> > What is the number of non zeroes per row (and number of
>>>>>>>> features)
>>>>>>>> >> >>>> > in the
>>>>>>>> >> >>>> > sparse case? We've hit some issues with breeze sparse
>>>>>>>> support in
>>>>>>>> >> >>>> > the
>>>>>>>> >> >>>> > past
>>>>>>>> >> >>>> > but for sufficiently sparse data it's still pretty good.
>>>>>>>> >> >>>> >
>>>>>>>> >> >>>> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <
>>>>>>>> dbtsai@stanford.edu> wrote:
>>>>>>>> >> >>>> > >
>>>>>>>> >> >>>> > > Hi all,
>>>>>>>> >> >>>> > >
>>>>>>>> >> >>>> > > I'm benchmarking Logistic Regression in MLlib using the
>>>>>>>> newly
>>>>>>>> >> >>>> > > added
>>>>>>>> >> >>>> > optimizer LBFGS and GD. I'm using the same dataset and
>>>>>>>> the same
>>>>>>>> >> >>>> > methodology
>>>>>>>> >> >>>> > in this paper,
>>>>>>>> http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>>>>>>>> >> >>>> > >
>>>>>>>> >> >>>> > > I want to know how Spark scale while adding workers,
>>>>>>>> and how
>>>>>>>> >> >>>> > > optimizers
>>>>>>>> >> >>>> > and input format (sparse or dense) impact performance.
>>>>>>>> >> >>>> > >
>>>>>>>> >> >>>> > > The benchmark code can be found here,
>>>>>>>> >> >>>> > https://github.com/dbtsai/spark-lbfgs-benchmark
>>>>>>>> >> >>>> > >
>>>>>>>> >> >>>> > > The first dataset I benchmarked is a9a which only has
>>>>>>>> 2.2MB. I
>>>>>>>> >> >>>> > duplicated the dataset, and made it 762MB to have 11M
>>>>>>>> rows. This
>>>>>>>> >> >>>> > dataset
>>>>>>>> >> >>>> > has 123 features and 11% of the data are non-zero
>>>>>>>> elements.
>>>>>>>> >> >>>> > >
>>>>>>>> >> >>>> > > In this benchmark, all the dataset is cached in memory.
>>>>>>>> >> >>>> > >
>>>>>>>> >> >>>> > > As we expect, LBFGS converges faster than GD, and at
>>>>>>>> some point,
>>>>>>>> >> >>>> > > no
>>>>>>>> >> >>>> > matter how we push GD, it will converge slower and slower.
>>>>>>>> >> >>>> > >
>>>>>>>> >> >>>> > > However, it's surprising that sparse format runs slower
>>>>>>>> than
>>>>>>>> >> >>>> > > dense
>>>>>>>> >> >>>> > format. I did see that sparse format takes significantly
>>>>>>>> smaller
>>>>>>>> >> >>>> > amount
>>>>>>>> >> >>>> > of
>>>>>>>> >> >>>> > memory in caching RDD, but sparse is 40% slower than
>>>>>>>> dense. I think
>>>>>>>> >> >>>> > sparse
>>>>>>>> >> >>>> > should be fast since when we compute x wT, since x is
>>>>>>>> sparse, we
>>>>>>>> >> >>>> > can do
>>>>>>>> >> >>>> > it
>>>>>>>> >> >>>> > faster. I wonder if there is anything I'm doing wrong.
>>>>>>>> >> >>>> > >
>>>>>>>> >> >>>> > > The attachment is the benchmark result.
>>>>>>>> >> >>>> > >
>>>>>>>> >> >>>> > > Thanks.
>>>>>>>> >> >>>> > >
>>>>>>>> >> >>>> > > Sincerely,
>>>>>>>> >> >>>> > >
>>>>>>>> >> >>>> > > DB Tsai
>>>>>>>> >> >>>> > > -------------------------------------------------------
>>>>>>>> >> >>>> > > My Blog: https://www.dbtsai.com
>>>>>>>> >> >>>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>> >> >>>> >
>>>>>>>> >> >>>
>>>>>>>> >> >>>
>>>>>>>> >
>>>>>>>> >
>>>>>>>>
>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>

--001a1135e8f293393904f81c584f--

From dev-return-7444-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 28 21:36:55 2014
Return-Path: <dev-return-7444-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2727F10A80
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Apr 2014 21:36:55 +0000 (UTC)
Received: (qmail 57955 invoked by uid 500); 28 Apr 2014 21:36:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57866 invoked by uid 500); 28 Apr 2014 21:36:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57849 invoked by uid 99); 28 Apr 2014 21:36:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Apr 2014 21:36:52 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Apr 2014 21:36:48 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id C580C1015A9
	for <dev@spark.apache.org>; Mon, 28 Apr 2014 14:36:27 -0700 (PDT)
Received: from mail-qc0-f179.google.com (mail-qc0-f179.google.com [209.85.216.179])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 9D9811016AA
	for <dev@spark.apache.org>; Mon, 28 Apr 2014 14:36:26 -0700 (PDT)
Received: by mail-qc0-f179.google.com with SMTP id l6so6687443qcy.38
        for <dev@spark.apache.org>; Mon, 28 Apr 2014 14:36:25 -0700 (PDT)
X-Gm-Message-State: ALoCoQlT3MRFeHkpkDjRocTE/YRXDOHNomsc7Cpsb/yIiRzbg/W5ttwdhKDbUhYo7VbRc5vTUewD
MIME-Version: 1.0
X-Received: by 10.224.30.131 with SMTP id u3mr37529241qac.50.1398720985692;
 Mon, 28 Apr 2014 14:36:25 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Mon, 28 Apr 2014 14:36:25 -0700 (PDT)
In-Reply-To: <CALW2ey312i1iRBH9Tg_zSb1jd_hGDoXtNuy-MLC5fEGZ=Vo+Xg@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxZFmxXkrNhdUNF3je7k0Eg_YjY2WnvVW5SSk__FAWNz4w@mail.gmail.com>
	<CAJgQjQ-k5P3CxVFCE-pHtuH9fVwwCcPc6PBp1NgxM2uueOASsw@mail.gmail.com>
	<CAJgQjQ_ePvspakEfiHzxdDH30EeEzjxsz0y_G0n51VophVL1hg@mail.gmail.com>
	<CAEYYnxZb3GbjdVVWyF0F4c_sEikadefP88rnWuGtHrx9HwMBJQ@mail.gmail.com>
	<CAJgQjQ8s7dP-rUtiiSUV2imeL9r++_WEJj1RyWCy+b3QBEcxRw@mail.gmail.com>
	<CAEYYnxaSh532XmO7224BiM5D2g_Q0+k49AaFbuT6LxDCmSUZBw@mail.gmail.com>
	<CAEYYnxaKTW-74AmKSAAg_kQkAyqePjRNhn9rS2VVbt8wp_4CsA@mail.gmail.com>
	<CAEYYnxZNLk+-Hh8o7V88GBR+pTZxE0qm+mw4YxFeun7grsPEng@mail.gmail.com>
	<CALW2ey0PqNry2qK_SRMs6C=S3tZ_3KoGYHeKPHG0G8EuQ6vXag@mail.gmail.com>
	<CAEYYnxYEOHFEp7obAWmVq=SMXxpLp3hJavyM0NAzwQ7SsN+ixA@mail.gmail.com>
	<CAEYYnxYx1s+C9Qe_hHbXAvyYbeGCikMaXDHjqtcQf9tUG9cfbw@mail.gmail.com>
	<CAEYYnxb4QLneUjfj5dG80Pzae8MhEQG090YQ2G4oq7VccayM-g@mail.gmail.com>
	<CALW2ey312i1iRBH9Tg_zSb1jd_hGDoXtNuy-MLC5fEGZ=Vo+Xg@mail.gmail.com>
Date: Mon, 28 Apr 2014 14:36:25 -0700
Message-ID: <CAEYYnxbT1XMs+URBaOUPCT4i1gXTHCX8DL4OSddG-09zW=paGg@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: DB Tsai <dbtsai@stanford.edu>
To: David Hall <dlwh@cs.berkeley.edu>
Cc: Xiangrui Meng <mengxr@gmail.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bdc90a07e485904f8211b60
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc90a07e485904f8211b60
Content-Type: text/plain; charset=UTF-8

Hi David,

I got most of the stuff working, and the loss is monotonically decreasing
by getting the history from iterator of state.

However, in the costFun, I need to know what current iteration is it for
miniBatch, which means for one iteration, if optimizer calls costFun
several times for line search, it should pass the same iteration into
costFun. So I pass the lbfgs optimizer into costFun as the following code,
and try to find the current iteration in lbfgs object. Unfortunately, it
seems that the current iteration is not available in this object.

Any idea for getting this in costFun? Originally, I've a counter inside
costFun which gives the # of iterations. However, it's not what I want now
since it also counts line search.

val lbfgs = new BreezeLBFGS[BDV[Double]](maxNumIterations, numCorrections,
convergenceTol)

val costFun =
      new CostFun(data, gradient, updater, miniBatchFraction, lbfgs,
miniBatchSize)

val states = lbfgs.iterations(new CachedDiffFunction(costFun),
initialWeights.toBreeze.toDenseVector)


Thanks.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Mon, Apr 28, 2014 at 8:55 AM, David Hall <dlwh@cs.berkeley.edu> wrote:

> That's right.
>
> FWIW, caching should be automatic now, but it might be the version of
> Breeze you're using doesn't do that yet.
>
> Also, In breeze.util._ there's an implicit that adds a tee method to
> iterator, and also a last method. Both are useful for things like this.
>
> -- David
>
>
> On Sun, Apr 27, 2014 at 11:53 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>
>> I think I figure it out. Instead of calling minimize, and record the loss
>> in the DiffFunction, I should do the following.
>>
>> val states = lbfgs.iterations(new CachedDiffFunction(costFun),
>> initialWeights.toBreeze.toDenseVector)
>> states.foreach(state => lossHistory.append(state.value))
>>
>> All the losses in states should be decreasing now. Am I right?
>>
>>
>>
>> Sincerely,
>>
>> DB Tsai
>> -------------------------------------------------------
>> My Blog: https://www.dbtsai.com
>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>
>>
>> On Sun, Apr 27, 2014 at 11:31 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>
>>> Also, how many failure of rejection will terminate the optimization
>>> process? How is it related to "numberOfImprovementFailures"?
>>>
>>> Thanks.
>>>
>>>
>>> Sincerely,
>>>
>>> DB Tsai
>>> -------------------------------------------------------
>>> My Blog: https://www.dbtsai.com
>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>
>>>
>>> On Sun, Apr 27, 2014 at 11:28 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>
>>>> Hi David,
>>>>
>>>> I'm recording the loss history in the DiffFunction implementation, and
>>>> that's why the rejected step is also recorded in my loss history.
>>>>
>>>> Is there any api in Breeze LBFGS to get the history which already
>>>> excludes the reject step? Or should I just call "iterations" method and
>>>> check "iteratingShouldStop" instead?
>>>>
>>>> Thanks.
>>>>
>>>>
>>>> Sincerely,
>>>>
>>>> DB Tsai
>>>> -------------------------------------------------------
>>>> My Blog: https://www.dbtsai.com
>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>
>>>>
>>>> On Fri, Apr 25, 2014 at 3:10 PM, David Hall <dlwh@cs.berkeley.edu>wrote:
>>>>
>>>>> LBFGS will not take a step that sends the objective value up. It might
>>>>> try a step that is "too big" and reject it, so if you're just logging
>>>>> everything that gets tried by LBFGS, you could see that. The "iterations"
>>>>> method of the minimizer should never return an increasing objective value.
>>>>> If you're regularizing, are you including the regularizer in the objective
>>>>> value computation?
>>>>>
>>>>> GD is almost never worth your time.
>>>>>
>>>>> -- David
>>>>>
>>>>> On Fri, Apr 25, 2014 at 2:57 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>>>
>>>>>> Another interesting benchmark.
>>>>>>
>>>>>> *News20 dataset - 0.14M row, 1,355,191 features, 0.034% non-zero
>>>>>> elements.*
>>>>>>
>>>>>> LBFGS converges in 70 seconds, while GD seems to be not progressing.
>>>>>>
>>>>>> Dense feature vector will be too big to fit in the memory, so only
>>>>>> conduct the sparse benchmark.
>>>>>>
>>>>>> I saw the sometimes the loss bumps up, and it's weird for me. Since
>>>>>> the cost function of logistic regression is convex, it should be
>>>>>> monotonically decreasing.  David, any suggestion?
>>>>>>
>>>>>> The detail figure:
>>>>>>
>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/news20.pdf
>>>>>>
>>>>>>
>>>>>> *Rcv1 dataset - 6.8M row, 677,399 features, 0.15% non-zero elements.*
>>>>>>
>>>>>> LBFGS converges in 25 seconds, while GD also seems to be not
>>>>>> progressing.
>>>>>>
>>>>>> Only conduct sparse benchmark for the same reason. I also saw the
>>>>>> loss bumps up for unknown reason.
>>>>>>
>>>>>> The detail figure:
>>>>>>
>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/rcv1.pdf
>>>>>>
>>>>>>
>>>>>> Sincerely,
>>>>>>
>>>>>> DB Tsai
>>>>>> -------------------------------------------------------
>>>>>> My Blog: https://www.dbtsai.com
>>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>
>>>>>>
>>>>>> On Thu, Apr 24, 2014 at 2:36 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>>>>
>>>>>>> rcv1.binary is too sparse (0.15% non-zero elements), so dense
>>>>>>> format will not run due to out of memory. But sparse format runs really
>>>>>>> well.
>>>>>>>
>>>>>>>
>>>>>>> Sincerely,
>>>>>>>
>>>>>>> DB Tsai
>>>>>>> -------------------------------------------------------
>>>>>>> My Blog: https://www.dbtsai.com
>>>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>
>>>>>>>
>>>>>>> On Thu, Apr 24, 2014 at 1:54 PM, DB Tsai <dbtsai@stanford.edu>wrote:
>>>>>>>
>>>>>>>> I'm doing the timer in runMiniBatchSGD after  val numExamples =
>>>>>>>> data.count()
>>>>>>>>
>>>>>>>> See the following. Running rcv1 dataset now, and will update soon.
>>>>>>>>
>>>>>>>>     val startTime = System.nanoTime()
>>>>>>>>     for (i <- 1 to numIterations) {
>>>>>>>>       // Sample a subset (fraction miniBatchFraction) of the total
>>>>>>>> data
>>>>>>>>       // compute and sum up the subgradients on this subset (this
>>>>>>>> is one map-reduce)
>>>>>>>>       val (gradientSum, lossSum) = data.sample(false,
>>>>>>>> miniBatchFraction, 42 + i)
>>>>>>>>         .aggregate((BDV.zeros[Double](weights.size), 0.0))(
>>>>>>>>           seqOp = (c, v) => (c, v) match { case ((grad, loss),
>>>>>>>> (label, features)) =>
>>>>>>>>             val l = gradient.compute(features, label, weights,
>>>>>>>> Vectors.fromBreeze(grad))
>>>>>>>>             (grad, loss + l)
>>>>>>>>           },
>>>>>>>>           combOp = (c1, c2) => (c1, c2) match { case ((grad1,
>>>>>>>> loss1), (grad2, loss2)) =>
>>>>>>>>             (grad1 += grad2, loss1 + loss2)
>>>>>>>>           })
>>>>>>>>
>>>>>>>>       /**
>>>>>>>>        * NOTE(Xinghao): lossSum is computed using the weights from
>>>>>>>> the previous iteration
>>>>>>>>        * and regVal is the regularization value computed in the
>>>>>>>> previous iteration as well.
>>>>>>>>        */
>>>>>>>>       stochasticLossHistory.append(lossSum / miniBatchSize + regVal)
>>>>>>>>       val update = updater.compute(
>>>>>>>>         weights, Vectors.fromBreeze(gradientSum / miniBatchSize),
>>>>>>>> stepSize, i, regParam)
>>>>>>>>       weights = update._1
>>>>>>>>       regVal = update._2
>>>>>>>>       timeStamp.append(System.nanoTime() - startTime)
>>>>>>>>     }
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> Sincerely,
>>>>>>>>
>>>>>>>> DB Tsai
>>>>>>>> -------------------------------------------------------
>>>>>>>> My Blog: https://www.dbtsai.com
>>>>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>
>>>>>>>>
>>>>>>>> On Thu, Apr 24, 2014 at 1:44 PM, Xiangrui Meng <mengxr@gmail.com>wrote:
>>>>>>>>
>>>>>>>>> I don't understand why sparse falls behind dense so much at the
>>>>>>>>> very
>>>>>>>>> first iteration. I didn't see count() is called in
>>>>>>>>>
>>>>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/blob/master/src/main/scala/org/apache/spark/mllib/benchmark/BinaryLogisticRegression.scala
>>>>>>>>> . Maybe you have local uncommitted changes.
>>>>>>>>>
>>>>>>>>> Best,
>>>>>>>>> Xiangrui
>>>>>>>>>
>>>>>>>>> On Thu, Apr 24, 2014 at 11:26 AM, DB Tsai <dbtsai@stanford.edu>
>>>>>>>>> wrote:
>>>>>>>>> > Hi Xiangrui,
>>>>>>>>> >
>>>>>>>>> > Yes, I'm using yarn-cluster mode, and I did check # of executors
>>>>>>>>> I specified
>>>>>>>>> > are the same as the actual running executors.
>>>>>>>>> >
>>>>>>>>> > For caching and materialization, I've the timer in optimizer
>>>>>>>>> after calling
>>>>>>>>> > count(); as a result, the time for materialization in cache
>>>>>>>>> isn't in the
>>>>>>>>> > benchmark.
>>>>>>>>> >
>>>>>>>>> > The difference you saw is actually from dense feature or sparse
>>>>>>>>> feature
>>>>>>>>> > vector. For LBFGS and GD dense feature, you can see the first
>>>>>>>>> iteration
>>>>>>>>> > takes the same time. It's true for GD.
>>>>>>>>> >
>>>>>>>>> > I'm going to run rcv1.binary which only has 0.15% non-zero
>>>>>>>>> elements to
>>>>>>>>> > verify the hypothesis.
>>>>>>>>> >
>>>>>>>>> >
>>>>>>>>> > Sincerely,
>>>>>>>>> >
>>>>>>>>> > DB Tsai
>>>>>>>>> > -------------------------------------------------------
>>>>>>>>> > My Blog: https://www.dbtsai.com
>>>>>>>>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>> >
>>>>>>>>> >
>>>>>>>>> > On Thu, Apr 24, 2014 at 1:09 AM, Xiangrui Meng <mengxr@gmail.com>
>>>>>>>>> wrote:
>>>>>>>>> >>
>>>>>>>>> >> Hi DB,
>>>>>>>>> >>
>>>>>>>>> >> I saw you are using yarn-cluster mode for the benchmark. I
>>>>>>>>> tested the
>>>>>>>>> >> yarn-cluster mode and found that YARN does not always give you
>>>>>>>>> the
>>>>>>>>> >> exact number of executors requested. Just want to confirm that
>>>>>>>>> you've
>>>>>>>>> >> checked the number of executors.
>>>>>>>>> >>
>>>>>>>>> >> The second thing to check is that in the benchmark code, after
>>>>>>>>> you
>>>>>>>>> >> call cache, you should also call count() to materialize the
>>>>>>>>> RDD. I saw
>>>>>>>>> >> in the result, the real difference is actually at the first
>>>>>>>>> step.
>>>>>>>>> >> Adding intercept is not a cheap operation for sparse vectors.
>>>>>>>>> >>
>>>>>>>>> >> Best,
>>>>>>>>> >> Xiangrui
>>>>>>>>> >>
>>>>>>>>> >> On Thu, Apr 24, 2014 at 12:53 AM, Xiangrui Meng <
>>>>>>>>> mengxr@gmail.com> wrote:
>>>>>>>>> >> > I don't think it is easy to make sparse faster than dense
>>>>>>>>> with this
>>>>>>>>> >> > sparsity and feature dimension. You can try rcv1.binary,
>>>>>>>>> which should
>>>>>>>>> >> > show the difference easily.
>>>>>>>>> >> >
>>>>>>>>> >> > David, the breeze operators used here are
>>>>>>>>> >> >
>>>>>>>>> >> > 1. DenseVector dot SparseVector
>>>>>>>>> >> > 2. axpy DenseVector SparseVector
>>>>>>>>> >> >
>>>>>>>>> >> > However, the SparseVector is passed in as Vector[Double]
>>>>>>>>> instead of
>>>>>>>>> >> > SparseVector[Double]. It might use the axpy impl of
>>>>>>>>> [DenseVector,
>>>>>>>>> >> > Vector] and call activeIterator. I didn't check whether you
>>>>>>>>> used
>>>>>>>>> >> > multimethods on axpy.
>>>>>>>>> >> >
>>>>>>>>> >> > Best,
>>>>>>>>> >> > Xiangrui
>>>>>>>>> >> >
>>>>>>>>> >> > On Wed, Apr 23, 2014 at 10:35 PM, DB Tsai <
>>>>>>>>> dbtsai@stanford.edu> wrote:
>>>>>>>>> >> >> The figure showing the Log-Likelihood vs Time can be found
>>>>>>>>> here.
>>>>>>>>> >> >>
>>>>>>>>> >> >>
>>>>>>>>> >> >>
>>>>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
>>>>>>>>> >> >>
>>>>>>>>> >> >> Let me know if you can not open it. Thanks.
>>>>>>>>> >> >>
>>>>>>>>> >> >> Sincerely,
>>>>>>>>> >> >>
>>>>>>>>> >> >> DB Tsai
>>>>>>>>> >> >> -------------------------------------------------------
>>>>>>>>> >> >> My Blog: https://www.dbtsai.com
>>>>>>>>> >> >> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>> >> >>
>>>>>>>>> >> >>
>>>>>>>>> >> >> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman
>>>>>>>>> >> >> <shivaram@eecs.berkeley.edu> wrote:
>>>>>>>>> >> >>> I don't think the attachment came through in the list.
>>>>>>>>> Could you
>>>>>>>>> >> >>> upload the
>>>>>>>>> >> >>> results somewhere and link to them ?
>>>>>>>>> >> >>>
>>>>>>>>> >> >>>
>>>>>>>>> >> >>> On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com>
>>>>>>>>> wrote:
>>>>>>>>> >> >>>>
>>>>>>>>> >> >>>> 123 features per rows, and in average, 89% are zeros.
>>>>>>>>> >> >>>> On Apr 23, 2014 9:31 PM, "Evan Sparks" <
>>>>>>>>> evan.sparks@gmail.com> wrote:
>>>>>>>>> >> >>>>
>>>>>>>>> >> >>>> > What is the number of non zeroes per row (and number of
>>>>>>>>> features)
>>>>>>>>> >> >>>> > in the
>>>>>>>>> >> >>>> > sparse case? We've hit some issues with breeze sparse
>>>>>>>>> support in
>>>>>>>>> >> >>>> > the
>>>>>>>>> >> >>>> > past
>>>>>>>>> >> >>>> > but for sufficiently sparse data it's still pretty good.
>>>>>>>>> >> >>>> >
>>>>>>>>> >> >>>> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <
>>>>>>>>> dbtsai@stanford.edu> wrote:
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > Hi all,
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > I'm benchmarking Logistic Regression in MLlib using
>>>>>>>>> the newly
>>>>>>>>> >> >>>> > > added
>>>>>>>>> >> >>>> > optimizer LBFGS and GD. I'm using the same dataset and
>>>>>>>>> the same
>>>>>>>>> >> >>>> > methodology
>>>>>>>>> >> >>>> > in this paper,
>>>>>>>>> http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > I want to know how Spark scale while adding workers,
>>>>>>>>> and how
>>>>>>>>> >> >>>> > > optimizers
>>>>>>>>> >> >>>> > and input format (sparse or dense) impact performance.
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > The benchmark code can be found here,
>>>>>>>>> >> >>>> > https://github.com/dbtsai/spark-lbfgs-benchmark
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > The first dataset I benchmarked is a9a which only has
>>>>>>>>> 2.2MB. I
>>>>>>>>> >> >>>> > duplicated the dataset, and made it 762MB to have 11M
>>>>>>>>> rows. This
>>>>>>>>> >> >>>> > dataset
>>>>>>>>> >> >>>> > has 123 features and 11% of the data are non-zero
>>>>>>>>> elements.
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > In this benchmark, all the dataset is cached in memory.
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > As we expect, LBFGS converges faster than GD, and at
>>>>>>>>> some point,
>>>>>>>>> >> >>>> > > no
>>>>>>>>> >> >>>> > matter how we push GD, it will converge slower and
>>>>>>>>> slower.
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > However, it's surprising that sparse format runs
>>>>>>>>> slower than
>>>>>>>>> >> >>>> > > dense
>>>>>>>>> >> >>>> > format. I did see that sparse format takes significantly
>>>>>>>>> smaller
>>>>>>>>> >> >>>> > amount
>>>>>>>>> >> >>>> > of
>>>>>>>>> >> >>>> > memory in caching RDD, but sparse is 40% slower than
>>>>>>>>> dense. I think
>>>>>>>>> >> >>>> > sparse
>>>>>>>>> >> >>>> > should be fast since when we compute x wT, since x is
>>>>>>>>> sparse, we
>>>>>>>>> >> >>>> > can do
>>>>>>>>> >> >>>> > it
>>>>>>>>> >> >>>> > faster. I wonder if there is anything I'm doing wrong.
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > The attachment is the benchmark result.
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > Thanks.
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > Sincerely,
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > DB Tsai
>>>>>>>>> >> >>>> > > -------------------------------------------------------
>>>>>>>>> >> >>>> > > My Blog: https://www.dbtsai.com
>>>>>>>>> >> >>>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>> >> >>>> >
>>>>>>>>> >> >>>
>>>>>>>>> >> >>>
>>>>>>>>> >
>>>>>>>>> >
>>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>

--047d7bdc90a07e485904f8211b60--

From dev-return-7445-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Apr 28 21:36:58 2014
Return-Path: <dev-return-7445-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B76C610A81
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Apr 2014 21:36:58 +0000 (UTC)
Received: (qmail 58625 invoked by uid 500); 28 Apr 2014 21:36:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58580 invoked by uid 500); 28 Apr 2014 21:36:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58572 invoked by uid 99); 28 Apr 2014 21:36:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Apr 2014 21:36:57 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Apr 2014 21:36:52 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 851A11016DD
	for <dev@spark.apache.org>; Mon, 28 Apr 2014 14:36:28 -0700 (PDT)
Received: from mail-yh0-f50.google.com (mail-yh0-f50.google.com [209.85.213.50])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 7B03A1016E0
	for <dev@spark.apache.org>; Mon, 28 Apr 2014 14:36:27 -0700 (PDT)
Received: by mail-yh0-f50.google.com with SMTP id b6so4005765yha.9
        for <dev@spark.apache.org>; Mon, 28 Apr 2014 14:36:26 -0700 (PDT)
X-Gm-Message-State: ALoCoQlv3IeCEvyZ8oCKog1L861xCD1Nkvt+ubRCiznb5NjsGiKx2NNIWP8KbW2KHfJ+GF+d+8G1
MIME-Version: 1.0
X-Received: by 10.236.150.205 with SMTP id z53mr42420841yhj.75.1398720986746;
 Mon, 28 Apr 2014 14:36:26 -0700 (PDT)
Received: by 10.170.205.71 with HTTP; Mon, 28 Apr 2014 14:36:26 -0700 (PDT)
In-Reply-To: <CALW2ey312i1iRBH9Tg_zSb1jd_hGDoXtNuy-MLC5fEGZ=Vo+Xg@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxZFmxXkrNhdUNF3je7k0Eg_YjY2WnvVW5SSk__FAWNz4w@mail.gmail.com>
	<CAJgQjQ-k5P3CxVFCE-pHtuH9fVwwCcPc6PBp1NgxM2uueOASsw@mail.gmail.com>
	<CAJgQjQ_ePvspakEfiHzxdDH30EeEzjxsz0y_G0n51VophVL1hg@mail.gmail.com>
	<CAEYYnxZb3GbjdVVWyF0F4c_sEikadefP88rnWuGtHrx9HwMBJQ@mail.gmail.com>
	<CAJgQjQ8s7dP-rUtiiSUV2imeL9r++_WEJj1RyWCy+b3QBEcxRw@mail.gmail.com>
	<CAEYYnxaSh532XmO7224BiM5D2g_Q0+k49AaFbuT6LxDCmSUZBw@mail.gmail.com>
	<CAEYYnxaKTW-74AmKSAAg_kQkAyqePjRNhn9rS2VVbt8wp_4CsA@mail.gmail.com>
	<CAEYYnxZNLk+-Hh8o7V88GBR+pTZxE0qm+mw4YxFeun7grsPEng@mail.gmail.com>
	<CALW2ey0PqNry2qK_SRMs6C=S3tZ_3KoGYHeKPHG0G8EuQ6vXag@mail.gmail.com>
	<CAEYYnxYEOHFEp7obAWmVq=SMXxpLp3hJavyM0NAzwQ7SsN+ixA@mail.gmail.com>
	<CAEYYnxYx1s+C9Qe_hHbXAvyYbeGCikMaXDHjqtcQf9tUG9cfbw@mail.gmail.com>
	<CAEYYnxb4QLneUjfj5dG80Pzae8MhEQG090YQ2G4oq7VccayM-g@mail.gmail.com>
	<CALW2ey312i1iRBH9Tg_zSb1jd_hGDoXtNuy-MLC5fEGZ=Vo+Xg@mail.gmail.com>
Date: Mon, 28 Apr 2014 14:36:26 -0700
Message-ID: <CAEYYnxaj5fSJbVyyVwyxqUgek6EaTEyOVuS0C=BtQme4BJ3Meg@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: DB Tsai <dbtsai@stanford.edu>
To: David Hall <dlwh@cs.berkeley.edu>
Cc: Xiangrui Meng <mengxr@gmail.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=20cf303a2d618e270704f8211bd8
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf303a2d618e270704f8211bd8
Content-Type: text/plain; charset=UTF-8

Hi David,

I got most of the stuff working, and the loss is monotonically decreasing
by getting the history from iterator of state.

However, in the costFun, I need to know what current iteration is it for
miniBatch, which means for one iteration, if optimizer calls costFun
several times for line search, it should pass the same iteration into
costFun. So I pass the lbfgs optimizer into costFun as the following code,
and try to find the current iteration in lbfgs object. Unfortunately, it
seems that the current iteration is not available in this object.

Any idea for getting this in costFun? Originally, I've a counter inside
costFun which gives the # of iterations. However, it's not what I want now
since it also counts line search.

val lbfgs = new BreezeLBFGS[BDV[Double]](maxNumIterations, numCorrections,
convergenceTol)

val costFun =
      new CostFun(data, gradient, updater, miniBatchFraction, lbfgs,
miniBatchSize)

val states = lbfgs.iterations(new CachedDiffFunction(costFun),
initialWeights.toBreeze.toDenseVector)


Thanks.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Mon, Apr 28, 2014 at 8:55 AM, David Hall <dlwh@cs.berkeley.edu> wrote:

> That's right.
>
> FWIW, caching should be automatic now, but it might be the version of
> Breeze you're using doesn't do that yet.
>
> Also, In breeze.util._ there's an implicit that adds a tee method to
> iterator, and also a last method. Both are useful for things like this.
>
> -- David
>
>
> On Sun, Apr 27, 2014 at 11:53 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>
>> I think I figure it out. Instead of calling minimize, and record the loss
>> in the DiffFunction, I should do the following.
>>
>> val states = lbfgs.iterations(new CachedDiffFunction(costFun),
>> initialWeights.toBreeze.toDenseVector)
>> states.foreach(state => lossHistory.append(state.value))
>>
>> All the losses in states should be decreasing now. Am I right?
>>
>>
>>
>> Sincerely,
>>
>> DB Tsai
>> -------------------------------------------------------
>> My Blog: https://www.dbtsai.com
>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>
>>
>> On Sun, Apr 27, 2014 at 11:31 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>
>>> Also, how many failure of rejection will terminate the optimization
>>> process? How is it related to "numberOfImprovementFailures"?
>>>
>>> Thanks.
>>>
>>>
>>> Sincerely,
>>>
>>> DB Tsai
>>> -------------------------------------------------------
>>> My Blog: https://www.dbtsai.com
>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>
>>>
>>> On Sun, Apr 27, 2014 at 11:28 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>
>>>> Hi David,
>>>>
>>>> I'm recording the loss history in the DiffFunction implementation, and
>>>> that's why the rejected step is also recorded in my loss history.
>>>>
>>>> Is there any api in Breeze LBFGS to get the history which already
>>>> excludes the reject step? Or should I just call "iterations" method and
>>>> check "iteratingShouldStop" instead?
>>>>
>>>> Thanks.
>>>>
>>>>
>>>> Sincerely,
>>>>
>>>> DB Tsai
>>>> -------------------------------------------------------
>>>> My Blog: https://www.dbtsai.com
>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>
>>>>
>>>> On Fri, Apr 25, 2014 at 3:10 PM, David Hall <dlwh@cs.berkeley.edu>wrote:
>>>>
>>>>> LBFGS will not take a step that sends the objective value up. It might
>>>>> try a step that is "too big" and reject it, so if you're just logging
>>>>> everything that gets tried by LBFGS, you could see that. The "iterations"
>>>>> method of the minimizer should never return an increasing objective value.
>>>>> If you're regularizing, are you including the regularizer in the objective
>>>>> value computation?
>>>>>
>>>>> GD is almost never worth your time.
>>>>>
>>>>> -- David
>>>>>
>>>>> On Fri, Apr 25, 2014 at 2:57 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>>>
>>>>>> Another interesting benchmark.
>>>>>>
>>>>>> *News20 dataset - 0.14M row, 1,355,191 features, 0.034% non-zero
>>>>>> elements.*
>>>>>>
>>>>>> LBFGS converges in 70 seconds, while GD seems to be not progressing.
>>>>>>
>>>>>> Dense feature vector will be too big to fit in the memory, so only
>>>>>> conduct the sparse benchmark.
>>>>>>
>>>>>> I saw the sometimes the loss bumps up, and it's weird for me. Since
>>>>>> the cost function of logistic regression is convex, it should be
>>>>>> monotonically decreasing.  David, any suggestion?
>>>>>>
>>>>>> The detail figure:
>>>>>>
>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/news20.pdf
>>>>>>
>>>>>>
>>>>>> *Rcv1 dataset - 6.8M row, 677,399 features, 0.15% non-zero elements.*
>>>>>>
>>>>>> LBFGS converges in 25 seconds, while GD also seems to be not
>>>>>> progressing.
>>>>>>
>>>>>> Only conduct sparse benchmark for the same reason. I also saw the
>>>>>> loss bumps up for unknown reason.
>>>>>>
>>>>>> The detail figure:
>>>>>>
>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/rcv1.pdf
>>>>>>
>>>>>>
>>>>>> Sincerely,
>>>>>>
>>>>>> DB Tsai
>>>>>> -------------------------------------------------------
>>>>>> My Blog: https://www.dbtsai.com
>>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>
>>>>>>
>>>>>> On Thu, Apr 24, 2014 at 2:36 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>>>>
>>>>>>> rcv1.binary is too sparse (0.15% non-zero elements), so dense
>>>>>>> format will not run due to out of memory. But sparse format runs really
>>>>>>> well.
>>>>>>>
>>>>>>>
>>>>>>> Sincerely,
>>>>>>>
>>>>>>> DB Tsai
>>>>>>> -------------------------------------------------------
>>>>>>> My Blog: https://www.dbtsai.com
>>>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>
>>>>>>>
>>>>>>> On Thu, Apr 24, 2014 at 1:54 PM, DB Tsai <dbtsai@stanford.edu>wrote:
>>>>>>>
>>>>>>>> I'm doing the timer in runMiniBatchSGD after  val numExamples =
>>>>>>>> data.count()
>>>>>>>>
>>>>>>>> See the following. Running rcv1 dataset now, and will update soon.
>>>>>>>>
>>>>>>>>     val startTime = System.nanoTime()
>>>>>>>>     for (i <- 1 to numIterations) {
>>>>>>>>       // Sample a subset (fraction miniBatchFraction) of the total
>>>>>>>> data
>>>>>>>>       // compute and sum up the subgradients on this subset (this
>>>>>>>> is one map-reduce)
>>>>>>>>       val (gradientSum, lossSum) = data.sample(false,
>>>>>>>> miniBatchFraction, 42 + i)
>>>>>>>>         .aggregate((BDV.zeros[Double](weights.size), 0.0))(
>>>>>>>>           seqOp = (c, v) => (c, v) match { case ((grad, loss),
>>>>>>>> (label, features)) =>
>>>>>>>>             val l = gradient.compute(features, label, weights,
>>>>>>>> Vectors.fromBreeze(grad))
>>>>>>>>             (grad, loss + l)
>>>>>>>>           },
>>>>>>>>           combOp = (c1, c2) => (c1, c2) match { case ((grad1,
>>>>>>>> loss1), (grad2, loss2)) =>
>>>>>>>>             (grad1 += grad2, loss1 + loss2)
>>>>>>>>           })
>>>>>>>>
>>>>>>>>       /**
>>>>>>>>        * NOTE(Xinghao): lossSum is computed using the weights from
>>>>>>>> the previous iteration
>>>>>>>>        * and regVal is the regularization value computed in the
>>>>>>>> previous iteration as well.
>>>>>>>>        */
>>>>>>>>       stochasticLossHistory.append(lossSum / miniBatchSize + regVal)
>>>>>>>>       val update = updater.compute(
>>>>>>>>         weights, Vectors.fromBreeze(gradientSum / miniBatchSize),
>>>>>>>> stepSize, i, regParam)
>>>>>>>>       weights = update._1
>>>>>>>>       regVal = update._2
>>>>>>>>       timeStamp.append(System.nanoTime() - startTime)
>>>>>>>>     }
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> Sincerely,
>>>>>>>>
>>>>>>>> DB Tsai
>>>>>>>> -------------------------------------------------------
>>>>>>>> My Blog: https://www.dbtsai.com
>>>>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>
>>>>>>>>
>>>>>>>> On Thu, Apr 24, 2014 at 1:44 PM, Xiangrui Meng <mengxr@gmail.com>wrote:
>>>>>>>>
>>>>>>>>> I don't understand why sparse falls behind dense so much at the
>>>>>>>>> very
>>>>>>>>> first iteration. I didn't see count() is called in
>>>>>>>>>
>>>>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/blob/master/src/main/scala/org/apache/spark/mllib/benchmark/BinaryLogisticRegression.scala
>>>>>>>>> . Maybe you have local uncommitted changes.
>>>>>>>>>
>>>>>>>>> Best,
>>>>>>>>> Xiangrui
>>>>>>>>>
>>>>>>>>> On Thu, Apr 24, 2014 at 11:26 AM, DB Tsai <dbtsai@stanford.edu>
>>>>>>>>> wrote:
>>>>>>>>> > Hi Xiangrui,
>>>>>>>>> >
>>>>>>>>> > Yes, I'm using yarn-cluster mode, and I did check # of executors
>>>>>>>>> I specified
>>>>>>>>> > are the same as the actual running executors.
>>>>>>>>> >
>>>>>>>>> > For caching and materialization, I've the timer in optimizer
>>>>>>>>> after calling
>>>>>>>>> > count(); as a result, the time for materialization in cache
>>>>>>>>> isn't in the
>>>>>>>>> > benchmark.
>>>>>>>>> >
>>>>>>>>> > The difference you saw is actually from dense feature or sparse
>>>>>>>>> feature
>>>>>>>>> > vector. For LBFGS and GD dense feature, you can see the first
>>>>>>>>> iteration
>>>>>>>>> > takes the same time. It's true for GD.
>>>>>>>>> >
>>>>>>>>> > I'm going to run rcv1.binary which only has 0.15% non-zero
>>>>>>>>> elements to
>>>>>>>>> > verify the hypothesis.
>>>>>>>>> >
>>>>>>>>> >
>>>>>>>>> > Sincerely,
>>>>>>>>> >
>>>>>>>>> > DB Tsai
>>>>>>>>> > -------------------------------------------------------
>>>>>>>>> > My Blog: https://www.dbtsai.com
>>>>>>>>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>> >
>>>>>>>>> >
>>>>>>>>> > On Thu, Apr 24, 2014 at 1:09 AM, Xiangrui Meng <mengxr@gmail.com>
>>>>>>>>> wrote:
>>>>>>>>> >>
>>>>>>>>> >> Hi DB,
>>>>>>>>> >>
>>>>>>>>> >> I saw you are using yarn-cluster mode for the benchmark. I
>>>>>>>>> tested the
>>>>>>>>> >> yarn-cluster mode and found that YARN does not always give you
>>>>>>>>> the
>>>>>>>>> >> exact number of executors requested. Just want to confirm that
>>>>>>>>> you've
>>>>>>>>> >> checked the number of executors.
>>>>>>>>> >>
>>>>>>>>> >> The second thing to check is that in the benchmark code, after
>>>>>>>>> you
>>>>>>>>> >> call cache, you should also call count() to materialize the
>>>>>>>>> RDD. I saw
>>>>>>>>> >> in the result, the real difference is actually at the first
>>>>>>>>> step.
>>>>>>>>> >> Adding intercept is not a cheap operation for sparse vectors.
>>>>>>>>> >>
>>>>>>>>> >> Best,
>>>>>>>>> >> Xiangrui
>>>>>>>>> >>
>>>>>>>>> >> On Thu, Apr 24, 2014 at 12:53 AM, Xiangrui Meng <
>>>>>>>>> mengxr@gmail.com> wrote:
>>>>>>>>> >> > I don't think it is easy to make sparse faster than dense
>>>>>>>>> with this
>>>>>>>>> >> > sparsity and feature dimension. You can try rcv1.binary,
>>>>>>>>> which should
>>>>>>>>> >> > show the difference easily.
>>>>>>>>> >> >
>>>>>>>>> >> > David, the breeze operators used here are
>>>>>>>>> >> >
>>>>>>>>> >> > 1. DenseVector dot SparseVector
>>>>>>>>> >> > 2. axpy DenseVector SparseVector
>>>>>>>>> >> >
>>>>>>>>> >> > However, the SparseVector is passed in as Vector[Double]
>>>>>>>>> instead of
>>>>>>>>> >> > SparseVector[Double]. It might use the axpy impl of
>>>>>>>>> [DenseVector,
>>>>>>>>> >> > Vector] and call activeIterator. I didn't check whether you
>>>>>>>>> used
>>>>>>>>> >> > multimethods on axpy.
>>>>>>>>> >> >
>>>>>>>>> >> > Best,
>>>>>>>>> >> > Xiangrui
>>>>>>>>> >> >
>>>>>>>>> >> > On Wed, Apr 23, 2014 at 10:35 PM, DB Tsai <
>>>>>>>>> dbtsai@stanford.edu> wrote:
>>>>>>>>> >> >> The figure showing the Log-Likelihood vs Time can be found
>>>>>>>>> here.
>>>>>>>>> >> >>
>>>>>>>>> >> >>
>>>>>>>>> >> >>
>>>>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
>>>>>>>>> >> >>
>>>>>>>>> >> >> Let me know if you can not open it. Thanks.
>>>>>>>>> >> >>
>>>>>>>>> >> >> Sincerely,
>>>>>>>>> >> >>
>>>>>>>>> >> >> DB Tsai
>>>>>>>>> >> >> -------------------------------------------------------
>>>>>>>>> >> >> My Blog: https://www.dbtsai.com
>>>>>>>>> >> >> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>> >> >>
>>>>>>>>> >> >>
>>>>>>>>> >> >> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman
>>>>>>>>> >> >> <shivaram@eecs.berkeley.edu> wrote:
>>>>>>>>> >> >>> I don't think the attachment came through in the list.
>>>>>>>>> Could you
>>>>>>>>> >> >>> upload the
>>>>>>>>> >> >>> results somewhere and link to them ?
>>>>>>>>> >> >>>
>>>>>>>>> >> >>>
>>>>>>>>> >> >>> On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com>
>>>>>>>>> wrote:
>>>>>>>>> >> >>>>
>>>>>>>>> >> >>>> 123 features per rows, and in average, 89% are zeros.
>>>>>>>>> >> >>>> On Apr 23, 2014 9:31 PM, "Evan Sparks" <
>>>>>>>>> evan.sparks@gmail.com> wrote:
>>>>>>>>> >> >>>>
>>>>>>>>> >> >>>> > What is the number of non zeroes per row (and number of
>>>>>>>>> features)
>>>>>>>>> >> >>>> > in the
>>>>>>>>> >> >>>> > sparse case? We've hit some issues with breeze sparse
>>>>>>>>> support in
>>>>>>>>> >> >>>> > the
>>>>>>>>> >> >>>> > past
>>>>>>>>> >> >>>> > but for sufficiently sparse data it's still pretty good.
>>>>>>>>> >> >>>> >
>>>>>>>>> >> >>>> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <
>>>>>>>>> dbtsai@stanford.edu> wrote:
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > Hi all,
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > I'm benchmarking Logistic Regression in MLlib using
>>>>>>>>> the newly
>>>>>>>>> >> >>>> > > added
>>>>>>>>> >> >>>> > optimizer LBFGS and GD. I'm using the same dataset and
>>>>>>>>> the same
>>>>>>>>> >> >>>> > methodology
>>>>>>>>> >> >>>> > in this paper,
>>>>>>>>> http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > I want to know how Spark scale while adding workers,
>>>>>>>>> and how
>>>>>>>>> >> >>>> > > optimizers
>>>>>>>>> >> >>>> > and input format (sparse or dense) impact performance.
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > The benchmark code can be found here,
>>>>>>>>> >> >>>> > https://github.com/dbtsai/spark-lbfgs-benchmark
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > The first dataset I benchmarked is a9a which only has
>>>>>>>>> 2.2MB. I
>>>>>>>>> >> >>>> > duplicated the dataset, and made it 762MB to have 11M
>>>>>>>>> rows. This
>>>>>>>>> >> >>>> > dataset
>>>>>>>>> >> >>>> > has 123 features and 11% of the data are non-zero
>>>>>>>>> elements.
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > In this benchmark, all the dataset is cached in memory.
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > As we expect, LBFGS converges faster than GD, and at
>>>>>>>>> some point,
>>>>>>>>> >> >>>> > > no
>>>>>>>>> >> >>>> > matter how we push GD, it will converge slower and
>>>>>>>>> slower.
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > However, it's surprising that sparse format runs
>>>>>>>>> slower than
>>>>>>>>> >> >>>> > > dense
>>>>>>>>> >> >>>> > format. I did see that sparse format takes significantly
>>>>>>>>> smaller
>>>>>>>>> >> >>>> > amount
>>>>>>>>> >> >>>> > of
>>>>>>>>> >> >>>> > memory in caching RDD, but sparse is 40% slower than
>>>>>>>>> dense. I think
>>>>>>>>> >> >>>> > sparse
>>>>>>>>> >> >>>> > should be fast since when we compute x wT, since x is
>>>>>>>>> sparse, we
>>>>>>>>> >> >>>> > can do
>>>>>>>>> >> >>>> > it
>>>>>>>>> >> >>>> > faster. I wonder if there is anything I'm doing wrong.
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > The attachment is the benchmark result.
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > Thanks.
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > Sincerely,
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > DB Tsai
>>>>>>>>> >> >>>> > > -------------------------------------------------------
>>>>>>>>> >> >>>> > > My Blog: https://www.dbtsai.com
>>>>>>>>> >> >>>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>> >> >>>> >
>>>>>>>>> >> >>>
>>>>>>>>> >> >>>
>>>>>>>>> >
>>>>>>>>> >
>>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>

--20cf303a2d618e270704f8211bd8--

From dev-return-7446-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 29 08:06:14 2014
Return-Path: <dev-return-7446-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 75F9110019
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Apr 2014 08:06:14 +0000 (UTC)
Received: (qmail 20871 invoked by uid 500); 29 Apr 2014 08:06:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20482 invoked by uid 500); 29 Apr 2014 08:06:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20458 invoked by uid 99); 29 Apr 2014 08:06:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Apr 2014 08:06:04 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.170 as permitted sender)
Received: from [209.85.214.170] (HELO mail-ob0-f170.google.com) (209.85.214.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Apr 2014 08:05:59 +0000
Received: by mail-ob0-f170.google.com with SMTP id vb8so8518251obc.15
        for <dev@spark.apache.org>; Tue, 29 Apr 2014 01:05:38 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=eHD30iG3Szd10QasPrAYmnXz08+ywcdUQxxcZoo/cEc=;
        b=ltfbhVvZbI3OxBz/FI4B15IrxZOX4J6J0wV97eo5Li+2zlCj2aMfXhMFyrye4+6Nay
         Z+FXpkWFvp/SLcixDvzza2tFI2QRrlwCNInY+NE5LRz3WmkU2OV1s/WcXXz3cAKXBQKc
         DfDLv64kn3loFqMC+T+NrQfImL7wtPdLFnuSMSF48LNDPRW3xO0i19YwA0DQ/+SHKy+p
         KQ2QUB6SvKtOSEo5n3wliDJhS67noS7LLP+cVkT5gfRQ8oTpqtjWsW9JaF4i+VBbxyfS
         /HfoYQuMsdry8YLHWEi8YtLxsD5f1LSm4bNP71qC0f07dCG8jldC6L5KWaT7NzVEhFDs
         X68Q==
MIME-Version: 1.0
X-Received: by 10.60.65.1 with SMTP id t1mr26700834oes.7.1398758738775; Tue,
 29 Apr 2014 01:05:38 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Tue, 29 Apr 2014 01:05:38 -0700 (PDT)
Date: Tue, 29 Apr 2014 01:05:38 -0700
Message-ID: <CABPQxstL6nwTO2H9p8=GJh1g2zxOJd02Wt7L06mCLjo-vwwG9Q@mail.gmail.com>
Subject: Spark 1.0.0 rc3
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c257a4c0581404f829e599
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c257a4c0581404f829e599
Content-Type: text/plain; charset=ISO-8859-1

Hey All,

This is not an official vote, but I wanted to cut an RC so that people can
test against the Maven artifacts, test building with their configuration,
etc. We are still chasing down a few issues and updating docs, etc.

If you have issues or bug reports for this release, please send an e-mail
to the Spark dev list and/or file a JIRA.

Commit: d636772 (v1.0.0-rc3)
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d636772ea9f98e449a038567b7975b1a07de3221

Binaries:
http://people.apache.org/~pwendell/spark-1.0.0-rc3/

Docs:
http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/

Repository:
https://repository.apache.org/content/repositories/orgapachespark-1012/

== API Changes ==
If you want to test building against Spark there are some minor API
changes. We'll get these written up for the final release but I'm noting a
few here (not comprehensive):

changes to ML vector specification:
http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/mllib-guide.html#from-09-to-10

changes to the Java API:
http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark

coGroup and related functions now return Iterable[T] instead of Seq[T]
==> Call toSeq on the result to restore the old behavior

SparkContext.jarOfClass returns Option[String] instead of Seq[String]
==> Call toSeq on the result to restore old behavior

Streaming classes have been renamed:
NetworkReceiver -> Receiver

--001a11c257a4c0581404f829e599--

From dev-return-7447-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 29 16:44:31 2014
Return-Path: <dev-return-7447-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2960510464
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Apr 2014 16:44:31 +0000 (UTC)
Received: (qmail 13407 invoked by uid 500); 29 Apr 2014 16:44:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13185 invoked by uid 500); 29 Apr 2014 16:44:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12955 invoked by uid 99); 29 Apr 2014 16:44:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Apr 2014 16:44:00 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_QUOTING
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of deanwampler@gmail.com designates 209.85.213.52 as permitted sender)
Received: from [209.85.213.52] (HELO mail-yh0-f52.google.com) (209.85.213.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Apr 2014 16:43:56 +0000
Received: by mail-yh0-f52.google.com with SMTP id a41so260752yho.11
        for <dev@spark.apache.org>; Tue, 29 Apr 2014 09:43:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=BP60SLpc4touEYVug0x8PclGNYev9EcXu0XJsQhvm5Q=;
        b=Xlr5TMPLecrs9dk/ebLIEisjMA2uggVgcEM5pTUT5tGq8q7NT6yLznvzWiF9Of/XPg
         Y5TTDNiEXcO+mqHhDxU0GavDcAslRI7dzn5rDHfzrbUIM2p0D+YGuxISxWgDXYASoWjX
         C6jK03kT971ja3yQNS1zX9dAF84MCz+RvXmhfO/37qvskLd7HqaFIumdGtMIDZ4DCcTV
         NM3V8SYJBnykV6hDOAynrfj27Epmr0olo81Cbjd3ExvMWa/62BcB/uPMPYr2pGhl5tEu
         abhV03uVVKrggsbMNm2mUzb61Ef/PWtAIi6w9+crO5bxPFkUbCkmCX97KwWLNqFhLAQi
         4SbA==
X-Received: by 10.236.7.47 with SMTP id 35mr48862619yho.23.1398789813801; Tue,
 29 Apr 2014 09:43:33 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.170.173.216 with HTTP; Tue, 29 Apr 2014 09:43:13 -0700 (PDT)
In-Reply-To: <CABPQxstL6nwTO2H9p8=GJh1g2zxOJd02Wt7L06mCLjo-vwwG9Q@mail.gmail.com>
References: <CABPQxstL6nwTO2H9p8=GJh1g2zxOJd02Wt7L06mCLjo-vwwG9Q@mail.gmail.com>
From: Dean Wampler <deanwampler@gmail.com>
Date: Tue, 29 Apr 2014 11:43:13 -0500
Message-ID: <CAKW0i0yEHeMQL7ve0=kV=U7DBLYXDY9+aeydohnpwu9Ybesu1Q@mail.gmail.com>
Subject: Re: Spark 1.0.0 rc3
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134036af79a4d04f83121dc
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134036af79a4d04f83121dc
Content-Type: text/plain; charset=UTF-8

I'm observing one anomalous behavior. With the 1.0.0 libraries, it's using
HDFS classes for file I/O, while the same script compiled and running with
0.9.1 uses only the local-mode File IO.

The script is a variation of the Word Count script. Here are the "guts":

object WordCount2 {
  def main(args: Array[String]) = {

    val sc = new SparkContext("local", "Word Count (2)")

    val input = sc.textFile(".../some/local/file").map(line =>
line.toLowerCase)
    input.cache

    val wc2 = input
      .flatMap(line => line.split("""\W+"""))
      .map(word => (word, 1))
      .reduceByKey((count1, count2) => count1 + count2)

    wc2.saveAsTextFile("output/some/directory")

    sc.stop()

It works fine compiled and executed with 0.9.1. If I recompile and run with
1.0.0-RC1, where the same output directory still exists, I get this
familiar Hadoop-ish exception:

[error] (run-main-0) org.apache.hadoop.mapred.FileAlreadyExistsException:
Output directory
file:/Users/deanwampler/projects/typesafe/activator/activator-spark/output/kjv-wc
already exists
org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory
file:/Users/deanwampler/projects/typesafe/activator/activator-spark/output/kjv-wc
already exists
 at
org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:121)
at
org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:749)
 at
org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:662)
at
org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:581)
 at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1057)
at spark.activator.WordCount2$.main(WordCount2.scala:42)
 at spark.activator.WordCount2.main(WordCount2.scala)
...

Thoughts?


On Tue, Apr 29, 2014 at 3:05 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> Hey All,
>
> This is not an official vote, but I wanted to cut an RC so that people can
> test against the Maven artifacts, test building with their configuration,
> etc. We are still chasing down a few issues and updating docs, etc.
>
> If you have issues or bug reports for this release, please send an e-mail
> to the Spark dev list and/or file a JIRA.
>
> Commit: d636772 (v1.0.0-rc3)
>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d636772ea9f98e449a038567b7975b1a07de3221
>
> Binaries:
> http://people.apache.org/~pwendell/spark-1.0.0-rc3/
>
> Docs:
> http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/
>
> Repository:
> https://repository.apache.org/content/repositories/orgapachespark-1012/
>
> == API Changes ==
> If you want to test building against Spark there are some minor API
> changes. We'll get these written up for the final release but I'm noting a
> few here (not comprehensive):
>
> changes to ML vector specification:
>
> http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/mllib-guide.html#from-09-to-10
>
> changes to the Java API:
>
> http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>
> coGroup and related functions now return Iterable[T] instead of Seq[T]
> ==> Call toSeq on the result to restore the old behavior
>
> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> ==> Call toSeq on the result to restore old behavior
>
> Streaming classes have been renamed:
> NetworkReceiver -> Receiver
>



-- 
Dean Wampler, Ph.D.
Typesafe
@deanwampler
http://typesafe.com
http://polyglotprogramming.com

--001a1134036af79a4d04f83121dc--

From dev-return-7448-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 29 17:11:06 2014
Return-Path: <dev-return-7448-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2CDA91057A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Apr 2014 17:11:06 +0000 (UTC)
Received: (qmail 65363 invoked by uid 500); 29 Apr 2014 17:11:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65296 invoked by uid 500); 29 Apr 2014 17:11:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65288 invoked by uid 99); 29 Apr 2014 17:11:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Apr 2014 17:11:04 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Apr 2014 17:10:59 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 7339E1012B3
	for <dev@spark.apache.org>; Tue, 29 Apr 2014 10:10:35 -0700 (PDT)
Received: from mail-qg0-f53.google.com (mail-qg0-f53.google.com [209.85.192.53])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 93CCE1013AD
	for <dev@spark.apache.org>; Tue, 29 Apr 2014 10:10:34 -0700 (PDT)
Received: by mail-qg0-f53.google.com with SMTP id i50so564364qgf.12
        for <dev@spark.apache.org>; Tue, 29 Apr 2014 10:10:33 -0700 (PDT)
X-Gm-Message-State: ALoCoQnrMsf+0Dy4p+iS7JIlEE4OIxarMbp8c7DSUg6UOnRVHRnV6GGvVIWa/WBjNZtWy1jX/ryQ
MIME-Version: 1.0
X-Received: by 10.140.32.139 with SMTP id h11mr657240qgh.49.1398791433734;
 Tue, 29 Apr 2014 10:10:33 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Tue, 29 Apr 2014 10:10:33 -0700 (PDT)
Date: Tue, 29 Apr 2014 10:10:33 -0700
Message-ID: <CAEYYnxbmqnUD4hDRC79Dc2r6AfeiER5vZQXfw8ugR=3ozFR-Ng@mail.gmail.com>
Subject: Code Review for SPARK-1516: Throw exception in yarn client instead of System.exit
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113b599085fc4b04f831825f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113b599085fc4b04f831825f
Content-Type: text/plain; charset=UTF-8

Hi All,

Since we're launching Spark Yarn Job in our tomcat application, the default
behavior of calling System.exit when job is finished or runs into any error
isn't desirable.

We create this PR https://github.com/apache/spark/pull/490 to address this
issue. Since the logical is fairly straightforward, we wonder if this can
be reviewed and have this in 1.0.

Thanks.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai

--001a113b599085fc4b04f831825f--

From dev-return-7449-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 29 18:24:11 2014
Return-Path: <dev-return-7449-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4401610913
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Apr 2014 18:24:11 +0000 (UTC)
Received: (qmail 56676 invoked by uid 500); 29 Apr 2014 18:24:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 56633 invoked by uid 500); 29 Apr 2014 18:24:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56625 invoked by uid 99); 29 Apr 2014 18:24:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Apr 2014 18:24:10 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of vanzin@cloudera.com designates 209.85.216.48 as permitted sender)
Received: from [209.85.216.48] (HELO mail-qa0-f48.google.com) (209.85.216.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Apr 2014 18:24:04 +0000
Received: by mail-qa0-f48.google.com with SMTP id j15so583788qaq.35
        for <dev@spark.apache.org>; Tue, 29 Apr 2014 11:23:44 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=kCqKvRWjA0l35q+61G6ezzXWgr3NDq/VHICyGgfdWK4=;
        b=AuT+/5+VvhtMLh9XQEG8ZVWnxJGciPKCh7mVPv175HmHvZePzU/Ng0g/XANpgoU09S
         vEoEV6LYVoLCwaR/86RI4kbsqZKkQYzRvksUQowcjAfHR2rdp24+Dma7y2nNLfbwSWAF
         2xgEORDOH415o1dlaaortbQawJhJ9YZ23DWCnIIKbIk02FNF6CFLnKF1TPAmirs4Nh3O
         DAN5nx1cIZ9+IsJ8GMwFdRnfEokR/zWZRmvuNh2PgidC3qReEY0DWjJWNYYuk7DiIHqv
         amv91z9XKfWzj9VszekhrJAr9IyIM2QgKwWul2JrxMfPLdKGWoMFtaLDUvWv7BuAne6P
         WwRg==
X-Gm-Message-State: ALoCoQlK4XzPlsXamEXu3l237v49SJ9e5PyzDb2c++bIntbOwynfDP+5lfCHgGS+h2i+z3u0p4r3
MIME-Version: 1.0
X-Received: by 10.140.26.243 with SMTP id 106mr1033903qgv.91.1398795823982;
 Tue, 29 Apr 2014 11:23:43 -0700 (PDT)
Received: by 10.229.211.5 with HTTP; Tue, 29 Apr 2014 11:23:43 -0700 (PDT)
In-Reply-To: <CABPQxstL6nwTO2H9p8=GJh1g2zxOJd02Wt7L06mCLjo-vwwG9Q@mail.gmail.com>
References: <CABPQxstL6nwTO2H9p8=GJh1g2zxOJd02Wt7L06mCLjo-vwwG9Q@mail.gmail.com>
Date: Tue, 29 Apr 2014 11:23:43 -0700
Message-ID: <CAAOnQ7s26S7MOXuM81Lx7yQT4bz+5pCNrc_2-OfU6WRm+vMAcA@mail.gmail.com>
Subject: Re: Spark 1.0.0 rc3
From: Marcelo Vanzin <vanzin@cloudera.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Patrick,

What are the expectations / guarantees on binary compatibility between
0.9 and 1.0?

You mention some API changes, which kinda hint that binary
compatibility has already been broken, but just wanted to point out
there are other cases. e.g.:

Exception in thread "main" java.lang.reflect.InvocationTargetException
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:236)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:47)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.NoSuchMethodError:
org.apache.spark.SparkContext$.rddToOrderedRDDFunctions(Lorg/apache/spark/rdd/RDD;Lscala/Function1;Lscala/reflect/ClassTag;Lscala/reflect/ClassTag;)Lorg/apache/spark/rdd/OrderedRDDFunctions;

(Compiled against 0.9, run against 1.0.)
Offending code:

      val top10 = counts.sortByKey(false).take(10)

Recompiling fixes the problem.


On Tue, Apr 29, 2014 at 1:05 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> Hey All,
>
> This is not an official vote, but I wanted to cut an RC so that people can
> test against the Maven artifacts, test building with their configuration,
> etc. We are still chasing down a few issues and updating docs, etc.
>
> If you have issues or bug reports for this release, please send an e-mail
> to the Spark dev list and/or file a JIRA.
>
> Commit: d636772 (v1.0.0-rc3)
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d636772ea9f98e449a038567b7975b1a07de3221
>
> Binaries:
> http://people.apache.org/~pwendell/spark-1.0.0-rc3/
>
> Docs:
> http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/
>
> Repository:
> https://repository.apache.org/content/repositories/orgapachespark-1012/
>
> == API Changes ==
> If you want to test building against Spark there are some minor API
> changes. We'll get these written up for the final release but I'm noting a
> few here (not comprehensive):
>
> changes to ML vector specification:
> http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/mllib-guide.html#from-09-to-10
>
> changes to the Java API:
> http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>
> coGroup and related functions now return Iterable[T] instead of Seq[T]
> ==> Call toSeq on the result to restore the old behavior
>
> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> ==> Call toSeq on the result to restore old behavior
>
> Streaming classes have been renamed:
> NetworkReceiver -> Receiver



-- 
Marcelo

From dev-return-7450-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 29 18:47:49 2014
Return-Path: <dev-return-7450-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E99B510A26
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Apr 2014 18:47:49 +0000 (UTC)
Received: (qmail 14796 invoked by uid 500); 29 Apr 2014 18:47:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14750 invoked by uid 500); 29 Apr 2014 18:47:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14742 invoked by uid 99); 29 Apr 2014 18:47:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Apr 2014 18:47:46 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.179 as permitted sender)
Received: from [209.85.214.179] (HELO mail-ob0-f179.google.com) (209.85.214.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Apr 2014 18:47:41 +0000
Received: by mail-ob0-f179.google.com with SMTP id vb8so738679obc.38
        for <dev@spark.apache.org>; Tue, 29 Apr 2014 11:47:18 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=kyWhSwObxhmsKEPiwa6N4fl92P5ySZSAtiDMH8RwjiI=;
        b=t0JCa1mt8Ss5QWGB+p5Hm/Za2OFkVetK8mBe3mWWXv4IRmH9dtjwOZCUITx3ByVgeL
         +Uy4sA9GBOB2uEzaT0Zv6StBxJlr4TUThx5eJjrLbwa0a1a962eNRTQLXjSKoVHCZCpk
         xP5Y3q6y5YJc9l4Lccr+RAfrhwppdHPkrL2FtuIhNh2oniMWjFLay1d2Gnkih0qH0DYI
         ad7XYUsttkXkemui6BEyiRio4clki9KRIiUEzHBcmt/A1FDwjV2ZY+qge4mJPC9K3Scc
         RycWhxnlLEsi7I2z7T97wYQ0CvFdlK6hDEgtcU197/qdaEI8KMTU3RDmE6sX5q9Y5Dw0
         fVwQ==
MIME-Version: 1.0
X-Received: by 10.60.92.170 with SMTP id cn10mr983696oeb.76.1398797238675;
 Tue, 29 Apr 2014 11:47:18 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Tue, 29 Apr 2014 11:47:18 -0700 (PDT)
In-Reply-To: <CAAOnQ7s26S7MOXuM81Lx7yQT4bz+5pCNrc_2-OfU6WRm+vMAcA@mail.gmail.com>
References: <CABPQxstL6nwTO2H9p8=GJh1g2zxOJd02Wt7L06mCLjo-vwwG9Q@mail.gmail.com>
	<CAAOnQ7s26S7MOXuM81Lx7yQT4bz+5pCNrc_2-OfU6WRm+vMAcA@mail.gmail.com>
Date: Tue, 29 Apr 2014 11:47:18 -0700
Message-ID: <CABPQxsusF5T6OLb=8Bah2Dtbq+iBbbLz6-65i9owSBbm3NA50A@mail.gmail.com>
Subject: Re: Spark 1.0.0 rc3
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

> What are the expectations / guarantees on binary compatibility between
> 0.9 and 1.0?

There are not guarantees.

From dev-return-7451-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 29 18:49:11 2014
Return-Path: <dev-return-7451-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6E16A10A34
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Apr 2014 18:49:11 +0000 (UTC)
Received: (qmail 18203 invoked by uid 500); 29 Apr 2014 18:49:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18111 invoked by uid 500); 29 Apr 2014 18:49:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18102 invoked by uid 99); 29 Apr 2014 18:49:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Apr 2014 18:49:09 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.46 as permitted sender)
Received: from [209.85.219.46] (HELO mail-oa0-f46.google.com) (209.85.219.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Apr 2014 18:49:04 +0000
Received: by mail-oa0-f46.google.com with SMTP id i4so26074oah.19
        for <dev@spark.apache.org>; Tue, 29 Apr 2014 11:48:43 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=dq1ZlbHAmj+G5lc36ApEIiHQgf5HNCiszXBWT0x63Xk=;
        b=AvzxfGa07GkgtQIyMvZcrRgnQ2GqVXlEfS/ubN45xh3IPVCPi2I5PdEv4JOPxwDd9B
         H2qz6NgqffdDqKp4rv0k4Cc93gWnkESWk0oeSJroGNWqDiX7OTW1kgjYjRAqIlFByesm
         aKcK06aKwQdaP/Ys5vv5zI4k5SqVEZSjR9Dh8R5JrXR0FHssu/yxdwaGQjFZCpqTX6S9
         RjoRAvr8w3c3T3dB7R/jOE7LH558frVZkiWvC0krXN/6zqTls+WHICs22sUJ09ZCInUz
         VU5JJ2oop19t1OXtdcUgZdDLtE6un/Tgt4aeyEmyKAsRdemVBKoKC9JHzg0zs65XzmEF
         12Hg==
MIME-Version: 1.0
X-Received: by 10.182.225.163 with SMTP id rl3mr2753137obc.79.1398797323690;
 Tue, 29 Apr 2014 11:48:43 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Tue, 29 Apr 2014 11:48:43 -0700 (PDT)
In-Reply-To: <CABPQxsusF5T6OLb=8Bah2Dtbq+iBbbLz6-65i9owSBbm3NA50A@mail.gmail.com>
References: <CABPQxstL6nwTO2H9p8=GJh1g2zxOJd02Wt7L06mCLjo-vwwG9Q@mail.gmail.com>
	<CAAOnQ7s26S7MOXuM81Lx7yQT4bz+5pCNrc_2-OfU6WRm+vMAcA@mail.gmail.com>
	<CABPQxsusF5T6OLb=8Bah2Dtbq+iBbbLz6-65i9owSBbm3NA50A@mail.gmail.com>
Date: Tue, 29 Apr 2014 11:48:43 -0700
Message-ID: <CABPQxsszaMASiwRWPS1u2OiwQxUz+nzx8BGh0KLSXkG6XtqCYQ@mail.gmail.com>
Subject: Re: Spark 1.0.0 rc3
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Sorry got cut off. For 0.9.0 and 1.0.0 they are not binary compatible
and in a few cases not source compatible. 1.X will be source
compatible. We are also planning to support binary compatibility in
1.X but I'm waiting util we make a few releases to officially promise
that, since Scala makes this pretty tricky.

On Tue, Apr 29, 2014 at 11:47 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>> What are the expectations / guarantees on binary compatibility between
>> 0.9 and 1.0?
>
> There are not guarantees.

From dev-return-7452-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 29 18:55:00 2014
Return-Path: <dev-return-7452-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2929B10A6E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Apr 2014 18:55:00 +0000 (UTC)
Received: (qmail 32847 invoked by uid 500); 29 Apr 2014 18:54:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32803 invoked by uid 500); 29 Apr 2014 18:54:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32792 invoked by uid 99); 29 Apr 2014 18:54:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Apr 2014 18:54:58 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_QUOTING
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.43 as permitted sender)
Received: from [209.85.219.43] (HELO mail-oa0-f43.google.com) (209.85.219.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Apr 2014 18:54:54 +0000
Received: by mail-oa0-f43.google.com with SMTP id eb12so747233oac.16
        for <dev@spark.apache.org>; Tue, 29 Apr 2014 11:54:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=c0EQBrd1cSkau2NeIZswKdG8IyutmjOwSMXhVx7n3sw=;
        b=DK4Nj5o/qSAsnKai/CgtGwi54jGVROTwicOu+fD0FsqrynPVsgXgarYyoNies4O0+A
         vDUpumSHfKdxVz3//zIqUs3qK2/4a+6PueQOrpQQ0+AkXREs1VGhCyLNMjKyeG0sV41h
         myyjVOYJ6Lh9VnzENjAkOz8O2jtnPfZeMshv4QFKKIX99GumqhaMZdOSHO6TfUDQlxJo
         gsMUOdIdhp/ttbHvj5V7GqLRusxT87XCLGrsT/TvJzMHn6stHyeN+bYyiCRVD6OgEZuv
         nYkfsY0zZCoynGriZTVL/eF9klrl/IzSFEWiz5DX+JyffnIqHnVPv3Mif2xXiB4vKDRs
         jcOQ==
MIME-Version: 1.0
X-Received: by 10.60.159.36 with SMTP id wz4mr1126095oeb.30.1398797671571;
 Tue, 29 Apr 2014 11:54:31 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Tue, 29 Apr 2014 11:54:31 -0700 (PDT)
In-Reply-To: <CAKW0i0yEHeMQL7ve0=kV=U7DBLYXDY9+aeydohnpwu9Ybesu1Q@mail.gmail.com>
References: <CABPQxstL6nwTO2H9p8=GJh1g2zxOJd02Wt7L06mCLjo-vwwG9Q@mail.gmail.com>
	<CAKW0i0yEHeMQL7ve0=kV=U7DBLYXDY9+aeydohnpwu9Ybesu1Q@mail.gmail.com>
Date: Tue, 29 Apr 2014 11:54:31 -0700
Message-ID: <CABPQxsudkXB02HObvgK1=bHgDYjwVMsxQ79zQ++35N-nvR+1Bg@mail.gmail.com>
Subject: Re: Spark 1.0.0 rc3
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Dean,

We always used the Hadoop libraries here to read and write local
files. In Spark 1.0 we started enforcing the rule that you can't
over-write an existing directory because it can cause
confusing/undefined behavior if multiple jobs output to the directory
(they partially clobber each other's output).

https://issues.apache.org/jira/browse/SPARK-1100
https://github.com/apache/spark/pull/11

In the JIRA I actually proposed slightly deviating from Hadoop
semantics and allowing the directory to exist if it is empty, but I
think in the end we decided to just go with the exact same semantics
as Hadoop (i.e. empty directories are a problem).

- Patrick

On Tue, Apr 29, 2014 at 9:43 AM, Dean Wampler <deanwampler@gmail.com> wrote:
> I'm observing one anomalous behavior. With the 1.0.0 libraries, it's using
> HDFS classes for file I/O, while the same script compiled and running with
> 0.9.1 uses only the local-mode File IO.
>
> The script is a variation of the Word Count script. Here are the "guts":
>
> object WordCount2 {
>   def main(args: Array[String]) = {
>
>     val sc = new SparkContext("local", "Word Count (2)")
>
>     val input = sc.textFile(".../some/local/file").map(line =>
> line.toLowerCase)
>     input.cache
>
>     val wc2 = input
>       .flatMap(line => line.split("""\W+"""))
>       .map(word => (word, 1))
>       .reduceByKey((count1, count2) => count1 + count2)
>
>     wc2.saveAsTextFile("output/some/directory")
>
>     sc.stop()
>
> It works fine compiled and executed with 0.9.1. If I recompile and run with
> 1.0.0-RC1, where the same output directory still exists, I get this
> familiar Hadoop-ish exception:
>
> [error] (run-main-0) org.apache.hadoop.mapred.FileAlreadyExistsException:
> Output directory
> file:/Users/deanwampler/projects/typesafe/activator/activator-spark/output/kjv-wc
> already exists
> org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory
> file:/Users/deanwampler/projects/typesafe/activator/activator-spark/output/kjv-wc
> already exists
>  at
> org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:121)
> at
> org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:749)
>  at
> org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:662)
> at
> org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:581)
>  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1057)
> at spark.activator.WordCount2$.main(WordCount2.scala:42)
>  at spark.activator.WordCount2.main(WordCount2.scala)
> ...
>
> Thoughts?
>
>
> On Tue, Apr 29, 2014 at 3:05 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>
>> Hey All,
>>
>> This is not an official vote, but I wanted to cut an RC so that people can
>> test against the Maven artifacts, test building with their configuration,
>> etc. We are still chasing down a few issues and updating docs, etc.
>>
>> If you have issues or bug reports for this release, please send an e-mail
>> to the Spark dev list and/or file a JIRA.
>>
>> Commit: d636772 (v1.0.0-rc3)
>>
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d636772ea9f98e449a038567b7975b1a07de3221
>>
>> Binaries:
>> http://people.apache.org/~pwendell/spark-1.0.0-rc3/
>>
>> Docs:
>> http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/
>>
>> Repository:
>> https://repository.apache.org/content/repositories/orgapachespark-1012/
>>
>> == API Changes ==
>> If you want to test building against Spark there are some minor API
>> changes. We'll get these written up for the final release but I'm noting a
>> few here (not comprehensive):
>>
>> changes to ML vector specification:
>>
>> http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/mllib-guide.html#from-09-to-10
>>
>> changes to the Java API:
>>
>> http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>>
>> coGroup and related functions now return Iterable[T] instead of Seq[T]
>> ==> Call toSeq on the result to restore the old behavior
>>
>> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
>> ==> Call toSeq on the result to restore old behavior
>>
>> Streaming classes have been renamed:
>> NetworkReceiver -> Receiver
>>
>
>
>
> --
> Dean Wampler, Ph.D.
> Typesafe
> @deanwampler
> http://typesafe.com
> http://polyglotprogramming.com

From dev-return-7453-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 29 21:20:54 2014
Return-Path: <dev-return-7453-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E7A15101DA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Apr 2014 21:20:54 +0000 (UTC)
Received: (qmail 10446 invoked by uid 500); 29 Apr 2014 21:20:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10339 invoked by uid 500); 29 Apr 2014 21:20:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10330 invoked by uid 99); 29 Apr 2014 21:20:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Apr 2014 21:20:53 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_QUOTING
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of deanwampler@gmail.com designates 209.85.160.177 as permitted sender)
Received: from [209.85.160.177] (HELO mail-yk0-f177.google.com) (209.85.160.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Apr 2014 21:20:49 +0000
Received: by mail-yk0-f177.google.com with SMTP id 9so732167ykp.22
        for <dev@spark.apache.org>; Tue, 29 Apr 2014 14:20:28 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=QK221eBemCXtmwC3apKJM4kyvQIrRiGwATVP9MNKsv0=;
        b=JKGhHkkVASiGjRKi4iT1LfSYissRI/hU6x2JUQ78rrYIf59kyKgQjhvfcEjEMESZSV
         0gIh+tpWpRV+krgNKFLQz1E6JUKQMcK8x/4lUYNMhGcznyAm2gNJkFOsNR78cCe41v+B
         jqgetfdWNThJd7L2lNI0OKBvJ6d5yfUcjqwg+jhCDwCAJ3LG4pp7q9riTDRcs+re8bjY
         q7QvaQvBMJWBUjOhHlnl6PAUOAWPUcj+ih9zq1oJYECDsv7Q2kWE6VAB508nHb0fA2eT
         Qi0RZr7TcdLvirYcRlsTKB/aZrXtvME4Q4YZAfIkj+xtMfbupGA7+QVJCl5C/1WFoiHw
         oeFA==
X-Received: by 10.236.14.2 with SMTP id c2mr273541yhc.73.1398806428676; Tue,
 29 Apr 2014 14:20:28 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.170.173.216 with HTTP; Tue, 29 Apr 2014 14:20:08 -0700 (PDT)
In-Reply-To: <CABPQxsudkXB02HObvgK1=bHgDYjwVMsxQ79zQ++35N-nvR+1Bg@mail.gmail.com>
References: <CABPQxstL6nwTO2H9p8=GJh1g2zxOJd02Wt7L06mCLjo-vwwG9Q@mail.gmail.com>
 <CAKW0i0yEHeMQL7ve0=kV=U7DBLYXDY9+aeydohnpwu9Ybesu1Q@mail.gmail.com> <CABPQxsudkXB02HObvgK1=bHgDYjwVMsxQ79zQ++35N-nvR+1Bg@mail.gmail.com>
From: Dean Wampler <deanwampler@gmail.com>
Date: Tue, 29 Apr 2014 16:20:08 -0500
Message-ID: <CAKW0i0yLR0xxE-CFcH+uRjyRxdi=0aK4GFLHbXjj3GBP-CzbkA@mail.gmail.com>
Subject: Re: Spark 1.0.0 rc3
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0139fc844a77eb04f83500ad
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0139fc844a77eb04f83500ad
Content-Type: text/plain; charset=UTF-8

Thanks. I'm fine with the logic change, although I was a bit surprised to
see Hadoop used for file I/O.

Anyway, the jira issue and pull request discussions mention a flag to
enable overwrites. That would be very convenient for a tutorial I'm
writing, although I wouldn't recommend it for normal use, of course.
However, I can't figure out if this actually exists. I found the
spark.files.overwrite property, but that doesn't apply.  Does this override
flag, method call, or method argument actually exist?

Thanks,
Dean


On Tue, Apr 29, 2014 at 1:54 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Hi Dean,
>
> We always used the Hadoop libraries here to read and write local
> files. In Spark 1.0 we started enforcing the rule that you can't
> over-write an existing directory because it can cause
> confusing/undefined behavior if multiple jobs output to the directory
> (they partially clobber each other's output).
>
> https://issues.apache.org/jira/browse/SPARK-1100
> https://github.com/apache/spark/pull/11
>
> In the JIRA I actually proposed slightly deviating from Hadoop
> semantics and allowing the directory to exist if it is empty, but I
> think in the end we decided to just go with the exact same semantics
> as Hadoop (i.e. empty directories are a problem).
>
> - Patrick
>
> On Tue, Apr 29, 2014 at 9:43 AM, Dean Wampler <deanwampler@gmail.com>
> wrote:
> > I'm observing one anomalous behavior. With the 1.0.0 libraries, it's
> using
> > HDFS classes for file I/O, while the same script compiled and running
> with
> > 0.9.1 uses only the local-mode File IO.
> >
> > The script is a variation of the Word Count script. Here are the "guts":
> >
> > object WordCount2 {
> >   def main(args: Array[String]) = {
> >
> >     val sc = new SparkContext("local", "Word Count (2)")
> >
> >     val input = sc.textFile(".../some/local/file").map(line =>
> > line.toLowerCase)
> >     input.cache
> >
> >     val wc2 = input
> >       .flatMap(line => line.split("""\W+"""))
> >       .map(word => (word, 1))
> >       .reduceByKey((count1, count2) => count1 + count2)
> >
> >     wc2.saveAsTextFile("output/some/directory")
> >
> >     sc.stop()
> >
> > It works fine compiled and executed with 0.9.1. If I recompile and run
> with
> > 1.0.0-RC1, where the same output directory still exists, I get this
> > familiar Hadoop-ish exception:
> >
> > [error] (run-main-0) org.apache.hadoop.mapred.FileAlreadyExistsException:
> > Output directory
> >
> file:/Users/deanwampler/projects/typesafe/activator/activator-spark/output/kjv-wc
> > already exists
> > org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory
> >
> file:/Users/deanwampler/projects/typesafe/activator/activator-spark/output/kjv-wc
> > already exists
> >  at
> >
> org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:121)
> > at
> >
> org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:749)
> >  at
> >
> org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:662)
> > at
> >
> org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:581)
> >  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1057)
> > at spark.activator.WordCount2$.main(WordCount2.scala:42)
> >  at spark.activator.WordCount2.main(WordCount2.scala)
> > ...
> >
> > Thoughts?
> >
> >
> > On Tue, Apr 29, 2014 at 3:05 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >
> >> Hey All,
> >>
> >> This is not an official vote, but I wanted to cut an RC so that people
> can
> >> test against the Maven artifacts, test building with their
> configuration,
> >> etc. We are still chasing down a few issues and updating docs, etc.
> >>
> >> If you have issues or bug reports for this release, please send an
> e-mail
> >> to the Spark dev list and/or file a JIRA.
> >>
> >> Commit: d636772 (v1.0.0-rc3)
> >>
> >>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d636772ea9f98e449a038567b7975b1a07de3221
> >>
> >> Binaries:
> >> http://people.apache.org/~pwendell/spark-1.0.0-rc3/
> >>
> >> Docs:
> >> http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/
> >>
> >> Repository:
> >> https://repository.apache.org/content/repositories/orgapachespark-1012/
> >>
> >> == API Changes ==
> >> If you want to test building against Spark there are some minor API
> >> changes. We'll get these written up for the final release but I'm
> noting a
> >> few here (not comprehensive):
> >>
> >> changes to ML vector specification:
> >>
> >>
> http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/mllib-guide.html#from-09-to-10
> >>
> >> changes to the Java API:
> >>
> >>
> http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
> >>
> >> coGroup and related functions now return Iterable[T] instead of Seq[T]
> >> ==> Call toSeq on the result to restore the old behavior
> >>
> >> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> >> ==> Call toSeq on the result to restore old behavior
> >>
> >> Streaming classes have been renamed:
> >> NetworkReceiver -> Receiver
> >>
> >
> >
> >
> > --
> > Dean Wampler, Ph.D.
> > Typesafe
> > @deanwampler
> > http://typesafe.com
> > http://polyglotprogramming.com
>



-- 
Dean Wampler, Ph.D.
Typesafe
@deanwampler
http://typesafe.com
http://polyglotprogramming.com

--089e0139fc844a77eb04f83500ad--

From dev-return-7454-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Apr 29 22:31:27 2014
Return-Path: <dev-return-7454-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 12F16105BC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Apr 2014 22:31:27 +0000 (UTC)
Received: (qmail 51686 invoked by uid 500); 29 Apr 2014 22:31:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51622 invoked by uid 500); 29 Apr 2014 22:31:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51614 invoked by uid 99); 29 Apr 2014 22:31:24 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Apr 2014 22:31:24 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Apr 2014 22:31:19 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 408FB100F10
	for <dev@spark.apache.org>; Tue, 29 Apr 2014 15:30:56 -0700 (PDT)
Received: from mail-qg0-f42.google.com (mail-qg0-f42.google.com [209.85.192.42])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 902571017D2
	for <dev@spark.apache.org>; Tue, 29 Apr 2014 15:30:54 -0700 (PDT)
Received: by mail-qg0-f42.google.com with SMTP id z60so306912qgd.15
        for <dev@spark.apache.org>; Tue, 29 Apr 2014 15:30:53 -0700 (PDT)
X-Gm-Message-State: ALoCoQkIcjrbTQLoaV/mEoqsL+QgVzk9HNSE+OxyzrZgzLkK5R9kE5BKGMDnUzg5/1xUHaH/SukS
MIME-Version: 1.0
X-Received: by 10.140.32.139 with SMTP id h11mr569582qgh.49.1398810653600;
 Tue, 29 Apr 2014 15:30:53 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Tue, 29 Apr 2014 15:30:53 -0700 (PDT)
In-Reply-To: <CALW2ey312i1iRBH9Tg_zSb1jd_hGDoXtNuy-MLC5fEGZ=Vo+Xg@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxZFmxXkrNhdUNF3je7k0Eg_YjY2WnvVW5SSk__FAWNz4w@mail.gmail.com>
	<CAJgQjQ-k5P3CxVFCE-pHtuH9fVwwCcPc6PBp1NgxM2uueOASsw@mail.gmail.com>
	<CAJgQjQ_ePvspakEfiHzxdDH30EeEzjxsz0y_G0n51VophVL1hg@mail.gmail.com>
	<CAEYYnxZb3GbjdVVWyF0F4c_sEikadefP88rnWuGtHrx9HwMBJQ@mail.gmail.com>
	<CAJgQjQ8s7dP-rUtiiSUV2imeL9r++_WEJj1RyWCy+b3QBEcxRw@mail.gmail.com>
	<CAEYYnxaSh532XmO7224BiM5D2g_Q0+k49AaFbuT6LxDCmSUZBw@mail.gmail.com>
	<CAEYYnxaKTW-74AmKSAAg_kQkAyqePjRNhn9rS2VVbt8wp_4CsA@mail.gmail.com>
	<CAEYYnxZNLk+-Hh8o7V88GBR+pTZxE0qm+mw4YxFeun7grsPEng@mail.gmail.com>
	<CALW2ey0PqNry2qK_SRMs6C=S3tZ_3KoGYHeKPHG0G8EuQ6vXag@mail.gmail.com>
	<CAEYYnxYEOHFEp7obAWmVq=SMXxpLp3hJavyM0NAzwQ7SsN+ixA@mail.gmail.com>
	<CAEYYnxYx1s+C9Qe_hHbXAvyYbeGCikMaXDHjqtcQf9tUG9cfbw@mail.gmail.com>
	<CAEYYnxb4QLneUjfj5dG80Pzae8MhEQG090YQ2G4oq7VccayM-g@mail.gmail.com>
	<CALW2ey312i1iRBH9Tg_zSb1jd_hGDoXtNuy-MLC5fEGZ=Vo+Xg@mail.gmail.com>
Date: Tue, 29 Apr 2014 15:30:53 -0700
Message-ID: <CAEYYnxb-q-MS2OUzEf5Q2fYLXVu7XKO0Kkj2uYWWHxfCWCATCw@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: DB Tsai <dbtsai@stanford.edu>
To: David Hall <dlwh@cs.berkeley.edu>
Cc: Xiangrui Meng <mengxr@gmail.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113b59901deebd04f835fcef
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113b59901deebd04f835fcef
Content-Type: text/plain; charset=UTF-8

Have a quick hack to understand the behavior of SLBFGS
(Stochastic-LBFGS) by overwriting the breeze iterations method to get the
current LBFGS step to ensure that the objective function is the same during
the line search step. David, the following is my code, have a better way to
inject into it?

https://github.com/dbtsai/spark/tree/dbtsai-lbfgshack

Couple findings,

1) miniBatch (using rdd sample api) for each iteration is slower than full
data training when the full data is cached. Probably because sample is not
efficiency in Spark.

2) Since in the line search steps, we use the same sample of data (the same
objective function), the SLBFGS actually converges well.

3) For news20 dataset, with 0.05 miniBatch size, it takes 14 SLBFGS steps
(29 data iterations, 74.5seconds) to converge to loss < 0.10. For LBFGS
with full data training, it takes 9 LBFGS steps (12 data iterations, 37.6
seconds) to converge to loss < 0.10.

It seems that as long as the noisy gradient happens in different SLBFGS
steps, it still works.

(ps, I also tried in line search step, I use different sample of data, and
it just doesn't work as we expect.)



Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Mon, Apr 28, 2014 at 8:55 AM, David Hall <dlwh@cs.berkeley.edu> wrote:

> That's right.
>
> FWIW, caching should be automatic now, but it might be the version of
> Breeze you're using doesn't do that yet.
>
> Also, In breeze.util._ there's an implicit that adds a tee method to
> iterator, and also a last method. Both are useful for things like this.
>
> -- David
>
>
> On Sun, Apr 27, 2014 at 11:53 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>
>> I think I figure it out. Instead of calling minimize, and record the loss
>> in the DiffFunction, I should do the following.
>>
>> val states = lbfgs.iterations(new CachedDiffFunction(costFun),
>> initialWeights.toBreeze.toDenseVector)
>> states.foreach(state => lossHistory.append(state.value))
>>
>> All the losses in states should be decreasing now. Am I right?
>>
>>
>>
>> Sincerely,
>>
>> DB Tsai
>> -------------------------------------------------------
>> My Blog: https://www.dbtsai.com
>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>
>>
>> On Sun, Apr 27, 2014 at 11:31 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>
>>> Also, how many failure of rejection will terminate the optimization
>>> process? How is it related to "numberOfImprovementFailures"?
>>>
>>> Thanks.
>>>
>>>
>>> Sincerely,
>>>
>>> DB Tsai
>>> -------------------------------------------------------
>>> My Blog: https://www.dbtsai.com
>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>
>>>
>>> On Sun, Apr 27, 2014 at 11:28 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>
>>>> Hi David,
>>>>
>>>> I'm recording the loss history in the DiffFunction implementation, and
>>>> that's why the rejected step is also recorded in my loss history.
>>>>
>>>> Is there any api in Breeze LBFGS to get the history which already
>>>> excludes the reject step? Or should I just call "iterations" method and
>>>> check "iteratingShouldStop" instead?
>>>>
>>>> Thanks.
>>>>
>>>>
>>>> Sincerely,
>>>>
>>>> DB Tsai
>>>> -------------------------------------------------------
>>>> My Blog: https://www.dbtsai.com
>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>
>>>>
>>>> On Fri, Apr 25, 2014 at 3:10 PM, David Hall <dlwh@cs.berkeley.edu>wrote:
>>>>
>>>>> LBFGS will not take a step that sends the objective value up. It might
>>>>> try a step that is "too big" and reject it, so if you're just logging
>>>>> everything that gets tried by LBFGS, you could see that. The "iterations"
>>>>> method of the minimizer should never return an increasing objective value.
>>>>> If you're regularizing, are you including the regularizer in the objective
>>>>> value computation?
>>>>>
>>>>> GD is almost never worth your time.
>>>>>
>>>>> -- David
>>>>>
>>>>> On Fri, Apr 25, 2014 at 2:57 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>>>
>>>>>> Another interesting benchmark.
>>>>>>
>>>>>> *News20 dataset - 0.14M row, 1,355,191 features, 0.034% non-zero
>>>>>> elements.*
>>>>>>
>>>>>> LBFGS converges in 70 seconds, while GD seems to be not progressing.
>>>>>>
>>>>>> Dense feature vector will be too big to fit in the memory, so only
>>>>>> conduct the sparse benchmark.
>>>>>>
>>>>>> I saw the sometimes the loss bumps up, and it's weird for me. Since
>>>>>> the cost function of logistic regression is convex, it should be
>>>>>> monotonically decreasing.  David, any suggestion?
>>>>>>
>>>>>> The detail figure:
>>>>>>
>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/news20.pdf
>>>>>>
>>>>>>
>>>>>> *Rcv1 dataset - 6.8M row, 677,399 features, 0.15% non-zero elements.*
>>>>>>
>>>>>> LBFGS converges in 25 seconds, while GD also seems to be not
>>>>>> progressing.
>>>>>>
>>>>>> Only conduct sparse benchmark for the same reason. I also saw the
>>>>>> loss bumps up for unknown reason.
>>>>>>
>>>>>> The detail figure:
>>>>>>
>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/rcv1.pdf
>>>>>>
>>>>>>
>>>>>> Sincerely,
>>>>>>
>>>>>> DB Tsai
>>>>>> -------------------------------------------------------
>>>>>> My Blog: https://www.dbtsai.com
>>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>
>>>>>>
>>>>>> On Thu, Apr 24, 2014 at 2:36 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>>>>
>>>>>>> rcv1.binary is too sparse (0.15% non-zero elements), so dense
>>>>>>> format will not run due to out of memory. But sparse format runs really
>>>>>>> well.
>>>>>>>
>>>>>>>
>>>>>>> Sincerely,
>>>>>>>
>>>>>>> DB Tsai
>>>>>>> -------------------------------------------------------
>>>>>>> My Blog: https://www.dbtsai.com
>>>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>
>>>>>>>
>>>>>>> On Thu, Apr 24, 2014 at 1:54 PM, DB Tsai <dbtsai@stanford.edu>wrote:
>>>>>>>
>>>>>>>> I'm doing the timer in runMiniBatchSGD after  val numExamples =
>>>>>>>> data.count()
>>>>>>>>
>>>>>>>> See the following. Running rcv1 dataset now, and will update soon.
>>>>>>>>
>>>>>>>>     val startTime = System.nanoTime()
>>>>>>>>     for (i <- 1 to numIterations) {
>>>>>>>>       // Sample a subset (fraction miniBatchFraction) of the total
>>>>>>>> data
>>>>>>>>       // compute and sum up the subgradients on this subset (this
>>>>>>>> is one map-reduce)
>>>>>>>>       val (gradientSum, lossSum) = data.sample(false,
>>>>>>>> miniBatchFraction, 42 + i)
>>>>>>>>         .aggregate((BDV.zeros[Double](weights.size), 0.0))(
>>>>>>>>           seqOp = (c, v) => (c, v) match { case ((grad, loss),
>>>>>>>> (label, features)) =>
>>>>>>>>             val l = gradient.compute(features, label, weights,
>>>>>>>> Vectors.fromBreeze(grad))
>>>>>>>>             (grad, loss + l)
>>>>>>>>           },
>>>>>>>>           combOp = (c1, c2) => (c1, c2) match { case ((grad1,
>>>>>>>> loss1), (grad2, loss2)) =>
>>>>>>>>             (grad1 += grad2, loss1 + loss2)
>>>>>>>>           })
>>>>>>>>
>>>>>>>>       /**
>>>>>>>>        * NOTE(Xinghao): lossSum is computed using the weights from
>>>>>>>> the previous iteration
>>>>>>>>        * and regVal is the regularization value computed in the
>>>>>>>> previous iteration as well.
>>>>>>>>        */
>>>>>>>>       stochasticLossHistory.append(lossSum / miniBatchSize + regVal)
>>>>>>>>       val update = updater.compute(
>>>>>>>>         weights, Vectors.fromBreeze(gradientSum / miniBatchSize),
>>>>>>>> stepSize, i, regParam)
>>>>>>>>       weights = update._1
>>>>>>>>       regVal = update._2
>>>>>>>>       timeStamp.append(System.nanoTime() - startTime)
>>>>>>>>     }
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>> Sincerely,
>>>>>>>>
>>>>>>>> DB Tsai
>>>>>>>> -------------------------------------------------------
>>>>>>>> My Blog: https://www.dbtsai.com
>>>>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>
>>>>>>>>
>>>>>>>> On Thu, Apr 24, 2014 at 1:44 PM, Xiangrui Meng <mengxr@gmail.com>wrote:
>>>>>>>>
>>>>>>>>> I don't understand why sparse falls behind dense so much at the
>>>>>>>>> very
>>>>>>>>> first iteration. I didn't see count() is called in
>>>>>>>>>
>>>>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/blob/master/src/main/scala/org/apache/spark/mllib/benchmark/BinaryLogisticRegression.scala
>>>>>>>>> . Maybe you have local uncommitted changes.
>>>>>>>>>
>>>>>>>>> Best,
>>>>>>>>> Xiangrui
>>>>>>>>>
>>>>>>>>> On Thu, Apr 24, 2014 at 11:26 AM, DB Tsai <dbtsai@stanford.edu>
>>>>>>>>> wrote:
>>>>>>>>> > Hi Xiangrui,
>>>>>>>>> >
>>>>>>>>> > Yes, I'm using yarn-cluster mode, and I did check # of executors
>>>>>>>>> I specified
>>>>>>>>> > are the same as the actual running executors.
>>>>>>>>> >
>>>>>>>>> > For caching and materialization, I've the timer in optimizer
>>>>>>>>> after calling
>>>>>>>>> > count(); as a result, the time for materialization in cache
>>>>>>>>> isn't in the
>>>>>>>>> > benchmark.
>>>>>>>>> >
>>>>>>>>> > The difference you saw is actually from dense feature or sparse
>>>>>>>>> feature
>>>>>>>>> > vector. For LBFGS and GD dense feature, you can see the first
>>>>>>>>> iteration
>>>>>>>>> > takes the same time. It's true for GD.
>>>>>>>>> >
>>>>>>>>> > I'm going to run rcv1.binary which only has 0.15% non-zero
>>>>>>>>> elements to
>>>>>>>>> > verify the hypothesis.
>>>>>>>>> >
>>>>>>>>> >
>>>>>>>>> > Sincerely,
>>>>>>>>> >
>>>>>>>>> > DB Tsai
>>>>>>>>> > -------------------------------------------------------
>>>>>>>>> > My Blog: https://www.dbtsai.com
>>>>>>>>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>> >
>>>>>>>>> >
>>>>>>>>> > On Thu, Apr 24, 2014 at 1:09 AM, Xiangrui Meng <mengxr@gmail.com>
>>>>>>>>> wrote:
>>>>>>>>> >>
>>>>>>>>> >> Hi DB,
>>>>>>>>> >>
>>>>>>>>> >> I saw you are using yarn-cluster mode for the benchmark. I
>>>>>>>>> tested the
>>>>>>>>> >> yarn-cluster mode and found that YARN does not always give you
>>>>>>>>> the
>>>>>>>>> >> exact number of executors requested. Just want to confirm that
>>>>>>>>> you've
>>>>>>>>> >> checked the number of executors.
>>>>>>>>> >>
>>>>>>>>> >> The second thing to check is that in the benchmark code, after
>>>>>>>>> you
>>>>>>>>> >> call cache, you should also call count() to materialize the
>>>>>>>>> RDD. I saw
>>>>>>>>> >> in the result, the real difference is actually at the first
>>>>>>>>> step.
>>>>>>>>> >> Adding intercept is not a cheap operation for sparse vectors.
>>>>>>>>> >>
>>>>>>>>> >> Best,
>>>>>>>>> >> Xiangrui
>>>>>>>>> >>
>>>>>>>>> >> On Thu, Apr 24, 2014 at 12:53 AM, Xiangrui Meng <
>>>>>>>>> mengxr@gmail.com> wrote:
>>>>>>>>> >> > I don't think it is easy to make sparse faster than dense
>>>>>>>>> with this
>>>>>>>>> >> > sparsity and feature dimension. You can try rcv1.binary,
>>>>>>>>> which should
>>>>>>>>> >> > show the difference easily.
>>>>>>>>> >> >
>>>>>>>>> >> > David, the breeze operators used here are
>>>>>>>>> >> >
>>>>>>>>> >> > 1. DenseVector dot SparseVector
>>>>>>>>> >> > 2. axpy DenseVector SparseVector
>>>>>>>>> >> >
>>>>>>>>> >> > However, the SparseVector is passed in as Vector[Double]
>>>>>>>>> instead of
>>>>>>>>> >> > SparseVector[Double]. It might use the axpy impl of
>>>>>>>>> [DenseVector,
>>>>>>>>> >> > Vector] and call activeIterator. I didn't check whether you
>>>>>>>>> used
>>>>>>>>> >> > multimethods on axpy.
>>>>>>>>> >> >
>>>>>>>>> >> > Best,
>>>>>>>>> >> > Xiangrui
>>>>>>>>> >> >
>>>>>>>>> >> > On Wed, Apr 23, 2014 at 10:35 PM, DB Tsai <
>>>>>>>>> dbtsai@stanford.edu> wrote:
>>>>>>>>> >> >> The figure showing the Log-Likelihood vs Time can be found
>>>>>>>>> here.
>>>>>>>>> >> >>
>>>>>>>>> >> >>
>>>>>>>>> >> >>
>>>>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
>>>>>>>>> >> >>
>>>>>>>>> >> >> Let me know if you can not open it. Thanks.
>>>>>>>>> >> >>
>>>>>>>>> >> >> Sincerely,
>>>>>>>>> >> >>
>>>>>>>>> >> >> DB Tsai
>>>>>>>>> >> >> -------------------------------------------------------
>>>>>>>>> >> >> My Blog: https://www.dbtsai.com
>>>>>>>>> >> >> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>> >> >>
>>>>>>>>> >> >>
>>>>>>>>> >> >> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman
>>>>>>>>> >> >> <shivaram@eecs.berkeley.edu> wrote:
>>>>>>>>> >> >>> I don't think the attachment came through in the list.
>>>>>>>>> Could you
>>>>>>>>> >> >>> upload the
>>>>>>>>> >> >>> results somewhere and link to them ?
>>>>>>>>> >> >>>
>>>>>>>>> >> >>>
>>>>>>>>> >> >>> On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <dbtsai@dbtsai.com>
>>>>>>>>> wrote:
>>>>>>>>> >> >>>>
>>>>>>>>> >> >>>> 123 features per rows, and in average, 89% are zeros.
>>>>>>>>> >> >>>> On Apr 23, 2014 9:31 PM, "Evan Sparks" <
>>>>>>>>> evan.sparks@gmail.com> wrote:
>>>>>>>>> >> >>>>
>>>>>>>>> >> >>>> > What is the number of non zeroes per row (and number of
>>>>>>>>> features)
>>>>>>>>> >> >>>> > in the
>>>>>>>>> >> >>>> > sparse case? We've hit some issues with breeze sparse
>>>>>>>>> support in
>>>>>>>>> >> >>>> > the
>>>>>>>>> >> >>>> > past
>>>>>>>>> >> >>>> > but for sufficiently sparse data it's still pretty good.
>>>>>>>>> >> >>>> >
>>>>>>>>> >> >>>> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <
>>>>>>>>> dbtsai@stanford.edu> wrote:
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > Hi all,
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > I'm benchmarking Logistic Regression in MLlib using
>>>>>>>>> the newly
>>>>>>>>> >> >>>> > > added
>>>>>>>>> >> >>>> > optimizer LBFGS and GD. I'm using the same dataset and
>>>>>>>>> the same
>>>>>>>>> >> >>>> > methodology
>>>>>>>>> >> >>>> > in this paper,
>>>>>>>>> http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > I want to know how Spark scale while adding workers,
>>>>>>>>> and how
>>>>>>>>> >> >>>> > > optimizers
>>>>>>>>> >> >>>> > and input format (sparse or dense) impact performance.
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > The benchmark code can be found here,
>>>>>>>>> >> >>>> > https://github.com/dbtsai/spark-lbfgs-benchmark
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > The first dataset I benchmarked is a9a which only has
>>>>>>>>> 2.2MB. I
>>>>>>>>> >> >>>> > duplicated the dataset, and made it 762MB to have 11M
>>>>>>>>> rows. This
>>>>>>>>> >> >>>> > dataset
>>>>>>>>> >> >>>> > has 123 features and 11% of the data are non-zero
>>>>>>>>> elements.
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > In this benchmark, all the dataset is cached in memory.
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > As we expect, LBFGS converges faster than GD, and at
>>>>>>>>> some point,
>>>>>>>>> >> >>>> > > no
>>>>>>>>> >> >>>> > matter how we push GD, it will converge slower and
>>>>>>>>> slower.
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > However, it's surprising that sparse format runs
>>>>>>>>> slower than
>>>>>>>>> >> >>>> > > dense
>>>>>>>>> >> >>>> > format. I did see that sparse format takes significantly
>>>>>>>>> smaller
>>>>>>>>> >> >>>> > amount
>>>>>>>>> >> >>>> > of
>>>>>>>>> >> >>>> > memory in caching RDD, but sparse is 40% slower than
>>>>>>>>> dense. I think
>>>>>>>>> >> >>>> > sparse
>>>>>>>>> >> >>>> > should be fast since when we compute x wT, since x is
>>>>>>>>> sparse, we
>>>>>>>>> >> >>>> > can do
>>>>>>>>> >> >>>> > it
>>>>>>>>> >> >>>> > faster. I wonder if there is anything I'm doing wrong.
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > The attachment is the benchmark result.
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > Thanks.
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > Sincerely,
>>>>>>>>> >> >>>> > >
>>>>>>>>> >> >>>> > > DB Tsai
>>>>>>>>> >> >>>> > > -------------------------------------------------------
>>>>>>>>> >> >>>> > > My Blog: https://www.dbtsai.com
>>>>>>>>> >> >>>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>> >> >>>> >
>>>>>>>>> >> >>>
>>>>>>>>> >> >>>
>>>>>>>>> >
>>>>>>>>> >
>>>>>>>>>
>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>

--001a113b59901deebd04f835fcef--

From dev-return-7455-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 30 04:51:28 2014
Return-Path: <dev-return-7455-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F19E4100AD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Apr 2014 04:51:27 +0000 (UTC)
Received: (qmail 44149 invoked by uid 500); 30 Apr 2014 04:51:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43265 invoked by uid 500); 30 Apr 2014 04:51:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42910 invoked by uid 99); 30 Apr 2014 04:51:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 04:51:16 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of david.lw.hall@gmail.com designates 209.85.212.171 as permitted sender)
Received: from [209.85.212.171] (HELO mail-wi0-f171.google.com) (209.85.212.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 04:51:13 +0000
Received: by mail-wi0-f171.google.com with SMTP id hm4so313611wib.4
        for <dev@spark.apache.org>; Tue, 29 Apr 2014 21:50:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:cc:content-type;
        bh=GvlHzMxZ2dluNLV2wyL0oOcLMnoN5jmFtDsrHys6beA=;
        b=Mz7xppw6SSzR38WxSi2GPOXOr2/gLZe+BsAESKGcZPhgqDw3oQ9086A2LjAqDTyi59
         lUC0XefdmaxudIydyc/aTi1vQ6yLi/OomaKGmuFoclvYwjT1g15sEt8T64ITdJ6RHJt3
         fhW0eATjKH/DJGg72ONzId7Wpwpk+90SzOdQrjf7NhO5c1L/dULk7oqfpezLAqJEDdOh
         qBTMLnDyRV5AxONfHgDKEVnz7sRlDDxDSTyaHOF4zn1UKNZ5z8ed+Aew8OsusSeqYpoZ
         jTDVYdYTR+tlun/hMRm4JErHX7gStPOir1wXGtH44lrI7bvOldHHOzQ+mRdDr35dkS31
         je8Q==
MIME-Version: 1.0
X-Received: by 10.180.12.206 with SMTP id a14mr1649943wic.48.1398833450626;
 Tue, 29 Apr 2014 21:50:50 -0700 (PDT)
Sender: david.lw.hall@gmail.com
Received: by 10.227.232.207 with HTTP; Tue, 29 Apr 2014 21:50:50 -0700 (PDT)
In-Reply-To: <CAEYYnxb-q-MS2OUzEf5Q2fYLXVu7XKO0Kkj2uYWWHxfCWCATCw@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxZFmxXkrNhdUNF3je7k0Eg_YjY2WnvVW5SSk__FAWNz4w@mail.gmail.com>
	<CAJgQjQ-k5P3CxVFCE-pHtuH9fVwwCcPc6PBp1NgxM2uueOASsw@mail.gmail.com>
	<CAJgQjQ_ePvspakEfiHzxdDH30EeEzjxsz0y_G0n51VophVL1hg@mail.gmail.com>
	<CAEYYnxZb3GbjdVVWyF0F4c_sEikadefP88rnWuGtHrx9HwMBJQ@mail.gmail.com>
	<CAJgQjQ8s7dP-rUtiiSUV2imeL9r++_WEJj1RyWCy+b3QBEcxRw@mail.gmail.com>
	<CAEYYnxaSh532XmO7224BiM5D2g_Q0+k49AaFbuT6LxDCmSUZBw@mail.gmail.com>
	<CAEYYnxaKTW-74AmKSAAg_kQkAyqePjRNhn9rS2VVbt8wp_4CsA@mail.gmail.com>
	<CAEYYnxZNLk+-Hh8o7V88GBR+pTZxE0qm+mw4YxFeun7grsPEng@mail.gmail.com>
	<CALW2ey0PqNry2qK_SRMs6C=S3tZ_3KoGYHeKPHG0G8EuQ6vXag@mail.gmail.com>
	<CAEYYnxYEOHFEp7obAWmVq=SMXxpLp3hJavyM0NAzwQ7SsN+ixA@mail.gmail.com>
	<CAEYYnxYx1s+C9Qe_hHbXAvyYbeGCikMaXDHjqtcQf9tUG9cfbw@mail.gmail.com>
	<CAEYYnxb4QLneUjfj5dG80Pzae8MhEQG090YQ2G4oq7VccayM-g@mail.gmail.com>
	<CALW2ey312i1iRBH9Tg_zSb1jd_hGDoXtNuy-MLC5fEGZ=Vo+Xg@mail.gmail.com>
	<CAEYYnxb-q-MS2OUzEf5Q2fYLXVu7XKO0Kkj2uYWWHxfCWCATCw@mail.gmail.com>
Date: Tue, 29 Apr 2014 21:50:50 -0700
X-Google-Sender-Auth: BS3udjWJEPQZCF-kH1mHix8FXvU
Message-ID: <CALW2ey26aub3W6siDkizrKctW9YjJMsdAPvGzE+jt=wBTvRnyw@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: David Hall <dlwh@cs.berkeley.edu>
To: DB Tsai <dbtsai@stanford.edu>
Cc: Xiangrui Meng <mengxr@gmail.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c24c9aecb37904f83b4a03
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c24c9aecb37904f83b4a03
Content-Type: text/plain; charset=UTF-8

Yeah, that's probably the easiest though obviously pretty hacky.

I'm surprised that the hessian approximation isn't worse than it is. (As
in, I'd expect error messages.) It's obviously line searching much more, so
the approximation must be worse. You might be interested in this online
bfgs:
http://jmlr.org/proceedings/papers/v2/schraudolph07a/schraudolph07a.pdf

-- David


On Tue, Apr 29, 2014 at 3:30 PM, DB Tsai <dbtsai@stanford.edu> wrote:

> Have a quick hack to understand the behavior of SLBFGS
> (Stochastic-LBFGS) by overwriting the breeze iterations method to get the
> current LBFGS step to ensure that the objective function is the same during
> the line search step. David, the following is my code, have a better way to
> inject into it?
>
> https://github.com/dbtsai/spark/tree/dbtsai-lbfgshack
>
> Couple findings,
>
> 1) miniBatch (using rdd sample api) for each iteration is slower than full
> data training when the full data is cached. Probably because sample is not
> efficiency in Spark.
>
> 2) Since in the line search steps, we use the same sample of data (the
> same objective function), the SLBFGS actually converges well.
>
> 3) For news20 dataset, with 0.05 miniBatch size, it takes 14 SLBFGS steps
> (29 data iterations, 74.5seconds) to converge to loss < 0.10. For LBFGS
> with full data training, it takes 9 LBFGS steps (12 data iterations, 37.6
> seconds) to converge to loss < 0.10.
>
> It seems that as long as the noisy gradient happens in different SLBFGS
> steps, it still works.
>
> (ps, I also tried in line search step, I use different sample of data, and
> it just doesn't work as we expect.)
>
>
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Mon, Apr 28, 2014 at 8:55 AM, David Hall <dlwh@cs.berkeley.edu> wrote:
>
>> That's right.
>>
>> FWIW, caching should be automatic now, but it might be the version of
>> Breeze you're using doesn't do that yet.
>>
>> Also, In breeze.util._ there's an implicit that adds a tee method to
>> iterator, and also a last method. Both are useful for things like this.
>>
>> -- David
>>
>>
>> On Sun, Apr 27, 2014 at 11:53 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>
>>> I think I figure it out. Instead of calling minimize, and record the
>>> loss in the DiffFunction, I should do the following.
>>>
>>> val states = lbfgs.iterations(new CachedDiffFunction(costFun),
>>> initialWeights.toBreeze.toDenseVector)
>>> states.foreach(state => lossHistory.append(state.value))
>>>
>>> All the losses in states should be decreasing now. Am I right?
>>>
>>>
>>>
>>> Sincerely,
>>>
>>> DB Tsai
>>> -------------------------------------------------------
>>> My Blog: https://www.dbtsai.com
>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>
>>>
>>> On Sun, Apr 27, 2014 at 11:31 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>
>>>> Also, how many failure of rejection will terminate the optimization
>>>> process? How is it related to "numberOfImprovementFailures"?
>>>>
>>>> Thanks.
>>>>
>>>>
>>>> Sincerely,
>>>>
>>>> DB Tsai
>>>> -------------------------------------------------------
>>>> My Blog: https://www.dbtsai.com
>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>
>>>>
>>>> On Sun, Apr 27, 2014 at 11:28 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>>
>>>>> Hi David,
>>>>>
>>>>> I'm recording the loss history in the DiffFunction implementation, and
>>>>> that's why the rejected step is also recorded in my loss history.
>>>>>
>>>>> Is there any api in Breeze LBFGS to get the history which already
>>>>> excludes the reject step? Or should I just call "iterations" method and
>>>>> check "iteratingShouldStop" instead?
>>>>>
>>>>> Thanks.
>>>>>
>>>>>
>>>>> Sincerely,
>>>>>
>>>>> DB Tsai
>>>>> -------------------------------------------------------
>>>>> My Blog: https://www.dbtsai.com
>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>
>>>>>
>>>>> On Fri, Apr 25, 2014 at 3:10 PM, David Hall <dlwh@cs.berkeley.edu>wrote:
>>>>>
>>>>>> LBFGS will not take a step that sends the objective value up. It
>>>>>> might try a step that is "too big" and reject it, so if you're just logging
>>>>>> everything that gets tried by LBFGS, you could see that. The "iterations"
>>>>>> method of the minimizer should never return an increasing objective value.
>>>>>> If you're regularizing, are you including the regularizer in the objective
>>>>>> value computation?
>>>>>>
>>>>>> GD is almost never worth your time.
>>>>>>
>>>>>> -- David
>>>>>>
>>>>>> On Fri, Apr 25, 2014 at 2:57 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>>>>
>>>>>>> Another interesting benchmark.
>>>>>>>
>>>>>>> *News20 dataset - 0.14M row, 1,355,191 features, 0.034% non-zero
>>>>>>> elements.*
>>>>>>>
>>>>>>> LBFGS converges in 70 seconds, while GD seems to be not progressing.
>>>>>>>
>>>>>>> Dense feature vector will be too big to fit in the memory, so only
>>>>>>> conduct the sparse benchmark.
>>>>>>>
>>>>>>> I saw the sometimes the loss bumps up, and it's weird for me. Since
>>>>>>> the cost function of logistic regression is convex, it should be
>>>>>>> monotonically decreasing.  David, any suggestion?
>>>>>>>
>>>>>>> The detail figure:
>>>>>>>
>>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/news20.pdf
>>>>>>>
>>>>>>>
>>>>>>> *Rcv1 dataset - 6.8M row, 677,399 features, 0.15% non-zero elements.*
>>>>>>>
>>>>>>> LBFGS converges in 25 seconds, while GD also seems to be not
>>>>>>> progressing.
>>>>>>>
>>>>>>> Only conduct sparse benchmark for the same reason. I also saw the
>>>>>>> loss bumps up for unknown reason.
>>>>>>>
>>>>>>> The detail figure:
>>>>>>>
>>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/rcv1.pdf
>>>>>>>
>>>>>>>
>>>>>>> Sincerely,
>>>>>>>
>>>>>>> DB Tsai
>>>>>>> -------------------------------------------------------
>>>>>>> My Blog: https://www.dbtsai.com
>>>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>
>>>>>>>
>>>>>>> On Thu, Apr 24, 2014 at 2:36 PM, DB Tsai <dbtsai@stanford.edu>wrote:
>>>>>>>
>>>>>>>> rcv1.binary is too sparse (0.15% non-zero elements), so dense
>>>>>>>> format will not run due to out of memory. But sparse format runs really
>>>>>>>> well.
>>>>>>>>
>>>>>>>>
>>>>>>>> Sincerely,
>>>>>>>>
>>>>>>>> DB Tsai
>>>>>>>> -------------------------------------------------------
>>>>>>>> My Blog: https://www.dbtsai.com
>>>>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>
>>>>>>>>
>>>>>>>> On Thu, Apr 24, 2014 at 1:54 PM, DB Tsai <dbtsai@stanford.edu>wrote:
>>>>>>>>
>>>>>>>>> I'm doing the timer in runMiniBatchSGD after  val numExamples =
>>>>>>>>> data.count()
>>>>>>>>>
>>>>>>>>> See the following. Running rcv1 dataset now, and will update soon.
>>>>>>>>>
>>>>>>>>>     val startTime = System.nanoTime()
>>>>>>>>>     for (i <- 1 to numIterations) {
>>>>>>>>>       // Sample a subset (fraction miniBatchFraction) of the total
>>>>>>>>> data
>>>>>>>>>       // compute and sum up the subgradients on this subset (this
>>>>>>>>> is one map-reduce)
>>>>>>>>>       val (gradientSum, lossSum) = data.sample(false,
>>>>>>>>> miniBatchFraction, 42 + i)
>>>>>>>>>         .aggregate((BDV.zeros[Double](weights.size), 0.0))(
>>>>>>>>>           seqOp = (c, v) => (c, v) match { case ((grad, loss),
>>>>>>>>> (label, features)) =>
>>>>>>>>>             val l = gradient.compute(features, label, weights,
>>>>>>>>> Vectors.fromBreeze(grad))
>>>>>>>>>             (grad, loss + l)
>>>>>>>>>           },
>>>>>>>>>           combOp = (c1, c2) => (c1, c2) match { case ((grad1,
>>>>>>>>> loss1), (grad2, loss2)) =>
>>>>>>>>>             (grad1 += grad2, loss1 + loss2)
>>>>>>>>>           })
>>>>>>>>>
>>>>>>>>>       /**
>>>>>>>>>        * NOTE(Xinghao): lossSum is computed using the weights from
>>>>>>>>> the previous iteration
>>>>>>>>>        * and regVal is the regularization value computed in the
>>>>>>>>> previous iteration as well.
>>>>>>>>>        */
>>>>>>>>>       stochasticLossHistory.append(lossSum / miniBatchSize +
>>>>>>>>> regVal)
>>>>>>>>>       val update = updater.compute(
>>>>>>>>>         weights, Vectors.fromBreeze(gradientSum / miniBatchSize),
>>>>>>>>> stepSize, i, regParam)
>>>>>>>>>       weights = update._1
>>>>>>>>>       regVal = update._2
>>>>>>>>>       timeStamp.append(System.nanoTime() - startTime)
>>>>>>>>>     }
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> Sincerely,
>>>>>>>>>
>>>>>>>>> DB Tsai
>>>>>>>>> -------------------------------------------------------
>>>>>>>>> My Blog: https://www.dbtsai.com
>>>>>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> On Thu, Apr 24, 2014 at 1:44 PM, Xiangrui Meng <mengxr@gmail.com>wrote:
>>>>>>>>>
>>>>>>>>>> I don't understand why sparse falls behind dense so much at the
>>>>>>>>>> very
>>>>>>>>>> first iteration. I didn't see count() is called in
>>>>>>>>>>
>>>>>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/blob/master/src/main/scala/org/apache/spark/mllib/benchmark/BinaryLogisticRegression.scala
>>>>>>>>>> . Maybe you have local uncommitted changes.
>>>>>>>>>>
>>>>>>>>>> Best,
>>>>>>>>>> Xiangrui
>>>>>>>>>>
>>>>>>>>>> On Thu, Apr 24, 2014 at 11:26 AM, DB Tsai <dbtsai@stanford.edu>
>>>>>>>>>> wrote:
>>>>>>>>>> > Hi Xiangrui,
>>>>>>>>>> >
>>>>>>>>>> > Yes, I'm using yarn-cluster mode, and I did check # of
>>>>>>>>>> executors I specified
>>>>>>>>>> > are the same as the actual running executors.
>>>>>>>>>> >
>>>>>>>>>> > For caching and materialization, I've the timer in optimizer
>>>>>>>>>> after calling
>>>>>>>>>> > count(); as a result, the time for materialization in cache
>>>>>>>>>> isn't in the
>>>>>>>>>> > benchmark.
>>>>>>>>>> >
>>>>>>>>>> > The difference you saw is actually from dense feature or sparse
>>>>>>>>>> feature
>>>>>>>>>> > vector. For LBFGS and GD dense feature, you can see the first
>>>>>>>>>> iteration
>>>>>>>>>> > takes the same time. It's true for GD.
>>>>>>>>>> >
>>>>>>>>>> > I'm going to run rcv1.binary which only has 0.15% non-zero
>>>>>>>>>> elements to
>>>>>>>>>> > verify the hypothesis.
>>>>>>>>>> >
>>>>>>>>>> >
>>>>>>>>>> > Sincerely,
>>>>>>>>>> >
>>>>>>>>>> > DB Tsai
>>>>>>>>>> > -------------------------------------------------------
>>>>>>>>>> > My Blog: https://www.dbtsai.com
>>>>>>>>>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>>> >
>>>>>>>>>> >
>>>>>>>>>> > On Thu, Apr 24, 2014 at 1:09 AM, Xiangrui Meng <
>>>>>>>>>> mengxr@gmail.com> wrote:
>>>>>>>>>> >>
>>>>>>>>>> >> Hi DB,
>>>>>>>>>> >>
>>>>>>>>>> >> I saw you are using yarn-cluster mode for the benchmark. I
>>>>>>>>>> tested the
>>>>>>>>>> >> yarn-cluster mode and found that YARN does not always give you
>>>>>>>>>> the
>>>>>>>>>> >> exact number of executors requested. Just want to confirm that
>>>>>>>>>> you've
>>>>>>>>>> >> checked the number of executors.
>>>>>>>>>> >>
>>>>>>>>>> >> The second thing to check is that in the benchmark code, after
>>>>>>>>>> you
>>>>>>>>>> >> call cache, you should also call count() to materialize the
>>>>>>>>>> RDD. I saw
>>>>>>>>>> >> in the result, the real difference is actually at the first
>>>>>>>>>> step.
>>>>>>>>>> >> Adding intercept is not a cheap operation for sparse vectors.
>>>>>>>>>> >>
>>>>>>>>>> >> Best,
>>>>>>>>>> >> Xiangrui
>>>>>>>>>> >>
>>>>>>>>>> >> On Thu, Apr 24, 2014 at 12:53 AM, Xiangrui Meng <
>>>>>>>>>> mengxr@gmail.com> wrote:
>>>>>>>>>> >> > I don't think it is easy to make sparse faster than dense
>>>>>>>>>> with this
>>>>>>>>>> >> > sparsity and feature dimension. You can try rcv1.binary,
>>>>>>>>>> which should
>>>>>>>>>> >> > show the difference easily.
>>>>>>>>>> >> >
>>>>>>>>>> >> > David, the breeze operators used here are
>>>>>>>>>> >> >
>>>>>>>>>> >> > 1. DenseVector dot SparseVector
>>>>>>>>>> >> > 2. axpy DenseVector SparseVector
>>>>>>>>>> >> >
>>>>>>>>>> >> > However, the SparseVector is passed in as Vector[Double]
>>>>>>>>>> instead of
>>>>>>>>>> >> > SparseVector[Double]. It might use the axpy impl of
>>>>>>>>>> [DenseVector,
>>>>>>>>>> >> > Vector] and call activeIterator. I didn't check whether you
>>>>>>>>>> used
>>>>>>>>>> >> > multimethods on axpy.
>>>>>>>>>> >> >
>>>>>>>>>> >> > Best,
>>>>>>>>>> >> > Xiangrui
>>>>>>>>>> >> >
>>>>>>>>>> >> > On Wed, Apr 23, 2014 at 10:35 PM, DB Tsai <
>>>>>>>>>> dbtsai@stanford.edu> wrote:
>>>>>>>>>> >> >> The figure showing the Log-Likelihood vs Time can be found
>>>>>>>>>> here.
>>>>>>>>>> >> >>
>>>>>>>>>> >> >>
>>>>>>>>>> >> >>
>>>>>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
>>>>>>>>>> >> >>
>>>>>>>>>> >> >> Let me know if you can not open it. Thanks.
>>>>>>>>>> >> >>
>>>>>>>>>> >> >> Sincerely,
>>>>>>>>>> >> >>
>>>>>>>>>> >> >> DB Tsai
>>>>>>>>>> >> >> -------------------------------------------------------
>>>>>>>>>> >> >> My Blog: https://www.dbtsai.com
>>>>>>>>>> >> >> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>>> >> >>
>>>>>>>>>> >> >>
>>>>>>>>>> >> >> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman
>>>>>>>>>> >> >> <shivaram@eecs.berkeley.edu> wrote:
>>>>>>>>>> >> >>> I don't think the attachment came through in the list.
>>>>>>>>>> Could you
>>>>>>>>>> >> >>> upload the
>>>>>>>>>> >> >>> results somewhere and link to them ?
>>>>>>>>>> >> >>>
>>>>>>>>>> >> >>>
>>>>>>>>>> >> >>> On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <
>>>>>>>>>> dbtsai@dbtsai.com> wrote:
>>>>>>>>>> >> >>>>
>>>>>>>>>> >> >>>> 123 features per rows, and in average, 89% are zeros.
>>>>>>>>>> >> >>>> On Apr 23, 2014 9:31 PM, "Evan Sparks" <
>>>>>>>>>> evan.sparks@gmail.com> wrote:
>>>>>>>>>> >> >>>>
>>>>>>>>>> >> >>>> > What is the number of non zeroes per row (and number of
>>>>>>>>>> features)
>>>>>>>>>> >> >>>> > in the
>>>>>>>>>> >> >>>> > sparse case? We've hit some issues with breeze sparse
>>>>>>>>>> support in
>>>>>>>>>> >> >>>> > the
>>>>>>>>>> >> >>>> > past
>>>>>>>>>> >> >>>> > but for sufficiently sparse data it's still pretty good.
>>>>>>>>>> >> >>>> >
>>>>>>>>>> >> >>>> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <
>>>>>>>>>> dbtsai@stanford.edu> wrote:
>>>>>>>>>> >> >>>> > >
>>>>>>>>>> >> >>>> > > Hi all,
>>>>>>>>>> >> >>>> > >
>>>>>>>>>> >> >>>> > > I'm benchmarking Logistic Regression in MLlib using
>>>>>>>>>> the newly
>>>>>>>>>> >> >>>> > > added
>>>>>>>>>> >> >>>> > optimizer LBFGS and GD. I'm using the same dataset and
>>>>>>>>>> the same
>>>>>>>>>> >> >>>> > methodology
>>>>>>>>>> >> >>>> > in this paper,
>>>>>>>>>> http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>>>>>>>>>> >> >>>> > >
>>>>>>>>>> >> >>>> > > I want to know how Spark scale while adding workers,
>>>>>>>>>> and how
>>>>>>>>>> >> >>>> > > optimizers
>>>>>>>>>> >> >>>> > and input format (sparse or dense) impact performance.
>>>>>>>>>> >> >>>> > >
>>>>>>>>>> >> >>>> > > The benchmark code can be found here,
>>>>>>>>>> >> >>>> > https://github.com/dbtsai/spark-lbfgs-benchmark
>>>>>>>>>> >> >>>> > >
>>>>>>>>>> >> >>>> > > The first dataset I benchmarked is a9a which only has
>>>>>>>>>> 2.2MB. I
>>>>>>>>>> >> >>>> > duplicated the dataset, and made it 762MB to have 11M
>>>>>>>>>> rows. This
>>>>>>>>>> >> >>>> > dataset
>>>>>>>>>> >> >>>> > has 123 features and 11% of the data are non-zero
>>>>>>>>>> elements.
>>>>>>>>>> >> >>>> > >
>>>>>>>>>> >> >>>> > > In this benchmark, all the dataset is cached in
>>>>>>>>>> memory.
>>>>>>>>>> >> >>>> > >
>>>>>>>>>> >> >>>> > > As we expect, LBFGS converges faster than GD, and at
>>>>>>>>>> some point,
>>>>>>>>>> >> >>>> > > no
>>>>>>>>>> >> >>>> > matter how we push GD, it will converge slower and
>>>>>>>>>> slower.
>>>>>>>>>> >> >>>> > >
>>>>>>>>>> >> >>>> > > However, it's surprising that sparse format runs
>>>>>>>>>> slower than
>>>>>>>>>> >> >>>> > > dense
>>>>>>>>>> >> >>>> > format. I did see that sparse format takes
>>>>>>>>>> significantly smaller
>>>>>>>>>> >> >>>> > amount
>>>>>>>>>> >> >>>> > of
>>>>>>>>>> >> >>>> > memory in caching RDD, but sparse is 40% slower than
>>>>>>>>>> dense. I think
>>>>>>>>>> >> >>>> > sparse
>>>>>>>>>> >> >>>> > should be fast since when we compute x wT, since x is
>>>>>>>>>> sparse, we
>>>>>>>>>> >> >>>> > can do
>>>>>>>>>> >> >>>> > it
>>>>>>>>>> >> >>>> > faster. I wonder if there is anything I'm doing wrong.
>>>>>>>>>> >> >>>> > >
>>>>>>>>>> >> >>>> > > The attachment is the benchmark result.
>>>>>>>>>> >> >>>> > >
>>>>>>>>>> >> >>>> > > Thanks.
>>>>>>>>>> >> >>>> > >
>>>>>>>>>> >> >>>> > > Sincerely,
>>>>>>>>>> >> >>>> > >
>>>>>>>>>> >> >>>> > > DB Tsai
>>>>>>>>>> >> >>>> > >
>>>>>>>>>> -------------------------------------------------------
>>>>>>>>>> >> >>>> > > My Blog: https://www.dbtsai.com
>>>>>>>>>> >> >>>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>>> >> >>>> >
>>>>>>>>>> >> >>>
>>>>>>>>>> >> >>>
>>>>>>>>>> >
>>>>>>>>>> >
>>>>>>>>>>
>>>>>>>>>
>>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>

--001a11c24c9aecb37904f83b4a03--

From dev-return-7456-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 30 05:18:47 2014
Return-Path: <dev-return-7456-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BA41F10177
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Apr 2014 05:18:47 +0000 (UTC)
Received: (qmail 85020 invoked by uid 500); 30 Apr 2014 05:18:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84592 invoked by uid 500); 30 Apr 2014 05:18:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84584 invoked by uid 99); 30 Apr 2014 05:18:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 05:18:44 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 05:18:40 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 8190C101AF7
	for <dev@spark.apache.org>; Tue, 29 Apr 2014 22:18:16 -0700 (PDT)
Received: from mail-qg0-f48.google.com (mail-qg0-f48.google.com [209.85.192.48])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 2BE7E101AF6
	for <dev@spark.apache.org>; Tue, 29 Apr 2014 22:18:15 -0700 (PDT)
Received: by mail-qg0-f48.google.com with SMTP id q108so1343415qgd.35
        for <dev@spark.apache.org>; Tue, 29 Apr 2014 22:18:14 -0700 (PDT)
X-Gm-Message-State: ALoCoQkO3i3bbEFnhg6aRAKdZM15CQZ7tDYI69rkpFUjaNJ1JTZcwwdfx0YmUTJiCYeWXuz23KSz
MIME-Version: 1.0
X-Received: by 10.224.15.137 with SMTP id k9mr1163453qaa.104.1398835094326;
 Tue, 29 Apr 2014 22:18:14 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Tue, 29 Apr 2014 22:18:14 -0700 (PDT)
In-Reply-To: <CALW2ey26aub3W6siDkizrKctW9YjJMsdAPvGzE+jt=wBTvRnyw@mail.gmail.com>
References: <CAEYYnxbj9uQW1iMvQZ9giHzbEXrdqNt6ea66UXhsGfivxxcFbw@mail.gmail.com>
	<FCF38381-637E-42F7-8732-17E1FC0BF51C@gmail.com>
	<CAEYYnxYT-yoziE1JKxC3DFJsZ2Pa=xw2LzetSiWFBpLC+u-=PQ@mail.gmail.com>
	<CAKx7Bf_9w0909zJN=KomXYL-4a_wT_OcA_GmzmHqZTDa69Nh4g@mail.gmail.com>
	<CAEYYnxZFmxXkrNhdUNF3je7k0Eg_YjY2WnvVW5SSk__FAWNz4w@mail.gmail.com>
	<CAJgQjQ-k5P3CxVFCE-pHtuH9fVwwCcPc6PBp1NgxM2uueOASsw@mail.gmail.com>
	<CAJgQjQ_ePvspakEfiHzxdDH30EeEzjxsz0y_G0n51VophVL1hg@mail.gmail.com>
	<CAEYYnxZb3GbjdVVWyF0F4c_sEikadefP88rnWuGtHrx9HwMBJQ@mail.gmail.com>
	<CAJgQjQ8s7dP-rUtiiSUV2imeL9r++_WEJj1RyWCy+b3QBEcxRw@mail.gmail.com>
	<CAEYYnxaSh532XmO7224BiM5D2g_Q0+k49AaFbuT6LxDCmSUZBw@mail.gmail.com>
	<CAEYYnxaKTW-74AmKSAAg_kQkAyqePjRNhn9rS2VVbt8wp_4CsA@mail.gmail.com>
	<CAEYYnxZNLk+-Hh8o7V88GBR+pTZxE0qm+mw4YxFeun7grsPEng@mail.gmail.com>
	<CALW2ey0PqNry2qK_SRMs6C=S3tZ_3KoGYHeKPHG0G8EuQ6vXag@mail.gmail.com>
	<CAEYYnxYEOHFEp7obAWmVq=SMXxpLp3hJavyM0NAzwQ7SsN+ixA@mail.gmail.com>
	<CAEYYnxYx1s+C9Qe_hHbXAvyYbeGCikMaXDHjqtcQf9tUG9cfbw@mail.gmail.com>
	<CAEYYnxb4QLneUjfj5dG80Pzae8MhEQG090YQ2G4oq7VccayM-g@mail.gmail.com>
	<CALW2ey312i1iRBH9Tg_zSb1jd_hGDoXtNuy-MLC5fEGZ=Vo+Xg@mail.gmail.com>
	<CAEYYnxb-q-MS2OUzEf5Q2fYLXVu7XKO0Kkj2uYWWHxfCWCATCw@mail.gmail.com>
	<CALW2ey26aub3W6siDkizrKctW9YjJMsdAPvGzE+jt=wBTvRnyw@mail.gmail.com>
Date: Tue, 29 Apr 2014 22:18:14 -0700
Message-ID: <CAEYYnxbDWnwO+qxg0O0hbwAW7e7=Wodte+az9srpoRO-VXtaFw@mail.gmail.com>
Subject: Re: MLlib - logistic regression with GD vs LBFGS, sparse vs dense
 benchmark result
From: DB Tsai <dbtsai@stanford.edu>
To: David Hall <dlwh@cs.berkeley.edu>
Cc: Xiangrui Meng <mengxr@gmail.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bdc912ee5de2504f83bac06
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc912ee5de2504f83bac06
Content-Type: text/plain; charset=UTF-8

Yeah, the approximation of hssian in LBFGS isn't stateless, and it does
depend on previous LBFGS step as Xiangrui also pointed out. It's surprising
that it works without error message. I also saw the loss is fluctuating
like SGD during the training.

We will remove the miniBatch mode in LBFGS in Spark before we've deeper
understanding of how "stochastic" LBFGS works.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Tue, Apr 29, 2014 at 9:50 PM, David Hall <dlwh@cs.berkeley.edu> wrote:

> Yeah, that's probably the easiest though obviously pretty hacky.
>
> I'm surprised that the hessian approximation isn't worse than it is. (As
> in, I'd expect error messages.) It's obviously line searching much more, so
> the approximation must be worse. You might be interested in this online
> bfgs:
> http://jmlr.org/proceedings/papers/v2/schraudolph07a/schraudolph07a.pdf
>
> -- David
>
>
> On Tue, Apr 29, 2014 at 3:30 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>
>> Have a quick hack to understand the behavior of SLBFGS
>> (Stochastic-LBFGS) by overwriting the breeze iterations method to get the
>> current LBFGS step to ensure that the objective function is the same during
>> the line search step. David, the following is my code, have a better way to
>> inject into it?
>>
>> https://github.com/dbtsai/spark/tree/dbtsai-lbfgshack
>>
>> Couple findings,
>>
>> 1) miniBatch (using rdd sample api) for each iteration is slower than
>> full data training when the full data is cached. Probably because sample is
>> not efficiency in Spark.
>>
>> 2) Since in the line search steps, we use the same sample of data (the
>> same objective function), the SLBFGS actually converges well.
>>
>> 3) For news20 dataset, with 0.05 miniBatch size, it takes 14 SLBFGS steps
>> (29 data iterations, 74.5seconds) to converge to loss < 0.10. For LBFGS
>> with full data training, it takes 9 LBFGS steps (12 data iterations, 37.6
>> seconds) to converge to loss < 0.10.
>>
>> It seems that as long as the noisy gradient happens in different SLBFGS
>> steps, it still works.
>>
>> (ps, I also tried in line search step, I use different sample of data,
>> and it just doesn't work as we expect.)
>>
>>
>>
>> Sincerely,
>>
>> DB Tsai
>> -------------------------------------------------------
>> My Blog: https://www.dbtsai.com
>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>
>>
>> On Mon, Apr 28, 2014 at 8:55 AM, David Hall <dlwh@cs.berkeley.edu> wrote:
>>
>>> That's right.
>>>
>>> FWIW, caching should be automatic now, but it might be the version of
>>> Breeze you're using doesn't do that yet.
>>>
>>> Also, In breeze.util._ there's an implicit that adds a tee method to
>>> iterator, and also a last method. Both are useful for things like this.
>>>
>>> -- David
>>>
>>>
>>> On Sun, Apr 27, 2014 at 11:53 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>
>>>> I think I figure it out. Instead of calling minimize, and record the
>>>> loss in the DiffFunction, I should do the following.
>>>>
>>>> val states = lbfgs.iterations(new CachedDiffFunction(costFun),
>>>> initialWeights.toBreeze.toDenseVector)
>>>> states.foreach(state => lossHistory.append(state.value))
>>>>
>>>> All the losses in states should be decreasing now. Am I right?
>>>>
>>>>
>>>>
>>>> Sincerely,
>>>>
>>>> DB Tsai
>>>> -------------------------------------------------------
>>>> My Blog: https://www.dbtsai.com
>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>
>>>>
>>>> On Sun, Apr 27, 2014 at 11:31 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>>
>>>>> Also, how many failure of rejection will terminate the optimization
>>>>> process? How is it related to "numberOfImprovementFailures"?
>>>>>
>>>>> Thanks.
>>>>>
>>>>>
>>>>> Sincerely,
>>>>>
>>>>> DB Tsai
>>>>> -------------------------------------------------------
>>>>> My Blog: https://www.dbtsai.com
>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>
>>>>>
>>>>> On Sun, Apr 27, 2014 at 11:28 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>>>
>>>>>> Hi David,
>>>>>>
>>>>>> I'm recording the loss history in the DiffFunction implementation,
>>>>>> and that's why the rejected step is also recorded in my loss history.
>>>>>>
>>>>>> Is there any api in Breeze LBFGS to get the history which already
>>>>>> excludes the reject step? Or should I just call "iterations" method and
>>>>>> check "iteratingShouldStop" instead?
>>>>>>
>>>>>> Thanks.
>>>>>>
>>>>>>
>>>>>> Sincerely,
>>>>>>
>>>>>> DB Tsai
>>>>>> -------------------------------------------------------
>>>>>> My Blog: https://www.dbtsai.com
>>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>
>>>>>>
>>>>>> On Fri, Apr 25, 2014 at 3:10 PM, David Hall <dlwh@cs.berkeley.edu>wrote:
>>>>>>
>>>>>>> LBFGS will not take a step that sends the objective value up. It
>>>>>>> might try a step that is "too big" and reject it, so if you're just logging
>>>>>>> everything that gets tried by LBFGS, you could see that. The "iterations"
>>>>>>> method of the minimizer should never return an increasing objective value.
>>>>>>> If you're regularizing, are you including the regularizer in the objective
>>>>>>> value computation?
>>>>>>>
>>>>>>> GD is almost never worth your time.
>>>>>>>
>>>>>>> -- David
>>>>>>>
>>>>>>> On Fri, Apr 25, 2014 at 2:57 PM, DB Tsai <dbtsai@stanford.edu>wrote:
>>>>>>>
>>>>>>>> Another interesting benchmark.
>>>>>>>>
>>>>>>>> *News20 dataset - 0.14M row, 1,355,191 features, 0.034% non-zero
>>>>>>>> elements.*
>>>>>>>>
>>>>>>>> LBFGS converges in 70 seconds, while GD seems to be not
>>>>>>>> progressing.
>>>>>>>>
>>>>>>>> Dense feature vector will be too big to fit in the memory, so only
>>>>>>>> conduct the sparse benchmark.
>>>>>>>>
>>>>>>>> I saw the sometimes the loss bumps up, and it's weird for me. Since
>>>>>>>> the cost function of logistic regression is convex, it should be
>>>>>>>> monotonically decreasing.  David, any suggestion?
>>>>>>>>
>>>>>>>> The detail figure:
>>>>>>>>
>>>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/news20.pdf
>>>>>>>>
>>>>>>>>
>>>>>>>> *Rcv1 dataset - 6.8M row, 677,399 features, 0.15% non-zero
>>>>>>>> elements.*
>>>>>>>>
>>>>>>>> LBFGS converges in 25 seconds, while GD also seems to be not
>>>>>>>> progressing.
>>>>>>>>
>>>>>>>> Only conduct sparse benchmark for the same reason. I also saw the
>>>>>>>> loss bumps up for unknown reason.
>>>>>>>>
>>>>>>>> The detail figure:
>>>>>>>>
>>>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/0b774682e398b4f7e0ce01a69c44000eb0e73454/result/rcv1.pdf
>>>>>>>>
>>>>>>>>
>>>>>>>> Sincerely,
>>>>>>>>
>>>>>>>> DB Tsai
>>>>>>>> -------------------------------------------------------
>>>>>>>> My Blog: https://www.dbtsai.com
>>>>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>
>>>>>>>>
>>>>>>>> On Thu, Apr 24, 2014 at 2:36 PM, DB Tsai <dbtsai@stanford.edu>wrote:
>>>>>>>>
>>>>>>>>> rcv1.binary is too sparse (0.15% non-zero elements), so dense
>>>>>>>>> format will not run due to out of memory. But sparse format runs really
>>>>>>>>> well.
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> Sincerely,
>>>>>>>>>
>>>>>>>>> DB Tsai
>>>>>>>>> -------------------------------------------------------
>>>>>>>>> My Blog: https://www.dbtsai.com
>>>>>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> On Thu, Apr 24, 2014 at 1:54 PM, DB Tsai <dbtsai@stanford.edu>wrote:
>>>>>>>>>
>>>>>>>>>> I'm doing the timer in runMiniBatchSGD after  val numExamples =
>>>>>>>>>> data.count()
>>>>>>>>>>
>>>>>>>>>> See the following. Running rcv1 dataset now, and will update soon.
>>>>>>>>>>
>>>>>>>>>>     val startTime = System.nanoTime()
>>>>>>>>>>     for (i <- 1 to numIterations) {
>>>>>>>>>>       // Sample a subset (fraction miniBatchFraction) of the
>>>>>>>>>> total data
>>>>>>>>>>       // compute and sum up the subgradients on this subset (this
>>>>>>>>>> is one map-reduce)
>>>>>>>>>>       val (gradientSum, lossSum) = data.sample(false,
>>>>>>>>>> miniBatchFraction, 42 + i)
>>>>>>>>>>         .aggregate((BDV.zeros[Double](weights.size), 0.0))(
>>>>>>>>>>           seqOp = (c, v) => (c, v) match { case ((grad, loss),
>>>>>>>>>> (label, features)) =>
>>>>>>>>>>             val l = gradient.compute(features, label, weights,
>>>>>>>>>> Vectors.fromBreeze(grad))
>>>>>>>>>>             (grad, loss + l)
>>>>>>>>>>           },
>>>>>>>>>>           combOp = (c1, c2) => (c1, c2) match { case ((grad1,
>>>>>>>>>> loss1), (grad2, loss2)) =>
>>>>>>>>>>             (grad1 += grad2, loss1 + loss2)
>>>>>>>>>>           })
>>>>>>>>>>
>>>>>>>>>>       /**
>>>>>>>>>>        * NOTE(Xinghao): lossSum is computed using the weights
>>>>>>>>>> from the previous iteration
>>>>>>>>>>        * and regVal is the regularization value computed in the
>>>>>>>>>> previous iteration as well.
>>>>>>>>>>        */
>>>>>>>>>>       stochasticLossHistory.append(lossSum / miniBatchSize +
>>>>>>>>>> regVal)
>>>>>>>>>>       val update = updater.compute(
>>>>>>>>>>         weights, Vectors.fromBreeze(gradientSum / miniBatchSize),
>>>>>>>>>> stepSize, i, regParam)
>>>>>>>>>>       weights = update._1
>>>>>>>>>>       regVal = update._2
>>>>>>>>>>       timeStamp.append(System.nanoTime() - startTime)
>>>>>>>>>>     }
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> Sincerely,
>>>>>>>>>>
>>>>>>>>>> DB Tsai
>>>>>>>>>> -------------------------------------------------------
>>>>>>>>>> My Blog: https://www.dbtsai.com
>>>>>>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> On Thu, Apr 24, 2014 at 1:44 PM, Xiangrui Meng <mengxr@gmail.com>wrote:
>>>>>>>>>>
>>>>>>>>>>> I don't understand why sparse falls behind dense so much at the
>>>>>>>>>>> very
>>>>>>>>>>> first iteration. I didn't see count() is called in
>>>>>>>>>>>
>>>>>>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/blob/master/src/main/scala/org/apache/spark/mllib/benchmark/BinaryLogisticRegression.scala
>>>>>>>>>>> . Maybe you have local uncommitted changes.
>>>>>>>>>>>
>>>>>>>>>>> Best,
>>>>>>>>>>> Xiangrui
>>>>>>>>>>>
>>>>>>>>>>> On Thu, Apr 24, 2014 at 11:26 AM, DB Tsai <dbtsai@stanford.edu>
>>>>>>>>>>> wrote:
>>>>>>>>>>> > Hi Xiangrui,
>>>>>>>>>>> >
>>>>>>>>>>> > Yes, I'm using yarn-cluster mode, and I did check # of
>>>>>>>>>>> executors I specified
>>>>>>>>>>> > are the same as the actual running executors.
>>>>>>>>>>> >
>>>>>>>>>>> > For caching and materialization, I've the timer in optimizer
>>>>>>>>>>> after calling
>>>>>>>>>>> > count(); as a result, the time for materialization in cache
>>>>>>>>>>> isn't in the
>>>>>>>>>>> > benchmark.
>>>>>>>>>>> >
>>>>>>>>>>> > The difference you saw is actually from dense feature or
>>>>>>>>>>> sparse feature
>>>>>>>>>>> > vector. For LBFGS and GD dense feature, you can see the first
>>>>>>>>>>> iteration
>>>>>>>>>>> > takes the same time. It's true for GD.
>>>>>>>>>>> >
>>>>>>>>>>> > I'm going to run rcv1.binary which only has 0.15% non-zero
>>>>>>>>>>> elements to
>>>>>>>>>>> > verify the hypothesis.
>>>>>>>>>>> >
>>>>>>>>>>> >
>>>>>>>>>>> > Sincerely,
>>>>>>>>>>> >
>>>>>>>>>>> > DB Tsai
>>>>>>>>>>> > -------------------------------------------------------
>>>>>>>>>>> > My Blog: https://www.dbtsai.com
>>>>>>>>>>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>>>> >
>>>>>>>>>>> >
>>>>>>>>>>> > On Thu, Apr 24, 2014 at 1:09 AM, Xiangrui Meng <
>>>>>>>>>>> mengxr@gmail.com> wrote:
>>>>>>>>>>> >>
>>>>>>>>>>> >> Hi DB,
>>>>>>>>>>> >>
>>>>>>>>>>> >> I saw you are using yarn-cluster mode for the benchmark. I
>>>>>>>>>>> tested the
>>>>>>>>>>> >> yarn-cluster mode and found that YARN does not always give
>>>>>>>>>>> you the
>>>>>>>>>>> >> exact number of executors requested. Just want to confirm
>>>>>>>>>>> that you've
>>>>>>>>>>> >> checked the number of executors.
>>>>>>>>>>> >>
>>>>>>>>>>> >> The second thing to check is that in the benchmark code,
>>>>>>>>>>> after you
>>>>>>>>>>> >> call cache, you should also call count() to materialize the
>>>>>>>>>>> RDD. I saw
>>>>>>>>>>> >> in the result, the real difference is actually at the first
>>>>>>>>>>> step.
>>>>>>>>>>> >> Adding intercept is not a cheap operation for sparse vectors.
>>>>>>>>>>> >>
>>>>>>>>>>> >> Best,
>>>>>>>>>>> >> Xiangrui
>>>>>>>>>>> >>
>>>>>>>>>>> >> On Thu, Apr 24, 2014 at 12:53 AM, Xiangrui Meng <
>>>>>>>>>>> mengxr@gmail.com> wrote:
>>>>>>>>>>> >> > I don't think it is easy to make sparse faster than dense
>>>>>>>>>>> with this
>>>>>>>>>>> >> > sparsity and feature dimension. You can try rcv1.binary,
>>>>>>>>>>> which should
>>>>>>>>>>> >> > show the difference easily.
>>>>>>>>>>> >> >
>>>>>>>>>>> >> > David, the breeze operators used here are
>>>>>>>>>>> >> >
>>>>>>>>>>> >> > 1. DenseVector dot SparseVector
>>>>>>>>>>> >> > 2. axpy DenseVector SparseVector
>>>>>>>>>>> >> >
>>>>>>>>>>> >> > However, the SparseVector is passed in as Vector[Double]
>>>>>>>>>>> instead of
>>>>>>>>>>> >> > SparseVector[Double]. It might use the axpy impl of
>>>>>>>>>>> [DenseVector,
>>>>>>>>>>> >> > Vector] and call activeIterator. I didn't check whether you
>>>>>>>>>>> used
>>>>>>>>>>> >> > multimethods on axpy.
>>>>>>>>>>> >> >
>>>>>>>>>>> >> > Best,
>>>>>>>>>>> >> > Xiangrui
>>>>>>>>>>> >> >
>>>>>>>>>>> >> > On Wed, Apr 23, 2014 at 10:35 PM, DB Tsai <
>>>>>>>>>>> dbtsai@stanford.edu> wrote:
>>>>>>>>>>> >> >> The figure showing the Log-Likelihood vs Time can be found
>>>>>>>>>>> here.
>>>>>>>>>>> >> >>
>>>>>>>>>>> >> >>
>>>>>>>>>>> >> >>
>>>>>>>>>>> https://github.com/dbtsai/spark-lbfgs-benchmark/raw/fd703303fb1c16ef5714901739154728550becf4/result/a9a11M.pdf
>>>>>>>>>>> >> >>
>>>>>>>>>>> >> >> Let me know if you can not open it. Thanks.
>>>>>>>>>>> >> >>
>>>>>>>>>>> >> >> Sincerely,
>>>>>>>>>>> >> >>
>>>>>>>>>>> >> >> DB Tsai
>>>>>>>>>>> >> >> -------------------------------------------------------
>>>>>>>>>>> >> >> My Blog: https://www.dbtsai.com
>>>>>>>>>>> >> >> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>>>> >> >>
>>>>>>>>>>> >> >>
>>>>>>>>>>> >> >> On Wed, Apr 23, 2014 at 9:34 PM, Shivaram Venkataraman
>>>>>>>>>>> >> >> <shivaram@eecs.berkeley.edu> wrote:
>>>>>>>>>>> >> >>> I don't think the attachment came through in the list.
>>>>>>>>>>> Could you
>>>>>>>>>>> >> >>> upload the
>>>>>>>>>>> >> >>> results somewhere and link to them ?
>>>>>>>>>>> >> >>>
>>>>>>>>>>> >> >>>
>>>>>>>>>>> >> >>> On Wed, Apr 23, 2014 at 9:32 PM, DB Tsai <
>>>>>>>>>>> dbtsai@dbtsai.com> wrote:
>>>>>>>>>>> >> >>>>
>>>>>>>>>>> >> >>>> 123 features per rows, and in average, 89% are zeros.
>>>>>>>>>>> >> >>>> On Apr 23, 2014 9:31 PM, "Evan Sparks" <
>>>>>>>>>>> evan.sparks@gmail.com> wrote:
>>>>>>>>>>> >> >>>>
>>>>>>>>>>> >> >>>> > What is the number of non zeroes per row (and number
>>>>>>>>>>> of features)
>>>>>>>>>>> >> >>>> > in the
>>>>>>>>>>> >> >>>> > sparse case? We've hit some issues with breeze sparse
>>>>>>>>>>> support in
>>>>>>>>>>> >> >>>> > the
>>>>>>>>>>> >> >>>> > past
>>>>>>>>>>> >> >>>> > but for sufficiently sparse data it's still pretty
>>>>>>>>>>> good.
>>>>>>>>>>> >> >>>> >
>>>>>>>>>>> >> >>>> > > On Apr 23, 2014, at 9:21 PM, DB Tsai <
>>>>>>>>>>> dbtsai@stanford.edu> wrote:
>>>>>>>>>>> >> >>>> > >
>>>>>>>>>>> >> >>>> > > Hi all,
>>>>>>>>>>> >> >>>> > >
>>>>>>>>>>> >> >>>> > > I'm benchmarking Logistic Regression in MLlib using
>>>>>>>>>>> the newly
>>>>>>>>>>> >> >>>> > > added
>>>>>>>>>>> >> >>>> > optimizer LBFGS and GD. I'm using the same dataset and
>>>>>>>>>>> the same
>>>>>>>>>>> >> >>>> > methodology
>>>>>>>>>>> >> >>>> > in this paper,
>>>>>>>>>>> http://www.csie.ntu.edu.tw/~cjlin/papers/l1.pdf
>>>>>>>>>>> >> >>>> > >
>>>>>>>>>>> >> >>>> > > I want to know how Spark scale while adding workers,
>>>>>>>>>>> and how
>>>>>>>>>>> >> >>>> > > optimizers
>>>>>>>>>>> >> >>>> > and input format (sparse or dense) impact performance.
>>>>>>>>>>> >> >>>> > >
>>>>>>>>>>> >> >>>> > > The benchmark code can be found here,
>>>>>>>>>>> >> >>>> > https://github.com/dbtsai/spark-lbfgs-benchmark
>>>>>>>>>>> >> >>>> > >
>>>>>>>>>>> >> >>>> > > The first dataset I benchmarked is a9a which only
>>>>>>>>>>> has 2.2MB. I
>>>>>>>>>>> >> >>>> > duplicated the dataset, and made it 762MB to have 11M
>>>>>>>>>>> rows. This
>>>>>>>>>>> >> >>>> > dataset
>>>>>>>>>>> >> >>>> > has 123 features and 11% of the data are non-zero
>>>>>>>>>>> elements.
>>>>>>>>>>> >> >>>> > >
>>>>>>>>>>> >> >>>> > > In this benchmark, all the dataset is cached in
>>>>>>>>>>> memory.
>>>>>>>>>>> >> >>>> > >
>>>>>>>>>>> >> >>>> > > As we expect, LBFGS converges faster than GD, and at
>>>>>>>>>>> some point,
>>>>>>>>>>> >> >>>> > > no
>>>>>>>>>>> >> >>>> > matter how we push GD, it will converge slower and
>>>>>>>>>>> slower.
>>>>>>>>>>> >> >>>> > >
>>>>>>>>>>> >> >>>> > > However, it's surprising that sparse format runs
>>>>>>>>>>> slower than
>>>>>>>>>>> >> >>>> > > dense
>>>>>>>>>>> >> >>>> > format. I did see that sparse format takes
>>>>>>>>>>> significantly smaller
>>>>>>>>>>> >> >>>> > amount
>>>>>>>>>>> >> >>>> > of
>>>>>>>>>>> >> >>>> > memory in caching RDD, but sparse is 40% slower than
>>>>>>>>>>> dense. I think
>>>>>>>>>>> >> >>>> > sparse
>>>>>>>>>>> >> >>>> > should be fast since when we compute x wT, since x is
>>>>>>>>>>> sparse, we
>>>>>>>>>>> >> >>>> > can do
>>>>>>>>>>> >> >>>> > it
>>>>>>>>>>> >> >>>> > faster. I wonder if there is anything I'm doing wrong.
>>>>>>>>>>> >> >>>> > >
>>>>>>>>>>> >> >>>> > > The attachment is the benchmark result.
>>>>>>>>>>> >> >>>> > >
>>>>>>>>>>> >> >>>> > > Thanks.
>>>>>>>>>>> >> >>>> > >
>>>>>>>>>>> >> >>>> > > Sincerely,
>>>>>>>>>>> >> >>>> > >
>>>>>>>>>>> >> >>>> > > DB Tsai
>>>>>>>>>>> >> >>>> > >
>>>>>>>>>>> -------------------------------------------------------
>>>>>>>>>>> >> >>>> > > My Blog: https://www.dbtsai.com
>>>>>>>>>>> >> >>>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>>>>> >> >>>> >
>>>>>>>>>>> >> >>>
>>>>>>>>>>> >> >>>
>>>>>>>>>>> >
>>>>>>>>>>> >
>>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>
>>>>>>>>
>>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>

--047d7bdc912ee5de2504f83bac06--

From dev-return-7457-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 30 05:43:53 2014
Return-Path: <dev-return-7457-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E1E90102DF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Apr 2014 05:43:53 +0000 (UTC)
Received: (qmail 33194 invoked by uid 500); 30 Apr 2014 05:43:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32860 invoked by uid 500); 30 Apr 2014 05:43:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32850 invoked by uid 99); 30 Apr 2014 05:43:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 05:43:52 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_QUOTING
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.178 as permitted sender)
Received: from [209.85.214.178] (HELO mail-ob0-f178.google.com) (209.85.214.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 05:43:48 +0000
Received: by mail-ob0-f178.google.com with SMTP id wn1so1452520obc.9
        for <dev@spark.apache.org>; Tue, 29 Apr 2014 22:43:25 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=DFYYr5xq0PoiXnXkxiGJ3vj3pWTxOpPowEV7HHF1i5s=;
        b=cJ6qt9TkinJOMxyGFQlFW1Rb24SLv9ImoMIXwW/iM8Pjm+zVtGaD1Wx8GU3rEvs/0F
         06OznGDlzL8xAZke3gDo0U3/KT/gt1ysuzryUOfw+rWlgzYYIUOeYPue6HRyliTp106D
         AKNy8El5dyf/Xptm1l2ksbmecBF0aZ9tat7X+uMXN+0vw6w2h8LSSYSHmFgNM7kh2+MV
         3DMgVbVYKvJqz5tZFJcPZDvp4j+xfbWLp2FH8ykCMXvA5QL1mlX9UNoqZvktpbbZ14WT
         E+7RkC7miE995iQjdir0R1UK45tgIbWEoxYM0yGaRkzuZL2kLAKepEbiA/ejEVriA8jq
         nnWQ==
MIME-Version: 1.0
X-Received: by 10.182.33.99 with SMTP id q3mr1953413obi.33.1398836605492; Tue,
 29 Apr 2014 22:43:25 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Tue, 29 Apr 2014 22:43:25 -0700 (PDT)
In-Reply-To: <CAKW0i0yLR0xxE-CFcH+uRjyRxdi=0aK4GFLHbXjj3GBP-CzbkA@mail.gmail.com>
References: <CABPQxstL6nwTO2H9p8=GJh1g2zxOJd02Wt7L06mCLjo-vwwG9Q@mail.gmail.com>
	<CAKW0i0yEHeMQL7ve0=kV=U7DBLYXDY9+aeydohnpwu9Ybesu1Q@mail.gmail.com>
	<CABPQxsudkXB02HObvgK1=bHgDYjwVMsxQ79zQ++35N-nvR+1Bg@mail.gmail.com>
	<CAKW0i0yLR0xxE-CFcH+uRjyRxdi=0aK4GFLHbXjj3GBP-CzbkA@mail.gmail.com>
Date: Tue, 29 Apr 2014 22:43:25 -0700
Message-ID: <CABPQxsveNgm_VbA6sygXYNjzmj5tZO1AX6V5An3uF-f2kMznAg@mail.gmail.com>
Subject: Re: Spark 1.0.0 rc3
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

That suggestion got lost along the way and IIRC the patch didn't have
that. It's a good idea though, if nothing else to provide a simple
means for backwards compatibility.

I created a JIRA for this. It's very straightforward so maybe someone
can pick it up quickly:
https://issues.apache.org/jira/browse/SPARK-1677


On Tue, Apr 29, 2014 at 2:20 PM, Dean Wampler <deanwampler@gmail.com> wrote:
> Thanks. I'm fine with the logic change, although I was a bit surprised to
> see Hadoop used for file I/O.
>
> Anyway, the jira issue and pull request discussions mention a flag to
> enable overwrites. That would be very convenient for a tutorial I'm
> writing, although I wouldn't recommend it for normal use, of course.
> However, I can't figure out if this actually exists. I found the
> spark.files.overwrite property, but that doesn't apply.  Does this override
> flag, method call, or method argument actually exist?
>
> Thanks,
> Dean
>
>
> On Tue, Apr 29, 2014 at 1:54 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>
>> Hi Dean,
>>
>> We always used the Hadoop libraries here to read and write local
>> files. In Spark 1.0 we started enforcing the rule that you can't
>> over-write an existing directory because it can cause
>> confusing/undefined behavior if multiple jobs output to the directory
>> (they partially clobber each other's output).
>>
>> https://issues.apache.org/jira/browse/SPARK-1100
>> https://github.com/apache/spark/pull/11
>>
>> In the JIRA I actually proposed slightly deviating from Hadoop
>> semantics and allowing the directory to exist if it is empty, but I
>> think in the end we decided to just go with the exact same semantics
>> as Hadoop (i.e. empty directories are a problem).
>>
>> - Patrick
>>
>> On Tue, Apr 29, 2014 at 9:43 AM, Dean Wampler <deanwampler@gmail.com>
>> wrote:
>> > I'm observing one anomalous behavior. With the 1.0.0 libraries, it's
>> using
>> > HDFS classes for file I/O, while the same script compiled and running
>> with
>> > 0.9.1 uses only the local-mode File IO.
>> >
>> > The script is a variation of the Word Count script. Here are the "guts":
>> >
>> > object WordCount2 {
>> >   def main(args: Array[String]) = {
>> >
>> >     val sc = new SparkContext("local", "Word Count (2)")
>> >
>> >     val input = sc.textFile(".../some/local/file").map(line =>
>> > line.toLowerCase)
>> >     input.cache
>> >
>> >     val wc2 = input
>> >       .flatMap(line => line.split("""\W+"""))
>> >       .map(word => (word, 1))
>> >       .reduceByKey((count1, count2) => count1 + count2)
>> >
>> >     wc2.saveAsTextFile("output/some/directory")
>> >
>> >     sc.stop()
>> >
>> > It works fine compiled and executed with 0.9.1. If I recompile and run
>> with
>> > 1.0.0-RC1, where the same output directory still exists, I get this
>> > familiar Hadoop-ish exception:
>> >
>> > [error] (run-main-0) org.apache.hadoop.mapred.FileAlreadyExistsException:
>> > Output directory
>> >
>> file:/Users/deanwampler/projects/typesafe/activator/activator-spark/output/kjv-wc
>> > already exists
>> > org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory
>> >
>> file:/Users/deanwampler/projects/typesafe/activator/activator-spark/output/kjv-wc
>> > already exists
>> >  at
>> >
>> org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:121)
>> > at
>> >
>> org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:749)
>> >  at
>> >
>> org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:662)
>> > at
>> >
>> org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:581)
>> >  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1057)
>> > at spark.activator.WordCount2$.main(WordCount2.scala:42)
>> >  at spark.activator.WordCount2.main(WordCount2.scala)
>> > ...
>> >
>> > Thoughts?
>> >
>> >
>> > On Tue, Apr 29, 2014 at 3:05 AM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>> >
>> >> Hey All,
>> >>
>> >> This is not an official vote, but I wanted to cut an RC so that people
>> can
>> >> test against the Maven artifacts, test building with their
>> configuration,
>> >> etc. We are still chasing down a few issues and updating docs, etc.
>> >>
>> >> If you have issues or bug reports for this release, please send an
>> e-mail
>> >> to the Spark dev list and/or file a JIRA.
>> >>
>> >> Commit: d636772 (v1.0.0-rc3)
>> >>
>> >>
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d636772ea9f98e449a038567b7975b1a07de3221
>> >>
>> >> Binaries:
>> >> http://people.apache.org/~pwendell/spark-1.0.0-rc3/
>> >>
>> >> Docs:
>> >> http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/
>> >>
>> >> Repository:
>> >> https://repository.apache.org/content/repositories/orgapachespark-1012/
>> >>
>> >> == API Changes ==
>> >> If you want to test building against Spark there are some minor API
>> >> changes. We'll get these written up for the final release but I'm
>> noting a
>> >> few here (not comprehensive):
>> >>
>> >> changes to ML vector specification:
>> >>
>> >>
>> http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/mllib-guide.html#from-09-to-10
>> >>
>> >> changes to the Java API:
>> >>
>> >>
>> http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>> >>
>> >> coGroup and related functions now return Iterable[T] instead of Seq[T]
>> >> ==> Call toSeq on the result to restore the old behavior
>> >>
>> >> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
>> >> ==> Call toSeq on the result to restore old behavior
>> >>
>> >> Streaming classes have been renamed:
>> >> NetworkReceiver -> Receiver
>> >>
>> >
>> >
>> >
>> > --
>> > Dean Wampler, Ph.D.
>> > Typesafe
>> > @deanwampler
>> > http://typesafe.com
>> > http://polyglotprogramming.com
>>
>
>
>
> --
> Dean Wampler, Ph.D.
> Typesafe
> @deanwampler
> http://typesafe.com
> http://polyglotprogramming.com

From dev-return-7458-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 30 08:14:17 2014
Return-Path: <dev-return-7458-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D24F11079E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Apr 2014 08:14:16 +0000 (UTC)
Received: (qmail 59988 invoked by uid 500); 30 Apr 2014 08:14:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59419 invoked by uid 500); 30 Apr 2014 08:14:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59410 invoked by uid 99); 30 Apr 2014 08:14:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 08:14:10 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ju.han.felix@gmail.com designates 209.85.213.174 as permitted sender)
Received: from [209.85.213.174] (HELO mail-ig0-f174.google.com) (209.85.213.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 08:14:06 +0000
Received: by mail-ig0-f174.google.com with SMTP id h18so7271075igc.1
        for <dev@spark.incubator.apache.org>; Wed, 30 Apr 2014 01:13:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=9TyijZXrEXoJALIUCqmdw23YfOB/LyIpRb2lLe29qBc=;
        b=yflv4U6Z5g6/nv6PitHi7N8J+yoWuvY0gZDSgCnCSRns69nP/NTUDz1v8LokGlk1Qr
         rTjvZXMeC0xr0XtrEsmorEB/O1kB50pNT0QUj1xi/1I/1C4N/oUVq34Arfp20iH9Pre7
         awD77sDxRQqxdgfrLPSCUoeXj8O1fuzNau9+v0qNt16hsPf6o2MfQrf6z2nKQZvbkJdS
         phu5FyhhoybUfIUy2vnHumVsUT2uPKPZfhRudnzSzUv8c6b22b0eaLrODZ4hN/kIHcgF
         l0scSC+LrEOq+hj/E1k5f7IW4fQQwwHE2SaRN30bXzmbqe34AfIeQYrHXQSvlnyqy0qL
         b4SA==
MIME-Version: 1.0
X-Received: by 10.43.10.131 with SMTP id pa3mr2439868icb.18.1398845623636;
 Wed, 30 Apr 2014 01:13:43 -0700 (PDT)
Received: by 10.64.133.227 with HTTP; Wed, 30 Apr 2014 01:13:43 -0700 (PDT)
In-Reply-To: <CA+ndhHrh3R1QC3=_CH_hQ4eWoW1sUfZmFkunNo9pm9PYhP2Yjw@mail.gmail.com>
References: <CA+ndhHqRqNmMGVNUMw=tBGszh=kjBYbQKYe3aMWWqS7tmE7RRA@mail.gmail.com>
	<CA+ndhHrh3R1QC3=_CH_hQ4eWoW1sUfZmFkunNo9pm9PYhP2Yjw@mail.gmail.com>
Date: Wed, 30 Apr 2014 10:13:43 +0200
Message-ID: <CA+ndhHqubenK=t-=Soy-uMF=7oGGHr6_8zvyuGwZYAKj2v1uBQ@mail.gmail.com>
Subject: Fwd: Spark RDD cache memory usage
From: Han JU <ju.han.felix@gmail.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=bcaec50e61397e1b6a04f83e203f
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec50e61397e1b6a04f83e203f
Content-Type: text/plain; charset=UTF-8

Hi,

As I understand, by default in Spark a fraction of the executor memory
(60%) is reserved for RDD caching. So if there's no explicit caching in the
code (eg. rdd.cache() etc.), or if we persist RDD with
StorageLevel.DISK_ONLY, is this part of memory wasted? Does Spark allocates
the RDD cache memory dynamically? Or does spark automatically caches RDDs
when it can?

I've posted this question in user list but got no response there, so I try
the dev list. Sorry for spam.

Thanks.

-- 
*JU Han*

Data Engineer @ Botify.com

+33 0619608888

--bcaec50e61397e1b6a04f83e203f--

From dev-return-7459-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 30 18:29:34 2014
Return-Path: <dev-return-7459-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3C63D11CBB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Apr 2014 18:29:34 +0000 (UTC)
Received: (qmail 91429 invoked by uid 500); 30 Apr 2014 18:29:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91371 invoked by uid 500); 30 Apr 2014 18:29:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91351 invoked by uid 99); 30 Apr 2014 18:29:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 18:29:30 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vanzin@cloudera.com designates 209.85.216.175 as permitted sender)
Received: from [209.85.216.175] (HELO mail-qc0-f175.google.com) (209.85.216.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 18:29:26 +0000
Received: by mail-qc0-f175.google.com with SMTP id w7so777609qcr.6
        for <dev@spark.apache.org>; Wed, 30 Apr 2014 11:29:03 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=0wUi1JnGPhUyglx4guzngXDrIU8v471jXM1yoQPc57w=;
        b=ZfN4/IZJnw+4ZwWeQDQ9hc74NgCNn1wwC7aoC2osXQNGbkW0IfAGKKETN+mVckCPNN
         XjG3kyeYlQdHUsxKHQXJLbzLT0YVznZQ7S4W1/BuXo7Uln6vKWCPeCGV78qVrOV0O4u4
         rV/dNH8PBqN91wPlgfrEVNrQ57XqrvXumFQMS+e1R/RjR8eEOkh08WE/M3rlYA/wW6or
         sTl/z/MdHsndFw/X3V81tagqCu7u7jLX8o48za6DXqIELBpTT8sPxGA3qGCwdgNRdStU
         kBUIq0TV/g4lixc3yHEhMiTwEECIKhtBLrFmZxCHfrCaiwHB3LqdTqgIT8a2Zp/U7YDR
         MDKA==
X-Gm-Message-State: ALoCoQkTJ2xaF3l98nkjzclMq/kQ7ZUYnWcsxgVIqMKpvW06HOkgS/GAzABJI0++6gpXKj991xvB
MIME-Version: 1.0
X-Received: by 10.140.18.173 with SMTP id 42mr7226324qgf.94.1398882543378;
 Wed, 30 Apr 2014 11:29:03 -0700 (PDT)
Received: by 10.229.211.5 with HTTP; Wed, 30 Apr 2014 11:29:03 -0700 (PDT)
Date: Wed, 30 Apr 2014 11:29:03 -0700
Message-ID: <CAAOnQ7ukPXuqo21v+d36tk-DcUWQWBmKwc3Sp7okhZF3G4NNUw@mail.gmail.com>
Subject: SparkSubmit and --driver-java-options
From: Marcelo Vanzin <vanzin@cloudera.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hello all,

Maybe my brain is not evolved enough to be able to trace through what
happens with command-line arguments as they're parsed through all the
shell scripts... but I really can't figure out how to pass more than a
single JVM option on the command line.

Unless someone has an obvious workaround that I'm missing, I'd like to
propose something that is actually pretty standard in JVM tools: using
-J. From javac:

  -J<flag>                   Pass <flag> directly to the runtime system

So "javac -J-Xmx1g" would pass "-Xmx1g" to the underlying JVM. You can
use several of those to pass multiple options (unlike
--driver-java-options), so it helps that it's a short syntax.

Unless someone has some issue with that I'll work on a patch for it...
(well, I'm going to do it locally for me anyway because I really can't
figure out how to do what I want to otherwise.)


-- 
Marcelo

From dev-return-7460-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 30 19:49:48 2014
Return-Path: <dev-return-7460-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DB5EA11F5F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Apr 2014 19:49:47 +0000 (UTC)
Received: (qmail 71790 invoked by uid 500); 30 Apr 2014 19:49:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71695 invoked by uid 500); 30 Apr 2014 19:49:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71687 invoked by uid 99); 30 Apr 2014 19:49:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 19:49:45 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.45 as permitted sender)
Received: from [209.85.219.45] (HELO mail-oa0-f45.google.com) (209.85.219.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 19:49:42 +0000
Received: by mail-oa0-f45.google.com with SMTP id eb12so2524534oac.4
        for <dev@spark.apache.org>; Wed, 30 Apr 2014 12:49:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=aBVQeTFgExXHH+LpBqOmupYeU1h0X6QGLa/qiRGy0/Q=;
        b=uvCAjNDtHkYNdvD1o+fe5evuJZ1Pr8V+zH120b9Mnm0tAYoSUAOYRPnNN93Qb9QN7y
         hUOdN+pCMuJ9MR/ovC9Cen7cmRjqUvP5m0YO2TugfYYK8GLoAIyk6FYlxNcUPMBBe7E6
         mDlhKK+WiIzqxZy3BLY5BmEfzLkXW2E1b+mLk3UjPw6TGVZ20mk3yeAsBgd1rvEDAkbr
         tnRQEjcJ39viD6BIkGeRHXhJ0jrA/2glEJMrcMmPJIo3wZ6/Yf4hmOJaswEvGCTMQXOP
         8SSefWl5UkXJMuoBzk6IbKNHy1XaWTvNH+pVWpugzMHSzc8xYDi5oHHeldUI9x5wkD+v
         uixA==
MIME-Version: 1.0
X-Received: by 10.60.92.170 with SMTP id cn10mr3270363oeb.76.1398887359327;
 Wed, 30 Apr 2014 12:49:19 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Wed, 30 Apr 2014 12:49:19 -0700 (PDT)
In-Reply-To: <CAAOnQ7ukPXuqo21v+d36tk-DcUWQWBmKwc3Sp7okhZF3G4NNUw@mail.gmail.com>
References: <CAAOnQ7ukPXuqo21v+d36tk-DcUWQWBmKwc3Sp7okhZF3G4NNUw@mail.gmail.com>
Date: Wed, 30 Apr 2014 12:49:19 -0700
Message-ID: <CABPQxsv-eRJ+hneVkUofksYpCUzvr31tkqfivuUpb4e0q8ikAQ@mail.gmail.com>
Subject: Re: SparkSubmit and --driver-java-options
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I added a fix for this recently and it didn't require adding -J
notation - are you trying it with this patch?

https://issues.apache.org/jira/browse/SPARK-1654

 ./bin/spark-shell --driver-java-options "-Dfoo=a -Dbar=b"
scala> sys.props.get("foo")
res0: Option[String] = Some(a)
scala> sys.props.get("bar")
res1: Option[String] = Some(b)

- Patrick

On Wed, Apr 30, 2014 at 11:29 AM, Marcelo Vanzin <vanzin@cloudera.com> wrote:
> Hello all,
>
> Maybe my brain is not evolved enough to be able to trace through what
> happens with command-line arguments as they're parsed through all the
> shell scripts... but I really can't figure out how to pass more than a
> single JVM option on the command line.
>
> Unless someone has an obvious workaround that I'm missing, I'd like to
> propose something that is actually pretty standard in JVM tools: using
> -J. From javac:
>
>   -J<flag>                   Pass <flag> directly to the runtime system
>
> So "javac -J-Xmx1g" would pass "-Xmx1g" to the underlying JVM. You can
> use several of those to pass multiple options (unlike
> --driver-java-options), so it helps that it's a short syntax.
>
> Unless someone has some issue with that I'll work on a patch for it...
> (well, I'm going to do it locally for me anyway because I really can't
> figure out how to do what I want to otherwise.)
>
>
> --
> Marcelo

From dev-return-7461-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 30 20:07:25 2014
Return-Path: <dev-return-7461-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C212F11FF2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Apr 2014 20:07:25 +0000 (UTC)
Received: (qmail 385 invoked by uid 500); 30 Apr 2014 20:07:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 336 invoked by uid 500); 30 Apr 2014 20:07:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 280 invoked by uid 99); 30 Apr 2014 20:07:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 20:07:23 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vanzin@cloudera.com designates 209.85.192.51 as permitted sender)
Received: from [209.85.192.51] (HELO mail-qg0-f51.google.com) (209.85.192.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 20:07:19 +0000
Received: by mail-qg0-f51.google.com with SMTP id z60so2401944qgd.24
        for <dev@spark.apache.org>; Wed, 30 Apr 2014 13:06:57 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=Q+DMCgpixfu/ZpHTmoCJcZbq3L0deK3KVlWLOX8C72c=;
        b=KQ+1/cXSBifpX33sDE/5xaNJ3nwXnKR2kxlcazGslG3j4NrfI+ZKfDeO/tDcg+fi0m
         Lpx3mbJIZt+OI0AivGKPrDvBXfYZIwIrhjLk3YCgFKlB8Itc402Kk2BFCGSoWDHbyVYa
         yEotVMklqfBAQ+Kke3QA/BvPU9Thl9DxWZ3TrahMngoUZhxF2R51HRKuuOxIbvd0Q1BU
         58e/CWAb5MgqeTWk2L2d7b0AWOyARhZuoOPN8VFHMcrBfLpRpnjXmREAU2gE4WOKxOpA
         7ixT+y8R3wZpqQCPsGVbPFErvpreOO7ZWUoZdlrFhnTit4amiZ/f16YwsI/U0hFscOfZ
         K49g==
X-Gm-Message-State: ALoCoQk56hi5Jkj0D+0nMQcqImr0l3Lg3tot63dO4TjDe54SaAA9580grX5By8Kgfcc6XNlDqSaL
MIME-Version: 1.0
X-Received: by 10.224.73.136 with SMTP id q8mr8474349qaj.54.1398888417014;
 Wed, 30 Apr 2014 13:06:57 -0700 (PDT)
Received: by 10.229.211.5 with HTTP; Wed, 30 Apr 2014 13:06:56 -0700 (PDT)
In-Reply-To: <CABPQxsv-eRJ+hneVkUofksYpCUzvr31tkqfivuUpb4e0q8ikAQ@mail.gmail.com>
References: <CAAOnQ7ukPXuqo21v+d36tk-DcUWQWBmKwc3Sp7okhZF3G4NNUw@mail.gmail.com>
	<CABPQxsv-eRJ+hneVkUofksYpCUzvr31tkqfivuUpb4e0q8ikAQ@mail.gmail.com>
Date: Wed, 30 Apr 2014 13:06:56 -0700
Message-ID: <CAAOnQ7uv8u0J_hpogGgcfO7_cEk40gRAXD1MSr6M4y3nbFew-g@mail.gmail.com>
Subject: Re: SparkSubmit and --driver-java-options
From: Marcelo Vanzin <vanzin@cloudera.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Just pulled again just in case. Verified your fix is there.

$ ./bin/spark-submit --master yarn --deploy-mode client
--driver-java-options "-Dfoo -Dbar" blah blah blah
error: Unrecognized option '-Dbar'.
run with --help for more information or --verbose for debugging output


On Wed, Apr 30, 2014 at 12:49 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> I added a fix for this recently and it didn't require adding -J
> notation - are you trying it with this patch?
>
> https://issues.apache.org/jira/browse/SPARK-1654
>
>  ./bin/spark-shell --driver-java-options "-Dfoo=a -Dbar=b"
> scala> sys.props.get("foo")
> res0: Option[String] = Some(a)
> scala> sys.props.get("bar")
> res1: Option[String] = Some(b)
>
> - Patrick
>
> On Wed, Apr 30, 2014 at 11:29 AM, Marcelo Vanzin <vanzin@cloudera.com> wrote:
>> Hello all,
>>
>> Maybe my brain is not evolved enough to be able to trace through what
>> happens with command-line arguments as they're parsed through all the
>> shell scripts... but I really can't figure out how to pass more than a
>> single JVM option on the command line.
>>
>> Unless someone has an obvious workaround that I'm missing, I'd like to
>> propose something that is actually pretty standard in JVM tools: using
>> -J. From javac:
>>
>>   -J<flag>                   Pass <flag> directly to the runtime system
>>
>> So "javac -J-Xmx1g" would pass "-Xmx1g" to the underlying JVM. You can
>> use several of those to pass multiple options (unlike
>> --driver-java-options), so it helps that it's a short syntax.
>>
>> Unless someone has some issue with that I'll work on a patch for it...
>> (well, I'm going to do it locally for me anyway because I really can't
>> figure out how to do what I want to otherwise.)
>>
>>
>> --
>> Marcelo



-- 
Marcelo

From dev-return-7462-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 30 20:41:40 2014
Return-Path: <dev-return-7462-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3D6EF10172
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Apr 2014 20:41:40 +0000 (UTC)
Received: (qmail 96535 invoked by uid 500); 30 Apr 2014 20:41:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96407 invoked by uid 500); 30 Apr 2014 20:41:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96311 invoked by uid 99); 30 Apr 2014 20:41:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 20:41:26 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.171 as permitted sender)
Received: from [209.85.214.171] (HELO mail-ob0-f171.google.com) (209.85.214.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 20:41:23 +0000
Received: by mail-ob0-f171.google.com with SMTP id uy5so2667170obc.16
        for <dev@spark.apache.org>; Wed, 30 Apr 2014 13:41:00 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=kMMC93+y2dkMBA/EJSqoSH7BECZlu8Jtfkm5Qn7mSrE=;
        b=uECCXAQK8LhBGOPmzSS0wCOTn4ISmF9GdvnFPqT9ZMBD/B+PHVIZCwQ0SdGCUmTaY/
         +NRtJPiRiYywfI69QKHYKNXi990jRUcU7udHyDDKE0DuQrKOU9KIJL2IyxfvyODLrXn5
         0O581pwrF+R6v78bVhOVIKUuEzLaB19I7faJqW1zE8TgmOihIvDLdP7WvE6fiw1pmtfz
         LfL4rh4mW+E9plqpN7ZoV76AeBKbJ/Os+HzjVZMUiRAGsl50EwJPu/pUuew8qmWoYqDA
         TiDuhOzeLzJjonMicaEkoz7u7fqMWY8iLjprgQcF/hXjcSksWnEZ7oZ82lj/aweBS0DE
         sP8Q==
MIME-Version: 1.0
X-Received: by 10.182.102.99 with SMTP id fn3mr4091942obb.57.1398890460119;
 Wed, 30 Apr 2014 13:41:00 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Wed, 30 Apr 2014 13:41:00 -0700 (PDT)
In-Reply-To: <CAAOnQ7uv8u0J_hpogGgcfO7_cEk40gRAXD1MSr6M4y3nbFew-g@mail.gmail.com>
References: <CAAOnQ7ukPXuqo21v+d36tk-DcUWQWBmKwc3Sp7okhZF3G4NNUw@mail.gmail.com>
	<CABPQxsv-eRJ+hneVkUofksYpCUzvr31tkqfivuUpb4e0q8ikAQ@mail.gmail.com>
	<CAAOnQ7uv8u0J_hpogGgcfO7_cEk40gRAXD1MSr6M4y3nbFew-g@mail.gmail.com>
Date: Wed, 30 Apr 2014 13:41:00 -0700
Message-ID: <CABPQxstyQW98FiOqWeq63P=ezZEPuycYiPtPRnr=vp8GjW_dZg@mail.gmail.com>
Subject: Re: SparkSubmit and --driver-java-options
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah I think the problem is that the spark-submit script doesn't pass
the argument array to spark-class in the right way, so any quoted
strings get flattened.

We do:
ORIG_ARGS=$@
$SPARK_HOME/bin/spark-class org.apache.spark.deploy.SparkSubmit $ORIG_ARGS

This works:
// remove all the code relating to `shift`ing the arguments
$SPARK_HOME/bin/spark-class org.apache.spark.deploy.SparkSubmit "$@"

Not sure, but I think the issue is that when you make a copy of $@ in
bash the type actually changes from an array to something else.

My patch fixes this for spark-shell but I didn't realize that
spark-submit does the same thing.
https://github.com/apache/spark/pull/576/files#diff-bc287993dfd11fd18794041e169ffd72L23

I think we'll need to figure out how to do this correctly in the bash
script so that quoted strings get passed in the right way.

On Wed, Apr 30, 2014 at 1:06 PM, Marcelo Vanzin <vanzin@cloudera.com> wrote:
> Just pulled again just in case. Verified your fix is there.
>
> $ ./bin/spark-submit --master yarn --deploy-mode client
> --driver-java-options "-Dfoo -Dbar" blah blah blah
> error: Unrecognized option '-Dbar'.
> run with --help for more information or --verbose for debugging output
>
>
> On Wed, Apr 30, 2014 at 12:49 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>> I added a fix for this recently and it didn't require adding -J
>> notation - are you trying it with this patch?
>>
>> https://issues.apache.org/jira/browse/SPARK-1654
>>
>>  ./bin/spark-shell --driver-java-options "-Dfoo=a -Dbar=b"
>> scala> sys.props.get("foo")
>> res0: Option[String] = Some(a)
>> scala> sys.props.get("bar")
>> res1: Option[String] = Some(b)
>>
>> - Patrick
>>
>> On Wed, Apr 30, 2014 at 11:29 AM, Marcelo Vanzin <vanzin@cloudera.com> wrote:
>>> Hello all,
>>>
>>> Maybe my brain is not evolved enough to be able to trace through what
>>> happens with command-line arguments as they're parsed through all the
>>> shell scripts... but I really can't figure out how to pass more than a
>>> single JVM option on the command line.
>>>
>>> Unless someone has an obvious workaround that I'm missing, I'd like to
>>> propose something that is actually pretty standard in JVM tools: using
>>> -J. From javac:
>>>
>>>   -J<flag>                   Pass <flag> directly to the runtime system
>>>
>>> So "javac -J-Xmx1g" would pass "-Xmx1g" to the underlying JVM. You can
>>> use several of those to pass multiple options (unlike
>>> --driver-java-options), so it helps that it's a short syntax.
>>>
>>> Unless someone has some issue with that I'll work on a patch for it...
>>> (well, I'm going to do it locally for me anyway because I really can't
>>> figure out how to do what I want to otherwise.)
>>>
>>>
>>> --
>>> Marcelo
>
>
>
> --
> Marcelo

From dev-return-7463-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 30 20:49:27 2014
Return-Path: <dev-return-7463-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6097A101F3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Apr 2014 20:49:27 +0000 (UTC)
Received: (qmail 19066 invoked by uid 500); 30 Apr 2014 20:49:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18851 invoked by uid 500); 30 Apr 2014 20:49:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18802 invoked by uid 99); 30 Apr 2014 20:49:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 20:49:21 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vanzin@cloudera.com designates 209.85.192.50 as permitted sender)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 20:49:16 +0000
Received: by mail-qg0-f50.google.com with SMTP id 63so2512098qgz.9
        for <dev@spark.apache.org>; Wed, 30 Apr 2014 13:48:53 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=ydyuzBws47tZo+7W8nGV5OZNs95NB+dSzMYP5FMGfs8=;
        b=WnT8/BFl22j2wGjYNLt3TvtDFqbnLObSFmfSntDgu+YsHH7R1k8RBkx5IyD9wLPyq5
         gFDmoIzAPwWUWR0eoxiE/DOMfh5w1g1p7qcOYfXXa4/OWKSH23T7h1t24pTl32LQa3Qu
         TGDrgndwZVx66Bbfzxia0nb7+I5+MnbZ5Frd+kPsz+6ujesiV9YukkgqeG02Uw04DFdy
         q9lpIMzIeLRTcgkXxWeeJuAHHDKIxRf20vIS+Y+brD4JWwexrBUz8S8JIUEAH4uviyGa
         Ued6sSkRL/VRPtnImP7Mk6pr/xmGECI8UyjjwyJSLd8svIoVLSsz5DrDQtJpUwHKyZ61
         o9nQ==
X-Gm-Message-State: ALoCoQl0OJEjXUUtBrI/IglZGuj8DW+ggv37TB/MU0q8lTk81e/5wSvqAQgRzUYdJVqh8ZDsMFXF
MIME-Version: 1.0
X-Received: by 10.224.67.131 with SMTP id r3mr8587435qai.75.1398890933535;
 Wed, 30 Apr 2014 13:48:53 -0700 (PDT)
Received: by 10.229.211.5 with HTTP; Wed, 30 Apr 2014 13:48:53 -0700 (PDT)
In-Reply-To: <CABPQxstyQW98FiOqWeq63P=ezZEPuycYiPtPRnr=vp8GjW_dZg@mail.gmail.com>
References: <CAAOnQ7ukPXuqo21v+d36tk-DcUWQWBmKwc3Sp7okhZF3G4NNUw@mail.gmail.com>
	<CABPQxsv-eRJ+hneVkUofksYpCUzvr31tkqfivuUpb4e0q8ikAQ@mail.gmail.com>
	<CAAOnQ7uv8u0J_hpogGgcfO7_cEk40gRAXD1MSr6M4y3nbFew-g@mail.gmail.com>
	<CABPQxstyQW98FiOqWeq63P=ezZEPuycYiPtPRnr=vp8GjW_dZg@mail.gmail.com>
Date: Wed, 30 Apr 2014 13:48:53 -0700
Message-ID: <CAAOnQ7vKYcCoT8n_07+9muxGKa9ptvV_CiUuicuVPT-q9bwGaQ@mail.gmail.com>
Subject: Re: SparkSubmit and --driver-java-options
From: Marcelo Vanzin <vanzin@cloudera.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Wed, Apr 30, 2014 at 1:41 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Yeah I think the problem is that the spark-submit script doesn't pass
> the argument array to spark-class in the right way, so any quoted
> strings get flattened.
>
> I think we'll need to figure out how to do this correctly in the bash
> script so that quoted strings get passed in the right way.

I tried a few different approaches but finally ended up giving up; my
bash-fu is apparently not strong enough. If you can make it work
great, but I have "-J" working locally in case you give up like me.
:-)

-- 
Marcelo

From dev-return-7464-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 30 20:51:32 2014
Return-Path: <dev-return-7464-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7778C101FF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Apr 2014 20:51:32 +0000 (UTC)
Received: (qmail 22920 invoked by uid 500); 30 Apr 2014 20:51:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22824 invoked by uid 500); 30 Apr 2014 20:51:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22816 invoked by uid 99); 30 Apr 2014 20:51:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 20:51:30 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.46 as permitted sender)
Received: from [209.85.219.46] (HELO mail-oa0-f46.google.com) (209.85.219.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 20:51:26 +0000
Received: by mail-oa0-f46.google.com with SMTP id i4so1918235oah.5
        for <dev@spark.apache.org>; Wed, 30 Apr 2014 13:51:03 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=bdNL57346zLPy6wivteWKf6nF544gBdOJjc6j/tiXls=;
        b=phoaa6UuOT2ZBVbEsWAg53an0G//ijMIsE/q1uILeVZBbEop80dis1xW2CXseBOaJG
         pX8Epu4//iokydWpOFWAjNPAYB3K8TM+l+57lLeEbtE1oR6Av7xqbOAf47FO2T3/TM4e
         jm1IBUIOE2o1oNpW8qVkb9AivQVLDIDF6BTfCiHg+3vjUu7vh1CCUDS7KtnGn4KsfmXb
         32dPJvqcfhlSAIYS1VbjcJpDXhj89+sHYmybHEbf691vwSd0M50Yr6I5XfDeFwGI6vCj
         SWb1Xd/1Ei2ZGCN1j5YxO8BJRuh/+cO76/jUkHs90+5ZLv1wpJ9SjDXv+MZka3RL4dMV
         M3uw==
MIME-Version: 1.0
X-Received: by 10.182.74.234 with SMTP id x10mr5941985obv.1.1398891063531;
 Wed, 30 Apr 2014 13:51:03 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Wed, 30 Apr 2014 13:51:03 -0700 (PDT)
In-Reply-To: <CAAOnQ7vKYcCoT8n_07+9muxGKa9ptvV_CiUuicuVPT-q9bwGaQ@mail.gmail.com>
References: <CAAOnQ7ukPXuqo21v+d36tk-DcUWQWBmKwc3Sp7okhZF3G4NNUw@mail.gmail.com>
	<CABPQxsv-eRJ+hneVkUofksYpCUzvr31tkqfivuUpb4e0q8ikAQ@mail.gmail.com>
	<CAAOnQ7uv8u0J_hpogGgcfO7_cEk40gRAXD1MSr6M4y3nbFew-g@mail.gmail.com>
	<CABPQxstyQW98FiOqWeq63P=ezZEPuycYiPtPRnr=vp8GjW_dZg@mail.gmail.com>
	<CAAOnQ7vKYcCoT8n_07+9muxGKa9ptvV_CiUuicuVPT-q9bwGaQ@mail.gmail.com>
Date: Wed, 30 Apr 2014 13:51:03 -0700
Message-ID: <CABPQxsumtBxJcZ-WZAHt_3t-iXFntbDXwbU_Rdo8ZefFOvcnVw@mail.gmail.com>
Subject: Re: SparkSubmit and --driver-java-options
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

So I reproduced the problem here:

== test.sh ==
#!/bin/bash
for x in "$@"; do
  echo "arg: $x"
done
ARGS_COPY=$@
for x in "$ARGS_COPY"; do
  echo "arg_copy: $x"
done
==

./test.sh a b "c d e" f
arg: a
arg: b
arg: c d e
arg: f
arg_copy: a b c d e f

I'll dig around a bit more and see if we can fix it. Pretty sure we
aren't passing these argument arrays around correctly in bash.

On Wed, Apr 30, 2014 at 1:48 PM, Marcelo Vanzin <vanzin@cloudera.com> wrote:
> On Wed, Apr 30, 2014 at 1:41 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>> Yeah I think the problem is that the spark-submit script doesn't pass
>> the argument array to spark-class in the right way, so any quoted
>> strings get flattened.
>>
>> I think we'll need to figure out how to do this correctly in the bash
>> script so that quoted strings get passed in the right way.
>
> I tried a few different approaches but finally ended up giving up; my
> bash-fu is apparently not strong enough. If you can make it work
> great, but I have "-J" working locally in case you give up like me.
> :-)
>
> --
> Marcelo

From dev-return-7465-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 30 21:08:10 2014
Return-Path: <dev-return-7465-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1A28D102C5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Apr 2014 21:08:10 +0000 (UTC)
Received: (qmail 50682 invoked by uid 500); 30 Apr 2014 21:08:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50635 invoked by uid 500); 30 Apr 2014 21:08:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50627 invoked by uid 99); 30 Apr 2014 21:08:08 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 21:08:08 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of deanwampler@gmail.com designates 209.85.160.178 as permitted sender)
Received: from [209.85.160.178] (HELO mail-yk0-f178.google.com) (209.85.160.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 21:08:04 +0000
Received: by mail-yk0-f178.google.com with SMTP id 200so2004552ykr.23
        for <dev@spark.apache.org>; Wed, 30 Apr 2014 14:07:43 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=Q64W5PPY75dtxd70UGIjqv8/nPVRkh0eWLgDlur5pNo=;
        b=01ugmACxXIY6HEU0bCbUlz89rq/bC6VBlDt4lGFMYBhJhcq0jRFie3SUuDEd/fhl3m
         jHAyo7f/hdGL970K7d7pbljIlgDYAeUUYnPHPpN7a82MEieqIE73yBQJAxJsY6gt5rFk
         7jONS9FvRjRTOa0CmOU6qwbC4GjFkFMZoz8nT6HSKfWA0aKCRIq4s6SiqjDjCedTeSRn
         qoXa3n1QjG/dtVNc7XC8IAPQ589hgdyZTXdzsdFDq5NNkV+BZREd0jT/CRsYVCrZABeh
         aNhlq4jdmRGn1+EuK+YYL2EhtBRF7jPS+zNIJdfHZvBx7FXPH33O5p1SnAoBb1bLF6u1
         IgaA==
X-Received: by 10.236.23.163 with SMTP id v23mr9552279yhv.58.1398892063649;
 Wed, 30 Apr 2014 14:07:43 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.170.69.11 with HTTP; Wed, 30 Apr 2014 14:07:23 -0700 (PDT)
In-Reply-To: <CABPQxsumtBxJcZ-WZAHt_3t-iXFntbDXwbU_Rdo8ZefFOvcnVw@mail.gmail.com>
References: <CAAOnQ7ukPXuqo21v+d36tk-DcUWQWBmKwc3Sp7okhZF3G4NNUw@mail.gmail.com>
 <CABPQxsv-eRJ+hneVkUofksYpCUzvr31tkqfivuUpb4e0q8ikAQ@mail.gmail.com>
 <CAAOnQ7uv8u0J_hpogGgcfO7_cEk40gRAXD1MSr6M4y3nbFew-g@mail.gmail.com>
 <CABPQxstyQW98FiOqWeq63P=ezZEPuycYiPtPRnr=vp8GjW_dZg@mail.gmail.com>
 <CAAOnQ7vKYcCoT8n_07+9muxGKa9ptvV_CiUuicuVPT-q9bwGaQ@mail.gmail.com> <CABPQxsumtBxJcZ-WZAHt_3t-iXFntbDXwbU_Rdo8ZefFOvcnVw@mail.gmail.com>
From: Dean Wampler <deanwampler@gmail.com>
Date: Wed, 30 Apr 2014 16:07:23 -0500
Message-ID: <CAKW0i0xKjFMV8D+Ee_B6PMEP3rpvED4Z0TpyVOZrs_CVMO4daQ@mail.gmail.com>
Subject: Re: SparkSubmit and --driver-java-options
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e013d070088789504f848f05d
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013d070088789504f848f05d
Content-Type: text/plain; charset=UTF-8

Try this:

#!/bin/bash
for x in "$@"; do
  echo "arg: $x"
done
ARGS_COPY=("$@")     # Make ARGS_COPY an array with the array elements in $@

for x in "${ARGS_COPY[@]}"; do                # preserve array arguments.
  echo "arg_copy: $x"
done



On Wed, Apr 30, 2014 at 3:51 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> So I reproduced the problem here:
>
> == test.sh ==
> #!/bin/bash
> for x in "$@"; do
>   echo "arg: $x"
> done
> ARGS_COPY=$@
> for x in "$ARGS_COPY"; do
>   echo "arg_copy: $x"
> done
> ==
>
> ./test.sh a b "c d e" f
> arg: a
> arg: b
> arg: c d e
> arg: f
> arg_copy: a b c d e f
>
> I'll dig around a bit more and see if we can fix it. Pretty sure we
> aren't passing these argument arrays around correctly in bash.
>
> On Wed, Apr 30, 2014 at 1:48 PM, Marcelo Vanzin <vanzin@cloudera.com>
> wrote:
> > On Wed, Apr 30, 2014 at 1:41 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >> Yeah I think the problem is that the spark-submit script doesn't pass
> >> the argument array to spark-class in the right way, so any quoted
> >> strings get flattened.
> >>
> >> I think we'll need to figure out how to do this correctly in the bash
> >> script so that quoted strings get passed in the right way.
> >
> > I tried a few different approaches but finally ended up giving up; my
> > bash-fu is apparently not strong enough. If you can make it work
> > great, but I have "-J" working locally in case you give up like me.
> > :-)
> >
> > --
> > Marcelo
>



-- 
Dean Wampler, Ph.D.
Typesafe
@deanwampler
http://typesafe.com
http://polyglotprogramming.com

--089e013d070088789504f848f05d--

From dev-return-7466-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 30 21:10:14 2014
Return-Path: <dev-return-7466-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 174E1102DA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Apr 2014 21:10:14 +0000 (UTC)
Received: (qmail 54327 invoked by uid 500); 30 Apr 2014 21:10:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54276 invoked by uid 500); 30 Apr 2014 21:10:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54268 invoked by uid 99); 30 Apr 2014 21:10:12 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 21:10:12 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.46 as permitted sender)
Received: from [209.85.219.46] (HELO mail-oa0-f46.google.com) (209.85.219.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 21:10:08 +0000
Received: by mail-oa0-f46.google.com with SMTP id i4so1948285oah.19
        for <dev@spark.apache.org>; Wed, 30 Apr 2014 14:09:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=60B7CerAmhfO/zGVQrJzGyqbr2jrmfXsYU4Gs5+z91U=;
        b=BUeTBxLUmqPAz94qj4buA2dQjrfcuDWw9XH8kD1qQmZwSKIdALBXI+F8Hx1pjI3yFi
         o8ZlHz8afQx+0PWr/Kh+MZEm9Y9f+SM/dLsXNMaKWzC0qpKAns+eN14k4/MZe/zADlG1
         vfpVhXlevfi1iAAHPwhlGmsuLIVeTE0A9sE+mZTUUA2tWQNnZjXkdwifMDMkwfz3ksIN
         auQLNq3Nag6ufKGiLZ5ltsHQZ4Ql6fkzZhBwcP+RhtKOb39wn0A9IYd+y+h3QRG0aY4p
         tUKqTcDcyum4uBp7HJzlCMjU5Hyse27xj4L+hsgl0Uib3PPszqijvPy6lT6+Zd2bim/w
         BpZw==
MIME-Version: 1.0
X-Received: by 10.60.65.1 with SMTP id t1mr7551979oes.7.1398892187911; Wed, 30
 Apr 2014 14:09:47 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Wed, 30 Apr 2014 14:09:47 -0700 (PDT)
In-Reply-To: <CABPQxsumtBxJcZ-WZAHt_3t-iXFntbDXwbU_Rdo8ZefFOvcnVw@mail.gmail.com>
References: <CAAOnQ7ukPXuqo21v+d36tk-DcUWQWBmKwc3Sp7okhZF3G4NNUw@mail.gmail.com>
	<CABPQxsv-eRJ+hneVkUofksYpCUzvr31tkqfivuUpb4e0q8ikAQ@mail.gmail.com>
	<CAAOnQ7uv8u0J_hpogGgcfO7_cEk40gRAXD1MSr6M4y3nbFew-g@mail.gmail.com>
	<CABPQxstyQW98FiOqWeq63P=ezZEPuycYiPtPRnr=vp8GjW_dZg@mail.gmail.com>
	<CAAOnQ7vKYcCoT8n_07+9muxGKa9ptvV_CiUuicuVPT-q9bwGaQ@mail.gmail.com>
	<CABPQxsumtBxJcZ-WZAHt_3t-iXFntbDXwbU_Rdo8ZefFOvcnVw@mail.gmail.com>
Date: Wed, 30 Apr 2014 14:09:47 -0700
Message-ID: <CABPQxsvpRzXyYbcjnPuvshO16V0pykdpjgs=NOdfT-FMd9Uwmw@mail.gmail.com>
Subject: Re: SparkSubmit and --driver-java-options
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Marcelo - Mind trying the following diff locally? If it works I can
send a patch:

patrick@patrick-t430s:~/Documents/spark$ git diff bin/spark-submit
diff --git a/bin/spark-submit b/bin/spark-submit
index dd0d95d..49bc262 100755
--- a/bin/spark-submit
+++ b/bin/spark-submit
@@ -18,7 +18,7 @@
 #

 export SPARK_HOME="$(cd `dirname $0`/..; pwd)"
-ORIG_ARGS=$@
+ORIG_ARGS=("$@")

 while (($#)); do
   if [ "$1" = "--deploy-mode" ]; then
@@ -39,5 +39,5 @@ if [ ! -z $DRIVER_MEMORY ] && [ ! -z $DEPLOY_MODE ]
&& [ $DEPLOY_MODE = "client"
   export SPARK_MEM=$DRIVER_MEMORY
 fi

-$SPARK_HOME/bin/spark-class org.apache.spark.deploy.SparkSubmit $ORIG_ARGS
+$SPARK_HOME/bin/spark-class org.apache.spark.deploy.SparkSubmit
"${ORIG_ARGS[@]}"

On Wed, Apr 30, 2014 at 1:51 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> So I reproduced the problem here:
>
> == test.sh ==
> #!/bin/bash
> for x in "$@"; do
>   echo "arg: $x"
> done
> ARGS_COPY=$@
> for x in "$ARGS_COPY"; do
>   echo "arg_copy: $x"
> done
> ==
>
> ./test.sh a b "c d e" f
> arg: a
> arg: b
> arg: c d e
> arg: f
> arg_copy: a b c d e f
>
> I'll dig around a bit more and see if we can fix it. Pretty sure we
> aren't passing these argument arrays around correctly in bash.
>
> On Wed, Apr 30, 2014 at 1:48 PM, Marcelo Vanzin <vanzin@cloudera.com> wrote:
>> On Wed, Apr 30, 2014 at 1:41 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>> Yeah I think the problem is that the spark-submit script doesn't pass
>>> the argument array to spark-class in the right way, so any quoted
>>> strings get flattened.
>>>
>>> I think we'll need to figure out how to do this correctly in the bash
>>> script so that quoted strings get passed in the right way.
>>
>> I tried a few different approaches but finally ended up giving up; my
>> bash-fu is apparently not strong enough. If you can make it work
>> great, but I have "-J" working locally in case you give up like me.
>> :-)
>>
>> --
>> Marcelo

From dev-return-7467-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 30 21:15:10 2014
Return-Path: <dev-return-7467-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E929D10307
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Apr 2014 21:15:10 +0000 (UTC)
Received: (qmail 63672 invoked by uid 500); 30 Apr 2014 21:15:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63576 invoked by uid 500); 30 Apr 2014 21:15:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63568 invoked by uid 99); 30 Apr 2014 21:15:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 21:15:09 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vanzin@cloudera.com designates 209.85.216.54 as permitted sender)
Received: from [209.85.216.54] (HELO mail-qa0-f54.google.com) (209.85.216.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 21:15:06 +0000
Received: by mail-qa0-f54.google.com with SMTP id s7so2247749qap.27
        for <dev@spark.apache.org>; Wed, 30 Apr 2014 14:14:43 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=sbBwi8cK9mxpZHe83cORe5kI5Q4cT4HRwOEmjYiNKLo=;
        b=l1eOohAVX+oR3Pmp6wztJeHPsGX5SnxmHxKRjpo8pFwTP4x268R6mZ1870SGNskbRw
         ioL7q+fD5tYHBkMp3LUXRgBdb0tw0iH4NoLV6PmBJX7hMD9vihEmK512LmjJyvzdHbEe
         KjlsJtwbInX7nPQg/nnXNysi/vwQTQf5Hfkd1AxcIt0Pg04fZ6q6F1x+11gXsKolMPUD
         5YzESEU/rVE5ThNi5a6fVkZLASHEjOelJkNefB00lFlr2zfGFi8vQcA7XoJGyUOViRRR
         0T02Y25gynJl0ZzVqXlBn9Ud6B2f1BmrTYmt/4VElfNRsojnK/FAgvB3YbEB9c7F3mtl
         JDQQ==
X-Gm-Message-State: ALoCoQnd9oT84++2tUYSq7ctM8zrh/p0enzQvbx2N06V/1bvimPsJ24/cGKrUEmQfu4gtujDDauV
MIME-Version: 1.0
X-Received: by 10.140.94.39 with SMTP id f36mr8498262qge.64.1398892482955;
 Wed, 30 Apr 2014 14:14:42 -0700 (PDT)
Received: by 10.229.211.5 with HTTP; Wed, 30 Apr 2014 14:14:42 -0700 (PDT)
In-Reply-To: <CABPQxsvpRzXyYbcjnPuvshO16V0pykdpjgs=NOdfT-FMd9Uwmw@mail.gmail.com>
References: <CAAOnQ7ukPXuqo21v+d36tk-DcUWQWBmKwc3Sp7okhZF3G4NNUw@mail.gmail.com>
	<CABPQxsv-eRJ+hneVkUofksYpCUzvr31tkqfivuUpb4e0q8ikAQ@mail.gmail.com>
	<CAAOnQ7uv8u0J_hpogGgcfO7_cEk40gRAXD1MSr6M4y3nbFew-g@mail.gmail.com>
	<CABPQxstyQW98FiOqWeq63P=ezZEPuycYiPtPRnr=vp8GjW_dZg@mail.gmail.com>
	<CAAOnQ7vKYcCoT8n_07+9muxGKa9ptvV_CiUuicuVPT-q9bwGaQ@mail.gmail.com>
	<CABPQxsumtBxJcZ-WZAHt_3t-iXFntbDXwbU_Rdo8ZefFOvcnVw@mail.gmail.com>
	<CABPQxsvpRzXyYbcjnPuvshO16V0pykdpjgs=NOdfT-FMd9Uwmw@mail.gmail.com>
Date: Wed, 30 Apr 2014 14:14:42 -0700
Message-ID: <CAAOnQ7u1v0Quj7NaZf+tpaj=ujzbSke4OaBsZ=jzLfeRGy20XQ@mail.gmail.com>
Subject: Re: SparkSubmit and --driver-java-options
From: Marcelo Vanzin <vanzin@cloudera.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Cool, that seems to work. Thanks!

On Wed, Apr 30, 2014 at 2:09 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Marcelo - Mind trying the following diff locally? If it works I can
> send a patch:
>
> patrick@patrick-t430s:~/Documents/spark$ git diff bin/spark-submit
> diff --git a/bin/spark-submit b/bin/spark-submit
> index dd0d95d..49bc262 100755
> --- a/bin/spark-submit
> +++ b/bin/spark-submit
> @@ -18,7 +18,7 @@
>  #
>
>  export SPARK_HOME="$(cd `dirname $0`/..; pwd)"
> -ORIG_ARGS=$@
> +ORIG_ARGS=("$@")
>
>  while (($#)); do
>    if [ "$1" = "--deploy-mode" ]; then
> @@ -39,5 +39,5 @@ if [ ! -z $DRIVER_MEMORY ] && [ ! -z $DEPLOY_MODE ]
> && [ $DEPLOY_MODE = "client"
>    export SPARK_MEM=$DRIVER_MEMORY
>  fi
>
> -$SPARK_HOME/bin/spark-class org.apache.spark.deploy.SparkSubmit $ORIG_ARGS
> +$SPARK_HOME/bin/spark-class org.apache.spark.deploy.SparkSubmit
> "${ORIG_ARGS[@]}"
>
> On Wed, Apr 30, 2014 at 1:51 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>> So I reproduced the problem here:
>>
>> == test.sh ==
>> #!/bin/bash
>> for x in "$@"; do
>>   echo "arg: $x"
>> done
>> ARGS_COPY=$@
>> for x in "$ARGS_COPY"; do
>>   echo "arg_copy: $x"
>> done
>> ==
>>
>> ./test.sh a b "c d e" f
>> arg: a
>> arg: b
>> arg: c d e
>> arg: f
>> arg_copy: a b c d e f
>>
>> I'll dig around a bit more and see if we can fix it. Pretty sure we
>> aren't passing these argument arrays around correctly in bash.
>>
>> On Wed, Apr 30, 2014 at 1:48 PM, Marcelo Vanzin <vanzin@cloudera.com> wrote:
>>> On Wed, Apr 30, 2014 at 1:41 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>>> Yeah I think the problem is that the spark-submit script doesn't pass
>>>> the argument array to spark-class in the right way, so any quoted
>>>> strings get flattened.
>>>>
>>>> I think we'll need to figure out how to do this correctly in the bash
>>>> script so that quoted strings get passed in the right way.
>>>
>>> I tried a few different approaches but finally ended up giving up; my
>>> bash-fu is apparently not strong enough. If you can make it work
>>> great, but I have "-J" working locally in case you give up like me.
>>> :-)
>>>
>>> --
>>> Marcelo



-- 
Marcelo

From dev-return-7468-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Apr 30 21:27:02 2014
Return-Path: <dev-return-7468-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B19C210366
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Apr 2014 21:27:02 +0000 (UTC)
Received: (qmail 87086 invoked by uid 500); 30 Apr 2014 21:27:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86988 invoked by uid 500); 30 Apr 2014 21:27:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86978 invoked by uid 99); 30 Apr 2014 21:27:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 21:27:00 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.170 as permitted sender)
Received: from [209.85.214.170] (HELO mail-ob0-f170.google.com) (209.85.214.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Apr 2014 21:26:57 +0000
Received: by mail-ob0-f170.google.com with SMTP id vb8so2732073obc.1
        for <dev@spark.apache.org>; Wed, 30 Apr 2014 14:26:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=i3v3YaGTLl7L80jS25W7smT2GcMpYOn+acXZChkI3Fk=;
        b=n+lTSVYX4AKQDR+Okdwj51SmFDJJjHsfC53SI3Y8PvwWXnu3jQ3VfT+238WZZE03ug
         D4FqcvFzDMXeXu5BzKNUmSbs9AFoPPMrm18Mz9rZZ7yt6Qq1DPA+eWXBnC0qRH/xMhz2
         YnVLskzm/jpCERHmRiEm9oMnkTLeLOQw9hnZq7JmQOt7VuLEcXXKd2k8V7uYm8EAWWiD
         fVKVmT6e/5U+6Q1PUxals6JHngej342PSJSqVcJZMcMdFYbgG4H8REwFBZh2C5iMcA4D
         DMJ++7dOnA91/5aaqCn7FgE9YTMqFVIynR8Z6CQjErQ4iMbjkJQchiO6QiaUd/uFT0eS
         b0jg==
MIME-Version: 1.0
X-Received: by 10.60.159.36 with SMTP id wz4mr7740555oeb.30.1398893194311;
 Wed, 30 Apr 2014 14:26:34 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Wed, 30 Apr 2014 14:26:34 -0700 (PDT)
In-Reply-To: <CAAOnQ7u1v0Quj7NaZf+tpaj=ujzbSke4OaBsZ=jzLfeRGy20XQ@mail.gmail.com>
References: <CAAOnQ7ukPXuqo21v+d36tk-DcUWQWBmKwc3Sp7okhZF3G4NNUw@mail.gmail.com>
	<CABPQxsv-eRJ+hneVkUofksYpCUzvr31tkqfivuUpb4e0q8ikAQ@mail.gmail.com>
	<CAAOnQ7uv8u0J_hpogGgcfO7_cEk40gRAXD1MSr6M4y3nbFew-g@mail.gmail.com>
	<CABPQxstyQW98FiOqWeq63P=ezZEPuycYiPtPRnr=vp8GjW_dZg@mail.gmail.com>
	<CAAOnQ7vKYcCoT8n_07+9muxGKa9ptvV_CiUuicuVPT-q9bwGaQ@mail.gmail.com>
	<CABPQxsumtBxJcZ-WZAHt_3t-iXFntbDXwbU_Rdo8ZefFOvcnVw@mail.gmail.com>
	<CABPQxsvpRzXyYbcjnPuvshO16V0pykdpjgs=NOdfT-FMd9Uwmw@mail.gmail.com>
	<CAAOnQ7u1v0Quj7NaZf+tpaj=ujzbSke4OaBsZ=jzLfeRGy20XQ@mail.gmail.com>
Date: Wed, 30 Apr 2014 14:26:34 -0700
Message-ID: <CABPQxssvZHgtp=Z_Fm1MXHj2wBpi=GmkYsOhgCsNiW-ErytX8w@mail.gmail.com>
Subject: Re: SparkSubmit and --driver-java-options
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Dean - our e-mails crossed, but thanks for the tip. Was independently
arriving at your solution :)

Okay I'll submit something.

- Patrick

On Wed, Apr 30, 2014 at 2:14 PM, Marcelo Vanzin <vanzin@cloudera.com> wrote:
> Cool, that seems to work. Thanks!
>
> On Wed, Apr 30, 2014 at 2:09 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>> Marcelo - Mind trying the following diff locally? If it works I can
>> send a patch:
>>
>> patrick@patrick-t430s:~/Documents/spark$ git diff bin/spark-submit
>> diff --git a/bin/spark-submit b/bin/spark-submit
>> index dd0d95d..49bc262 100755
>> --- a/bin/spark-submit
>> +++ b/bin/spark-submit
>> @@ -18,7 +18,7 @@
>>  #
>>
>>  export SPARK_HOME="$(cd `dirname $0`/..; pwd)"
>> -ORIG_ARGS=$@
>> +ORIG_ARGS=("$@")
>>
>>  while (($#)); do
>>    if [ "$1" = "--deploy-mode" ]; then
>> @@ -39,5 +39,5 @@ if [ ! -z $DRIVER_MEMORY ] && [ ! -z $DEPLOY_MODE ]
>> && [ $DEPLOY_MODE = "client"
>>    export SPARK_MEM=$DRIVER_MEMORY
>>  fi
>>
>> -$SPARK_HOME/bin/spark-class org.apache.spark.deploy.SparkSubmit $ORIG_ARGS
>> +$SPARK_HOME/bin/spark-class org.apache.spark.deploy.SparkSubmit
>> "${ORIG_ARGS[@]}"
>>
>> On Wed, Apr 30, 2014 at 1:51 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>> So I reproduced the problem here:
>>>
>>> == test.sh ==
>>> #!/bin/bash
>>> for x in "$@"; do
>>>   echo "arg: $x"
>>> done
>>> ARGS_COPY=$@
>>> for x in "$ARGS_COPY"; do
>>>   echo "arg_copy: $x"
>>> done
>>> ==
>>>
>>> ./test.sh a b "c d e" f
>>> arg: a
>>> arg: b
>>> arg: c d e
>>> arg: f
>>> arg_copy: a b c d e f
>>>
>>> I'll dig around a bit more and see if we can fix it. Pretty sure we
>>> aren't passing these argument arrays around correctly in bash.
>>>
>>> On Wed, Apr 30, 2014 at 1:48 PM, Marcelo Vanzin <vanzin@cloudera.com> wrote:
>>>> On Wed, Apr 30, 2014 at 1:41 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>>>> Yeah I think the problem is that the spark-submit script doesn't pass
>>>>> the argument array to spark-class in the right way, so any quoted
>>>>> strings get flattened.
>>>>>
>>>>> I think we'll need to figure out how to do this correctly in the bash
>>>>> script so that quoted strings get passed in the right way.
>>>>
>>>> I tried a few different approaches but finally ended up giving up; my
>>>> bash-fu is apparently not strong enough. If you can make it work
>>>> great, but I have "-J" working locally in case you give up like me.
>>>> :-)
>>>>
>>>> --
>>>> Marcelo
>
>
>
> --
> Marcelo

