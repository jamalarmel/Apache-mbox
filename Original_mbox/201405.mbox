From dev-return-7469-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May  1 05:37:33 2014
Return-Path: <dev-return-7469-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ED1AE110C3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  1 May 2014 05:37:33 +0000 (UTC)
Received: (qmail 51465 invoked by uid 500); 1 May 2014 05:37:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50924 invoked by uid 500); 1 May 2014 05:37:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50916 invoked by uid 99); 1 May 2014 05:37:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 01 May 2014 05:37:22 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.173 as permitted sender)
Received: from [209.85.214.173] (HELO mail-ob0-f173.google.com) (209.85.214.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 01 May 2014 05:37:18 +0000
Received: by mail-ob0-f173.google.com with SMTP id wn1so3249886obc.32
        for <dev@spark.apache.org>; Wed, 30 Apr 2014 22:36:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=toiIrkHzKoApGLMahmAH8KoEFYuVECyMs7HXsp39ZIo=;
        b=ED1iY2gVkYpVygKGm74g3BEYH4+vKAGSRyxbZivZrkeB8IAu5SYVT21adZQoGswof0
         CeB2w7PP/QFYPhVo8xKN20nvzUG85kruAFf8yuxmzPSj8fjoQCIjDis2s3VfxRCk9M06
         rS/Grw1/TZ1AgaOGrWraIo7/qCP9AGnKyg9ALGfspfYad8L9msdLGOesLSNt2h6tq3Qz
         R6N/xm0JdfhbuuP0jkQSZxbnmnaLz6gleDO6WneTqz0Mv/m9rEd0zRmcr80X9H09Ggif
         lME0QOSOaSsNZ4I5lSizrxY7+W0fI1bfvu7QWqJwx6rNDqpAethKk9TDKJjWLUOfSImV
         xRvQ==
MIME-Version: 1.0
X-Received: by 10.182.144.194 with SMTP id so2mr7705192obb.31.1398922615555;
 Wed, 30 Apr 2014 22:36:55 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Wed, 30 Apr 2014 22:36:55 -0700 (PDT)
In-Reply-To: <CABPQxssvZHgtp=Z_Fm1MXHj2wBpi=GmkYsOhgCsNiW-ErytX8w@mail.gmail.com>
References: <CAAOnQ7ukPXuqo21v+d36tk-DcUWQWBmKwc3Sp7okhZF3G4NNUw@mail.gmail.com>
	<CABPQxsv-eRJ+hneVkUofksYpCUzvr31tkqfivuUpb4e0q8ikAQ@mail.gmail.com>
	<CAAOnQ7uv8u0J_hpogGgcfO7_cEk40gRAXD1MSr6M4y3nbFew-g@mail.gmail.com>
	<CABPQxstyQW98FiOqWeq63P=ezZEPuycYiPtPRnr=vp8GjW_dZg@mail.gmail.com>
	<CAAOnQ7vKYcCoT8n_07+9muxGKa9ptvV_CiUuicuVPT-q9bwGaQ@mail.gmail.com>
	<CABPQxsumtBxJcZ-WZAHt_3t-iXFntbDXwbU_Rdo8ZefFOvcnVw@mail.gmail.com>
	<CABPQxsvpRzXyYbcjnPuvshO16V0pykdpjgs=NOdfT-FMd9Uwmw@mail.gmail.com>
	<CAAOnQ7u1v0Quj7NaZf+tpaj=ujzbSke4OaBsZ=jzLfeRGy20XQ@mail.gmail.com>
	<CABPQxssvZHgtp=Z_Fm1MXHj2wBpi=GmkYsOhgCsNiW-ErytX8w@mail.gmail.com>
Date: Wed, 30 Apr 2014 22:36:55 -0700
Message-ID: <CABPQxsuQMm6cwyK7mc9w1UB4m5mHvXs7qtH49-NgfJFy-jLzug@mail.gmail.com>
Subject: Re: SparkSubmit and --driver-java-options
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Patch here:
https://github.com/apache/spark/pull/609

On Wed, Apr 30, 2014 at 2:26 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Dean - our e-mails crossed, but thanks for the tip. Was independently
> arriving at your solution :)
>
> Okay I'll submit something.
>
> - Patrick
>
> On Wed, Apr 30, 2014 at 2:14 PM, Marcelo Vanzin <vanzin@cloudera.com> wrote:
>> Cool, that seems to work. Thanks!
>>
>> On Wed, Apr 30, 2014 at 2:09 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>> Marcelo - Mind trying the following diff locally? If it works I can
>>> send a patch:
>>>
>>> patrick@patrick-t430s:~/Documents/spark$ git diff bin/spark-submit
>>> diff --git a/bin/spark-submit b/bin/spark-submit
>>> index dd0d95d..49bc262 100755
>>> --- a/bin/spark-submit
>>> +++ b/bin/spark-submit
>>> @@ -18,7 +18,7 @@
>>>  #
>>>
>>>  export SPARK_HOME="$(cd `dirname $0`/..; pwd)"
>>> -ORIG_ARGS=$@
>>> +ORIG_ARGS=("$@")
>>>
>>>  while (($#)); do
>>>    if [ "$1" = "--deploy-mode" ]; then
>>> @@ -39,5 +39,5 @@ if [ ! -z $DRIVER_MEMORY ] && [ ! -z $DEPLOY_MODE ]
>>> && [ $DEPLOY_MODE = "client"
>>>    export SPARK_MEM=$DRIVER_MEMORY
>>>  fi
>>>
>>> -$SPARK_HOME/bin/spark-class org.apache.spark.deploy.SparkSubmit $ORIG_ARGS
>>> +$SPARK_HOME/bin/spark-class org.apache.spark.deploy.SparkSubmit
>>> "${ORIG_ARGS[@]}"
>>>
>>> On Wed, Apr 30, 2014 at 1:51 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>>> So I reproduced the problem here:
>>>>
>>>> == test.sh ==
>>>> #!/bin/bash
>>>> for x in "$@"; do
>>>>   echo "arg: $x"
>>>> done
>>>> ARGS_COPY=$@
>>>> for x in "$ARGS_COPY"; do
>>>>   echo "arg_copy: $x"
>>>> done
>>>> ==
>>>>
>>>> ./test.sh a b "c d e" f
>>>> arg: a
>>>> arg: b
>>>> arg: c d e
>>>> arg: f
>>>> arg_copy: a b c d e f
>>>>
>>>> I'll dig around a bit more and see if we can fix it. Pretty sure we
>>>> aren't passing these argument arrays around correctly in bash.
>>>>
>>>> On Wed, Apr 30, 2014 at 1:48 PM, Marcelo Vanzin <vanzin@cloudera.com> wrote:
>>>>> On Wed, Apr 30, 2014 at 1:41 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>>>>> Yeah I think the problem is that the spark-submit script doesn't pass
>>>>>> the argument array to spark-class in the right way, so any quoted
>>>>>> strings get flattened.
>>>>>>
>>>>>> I think we'll need to figure out how to do this correctly in the bash
>>>>>> script so that quoted strings get passed in the right way.
>>>>>
>>>>> I tried a few different approaches but finally ended up giving up; my
>>>>> bash-fu is apparently not strong enough. If you can make it work
>>>>> great, but I have "-J" working locally in case you give up like me.
>>>>> :-)
>>>>>
>>>>> --
>>>>> Marcelo
>>
>>
>>
>> --
>> Marcelo

From dev-return-7470-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May  1 10:07:32 2014
Return-Path: <dev-return-7470-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2C74C11575
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  1 May 2014 10:07:32 +0000 (UTC)
Received: (qmail 98918 invoked by uid 500); 1 May 2014 10:07:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98512 invoked by uid 500); 1 May 2014 10:07:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98492 invoked by uid 99); 1 May 2014 10:07:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 01 May 2014 10:07:23 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10
	tests=RCVD_IN_DNSWL_LOW,SPF_HELO_PASS,SPF_SOFTFAIL
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of nicolas.lalevee@hibnet.org does not designate 216.86.168.182 as permitted sender)
Received: from [216.86.168.182] (HELO mxout-07.mxes.net) (216.86.168.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 01 May 2014 10:07:17 +0000
Received: from [192.168.1.21] (unknown [86.68.205.116])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by smtp.mxes.net (Postfix) with ESMTPSA id B851B22E1F4
	for <spark-dev@apache.org>; Thu,  1 May 2014 06:06:55 -0400 (EDT)
From: =?iso-8859-1?Q?Nicolas_Lalev=E9e?= <nicolas.lalevee@hibnet.org>
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: quoted-printable
Subject: Mailing list
Message-Id: <13B7C349-0F74-4D2E-B0E6-536A270F8B19@hibnet.org>
Date: Thu, 1 May 2014 12:06:53 +0200
To: spark-dev@apache.org
Mime-Version: 1.0 (Mac OS X Mail 6.6 \(1510\))
X-Mailer: Apple Mail (2.1510)
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

Your website seems a little bit incomplete. I have found this page [1] =
with list the two main mailing lists, users and dev. But I see a =
reference to a mailing list about "issues" which tracks the sparks =
issues when it was hosted at Atlassian. I guess it has moved ? where ?
And is there any mailing about the commits ?

Also, I found it weird that there is no page that is referencing the =
true code source, the git at the ASF, I only found references to the git =
at github.

This kind of stuff is important for me when I look at an opensource =
project. Looking at jira workflows, dev discussion, frequency of =
commits, often tells me about the quality of the community hence the =
software. And what is great at the ASF, is that almost all of that =
information is open.

I am also interested in your workflow, because Ant is moving from svn to =
git and we're still a little bit in the grey about the workflow. I am =
thus intrigued how do you work with github pull requests.

Nicolas

[1] https://spark.apache.org/community.html


From dev-return-7471-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May  1 15:54:19 2014
Return-Path: <dev-return-7471-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DDDBE101C6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  1 May 2014 15:54:18 +0000 (UTC)
Received: (qmail 33831 invoked by uid 500); 1 May 2014 15:54:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33737 invoked by uid 500); 1 May 2014 15:54:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33729 invoked by uid 99); 1 May 2014 15:54:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 01 May 2014 15:54:16 +0000
X-ASF-Spam-Status: No, hits=-5.8 required=10.0
	tests=ENV_AND_HDR_SPF_MATCH,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,USER_IN_DEF_SPF_WL
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of vmuttineni@ebay.com designates 216.113.172.65 as permitted sender)
Received: from [216.113.172.65] (HELO phx-mipot-002.corp.ebay.com) (216.113.172.65)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 01 May 2014 15:54:11 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=simple/simple;
  d=ebay.com; i=@ebay.com; q=dns/txt; s=ebaycorp;
  t=1398959651; x=1430495651;
  h=from:to:subject:date:message-id:mime-version;
  bh=tAs56xZZWM2bOVf4NeGp0qbkY2Cfe01MLe9OxDR9BZo=;
  b=SIf/JdePsi/9TfEz7bSKiZZBukBrKof42++HF2AjdEfzsnDyQ3LE4Nku
   /JJ08QIojMLblOOWs138i0JpQoEdF9c2WLuJJQ99ND7PCHhoR2fMHk/5u
   CuQxtPiSIWAdJ9exrTDbtDbqnlDclHzZCYNeT1nI+N+pNukRR0tnK3L8/
   Y=;
X-EBay-Corp: Yes
X-IronPort-AV: E=Sophos;i="4.97,965,1389772800"; 
   d="scan'208,217";a="202807233"
Received: from phx-vteml-004.corp.ebay.com (HELO PHX-EXMHT-002.corp.ebay.com) ([10.58.40.103])
  by phx-mipot-002.corp.ebay.com with ESMTP; 01 May 2014 08:53:50 -0700
Received: from PHX-EXRDA-S71.corp.ebay.com ([169.254.4.135]) by
 PHX-EXMHT-002.corp.ebay.com ([10.58.12.72]) with mapi id 14.03.0174.001; Thu,
 1 May 2014 08:52:51 -0700
From: "Muttineni, Vinay" <vmuttineni@ebay.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Spark R Interface
Thread-Topic: Spark R Interface
Thread-Index: Ac9lVSd+M7lrWsSdRYOOB+u1zTxB0g==
Date: Thu, 1 May 2014 15:52:51 +0000
Message-ID: <66B0336C091EE542A3F5EC764B63BE1C10FBF09B@PHX-EXRDA-S71.corp.ebay.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.58.61.242]
Content-Type: multipart/alternative;
	boundary="_000_66B0336C091EE542A3F5EC764B63BE1C10FBF09BPHXEXRDAS71corp_"
MIME-Version: 1.0
X-CFilter: Scanned
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_66B0336C091EE542A3F5EC764B63BE1C10FBF09BPHXEXRDAS71corp_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Hi All,
Any news/updates on the Spark R interface?
I see from here (http://mail-archives.apache.org/mod_mbox/spark-user/201401=
.mbox/%3CCAGiTW48+otjXNgTJ=3DtWG6faNgkG-NU4Bg31pxNmf7HLzvezvWA@mail.gmail.c=
om%3E ) that an alpha version was to be released in January?
Thanks
Vinay

--_000_66B0336C091EE542A3F5EC764B63BE1C10FBF09BPHXEXRDAS71corp_--

From dev-return-7472-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May  1 16:22:27 2014
Return-Path: <dev-return-7472-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B601110347
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  1 May 2014 16:22:27 +0000 (UTC)
Received: (qmail 15736 invoked by uid 500); 1 May 2014 16:22:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15688 invoked by uid 500); 1 May 2014 16:22:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15680 invoked by uid 99); 1 May 2014 16:22:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 01 May 2014 16:22:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of shivaram@berkeley.edu designates 209.85.212.180 as permitted sender)
Received: from [209.85.212.180] (HELO mail-wi0-f180.google.com) (209.85.212.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 01 May 2014 16:22:18 +0000
Received: by mail-wi0-f180.google.com with SMTP id hi5so95288wib.7
        for <dev@spark.apache.org>; Thu, 01 May 2014 09:21:57 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:in-reply-to:references
         :date:message-id:subject:from:to:content-type;
        bh=mbAFc+jgrK46yGHhxF9+0pIN718aa7NRO6jUiq6/ks0=;
        b=LmaWT9vhk70HQNqI3aSB+abW1TEJLJX+28QZWpFcV3d872YzEqkYDZOtyY4yemIVJH
         KqCYZi73G4oXMfSP8NEOvx3+BkBVXXqFamS2IW14yUKGzaXkMBcxg/sqrOsncVUzRADx
         SM34uSHFrxmhV5JZmj8gEfdDesDzptEfK/Y2tS3qXHc8jfNsVv4PijySoKoF7XzyA7uL
         3VX9kHb/Oxm67uyzQtKj4foano9sn01XYz5o0Sk9HK34Z1yabO0g49p/8yN2B9e8KLjz
         xcCUiM/eqQa2sVClJfaNmkrbXb3QFaNRWeZitO+0NwfjOugT7rWfkbFVMLGDKbIRWuSb
         ul4g==
X-Gm-Message-State: ALoCoQk0tD6EeoQV70fuhDL3QJiHedJ+1Zw3w1rLmB7QA1KWT33ZPbr2UETR68U4IT0XceKM5qsh
MIME-Version: 1.0
X-Received: by 10.180.93.41 with SMTP id cr9mr2847922wib.7.1398961317303; Thu,
 01 May 2014 09:21:57 -0700 (PDT)
Reply-To: shivaram@eecs.berkeley.edu
Received: by 10.216.92.11 with HTTP; Thu, 1 May 2014 09:21:57 -0700 (PDT)
In-Reply-To: <66B0336C091EE542A3F5EC764B63BE1C10FBF09B@PHX-EXRDA-S71.corp.ebay.com>
References: <66B0336C091EE542A3F5EC764B63BE1C10FBF09B@PHX-EXRDA-S71.corp.ebay.com>
Date: Thu, 1 May 2014 09:21:57 -0700
Message-ID: <CAKx7Bf_ZqQCMud3hmtFDOFesejyvx++D5JB3bLKdrag5x=xdbA@mail.gmail.com>
Subject: Re: Spark R Interface
From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d043893575f62c104f859100e
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043893575f62c104f859100e
Content-Type: text/plain; charset=UTF-8

There is a alpha version of SparkR that you can use from
https://github.com/amplab-extras/SparkR-pkg -- It works with Spark 0.9.0
and above and has most of the basic RDD functions implemented.

Thanks
Shivaram


On Thu, May 1, 2014 at 8:52 AM, Muttineni, Vinay <vmuttineni@ebay.com>wrote:

> Hi All,
> Any news/updates on the Spark R interface?
> I see from here (
> http://mail-archives.apache.org/mod_mbox/spark-user/201401.mbox/%3CCAGiTW48+otjXNgTJ=tWG6faNgkG-NU4Bg31pxNmf7HLzvezvWA@mail.gmail.com%3E) that an alpha version was to be released in January?
> Thanks
> Vinay
>

--f46d043893575f62c104f859100e--

From dev-return-7473-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May  1 17:41:59 2014
Return-Path: <dev-return-7473-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E349610675
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  1 May 2014 17:41:58 +0000 (UTC)
Received: (qmail 68364 invoked by uid 500); 1 May 2014 17:41:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68240 invoked by uid 500); 1 May 2014 17:41:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68232 invoked by uid 99); 1 May 2014 17:41:57 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 01 May 2014 17:41:57 +0000
X-ASF-Spam-Status: No, hits=-13.0 required=10.0
	tests=ENV_AND_HDR_SPF_MATCH,RCVD_IN_DNSWL_HI,SPF_PASS,USER_IN_DEF_SPF_WL
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of vmuttineni@ebay.com designates 216.113.175.153 as permitted sender)
Received: from [216.113.175.153] (HELO den-mipot-002.corp.ebay.com) (216.113.175.153)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 01 May 2014 17:41:53 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=simple/simple;
  d=ebay.com; i=@ebay.com; q=dns/txt; s=ebaycorp;
  t=1398966113; x=1430502113;
  h=from:to:subject:date:message-id:references:in-reply-to:
   content-transfer-encoding:mime-version;
  bh=pvx4MLgQPtxyVtIHzwQcPh/sa9hyGQJdYkwDaFbVYfg=;
  b=OY+YY3zKys+aXiWKsbCVfHVaW3eoiuNqn0eTRPniVwbQvzQw7uWmt1M6
   CiJKJhag0BxUSnY9nkkzZJgi0MT1zbNERv/txA5QX1PEtYuyQ17m3fwqQ
   mkpMdRsZCROOVPQoqF+X0DMCMDfEt5U33yZOzyFun3vN+mmS5CBbU94Bd
   M=;
X-EBay-Corp: Yes
X-IronPort-AV: E=Sophos;i="4.97,966,1389772800"; 
   d="scan'208";a="48615557"
Received: from den-vteml-001.corp.ebay.com (HELO PHX-EXMHT-003.corp.ebay.com) ([10.101.112.212])
  by den-mipot-002.corp.ebay.com with ESMTP; 01 May 2014 10:41:31 -0700
Received: from PHX-EXRDA-S71.corp.ebay.com ([169.254.4.135]) by
 PHX-EXMHT-003.corp.ebay.com ([10.58.12.74]) with mapi id 14.03.0174.001; Thu,
 1 May 2014 10:40:52 -0700
From: "Muttineni, Vinay" <vmuttineni@ebay.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>,
	"shivaram@eecs.berkeley.edu" <shivaram@eecs.berkeley.edu>
Subject: RE: Spark R Interface
Thread-Topic: Spark R Interface
Thread-Index: Ac9lVSd+M7lrWsSdRYOOB+u1zTxB0gAPv2qAAAvq+KA=
Date: Thu, 1 May 2014 17:40:52 +0000
Message-ID: <66B0336C091EE542A3F5EC764B63BE1C10FBF27C@PHX-EXRDA-S71.corp.ebay.com>
References: <66B0336C091EE542A3F5EC764B63BE1C10FBF09B@PHX-EXRDA-S71.corp.ebay.com>
 <CAKx7Bf_ZqQCMud3hmtFDOFesejyvx++D5JB3bLKdrag5x=xdbA@mail.gmail.com>
In-Reply-To: <CAKx7Bf_ZqQCMud3hmtFDOFesejyvx++D5JB3bLKdrag5x=xdbA@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.58.61.242]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-CFilter: Scanned den1
X-Virus-Checked: Checked by ClamAV on apache.org

VGhhbmsgeW91IFNoaXZhcmFtDQoNCi0tLS0tT3JpZ2luYWwgTWVzc2FnZS0tLS0tDQpGcm9tOiBT
aGl2YXJhbSBWZW5rYXRhcmFtYW4gW21haWx0bzpzaGl2YXJhbUBlZWNzLmJlcmtlbGV5LmVkdV0g
DQpTZW50OiBUaHVyc2RheSwgTWF5IDAxLCAyMDE0IDk6MjIgQU0NClRvOiBkZXZAc3BhcmsuYXBh
Y2hlLm9yZw0KU3ViamVjdDogUmU6IFNwYXJrIFIgSW50ZXJmYWNlDQoNClRoZXJlIGlzIGEgYWxw
aGEgdmVyc2lvbiBvZiBTcGFya1IgdGhhdCB5b3UgY2FuIHVzZSBmcm9tIGh0dHBzOi8vZ2l0aHVi
LmNvbS9hbXBsYWItZXh0cmFzL1NwYXJrUi1wa2cgLS0gSXQgd29ya3Mgd2l0aCBTcGFyayAwLjku
MCBhbmQgYWJvdmUgYW5kIGhhcyBtb3N0IG9mIHRoZSBiYXNpYyBSREQgZnVuY3Rpb25zIGltcGxl
bWVudGVkLg0KDQpUaGFua3MNClNoaXZhcmFtDQoNCg0KT24gVGh1LCBNYXkgMSwgMjAxNCBhdCA4
OjUyIEFNLCBNdXR0aW5lbmksIFZpbmF5IDx2bXV0dGluZW5pQGViYXkuY29tPndyb3RlOg0KDQo+
IEhpIEFsbCwNCj4gQW55IG5ld3MvdXBkYXRlcyBvbiB0aGUgU3BhcmsgUiBpbnRlcmZhY2U/DQo+
IEkgc2VlIGZyb20gaGVyZSAoDQo+IGh0dHA6Ly9tYWlsLWFyY2hpdmVzLmFwYWNoZS5vcmcvbW9k
X21ib3gvc3BhcmstdXNlci8yMDE0MDEubWJveC8lM0NDQUdpVFc0OCtvdGpYTmdUSj10V0c2ZmFO
Z2tHLU5VNEJnMzFweE5tZjdITHp2ZXp2V0FAbWFpbC5nbWFpbC5jb20lM0UpIHRoYXQgYW4gYWxw
aGEgdmVyc2lvbiB3YXMgdG8gYmUgcmVsZWFzZWQgaW4gSmFudWFyeT8NCj4gVGhhbmtzDQo+IFZp
bmF5DQo+DQo=

From dev-return-7474-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May  2 01:46:15 2014
Return-Path: <dev-return-7474-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8684E1189E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  2 May 2014 01:46:15 +0000 (UTC)
Received: (qmail 66506 invoked by uid 500); 2 May 2014 01:46:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66460 invoked by uid 500); 2 May 2014 01:46:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66452 invoked by uid 99); 2 May 2014 01:46:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 May 2014 01:46:13 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_QUOTING
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of suryavanshi.manu@gmail.com designates 209.85.215.46 as permitted sender)
Received: from [209.85.215.46] (HELO mail-la0-f46.google.com) (209.85.215.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 May 2014 01:46:09 +0000
Received: by mail-la0-f46.google.com with SMTP id pv20so2635515lab.5
        for <dev@spark.apache.org>; Thu, 01 May 2014 18:45:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=bu+ad66UBPOIddrlUHOziXqY2TmIg7znf8D+fIMsjdQ=;
        b=y9TeMqGhUBckIfweXO/xreMgvkU23AJ32sBquVhC+yLXrabjU0A7Iu2CQqRXTQ4a6a
         t0jEmZ4dC7vCU0N1C1L4qzBbZ05uzE1YQWZxnteM6BkxZ9he8hgkAASMFQuDniEAsYZ7
         2s6BQUGGT/J9GMMkwNkYF/1r5OsH8aMb8tinAK8V0qWvpIux6yThkju5NgWMRia+EbEz
         0twqgMEiOxal4g/Ke68WHkwRPcapF+KHZ7sy5aOc1OfFkWQlhN8l3EQomlpO5wL8wTNU
         OsDlrQs2Y58pRbJJc1YHb5jcHrA84XtOc+cSKmzqeWIkRr0YRq+TxdqFov/eI8ZHiMyx
         4ZFw==
MIME-Version: 1.0
X-Received: by 10.112.200.130 with SMTP id js2mr9486599lbc.28.1398995147560;
 Thu, 01 May 2014 18:45:47 -0700 (PDT)
Received: by 10.112.166.230 with HTTP; Thu, 1 May 2014 18:45:47 -0700 (PDT)
In-Reply-To: <CABPQxsveNgm_VbA6sygXYNjzmj5tZO1AX6V5An3uF-f2kMznAg@mail.gmail.com>
References: <CABPQxstL6nwTO2H9p8=GJh1g2zxOJd02Wt7L06mCLjo-vwwG9Q@mail.gmail.com>
	<CAKW0i0yEHeMQL7ve0=kV=U7DBLYXDY9+aeydohnpwu9Ybesu1Q@mail.gmail.com>
	<CABPQxsudkXB02HObvgK1=bHgDYjwVMsxQ79zQ++35N-nvR+1Bg@mail.gmail.com>
	<CAKW0i0yLR0xxE-CFcH+uRjyRxdi=0aK4GFLHbXjj3GBP-CzbkA@mail.gmail.com>
	<CABPQxsveNgm_VbA6sygXYNjzmj5tZO1AX6V5An3uF-f2kMznAg@mail.gmail.com>
Date: Thu, 1 May 2014 18:45:47 -0700
Message-ID: <CADJJraJ8UP_kNpVwHOjUWwqPTygvj0d4U=iGS=D9hwmjV9oCNQ@mail.gmail.com>
Subject: Re: Spark 1.0.0 rc3
From: Manu Suryavansh <suryavanshi.manu@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/related; boundary=001a11c331b6d0352d04f860f02a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c331b6d0352d04f860f02a
Content-Type: multipart/alternative; boundary=001a11c331b6d0352a04f860f029

--001a11c331b6d0352a04f860f029
Content-Type: text/plain; charset=UTF-8

Hi,

I tried to build the 1.0.0 rc3 version with Java 8 and I got the error
- java.util.concurrent.ExecutionException: java.lang.OutOfMemoryError: GC
overhead limit exceeded
I am building on a Core-i7(Quad core) windows laptop with 8 GB RAM.

Earlier I had tried to build Spark 0.9.1 with Java 8 and I had gotten an
error about comparator.class not found - which was mentioned today on
another thread, so I am not getting that error now. I have successfully
build Spark 0.9.0 with Java 1.7.

[image: Inline image 1]

Thanks,
Manu


On Tue, Apr 29, 2014 at 10:43 PM, Patrick Wendell <pwendell@gmail.com>wrote:

> That suggestion got lost along the way and IIRC the patch didn't have
> that. It's a good idea though, if nothing else to provide a simple
> means for backwards compatibility.
>
> I created a JIRA for this. It's very straightforward so maybe someone
> can pick it up quickly:
> https://issues.apache.org/jira/browse/SPARK-1677
>
>
> On Tue, Apr 29, 2014 at 2:20 PM, Dean Wampler <deanwampler@gmail.com>
> wrote:
> > Thanks. I'm fine with the logic change, although I was a bit surprised to
> > see Hadoop used for file I/O.
> >
> > Anyway, the jira issue and pull request discussions mention a flag to
> > enable overwrites. That would be very convenient for a tutorial I'm
> > writing, although I wouldn't recommend it for normal use, of course.
> > However, I can't figure out if this actually exists. I found the
> > spark.files.overwrite property, but that doesn't apply.  Does this
> override
> > flag, method call, or method argument actually exist?
> >
> > Thanks,
> > Dean
> >
> >
> > On Tue, Apr 29, 2014 at 1:54 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >
> >> Hi Dean,
> >>
> >> We always used the Hadoop libraries here to read and write local
> >> files. In Spark 1.0 we started enforcing the rule that you can't
> >> over-write an existing directory because it can cause
> >> confusing/undefined behavior if multiple jobs output to the directory
> >> (they partially clobber each other's output).
> >>
> >> https://issues.apache.org/jira/browse/SPARK-1100
> >> https://github.com/apache/spark/pull/11
> >>
> >> In the JIRA I actually proposed slightly deviating from Hadoop
> >> semantics and allowing the directory to exist if it is empty, but I
> >> think in the end we decided to just go with the exact same semantics
> >> as Hadoop (i.e. empty directories are a problem).
> >>
> >> - Patrick
> >>
> >> On Tue, Apr 29, 2014 at 9:43 AM, Dean Wampler <deanwampler@gmail.com>
> >> wrote:
> >> > I'm observing one anomalous behavior. With the 1.0.0 libraries, it's
> >> using
> >> > HDFS classes for file I/O, while the same script compiled and running
> >> with
> >> > 0.9.1 uses only the local-mode File IO.
> >> >
> >> > The script is a variation of the Word Count script. Here are the
> "guts":
> >> >
> >> > object WordCount2 {
> >> >   def main(args: Array[String]) = {
> >> >
> >> >     val sc = new SparkContext("local", "Word Count (2)")
> >> >
> >> >     val input = sc.textFile(".../some/local/file").map(line =>
> >> > line.toLowerCase)
> >> >     input.cache
> >> >
> >> >     val wc2 = input
> >> >       .flatMap(line => line.split("""\W+"""))
> >> >       .map(word => (word, 1))
> >> >       .reduceByKey((count1, count2) => count1 + count2)
> >> >
> >> >     wc2.saveAsTextFile("output/some/directory")
> >> >
> >> >     sc.stop()
> >> >
> >> > It works fine compiled and executed with 0.9.1. If I recompile and run
> >> with
> >> > 1.0.0-RC1, where the same output directory still exists, I get this
> >> > familiar Hadoop-ish exception:
> >> >
> >> > [error] (run-main-0)
> org.apache.hadoop.mapred.FileAlreadyExistsException:
> >> > Output directory
> >> >
> >>
> file:/Users/deanwampler/projects/typesafe/activator/activator-spark/output/kjv-wc
> >> > already exists
> >> > org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory
> >> >
> >>
> file:/Users/deanwampler/projects/typesafe/activator/activator-spark/output/kjv-wc
> >> > already exists
> >> >  at
> >> >
> >>
> org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:121)
> >> > at
> >> >
> >>
> org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:749)
> >> >  at
> >> >
> >>
> org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:662)
> >> > at
> >> >
> >>
> org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:581)
> >> >  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1057)
> >> > at spark.activator.WordCount2$.main(WordCount2.scala:42)
> >> >  at spark.activator.WordCount2.main(WordCount2.scala)
> >> > ...
> >> >
> >> > Thoughts?
> >> >
> >> >
> >> > On Tue, Apr 29, 2014 at 3:05 AM, Patrick Wendell <pwendell@gmail.com>
> >> wrote:
> >> >
> >> >> Hey All,
> >> >>
> >> >> This is not an official vote, but I wanted to cut an RC so that
> people
> >> can
> >> >> test against the Maven artifacts, test building with their
> >> configuration,
> >> >> etc. We are still chasing down a few issues and updating docs, etc.
> >> >>
> >> >> If you have issues or bug reports for this release, please send an
> >> e-mail
> >> >> to the Spark dev list and/or file a JIRA.
> >> >>
> >> >> Commit: d636772 (v1.0.0-rc3)
> >> >>
> >> >>
> >>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d636772ea9f98e449a038567b7975b1a07de3221
> >> >>
> >> >> Binaries:
> >> >> http://people.apache.org/~pwendell/spark-1.0.0-rc3/
> >> >>
> >> >> Docs:
> >> >> http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/
> >> >>
> >> >> Repository:
> >> >>
> https://repository.apache.org/content/repositories/orgapachespark-1012/
> >> >>
> >> >> == API Changes ==
> >> >> If you want to test building against Spark there are some minor API
> >> >> changes. We'll get these written up for the final release but I'm
> >> noting a
> >> >> few here (not comprehensive):
> >> >>
> >> >> changes to ML vector specification:
> >> >>
> >> >>
> >>
> http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/mllib-guide.html#from-09-to-10
> >> >>
> >> >> changes to the Java API:
> >> >>
> >> >>
> >>
> http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
> >> >>
> >> >> coGroup and related functions now return Iterable[T] instead of
> Seq[T]
> >> >> ==> Call toSeq on the result to restore the old behavior
> >> >>
> >> >> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> >> >> ==> Call toSeq on the result to restore old behavior
> >> >>
> >> >> Streaming classes have been renamed:
> >> >> NetworkReceiver -> Receiver
> >> >>
> >> >
> >> >
> >> >
> >> > --
> >> > Dean Wampler, Ph.D.
> >> > Typesafe
> >> > @deanwampler
> >> > http://typesafe.com
> >> > http://polyglotprogramming.com
> >>
> >
> >
> >
> > --
> > Dean Wampler, Ph.D.
> > Typesafe
> > @deanwampler
> > http://typesafe.com
> > http://polyglotprogramming.com
>



-- 
Manu Suryavansh

--001a11c331b6d0352a04f860f029
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi,<div><br></div><div>I tried to build the 1.0.0 rc3 vers=
ion with Java 8 and I got the error -=C2=A0java.util.concurrent.ExecutionEx=
ception: java.lang.OutOfMemoryError: GC overhead limit exceeded</div><div>I=
 am building on a Core-i7(Quad core) windows laptop with 8 GB RAM.</div>
<div><br></div><div>Earlier I had tried to build Spark 0.9.1 with Java 8 an=
d I had gotten an error about comparator.class not found - which was mentio=
ned today on another thread, so I am not getting that error now. I have suc=
cessfully build Spark 0.9.0 with Java 1.7.</div>

<div><br></div><div><img src=3D"cid:ii_145ba28eb174fbba" alt=3D"Inline imag=
e 1" width=3D"562" height=3D"337.94995829858215"><br></div><div><br></div><=
div>Thanks,</div><div>Manu</div></div><div class=3D"gmail_extra"><br><br><d=
iv class=3D"gmail_quote">
On Tue, Apr 29, 2014 at 10:43 PM, Patrick Wendell <span dir=3D"ltr">&lt;<a =
href=3D"mailto:pwendell@gmail.com" target=3D"_blank">pwendell@gmail.com</a>=
&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0 0=
 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
That suggestion got lost along the way and IIRC the patch didn&#39;t have<b=
r>
that. It&#39;s a good idea though, if nothing else to provide a simple<br>
means for backwards compatibility.<br>
<br>
I created a JIRA for this. It&#39;s very straightforward so maybe someone<b=
r>
can pick it up quickly:<br>
<a href=3D"https://issues.apache.org/jira/browse/SPARK-1677" target=3D"_bla=
nk">https://issues.apache.org/jira/browse/SPARK-1677</a><br>
<div class=3D"HOEnZb"><div class=3D"h5"><br>
<br>
On Tue, Apr 29, 2014 at 2:20 PM, Dean Wampler &lt;<a href=3D"mailto:deanwam=
pler@gmail.com">deanwampler@gmail.com</a>&gt; wrote:<br>
&gt; Thanks. I&#39;m fine with the logic change, although I was a bit surpr=
ised to<br>
&gt; see Hadoop used for file I/O.<br>
&gt;<br>
&gt; Anyway, the jira issue and pull request discussions mention a flag to<=
br>
&gt; enable overwrites. That would be very convenient for a tutorial I&#39;=
m<br>
&gt; writing, although I wouldn&#39;t recommend it for normal use, of cours=
e.<br>
&gt; However, I can&#39;t figure out if this actually exists. I found the<b=
r>
&gt; spark.files.overwrite property, but that doesn&#39;t apply. =C2=A0Does=
 this override<br>
&gt; flag, method call, or method argument actually exist?<br>
&gt;<br>
&gt; Thanks,<br>
&gt; Dean<br>
&gt;<br>
&gt;<br>
&gt; On Tue, Apr 29, 2014 at 1:54 PM, Patrick Wendell &lt;<a href=3D"mailto=
:pwendell@gmail.com">pwendell@gmail.com</a>&gt; wrote:<br>
&gt;<br>
&gt;&gt; Hi Dean,<br>
&gt;&gt;<br>
&gt;&gt; We always used the Hadoop libraries here to read and write local<b=
r>
&gt;&gt; files. In Spark 1.0 we started enforcing the rule that you can&#39=
;t<br>
&gt;&gt; over-write an existing directory because it can cause<br>
&gt;&gt; confusing/undefined behavior if multiple jobs output to the direct=
ory<br>
&gt;&gt; (they partially clobber each other&#39;s output).<br>
&gt;&gt;<br>
&gt;&gt; <a href=3D"https://issues.apache.org/jira/browse/SPARK-1100" targe=
t=3D"_blank">https://issues.apache.org/jira/browse/SPARK-1100</a><br>
&gt;&gt; <a href=3D"https://github.com/apache/spark/pull/11" target=3D"_bla=
nk">https://github.com/apache/spark/pull/11</a><br>
&gt;&gt;<br>
&gt;&gt; In the JIRA I actually proposed slightly deviating from Hadoop<br>
&gt;&gt; semantics and allowing the directory to exist if it is empty, but =
I<br>
&gt;&gt; think in the end we decided to just go with the exact same semanti=
cs<br>
&gt;&gt; as Hadoop (i.e. empty directories are a problem).<br>
&gt;&gt;<br>
&gt;&gt; - Patrick<br>
&gt;&gt;<br>
&gt;&gt; On Tue, Apr 29, 2014 at 9:43 AM, Dean Wampler &lt;<a href=3D"mailt=
o:deanwampler@gmail.com">deanwampler@gmail.com</a>&gt;<br>
&gt;&gt; wrote:<br>
&gt;&gt; &gt; I&#39;m observing one anomalous behavior. With the 1.0.0 libr=
aries, it&#39;s<br>
&gt;&gt; using<br>
&gt;&gt; &gt; HDFS classes for file I/O, while the same script compiled and=
 running<br>
&gt;&gt; with<br>
&gt;&gt; &gt; 0.9.1 uses only the local-mode File IO.<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; The script is a variation of the Word Count script. Here are =
the &quot;guts&quot;:<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; object WordCount2 {<br>
&gt;&gt; &gt; =C2=A0 def main(args: Array[String]) =3D {<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; =C2=A0 =C2=A0 val sc =3D new SparkContext(&quot;local&quot;, =
&quot;Word Count (2)&quot;)<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; =C2=A0 =C2=A0 val input =3D sc.textFile(&quot;.../some/local/=
file&quot;).map(line =3D&gt;<br>
&gt;&gt; &gt; line.toLowerCase)<br>
&gt;&gt; &gt; =C2=A0 =C2=A0 input.cache<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; =C2=A0 =C2=A0 val wc2 =3D input<br>
&gt;&gt; &gt; =C2=A0 =C2=A0 =C2=A0 .flatMap(line =3D&gt; line.split(&quot;&=
quot;&quot;\W+&quot;&quot;&quot;))<br>
&gt;&gt; &gt; =C2=A0 =C2=A0 =C2=A0 .map(word =3D&gt; (word, 1))<br>
&gt;&gt; &gt; =C2=A0 =C2=A0 =C2=A0 .reduceByKey((count1, count2) =3D&gt; co=
unt1 + count2)<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; =C2=A0 =C2=A0 wc2.saveAsTextFile(&quot;output/some/directory&=
quot;)<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; =C2=A0 =C2=A0 sc.stop()<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; It works fine compiled and executed with 0.9.1. If I recompil=
e and run<br>
&gt;&gt; with<br>
&gt;&gt; &gt; 1.0.0-RC1, where the same output directory still exists, I ge=
t this<br>
&gt;&gt; &gt; familiar Hadoop-ish exception:<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; [error] (run-main-0) org.apache.hadoop.mapred.FileAlreadyExis=
tsException:<br>
&gt;&gt; &gt; Output directory<br>
&gt;&gt; &gt;<br>
&gt;&gt; file:/Users/deanwampler/projects/typesafe/activator/activator-spar=
k/output/kjv-wc<br>
&gt;&gt; &gt; already exists<br>
&gt;&gt; &gt; org.apache.hadoop.mapred.FileAlreadyExistsException: Output d=
irectory<br>
&gt;&gt; &gt;<br>
&gt;&gt; file:/Users/deanwampler/projects/typesafe/activator/activator-spar=
k/output/kjv-wc<br>
&gt;&gt; &gt; already exists<br>
&gt;&gt; &gt; =C2=A0at<br>
&gt;&gt; &gt;<br>
&gt;&gt; org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOut=
putFormat.java:121)<br>
&gt;&gt; &gt; at<br>
&gt;&gt; &gt;<br>
&gt;&gt; org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDF=
unctions.scala:749)<br>
&gt;&gt; &gt; =C2=A0at<br>
&gt;&gt; &gt;<br>
&gt;&gt; org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunc=
tions.scala:662)<br>
&gt;&gt; &gt; at<br>
&gt;&gt; &gt;<br>
&gt;&gt; org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunc=
tions.scala:581)<br>
&gt;&gt; &gt; =C2=A0at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:10=
57)<br>
&gt;&gt; &gt; at spark.activator.WordCount2$.main(WordCount2.scala:42)<br>
&gt;&gt; &gt; =C2=A0at spark.activator.WordCount2.main(WordCount2.scala)<br=
>
&gt;&gt; &gt; ...<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; Thoughts?<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; On Tue, Apr 29, 2014 at 3:05 AM, Patrick Wendell &lt;<a href=
=3D"mailto:pwendell@gmail.com">pwendell@gmail.com</a>&gt;<br>
&gt;&gt; wrote:<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;&gt; Hey All,<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; This is not an official vote, but I wanted to cut an RC s=
o that people<br>
&gt;&gt; can<br>
&gt;&gt; &gt;&gt; test against the Maven artifacts, test building with thei=
r<br>
&gt;&gt; configuration,<br>
&gt;&gt; &gt;&gt; etc. We are still chasing down a few issues and updating =
docs, etc.<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; If you have issues or bug reports for this release, pleas=
e send an<br>
&gt;&gt; e-mail<br>
&gt;&gt; &gt;&gt; to the Spark dev list and/or file a JIRA.<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; Commit: d636772 (v1.0.0-rc3)<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; <a href=3D"https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=
=3Dcommit;h=3Dd636772ea9f98e449a038567b7975b1a07de3221" target=3D"_blank">h=
ttps://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3Dd636772=
ea9f98e449a038567b7975b1a07de3221</a><br>

&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; Binaries:<br>
&gt;&gt; &gt;&gt; <a href=3D"http://people.apache.org/~pwendell/spark-1.0.0=
-rc3/" target=3D"_blank">http://people.apache.org/~pwendell/spark-1.0.0-rc3=
/</a><br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; Docs:<br>
&gt;&gt; &gt;&gt; <a href=3D"http://people.apache.org/~pwendell/spark-1.0.0=
-rc3-docs/" target=3D"_blank">http://people.apache.org/~pwendell/spark-1.0.=
0-rc3-docs/</a><br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; Repository:<br>
&gt;&gt; &gt;&gt; <a href=3D"https://repository.apache.org/content/reposito=
ries/orgapachespark-1012/" target=3D"_blank">https://repository.apache.org/=
content/repositories/orgapachespark-1012/</a><br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; =3D=3D API Changes =3D=3D<br>
&gt;&gt; &gt;&gt; If you want to test building against Spark there are some=
 minor API<br>
&gt;&gt; &gt;&gt; changes. We&#39;ll get these written up for the final rel=
ease but I&#39;m<br>
&gt;&gt; noting a<br>
&gt;&gt; &gt;&gt; few here (not comprehensive):<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; changes to ML vector specification:<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; <a href=3D"http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs=
/mllib-guide.html#from-09-to-10" target=3D"_blank">http://people.apache.org=
/~pwendell/spark-1.0.0-rc3-docs/mllib-guide.html#from-09-to-10</a><br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; changes to the Java API:<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; <a href=3D"http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs=
/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark" targe=
t=3D"_blank">http://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/java-p=
rogramming-guide.html#upgrading-from-pre-10-versions-of-spark</a><br>

&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; coGroup and related functions now return Iterable[T] inst=
ead of Seq[T]<br>
&gt;&gt; &gt;&gt; =3D=3D&gt; Call toSeq on the result to restore the old be=
havior<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; SparkContext.jarOfClass returns Option[String] instead of=
 Seq[String]<br>
&gt;&gt; &gt;&gt; =3D=3D&gt; Call toSeq on the result to restore old behavi=
or<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;&gt; Streaming classes have been renamed:<br>
&gt;&gt; &gt;&gt; NetworkReceiver -&gt; Receiver<br>
&gt;&gt; &gt;&gt;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt;<br>
&gt;&gt; &gt; --<br>
&gt;&gt; &gt; Dean Wampler, Ph.D.<br>
&gt;&gt; &gt; Typesafe<br>
&gt;&gt; &gt; @deanwampler<br>
&gt;&gt; &gt; <a href=3D"http://typesafe.com" target=3D"_blank">http://type=
safe.com</a><br>
&gt;&gt; &gt; <a href=3D"http://polyglotprogramming.com" target=3D"_blank">=
http://polyglotprogramming.com</a><br>
&gt;&gt;<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; --<br>
&gt; Dean Wampler, Ph.D.<br>
&gt; Typesafe<br>
&gt; @deanwampler<br>
&gt; <a href=3D"http://typesafe.com" target=3D"_blank">http://typesafe.com<=
/a><br>
&gt; <a href=3D"http://polyglotprogramming.com" target=3D"_blank">http://po=
lyglotprogramming.com</a><br>
</div></div></blockquote></div><br><br clear=3D"all"><div><br></div>-- <br>=
Manu Suryavansh
</div>

--001a11c331b6d0352a04f860f029--
--001a11c331b6d0352d04f860f02a--

From dev-return-7475-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May  2 03:17:46 2014
Return-Path: <dev-return-7475-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 42F53119DF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  2 May 2014 03:17:46 +0000 (UTC)
Received: (qmail 46263 invoked by uid 500); 2 May 2014 03:17:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45862 invoked by uid 500); 2 May 2014 03:17:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45853 invoked by uid 99); 2 May 2014 03:17:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 May 2014 03:17:42 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 May 2014 03:17:37 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <madhu@madhu.com>)
	id 1Wg3xy-0008WU-Vf
	for dev@spark.incubator.apache.org; Thu, 01 May 2014 20:16:50 -0700
Date: Thu, 1 May 2014 20:16:35 -0700 (PDT)
From: Madhu <madhu@madhu.com>
To: dev@spark.incubator.apache.org
Message-ID: <1399000595943-6456.post@n3.nabble.com>
In-Reply-To: <CABPQxstL6nwTO2H9p8=GJh1g2zxOJd02Wt7L06mCLjo-vwwG9Q@mail.gmail.com>
References: <CABPQxstL6nwTO2H9p8=GJh1g2zxOJd02Wt7L06mCLjo-vwwG9Q@mail.gmail.com>
Subject: Re: Spark 1.0.0 rc3
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I'm guessing EC2 support is not there yet?

I was able to build using the binary download on both Windows 7 and RHEL 6
without issues.
I tried to create an EC2 cluster, but saw this:

~/spark-ec2
Initializing spark
~ ~/spark-ec2
ERROR: Unknown Spark version
Initializing shark
~ ~/spark-ec2 ~/spark-ec2
ERROR: Unknown Shark version

The spark dir on the EC2 master has only a conf dir, so it didn't deploy
properly.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-1-0-0-rc3-tp6427p6456.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7476-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May  2 09:25:05 2014
Return-Path: <dev-return-7476-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C3D6F10183
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  2 May 2014 09:25:05 +0000 (UTC)
Received: (qmail 58486 invoked by uid 500); 2 May 2014 09:25:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57644 invoked by uid 500); 2 May 2014 09:24:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57632 invoked by uid 99); 2 May 2014 09:24:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 May 2014 09:24:57 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.220.179 as permitted sender)
Received: from [209.85.220.179] (HELO mail-vc0-f179.google.com) (209.85.220.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 02 May 2014 09:24:52 +0000
Received: by mail-vc0-f179.google.com with SMTP id ij19so5059567vcb.24
        for <dev@spark.apache.org>; Fri, 02 May 2014 02:24:29 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=p8AkcWjRQd03Z6arin/hEUO8+loMgWAOKg7YVjeKYIs=;
        b=SufVETBVherxLQUnOMCj5jEaw6+I2utwsD0VExU6li1sJK/oYMlEdlCD2HjZUy0rvg
         Vmyx8KEeM1/0WQtzFeMPE6uP+So5VsBBEfdkxiT0bccSwlW0KGgY0VPR8yF4B1rE1fpO
         QV0zur/pu4Bwg2wOVE0HFsUTjRLfevHgNjZBf4ywyF+8frEDMv4pCh1LIgcTx1W3S+sa
         iATUrdQxQmz68qPIm+D8VZLvSEvyE4rHBQC4y9VLJdgnGnVf+QKVLZ6bK0sjX7OpMHss
         OXGkpOQjdRmjbKE5HJgLLkqfakYdB7qmLBhsZoV9XeNOPQHzagGX1EEywT286gOkFJQv
         7zdw==
X-Gm-Message-State: ALoCoQlKwZMpHN2e4+9+cWPChRRpD/sEeu0L0d594jQfI7qmeB/gwliGm0l2vLTrcl6aRKL/GfXW
MIME-Version: 1.0
X-Received: by 10.52.166.102 with SMTP id zf6mr10614000vdb.2.1399022669833;
 Fri, 02 May 2014 02:24:29 -0700 (PDT)
Received: by 10.58.111.69 with HTTP; Fri, 2 May 2014 02:24:29 -0700 (PDT)
Received: by 10.58.111.69 with HTTP; Fri, 2 May 2014 02:24:29 -0700 (PDT)
In-Reply-To: <JIRA.12711897.1399015832413.218330.1399015934570@arcas>
References: <JIRA.12711897.1399015832413@arcas>
	<JIRA.12711897.1399015832413.218330.1399015934570@arcas>
Date: Fri, 2 May 2014 10:24:29 +0100
Message-ID: <CAMAsSd+qHdi_7DqypBpRpJ96NG3iddWCzSdDQF2QyY0xPb+zFQ@mail.gmail.com>
Subject: Re: [jira] [Created] (SPARK-1698) Improve spark integration
From: Sean Owen <sowen@cloudera.com>
To: dev@spark.apache.org
Cc: issues@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c361dc44b2ed04f86759d1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c361dc44b2ed04f86759d1
Content-Type: text/plain; charset=UTF-8

#1 and #2 are not relevant the issue of jar size. These can be problems in
general, but don't think there have been issues attributable to file
clashes. Shading has mechanisms to deal with this anyway.

#3 is a problem in general too, but is not specific to shading. Where
versions collide, build processes like Maven and shading must be used to
resolve them. But this happens regardless of whether you shade a fat jar.

#4 is a real problem specific to Java 6. It does seem like it will be
important to identify and remove more unnecessary dependencies to work
around it.

But shading per se is not the problem, and it is important to make a
packaged jar for the app. What are you proposing? Dependencies to be
removed?
On May 2, 2014 8:32 AM, "Guoqiang Li (JIRA)" <jira@apache.org> wrote:

> Guoqiang Li created SPARK-1698:
> ----------------------------------
>
>              Summary: Improve spark integration
>                  Key: SPARK-1698
>                  URL: https://issues.apache.org/jira/browse/SPARK-1698
>              Project: Spark
>           Issue Type: Improvement
>           Components: Build, Deploy
>             Reporter: Guoqiang Li
>             Assignee: Guoqiang Li
>              Fix For: 1.0.0
>
>
> Use the shade plugin to create a big JAR with all the dependencies can
> cause a few problems
> 1. Missing jar's meta information
> 2. Some file is covered, eg: plugin.xml
> 3. Different versions of the jar may co-exist
> 4. Too big, java 6 does not support
>
>
>
>
> --
> This message was sent by Atlassian JIRA
> (v6.2#6252)
>

--001a11c361dc44b2ed04f86759d1--

From dev-return-7477-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May  3 11:08:18 2014
Return-Path: <dev-return-7477-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C323F11A0E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  3 May 2014 11:08:18 +0000 (UTC)
Received: (qmail 6630 invoked by uid 500); 3 May 2014 11:08:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5778 invoked by uid 500); 3 May 2014 11:08:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5765 invoked by uid 99); 3 May 2014 11:08:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 11:08:01 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.216.45 as permitted sender)
Received: from [209.85.216.45] (HELO mail-qa0-f45.google.com) (209.85.216.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 11:07:56 +0000
Received: by mail-qa0-f45.google.com with SMTP id hw13so5107109qab.32
        for <dev@spark.apache.org>; Sat, 03 May 2014 04:07:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=PyovucgEoFs/mhb93wqDabBQ1ZHF9iHbGBVzUR00DLI=;
        b=GYZGdPgdP4Z3h0YFboQg0u9b3j9LeWVIq2NYZqxw7q8O1p4/zo6uVX2hsMqLsoiQvS
         8pQFot0miJatW5brorVLwPMAJu8KFd8MNthhK+k/fO6/XyYHL1FqMpJUsjSUIubL9F2a
         tAUFBLyEPeNhVEFaa9Nf0Ej9Ye07l6zrorCP5QoVvtz1aE27rj1bFQ0d1NJj8Jle/5XC
         6zkpdk+c0p/ZWb2Jwr2ptFnYZ8BteSuwF2h9U9cjG3up7xhs9aPm0TTsCYyOCzvW1rlX
         DqElI9UJZAWnqQyTR3e6keu0QjjCZsYuzsqDan/yF8dgalRYtxEK08VmQKX1VjPaLi7A
         vvpw==
X-Received: by 10.140.18.180 with SMTP id 49mr27423359qgf.105.1399115253644;
        Sat, 03 May 2014 04:07:33 -0700 (PDT)
Received: from [192.168.2.13] ([69.157.95.72])
        by mx.google.com with ESMTPSA id i3sm2540794qgf.14.2014.05.03.04.07.33
        for <multiple recipients>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Sat, 03 May 2014 04:07:33 -0700 (PDT)
Date: Sat, 3 May 2014 07:14:41 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Cc: dev@spark.incubator.apache.org
Message-ID: <C5494AA2163D4E62AD768CC643E3F870@gmail.com>
In-Reply-To: <1399000595943-6456.post@n3.nabble.com>
References: <CABPQxstL6nwTO2H9p8=GJh1g2zxOJd02Wt7L06mCLjo-vwwG9Q@mail.gmail.com>
 <1399000595943-6456.post@n3.nabble.com>
Subject: Re: Spark 1.0.0 rc3
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="5364cfa1_12200854_1f6"
X-Virus-Checked: Checked by ClamAV on apache.org

--5364cfa1_12200854_1f6
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

SPARK_HADOOP_VERSION=2.3.0 sbt/sbt assembly 

and copy the generated jar to lib/ directory of my application, 

it seems that sbt cannot find the dependencies in the jar?

but everything works with the pre-built jar files downloaded from the link provided by Patrick

Best, 

-- 
Nan Zhu


On Thursday, May 1, 2014 at 11:16 PM, Madhu wrote:

> I'm guessing EC2 support is not there yet?
> 
> I was able to build using the binary download on both Windows 7 and RHEL 6
> without issues.
> I tried to create an EC2 cluster, but saw this:
> 
> ~/spark-ec2
> Initializing spark
> ~ ~/spark-ec2
> ERROR: Unknown Spark version
> Initializing shark
> ~ ~/spark-ec2 ~/spark-ec2
> ERROR: Unknown Shark version
> 
> The spark dir on the EC2 master has only a conf dir, so it didn't deploy
> properly.
> 
> 
> 
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-1-0-0-rc3-tp6427p6456.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com (http://Nabble.com).
> 
> 



--5364cfa1_12200854_1f6--


From dev-return-7478-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May  3 11:08:19 2014
Return-Path: <dev-return-7478-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DCBB511A0F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  3 May 2014 11:08:18 +0000 (UTC)
Received: (qmail 6690 invoked by uid 500); 3 May 2014 11:08:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6297 invoked by uid 500); 3 May 2014 11:08:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5764 invoked by uid 99); 3 May 2014 11:07:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 11:07:59 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.216.170 as permitted sender)
Received: from [209.85.216.170] (HELO mail-qc0-f170.google.com) (209.85.216.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 11:07:54 +0000
Received: by mail-qc0-f170.google.com with SMTP id x13so5944811qcv.15
        for <dev@spark.incubator.apache.org>; Sat, 03 May 2014 04:07:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=PyovucgEoFs/mhb93wqDabBQ1ZHF9iHbGBVzUR00DLI=;
        b=GYZGdPgdP4Z3h0YFboQg0u9b3j9LeWVIq2NYZqxw7q8O1p4/zo6uVX2hsMqLsoiQvS
         8pQFot0miJatW5brorVLwPMAJu8KFd8MNthhK+k/fO6/XyYHL1FqMpJUsjSUIubL9F2a
         tAUFBLyEPeNhVEFaa9Nf0Ej9Ye07l6zrorCP5QoVvtz1aE27rj1bFQ0d1NJj8Jle/5XC
         6zkpdk+c0p/ZWb2Jwr2ptFnYZ8BteSuwF2h9U9cjG3up7xhs9aPm0TTsCYyOCzvW1rlX
         DqElI9UJZAWnqQyTR3e6keu0QjjCZsYuzsqDan/yF8dgalRYtxEK08VmQKX1VjPaLi7A
         vvpw==
X-Received: by 10.140.18.180 with SMTP id 49mr27423359qgf.105.1399115253644;
        Sat, 03 May 2014 04:07:33 -0700 (PDT)
Received: from [192.168.2.13] ([69.157.95.72])
        by mx.google.com with ESMTPSA id i3sm2540794qgf.14.2014.05.03.04.07.33
        for <multiple recipients>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Sat, 03 May 2014 04:07:33 -0700 (PDT)
Date: Sat, 3 May 2014 07:14:41 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Cc: dev@spark.incubator.apache.org
Message-ID: <C5494AA2163D4E62AD768CC643E3F870@gmail.com>
In-Reply-To: <1399000595943-6456.post@n3.nabble.com>
References: <CABPQxstL6nwTO2H9p8=GJh1g2zxOJd02Wt7L06mCLjo-vwwG9Q@mail.gmail.com>
 <1399000595943-6456.post@n3.nabble.com>
Subject: Re: Spark 1.0.0 rc3
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="5364cfa1_12200854_1f6"
X-Virus-Checked: Checked by ClamAV on apache.org

--5364cfa1_12200854_1f6
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

SPARK_HADOOP_VERSION=2.3.0 sbt/sbt assembly 

and copy the generated jar to lib/ directory of my application, 

it seems that sbt cannot find the dependencies in the jar?

but everything works with the pre-built jar files downloaded from the link provided by Patrick

Best, 

-- 
Nan Zhu


On Thursday, May 1, 2014 at 11:16 PM, Madhu wrote:

> I'm guessing EC2 support is not there yet?
> 
> I was able to build using the binary download on both Windows 7 and RHEL 6
> without issues.
> I tried to create an EC2 cluster, but saw this:
> 
> ~/spark-ec2
> Initializing spark
> ~ ~/spark-ec2
> ERROR: Unknown Spark version
> Initializing shark
> ~ ~/spark-ec2 ~/spark-ec2
> ERROR: Unknown Shark version
> 
> The spark dir on the EC2 master has only a conf dir, so it didn't deploy
> properly.
> 
> 
> 
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-1-0-0-rc3-tp6427p6456.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com (http://Nabble.com).
> 
> 



--5364cfa1_12200854_1f6--


From dev-return-7479-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May  3 15:44:01 2014
Return-Path: <dev-return-7479-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 63D9011E05
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  3 May 2014 15:44:01 +0000 (UTC)
Received: (qmail 13428 invoked by uid 500); 3 May 2014 15:44:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13378 invoked by uid 500); 3 May 2014 15:44:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13370 invoked by uid 99); 3 May 2014 15:44:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 15:44:00 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of prodigyaj@gmail.com does not designate 216.139.250.139 as permitted sender)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 15:43:56 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <prodigyaj@gmail.com>)
	id 1Wgc5k-0003jm-67
	for dev@spark.incubator.apache.org; Sat, 03 May 2014 08:43:08 -0700
Date: Sat, 3 May 2014 08:42:53 -0700 (PDT)
From: Ajay Nair <prodigyaj@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1399131773131-6459.post@n3.nabble.com>
Subject: Apache Spark running out of the spark shell
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

I have written a code that works just about fine in the spark shell on EC2.
The ec2 script helped me configure my master and worker nodes. Now I want to
run the scala-spark code out side the interactive shell. How do I go about
doing it.

I was referring to the instructions mentioned here:
https://spark.apache.org/docs/0.9.1/quick-start.html

But this is confusing because it mentions about a simple project jar file
which I am not sure how to generate. I only have the file that runs directly
on my spark shell. Any easy intruction to get this quickly running as a job?

Thanks
AJ



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-running-out-of-the-spark-shell-tp6459.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7480-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May  3 17:06:57 2014
Return-Path: <dev-return-7480-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DA83B11F20
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  3 May 2014 17:06:57 +0000 (UTC)
Received: (qmail 74063 invoked by uid 500); 3 May 2014 17:06:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74011 invoked by uid 500); 3 May 2014 17:06:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74002 invoked by uid 99); 3 May 2014 17:06:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 17:06:56 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.182 as permitted sender)
Received: from [209.85.216.182] (HELO mail-qc0-f182.google.com) (209.85.216.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 17:06:52 +0000
Received: by mail-qc0-f182.google.com with SMTP id e16so3591915qcx.27
        for <dev@spark.apache.org>; Sat, 03 May 2014 10:06:31 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=j41wsip4MPj6F54Lo7tiEoSBj+Z3IqLorewqXvtcxbQ=;
        b=QRO/FKiwnAjOMUguP81LioWEYHVaCpFeGYEldQ5ERA6QpeGMeZ8P8NCMPSutQs3h0O
         QWbw+Ob8H5NHp6RQf/KVSkwihWSKD8pgsNGdCXIQoAN8ce26UpFtxc8kDDrDtJca5lVZ
         XNPGr4IrT7zt/bQjn/mhNLYo8AndY0ggBLv6D47BGsqN+lHDoyj2gGhm8VJNp9NYgcXF
         0PNhKdPBT4He+sD20xmCkG5aPR+/mfE5oU3am54C13mEF++XjV0l2lntoqo2tF5dvfWh
         PIieNaT/rxxO0JrMLV6HUDksmQPsZof1qT2yE27fKlJLSM4kjsk7wJrSi6RudBm2JL3e
         xKpw==
X-Gm-Message-State: ALoCoQnDKpguE3gQScKE7QJrKdd6Moe3iBUd3xdvaNUwH/X0PxoBopx4pSATDtHESccks1X9/pld
MIME-Version: 1.0
X-Received: by 10.140.81.48 with SMTP id e45mr29494236qgd.99.1399136791667;
 Sat, 03 May 2014 10:06:31 -0700 (PDT)
Received: by 10.140.94.2 with HTTP; Sat, 3 May 2014 10:06:31 -0700 (PDT)
In-Reply-To: <1399131773131-6459.post@n3.nabble.com>
References: <1399131773131-6459.post@n3.nabble.com>
Date: Sat, 3 May 2014 10:06:31 -0700
Message-ID: <CACBYxKJMOd=hCx1UJc18HKaEG_TD2wX8i8DBRfrGC17xzuNL1A@mail.gmail.com>
Subject: Re: Apache Spark running out of the spark shell
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11c123f275bc7504f881eb41
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c123f275bc7504f881eb41
Content-Type: text/plain; charset=UTF-8

Hi AJ,

You might find this helpful -
http://blog.cloudera.com/blog/2014/04/how-to-run-a-simple-apache-spark-app-in-cdh-5/

-Sandy


On Sat, May 3, 2014 at 8:42 AM, Ajay Nair <prodigyaj@gmail.com> wrote:

> Hi,
>
> I have written a code that works just about fine in the spark shell on EC2.
> The ec2 script helped me configure my master and worker nodes. Now I want
> to
> run the scala-spark code out side the interactive shell. How do I go about
> doing it.
>
> I was referring to the instructions mentioned here:
> https://spark.apache.org/docs/0.9.1/quick-start.html
>
> But this is confusing because it mentions about a simple project jar file
> which I am not sure how to generate. I only have the file that runs
> directly
> on my spark shell. Any easy intruction to get this quickly running as a
> job?
>
> Thanks
> AJ
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-running-out-of-the-spark-shell-tp6459.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c123f275bc7504f881eb41--

From dev-return-7481-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May  3 17:07:03 2014
Return-Path: <dev-return-7481-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8761711F21
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  3 May 2014 17:07:03 +0000 (UTC)
Received: (qmail 74774 invoked by uid 500); 3 May 2014 17:07:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74727 invoked by uid 500); 3 May 2014 17:07:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74701 invoked by uid 99); 3 May 2014 17:06:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 17:06:58 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.41 as permitted sender)
Received: from [209.85.216.41] (HELO mail-qa0-f41.google.com) (209.85.216.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 17:06:54 +0000
Received: by mail-qa0-f41.google.com with SMTP id dc16so3845503qab.14
        for <dev@spark.incubator.apache.org>; Sat, 03 May 2014 10:06:31 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=j41wsip4MPj6F54Lo7tiEoSBj+Z3IqLorewqXvtcxbQ=;
        b=m5w4zlurZOuDfg2h/ofpbrIlAfahhERZQMV5W4y5ozvhU3kpspsefvlbT0N24l0PNe
         F8tO7+Qyf/sipcFSknTBVVNIMMlbpaLw/VEPto2Z8pEcSNPbZrBJqu53YnJY5WIrB4gl
         xaCppkeTW//ZDEHyk4Kw7SGyVuK09noZlJ3Q6ZKX72ibJG0olN1lOxIuXVckAy8m7TXI
         BnAD0AynXgzcGQHRH1sfBgOnQ4JH8ULZht2rMhY9hLFWCYj/8VN5kBdu5L/jpxda3zwy
         IJ2kpWConEjJEQBma6nqwxbZKNE1iancVw/vm/U2fO7vcTSlZ+oIhI8nbAPlml5FtoSS
         54jQ==
X-Gm-Message-State: ALoCoQnjqxBowjAOc2pvetriyacIUSEhKMuzl9k5wR1uX5FRjnmZIoItwHkC/+4bx3kcQoeDig72
MIME-Version: 1.0
X-Received: by 10.140.81.48 with SMTP id e45mr29494236qgd.99.1399136791667;
 Sat, 03 May 2014 10:06:31 -0700 (PDT)
Received: by 10.140.94.2 with HTTP; Sat, 3 May 2014 10:06:31 -0700 (PDT)
In-Reply-To: <1399131773131-6459.post@n3.nabble.com>
References: <1399131773131-6459.post@n3.nabble.com>
Date: Sat, 3 May 2014 10:06:31 -0700
Message-ID: <CACBYxKJMOd=hCx1UJc18HKaEG_TD2wX8i8DBRfrGC17xzuNL1A@mail.gmail.com>
Subject: Re: Apache Spark running out of the spark shell
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11c123f275bc7504f881eb41
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c123f275bc7504f881eb41
Content-Type: text/plain; charset=UTF-8

Hi AJ,

You might find this helpful -
http://blog.cloudera.com/blog/2014/04/how-to-run-a-simple-apache-spark-app-in-cdh-5/

-Sandy


On Sat, May 3, 2014 at 8:42 AM, Ajay Nair <prodigyaj@gmail.com> wrote:

> Hi,
>
> I have written a code that works just about fine in the spark shell on EC2.
> The ec2 script helped me configure my master and worker nodes. Now I want
> to
> run the scala-spark code out side the interactive shell. How do I go about
> doing it.
>
> I was referring to the instructions mentioned here:
> https://spark.apache.org/docs/0.9.1/quick-start.html
>
> But this is confusing because it mentions about a simple project jar file
> which I am not sure how to generate. I only have the file that runs
> directly
> on my spark shell. Any easy intruction to get this quickly running as a
> job?
>
> Thanks
> AJ
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-running-out-of-the-spark-shell-tp6459.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c123f275bc7504f881eb41--

From dev-return-7482-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May  3 17:24:32 2014
Return-Path: <dev-return-7482-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 42C1811F51
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  3 May 2014 17:24:32 +0000 (UTC)
Received: (qmail 87285 invoked by uid 500); 3 May 2014 17:24:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87214 invoked by uid 500); 3 May 2014 17:24:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87205 invoked by uid 99); 3 May 2014 17:24:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 17:24:27 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [68.178.252.108] (HELO p3plsmtpa11-07.prod.phx3.secureserver.net) (68.178.252.108)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 17:24:22 +0000
Received: from [192.168.0.119] ([192.226.138.91])
	by p3plsmtpa11-07.prod.phx3.secureserver.net with 
	id xVPz1n00L1yWK4Y01VQ0E4; Sat, 03 May 2014 10:24:01 -0700
Content-Type: multipart/alternative; boundary="Apple-Mail=_1DBA1700-802E-4222-B5A2-54CE342D13D6"
Mime-Version: 1.0 (Mac OS X Mail 6.6 \(1510\))
Subject: Re: Apache Spark running out of the spark shell
From: Nicolas Garneau <ngarneau@ngarneau.com>
In-Reply-To: <1399131773131-6459.post@n3.nabble.com>
Date: Sat, 3 May 2014 13:23:59 -0400
Cc: dev@spark.incubator.apache.org
Message-Id: <05986E28-BAE8-48D7-9BE7-6314A053899D@ngarneau.com>
References: <1399131773131-6459.post@n3.nabble.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1510)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_1DBA1700-802E-4222-B5A2-54CE342D13D6
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=iso-8859-1

Hey AJ,

I created a little sample app using the spark's quick start.
Have a look here.
Assuming you used scala, using sbt is good for running your application =
in standalone mode.
The configuration file which is "simple.sbt" in my repo, holds all the =
dependencies needed to build your app.

Hope this helps!

Le 2014-05-03 =E0 11:42, Ajay Nair <prodigyaj@gmail.com> a =E9crit :

> Hi,
>=20
> I have written a code that works just about fine in the spark shell on =
EC2.
> The ec2 script helped me configure my master and worker nodes. Now I =
want to
> run the scala-spark code out side the interactive shell. How do I go =
about
> doing it.
>=20
> I was referring to the instructions mentioned here:
> https://spark.apache.org/docs/0.9.1/quick-start.html
>=20
> But this is confusing because it mentions about a simple project jar =
file
> which I am not sure how to generate. I only have the file that runs =
directly
> on my spark shell. Any easy intruction to get this quickly running as =
a job?
>=20
> Thanks
> AJ
>=20
>=20
>=20
> --
> View this message in context: =
http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-run=
ning-out-of-the-spark-shell-tp6459.html
> Sent from the Apache Spark Developers List mailing list archive at =
Nabble.com.
>=20

Nicolas Garneau
ngarneau@ngarneau.com


--Apple-Mail=_1DBA1700-802E-4222-B5A2-54CE342D13D6--

From dev-return-7483-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May  3 17:24:32 2014
Return-Path: <dev-return-7483-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5C9BF11F52
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  3 May 2014 17:24:32 +0000 (UTC)
Received: (qmail 87328 invoked by uid 500); 3 May 2014 17:24:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87230 invoked by uid 500); 3 May 2014 17:24:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87206 invoked by uid 99); 3 May 2014 17:24:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 17:24:27 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [68.178.252.108] (HELO p3plsmtpa11-07.prod.phx3.secureserver.net) (68.178.252.108)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 17:24:22 +0000
Received: from [192.168.0.119] ([192.226.138.91])
	by p3plsmtpa11-07.prod.phx3.secureserver.net with 
	id xVPz1n00L1yWK4Y01VQ0E4; Sat, 03 May 2014 10:24:01 -0700
Content-Type: multipart/alternative; boundary="Apple-Mail=_1DBA1700-802E-4222-B5A2-54CE342D13D6"
Mime-Version: 1.0 (Mac OS X Mail 6.6 \(1510\))
Subject: Re: Apache Spark running out of the spark shell
From: Nicolas Garneau <ngarneau@ngarneau.com>
In-Reply-To: <1399131773131-6459.post@n3.nabble.com>
Date: Sat, 3 May 2014 13:23:59 -0400
Cc: dev@spark.incubator.apache.org
Message-Id: <05986E28-BAE8-48D7-9BE7-6314A053899D@ngarneau.com>
References: <1399131773131-6459.post@n3.nabble.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1510)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_1DBA1700-802E-4222-B5A2-54CE342D13D6
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=iso-8859-1

Hey AJ,

I created a little sample app using the spark's quick start.
Have a look here.
Assuming you used scala, using sbt is good for running your application =
in standalone mode.
The configuration file which is "simple.sbt" in my repo, holds all the =
dependencies needed to build your app.

Hope this helps!

Le 2014-05-03 =E0 11:42, Ajay Nair <prodigyaj@gmail.com> a =E9crit :

> Hi,
>=20
> I have written a code that works just about fine in the spark shell on =
EC2.
> The ec2 script helped me configure my master and worker nodes. Now I =
want to
> run the scala-spark code out side the interactive shell. How do I go =
about
> doing it.
>=20
> I was referring to the instructions mentioned here:
> https://spark.apache.org/docs/0.9.1/quick-start.html
>=20
> But this is confusing because it mentions about a simple project jar =
file
> which I am not sure how to generate. I only have the file that runs =
directly
> on my spark shell. Any easy intruction to get this quickly running as =
a job?
>=20
> Thanks
> AJ
>=20
>=20
>=20
> --
> View this message in context: =
http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-run=
ning-out-of-the-spark-shell-tp6459.html
> Sent from the Apache Spark Developers List mailing list archive at =
Nabble.com.
>=20

Nicolas Garneau
ngarneau@ngarneau.com


--Apple-Mail=_1DBA1700-802E-4222-B5A2-54CE342D13D6--

From dev-return-7484-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May  3 17:29:04 2014
Return-Path: <dev-return-7484-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 85AB711F5C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  3 May 2014 17:29:04 +0000 (UTC)
Received: (qmail 93859 invoked by uid 500); 3 May 2014 17:29:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93759 invoked by uid 500); 3 May 2014 17:29:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93749 invoked by uid 99); 3 May 2014 17:29:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 17:29:02 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of prodigyaj@gmail.com does not designate 216.139.250.139 as permitted sender)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 17:28:59 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <prodigyaj@gmail.com>)
	id 1WgdjO-00018I-V9
	for dev@spark.incubator.apache.org; Sat, 03 May 2014 10:28:10 -0700
Date: Sat, 3 May 2014 10:27:55 -0700 (PDT)
From: Ajay Nair <prodigyaj@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1399138075940-6462.post@n3.nabble.com>
In-Reply-To: <05986E28-BAE8-48D7-9BE7-6314A053899D@ngarneau.com>
References: <1399131773131-6459.post@n3.nabble.com> <05986E28-BAE8-48D7-9BE7-6314A053899D@ngarneau.com>
Subject: Re: Apache Spark running out of the spark shell
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Thank you for the reply. Have you posted a link from where I follow the steps
?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-running-out-of-the-spark-shell-tp6459p6462.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7485-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May  3 17:29:22 2014
Return-Path: <dev-return-7485-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CAAC911F5F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  3 May 2014 17:29:22 +0000 (UTC)
Received: (qmail 94724 invoked by uid 500); 3 May 2014 17:29:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94697 invoked by uid 500); 3 May 2014 17:29:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94689 invoked by uid 99); 3 May 2014 17:29:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 17:29:18 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [68.178.252.110] (HELO p3plsmtpa11-09.prod.phx3.secureserver.net) (68.178.252.110)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 17:29:15 +0000
Received: from [192.168.0.119] ([192.226.138.91])
	by p3plsmtpa11-09.prod.phx3.secureserver.net with 
	id xVUp1n0061yWK4Y01VUqHl; Sat, 03 May 2014 10:28:51 -0700
Content-Type: multipart/alternative; boundary="Apple-Mail=_4DE802D4-E7F8-4649-A6D7-6DDD25CADCCD"
Mime-Version: 1.0 (Mac OS X Mail 6.6 \(1510\))
Subject: Re: Apache Spark running out of the spark shell
From: Nicolas Garneau <ngarneau@ngarneau.com>
In-Reply-To: <05986E28-BAE8-48D7-9BE7-6314A053899D@ngarneau.com>
Date: Sat, 3 May 2014 13:28:48 -0400
Cc: dev@spark.incubator.apache.org
Message-Id: <0053754E-9D2F-4C6E-8E99-ACD34B2D8BAF@ngarneau.com>
References: <1399131773131-6459.post@n3.nabble.com> <05986E28-BAE8-48D7-9BE7-6314A053899D@ngarneau.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1510)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_4DE802D4-E7F8-4649-A6D7-6DDD25CADCCD
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=iso-8859-1

Sorry, the link went wrong. I meant here:
https://github.com/ngarneau/spark-standalone

Le 2014-05-03 =E0 13:23, Nicolas Garneau <ngarneau@ngarneau.com> a =E9crit=
 :

> Hey AJ,
>=20
> I created a little sample app using the spark's quick start.
> Have a look here.
> Assuming you used scala, using sbt is good for running your =
application in standalone mode.
> The configuration file which is "simple.sbt" in my repo, holds all the =
dependencies needed to build your app.
>=20
> Hope this helps!
>=20
> Le 2014-05-03 =E0 11:42, Ajay Nair <prodigyaj@gmail.com> a =E9crit :
>=20
>> Hi,
>>=20
>> I have written a code that works just about fine in the spark shell =
on EC2.
>> The ec2 script helped me configure my master and worker nodes. Now I =
want to
>> run the scala-spark code out side the interactive shell. How do I go =
about
>> doing it.
>>=20
>> I was referring to the instructions mentioned here:
>> https://spark.apache.org/docs/0.9.1/quick-start.html
>>=20
>> But this is confusing because it mentions about a simple project jar =
file
>> which I am not sure how to generate. I only have the file that runs =
directly
>> on my spark shell. Any easy intruction to get this quickly running as =
a job?
>>=20
>> Thanks
>> AJ
>>=20
>>=20
>>=20
>> --
>> View this message in context: =
http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-run=
ning-out-of-the-spark-shell-tp6459.html
>> Sent from the Apache Spark Developers List mailing list archive at =
Nabble.com.
>>=20
>=20
> Nicolas Garneau
> ngarneau@ngarneau.com
>=20

Nicolas Garneau
418.569.3097
ngarneau@ngarneau.com


--Apple-Mail=_4DE802D4-E7F8-4649-A6D7-6DDD25CADCCD--

From dev-return-7486-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May  3 17:29:23 2014
Return-Path: <dev-return-7486-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6A0D911F60
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  3 May 2014 17:29:23 +0000 (UTC)
Received: (qmail 95069 invoked by uid 500); 3 May 2014 17:29:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94840 invoked by uid 500); 3 May 2014 17:29:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94698 invoked by uid 99); 3 May 2014 17:29:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 17:29:19 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [68.178.252.110] (HELO p3plsmtpa11-09.prod.phx3.secureserver.net) (68.178.252.110)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 17:29:15 +0000
Received: from [192.168.0.119] ([192.226.138.91])
	by p3plsmtpa11-09.prod.phx3.secureserver.net with 
	id xVUp1n0061yWK4Y01VUqHl; Sat, 03 May 2014 10:28:51 -0700
Content-Type: multipart/alternative; boundary="Apple-Mail=_4DE802D4-E7F8-4649-A6D7-6DDD25CADCCD"
Mime-Version: 1.0 (Mac OS X Mail 6.6 \(1510\))
Subject: Re: Apache Spark running out of the spark shell
From: Nicolas Garneau <ngarneau@ngarneau.com>
In-Reply-To: <05986E28-BAE8-48D7-9BE7-6314A053899D@ngarneau.com>
Date: Sat, 3 May 2014 13:28:48 -0400
Cc: dev@spark.incubator.apache.org
Message-Id: <0053754E-9D2F-4C6E-8E99-ACD34B2D8BAF@ngarneau.com>
References: <1399131773131-6459.post@n3.nabble.com> <05986E28-BAE8-48D7-9BE7-6314A053899D@ngarneau.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1510)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_4DE802D4-E7F8-4649-A6D7-6DDD25CADCCD
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=iso-8859-1

Sorry, the link went wrong. I meant here:
https://github.com/ngarneau/spark-standalone

Le 2014-05-03 =E0 13:23, Nicolas Garneau <ngarneau@ngarneau.com> a =E9crit=
 :

> Hey AJ,
>=20
> I created a little sample app using the spark's quick start.
> Have a look here.
> Assuming you used scala, using sbt is good for running your =
application in standalone mode.
> The configuration file which is "simple.sbt" in my repo, holds all the =
dependencies needed to build your app.
>=20
> Hope this helps!
>=20
> Le 2014-05-03 =E0 11:42, Ajay Nair <prodigyaj@gmail.com> a =E9crit :
>=20
>> Hi,
>>=20
>> I have written a code that works just about fine in the spark shell =
on EC2.
>> The ec2 script helped me configure my master and worker nodes. Now I =
want to
>> run the scala-spark code out side the interactive shell. How do I go =
about
>> doing it.
>>=20
>> I was referring to the instructions mentioned here:
>> https://spark.apache.org/docs/0.9.1/quick-start.html
>>=20
>> But this is confusing because it mentions about a simple project jar =
file
>> which I am not sure how to generate. I only have the file that runs =
directly
>> on my spark shell. Any easy intruction to get this quickly running as =
a job?
>>=20
>> Thanks
>> AJ
>>=20
>>=20
>>=20
>> --
>> View this message in context: =
http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-run=
ning-out-of-the-spark-shell-tp6459.html
>> Sent from the Apache Spark Developers List mailing list archive at =
Nabble.com.
>>=20
>=20
> Nicolas Garneau
> ngarneau@ngarneau.com
>=20

Nicolas Garneau
418.569.3097
ngarneau@ngarneau.com


--Apple-Mail=_4DE802D4-E7F8-4649-A6D7-6DDD25CADCCD--

From dev-return-7487-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May  3 17:29:39 2014
Return-Path: <dev-return-7487-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7EFB311F62
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  3 May 2014 17:29:39 +0000 (UTC)
Received: (qmail 96542 invoked by uid 500); 3 May 2014 17:29:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96477 invoked by uid 500); 3 May 2014 17:29:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96469 invoked by uid 99); 3 May 2014 17:29:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 17:29:37 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of prodigyaj@gmail.com does not designate 216.139.250.139 as permitted sender)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 17:29:33 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <prodigyaj@gmail.com>)
	id 1Wgdk0-0001A8-0F
	for dev@spark.incubator.apache.org; Sat, 03 May 2014 10:28:48 -0700
Date: Sat, 3 May 2014 10:28:32 -0700 (PDT)
From: Ajay Nair <prodigyaj@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1399138112984-6463.post@n3.nabble.com>
In-Reply-To: <CACBYxKJMOd=hCx1UJc18HKaEG_TD2wX8i8DBRfrGC17xzuNL1A@mail.gmail.com>
References: <1399131773131-6459.post@n3.nabble.com> <CACBYxKJMOd=hCx1UJc18HKaEG_TD2wX8i8DBRfrGC17xzuNL1A@mail.gmail.com>
Subject: Re: Apache Spark running out of the spark shell
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Thank you. Let me try this quickly !



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-running-out-of-the-spark-shell-tp6459p6463.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7488-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May  3 17:39:59 2014
Return-Path: <dev-return-7488-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9E9B111F7B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  3 May 2014 17:39:59 +0000 (UTC)
Received: (qmail 2378 invoked by uid 500); 3 May 2014 17:39:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2268 invoked by uid 500); 3 May 2014 17:39:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2260 invoked by uid 99); 3 May 2014 17:39:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 17:39:57 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of prodigyaj@gmail.com does not designate 216.139.250.139 as permitted sender)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 17:39:54 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <prodigyaj@gmail.com>)
	id 1Wgdty-0001aq-F2
	for dev@spark.incubator.apache.org; Sat, 03 May 2014 10:39:06 -0700
Date: Sat, 3 May 2014 10:38:51 -0700 (PDT)
From: Ajay Nair <prodigyaj@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1399138731449-6465.post@n3.nabble.com>
In-Reply-To: <0053754E-9D2F-4C6E-8E99-ACD34B2D8BAF@ngarneau.com>
References: <1399131773131-6459.post@n3.nabble.com> <05986E28-BAE8-48D7-9BE7-6314A053899D@ngarneau.com> <0053754E-9D2F-4C6E-8E99-ACD34B2D8BAF@ngarneau.com>
Subject: Re: Apache Spark running out of the spark shell
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Quick question, where should I place your folder. Inside the spark directory.
My Spark directory is in /root/spark
So currently I tried pulling your github code in /root/spark/spark-examples
and modified my home spark directory in the scala code.
I copied the sbt folder within the spark-examples folder. But when I try
running this command

$root/spark/spark-examples: sbt/sbt package

awk: cmd. line:1: fatal: cannot open file `./project/build.properties' for
reading (No such file or directory)
Launching sbt from sbt/sbt-launch-.jar
Error: Invalid or corrupt jarfile sbt/sbt-launch-.jar


However the sbt package runs fines (Expectedly) when i run it from
/root/spark folder.

Anything I am doing wrong here?





--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-running-out-of-the-spark-shell-tp6459p6465.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7489-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May  3 18:23:29 2014
Return-Path: <dev-return-7489-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9F3A610014
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  3 May 2014 18:23:29 +0000 (UTC)
Received: (qmail 41296 invoked by uid 500); 3 May 2014 18:23:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41212 invoked by uid 500); 3 May 2014 18:23:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41204 invoked by uid 99); 3 May 2014 18:23:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 18:23:27 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [68.178.252.110] (HELO p3plsmtpa11-09.prod.phx3.secureserver.net) (68.178.252.110)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 03 May 2014 18:23:24 +0000
Received: from [192.168.0.119] ([192.226.138.91])
	by p3plsmtpa11-09.prod.phx3.secureserver.net with 
	id xWNz1n0081yWK4Y01WP0nb; Sat, 03 May 2014 11:23:00 -0700
Content-Type: text/plain; charset=iso-8859-1
Mime-Version: 1.0 (Mac OS X Mail 6.6 \(1510\))
Subject: Re: Apache Spark running out of the spark shell
From: Nicolas Garneau <ngarneau@ngarneau.com>
In-Reply-To: <1399138731449-6465.post@n3.nabble.com>
Date: Sat, 3 May 2014 14:22:58 -0400
Content-Transfer-Encoding: quoted-printable
Message-Id: <8214075A-7014-4B0B-9312-2F1450793390@ngarneau.com>
References: <1399131773131-6459.post@n3.nabble.com> <05986E28-BAE8-48D7-9BE7-6314A053899D@ngarneau.com> <0053754E-9D2F-4C6E-8E99-ACD34B2D8BAF@ngarneau.com> <1399138731449-6465.post@n3.nabble.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1510)
X-Virus-Checked: Checked by ClamAV on apache.org

Hey AJ,

As I can see your path when running sbt is:

> $root/spark/spark-examples: sbt/sbt package

You should be within the app's folder that contains the simple.sbt, =
which is spark-standalone/;

> $root/spark/spark-examples/spark-standalone: sbt/sbt package
> $root/spark/spark-examples/spark-standalone: sbt/sbt run


Don't forget to move the sbt folder within your app's directory.

That being said, I think you can install sbt globally on your system so =
you'll be able to run the sbt command everywhere on your PC.
It'll be useful when creating multiple apps.

For example, the way I'm building it from A to Z:
$ git clone https://github.com/ngarneau/spark-standalone.git
$ cd spark-standalone
-- change the path of spark's home dir
$ sbt package (assuming sbt is installed globally)
$ sbt run (assuming sbt is installed globally)

Hope this helps!

Le 2014-05-03 =E0 13:38, Ajay Nair <prodigyaj@gmail.com> a =E9crit :

> Quick question, where should I place your folder. Inside the spark =
directory.
> My Spark directory is in /root/spark
> So currently I tried pulling your github code in =
/root/spark/spark-examples
> and modified my home spark directory in the scala code.
> I copied the sbt folder within the spark-examples folder. But when I =
try
> running this command
>=20
> $root/spark/spark-examples: sbt/sbt package
>=20
> awk: cmd. line:1: fatal: cannot open file `./project/build.properties' =
for
> reading (No such file or directory)
> Launching sbt from sbt/sbt-launch-.jar
> Error: Invalid or corrupt jarfile sbt/sbt-launch-.jar
>=20
>=20
> However the sbt package runs fines (Expectedly) when i run it from
> /root/spark folder.
>=20
> Anything I am doing wrong here?
>=20
>=20
>=20
>=20
>=20
> --
> View this message in context: =
http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-run=
ning-out-of-the-spark-shell-tp6459p6465.html
> Sent from the Apache Spark Developers List mailing list archive at =
Nabble.com.
>=20

Nicolas Garneau
ngarneau@ngarneau.com


From dev-return-7490-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May  4 04:31:10 2014
Return-Path: <dev-return-7490-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DB905108C5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  4 May 2014 04:31:10 +0000 (UTC)
Received: (qmail 15175 invoked by uid 500); 4 May 2014 04:31:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14544 invoked by uid 500); 4 May 2014 04:31:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14529 invoked by uid 99); 4 May 2014 04:31:08 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 04:31:08 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.180 as permitted sender)
Received: from [209.85.192.180] (HELO mail-pd0-f180.google.com) (209.85.192.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 04:31:02 +0000
Received: by mail-pd0-f180.google.com with SMTP id y10so107304pdj.25
        for <spark-dev@apache.org>; Sat, 03 May 2014 21:30:38 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=dy4TzClIRZFaYbuTroE2440MxtUO+h5koTIBdHMd0mg=;
        b=Y7IW0cVksEuezRB3RSRtsQ1POTtoJVZRQkjP0XcGWj/fCJvKfIakDAPMJEACUz9l8j
         8tZ12wrLk+cjnDL15uTvgAA/U0Ru5C4y2hyLWI8PqCnkdTM3JoOeDWqtt4lsyqJYB7af
         6SpLSesd3t1JklPiRApTMOsFGM2HdZRDqbqzvT47BF/dvt60mswmx2+vweyT6603Y7jI
         9VZjnLjSPSeXEImMKAkkv3BJ0pOR3OBribGjJ6KgWg9YLkN5a0lmfaivsufjLvzAMcbY
         nUHyA5zJ/3fY/dhMqpJJv8eEPBEgzNsqHCo9eFyttjtTehoo5vbBE7ZR1WIUnWqBPpf5
         gmhQ==
X-Received: by 10.66.136.131 with SMTP id qa3mr54546746pab.77.1399177838686;
        Sat, 03 May 2014 21:30:38 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id nw13sm34978781pab.37.2014.05.03.21.30.34
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sat, 03 May 2014 21:30:35 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: Mailing list
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <13B7C349-0F74-4D2E-B0E6-536A270F8B19@hibnet.org>
Date: Sat, 3 May 2014 21:30:31 -0700
Cc: spark-dev@apache.org
Content-Transfer-Encoding: quoted-printable
Message-Id: <A7DDB0A7-BA1E-41C9-8E66-5872E4D31552@gmail.com>
References: <13B7C349-0F74-4D2E-B0E6-536A270F8B19@hibnet.org>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Nicolas,

Good catches on these things.

> Your website seems a little bit incomplete. I have found this page [1] =
with list the two main mailing lists, users and dev. But I see a =
reference to a mailing list about "issues" which tracks the sparks =
issues when it was hosted at Atlassian. I guess it has moved ? where ?
> And is there any mailing about the commits ?

Good catch, this was an old link and I=92ve fixed it now. I also added =
the one for commits.

> Also, I found it weird that there is no page that is referencing the =
true code source, the git at the ASF, I only found references to the git =
at github.

The GitHub repo is actually a mirror managed by the ASF, but the =93git =
tag=94 link at http://spark.apache.org/downloads.html also points to the =
source repo. The problem is that our contribution process is through =
GitHub so it=92s easier to point people to something that they can use =
to contribute.

> I am also interested in your workflow, because Ant is moving from svn =
to git and we're still a little bit in the grey about the workflow. I am =
thus intrigued how do you work with github pull requests.

Take a look at =
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark =
and =
https://cwiki.apache.org/confluence/display/SPARK/Reviewing+and+Merging+Pa=
tches to see our contribution process. In a nutshell, it works as =
follows:

- Anyone can make a patch by forking the GitHub repo and sending a pull =
request (GitHub=92s internal patch mechanism)
- Committers review the patch and ask for changes; contributors can push =
additional changes into their pull request to respond
- When the patch looks good, we use a script to merge it into the source =
Apache repo; this also squashes the changes into one commit, making the =
Git history sane and facilitating reverts, cherry-picks into other =
branches, etc.

Note by the way that using GitHub is not at all necessary for using Git. =
We happened to do our development on GitHub before moving to the ASF, =
and all our developers were used to its interface, so we stuck with it. =
It definitely beats attaching patches on JIRA but it may not be the =
first step you want to take in moving to Git.

Matei

>=20
> Nicolas
>=20
> [1] https://spark.apache.org/community.html
>=20


From dev-return-7491-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May  4 04:31:13 2014
Return-Path: <dev-return-7491-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6489F108C6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  4 May 2014 04:31:13 +0000 (UTC)
Received: (qmail 15272 invoked by uid 500); 4 May 2014 04:31:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14546 invoked by uid 500); 4 May 2014 04:31:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14528 invoked by uid 99); 4 May 2014 04:31:08 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 04:31:08 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.179 as permitted sender)
Received: from [209.85.192.179] (HELO mail-pd0-f179.google.com) (209.85.192.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 04:31:02 +0000
Received: by mail-pd0-f179.google.com with SMTP id g10so4412400pdj.24
        for <dev@spark.apache.org>; Sat, 03 May 2014 21:30:38 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=dy4TzClIRZFaYbuTroE2440MxtUO+h5koTIBdHMd0mg=;
        b=Y7IW0cVksEuezRB3RSRtsQ1POTtoJVZRQkjP0XcGWj/fCJvKfIakDAPMJEACUz9l8j
         8tZ12wrLk+cjnDL15uTvgAA/U0Ru5C4y2hyLWI8PqCnkdTM3JoOeDWqtt4lsyqJYB7af
         6SpLSesd3t1JklPiRApTMOsFGM2HdZRDqbqzvT47BF/dvt60mswmx2+vweyT6603Y7jI
         9VZjnLjSPSeXEImMKAkkv3BJ0pOR3OBribGjJ6KgWg9YLkN5a0lmfaivsufjLvzAMcbY
         nUHyA5zJ/3fY/dhMqpJJv8eEPBEgzNsqHCo9eFyttjtTehoo5vbBE7ZR1WIUnWqBPpf5
         gmhQ==
X-Received: by 10.66.136.131 with SMTP id qa3mr54546746pab.77.1399177838686;
        Sat, 03 May 2014 21:30:38 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id nw13sm34978781pab.37.2014.05.03.21.30.34
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sat, 03 May 2014 21:30:35 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: Mailing list
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <13B7C349-0F74-4D2E-B0E6-536A270F8B19@hibnet.org>
Date: Sat, 3 May 2014 21:30:31 -0700
Cc: spark-dev@apache.org
Content-Transfer-Encoding: quoted-printable
Message-Id: <A7DDB0A7-BA1E-41C9-8E66-5872E4D31552@gmail.com>
References: <13B7C349-0F74-4D2E-B0E6-536A270F8B19@hibnet.org>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Nicolas,

Good catches on these things.

> Your website seems a little bit incomplete. I have found this page [1] =
with list the two main mailing lists, users and dev. But I see a =
reference to a mailing list about "issues" which tracks the sparks =
issues when it was hosted at Atlassian. I guess it has moved ? where ?
> And is there any mailing about the commits ?

Good catch, this was an old link and I=92ve fixed it now. I also added =
the one for commits.

> Also, I found it weird that there is no page that is referencing the =
true code source, the git at the ASF, I only found references to the git =
at github.

The GitHub repo is actually a mirror managed by the ASF, but the =93git =
tag=94 link at http://spark.apache.org/downloads.html also points to the =
source repo. The problem is that our contribution process is through =
GitHub so it=92s easier to point people to something that they can use =
to contribute.

> I am also interested in your workflow, because Ant is moving from svn =
to git and we're still a little bit in the grey about the workflow. I am =
thus intrigued how do you work with github pull requests.

Take a look at =
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark =
and =
https://cwiki.apache.org/confluence/display/SPARK/Reviewing+and+Merging+Pa=
tches to see our contribution process. In a nutshell, it works as =
follows:

- Anyone can make a patch by forking the GitHub repo and sending a pull =
request (GitHub=92s internal patch mechanism)
- Committers review the patch and ask for changes; contributors can push =
additional changes into their pull request to respond
- When the patch looks good, we use a script to merge it into the source =
Apache repo; this also squashes the changes into one commit, making the =
Git history sane and facilitating reverts, cherry-picks into other =
branches, etc.

Note by the way that using GitHub is not at all necessary for using Git. =
We happened to do our development on GitHub before moving to the ASF, =
and all our developers were used to its interface, so we stuck with it. =
It definitely beats attaching patches on JIRA but it may not be the =
first step you want to take in moving to Git.

Matei

>=20
> Nicolas
>=20
> [1] https://spark.apache.org/community.html
>=20


From dev-return-7492-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May  4 08:13:03 2014
Return-Path: <dev-return-7492-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B3AAE10BD8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  4 May 2014 08:13:03 +0000 (UTC)
Received: (qmail 38484 invoked by uid 500); 4 May 2014 08:13:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38210 invoked by uid 500); 4 May 2014 08:13:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38201 invoked by uid 99); 4 May 2014 08:13:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 08:13:00 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of manish9ue@gmail.com designates 209.85.128.174 as permitted sender)
Received: from [209.85.128.174] (HELO mail-ve0-f174.google.com) (209.85.128.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 08:12:55 +0000
Received: by mail-ve0-f174.google.com with SMTP id jw12so515079veb.5
        for <dev@spark.apache.org>; Sun, 04 May 2014 01:12:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=MG0zK6R52M9l/cjw+QYZ2u7ioos8ZbilzjXIoV6plJk=;
        b=Sprh/PLf349oswCUv5RYm0jk3qf3oVjY2DpSbNWNT03ndha5NGyMy8hM4ftrDbgext
         mxWZuWGcdK7TvO6pPNlfjbfbu27e7ZI2lDAHvMfl1D9IUOci1N7acs388qWQJQEvaf81
         ZyDIo0yR/DPrvGovpDdqxFUqWtN1Le3Clyvj3pFa3qMWzZp8PjiUMXd30F19aatmpZz4
         oJ9vyYD1Y2HwKFA1RVsZYi4HuDVasQ17cpahhkwbFdADf1JJzLTexpStqw65MXnv5c4t
         Yy6BfNPUjaNF42+OMK0wl4OVOEBehYUAuLYHPaj8cr4Qv4RUEjWv/AVFbRVewMVE4XUG
         IZKg==
MIME-Version: 1.0
X-Received: by 10.220.159.4 with SMTP id h4mr22406771vcx.1.1399191154767; Sun,
 04 May 2014 01:12:34 -0700 (PDT)
Received: by 10.58.104.162 with HTTP; Sun, 4 May 2014 01:12:34 -0700 (PDT)
Date: Sun, 4 May 2014 01:12:34 -0700
Message-ID: <CAF4jm1C=cqLCezyeiR=p528BWGZjNtvjRMYYmASXn-+CveA-5A@mail.gmail.com>
Subject: reduce, transform, combine
From: Manish Amde <manish9ue@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2ca0cc0bdce04f88e930c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ca0cc0bdce04f88e930c
Content-Type: text/plain; charset=UTF-8

I am currently using the RDD aggregate operation to reduce (fold) per
partition and then combine using the RDD aggregate operation.
def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U)
=> U): U

I need to perform a transform operation after the seqOp and before the
combOp. The signature would look like
def foldTransformCombine[U: ClassTag](zeroReduceValue: V, zeroCombineValue:
U)(seqOp: (V, T) => V, transformOp: (V) => U, combOp: (U, U) => U): U

This is especially useful in the scenario where the transformOp is
expensive and should be performed once per partition before combining. Is
there a way to accomplish this with existing RDD operations? If yes, great
but if not, should we consider adding such a general transformation to the
list of RDD operations?

-Manish

--001a11c2ca0cc0bdce04f88e930c--

From dev-return-7493-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May  4 08:41:06 2014
Return-Path: <dev-return-7493-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 27B6410C3E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  4 May 2014 08:41:06 +0000 (UTC)
Received: (qmail 62008 invoked by uid 500); 4 May 2014 08:41:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61519 invoked by uid 500); 4 May 2014 08:41:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61504 invoked by uid 99); 4 May 2014 08:41:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 08:41:02 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.81 as permitted sender)
Received: from [171.67.219.81] (HELO smtp.stanford.edu) (171.67.219.81)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 08:40:58 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 5C0CF21869
	for <dev@spark.apache.org>; Sun,  4 May 2014 01:40:34 -0700 (PDT)
Received: from mail-qg0-f48.google.com (mail-qg0-f48.google.com [209.85.192.48])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id DA2B720C25
	for <dev@spark.apache.org>; Sun,  4 May 2014 01:40:33 -0700 (PDT)
Received: by mail-qg0-f48.google.com with SMTP id i50so5082413qgf.7
        for <dev@spark.apache.org>; Sun, 04 May 2014 01:40:33 -0700 (PDT)
X-Gm-Message-State: ALoCoQn1tOxp6oFj+Loeo4ZlYD7Exs9k0BrvCaZxa2qZrWznGFt58QUYGghPfMFD1BUNASHNYeBF
MIME-Version: 1.0
X-Received: by 10.224.15.137 with SMTP id k9mr625495qaa.104.1399192833036;
 Sun, 04 May 2014 01:40:33 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Sun, 4 May 2014 01:40:32 -0700 (PDT)
In-Reply-To: <CAF4jm1C=cqLCezyeiR=p528BWGZjNtvjRMYYmASXn-+CveA-5A@mail.gmail.com>
References: <CAF4jm1C=cqLCezyeiR=p528BWGZjNtvjRMYYmASXn-+CveA-5A@mail.gmail.com>
Date: Sun, 4 May 2014 01:40:32 -0700
Message-ID: <CAEYYnxYpGibGoy3RZ0XfQZq4yLURMBT32YtOoQZJXtx1cb587w@mail.gmail.com>
Subject: Re: reduce, transform, combine
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bdc912ec95ef204f88ef762
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc912ec95ef204f88ef762
Content-Type: text/plain; charset=UTF-8

You could easily achieve this by mapPartition. However, it seems that it
can not be done by using aggregate type of operation. I can see that it's a
general useful operation. For now, you could use mapPartition.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Sun, May 4, 2014 at 1:12 AM, Manish Amde <manish9ue@gmail.com> wrote:

> I am currently using the RDD aggregate operation to reduce (fold) per
> partition and then combine using the RDD aggregate operation.
> def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U)
> => U): U
>
> I need to perform a transform operation after the seqOp and before the
> combOp. The signature would look like
> def foldTransformCombine[U: ClassTag](zeroReduceValue: V, zeroCombineValue:
> U)(seqOp: (V, T) => V, transformOp: (V) => U, combOp: (U, U) => U): U
>
> This is especially useful in the scenario where the transformOp is
> expensive and should be performed once per partition before combining. Is
> there a way to accomplish this with existing RDD operations? If yes, great
> but if not, should we consider adding such a general transformation to the
> list of RDD operations?
>
> -Manish
>

--047d7bdc912ec95ef204f88ef762--

From dev-return-7494-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May  4 15:13:04 2014
Return-Path: <dev-return-7494-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A60AE1116C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  4 May 2014 15:13:04 +0000 (UTC)
Received: (qmail 22235 invoked by uid 500); 4 May 2014 15:13:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22191 invoked by uid 500); 4 May 2014 15:13:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22183 invoked by uid 99); 4 May 2014 15:13:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 15:13:02 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_HELO_PASS,SPF_SOFTFAIL
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of nicolas.lalevee@hibnet.org does not designate 216.86.168.182 as permitted sender)
Received: from [216.86.168.182] (HELO mxout-07.mxes.net) (216.86.168.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 15:12:58 +0000
Received: from [192.168.1.21] (unknown [86.68.205.116])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by smtp.mxes.net (Postfix) with ESMTPSA id B178D22E256
	for <dev@spark.apache.org>; Sun,  4 May 2014 11:12:33 -0400 (EDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 6.6 \(1510\))
Subject: Re: Mailing list
From: =?windows-1252?Q?Nicolas_Lalev=E9e?= <nicolas.lalevee@hibnet.org>
In-Reply-To: <A7DDB0A7-BA1E-41C9-8E66-5872E4D31552@gmail.com>
Date: Sun, 4 May 2014 17:12:29 +0200
Content-Transfer-Encoding: quoted-printable
Message-Id: <DFEBB1B0-9ABD-4671-A46B-BBC1D3636EC0@hibnet.org>
References: <13B7C349-0F74-4D2E-B0E6-536A270F8B19@hibnet.org> <A7DDB0A7-BA1E-41C9-8E66-5872E4D31552@gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1510)
X-Virus-Checked: Checked by ClamAV on apache.org


Le 4 mai 2014 =E0 06:30, Matei Zaharia <matei.zaharia@gmail.com> a =E9crit=
 :

> Hi Nicolas,
>=20
> Good catches on these things.
>=20
>> Your website seems a little bit incomplete. I have found this page =
[1] with list the two main mailing lists, users and dev. But I see a =
reference to a mailing list about "issues" which tracks the sparks =
issues when it was hosted at Atlassian. I guess it has moved ? where ?
>> And is there any mailing about the commits ?
>=20
> Good catch, this was an old link and I=92ve fixed it now. I also added =
the one for commits.
>=20
>> Also, I found it weird that there is no page that is referencing the =
true code source, the git at the ASF, I only found references to the git =
at github.
>=20
> The GitHub repo is actually a mirror managed by the ASF, but the =93git =
tag=94 link at http://spark.apache.org/downloads.html also points to the =
source repo. The problem is that our contribution process is through =
GitHub so it=92s easier to point people to something that they can use =
to contribute.
>=20
>> I am also interested in your workflow, because Ant is moving from svn =
to git and we're still a little bit in the grey about the workflow. I am =
thus intrigued how do you work with github pull requests.
>=20
> Take a look at =
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark =
and =
https://cwiki.apache.org/confluence/display/SPARK/Reviewing+and+Merging+Pa=
tches to see our contribution process. In a nutshell, it works as =
follows:
>=20
> - Anyone can make a patch by forking the GitHub repo and sending a =
pull request (GitHub=92s internal patch mechanism)
> - Committers review the patch and ask for changes; contributors can =
push additional changes into their pull request to respond
> - When the patch looks good, we use a script to merge it into the =
source Apache repo; this also squashes the changes into one commit, =
making the Git history sane and facilitating reverts, cherry-picks into =
other branches, etc.

The script you're talking about, is it merge_spark_pr.py [1] ?

> Note by the way that using GitHub is not at all necessary for using =
Git. We happened to do our development on GitHub before moving to the =
ASF, and all our developers were used to its interface, so we stuck with =
it. It definitely beats attaching patches on JIRA but it may not be the =
first step you want to take in moving to Git.

Your workflow is indeed interesting. I guess most of Ant committers and =
potential contributors have experience with github too, so at some point =
we'll have to handle it. I'll discuss with the Ant dev community.

Thank you Matei for the fix on the site and for the clear response.

Nicolas

[1] https://github.com/apache/spark/blob/master/dev/merge_spark_pr.py


From dev-return-7495-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May  4 17:07:05 2014
Return-Path: <dev-return-7495-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 184EC11394
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  4 May 2014 17:07:05 +0000 (UTC)
Received: (qmail 45402 invoked by uid 500); 4 May 2014 17:07:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45315 invoked by uid 500); 4 May 2014 17:07:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45307 invoked by uid 99); 4 May 2014 17:07:02 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 17:07:02 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of manish9ue@gmail.com designates 209.85.192.43 as permitted sender)
Received: from [209.85.192.43] (HELO mail-qg0-f43.google.com) (209.85.192.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 17:06:58 +0000
Received: by mail-qg0-f43.google.com with SMTP id 63so1008394qgz.30
        for <dev@spark.apache.org>; Sun, 04 May 2014 10:06:38 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:in-reply-to:references:from:to:cc
         :subject:content-type;
        bh=d2TBtwsj0mflGuXlHKXe8QnDOFM3gIwYIG06YO/Nbrg=;
        b=pgxjb4L/0QhXoXvfAbZhpfmu+e6sAe3omNVZn7zzzmcrSqJhl2WBcBV6xJ3zP/rI8h
         kgQs8nTAQJu8Iatz1ysLEa7pj+yqgBOef5sadPeH/MW0nU+Rx9pb5AYHPL+6kpqvzyrg
         CVXj8XUD5qaoJ39LwPDeVgHdwAnvKFFI4G/D8+HAWaR8gx0jxCz0ZnpsUuYFLAEH7M2T
         fB40RY9vnrU8aZfsqHB8oKkOGjfAxITJMX5p5hgydXrhteYc4154QQR850MnucE+VzAw
         IIlc46jWfPifI4Rjzkky7LRfqLfyWjoynUiPTFDBvfbiYGrgFb7pB0jEw6BlztbcN23R
         asaA==
X-Received: by 10.140.106.3 with SMTP id d3mr9090618qgf.44.1399223198038;
        Sun, 04 May 2014 10:06:38 -0700 (PDT)
Received: from hedwig-10.prd.orcali.com (ec2-54-85-253-178.compute-1.amazonaws.com. [54.85.253.178])
        by mx.google.com with ESMTPSA id z8sm11680201qaw.17.2014.05.04.10.06.37
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 04 May 2014 10:06:37 -0700 (PDT)
X-Google-Original-Date: Sun, 04 May 2014 17:06:37 GMT
MIME-Version: 1.0
X-Mailer: Nodemailer (0.5.0; +http://www.nodemailer.com/)
Date: Sun, 04 May 2014 10:06:37 -0700 (PDT)
Message-Id: <1399223197404.de995d58@Nodemailer>
In-Reply-To: <CAEYYnxYpGibGoy3RZ0XfQZq4yLURMBT32YtOoQZJXtx1cb587w@mail.gmail.com>
References: <CAEYYnxYpGibGoy3RZ0XfQZq4yLURMBT32YtOoQZJXtx1cb587w@mail.gmail.com>
X-Orchestra-Oid: 7F8319E7-74E5-4000-8B45-A76D7DEDCB6F
X-Orchestra-Sig: 75234e1e717a2c068612ffa5927a67e1863be087
X-Orchestra-Thrid: T64ACC960-5614-4119-B9F2-2707D1DD44B9_1467158264750741827
X-Orchestra-Thrid-Sig: c0f215fdee70c8543a59b6045d249e758303b801
X-Orchestra-Account: 60a666eef1c53d01a0c8cb2f0716582e434c8068
From: "Manish Amde" <manish9ue@gmail.com>
To: dev@spark.apache.org
Cc: dev@spark.apache.org
Subject: Re: reduce, transform, combine
Content-Type: multipart/alternative;
 boundary="----Nodemailer-0.5.0-?=_1-1399223197638"
X-Virus-Checked: Checked by ClamAV on apache.org

------Nodemailer-0.5.0-?=_1-1399223197638
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

Thanks DB. I will work with mapPartition for now.=C2=A0


Question to the community in general: should we consider adding such an =
operation to RDDs especially as a developer API=3F

On Sun, May 4, 2014 at 1:41 AM, DB Tsai <dbtsai@stanford.edu> wrote:

> You could easily achieve this by mapPartition. However, it seems that it
> can not be done by using aggregate type of operation. I can see that it's=
 a
> general useful operation. For now, you could use mapPartition.
> Sincerely,
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
> On Sun, May 4, 2014 at 1:12 AM, Manish Amde <manish9ue@gmail.com> wrote:
>> I am currently using the RDD aggregate operation to reduce (fold) per
>> partition and then combine using the RDD aggregate operation.
>> def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) =3D> U, combOp: =
(U, U)
>> =3D> U): U
>>
>> I need to perform a transform operation after the seqOp and before the
>> combOp. The signature would look like
>> def foldTransformCombine[U: ClassTag](zeroReduceValue: V, =
zeroCombineValue:
>> U)(seqOp: (V, T) =3D> V, transformOp: (V) =3D> U, combOp: (U, U) =3D> =
U): U
>>
>> This is especially useful in the scenario where the transformOp is
>> expensive and should be performed once per partition before combining. =
Is
>> there a way to accomplish this with existing RDD operations=3F If yes, =
great
>> but if not, should we consider adding such a general transformation to =
the
>> list of RDD operations=3F
>>
>> -Manish
>>
------Nodemailer-0.5.0-?=_1-1399223197638--

From dev-return-7496-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May  4 17:36:11 2014
Return-Path: <dev-return-7496-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1A26111491
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  4 May 2014 17:36:11 +0000 (UTC)
Received: (qmail 71850 invoked by uid 500); 4 May 2014 17:36:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71742 invoked by uid 500); 4 May 2014 17:36:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71733 invoked by uid 99); 4 May 2014 17:36:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 17:36:09 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.214.174] (HELO mail-ob0-f174.google.com) (209.85.214.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 17:36:05 +0000
Received: by mail-ob0-f174.google.com with SMTP id uz6so664535obc.19
        for <dev@spark.apache.org>; Sun, 04 May 2014 10:35:43 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=0dBu7I1kio8DE3zp9fB5O8AoibSYrmxS04V1ClhzwtA=;
        b=TuFtxHSWsS4sZt+oEnHdPSmNFLK1Eq81LNbsRNn0RskifEIrF1JzYJ141UQQaAKkW1
         hPb5df3sSHvAQdDGd4LGi7WHBhOEmmOsyOWT0h8SBhNob3jKt/X8B3Fro7TzXwTm4Usv
         lzmdpul/No4l1EjF8CwrUwbmlJpyc9DjXD3kSksRg/Hpgw5pmRlu5svnJFdfDn54TdgZ
         67KFmg8/rBMbgg0DhXipHgMgDdcpWKKOAEeUb9cN8tTekJXH+Krky05tdErjm8k8H3jR
         RZIOLxDnmnyC8ljwXrfIDHoMnp1xtybbdpdLmQKWfW/BhyfdJVBZlULhFAl273QvqhJw
         KDXA==
X-Gm-Message-State: ALoCoQn1ezQz3YUKAoQYxmvD6tz7NSGuScdeL05+BmSw7VSzHfNNqtpYgurJSIg78fBhoxUie1CL
MIME-Version: 1.0
X-Received: by 10.182.29.33 with SMTP id g1mr3006366obh.53.1399224943832; Sun,
 04 May 2014 10:35:43 -0700 (PDT)
Received: by 10.182.184.40 with HTTP; Sun, 4 May 2014 10:35:42 -0700 (PDT)
Date: Sun, 4 May 2014 10:35:42 -0700
Message-ID: <CAJ3iqPRnJp-ezZAEJJDd4OSeF4f0qmEbaQAAJo2Y7YSrRXZc5A@mail.gmail.com>
Subject: bug using kryo as closure serializer
From: Soren Macbeth <soren@yieldbot.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2b8b2bd0c9c04f896712b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2b8b2bd0c9c04f896712b
Content-Type: text/plain; charset=UTF-8

apologies for the cross-list posts, but I've gotten zero response in the
user list and I guess this list is probably more appropriate.

According to the documentation, using the KryoSerializer for closures is
supported. However, when I try to set `spark.closure.serializer` to
`org.apache.spark.serializer.KryoSerializer` thing fail pretty miserably.

The first thing that happens it that is throws exceptions over and over
that it cannot locate my registrator class, which is located in my assembly
jar like so:

14/05/04 12:03:20 ERROR serializer.KryoSerializer: Failed to run
spark.kryo.registrator
java.lang.ClassNotFoundException: pickles.kryo.PicklesRegistrator
at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:270)
at
org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$2.apply(KryoSerializer.scala:63)
at
org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$2.apply(KryoSerializer.scala:61)
at scala.Option.foreach(Option.scala:236)
at
org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:61)
at
org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:116)
at
org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:79)
at
org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:180)
at
org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
at
org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)
at
org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:724)

Now, I would expect it not to be able to find this class since it hasn't
yet fetched my assembly jar to the executors. Once it does fetch my jar,
those expections stop. Next, all the executor task die with the following
exception:

java.nio.ReadOnlyBufferException
at java.nio.ByteBuffer.array(ByteBuffer.java:961)
at
org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:136)
at
org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:193)
at
org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
at
org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:415)
at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)
at
org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:724)

AFAIK, I'm not doing anything out of the ordinary, just turning on kryo and
using the registrator mechanism to register a couple custom serializers.

The reason I tried turning on kryo for closure in the first place is
because of a different bug that I was hitting during fetching and
deserializing of tasks from my executors, which I detailed here:

http://apache-spark-user-list.1001560.n3.nabble.com/Crazy-Kryo-Exception-td5257.html

Here's hoping some on this list can help me track down what's happening as
I didn't get a single reply on the user list.

--001a11c2b8b2bd0c9c04f896712b--

From dev-return-7497-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May  4 17:36:52 2014
Return-Path: <dev-return-7497-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4393C11495
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  4 May 2014 17:36:52 +0000 (UTC)
Received: (qmail 72783 invoked by uid 500); 4 May 2014 17:36:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72660 invoked by uid 500); 4 May 2014 17:36:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72646 invoked by uid 99); 4 May 2014 17:36:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 17:36:50 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of prodigyaj@gmail.com does not designate 216.139.250.139 as permitted sender)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 17:36:45 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <prodigyaj@gmail.com>)
	id 1Wh0KW-0004hy-A2
	for dev@spark.incubator.apache.org; Sun, 04 May 2014 10:36:00 -0700
Date: Sun, 4 May 2014 10:35:45 -0700 (PDT)
From: Ajay Nair <prodigyaj@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1399224945287-6472.post@n3.nabble.com>
In-Reply-To: <8214075A-7014-4B0B-9312-2F1450793390@ngarneau.com>
References: <1399131773131-6459.post@n3.nabble.com> <05986E28-BAE8-48D7-9BE7-6314A053899D@ngarneau.com> <0053754E-9D2F-4C6E-8E99-ACD34B2D8BAF@ngarneau.com> <1399138731449-6465.post@n3.nabble.com> <8214075A-7014-4B0B-9312-2F1450793390@ngarneau.com>
Subject: Re: Apache Spark running out of the spark shell
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Thank you. I am trying this now



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-running-out-of-the-spark-shell-tp6459p6472.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7498-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May  4 17:48:28 2014
Return-Path: <dev-return-7498-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A1455114B4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  4 May 2014 17:48:28 +0000 (UTC)
Received: (qmail 82517 invoked by uid 500); 4 May 2014 17:48:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82426 invoked by uid 500); 4 May 2014 17:48:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82418 invoked by uid 99); 4 May 2014 17:48:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 17:48:26 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [173.201.192.105] (HELO p3plsmtpa06-04.prod.phx3.secureserver.net) (173.201.192.105)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 17:48:22 +0000
Received: from [192.168.0.105] ([74.58.20.150])
	by p3plsmtpa06-04.prod.phx3.secureserver.net with 
	id xtnz1n00B3EGyu201to0vH; Sun, 04 May 2014 10:48:01 -0700
From: Nicolas Garneau <ngarneau@ngarneau.com>
Content-Type: multipart/alternative; boundary="Apple-Mail=_C0F6C5F7-3757-4305-A92F-020C50041C1C"
Message-Id: <8C94C071-0A99-44E0-B025-8EB468EFE85E@ngarneau.com>
Mime-Version: 1.0 (Mac OS X Mail 6.6 \(1510\))
Subject: Re: Apache Spark running out of the spark shell
Date: Sun, 4 May 2014 13:47:59 -0400
References: <1399131773131-6459.post@n3.nabble.com> <05986E28-BAE8-48D7-9BE7-6314A053899D@ngarneau.com> <0053754E-9D2F-4C6E-8E99-ACD34B2D8BAF@ngarneau.com> <1399138731449-6465.post@n3.nabble.com> <8214075A-7014-4B0B-9312-2F1450793390@ngarneau.com> <1399224945287-6472.post@n3.nabble.com>
To: dev@spark.apache.org
In-Reply-To: <1399224945287-6472.post@n3.nabble.com>
X-Mailer: Apple Mail (2.1510)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_C0F6C5F7-3757-4305-A92F-020C50041C1C
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=iso-8859-1

Hey AJ,

If you plan to launch your job on a cluster, consider using the =
spark-submit command.
Running this in the spark's home directory gives you a help on how to =
use this:

$ ./bin/spark-submit

I haven't tried it yet but considering this post, it will be the =
preferred way to launch jobs:
=
http://apache-spark-user-list.1001560.n3.nabble.com/Running-a-spark-submit=
-compatible-app-in-spark-shell-td4905.html

Cheers

Le 2014-05-04 =E0 13:35, Ajay Nair <prodigyaj@gmail.com> a =E9crit :

> Thank you. I am trying this now
>=20
>=20
>=20
> --
> View this message in context: =
http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-run=
ning-out-of-the-spark-shell-tp6459p6472.html
> Sent from the Apache Spark Developers List mailing list archive at =
Nabble.com.
>=20

Nicolas Garneau
ngarneau@ngarneau.com


--Apple-Mail=_C0F6C5F7-3757-4305-A92F-020C50041C1C--

From dev-return-7499-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May  4 19:51:27 2014
Return-Path: <dev-return-7499-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 66AFE11680
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  4 May 2014 19:51:27 +0000 (UTC)
Received: (qmail 82669 invoked by uid 500); 4 May 2014 19:51:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82572 invoked by uid 500); 4 May 2014 19:51:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82559 invoked by uid 99); 4 May 2014 19:51:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 19:51:25 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.47] (HELO mail-qg0-f47.google.com) (209.85.192.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 19:51:21 +0000
Received: by mail-qg0-f47.google.com with SMTP id e89so6816346qgf.20
        for <dev@spark.apache.org>; Sun, 04 May 2014 12:51:00 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=/ZOckFyNvHCPHRyrhARJLupiRhfWkqxzHrPrVA0KN7Y=;
        b=Gk838xS8mZTRWMjgDL7T1FAGPMe0w/rJlj+mSNSBdGToER4oEwwu8rpqdH+xZX20TI
         iJUVIKzv3EkT+uye5kkDLp5H0IhtLzMZm5jLnbf7LYiVcDyFAIn5ForowOsKZB+Dw34g
         APjNSg8pGuIyX2duwf5xqD6ucHfD8M+hh2c4JcsdX3LKF8ZNsTDMXwo6g9LxKpFvZLm0
         UNuY1hdbXsu2J9oFcIVVnTBpRgMSCP8trjG2i09w5YHr1HqXSf+qklfCPWuptwpgFAFj
         JSy7DcsMK7zlZQDWeqGI6ogpFGwr3hpHRtp0pgyqE6ukycPwr89b+mktVMaYyUh+D3hz
         8CYw==
X-Gm-Message-State: ALoCoQlMrnPQNnHnzRzejAcZ8fk+wn9M5b+vIo5Sau7eakyWrWXQns2G3oPAXKmb1nv1d5+1cE/Y
MIME-Version: 1.0
X-Received: by 10.140.95.164 with SMTP id i33mr12025477qge.6.1399233059976;
 Sun, 04 May 2014 12:50:59 -0700 (PDT)
Received: by 10.96.126.1 with HTTP; Sun, 4 May 2014 12:50:59 -0700 (PDT)
In-Reply-To: <CAJ3iqPRnJp-ezZAEJJDd4OSeF4f0qmEbaQAAJo2Y7YSrRXZc5A@mail.gmail.com>
References: <CAJ3iqPRnJp-ezZAEJJDd4OSeF4f0qmEbaQAAJo2Y7YSrRXZc5A@mail.gmail.com>
Date: Sun, 4 May 2014 12:50:59 -0700
Message-ID: <CAPh_B=abv5w67MOsBESXku_OFNY_TsZFmAYnu-EGM+7GWEe2VQ@mail.gmail.com>
Subject: Re: bug using kryo as closure serializer
From: Reynold Xin <rxin@databricks.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c161e27fa4da04f8985542
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c161e27fa4da04f8985542
Content-Type: text/plain; charset=UTF-8

I added the config option to use the non-default serializer. However, at
the time, Kryo fails serializing pretty much any closures so that option
was never really used / recommended.

Since then the Scala ecosystem has developed, and some other projects are
starting to use Kryo to serialize more Scala data structures, so I wouldn't
be surprised if there is a way to work around this now. However, I don't
have enough time to look into it at this point. If you do, please do post
your findings. Thanks.



On Sun, May 4, 2014 at 10:35 AM, Soren Macbeth <soren@yieldbot.com> wrote:

> apologies for the cross-list posts, but I've gotten zero response in the
> user list and I guess this list is probably more appropriate.
>
> According to the documentation, using the KryoSerializer for closures is
> supported. However, when I try to set `spark.closure.serializer` to
> `org.apache.spark.serializer.KryoSerializer` thing fail pretty miserably.
>
> The first thing that happens it that is throws exceptions over and over
> that it cannot locate my registrator class, which is located in my assembly
> jar like so:
>
> 14/05/04 12:03:20 ERROR serializer.KryoSerializer: Failed to run
> spark.kryo.registrator
> java.lang.ClassNotFoundException: pickles.kryo.PicklesRegistrator
> at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
> at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
> at java.security.AccessController.doPrivileged(Native Method)
> at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
> at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
> at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
> at java.lang.Class.forName0(Native Method)
> at java.lang.Class.forName(Class.java:270)
> at
>
> org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$2.apply(KryoSerializer.scala:63)
> at
>
> org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$2.apply(KryoSerializer.scala:61)
> at scala.Option.foreach(Option.scala:236)
> at
> org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:61)
> at
>
> org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:116)
> at
>
> org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:79)
> at
>
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:180)
> at
>
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
> at
>
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
> at java.security.AccessController.doPrivileged(Native Method)
> at javax.security.auth.Subject.doAs(Subject.java:415)
> at
>
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)
> at
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
> at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
> at
>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> at
>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> at java.lang.Thread.run(Thread.java:724)
>
> Now, I would expect it not to be able to find this class since it hasn't
> yet fetched my assembly jar to the executors. Once it does fetch my jar,
> those expections stop. Next, all the executor task die with the following
> exception:
>
> java.nio.ReadOnlyBufferException
> at java.nio.ByteBuffer.array(ByteBuffer.java:961)
> at
>
> org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:136)
> at
>
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:193)
> at
>
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
> at
>
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
> at java.security.AccessController.doPrivileged(Native Method)
> at javax.security.auth.Subject.doAs(Subject.java:415)
> at
>
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)
> at
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
> at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
> at
>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> at
>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> at java.lang.Thread.run(Thread.java:724)
>
> AFAIK, I'm not doing anything out of the ordinary, just turning on kryo and
> using the registrator mechanism to register a couple custom serializers.
>
> The reason I tried turning on kryo for closure in the first place is
> because of a different bug that I was hitting during fetching and
> deserializing of tasks from my executors, which I detailed here:
>
>
> http://apache-spark-user-list.1001560.n3.nabble.com/Crazy-Kryo-Exception-td5257.html
>
> Here's hoping some on this list can help me track down what's happening as
> I didn't get a single reply on the user list.
>

--001a11c161e27fa4da04f8985542--

From dev-return-7500-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May  4 20:39:55 2014
Return-Path: <dev-return-7500-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E01AA1172F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  4 May 2014 20:39:55 +0000 (UTC)
Received: (qmail 26436 invoked by uid 500); 4 May 2014 20:39:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26366 invoked by uid 500); 4 May 2014 20:39:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26358 invoked by uid 99); 4 May 2014 20:39:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 20:39:53 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.216.48 as permitted sender)
Received: from [209.85.216.48] (HELO mail-qa0-f48.google.com) (209.85.216.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 20:39:49 +0000
Received: by mail-qa0-f48.google.com with SMTP id i13so1603195qae.7
        for <dev@spark.apache.org>; Sun, 04 May 2014 13:39:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=d24R6FFPrQDai3pyPd5sypAoy4gAsTc2/PKmw68PgU0=;
        b=diSdOsher2aWRKWvHxmACgb7MkSJBUDTWslCf3iv/Qwlgd+y0zwg+kb58nDkTiYtR1
         30iDcXmg3qKVT8by2CvSsuoK6V6TTNXzcP35Iicafd2vw/x1ZIHaYcvUr/Al3KkEK6dB
         uq/ezysTUaXsZuOwHNQ8ZhjtmucTeUG9wSWrP03mATLNeK7kXA2vQcbFKTBOJbiWXZg9
         ozDKwSd06dpYd1NrLF/RuE5xNc2hhM1c/UJzse+weH7DmMiS8r6JcmRF7E3SsZARK0SN
         Rr0DCVyG6T/ZzoA+6+NhETzCW7L6zzwsF5WM8c5ujmYXjQvFeIyhXCzL0x2O971GADDU
         Trrw==
MIME-Version: 1.0
X-Received: by 10.224.163.73 with SMTP id z9mr39722437qax.90.1399235966572;
 Sun, 04 May 2014 13:39:26 -0700 (PDT)
Received: by 10.140.21.37 with HTTP; Sun, 4 May 2014 13:39:26 -0700 (PDT)
In-Reply-To: <CAPh_B=abv5w67MOsBESXku_OFNY_TsZFmAYnu-EGM+7GWEe2VQ@mail.gmail.com>
References: <CAJ3iqPRnJp-ezZAEJJDd4OSeF4f0qmEbaQAAJo2Y7YSrRXZc5A@mail.gmail.com>
	<CAPh_B=abv5w67MOsBESXku_OFNY_TsZFmAYnu-EGM+7GWEe2VQ@mail.gmail.com>
Date: Mon, 5 May 2014 02:09:26 +0530
Message-ID: <CAJiQeYLabCHSr3fF6ojsjtL1z0w-U5opWoRkJavLuNo0vNCXvg@mail.gmail.com>
Subject: Re: bug using kryo as closure serializer
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On a slightly related note (apologies Soren for hijacking the thread),
Reynold how much better is kryo from spark's usage point of view
compared to the default java serialization (in general, not for
closures) ?
The numbers on kyro site are interesting, but since you have played
the most with kryo in context of spark (i think) - how do you rate it
along lines of :

1) computational overhead compared to java serialization.
2) memory overhead.
3) generated byte[] size.


Particularly given the bugs Patrick and I had looked into in past
along flush, etc I was always skeptical about using kyro.
But given the pretty nasty issues with OOM's via java serialization we
are seeing, wanted to know your thoughts on use of kyro with spark.
(Will be slightly involved to ensure everything gets registered, but I
want to go down the path assuming I hear good things in context of
spark)

Thanks,
Mridul


On Mon, May 5, 2014 at 1:20 AM, Reynold Xin <rxin@databricks.com> wrote:
> I added the config option to use the non-default serializer. However, at
> the time, Kryo fails serializing pretty much any closures so that option
> was never really used / recommended.
>
> Since then the Scala ecosystem has developed, and some other projects are
> starting to use Kryo to serialize more Scala data structures, so I wouldn't
> be surprised if there is a way to work around this now. However, I don't
> have enough time to look into it at this point. If you do, please do post
> your findings. Thanks.
>
>
>
> On Sun, May 4, 2014 at 10:35 AM, Soren Macbeth <soren@yieldbot.com> wrote:
>
>> apologies for the cross-list posts, but I've gotten zero response in the
>> user list and I guess this list is probably more appropriate.
>>
>> According to the documentation, using the KryoSerializer for closures is
>> supported. However, when I try to set `spark.closure.serializer` to
>> `org.apache.spark.serializer.KryoSerializer` thing fail pretty miserably.
>>
>> The first thing that happens it that is throws exceptions over and over
>> that it cannot locate my registrator class, which is located in my assembly
>> jar like so:
>>
>> 14/05/04 12:03:20 ERROR serializer.KryoSerializer: Failed to run
>> spark.kryo.registrator
>> java.lang.ClassNotFoundException: pickles.kryo.PicklesRegistrator
>> at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
>> at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
>> at java.security.AccessController.doPrivileged(Native Method)
>> at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
>> at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
>> at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
>> at java.lang.Class.forName0(Native Method)
>> at java.lang.Class.forName(Class.java:270)
>> at
>>
>> org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$2.apply(KryoSerializer.scala:63)
>> at
>>
>> org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$2.apply(KryoSerializer.scala:61)
>> at scala.Option.foreach(Option.scala:236)
>> at
>> org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:61)
>> at
>>
>> org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:116)
>> at
>>
>> org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:79)
>> at
>>
>> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:180)
>> at
>>
>> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
>> at
>>
>> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
>> at java.security.AccessController.doPrivileged(Native Method)
>> at javax.security.auth.Subject.doAs(Subject.java:415)
>> at
>>
>> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)
>> at
>> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
>> at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
>> at
>>
>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>> at
>>
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>> at java.lang.Thread.run(Thread.java:724)
>>
>> Now, I would expect it not to be able to find this class since it hasn't
>> yet fetched my assembly jar to the executors. Once it does fetch my jar,
>> those expections stop. Next, all the executor task die with the following
>> exception:
>>
>> java.nio.ReadOnlyBufferException
>> at java.nio.ByteBuffer.array(ByteBuffer.java:961)
>> at
>>
>> org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:136)
>> at
>>
>> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:193)
>> at
>>
>> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
>> at
>>
>> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
>> at java.security.AccessController.doPrivileged(Native Method)
>> at javax.security.auth.Subject.doAs(Subject.java:415)
>> at
>>
>> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)
>> at
>> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
>> at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
>> at
>>
>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>> at
>>
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>> at java.lang.Thread.run(Thread.java:724)
>>
>> AFAIK, I'm not doing anything out of the ordinary, just turning on kryo and
>> using the registrator mechanism to register a couple custom serializers.
>>
>> The reason I tried turning on kryo for closure in the first place is
>> because of a different bug that I was hitting during fetching and
>> deserializing of tasks from my executors, which I detailed here:
>>
>>
>> http://apache-spark-user-list.1001560.n3.nabble.com/Crazy-Kryo-Exception-td5257.html
>>
>> Here's hoping some on this list can help me track down what's happening as
>> I didn't get a single reply on the user list.
>>

From dev-return-7501-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May  4 22:24:46 2014
Return-Path: <dev-return-7501-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3057E11868
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  4 May 2014 22:24:46 +0000 (UTC)
Received: (qmail 11461 invoked by uid 500); 4 May 2014 22:24:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11363 invoked by uid 500); 4 May 2014 22:24:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11355 invoked by uid 99); 4 May 2014 22:24:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 22:24:44 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of prodigyaj@gmail.com does not designate 216.139.250.139 as permitted sender)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 22:24:40 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <prodigyaj@gmail.com>)
	id 1Wh4p8-0000sB-IK
	for dev@spark.incubator.apache.org; Sun, 04 May 2014 15:23:54 -0700
Date: Sun, 4 May 2014 15:23:39 -0700 (PDT)
From: Ajay Nair <prodigyaj@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1399242219549-6478.post@n3.nabble.com>
In-Reply-To: <8C94C071-0A99-44E0-B025-8EB468EFE85E@ngarneau.com>
References: <1399131773131-6459.post@n3.nabble.com> <05986E28-BAE8-48D7-9BE7-6314A053899D@ngarneau.com> <0053754E-9D2F-4C6E-8E99-ACD34B2D8BAF@ngarneau.com> <1399138731449-6465.post@n3.nabble.com> <8214075A-7014-4B0B-9312-2F1450793390@ngarneau.com> <1399224945287-6472.post@n3.nabble.com> <8C94C071-0A99-44E0-B025-8EB468EFE85E@ngarneau.com>
Subject: Re: Apache Spark running out of the spark shell
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Now I got it to work .. well almost. However I needed to copy the project/
folder to the spark-standalone folder as the package build was failing
because it could not find buil properties. After the copy the build was
successful. However when I run it I get errors but it still gives me the
output.

[error] 14/05/04 21:58:19 INFO spark.SparkContext: Job finished: count at
SimpleApp.scala:11, took 0.040651597 s
[error] 14/05/04 21:58:19 INFO scheduler.TaskSetManager: Finished TID 3 in
17 ms on localhost (progress: 2/2)
[info] Lines with a: 3, Lines with b: 2
[error] 14/05/04 21:58:19 INFO scheduler.TaskSchedulerImpl: Removed TaskSet
1.0, whose tasks have all completed, from pool 
[success] Total time: 5 s, completed May 4, 2014 9:58:20 PM


You can see the [info] that contains the output. All the lines i get mention
[errors], any reason why ?

I have configured my ec2 machines master and slave nodes and this code I
think tries to run in the local mode.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-Spark-running-out-of-the-spark-shell-tp6459p6478.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7502-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May  4 22:54:43 2014
Return-Path: <dev-return-7502-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C2763118D5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  4 May 2014 22:54:43 +0000 (UTC)
Received: (qmail 30811 invoked by uid 500); 4 May 2014 22:54:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30727 invoked by uid 500); 4 May 2014 22:54:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30719 invoked by uid 99); 4 May 2014 22:54:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 22:54:41 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.214.173] (HELO mail-ob0-f173.google.com) (209.85.214.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 22:54:37 +0000
Received: by mail-ob0-f173.google.com with SMTP id wm4so4434590obc.32
        for <dev@spark.apache.org>; Sun, 04 May 2014 15:54:16 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=3tnGI1Vp4fWDS2uV4SjYICnEsj/rBdZvL4tMcfJakRo=;
        b=F98YLDgQsRg3JSp7jkNruR4igS4UeQxFsi+P4/qZqLYvLjrZgeOVEVWrSNbrKIykv8
         x2HLP48UQlAh031edBHymGUD3jVU0Y5ZL4E14PujiS/3q4NoISJFv304OKHdD2U8dWJy
         dyljyq7x7WFhu03E0h39xlnTSrXBaUQ/gDHNPKM4M96XEo5St/67gH61IqXPlu4MLL8R
         b3UG41GWSHmcXVte0yk6pXE1DrhsnRZUskw5MnRJqbIilnVoa2KtKBP3D51iJCKDPX90
         3ZNstbsGjDETbDWIU8II/0ZKDHE8XiEweGEd82QT7wLXIGROkdWAC5DhWdAjuBncZ63F
         HFMw==
X-Gm-Message-State: ALoCoQmhxhiEvq3bcWM4yhbHNJ7a309HGvVOEsbwDa78FGO69sRFsK9D8V9fhiRdZh+WoukPNIvI
MIME-Version: 1.0
X-Received: by 10.60.16.103 with SMTP id f7mr29552200oed.8.1399244056661; Sun,
 04 May 2014 15:54:16 -0700 (PDT)
Received: by 10.182.184.40 with HTTP; Sun, 4 May 2014 15:54:16 -0700 (PDT)
In-Reply-To: <CAPh_B=abv5w67MOsBESXku_OFNY_TsZFmAYnu-EGM+7GWEe2VQ@mail.gmail.com>
References: <CAJ3iqPRnJp-ezZAEJJDd4OSeF4f0qmEbaQAAJo2Y7YSrRXZc5A@mail.gmail.com>
	<CAPh_B=abv5w67MOsBESXku_OFNY_TsZFmAYnu-EGM+7GWEe2VQ@mail.gmail.com>
Date: Sun, 4 May 2014 15:54:16 -0700
Message-ID: <CAJ3iqPQqdZVztd+nnaOkueCNkkJQS2RfkeRP_iSvCZomYcyjOQ@mail.gmail.com>
Subject: Re: bug using kryo as closure serializer
From: Soren Macbeth <soren@yieldbot.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0149c04ef3a77404f89ae4c8
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149c04ef3a77404f89ae4c8
Content-Type: text/plain; charset=UTF-8

Thanks for the reply!

Ok, if that's the case, I'd recommend a note to that affect in the docs at
least.

Just to give some more context here, I'm working on a Clojure DSL for Spark
called Flambo, which I plan to open source shortly. If I could I'd like to
focus on the initial bug that I hit.

Exception in thread "main" org.apache.spark.SparkException: Job aborted:
Exception while deserializing and fetching task:
com.esotericsoftware.kryo.KryoException:
java.lang.IllegalArgumentException: Can not set final
scala.collection.convert.Wrappers field
scala.collection.convert.Wrappers$SeqWrapper.$outer to
clojure.lang.PersistentVector
Serialization trace:
$outer (scala.collection.convert.Wrappers$SeqWrapper)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1020)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1018)
        at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at
scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1018)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
        at scala.Option.foreach(Option.scala:236)
        at
org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:604)
        at
org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:190)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at
scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)

This happens immediately after all the tasks of a reduce stage complete
successfully. Here is the function throwing the exception:

https://github.com/apache/spark/blob/4bc07eebbf5e2ea0c0b6f1642049515025d88d07/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala#L43

This is where I get lost. From googling around, it seems that scala is
trying to wrap the result of my task, which contain
clojure.lang.PersistentVector objects in a scala collection, but I don't
know why it's doing that. I have a registered kryo serializer for
clojure.lang.PersistentVector.

based on this line is looks like it's trying to use the closure serializer,
yet the expection thrown is from com.esotericsoftware.kryo.KryoException:

https://github.com/apache/spark/blob/4bc07eebbf5e2ea0c0b6f1642049515025d88d07/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala#L39

Would storing my RDD as MEMORY_ONLY_SER prevent the closure serializer from
trying to deal with my clojure.lang.PeristentVector class?

Where do I go from here?


On Sun, May 4, 2014 at 12:50 PM, Reynold Xin <rxin@databricks.com> wrote:

> I added the config option to use the non-default serializer. However, at
> the time, Kryo fails serializing pretty much any closures so that option
> was never really used / recommended.
>
> Since then the Scala ecosystem has developed, and some other projects are
> starting to use Kryo to serialize more Scala data structures, so I wouldn't
> be surprised if there is a way to work around this now. However, I don't
> have enough time to look into it at this point. If you do, please do post
> your findings. Thanks.
>
>
>
> On Sun, May 4, 2014 at 10:35 AM, Soren Macbeth <soren@yieldbot.com> wrote:
>
> > apologies for the cross-list posts, but I've gotten zero response in the
> > user list and I guess this list is probably more appropriate.
> >
> > According to the documentation, using the KryoSerializer for closures is
> > supported. However, when I try to set `spark.closure.serializer` to
> > `org.apache.spark.serializer.KryoSerializer` thing fail pretty miserably.
> >
> > The first thing that happens it that is throws exceptions over and over
> > that it cannot locate my registrator class, which is located in my
> assembly
> > jar like so:
> >
> > 14/05/04 12:03:20 ERROR serializer.KryoSerializer: Failed to run
> > spark.kryo.registrator
> > java.lang.ClassNotFoundException: pickles.kryo.PicklesRegistrator
> > at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
> > at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
> > at java.security.AccessController.doPrivileged(Native Method)
> > at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
> > at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
> > at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
> > at java.lang.Class.forName0(Native Method)
> > at java.lang.Class.forName(Class.java:270)
> > at
> >
> >
> org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$2.apply(KryoSerializer.scala:63)
> > at
> >
> >
> org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$2.apply(KryoSerializer.scala:61)
> > at scala.Option.foreach(Option.scala:236)
> > at
> >
> org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:61)
> > at
> >
> >
> org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:116)
> > at
> >
> >
> org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:79)
> > at
> >
> >
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:180)
> > at
> >
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
> > at
> >
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
> > at java.security.AccessController.doPrivileged(Native Method)
> > at javax.security.auth.Subject.doAs(Subject.java:415)
> > at
> >
> >
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)
> > at
> >
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
> > at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
> > at
> >
> >
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> > at
> >
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> > at java.lang.Thread.run(Thread.java:724)
> >
> > Now, I would expect it not to be able to find this class since it hasn't
> > yet fetched my assembly jar to the executors. Once it does fetch my jar,
> > those expections stop. Next, all the executor task die with the following
> > exception:
> >
> > java.nio.ReadOnlyBufferException
> > at java.nio.ByteBuffer.array(ByteBuffer.java:961)
> > at
> >
> >
> org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:136)
> > at
> >
> >
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:193)
> > at
> >
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
> > at
> >
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
> > at java.security.AccessController.doPrivileged(Native Method)
> > at javax.security.auth.Subject.doAs(Subject.java:415)
> > at
> >
> >
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)
> > at
> >
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
> > at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
> > at
> >
> >
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> > at
> >
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> > at java.lang.Thread.run(Thread.java:724)
> >
> > AFAIK, I'm not doing anything out of the ordinary, just turning on kryo
> and
> > using the registrator mechanism to register a couple custom serializers.
> >
> > The reason I tried turning on kryo for closure in the first place is
> > because of a different bug that I was hitting during fetching and
> > deserializing of tasks from my executors, which I detailed here:
> >
> >
> >
> http://apache-spark-user-list.1001560.n3.nabble.com/Crazy-Kryo-Exception-td5257.html
> >
> > Here's hoping some on this list can help me track down what's happening
> as
> > I didn't get a single reply on the user list.
> >
>

--089e0149c04ef3a77404f89ae4c8--

From dev-return-7503-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May  4 23:25:25 2014
Return-Path: <dev-return-7503-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EB8BC11935
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  4 May 2014 23:25:24 +0000 (UTC)
Received: (qmail 50896 invoked by uid 500); 4 May 2014 23:25:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50798 invoked by uid 500); 4 May 2014 23:25:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50779 invoked by uid 99); 4 May 2014 23:25:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 23:25:18 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [173.201.192.103] (HELO p3plsmtpa06-02.prod.phx3.secureserver.net) (173.201.192.103)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 04 May 2014 23:25:14 +0000
Received: from [192.168.0.118] ([192.226.138.91])
	by p3plsmtpa06-02.prod.phx3.secureserver.net with 
	id xzQh1n0011yWK4Y01zQj97; Sun, 04 May 2014 16:24:46 -0700
Subject: Re: Apache Spark running out of the spark shell
References: <1399131773131-6459.post@n3.nabble.com> <05986E28-BAE8-48D7-9BE7-6314A053899D@ngarneau.com> <0053754E-9D2F-4C6E-8E99-ACD34B2D8BAF@ngarneau.com> <1399138731449-6465.post@n3.nabble.com> <8214075A-7014-4B0B-9312-2F1450793390@ngarneau.com> <1399224945287-6472.post@n3.nabble.com> <8C94C071-0A99-44E0-B025-8EB468EFE85E@ngarneau.com> <1399242219549-6478.post@n3.nabble.com>
From: Nicolas Garneau <ngarneau@ngarneau.com>
Content-Type: text/plain;
	charset=us-ascii
X-Mailer: iPhone Mail (11D201)
In-Reply-To: <1399242219549-6478.post@n3.nabble.com>
Message-Id: <55E388D3-FC15-477E-A823-8340036E46E1@ngarneau.com>
Date: Sun, 4 May 2014 19:24:38 -0400
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Mime-Version: 1.0 (1.0)
X-Virus-Checked: Checked by ClamAV on apache.org

Hey AJ,

I have tried to run on a cluster yet, only on local mode.
I'll try to get something running on a cluster soon and keep you posted.

Nicolas Garneau

> On May 4, 2014, at 6:23 PM, Ajay Nair <prodigyaj@gmail.com> wrote:
>=20
> Now I got it to work .. well almost. However I needed to copy the project/=

> folder to the spark-standalone folder as the package build was failing
> because it could not find buil properties. After the copy the build was
> successful. However when I run it I get errors but it still gives me the
> output.
>=20
> [error] 14/05/04 21:58:19 INFO spark.SparkContext: Job finished: count at
> SimpleApp.scala:11, took 0.040651597 s
> [error] 14/05/04 21:58:19 INFO scheduler.TaskSetManager: Finished TID 3 in=

> 17 ms on localhost (progress: 2/2)
> [info] Lines with a: 3, Lines with b: 2
> [error] 14/05/04 21:58:19 INFO scheduler.TaskSchedulerImpl: Removed TaskSe=
t
> 1.0, whose tasks have all completed, from pool=20
> [success] Total time: 5 s, completed May 4, 2014 9:58:20 PM
>=20
>=20
> You can see the [info] that contains the output. All the lines i get menti=
on
> [errors], any reason why ?
>=20
> I have configured my ec2 machines master and slave nodes and this code I
> think tries to run in the local mode.
>=20
>=20
>=20
> --
> View this message in context: http://apache-spark-developers-list.1001551.=
n3.nabble.com/Apache-Spark-running-out-of-the-spark-shell-tp6459p6478.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.=
com.
>=20

From dev-return-7504-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May  5 03:44:58 2014
Return-Path: <dev-return-7504-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C00DA11C4B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 May 2014 03:44:58 +0000 (UTC)
Received: (qmail 61804 invoked by uid 500); 5 May 2014 03:44:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61119 invoked by uid 500); 5 May 2014 03:44:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61110 invoked by uid 99); 5 May 2014 03:44:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 03:44:54 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.45] (HELO mail-qg0-f45.google.com) (209.85.192.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 03:44:50 +0000
Received: by mail-qg0-f45.google.com with SMTP id z60so1351337qgd.18
        for <dev@spark.apache.org>; Sun, 04 May 2014 20:44:27 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=rd4n1QYoi+W4O9etq0iA5XwahPd6srEFLUQVq6k1k5Y=;
        b=co29B8HNZNQ9bx3C+XzReGsz3ThZDgUUBmUdHzWJgKu8z4naYGjyJVU3aC25EDV9YA
         WfY/OmdlNiMYR7iSlsWv/pRqDAZSmnaoeBCaeAOcPpJlYWbs29jTgXO1inpB5kJ77ftH
         ndvBKUfxTWXgWGQbLj7gxkXUEHWV1lSllvmXnyBRUGXW7SPZls2Z3yGwZ1Eu5PyAMG0B
         HRzRRjPxZHpiuV/IV3TldGyJ7huipkQUiUuIWlMKODwcJnlsgSxNlv5dcFJK1Thc1+18
         tgwx1b1ArMm12oA+GlbTfB6lVC3a+wbdLRXY/cFyg2Qj9c6seDqmjG0FhWI+MUtriPIJ
         aocg==
X-Gm-Message-State: ALoCoQmxPkbRS0wTg+Yd7dsT78suyTSjvFoxFa+94EBEH2WsrzCbf6+TB3Gha4PKNSBrAd2Z0fp8
MIME-Version: 1.0
X-Received: by 10.224.49.67 with SMTP id u3mr41598621qaf.63.1399261467280;
 Sun, 04 May 2014 20:44:27 -0700 (PDT)
Received: by 10.96.126.1 with HTTP; Sun, 4 May 2014 20:44:26 -0700 (PDT)
In-Reply-To: <CAJiQeYLabCHSr3fF6ojsjtL1z0w-U5opWoRkJavLuNo0vNCXvg@mail.gmail.com>
References: <CAJ3iqPRnJp-ezZAEJJDd4OSeF4f0qmEbaQAAJo2Y7YSrRXZc5A@mail.gmail.com>
	<CAPh_B=abv5w67MOsBESXku_OFNY_TsZFmAYnu-EGM+7GWEe2VQ@mail.gmail.com>
	<CAJiQeYLabCHSr3fF6ojsjtL1z0w-U5opWoRkJavLuNo0vNCXvg@mail.gmail.com>
Date: Sun, 4 May 2014 20:44:26 -0700
Message-ID: <CAPh_B=Z3mNZp4A=B3M9TnQqe6n+foB6fQSmLEkVwB+mUbb=Akg@mail.gmail.com>
Subject: Re: bug using kryo as closure serializer
From: Reynold Xin <rxin@databricks.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2ef80b49bbc04f89ef2cc
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ef80b49bbc04f89ef2cc
Content-Type: text/plain; charset=UTF-8

Kryo does generate code for serialization, so the CPU overhead is quite
lower than Java (which I think just uses reflection). As I understand, they
also have a new implementation that uses unsafe intrinsics, which should
lead to even higher performance.

The generated byte[] size was a lot smaller in Kryo, especially for arrays
of objects of the same type. It doesn't need to write the class name for
every object, and it also reduces the size of ints and such using zig zag
encoding.

I don't have numbers around anymore, but when I was benchmarking them, kryo
was substantially better than java. The only reason it is not on by default
is because it doesn't always work.


On Sun, May 4, 2014 at 1:39 PM, Mridul Muralidharan <mridul@gmail.com>wrote:

> On a slightly related note (apologies Soren for hijacking the thread),
> Reynold how much better is kryo from spark's usage point of view
> compared to the default java serialization (in general, not for
> closures) ?
> The numbers on kyro site are interesting, but since you have played
> the most with kryo in context of spark (i think) - how do you rate it
> along lines of :
>
> 1) computational overhead compared to java serialization.
> 2) memory overhead.
> 3) generated byte[] size.
>
>
> Particularly given the bugs Patrick and I had looked into in past
> along flush, etc I was always skeptical about using kyro.
> But given the pretty nasty issues with OOM's via java serialization we
> are seeing, wanted to know your thoughts on use of kyro with spark.
> (Will be slightly involved to ensure everything gets registered, but I
> want to go down the path assuming I hear good things in context of
> spark)
>
> Thanks,
> Mridul
>
>
> On Mon, May 5, 2014 at 1:20 AM, Reynold Xin <rxin@databricks.com> wrote:
> > I added the config option to use the non-default serializer. However, at
> > the time, Kryo fails serializing pretty much any closures so that option
> > was never really used / recommended.
> >
> > Since then the Scala ecosystem has developed, and some other projects are
> > starting to use Kryo to serialize more Scala data structures, so I
> wouldn't
> > be surprised if there is a way to work around this now. However, I don't
> > have enough time to look into it at this point. If you do, please do post
> > your findings. Thanks.
> >
> >
> >
> > On Sun, May 4, 2014 at 10:35 AM, Soren Macbeth <soren@yieldbot.com>
> wrote:
> >
> >> apologies for the cross-list posts, but I've gotten zero response in the
> >> user list and I guess this list is probably more appropriate.
> >>
> >> According to the documentation, using the KryoSerializer for closures is
> >> supported. However, when I try to set `spark.closure.serializer` to
> >> `org.apache.spark.serializer.KryoSerializer` thing fail pretty
> miserably.
> >>
> >> The first thing that happens it that is throws exceptions over and over
> >> that it cannot locate my registrator class, which is located in my
> assembly
> >> jar like so:
> >>
> >> 14/05/04 12:03:20 ERROR serializer.KryoSerializer: Failed to run
> >> spark.kryo.registrator
> >> java.lang.ClassNotFoundException: pickles.kryo.PicklesRegistrator
> >> at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
> >> at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
> >> at java.security.AccessController.doPrivileged(Native Method)
> >> at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
> >> at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
> >> at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
> >> at java.lang.Class.forName0(Native Method)
> >> at java.lang.Class.forName(Class.java:270)
> >> at
> >>
> >>
> org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$2.apply(KryoSerializer.scala:63)
> >> at
> >>
> >>
> org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$2.apply(KryoSerializer.scala:61)
> >> at scala.Option.foreach(Option.scala:236)
> >> at
> >>
> org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:61)
> >> at
> >>
> >>
> org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:116)
> >> at
> >>
> >>
> org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:79)
> >> at
> >>
> >>
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:180)
> >> at
> >>
> >>
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
> >> at
> >>
> >>
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
> >> at java.security.AccessController.doPrivileged(Native Method)
> >> at javax.security.auth.Subject.doAs(Subject.java:415)
> >> at
> >>
> >>
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)
> >> at
> >>
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
> >> at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
> >> at
> >>
> >>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> >> at
> >>
> >>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> >> at java.lang.Thread.run(Thread.java:724)
> >>
> >> Now, I would expect it not to be able to find this class since it hasn't
> >> yet fetched my assembly jar to the executors. Once it does fetch my jar,
> >> those expections stop. Next, all the executor task die with the
> following
> >> exception:
> >>
> >> java.nio.ReadOnlyBufferException
> >> at java.nio.ByteBuffer.array(ByteBuffer.java:961)
> >> at
> >>
> >>
> org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:136)
> >> at
> >>
> >>
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:193)
> >> at
> >>
> >>
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
> >> at
> >>
> >>
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
> >> at java.security.AccessController.doPrivileged(Native Method)
> >> at javax.security.auth.Subject.doAs(Subject.java:415)
> >> at
> >>
> >>
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)
> >> at
> >>
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
> >> at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
> >> at
> >>
> >>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> >> at
> >>
> >>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> >> at java.lang.Thread.run(Thread.java:724)
> >>
> >> AFAIK, I'm not doing anything out of the ordinary, just turning on kryo
> and
> >> using the registrator mechanism to register a couple custom serializers.
> >>
> >> The reason I tried turning on kryo for closure in the first place is
> >> because of a different bug that I was hitting during fetching and
> >> deserializing of tasks from my executors, which I detailed here:
> >>
> >>
> >>
> http://apache-spark-user-list.1001560.n3.nabble.com/Crazy-Kryo-Exception-td5257.html
> >>
> >> Here's hoping some on this list can help me track down what's happening
> as
> >> I didn't get a single reply on the user list.
> >>
>

--001a11c2ef80b49bbc04f89ef2cc--

From dev-return-7505-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May  5 03:49:07 2014
Return-Path: <dev-return-7505-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 63A3B11C5A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 May 2014 03:49:07 +0000 (UTC)
Received: (qmail 63827 invoked by uid 500); 5 May 2014 03:49:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63791 invoked by uid 500); 5 May 2014 03:49:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63783 invoked by uid 99); 5 May 2014 03:49:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 03:49:04 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.180] (HELO mail-qc0-f180.google.com) (209.85.216.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 03:49:00 +0000
Received: by mail-qc0-f180.google.com with SMTP id i17so6348515qcy.25
        for <dev@spark.apache.org>; Sun, 04 May 2014 20:48:37 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=Jv9q/6kOwV5QqSj8abYC3VRMpljraaWWZb5TzyroYJE=;
        b=TTKoejXqFUuCfQJvmQioDEFbW09/27rjc3v5zD25QHgFfGMe9Bb4MGS2+pyNeMzcVd
         Fys6uTx0XsAWeU5vL8IdBk58HWbaAOCnPkD4mPx47e2KRbamz1IJlqejaQvtkab9yTIW
         7qnCwT9mB7/Vyzun/NQ4dOhWCYgDIihB5N+dx4d1PyXeB87iPIPC39TDLaKHtzzCLbkU
         +HkFqRypUrD314x6OMUPwWHma2j+KRltdN1UE6a4FzX6rzqgWdEiftYHLSWsiDl6VEml
         Li/nlR0ZMdX8SsJjiUdtOL9eIZvcKuvBOU8Y52kaqLM5eTxMau82kYzMy98Dr6fiAyDx
         5LQA==
X-Gm-Message-State: ALoCoQmVAeQVF1TDQOajw72aOttk9STDaE4FYVAjSaZkIVs6PYPKZ/c9tyPZW6TvBdNAP3NV2RrD
MIME-Version: 1.0
X-Received: by 10.224.151.82 with SMTP id b18mr17038478qaw.27.1399261717454;
 Sun, 04 May 2014 20:48:37 -0700 (PDT)
Received: by 10.96.126.1 with HTTP; Sun, 4 May 2014 20:48:37 -0700 (PDT)
In-Reply-To: <CAJ3iqPQqdZVztd+nnaOkueCNkkJQS2RfkeRP_iSvCZomYcyjOQ@mail.gmail.com>
References: <CAJ3iqPRnJp-ezZAEJJDd4OSeF4f0qmEbaQAAJo2Y7YSrRXZc5A@mail.gmail.com>
	<CAPh_B=abv5w67MOsBESXku_OFNY_TsZFmAYnu-EGM+7GWEe2VQ@mail.gmail.com>
	<CAJ3iqPQqdZVztd+nnaOkueCNkkJQS2RfkeRP_iSvCZomYcyjOQ@mail.gmail.com>
Date: Sun, 4 May 2014 20:48:37 -0700
Message-ID: <CAPh_B=asgxMXEBnXtzX1xwz7fOBVBwjVu7=3coY_L5WtatgD3g@mail.gmail.com>
Subject: Re: bug using kryo as closure serializer
From: Reynold Xin <rxin@databricks.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0149ced69e192d04f89f01da
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149ced69e192d04f89f01da
Content-Type: text/plain; charset=UTF-8

Good idea. I submitted a pull request for the doc update here:
https://github.com/apache/spark/pull/642


On Sun, May 4, 2014 at 3:54 PM, Soren Macbeth <soren@yieldbot.com> wrote:

> Thanks for the reply!
>
> Ok, if that's the case, I'd recommend a note to that affect in the docs at
> least.
>
> Just to give some more context here, I'm working on a Clojure DSL for Spark
> called Flambo, which I plan to open source shortly. If I could I'd like to
> focus on the initial bug that I hit.
>
> Exception in thread "main" org.apache.spark.SparkException: Job aborted:
> Exception while deserializing and fetching task:
> com.esotericsoftware.kryo.KryoException:
> java.lang.IllegalArgumentException: Can not set final
> scala.collection.convert.Wrappers field
> scala.collection.convert.Wrappers$SeqWrapper.$outer to
> clojure.lang.PersistentVector
> Serialization trace:
> $outer (scala.collection.convert.Wrappers$SeqWrapper)
>         at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1020)
>         at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1018)
>         at
>
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>         at
> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>         at org.apache.spark.scheduler.DAGScheduler.org
> $apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1018)
>         at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
>         at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
>         at scala.Option.foreach(Option.scala:236)
>         at
>
> org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:604)
>         at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:190)
>         at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>         at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>         at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>         at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>         at
>
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>         at
> scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>         at
>
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>         at
> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>         at
>
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>
> This happens immediately after all the tasks of a reduce stage complete
> successfully. Here is the function throwing the exception:
>
>
> https://github.com/apache/spark/blob/4bc07eebbf5e2ea0c0b6f1642049515025d88d07/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala#L43
>
> This is where I get lost. From googling around, it seems that scala is
> trying to wrap the result of my task, which contain
> clojure.lang.PersistentVector objects in a scala collection, but I don't
> know why it's doing that. I have a registered kryo serializer for
> clojure.lang.PersistentVector.
>
> based on this line is looks like it's trying to use the closure serializer,
> yet the expection thrown is from com.esotericsoftware.kryo.KryoException:
>
>
> https://github.com/apache/spark/blob/4bc07eebbf5e2ea0c0b6f1642049515025d88d07/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala#L39
>
> Would storing my RDD as MEMORY_ONLY_SER prevent the closure serializer from
> trying to deal with my clojure.lang.PeristentVector class?
>
> Where do I go from here?
>
>
> On Sun, May 4, 2014 at 12:50 PM, Reynold Xin <rxin@databricks.com> wrote:
>
> > I added the config option to use the non-default serializer. However, at
> > the time, Kryo fails serializing pretty much any closures so that option
> > was never really used / recommended.
> >
> > Since then the Scala ecosystem has developed, and some other projects are
> > starting to use Kryo to serialize more Scala data structures, so I
> wouldn't
> > be surprised if there is a way to work around this now. However, I don't
> > have enough time to look into it at this point. If you do, please do post
> > your findings. Thanks.
> >
> >
> >
> > On Sun, May 4, 2014 at 10:35 AM, Soren Macbeth <soren@yieldbot.com>
> wrote:
> >
> > > apologies for the cross-list posts, but I've gotten zero response in
> the
> > > user list and I guess this list is probably more appropriate.
> > >
> > > According to the documentation, using the KryoSerializer for closures
> is
> > > supported. However, when I try to set `spark.closure.serializer` to
> > > `org.apache.spark.serializer.KryoSerializer` thing fail pretty
> miserably.
> > >
> > > The first thing that happens it that is throws exceptions over and over
> > > that it cannot locate my registrator class, which is located in my
> > assembly
> > > jar like so:
> > >
> > > 14/05/04 12:03:20 ERROR serializer.KryoSerializer: Failed to run
> > > spark.kryo.registrator
> > > java.lang.ClassNotFoundException: pickles.kryo.PicklesRegistrator
> > > at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
> > > at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
> > > at java.security.AccessController.doPrivileged(Native Method)
> > > at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
> > > at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
> > > at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
> > > at java.lang.Class.forName0(Native Method)
> > > at java.lang.Class.forName(Class.java:270)
> > > at
> > >
> > >
> >
> org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$2.apply(KryoSerializer.scala:63)
> > > at
> > >
> > >
> >
> org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$2.apply(KryoSerializer.scala:61)
> > > at scala.Option.foreach(Option.scala:236)
> > > at
> > >
> >
> org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:61)
> > > at
> > >
> > >
> >
> org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:116)
> > > at
> > >
> > >
> >
> org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:79)
> > > at
> > >
> > >
> >
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:180)
> > > at
> > >
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
> > > at
> > >
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
> > > at java.security.AccessController.doPrivileged(Native Method)
> > > at javax.security.auth.Subject.doAs(Subject.java:415)
> > > at
> > >
> > >
> >
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)
> > > at
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
> > > at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
> > > at
> > >
> > >
> >
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> > > at
> > >
> > >
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> > > at java.lang.Thread.run(Thread.java:724)
> > >
> > > Now, I would expect it not to be able to find this class since it
> hasn't
> > > yet fetched my assembly jar to the executors. Once it does fetch my
> jar,
> > > those expections stop. Next, all the executor task die with the
> following
> > > exception:
> > >
> > > java.nio.ReadOnlyBufferException
> > > at java.nio.ByteBuffer.array(ByteBuffer.java:961)
> > > at
> > >
> > >
> >
> org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:136)
> > > at
> > >
> > >
> >
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:193)
> > > at
> > >
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
> > > at
> > >
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
> > > at java.security.AccessController.doPrivileged(Native Method)
> > > at javax.security.auth.Subject.doAs(Subject.java:415)
> > > at
> > >
> > >
> >
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)
> > > at
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
> > > at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
> > > at
> > >
> > >
> >
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> > > at
> > >
> > >
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> > > at java.lang.Thread.run(Thread.java:724)
> > >
> > > AFAIK, I'm not doing anything out of the ordinary, just turning on kryo
> > and
> > > using the registrator mechanism to register a couple custom
> serializers.
> > >
> > > The reason I tried turning on kryo for closure in the first place is
> > > because of a different bug that I was hitting during fetching and
> > > deserializing of tasks from my executors, which I detailed here:
> > >
> > >
> > >
> >
> http://apache-spark-user-list.1001560.n3.nabble.com/Crazy-Kryo-Exception-td5257.html
> > >
> > > Here's hoping some on this list can help me track down what's happening
> > as
> > > I didn't get a single reply on the user list.
> > >
> >
>

--089e0149ced69e192d04f89f01da--

From dev-return-7506-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May  5 04:09:01 2014
Return-Path: <dev-return-7506-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 69ACC11CB6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 May 2014 04:09:01 +0000 (UTC)
Received: (qmail 77755 invoked by uid 500); 5 May 2014 04:08:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77691 invoked by uid 500); 5 May 2014 04:08:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77585 invoked by uid 99); 5 May 2014 04:08:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 04:08:47 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.219.54] (HELO mail-oa0-f54.google.com) (209.85.219.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 04:08:43 +0000
Received: by mail-oa0-f54.google.com with SMTP id j17so3480211oag.27
        for <dev@spark.apache.org>; Sun, 04 May 2014 21:08:19 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=Ql7DdKItW8fOUs4W757E3bLKRNZBb2RHbw6SeW2MblE=;
        b=Pdo7fIBfqoD9sTUinHQTvaH9lv0kCQsu6IXD2DJdzrOkbuz6vhtvrv1CRJOmOJR8od
         nm3SJlNafyillhoJ/eKzN6j/vyc9+Fd0F8ix5MO9owfrClOxRwBDsA4XE/6DVQQZoavC
         K6jlWf+zzoFgQybGSuouw4yNa6TJEBdBgTKJduMr0As8v3qfAdBKBihYtYi8VHW+7BLs
         idcU3LA08gFDXpJ2Nv5nsNtOwNDmxoAxTll85OGLaEMMwgS+FhHvojZxKSI8GlqTijvh
         F2nhI/qlNAId7XARMUIRmCxWFUJsxOn6CKhR4jFfWAi3g3WE9sNmLEtABZl+csqxePQh
         nQow==
X-Gm-Message-State: ALoCoQlIUiHVnzD4OvQupqwmViTJsnBS85f9XLXJVk9Oo7U74xCZHfK0s2HJPtoYDNiFBzm3hYJ9
MIME-Version: 1.0
X-Received: by 10.60.178.41 with SMTP id cv9mr229747oec.53.1399262899624; Sun,
 04 May 2014 21:08:19 -0700 (PDT)
Received: by 10.182.184.40 with HTTP; Sun, 4 May 2014 21:08:19 -0700 (PDT)
In-Reply-To: <CAPh_B=asgxMXEBnXtzX1xwz7fOBVBwjVu7=3coY_L5WtatgD3g@mail.gmail.com>
References: <CAJ3iqPRnJp-ezZAEJJDd4OSeF4f0qmEbaQAAJo2Y7YSrRXZc5A@mail.gmail.com>
	<CAPh_B=abv5w67MOsBESXku_OFNY_TsZFmAYnu-EGM+7GWEe2VQ@mail.gmail.com>
	<CAJ3iqPQqdZVztd+nnaOkueCNkkJQS2RfkeRP_iSvCZomYcyjOQ@mail.gmail.com>
	<CAPh_B=asgxMXEBnXtzX1xwz7fOBVBwjVu7=3coY_L5WtatgD3g@mail.gmail.com>
Date: Sun, 4 May 2014 21:08:19 -0700
Message-ID: <CAJ3iqPRgfdMnbcVKn1yX73CjmBh4_6QsH6orY=EZATPe2_gqmA@mail.gmail.com>
Subject: Re: bug using kryo as closure serializer
From: Soren Macbeth <soren@yieldbot.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e011835ea14724104f89f48bc
X-Virus-Checked: Checked by ClamAV on apache.org

--089e011835ea14724104f89f48bc
Content-Type: text/plain; charset=UTF-8

fwiw, it seems like it wouldn't be very difficult to integrate chill-scala,
since you're already chill-java and probably get kryo serialization of
closures and all sorts of other scala stuff for free. All that would be
needed would be to include the dependency and then update KryoSerializer to
register the stuff in chill-scala.

In that case, you could probably safely make kryo the default serializer,
which I think would be desirable in general.


On Sun, May 4, 2014 at 8:48 PM, Reynold Xin <rxin@databricks.com> wrote:

> Good idea. I submitted a pull request for the doc update here:
> https://github.com/apache/spark/pull/642
>
>
> On Sun, May 4, 2014 at 3:54 PM, Soren Macbeth <soren@yieldbot.com> wrote:
>
> > Thanks for the reply!
> >
> > Ok, if that's the case, I'd recommend a note to that affect in the docs
> at
> > least.
> >
> > Just to give some more context here, I'm working on a Clojure DSL for
> Spark
> > called Flambo, which I plan to open source shortly. If I could I'd like
> to
> > focus on the initial bug that I hit.
> >
> > Exception in thread "main" org.apache.spark.SparkException: Job aborted:
> > Exception while deserializing and fetching task:
> > com.esotericsoftware.kryo.KryoException:
> > java.lang.IllegalArgumentException: Can not set final
> > scala.collection.convert.Wrappers field
> > scala.collection.convert.Wrappers$SeqWrapper.$outer to
> > clojure.lang.PersistentVector
> > Serialization trace:
> > $outer (scala.collection.convert.Wrappers$SeqWrapper)
> >         at
> >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1020)
> >         at
> >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1018)
> >         at
> >
> >
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> >         at
> > scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> >         at org.apache.spark.scheduler.DAGScheduler.org
> > $apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1018)
> >         at
> >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
> >         at
> >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
> >         at scala.Option.foreach(Option.scala:236)
> >         at
> >
> >
> org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:604)
> >         at
> >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:190)
> >         at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
> >         at akka.actor.ActorCell.invoke(ActorCell.scala:456)
> >         at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
> >         at akka.dispatch.Mailbox.run(Mailbox.scala:219)
> >         at
> >
> >
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
> >         at
> > scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
> >         at
> >
> >
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
> >         at
> > scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
> >         at
> >
> >
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
> >
> > This happens immediately after all the tasks of a reduce stage complete
> > successfully. Here is the function throwing the exception:
> >
> >
> >
> https://github.com/apache/spark/blob/4bc07eebbf5e2ea0c0b6f1642049515025d88d07/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala#L43
> >
> > This is where I get lost. From googling around, it seems that scala is
> > trying to wrap the result of my task, which contain
> > clojure.lang.PersistentVector objects in a scala collection, but I don't
> > know why it's doing that. I have a registered kryo serializer for
> > clojure.lang.PersistentVector.
> >
> > based on this line is looks like it's trying to use the closure
> serializer,
> > yet the expection thrown is from com.esotericsoftware.kryo.KryoException:
> >
> >
> >
> https://github.com/apache/spark/blob/4bc07eebbf5e2ea0c0b6f1642049515025d88d07/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala#L39
> >
> > Would storing my RDD as MEMORY_ONLY_SER prevent the closure serializer
> from
> > trying to deal with my clojure.lang.PeristentVector class?
> >
> > Where do I go from here?
> >
> >
> > On Sun, May 4, 2014 at 12:50 PM, Reynold Xin <rxin@databricks.com>
> wrote:
> >
> > > I added the config option to use the non-default serializer. However,
> at
> > > the time, Kryo fails serializing pretty much any closures so that
> option
> > > was never really used / recommended.
> > >
> > > Since then the Scala ecosystem has developed, and some other projects
> are
> > > starting to use Kryo to serialize more Scala data structures, so I
> > wouldn't
> > > be surprised if there is a way to work around this now. However, I
> don't
> > > have enough time to look into it at this point. If you do, please do
> post
> > > your findings. Thanks.
> > >
> > >
> > >
> > > On Sun, May 4, 2014 at 10:35 AM, Soren Macbeth <soren@yieldbot.com>
> > wrote:
> > >
> > > > apologies for the cross-list posts, but I've gotten zero response in
> > the
> > > > user list and I guess this list is probably more appropriate.
> > > >
> > > > According to the documentation, using the KryoSerializer for closures
> > is
> > > > supported. However, when I try to set `spark.closure.serializer` to
> > > > `org.apache.spark.serializer.KryoSerializer` thing fail pretty
> > miserably.
> > > >
> > > > The first thing that happens it that is throws exceptions over and
> over
> > > > that it cannot locate my registrator class, which is located in my
> > > assembly
> > > > jar like so:
> > > >
> > > > 14/05/04 12:03:20 ERROR serializer.KryoSerializer: Failed to run
> > > > spark.kryo.registrator
> > > > java.lang.ClassNotFoundException: pickles.kryo.PicklesRegistrator
> > > > at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
> > > > at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
> > > > at java.security.AccessController.doPrivileged(Native Method)
> > > > at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
> > > > at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
> > > > at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
> > > > at java.lang.Class.forName0(Native Method)
> > > > at java.lang.Class.forName(Class.java:270)
> > > > at
> > > >
> > > >
> > >
> >
> org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$2.apply(KryoSerializer.scala:63)
> > > > at
> > > >
> > > >
> > >
> >
> org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$2.apply(KryoSerializer.scala:61)
> > > > at scala.Option.foreach(Option.scala:236)
> > > > at
> > > >
> > >
> >
> org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:61)
> > > > at
> > > >
> > > >
> > >
> >
> org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:116)
> > > > at
> > > >
> > > >
> > >
> >
> org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:79)
> > > > at
> > > >
> > > >
> > >
> >
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:180)
> > > > at
> > > >
> > > >
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
> > > > at
> > > >
> > > >
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
> > > > at java.security.AccessController.doPrivileged(Native Method)
> > > > at javax.security.auth.Subject.doAs(Subject.java:415)
> > > > at
> > > >
> > > >
> > >
> >
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)
> > > > at
> > > >
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
> > > > at
> > org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
> > > > at
> > > >
> > > >
> > >
> >
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> > > > at
> > > >
> > > >
> > >
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> > > > at java.lang.Thread.run(Thread.java:724)
> > > >
> > > > Now, I would expect it not to be able to find this class since it
> > hasn't
> > > > yet fetched my assembly jar to the executors. Once it does fetch my
> > jar,
> > > > those expections stop. Next, all the executor task die with the
> > following
> > > > exception:
> > > >
> > > > java.nio.ReadOnlyBufferException
> > > > at java.nio.ByteBuffer.array(ByteBuffer.java:961)
> > > > at
> > > >
> > > >
> > >
> >
> org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:136)
> > > > at
> > > >
> > > >
> > >
> >
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:193)
> > > > at
> > > >
> > > >
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
> > > > at
> > > >
> > > >
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
> > > > at java.security.AccessController.doPrivileged(Native Method)
> > > > at javax.security.auth.Subject.doAs(Subject.java:415)
> > > > at
> > > >
> > > >
> > >
> >
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)
> > > > at
> > > >
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
> > > > at
> > org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
> > > > at
> > > >
> > > >
> > >
> >
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> > > > at
> > > >
> > > >
> > >
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> > > > at java.lang.Thread.run(Thread.java:724)
> > > >
> > > > AFAIK, I'm not doing anything out of the ordinary, just turning on
> kryo
> > > and
> > > > using the registrator mechanism to register a couple custom
> > serializers.
> > > >
> > > > The reason I tried turning on kryo for closure in the first place is
> > > > because of a different bug that I was hitting during fetching and
> > > > deserializing of tasks from my executors, which I detailed here:
> > > >
> > > >
> > > >
> > >
> >
> http://apache-spark-user-list.1001560.n3.nabble.com/Crazy-Kryo-Exception-td5257.html
> > > >
> > > > Here's hoping some on this list can help me track down what's
> happening
> > > as
> > > > I didn't get a single reply on the user list.
> > > >
> > >
> >
>

--089e011835ea14724104f89f48bc--

From dev-return-7507-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May  5 05:05:15 2014
Return-Path: <dev-return-7507-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3EEC311D85
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 May 2014 05:05:15 +0000 (UTC)
Received: (qmail 42722 invoked by uid 500); 5 May 2014 05:05:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42422 invoked by uid 500); 5 May 2014 05:05:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42410 invoked by uid 99); 5 May 2014 05:05:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 05:05:11 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.172] (HELO mail-qc0-f172.google.com) (209.85.216.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 05:05:07 +0000
Received: by mail-qc0-f172.google.com with SMTP id l6so2551565qcy.3
        for <dev@spark.apache.org>; Sun, 04 May 2014 22:04:44 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=mwR9IzPVZbGeb+q8TEZIHXsqc0fAQOTX40eP4AyrLGM=;
        b=XM0gTSGUgYTbgCXRBf5YnoKj2Mxe396Wx+8wL29PzrltKkw1EtMVYTki+p0ljdavb7
         +czKegm4aBZ7WzhoRDXoJ8IYrqvBVeiRLZFcxgKdQJQ59yFJlpZlUMqbppl5Zwkontvt
         xiWacx+QH19dznp0bt5PS/dYU0DufD/uepve1ksXsv7d5UDx7ihMUm84cbZNqSVdDsgN
         nw0hIRjkPO/G9TOpa2+1XTdyusXen4X4Op7IjBXfY4B2JshBagMypGJwmIzs27gR7bZx
         m2XvuqunJLGx24LoqqVWpUiKvaXZWgeGp1b+M7fR/Y2cHt6bZq2MC+UNuKoWkDMK8zYB
         kJZw==
X-Gm-Message-State: ALoCoQkCjAm42Hy6M167soI4yIJB4S8Ws9T9OhqBkgVi9gRxu2vyQ4dcedtu2o+yZteLm9KyR6Aw
MIME-Version: 1.0
X-Received: by 10.140.97.55 with SMTP id l52mr38954281qge.19.1399266284301;
 Sun, 04 May 2014 22:04:44 -0700 (PDT)
Received: by 10.96.126.1 with HTTP; Sun, 4 May 2014 22:04:44 -0700 (PDT)
In-Reply-To: <CAJ3iqPRgfdMnbcVKn1yX73CjmBh4_6QsH6orY=EZATPe2_gqmA@mail.gmail.com>
References: <CAJ3iqPRnJp-ezZAEJJDd4OSeF4f0qmEbaQAAJo2Y7YSrRXZc5A@mail.gmail.com>
	<CAPh_B=abv5w67MOsBESXku_OFNY_TsZFmAYnu-EGM+7GWEe2VQ@mail.gmail.com>
	<CAJ3iqPQqdZVztd+nnaOkueCNkkJQS2RfkeRP_iSvCZomYcyjOQ@mail.gmail.com>
	<CAPh_B=asgxMXEBnXtzX1xwz7fOBVBwjVu7=3coY_L5WtatgD3g@mail.gmail.com>
	<CAJ3iqPRgfdMnbcVKn1yX73CjmBh4_6QsH6orY=EZATPe2_gqmA@mail.gmail.com>
Date: Sun, 4 May 2014 22:04:44 -0700
Message-ID: <CAPh_B=Y=taT3jo6ZUdvKkaixsWZGmez8k8zLCv_K+jWoXFajLA@mail.gmail.com>
Subject: Re: bug using kryo as closure serializer
From: Reynold Xin <rxin@databricks.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113a98ced29e8104f8a01188
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a98ced29e8104f8a01188
Content-Type: text/plain; charset=UTF-8

Thanks. Do you mind playing with chill-scala a little bit and see if it
actually works well for closures? One way to try is to hard code the
serializer to use Kryo with chill-scala, and then run through all the unit
tests.

If it works well, we can incorporate that in the next release (probably not
1.0, but after that).


On Sun, May 4, 2014 at 9:08 PM, Soren Macbeth <soren@yieldbot.com> wrote:

> fwiw, it seems like it wouldn't be very difficult to integrate chill-scala,
> since you're already chill-java and probably get kryo serialization of
> closures and all sorts of other scala stuff for free. All that would be
> needed would be to include the dependency and then update KryoSerializer to
> register the stuff in chill-scala.
>
> In that case, you could probably safely make kryo the default serializer,
> which I think would be desirable in general.
>
>
> On Sun, May 4, 2014 at 8:48 PM, Reynold Xin <rxin@databricks.com> wrote:
>
> > Good idea. I submitted a pull request for the doc update here:
> > https://github.com/apache/spark/pull/642
> >
> >
> > On Sun, May 4, 2014 at 3:54 PM, Soren Macbeth <soren@yieldbot.com>
> wrote:
> >
> > > Thanks for the reply!
> > >
> > > Ok, if that's the case, I'd recommend a note to that affect in the docs
> > at
> > > least.
> > >
> > > Just to give some more context here, I'm working on a Clojure DSL for
> > Spark
> > > called Flambo, which I plan to open source shortly. If I could I'd like
> > to
> > > focus on the initial bug that I hit.
> > >
> > > Exception in thread "main" org.apache.spark.SparkException: Job
> aborted:
> > > Exception while deserializing and fetching task:
> > > com.esotericsoftware.kryo.KryoException:
> > > java.lang.IllegalArgumentException: Can not set final
> > > scala.collection.convert.Wrappers field
> > > scala.collection.convert.Wrappers$SeqWrapper.$outer to
> > > clojure.lang.PersistentVector
> > > Serialization trace:
> > > $outer (scala.collection.convert.Wrappers$SeqWrapper)
> > >         at
> > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1020)
> > >         at
> > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1018)
> > >         at
> > >
> > >
> >
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> > >         at
> > > scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> > >         at org.apache.spark.scheduler.DAGScheduler.org
> > >
> $apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1018)
> > >         at
> > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
> > >         at
> > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
> > >         at scala.Option.foreach(Option.scala:236)
> > >         at
> > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:604)
> > >         at
> > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:190)
> > >         at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
> > >         at akka.actor.ActorCell.invoke(ActorCell.scala:456)
> > >         at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
> > >         at akka.dispatch.Mailbox.run(Mailbox.scala:219)
> > >         at
> > >
> > >
> >
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
> > >         at
> > > scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
> > >         at
> > >
> > >
> >
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
> > >         at
> > >
> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
> > >         at
> > >
> > >
> >
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
> > >
> > > This happens immediately after all the tasks of a reduce stage complete
> > > successfully. Here is the function throwing the exception:
> > >
> > >
> > >
> >
> https://github.com/apache/spark/blob/4bc07eebbf5e2ea0c0b6f1642049515025d88d07/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala#L43
> > >
> > > This is where I get lost. From googling around, it seems that scala is
> > > trying to wrap the result of my task, which contain
> > > clojure.lang.PersistentVector objects in a scala collection, but I
> don't
> > > know why it's doing that. I have a registered kryo serializer for
> > > clojure.lang.PersistentVector.
> > >
> > > based on this line is looks like it's trying to use the closure
> > serializer,
> > > yet the expection thrown is from
> com.esotericsoftware.kryo.KryoException:
> > >
> > >
> > >
> >
> https://github.com/apache/spark/blob/4bc07eebbf5e2ea0c0b6f1642049515025d88d07/core/src/main/scala/org/apache/spark/scheduler/TaskResultGetter.scala#L39
> > >
> > > Would storing my RDD as MEMORY_ONLY_SER prevent the closure serializer
> > from
> > > trying to deal with my clojure.lang.PeristentVector class?
> > >
> > > Where do I go from here?
> > >
> > >
> > > On Sun, May 4, 2014 at 12:50 PM, Reynold Xin <rxin@databricks.com>
> > wrote:
> > >
> > > > I added the config option to use the non-default serializer. However,
> > at
> > > > the time, Kryo fails serializing pretty much any closures so that
> > option
> > > > was never really used / recommended.
> > > >
> > > > Since then the Scala ecosystem has developed, and some other projects
> > are
> > > > starting to use Kryo to serialize more Scala data structures, so I
> > > wouldn't
> > > > be surprised if there is a way to work around this now. However, I
> > don't
> > > > have enough time to look into it at this point. If you do, please do
> > post
> > > > your findings. Thanks.
> > > >
> > > >
> > > >
> > > > On Sun, May 4, 2014 at 10:35 AM, Soren Macbeth <soren@yieldbot.com>
> > > wrote:
> > > >
> > > > > apologies for the cross-list posts, but I've gotten zero response
> in
> > > the
> > > > > user list and I guess this list is probably more appropriate.
> > > > >
> > > > > According to the documentation, using the KryoSerializer for
> closures
> > > is
> > > > > supported. However, when I try to set `spark.closure.serializer` to
> > > > > `org.apache.spark.serializer.KryoSerializer` thing fail pretty
> > > miserably.
> > > > >
> > > > > The first thing that happens it that is throws exceptions over and
> > over
> > > > > that it cannot locate my registrator class, which is located in my
> > > > assembly
> > > > > jar like so:
> > > > >
> > > > > 14/05/04 12:03:20 ERROR serializer.KryoSerializer: Failed to run
> > > > > spark.kryo.registrator
> > > > > java.lang.ClassNotFoundException: pickles.kryo.PicklesRegistrator
> > > > > at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
> > > > > at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
> > > > > at java.security.AccessController.doPrivileged(Native Method)
> > > > > at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
> > > > > at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
> > > > > at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
> > > > > at java.lang.Class.forName0(Native Method)
> > > > > at java.lang.Class.forName(Class.java:270)
> > > > > at
> > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$2.apply(KryoSerializer.scala:63)
> > > > > at
> > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$2.apply(KryoSerializer.scala:61)
> > > > > at scala.Option.foreach(Option.scala:236)
> > > > > at
> > > > >
> > > >
> > >
> >
> org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:61)
> > > > > at
> > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.serializer.KryoSerializerInstance.<init>(KryoSerializer.scala:116)
> > > > > at
> > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:79)
> > > > > at
> > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:180)
> > > > > at
> > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
> > > > > at
> > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
> > > > > at java.security.AccessController.doPrivileged(Native Method)
> > > > > at javax.security.auth.Subject.doAs(Subject.java:415)
> > > > > at
> > > > >
> > > > >
> > > >
> > >
> >
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)
> > > > > at
> > > > >
> > > >
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
> > > > > at
> > > org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
> > > > > at
> > > > >
> > > > >
> > > >
> > >
> >
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> > > > > at
> > > > >
> > > > >
> > > >
> > >
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> > > > > at java.lang.Thread.run(Thread.java:724)
> > > > >
> > > > > Now, I would expect it not to be able to find this class since it
> > > hasn't
> > > > > yet fetched my assembly jar to the executors. Once it does fetch my
> > > jar,
> > > > > those expections stop. Next, all the executor task die with the
> > > following
> > > > > exception:
> > > > >
> > > > > java.nio.ReadOnlyBufferException
> > > > > at java.nio.ByteBuffer.array(ByteBuffer.java:961)
> > > > > at
> > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:136)
> > > > > at
> > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:193)
> > > > > at
> > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
> > > > > at
> > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
> > > > > at java.security.AccessController.doPrivileged(Native Method)
> > > > > at javax.security.auth.Subject.doAs(Subject.java:415)
> > > > > at
> > > > >
> > > > >
> > > >
> > >
> >
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1438)
> > > > > at
> > > > >
> > > >
> > >
> >
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
> > > > > at
> > > org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
> > > > > at
> > > > >
> > > > >
> > > >
> > >
> >
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> > > > > at
> > > > >
> > > > >
> > > >
> > >
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> > > > > at java.lang.Thread.run(Thread.java:724)
> > > > >
> > > > > AFAIK, I'm not doing anything out of the ordinary, just turning on
> > kryo
> > > > and
> > > > > using the registrator mechanism to register a couple custom
> > > serializers.
> > > > >
> > > > > The reason I tried turning on kryo for closure in the first place
> is
> > > > > because of a different bug that I was hitting during fetching and
> > > > > deserializing of tasks from my executors, which I detailed here:
> > > > >
> > > > >
> > > > >
> > > >
> > >
> >
> http://apache-spark-user-list.1001560.n3.nabble.com/Crazy-Kryo-Exception-td5257.html
> > > > >
> > > > > Here's hoping some on this list can help me track down what's
> > happening
> > > > as
> > > > > I didn't get a single reply on the user list.
> > > > >
> > > >
> > >
> >
>

--001a113a98ced29e8104f8a01188--

From dev-return-7508-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May  5 05:22:32 2014
Return-Path: <dev-return-7508-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C51D611DD5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 May 2014 05:22:32 +0000 (UTC)
Received: (qmail 73609 invoked by uid 500); 5 May 2014 05:22:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73572 invoked by uid 500); 5 May 2014 05:22:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73564 invoked by uid 99); 5 May 2014 05:22:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 05:22:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.214.182] (HELO mail-ob0-f182.google.com) (209.85.214.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 05:22:26 +0000
Received: by mail-ob0-f182.google.com with SMTP id wn1so3138012obc.13
        for <dev@spark.apache.org>; Sun, 04 May 2014 22:22:03 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=yZisDlTbecvigGVdj2Lx0y8an8MVxiAPq/8U1oGOAHM=;
        b=QdNlRb7VLs+DI1k0fIQgvfHrtUzMuRMU/s8pr3hvBihZouflyGqwvF7nBiXvvZpwrN
         iiHXNbgcK0GEcS0G5Xjm4hI2reZut2jblxMKybN8nsFmNUFSA0L6I8co7qzKjNSvrPUy
         n6XBR6ISgczasxd1pDezIm4oTqyLSYC7InqECNjrPikoo9A3tEbG4NwBzpKUAItmK2L6
         7hVzo//eEDQa0eSzfszPeH35cjeC9HbedqbTrTIrLnCSPXISYhWRarHGDv3j9Aw5nEXc
         P9y/7WJn4IADLHz6xggQWP532+5S5dPh4zYEnEdpwz2yNwGCTL1N5mFWNGgYIC8lTLk3
         eY3A==
X-Gm-Message-State: ALoCoQlbf5y6Y4yJ7g1cNn4u4Rf3ZNowi3BS3T/zRQFWuTTvlzWetEnnO7PxMQ2WaY5lpVIw/etL
MIME-Version: 1.0
X-Received: by 10.60.133.107 with SMTP id pb11mr1329209oeb.48.1399267322916;
 Sun, 04 May 2014 22:22:02 -0700 (PDT)
Received: by 10.182.184.40 with HTTP; Sun, 4 May 2014 22:22:02 -0700 (PDT)
In-Reply-To: <CAPh_B=Y=taT3jo6ZUdvKkaixsWZGmez8k8zLCv_K+jWoXFajLA@mail.gmail.com>
References: <CAJ3iqPRnJp-ezZAEJJDd4OSeF4f0qmEbaQAAJo2Y7YSrRXZc5A@mail.gmail.com>
	<CAPh_B=abv5w67MOsBESXku_OFNY_TsZFmAYnu-EGM+7GWEe2VQ@mail.gmail.com>
	<CAJ3iqPQqdZVztd+nnaOkueCNkkJQS2RfkeRP_iSvCZomYcyjOQ@mail.gmail.com>
	<CAPh_B=asgxMXEBnXtzX1xwz7fOBVBwjVu7=3coY_L5WtatgD3g@mail.gmail.com>
	<CAJ3iqPRgfdMnbcVKn1yX73CjmBh4_6QsH6orY=EZATPe2_gqmA@mail.gmail.com>
	<CAPh_B=Y=taT3jo6ZUdvKkaixsWZGmez8k8zLCv_K+jWoXFajLA@mail.gmail.com>
Date: Sun, 4 May 2014 22:22:02 -0700
Message-ID: <CAJ3iqPTnTNz_t8e_6u9jHpFne0sOY-kLDTRvoWTPZ7=5bHB4xQ@mail.gmail.com>
Subject: Re: bug using kryo as closure serializer
From: Soren Macbeth <soren@yieldbot.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b4727f2ba9edb04f8a04ff3
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b4727f2ba9edb04f8a04ff3
Content-Type: text/plain; charset=UTF-8

that would violate my personal oath of never writing a single line of
scala, but I might be able to do that if I can get past the issue this
issue I'm struggling with in this thread.

On Sunday, May 4, 2014, Reynold Xin <rxin@databricks.com> wrote:

> Thanks. Do you mind playing with chill-scala a little bit and see if it
> actually works well for closures? One way to try is to hard code the
> serializer to use Kryo with chill-scala, and then run through all the unit
> tests.
>
> If it works well, we can incorporate that in the next release (probably not
> 1.0, but after that).
>
>
> On Sun, May 4, 2014 at 9:08 PM, Soren Macbeth <soren@yieldbot.com> wrote:
>
> > fwiw, it seems like it wouldn't be very difficult to integrate
> chill-scala,
> > since you're already chill-java and probably get kryo serialization of
> > closures and all sorts of other scala stuff for free. All that would be
> > needed would be to include the dependency and then update KryoSerializer
> to
> > register the stuff in chill-scala.
> >
> > In that case, you could probably safely make kryo the default serializer,
> > which I think would be desirable in general.
> >
> >
> > On Sun, May 4, 2014 at 8:48 PM, Reynold Xin <rxin@databricks.com> wrote:
> >
> > > Good idea. I submitted a pull request for the doc update here:
> > > https://github.com/apache/spark/pull/642
> > >
> > >
> > > On Sun, May 4, 2014 at 3:54 PM, Soren Macbeth <soren@yieldbot.com>
> > wrote:
> > >
> > > > Thanks for the reply!
> > > >
> > > > Ok, if that's the case, I'd recommend a note to that affect in the
> docs
> > > at
> > > > least.
> > > >
> > > > Just to give some more context here, I'm working on a Clojure DSL for
> > > Spark
> > > > called Flambo, which I plan to open source shortly. If I could I'd
> like
> > > to
> > > > focus on the initial bug that I hit.
> > > >
> > > > Exception in thread "main" org.apache.spark.SparkException: Job
> > aborted:
> > > > Exception while deserializing and fetching task:
> > > > com.esotericsoftware.kryo.KryoException:
> > > > java.lang.IllegalArgumentException: Can not set final
> > > > scala.collection.convert.Wrappers field
> > > > scala.collection.convert.Wrappers$SeqWrapper.$outer to
> > > > clojure.lang.PersistentVector
> > > > Serialization trace:
> > > > $outer (scala.collection.convert.Wrappers$SeqWrapper)
> > > >         at
> > > >
> > > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1020)
> > > >         at
> > > >
> > > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1018)
> > > >         at
> > > >
> > > >
> > >
> >
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> > > >         at
> > > > scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> > > >         at org.apache.spark.scheduler.DAGScheduler.org
> > > >
> > $apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1018)
> > > >         at
> > > >
> > > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
> > > >         at
> > > >
> > > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
> > > >         at scala.Option.foreach(Option.scala:236)
> > > >         at
> > > >
> > > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:604)
> > > >         at
> > > >
> > > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:190)
> > > >         at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
> > > >         at akka.actor.ActorCell.invoke(ActorCell.scala:456)
> > > >

--047d7b4727f2ba9edb04f8a04ff3--

From dev-return-7509-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May  5 05:24:51 2014
Return-Path: <dev-return-7509-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4162C11DDF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 May 2014 05:24:51 +0000 (UTC)
Received: (qmail 75315 invoked by uid 500); 5 May 2014 05:24:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75269 invoked by uid 500); 5 May 2014 05:24:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75260 invoked by uid 99); 5 May 2014 05:24:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 05:24:48 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.48] (HELO mail-qa0-f48.google.com) (209.85.216.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 05:24:45 +0000
Received: by mail-qa0-f48.google.com with SMTP id i13so1954033qae.35
        for <dev@spark.apache.org>; Sun, 04 May 2014 22:24:22 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=gZ4S2OavaBLyQrksXgBZUaRVNJTFazXGigH6W1TmG5s=;
        b=JkDpu1gkpT1H/0ZlJ91fdN+7znn24IGFUQSqCETNdoEioWt06wXfr0hd46qdBigvTz
         31a/KEjwxLS3EmPBkSYGr5PKPBQAuPEMSoikLUQTpVjluKtDvAsst1PZ49pwGcnlirD9
         vyuIBNtGc3T4HaYnBbL7qCv3IWx1bA89lw8K9mhorDIxzSkx2BFEVJu9MUtMmJNjU6cQ
         FqA09xQxn4UEyhxkokSnQ587EuP2QLOeVbqcJsKOct3pZa3Wmu5TlBvJdwcLT/kpJkhY
         mZJ78kpl7qKA/gWjgo7BaE5FRlL5tkBf0J50GhVpJkBqMbKgx3NY6bErHdOWnD3Z+77q
         491g==
X-Gm-Message-State: ALoCoQllv3yM+Vj/jyd2NZP2OTpM2ZHwxx3h7b1xbjS7Yx35yDI19IeDXqj0A5IdHSDf3O7KPg6f
MIME-Version: 1.0
X-Received: by 10.224.49.67 with SMTP id u3mr42007378qaf.63.1399267462337;
 Sun, 04 May 2014 22:24:22 -0700 (PDT)
Received: by 10.96.126.1 with HTTP; Sun, 4 May 2014 22:24:22 -0700 (PDT)
In-Reply-To: <CAJ3iqPTnTNz_t8e_6u9jHpFne0sOY-kLDTRvoWTPZ7=5bHB4xQ@mail.gmail.com>
References: <CAJ3iqPRnJp-ezZAEJJDd4OSeF4f0qmEbaQAAJo2Y7YSrRXZc5A@mail.gmail.com>
	<CAPh_B=abv5w67MOsBESXku_OFNY_TsZFmAYnu-EGM+7GWEe2VQ@mail.gmail.com>
	<CAJ3iqPQqdZVztd+nnaOkueCNkkJQS2RfkeRP_iSvCZomYcyjOQ@mail.gmail.com>
	<CAPh_B=asgxMXEBnXtzX1xwz7fOBVBwjVu7=3coY_L5WtatgD3g@mail.gmail.com>
	<CAJ3iqPRgfdMnbcVKn1yX73CjmBh4_6QsH6orY=EZATPe2_gqmA@mail.gmail.com>
	<CAPh_B=Y=taT3jo6ZUdvKkaixsWZGmez8k8zLCv_K+jWoXFajLA@mail.gmail.com>
	<CAJ3iqPTnTNz_t8e_6u9jHpFne0sOY-kLDTRvoWTPZ7=5bHB4xQ@mail.gmail.com>
Date: Sun, 4 May 2014 22:24:22 -0700
Message-ID: <CAPh_B=buc5ZuuK1_oYSuSdg8e=TFcHjkWuNihpCmzkY+jW112g@mail.gmail.com>
Subject: Re: bug using kryo as closure serializer
From: Reynold Xin <rxin@databricks.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2ef8009f5fa04f8a0581d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ef8009f5fa04f8a0581d
Content-Type: text/plain; charset=UTF-8

Technically you only need to change the build file, and change part of a
line in SparkEnv so you don't have to break your oath :)



On Sun, May 4, 2014 at 10:22 PM, Soren Macbeth <soren@yieldbot.com> wrote:

> that would violate my personal oath of never writing a single line of
> scala, but I might be able to do that if I can get past the issue this
> issue I'm struggling with in this thread.
>
> On Sunday, May 4, 2014, Reynold Xin <rxin@databricks.com> wrote:
>
> > Thanks. Do you mind playing with chill-scala a little bit and see if it
> > actually works well for closures? One way to try is to hard code the
> > serializer to use Kryo with chill-scala, and then run through all the
> unit
> > tests.
> >
> > If it works well, we can incorporate that in the next release (probably
> not
> > 1.0, but after that).
> >
> >
> > On Sun, May 4, 2014 at 9:08 PM, Soren Macbeth <soren@yieldbot.com>
> wrote:
> >
> > > fwiw, it seems like it wouldn't be very difficult to integrate
> > chill-scala,
> > > since you're already chill-java and probably get kryo serialization of
> > > closures and all sorts of other scala stuff for free. All that would be
> > > needed would be to include the dependency and then update
> KryoSerializer
> > to
> > > register the stuff in chill-scala.
> > >
> > > In that case, you could probably safely make kryo the default
> serializer,
> > > which I think would be desirable in general.
> > >
> > >
> > > On Sun, May 4, 2014 at 8:48 PM, Reynold Xin <rxin@databricks.com>
> wrote:
> > >
> > > > Good idea. I submitted a pull request for the doc update here:
> > > > https://github.com/apache/spark/pull/642
> > > >
> > > >
> > > > On Sun, May 4, 2014 at 3:54 PM, Soren Macbeth <soren@yieldbot.com>
> > > wrote:
> > > >
> > > > > Thanks for the reply!
> > > > >
> > > > > Ok, if that's the case, I'd recommend a note to that affect in the
> > docs
> > > > at
> > > > > least.
> > > > >
> > > > > Just to give some more context here, I'm working on a Clojure DSL
> for
> > > > Spark
> > > > > called Flambo, which I plan to open source shortly. If I could I'd
> > like
> > > > to
> > > > > focus on the initial bug that I hit.
> > > > >
> > > > > Exception in thread "main" org.apache.spark.SparkException: Job
> > > aborted:
> > > > > Exception while deserializing and fetching task:
> > > > > com.esotericsoftware.kryo.KryoException:
> > > > > java.lang.IllegalArgumentException: Can not set final
> > > > > scala.collection.convert.Wrappers field
> > > > > scala.collection.convert.Wrappers$SeqWrapper.$outer to
> > > > > clojure.lang.PersistentVector
> > > > > Serialization trace:
> > > > > $outer (scala.collection.convert.Wrappers$SeqWrapper)
> > > > >         at
> > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1020)
> > > > >         at
> > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1018)
> > > > >         at
> > > > >
> > > > >
> > > >
> > >
> >
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> > > > >         at
> > > > > scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> > > > >         at org.apache.spark.scheduler.DAGScheduler.org
> > > > >
> > >
> $apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1018)
> > > > >         at
> > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
> > > > >         at
> > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
> > > > >         at scala.Option.foreach(Option.scala:236)
> > > > >         at
> > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:604)
> > > > >         at
> > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:190)
> > > > >         at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
> > > > >         at akka.actor.ActorCell.invoke(ActorCell.scala:456)
> > > > >
>

--001a11c2ef8009f5fa04f8a0581d--

From dev-return-7510-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May  5 08:42:01 2014
Return-Path: <dev-return-7510-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4BF7D11162
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 May 2014 08:42:01 +0000 (UTC)
Received: (qmail 20848 invoked by uid 500); 5 May 2014 08:41:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20262 invoked by uid 500); 5 May 2014 08:41:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20253 invoked by uid 99); 5 May 2014 08:41:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 08:41:55 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of prodigyaj@gmail.com does not designate 216.139.250.139 as permitted sender)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 08:41:52 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <prodigyaj@gmail.com>)
	id 1WhESN-0002wa-Nd
	for dev@spark.incubator.apache.org; Mon, 05 May 2014 01:41:03 -0700
Date: Mon, 5 May 2014 01:40:48 -0700 (PDT)
From: Ajay Nair <prodigyaj@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1399279248689-6487.post@n3.nabble.com>
Subject: Apache spark on 27gb wikipedia data
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

I am using 1 master and 3 slave workers for processing 27gb of Wikipedia
data that is tab separated and every line contains wikipedia page
information. The tab separated data has title of the page and the page
contents. I am using the regular expression to extract links as mentioned in
the site below:
http://ampcamp.berkeley.edu/big-data-mini-course/graph-analytics-with-graphx.html#running-pagerank-on-wikipedia

Although it runs fne for around 300Mb data set, it runs in to issues when I
try to execute the same code using the 27gb data from hdfs.
The error thrown is given below:
14/05/05 07:15:22 WARN scheduler.TaskSetManager: Loss was due to
java.lang.OutOfMemoryError
java.lang.OutOfMemoryError: GC overhead limit exceeded
	at java.util.regex.Matcher.<init>(Matcher.java:224)

Is there any way to over come this issue?

My cluster is a ec2 m3.large machine.

Thanks
Ajay



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-spark-on-27gb-wikipedia-data-tp6487.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7511-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May  5 08:52:35 2014
Return-Path: <dev-return-7511-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7BC79111B0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 May 2014 08:52:35 +0000 (UTC)
Received: (qmail 40782 invoked by uid 500); 5 May 2014 08:52:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40722 invoked by uid 500); 5 May 2014 08:52:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40545 invoked by uid 99); 5 May 2014 08:52:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 08:52:30 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of scrapcodes@gmail.com designates 209.85.220.172 as permitted sender)
Received: from [209.85.220.172] (HELO mail-vc0-f172.google.com) (209.85.220.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 08:52:27 +0000
Received: by mail-vc0-f172.google.com with SMTP id id10so8367835vcb.31
        for <dev@spark.apache.org>; Mon, 05 May 2014 01:52:04 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=4qfcm3oM/SDwbVBvoY7m2hM8bHZOrxomXFO8Tzj1xLI=;
        b=yn66d+3CHPz+50WmX9OJdJOzM0g7jm9bTklwKBvGPHHA97qK39MXE1iIrmHGJp80yR
         OVwRyPkrzeYf8RMq7dS1JqaxVSY2lpAM14NYa1ac1WeSF3WrrapHtqMSfhPX6WtvGVEv
         FFaq1/rVb80VS3oBSHVxNzC+0HaUnUfYN2Oz3jeGi7REtEWw6ttesqRxVsCciHgKZH+c
         8ofNvP2vn0aCefQxOTwXr+4ATERfO4STMf5TLRNzvkj+N6zoVK6lb/wU/YDzG+vr2mGR
         2zpY6Tnagx+tGnjv+yvl8VnZRgwqSXGiP7YfU4S5DwD1wOEiW2Z9/+c4OotTcrVyrcoU
         ZFIg==
X-Received: by 10.58.77.238 with SMTP id v14mr972180vew.27.1399279924334; Mon,
 05 May 2014 01:52:04 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.19.164 with HTTP; Mon, 5 May 2014 01:51:44 -0700 (PDT)
In-Reply-To: <1399279248689-6487.post@n3.nabble.com>
References: <1399279248689-6487.post@n3.nabble.com>
From: Prashant Sharma <scrapcodes@gmail.com>
Date: Mon, 5 May 2014 14:21:44 +0530
Message-ID: <CAOYDGoAZ28jZBMia-mr3zEYJBHLDhDjPQCQSc2jtudYY_YnUvA@mail.gmail.com>
Subject: Re: Apache spark on 27gb wikipedia data
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b2e5168d4d78704f8a33e3a
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b2e5168d4d78704f8a33e3a
Content-Type: text/plain; charset=UTF-8

I just thought may be we could put a warning whenever that error comes user
can tune either memoryFraction or executor memory options. And this warning
get's displayed when TaskSetManager receives task failures due to  OOM.

Prashant Sharma


On Mon, May 5, 2014 at 2:10 PM, Ajay Nair <prodigyaj@gmail.com> wrote:

> Hi,
>
> I am using 1 master and 3 slave workers for processing 27gb of Wikipedia
> data that is tab separated and every line contains wikipedia page
> information. The tab separated data has title of the page and the page
> contents. I am using the regular expression to extract links as mentioned
> in
> the site below:
>
> http://ampcamp.berkeley.edu/big-data-mini-course/graph-analytics-with-graphx.html#running-pagerank-on-wikipedia
>
> Although it runs fne for around 300Mb data set, it runs in to issues when I
> try to execute the same code using the 27gb data from hdfs.
> The error thrown is given below:
> 14/05/05 07:15:22 WARN scheduler.TaskSetManager: Loss was due to
> java.lang.OutOfMemoryError
> java.lang.OutOfMemoryError: GC overhead limit exceeded
>         at java.util.regex.Matcher.<init>(Matcher.java:224)
>
> Is there any way to over come this issue?
>
> My cluster is a ec2 m3.large machine.
>
> Thanks
> Ajay
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-spark-on-27gb-wikipedia-data-tp6487.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--047d7b2e5168d4d78704f8a33e3a--

From dev-return-7512-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May  5 15:57:11 2014
Return-Path: <dev-return-7512-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6122111D7D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 May 2014 15:57:11 +0000 (UTC)
Received: (qmail 88198 invoked by uid 500); 5 May 2014 15:57:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88103 invoked by uid 500); 5 May 2014 15:57:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88093 invoked by uid 99); 5 May 2014 15:57:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 15:57:07 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.214.176] (HELO mail-ob0-f176.google.com) (209.85.214.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 15:57:03 +0000
Received: by mail-ob0-f176.google.com with SMTP id wp4so8671602obc.35
        for <dev@spark.apache.org>; Mon, 05 May 2014 08:56:42 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=rbx4U8cVoyUa9QyDdo7XCrX6FtDH8bUy7gTwf4KdEcc=;
        b=l2xpsNLF/1a0lYRQzK0yveIfYHs+bl5sReOs4JdM2FMxV7vPXaZG5lSlvB0C6zZFWY
         g5+UL07t+m8zmh7OgP0jZVceoLOwTioyAuz3bsHSVAnVDJpdivnJ1kUYLtvp2zA+WacS
         A1KPTYua/IGIAPIYAem3xyHCjSVkbWbhEu65g3M1Dl5ADjFao+SS94rukwzUDjUBMbNd
         FUKMoXJvDZEe70guu3YK2d6LWOOlxZ8Bzsycdl1McbvZMG0cB+GBspxCATjZp3YO+D1u
         luGwfBhBYRC4IgZs5Fk+aSeg1ZWd0alec5grbDIlvkIwIqvS2i/HMhp+X7k57+owPLaM
         DrIg==
X-Gm-Message-State: ALoCoQm7bMB0dfRDm64quyWOmR6R5tIS1/iqBK8LcdwzjX11HhBFD15AKJIrknhtzXJdh8drLUHd
MIME-Version: 1.0
X-Received: by 10.182.45.161 with SMTP id o1mr12236029obm.29.1399305402276;
 Mon, 05 May 2014 08:56:42 -0700 (PDT)
Received: by 10.182.184.40 with HTTP; Mon, 5 May 2014 08:56:42 -0700 (PDT)
In-Reply-To: <CAPh_B=buc5ZuuK1_oYSuSdg8e=TFcHjkWuNihpCmzkY+jW112g@mail.gmail.com>
References: <CAJ3iqPRnJp-ezZAEJJDd4OSeF4f0qmEbaQAAJo2Y7YSrRXZc5A@mail.gmail.com>
	<CAPh_B=abv5w67MOsBESXku_OFNY_TsZFmAYnu-EGM+7GWEe2VQ@mail.gmail.com>
	<CAJ3iqPQqdZVztd+nnaOkueCNkkJQS2RfkeRP_iSvCZomYcyjOQ@mail.gmail.com>
	<CAPh_B=asgxMXEBnXtzX1xwz7fOBVBwjVu7=3coY_L5WtatgD3g@mail.gmail.com>
	<CAJ3iqPRgfdMnbcVKn1yX73CjmBh4_6QsH6orY=EZATPe2_gqmA@mail.gmail.com>
	<CAPh_B=Y=taT3jo6ZUdvKkaixsWZGmez8k8zLCv_K+jWoXFajLA@mail.gmail.com>
	<CAJ3iqPTnTNz_t8e_6u9jHpFne0sOY-kLDTRvoWTPZ7=5bHB4xQ@mail.gmail.com>
	<CAPh_B=buc5ZuuK1_oYSuSdg8e=TFcHjkWuNihpCmzkY+jW112g@mail.gmail.com>
Date: Mon, 5 May 2014 08:56:42 -0700
Message-ID: <CAJ3iqPS36L8UVGQ_gtrcE_f7V1TEJ7vzhURa8-3AL5qsRRizuA@mail.gmail.com>
Subject: Re: bug using kryo as closure serializer
From: Soren Macbeth <soren@yieldbot.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0158c1086fc35904f8a92db2
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158c1086fc35904f8a92db2
Content-Type: text/plain; charset=UTF-8

I just took a peek at KryoSerializer and it looks like you're already using
all the scala stuff from chill in there, so I would imagine that scala
things should serialize pretty well.

Seems like the readonly bytebuffer thing might be some other sort of
downstream bug.


On Sun, May 4, 2014 at 10:24 PM, Reynold Xin <rxin@databricks.com> wrote:

> Technically you only need to change the build file, and change part of a
> line in SparkEnv so you don't have to break your oath :)
>
>
>
> On Sun, May 4, 2014 at 10:22 PM, Soren Macbeth <soren@yieldbot.com> wrote:
>
> > that would violate my personal oath of never writing a single line of
> > scala, but I might be able to do that if I can get past the issue this
> > issue I'm struggling with in this thread.
> >
> > On Sunday, May 4, 2014, Reynold Xin <rxin@databricks.com> wrote:
> >
> > > Thanks. Do you mind playing with chill-scala a little bit and see if it
> > > actually works well for closures? One way to try is to hard code the
> > > serializer to use Kryo with chill-scala, and then run through all the
> > unit
> > > tests.
> > >
> > > If it works well, we can incorporate that in the next release (probably
> > not
> > > 1.0, but after that).
> > >
> > >
> > > On Sun, May 4, 2014 at 9:08 PM, Soren Macbeth <soren@yieldbot.com>
> > wrote:
> > >
> > > > fwiw, it seems like it wouldn't be very difficult to integrate
> > > chill-scala,
> > > > since you're already chill-java and probably get kryo serialization
> of
> > > > closures and all sorts of other scala stuff for free. All that would
> be
> > > > needed would be to include the dependency and then update
> > KryoSerializer
> > > to
> > > > register the stuff in chill-scala.
> > > >
> > > > In that case, you could probably safely make kryo the default
> > serializer,
> > > > which I think would be desirable in general.
> > > >
> > > >
> > > > On Sun, May 4, 2014 at 8:48 PM, Reynold Xin <rxin@databricks.com>
> > wrote:
> > > >
> > > > > Good idea. I submitted a pull request for the doc update here:
> > > > > https://github.com/apache/spark/pull/642
> > > > >
> > > > >
> > > > > On Sun, May 4, 2014 at 3:54 PM, Soren Macbeth <soren@yieldbot.com>
> > > > wrote:
> > > > >
> > > > > > Thanks for the reply!
> > > > > >
> > > > > > Ok, if that's the case, I'd recommend a note to that affect in
> the
> > > docs
> > > > > at
> > > > > > least.
> > > > > >
> > > > > > Just to give some more context here, I'm working on a Clojure DSL
> > for
> > > > > Spark
> > > > > > called Flambo, which I plan to open source shortly. If I could
> I'd
> > > like
> > > > > to
> > > > > > focus on the initial bug that I hit.
> > > > > >
> > > > > > Exception in thread "main" org.apache.spark.SparkException: Job
> > > > aborted:
> > > > > > Exception while deserializing and fetching task:
> > > > > > com.esotericsoftware.kryo.KryoException:
> > > > > > java.lang.IllegalArgumentException: Can not set final
> > > > > > scala.collection.convert.Wrappers field
> > > > > > scala.collection.convert.Wrappers$SeqWrapper.$outer to
> > > > > > clojure.lang.PersistentVector
> > > > > > Serialization trace:
> > > > > > $outer (scala.collection.convert.Wrappers$SeqWrapper)
> > > > > >         at
> > > > > >
> > > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1020)
> > > > > >         at
> > > > > >
> > > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1018)
> > > > > >         at
> > > > > >
> > > > > >
> > > > >
> > > >
> > >
> >
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> > > > > >         at
> > > > > >
> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> > > > > >         at org.apache.spark.scheduler.DAGScheduler.org
> > > > > >
> > > >
> > $apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1018)
> > > > > >         at
> > > > > >
> > > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
> > > > > >         at
> > > > > >
> > > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
> > > > > >         at scala.Option.foreach(Option.scala:236)
> > > > > >         at
> > > > > >
> > > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:604)
> > > > > >         at
> > > > > >
> > > > > >
> > > > >
> > > >
> > >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:190)
> > > > > >         at
> akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
> > > > > >         at akka.actor.ActorCell.invoke(ActorCell.scala:456)
> > > > > >
> >
>

--089e0158c1086fc35904f8a92db2--

From dev-return-7513-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May  5 16:05:13 2014
Return-Path: <dev-return-7513-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4B74B11DED
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 May 2014 16:05:13 +0000 (UTC)
Received: (qmail 8538 invoked by uid 500); 5 May 2014 16:05:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8490 invoked by uid 500); 5 May 2014 16:05:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8482 invoked by uid 99); 5 May 2014 16:05:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 16:05:11 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of prodigyaj@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 16:05:06 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <prodigyaj@gmail.com>)
	id 1WhLNm-0008Ms-A9
	for dev@spark.incubator.apache.org; Mon, 05 May 2014 09:04:46 -0700
Date: Mon, 5 May 2014 09:04:46 -0700 (PDT)
From: Ajay Nair <prodigyaj@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1399305886301-6490.post@n3.nabble.com>
In-Reply-To: <CAOYDGoAZ28jZBMia-mr3zEYJBHLDhDjPQCQSc2jtudYY_YnUvA@mail.gmail.com>
References: <1399279248689-6487.post@n3.nabble.com> <CAOYDGoAZ28jZBMia-mr3zEYJBHLDhDjPQCQSc2jtudYY_YnUvA@mail.gmail.com>
Subject: Re: Apache spark on 27gb wikipedia data
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

Is there any way to overcome this error? I am running this from the
spark-shell, is that the cause of concern ?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-spark-on-27gb-wikipedia-data-tp6487p6490.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7514-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May  5 20:58:59 2014
Return-Path: <dev-return-7514-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0FC86119DD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 May 2014 20:58:59 +0000 (UTC)
Received: (qmail 46554 invoked by uid 500); 5 May 2014 20:58:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46462 invoked by uid 500); 5 May 2014 20:58:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46453 invoked by uid 99); 5 May 2014 20:58:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 20:58:56 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.49 as permitted sender)
Received: from [209.85.220.49] (HELO mail-pa0-f49.google.com) (209.85.220.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 20:58:51 +0000
Received: by mail-pa0-f49.google.com with SMTP id lj1so4900677pab.8
        for <dev@spark.apache.org>; Mon, 05 May 2014 13:58:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=zW3UVcUCAtknztu8lcYn22IF+CVS9WWqixqiqDCQXS4=;
        b=mdgrDu5a4RC+nAid/CxLSrFmb5Yvvrpb9mYZBMlpDEpdwXbjvMD++h1+PIjU+4IoRS
         /bqLn8sS7w5ssZeOa4eGXA5EYylbIHEVvtTcgI3NMPsUY2hoxu0/FjSj5gIcEmrIRRql
         dlWgGuussSQm5VTpTTZO6qt+PaImFZdABk0/U0EHCyJ9zTr1TFX4GBjMnmVrotaffqTY
         Mnk1JSW/Ftm2v2IVoGP06XQmYptd6ocxPaCqyiZuuGnIU82lgglvpT/BRovvhe8xjCk0
         QdBvQOvDcCZqvnLU0DHjxxGb7tzmGbnNoKpmsS89pVGutLaKagmXuMTu2xfe3FLOWr3o
         Dfrg==
X-Received: by 10.66.221.99 with SMTP id qd3mr28641851pac.46.1399323511264;
        Mon, 05 May 2014 13:58:31 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id yq4sm80749720pab.34.2014.05.05.13.58.29
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 05 May 2014 13:58:30 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: Mailing list
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <DFEBB1B0-9ABD-4671-A46B-BBC1D3636EC0@hibnet.org>
Date: Mon, 5 May 2014 13:58:25 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <9DA476A6-E2C8-4591-84E9-26D6A0867BCF@gmail.com>
References: <13B7C349-0F74-4D2E-B0E6-536A270F8B19@hibnet.org> <A7DDB0A7-BA1E-41C9-8E66-5872E4D31552@gmail.com> <DFEBB1B0-9ABD-4671-A46B-BBC1D3636EC0@hibnet.org>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org


> The script you're talking about, is it merge_spark_pr.py [1] ?

Yup, that=92s it.

>=20
>> Note by the way that using GitHub is not at all necessary for using =
Git. We happened to do our development on GitHub before moving to the =
ASF, and all our developers were used to its interface, so we stuck with =
it. It definitely beats attaching patches on JIRA but it may not be the =
first step you want to take in moving to Git.
>=20
> Your workflow is indeed interesting. I guess most of Ant committers =
and potential contributors have experience with github too, so at some =
point we'll have to handle it. I'll discuss with the Ant dev community.
>=20
> Thank you Matei for the fix on the site and for the clear response.

Cool, glad to hear this is useful!

>=20
> Nicolas
>=20
> [1] https://github.com/apache/spark/blob/master/dev/merge_spark_pr.py
>=20


From dev-return-7515-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May  5 21:41:45 2014
Return-Path: <dev-return-7515-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0F2B511B5D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 May 2014 21:41:45 +0000 (UTC)
Received: (qmail 37235 invoked by uid 500); 5 May 2014 21:41:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37165 invoked by uid 500); 5 May 2014 21:41:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37157 invoked by uid 99); 5 May 2014 21:41:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 21:41:41 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.214.170 as permitted sender)
Received: from [209.85.214.170] (HELO mail-ob0-f170.google.com) (209.85.214.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 21:41:36 +0000
Received: by mail-ob0-f170.google.com with SMTP id uy5so3466887obc.29
        for <dev@spark.apache.org>; Mon, 05 May 2014 14:41:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=9RhJjlOO+h8ZF807r3N+d+x+AeKrxGSrpSz4zmgyI7Y=;
        b=s0hY0Pp4BgxNlNaKbSzZKSxK277f1pysCibtV22pKJzJZ1qd8SHMN/JRESlcO1nnWx
         ens6gat3gztjAFEp485jOEiRrwyBU8vaGPeEALEJd1knRiaJQwkRkk7Fdy+Auds3S/cU
         WtSJAVr7XPZvFPYpTu3A9mi0QfMr6pSbfyDHTEsMUAoVm6OVK31gdy57Ytnug3YynbAB
         Z8i26GIrLhY4EUWsqc+USzkAR0M57YAuJ/v5Q9Qoj5qLjCCtcrB0PpAOF9mVVJn/rLw1
         fdJgcTxlxAXFFSgr7HqSh+m4r6gFwce1tssDC25pMxti6WudNMcn4WtUtl8XGpsuVNAh
         pnVw==
MIME-Version: 1.0
X-Received: by 10.182.22.33 with SMTP id a1mr5167975obf.60.1399326075604; Mon,
 05 May 2014 14:41:15 -0700 (PDT)
Received: by 10.182.246.164 with HTTP; Mon, 5 May 2014 14:41:15 -0700 (PDT)
Date: Mon, 5 May 2014 14:41:15 -0700
Message-ID: <CA+B-+fzM9sxc+F7hxoWMo6hM6u64gMJMQSENAt7+GXRdTUeAfg@mail.gmail.com>
Subject: mllib vector templates
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1133177ca9546d04f8adfd1c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133177ca9546d04f8adfd1c
Content-Type: text/plain; charset=UTF-8

Hi,

Why mllib vector is using double as default ?

/**

 * Represents a numeric vector, whose index type is Int and value type is
Double.

 */

trait Vector extends Serializable {


  /**

   * Size of the vector.

   */

  def size: Int


  /**

   * Converts the instance to a double array.

   */

  def toArray: Array[Double]

Don't we need a template on float/double ? This will give us memory
savings...

Thanks.

Deb

--001a1133177ca9546d04f8adfd1c--

From dev-return-7516-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May  5 21:45:37 2014
Return-Path: <dev-return-7516-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 28B0811B9B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 May 2014 21:45:37 +0000 (UTC)
Received: (qmail 58615 invoked by uid 500); 5 May 2014 21:45:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58457 invoked by uid 500); 5 May 2014 21:45:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58379 invoked by uid 99); 5 May 2014 21:45:32 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 21:45:32 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.81 as permitted sender)
Received: from [171.67.219.81] (HELO smtp.stanford.edu) (171.67.219.81)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 21:45:28 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 241BE2102D
	for <dev@spark.apache.org>; Mon,  5 May 2014 14:45:04 -0700 (PDT)
Received: from mail-qa0-f48.google.com (mail-qa0-f48.google.com [209.85.216.48])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 8F3C32102F
	for <dev@spark.apache.org>; Mon,  5 May 2014 14:45:03 -0700 (PDT)
Received: by mail-qa0-f48.google.com with SMTP id i13so3037713qae.35
        for <dev@spark.apache.org>; Mon, 05 May 2014 14:45:02 -0700 (PDT)
X-Gm-Message-State: ALoCoQmGHQgLAUI3+jgsR3/J06TtkI849VSYIQsw17hoNokeUtrBtzcH5cUMjoCFodCKdBDWfK6Q
MIME-Version: 1.0
X-Received: by 10.140.51.172 with SMTP id u41mr45818439qga.69.1399326302649;
 Mon, 05 May 2014 14:45:02 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Mon, 5 May 2014 14:45:02 -0700 (PDT)
In-Reply-To: <CA+B-+fzM9sxc+F7hxoWMo6hM6u64gMJMQSENAt7+GXRdTUeAfg@mail.gmail.com>
References: <CA+B-+fzM9sxc+F7hxoWMo6hM6u64gMJMQSENAt7+GXRdTUeAfg@mail.gmail.com>
Date: Mon, 5 May 2014 14:45:02 -0700
Message-ID: <CAEYYnxYGGeTCtzGjN3+VRsRwB=ni7izCA21MXudn0Aq=kEaxtw@mail.gmail.com>
Subject: Re: mllib vector templates
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113517d2320f0704f8ae0b5c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113517d2320f0704f8ae0b5c
Content-Type: text/plain; charset=UTF-8

+1  Would be nice that we can use different type in Vector.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Mon, May 5, 2014 at 2:41 PM, Debasish Das <debasish.das83@gmail.com>wrote:

> Hi,
>
> Why mllib vector is using double as default ?
>
> /**
>
>  * Represents a numeric vector, whose index type is Int and value type is
> Double.
>
>  */
>
> trait Vector extends Serializable {
>
>
>   /**
>
>    * Size of the vector.
>
>    */
>
>   def size: Int
>
>
>   /**
>
>    * Converts the instance to a double array.
>
>    */
>
>   def toArray: Array[Double]
>
> Don't we need a template on float/double ? This will give us memory
> savings...
>
> Thanks.
>
> Deb
>

--001a113517d2320f0704f8ae0b5c--

From dev-return-7517-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May  5 21:56:51 2014
Return-Path: <dev-return-7517-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C270E11BF2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 May 2014 21:56:51 +0000 (UTC)
Received: (qmail 84603 invoked by uid 500); 5 May 2014 21:56:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84483 invoked by uid 500); 5 May 2014 21:56:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84475 invoked by uid 99); 5 May 2014 21:56:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 21:56:49 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.219.53 as permitted sender)
Received: from [209.85.219.53] (HELO mail-oa0-f53.google.com) (209.85.219.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 21:56:44 +0000
Received: by mail-oa0-f53.google.com with SMTP id m1so6679368oag.40
        for <dev@spark.apache.org>; Mon, 05 May 2014 14:56:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=XzjZ7nitP2zgepE3YxGjz93Nj0Sdgxdy5cF2rLxjmCY=;
        b=hbgdWq6Crp+4DfZCGOebGL3KUH2g+EWY0OVeJxpqKzOm7un8XpsgpGIrA8qMnhUq9D
         l9JtYMpWVEusWwRXLqXsHjiRkhQVPC8sDNfoaR1juczf4adPlFjbUeaBKsG3ltHAGT04
         L8o+v9cKvkW1NquixvV41tZBxW861t82rrvU9DdkQM1VY/mDf3FbgvE8y0ddNJa56wDF
         Do8i32XQDer129SMfwmycZAiY646yNTnAlXgHJ52NVTPcTZAihSYjIbpcCQsDN1p39rL
         PyQP5PFIuKgwP1PDNheJ4Mnw8bv4wDzDQOl74tf314izBCq9sVrplIDqHWv+a7q9ak9C
         qDgQ==
MIME-Version: 1.0
X-Received: by 10.60.134.137 with SMTP id pk9mr35690729oeb.40.1399326981122;
 Mon, 05 May 2014 14:56:21 -0700 (PDT)
Received: by 10.182.246.164 with HTTP; Mon, 5 May 2014 14:56:21 -0700 (PDT)
In-Reply-To: <CAEYYnxYGGeTCtzGjN3+VRsRwB=ni7izCA21MXudn0Aq=kEaxtw@mail.gmail.com>
References: <CA+B-+fzM9sxc+F7hxoWMo6hM6u64gMJMQSENAt7+GXRdTUeAfg@mail.gmail.com>
	<CAEYYnxYGGeTCtzGjN3+VRsRwB=ni7izCA21MXudn0Aq=kEaxtw@mail.gmail.com>
Date: Mon, 5 May 2014 14:56:21 -0700
Message-ID: <CA+B-+fxSfun1gzWTcm+m1jSyyxi0-CDjE4TkMY_0Vb2WkKAowA@mail.gmail.com>
Subject: Re: mllib vector templates
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b471dfaa272ad04f8ae3357
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b471dfaa272ad04f8ae3357
Content-Type: text/plain; charset=UTF-8

Is this a breeze issue or breeze can take templates on float / double ?

If breeze can take templates then it is a minor fix for Vectors.scala right
?

Thanks.
Deb


On Mon, May 5, 2014 at 2:45 PM, DB Tsai <dbtsai@stanford.edu> wrote:

> +1  Would be nice that we can use different type in Vector.
>
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Mon, May 5, 2014 at 2:41 PM, Debasish Das <debasish.das83@gmail.com
> >wrote:
>
> > Hi,
> >
> > Why mllib vector is using double as default ?
> >
> > /**
> >
> >  * Represents a numeric vector, whose index type is Int and value type is
> > Double.
> >
> >  */
> >
> > trait Vector extends Serializable {
> >
> >
> >   /**
> >
> >    * Size of the vector.
> >
> >    */
> >
> >   def size: Int
> >
> >
> >   /**
> >
> >    * Converts the instance to a double array.
> >
> >    */
> >
> >   def toArray: Array[Double]
> >
> > Don't we need a template on float/double ? This will give us memory
> > savings...
> >
> > Thanks.
> >
> > Deb
> >
>

--047d7b471dfaa272ad04f8ae3357--

From dev-return-7518-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May  5 22:03:27 2014
Return-Path: <dev-return-7518-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1CD7B11C33
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 May 2014 22:03:27 +0000 (UTC)
Received: (qmail 91644 invoked by uid 500); 5 May 2014 22:03:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91579 invoked by uid 500); 5 May 2014 22:03:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91520 invoked by uid 99); 5 May 2014 22:03:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 22:03:17 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 22:03:13 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 03241101222
	for <dev@spark.apache.org>; Mon,  5 May 2014 15:02:50 -0700 (PDT)
Received: from mail-qa0-f45.google.com (mail-qa0-f45.google.com [209.85.216.45])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 5A1961010BA
	for <dev@spark.apache.org>; Mon,  5 May 2014 15:02:49 -0700 (PDT)
Received: by mail-qa0-f45.google.com with SMTP id hw13so7377764qab.4
        for <dev@spark.apache.org>; Mon, 05 May 2014 15:02:48 -0700 (PDT)
X-Gm-Message-State: ALoCoQnU1l/9O9VwaDdpeuAf/FqafzPaZI1heK7CeyB3kohNQHNTRXE7Zzxe4oXDQclTO5MjuZe7
MIME-Version: 1.0
X-Received: by 10.224.30.131 with SMTP id u3mr50274078qac.50.1399327368312;
 Mon, 05 May 2014 15:02:48 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Mon, 5 May 2014 15:02:48 -0700 (PDT)
In-Reply-To: <CA+B-+fxSfun1gzWTcm+m1jSyyxi0-CDjE4TkMY_0Vb2WkKAowA@mail.gmail.com>
References: <CA+B-+fzM9sxc+F7hxoWMo6hM6u64gMJMQSENAt7+GXRdTUeAfg@mail.gmail.com>
	<CAEYYnxYGGeTCtzGjN3+VRsRwB=ni7izCA21MXudn0Aq=kEaxtw@mail.gmail.com>
	<CA+B-+fxSfun1gzWTcm+m1jSyyxi0-CDjE4TkMY_0Vb2WkKAowA@mail.gmail.com>
Date: Mon, 5 May 2014 15:02:48 -0700
Message-ID: <CAEYYnxZMnAiBwNW8O7_moMbX8hBaZO1Gw=TY2gcMOhJrRHVb9Q@mail.gmail.com>
Subject: Re: mllib vector templates
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bdc90a0b6a99b04f8ae4a10
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc90a0b6a99b04f8ae4a10
Content-Type: text/plain; charset=UTF-8

Breeze could take any type (Int, Long, Double, and Float) in the matrix
template.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Mon, May 5, 2014 at 2:56 PM, Debasish Das <debasish.das83@gmail.com>wrote:

> Is this a breeze issue or breeze can take templates on float / double ?
>
> If breeze can take templates then it is a minor fix for Vectors.scala right
> ?
>
> Thanks.
> Deb
>
>
> On Mon, May 5, 2014 at 2:45 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>
> > +1  Would be nice that we can use different type in Vector.
> >
> >
> > Sincerely,
> >
> > DB Tsai
> > -------------------------------------------------------
> > My Blog: https://www.dbtsai.com
> > LinkedIn: https://www.linkedin.com/in/dbtsai
> >
> >
> > On Mon, May 5, 2014 at 2:41 PM, Debasish Das <debasish.das83@gmail.com
> > >wrote:
> >
> > > Hi,
> > >
> > > Why mllib vector is using double as default ?
> > >
> > > /**
> > >
> > >  * Represents a numeric vector, whose index type is Int and value type
> is
> > > Double.
> > >
> > >  */
> > >
> > > trait Vector extends Serializable {
> > >
> > >
> > >   /**
> > >
> > >    * Size of the vector.
> > >
> > >    */
> > >
> > >   def size: Int
> > >
> > >
> > >   /**
> > >
> > >    * Converts the instance to a double array.
> > >
> > >    */
> > >
> > >   def toArray: Array[Double]
> > >
> > > Don't we need a template on float/double ? This will give us memory
> > > savings...
> > >
> > > Thanks.
> > >
> > > Deb
> > >
> >
>

--047d7bdc90a0b6a99b04f8ae4a10--

From dev-return-7519-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May  5 22:06:32 2014
Return-Path: <dev-return-7519-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 33A0C11C48
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 May 2014 22:06:32 +0000 (UTC)
Received: (qmail 98640 invoked by uid 500); 5 May 2014 22:06:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98522 invoked by uid 500); 5 May 2014 22:06:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98513 invoked by uid 99); 5 May 2014 22:06:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 22:06:30 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of david.lw.hall@gmail.com designates 74.125.82.180 as permitted sender)
Received: from [74.125.82.180] (HELO mail-we0-f180.google.com) (74.125.82.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 22:06:26 +0000
Received: by mail-we0-f180.google.com with SMTP id t61so2882994wes.11
        for <dev@spark.apache.org>; Mon, 05 May 2014 15:06:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=LPWJ3T8PKeDWbgAKqoJ10zbrkpWYxHCARCsUnK33PNc=;
        b=yxLlO4gXU7D6Cpeam23FPdRnkHTxoznaCOKrohchdfo6TfIOEuDIMSbiLVC2Q6DFsx
         oWnfIFRkz9EgRW2hJ1a0lOKcl/5hU6kHJqObJjSjRqTvcTMmMlGqnXfblXS8NATyTbex
         ZOgEjRi7qaBJ0Gus0BJNTX7GksAneDnvQKSx2QHqeQ+3Eg+djpFavrdVP/5pD4V/gX8Z
         wlK7PkJtrahqMObeRAXLdVj1NkpSyhjcHohy6zU6Q54f+IN5w7X1Y05oPAyYyOg1zL2T
         mN08PXiLgVjBZG6JJNutqIvtP8fxtVdEN8iQEMx9/biuYohRY90ZS0QjCHkhozQs9PQb
         QUWw==
MIME-Version: 1.0
X-Received: by 10.194.92.7 with SMTP id ci7mr29875788wjb.7.1399327565066; Mon,
 05 May 2014 15:06:05 -0700 (PDT)
Sender: david.lw.hall@gmail.com
Received: by 10.227.232.207 with HTTP; Mon, 5 May 2014 15:06:05 -0700 (PDT)
Received: by 10.227.232.207 with HTTP; Mon, 5 May 2014 15:06:05 -0700 (PDT)
In-Reply-To: <CAEYYnxZMnAiBwNW8O7_moMbX8hBaZO1Gw=TY2gcMOhJrRHVb9Q@mail.gmail.com>
References: <CA+B-+fzM9sxc+F7hxoWMo6hM6u64gMJMQSENAt7+GXRdTUeAfg@mail.gmail.com>
	<CAEYYnxYGGeTCtzGjN3+VRsRwB=ni7izCA21MXudn0Aq=kEaxtw@mail.gmail.com>
	<CA+B-+fxSfun1gzWTcm+m1jSyyxi0-CDjE4TkMY_0Vb2WkKAowA@mail.gmail.com>
	<CAEYYnxZMnAiBwNW8O7_moMbX8hBaZO1Gw=TY2gcMOhJrRHVb9Q@mail.gmail.com>
Date: Mon, 5 May 2014 15:06:05 -0700
X-Google-Sender-Auth: mGuEWHwJrNw_0pU0gK3KG5Oo4Ew
Message-ID: <CALW2ey1E1tE9gsbs4ftv8J1JQseANMaUt-muGCK=KG3UDLRC7Q@mail.gmail.com>
Subject: Re: mllib vector templates
From: David Hall <dlwh@cs.berkeley.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0149460270ba6404f8ae56f9
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149460270ba6404f8ae56f9
Content-Type: text/plain; charset=UTF-8

Lbfgs and other optimizers would not work immediately, as they require
vector spaces over double. Otherwise it should work.
On May 5, 2014 3:03 PM, "DB Tsai" <dbtsai@stanford.edu> wrote:

> Breeze could take any type (Int, Long, Double, and Float) in the matrix
> template.
>
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Mon, May 5, 2014 at 2:56 PM, Debasish Das <debasish.das83@gmail.com
> >wrote:
>
> > Is this a breeze issue or breeze can take templates on float / double ?
> >
> > If breeze can take templates then it is a minor fix for Vectors.scala
> right
> > ?
> >
> > Thanks.
> > Deb
> >
> >
> > On Mon, May 5, 2014 at 2:45 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> >
> > > +1  Would be nice that we can use different type in Vector.
> > >
> > >
> > > Sincerely,
> > >
> > > DB Tsai
> > > -------------------------------------------------------
> > > My Blog: https://www.dbtsai.com
> > > LinkedIn: https://www.linkedin.com/in/dbtsai
> > >
> > >
> > > On Mon, May 5, 2014 at 2:41 PM, Debasish Das <debasish.das83@gmail.com
> > > >wrote:
> > >
> > > > Hi,
> > > >
> > > > Why mllib vector is using double as default ?
> > > >
> > > > /**
> > > >
> > > >  * Represents a numeric vector, whose index type is Int and value
> type
> > is
> > > > Double.
> > > >
> > > >  */
> > > >
> > > > trait Vector extends Serializable {
> > > >
> > > >
> > > >   /**
> > > >
> > > >    * Size of the vector.
> > > >
> > > >    */
> > > >
> > > >   def size: Int
> > > >
> > > >
> > > >   /**
> > > >
> > > >    * Converts the instance to a double array.
> > > >
> > > >    */
> > > >
> > > >   def toArray: Array[Double]
> > > >
> > > > Don't we need a template on float/double ? This will give us memory
> > > > savings...
> > > >
> > > > Thanks.
> > > >
> > > > Deb
> > > >
> > >
> >
>

--089e0149460270ba6404f8ae56f9--

From dev-return-7520-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May  5 22:12:42 2014
Return-Path: <dev-return-7520-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C0D1C11C7A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 May 2014 22:12:42 +0000 (UTC)
Received: (qmail 11687 invoked by uid 500); 5 May 2014 22:12:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11587 invoked by uid 500); 5 May 2014 22:12:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11578 invoked by uid 99); 5 May 2014 22:12:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 22:12:40 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.214.176 as permitted sender)
Received: from [209.85.214.176] (HELO mail-ob0-f176.google.com) (209.85.214.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 22:12:37 +0000
Received: by mail-ob0-f176.google.com with SMTP id wp4so9242781obc.35
        for <dev@spark.apache.org>; Mon, 05 May 2014 15:12:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=z50cyHFN2E3Ka0l3O6W7BhXer4QspKPHajKQQltKmcI=;
        b=vOZFyJ9NFFG/omwMAoeNHeKEaUFjaAg9cWuqpdCfIXJNhPhKMtprWto1wii9sbso0+
         madIcLju/Ct1F/2gJBYWyDhcoCgSWvMHG6VUsaJGcJfDM6p46FYU9Feencq6WaaUwSyY
         vXwz8bJzNZnPyEgfSPZ32tYXK1ptrm1CJcavRC/V0tw7P3uoIeXPFygD+qrYeC03R5+Q
         hz+nyf7jyMI08JgpAdoMEA53x9brhN5XcNvgisExkRyBqdbHcnhaXu9CNmyteXvkpaEz
         bAVtSb9StzePIOh/aQ9zSV2dv0ArsUcDKHlCWp3Oe6+KU87Zsd1cGnKYL5wheZJ7mSK2
         Butw==
MIME-Version: 1.0
X-Received: by 10.182.225.137 with SMTP id rk9mr5690771obc.51.1399327935611;
 Mon, 05 May 2014 15:12:15 -0700 (PDT)
Received: by 10.182.246.164 with HTTP; Mon, 5 May 2014 15:12:15 -0700 (PDT)
In-Reply-To: <CALW2ey1E1tE9gsbs4ftv8J1JQseANMaUt-muGCK=KG3UDLRC7Q@mail.gmail.com>
References: <CA+B-+fzM9sxc+F7hxoWMo6hM6u64gMJMQSENAt7+GXRdTUeAfg@mail.gmail.com>
	<CAEYYnxYGGeTCtzGjN3+VRsRwB=ni7izCA21MXudn0Aq=kEaxtw@mail.gmail.com>
	<CA+B-+fxSfun1gzWTcm+m1jSyyxi0-CDjE4TkMY_0Vb2WkKAowA@mail.gmail.com>
	<CAEYYnxZMnAiBwNW8O7_moMbX8hBaZO1Gw=TY2gcMOhJrRHVb9Q@mail.gmail.com>
	<CALW2ey1E1tE9gsbs4ftv8J1JQseANMaUt-muGCK=KG3UDLRC7Q@mail.gmail.com>
Date: Mon, 5 May 2014 15:12:15 -0700
Message-ID: <CA+B-+fy-=Kgr9DqvK-Lo74M75V3izkKJ3xnxQZC-XCCMcwX94g@mail.gmail.com>
Subject: Re: mllib vector templates
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c3001886c8a304f8ae6c5e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3001886c8a304f8ae6c5e
Content-Type: text/plain; charset=UTF-8

Is any one facing issues due to this ? If not then I guess doubles are
fine...

For me it's not a big deal as there is enough memory available...


On Mon, May 5, 2014 at 3:06 PM, David Hall <dlwh@cs.berkeley.edu> wrote:

> Lbfgs and other optimizers would not work immediately, as they require
> vector spaces over double. Otherwise it should work.
> On May 5, 2014 3:03 PM, "DB Tsai" <dbtsai@stanford.edu> wrote:
>
> > Breeze could take any type (Int, Long, Double, and Float) in the matrix
> > template.
> >
> >
> > Sincerely,
> >
> > DB Tsai
> > -------------------------------------------------------
> > My Blog: https://www.dbtsai.com
> > LinkedIn: https://www.linkedin.com/in/dbtsai
> >
> >
> > On Mon, May 5, 2014 at 2:56 PM, Debasish Das <debasish.das83@gmail.com
> > >wrote:
> >
> > > Is this a breeze issue or breeze can take templates on float / double ?
> > >
> > > If breeze can take templates then it is a minor fix for Vectors.scala
> > right
> > > ?
> > >
> > > Thanks.
> > > Deb
> > >
> > >
> > > On Mon, May 5, 2014 at 2:45 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> > >
> > > > +1  Would be nice that we can use different type in Vector.
> > > >
> > > >
> > > > Sincerely,
> > > >
> > > > DB Tsai
> > > > -------------------------------------------------------
> > > > My Blog: https://www.dbtsai.com
> > > > LinkedIn: https://www.linkedin.com/in/dbtsai
> > > >
> > > >
> > > > On Mon, May 5, 2014 at 2:41 PM, Debasish Das <
> debasish.das83@gmail.com
> > > > >wrote:
> > > >
> > > > > Hi,
> > > > >
> > > > > Why mllib vector is using double as default ?
> > > > >
> > > > > /**
> > > > >
> > > > >  * Represents a numeric vector, whose index type is Int and value
> > type
> > > is
> > > > > Double.
> > > > >
> > > > >  */
> > > > >
> > > > > trait Vector extends Serializable {
> > > > >
> > > > >
> > > > >   /**
> > > > >
> > > > >    * Size of the vector.
> > > > >
> > > > >    */
> > > > >
> > > > >   def size: Int
> > > > >
> > > > >
> > > > >   /**
> > > > >
> > > > >    * Converts the instance to a double array.
> > > > >
> > > > >    */
> > > > >
> > > > >   def toArray: Array[Double]
> > > > >
> > > > > Don't we need a template on float/double ? This will give us memory
> > > > > savings...
> > > > >
> > > > > Thanks.
> > > > >
> > > > > Deb
> > > > >
> > > >
> > >
> >
>

--001a11c3001886c8a304f8ae6c5e--

From dev-return-7521-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May  5 22:14:39 2014
Return-Path: <dev-return-7521-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2E40611C89
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 May 2014 22:14:39 +0000 (UTC)
Received: (qmail 19214 invoked by uid 500); 5 May 2014 22:14:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19082 invoked by uid 500); 5 May 2014 22:14:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19074 invoked by uid 99); 5 May 2014 22:14:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 22:14:36 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of david.lw.hall@gmail.com designates 209.85.212.169 as permitted sender)
Received: from [209.85.212.169] (HELO mail-wi0-f169.google.com) (209.85.212.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 22:14:33 +0000
Received: by mail-wi0-f169.google.com with SMTP id hi2so1997293wib.2
        for <dev@spark.apache.org>; Mon, 05 May 2014 15:14:10 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=A3ZOLZzT/YkIQ/RQ3VsnMnSnydMdIVyZn843lQWxSAA=;
        b=vNQqKYuOyO27zE3eWnXYZ2PX/jndJ8QPydeyTtSYR4vR4NPGVgy4j2yQskw6WL6BKh
         dYRG/7gBaUTQ394tqHS7Lj3+ywM1NMR/edQ22zB6q4W6CzDWIRa7CAwsnPySiOu2ZFgh
         zc+HLwPNsoFp8QqdUQ7vAEyownN7+ZWomLl3y4dyUF1+5SVPrtIA0h++JW5pOoNQwJTC
         xk1wcAAIwNIWufTDZ1mZrSe4a4lDuYBLYIDXxP/PbU88SXOnYe2c5bPTHESIx3Q5IXGz
         8o6rNuseQe9rxZRDcHMf9UcDOhy5sLvteLkqvSBsqnobVyM18zhHHyCR3I+7+/JPghj0
         q6tQ==
MIME-Version: 1.0
X-Received: by 10.180.189.69 with SMTP id gg5mr18011071wic.52.1399328050835;
 Mon, 05 May 2014 15:14:10 -0700 (PDT)
Sender: david.lw.hall@gmail.com
Received: by 10.227.232.207 with HTTP; Mon, 5 May 2014 15:14:10 -0700 (PDT)
Received: by 10.227.232.207 with HTTP; Mon, 5 May 2014 15:14:10 -0700 (PDT)
In-Reply-To: <CA+B-+fy-=Kgr9DqvK-Lo74M75V3izkKJ3xnxQZC-XCCMcwX94g@mail.gmail.com>
References: <CA+B-+fzM9sxc+F7hxoWMo6hM6u64gMJMQSENAt7+GXRdTUeAfg@mail.gmail.com>
	<CAEYYnxYGGeTCtzGjN3+VRsRwB=ni7izCA21MXudn0Aq=kEaxtw@mail.gmail.com>
	<CA+B-+fxSfun1gzWTcm+m1jSyyxi0-CDjE4TkMY_0Vb2WkKAowA@mail.gmail.com>
	<CAEYYnxZMnAiBwNW8O7_moMbX8hBaZO1Gw=TY2gcMOhJrRHVb9Q@mail.gmail.com>
	<CALW2ey1E1tE9gsbs4ftv8J1JQseANMaUt-muGCK=KG3UDLRC7Q@mail.gmail.com>
	<CA+B-+fy-=Kgr9DqvK-Lo74M75V3izkKJ3xnxQZC-XCCMcwX94g@mail.gmail.com>
Date: Mon, 5 May 2014 15:14:10 -0700
X-Google-Sender-Auth: N2gdrYWBwzV2wCOBF_wCyJH6CBc
Message-ID: <CALW2ey38xVLWv9-LPE3AUwmjguMg_y-JyyS2B4Zo0S=8FC658A@mail.gmail.com>
Subject: Re: mllib vector templates
From: David Hall <dlwh@cs.berkeley.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c329fa64f85f04f8ae73ed
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c329fa64f85f04f8ae73ed
Content-Type: text/plain; charset=UTF-8

I should mention it shouldn't be too hard to change, but it is a current
limitation.
On May 5, 2014 3:12 PM, "Debasish Das" <debasish.das83@gmail.com> wrote:

> Is any one facing issues due to this ? If not then I guess doubles are
> fine...
>
> For me it's not a big deal as there is enough memory available...
>
>
> On Mon, May 5, 2014 at 3:06 PM, David Hall <dlwh@cs.berkeley.edu> wrote:
>
> > Lbfgs and other optimizers would not work immediately, as they require
> > vector spaces over double. Otherwise it should work.
> > On May 5, 2014 3:03 PM, "DB Tsai" <dbtsai@stanford.edu> wrote:
> >
> > > Breeze could take any type (Int, Long, Double, and Float) in the matrix
> > > template.
> > >
> > >
> > > Sincerely,
> > >
> > > DB Tsai
> > > -------------------------------------------------------
> > > My Blog: https://www.dbtsai.com
> > > LinkedIn: https://www.linkedin.com/in/dbtsai
> > >
> > >
> > > On Mon, May 5, 2014 at 2:56 PM, Debasish Das <debasish.das83@gmail.com
> > > >wrote:
> > >
> > > > Is this a breeze issue or breeze can take templates on float /
> double ?
> > > >
> > > > If breeze can take templates then it is a minor fix for Vectors.scala
> > > right
> > > > ?
> > > >
> > > > Thanks.
> > > > Deb
> > > >
> > > >
> > > > On Mon, May 5, 2014 at 2:45 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> > > >
> > > > > +1  Would be nice that we can use different type in Vector.
> > > > >
> > > > >
> > > > > Sincerely,
> > > > >
> > > > > DB Tsai
> > > > > -------------------------------------------------------
> > > > > My Blog: https://www.dbtsai.com
> > > > > LinkedIn: https://www.linkedin.com/in/dbtsai
> > > > >
> > > > >
> > > > > On Mon, May 5, 2014 at 2:41 PM, Debasish Das <
> > debasish.das83@gmail.com
> > > > > >wrote:
> > > > >
> > > > > > Hi,
> > > > > >
> > > > > > Why mllib vector is using double as default ?
> > > > > >
> > > > > > /**
> > > > > >
> > > > > >  * Represents a numeric vector, whose index type is Int and value
> > > type
> > > > is
> > > > > > Double.
> > > > > >
> > > > > >  */
> > > > > >
> > > > > > trait Vector extends Serializable {
> > > > > >
> > > > > >
> > > > > >   /**
> > > > > >
> > > > > >    * Size of the vector.
> > > > > >
> > > > > >    */
> > > > > >
> > > > > >   def size: Int
> > > > > >
> > > > > >
> > > > > >   /**
> > > > > >
> > > > > >    * Converts the instance to a double array.
> > > > > >
> > > > > >    */
> > > > > >
> > > > > >   def toArray: Array[Double]
> > > > > >
> > > > > > Don't we need a template on float/double ? This will give us
> memory
> > > > > > savings...
> > > > > >
> > > > > > Thanks.
> > > > > >
> > > > > > Deb
> > > > > >
> > > > >
> > > >
> > >
> >
>

--001a11c329fa64f85f04f8ae73ed--

From dev-return-7522-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May  5 22:41:02 2014
Return-Path: <dev-return-7522-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 29C7411D6B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 May 2014 22:41:02 +0000 (UTC)
Received: (qmail 75284 invoked by uid 500); 5 May 2014 22:40:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75248 invoked by uid 500); 5 May 2014 22:40:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75240 invoked by uid 99); 5 May 2014 22:40:57 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 22:40:57 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.81 as permitted sender)
Received: from [171.67.219.81] (HELO smtp.stanford.edu) (171.67.219.81)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 22:40:53 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 7CBC621440
	for <dev@spark.apache.org>; Mon,  5 May 2014 15:40:32 -0700 (PDT)
Received: from mail-qg0-f46.google.com (mail-qg0-f46.google.com [209.85.192.46])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id CCABC214E1
	for <dev@spark.apache.org>; Mon,  5 May 2014 15:40:31 -0700 (PDT)
Received: by mail-qg0-f46.google.com with SMTP id q108so5386120qgd.19
        for <dev@spark.apache.org>; Mon, 05 May 2014 15:40:31 -0700 (PDT)
X-Gm-Message-State: ALoCoQkhKgsdCsyNPSNpVKZ8NBa/ZaDL1A7MjZ1GaNHbC/7Ss1lWEkbl6NFYvQDZVWY79cgJydit
MIME-Version: 1.0
X-Received: by 10.140.28.198 with SMTP id 64mr35236944qgz.49.1399329631014;
 Mon, 05 May 2014 15:40:31 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Mon, 5 May 2014 15:40:30 -0700 (PDT)
In-Reply-To: <CALW2ey1E1tE9gsbs4ftv8J1JQseANMaUt-muGCK=KG3UDLRC7Q@mail.gmail.com>
References: <CA+B-+fzM9sxc+F7hxoWMo6hM6u64gMJMQSENAt7+GXRdTUeAfg@mail.gmail.com>
	<CAEYYnxYGGeTCtzGjN3+VRsRwB=ni7izCA21MXudn0Aq=kEaxtw@mail.gmail.com>
	<CA+B-+fxSfun1gzWTcm+m1jSyyxi0-CDjE4TkMY_0Vb2WkKAowA@mail.gmail.com>
	<CAEYYnxZMnAiBwNW8O7_moMbX8hBaZO1Gw=TY2gcMOhJrRHVb9Q@mail.gmail.com>
	<CALW2ey1E1tE9gsbs4ftv8J1JQseANMaUt-muGCK=KG3UDLRC7Q@mail.gmail.com>
Date: Mon, 5 May 2014 15:40:30 -0700
Message-ID: <CAEYYnxYo=9JwQpDQiNRvMp3n_tJLUOwWv_Mz28-P9VxW5QHAdg@mail.gmail.com>
Subject: Re: mllib vector templates
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11390f1c94cd6f04f8aed166
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11390f1c94cd6f04f8aed166
Content-Type: text/plain; charset=UTF-8

David,

Could we use Int, Long, Float as the data feature spaces, and Double for
optimizer?


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Mon, May 5, 2014 at 3:06 PM, David Hall <dlwh@cs.berkeley.edu> wrote:

> Lbfgs and other optimizers would not work immediately, as they require
> vector spaces over double. Otherwise it should work.
> On May 5, 2014 3:03 PM, "DB Tsai" <dbtsai@stanford.edu> wrote:
>
> > Breeze could take any type (Int, Long, Double, and Float) in the matrix
> > template.
> >
> >
> > Sincerely,
> >
> > DB Tsai
> > -------------------------------------------------------
> > My Blog: https://www.dbtsai.com
> > LinkedIn: https://www.linkedin.com/in/dbtsai
> >
> >
> > On Mon, May 5, 2014 at 2:56 PM, Debasish Das <debasish.das83@gmail.com
> > >wrote:
> >
> > > Is this a breeze issue or breeze can take templates on float / double ?
> > >
> > > If breeze can take templates then it is a minor fix for Vectors.scala
> > right
> > > ?
> > >
> > > Thanks.
> > > Deb
> > >
> > >
> > > On Mon, May 5, 2014 at 2:45 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> > >
> > > > +1  Would be nice that we can use different type in Vector.
> > > >
> > > >
> > > > Sincerely,
> > > >
> > > > DB Tsai
> > > > -------------------------------------------------------
> > > > My Blog: https://www.dbtsai.com
> > > > LinkedIn: https://www.linkedin.com/in/dbtsai
> > > >
> > > >
> > > > On Mon, May 5, 2014 at 2:41 PM, Debasish Das <
> debasish.das83@gmail.com
> > > > >wrote:
> > > >
> > > > > Hi,
> > > > >
> > > > > Why mllib vector is using double as default ?
> > > > >
> > > > > /**
> > > > >
> > > > >  * Represents a numeric vector, whose index type is Int and value
> > type
> > > is
> > > > > Double.
> > > > >
> > > > >  */
> > > > >
> > > > > trait Vector extends Serializable {
> > > > >
> > > > >
> > > > >   /**
> > > > >
> > > > >    * Size of the vector.
> > > > >
> > > > >    */
> > > > >
> > > > >   def size: Int
> > > > >
> > > > >
> > > > >   /**
> > > > >
> > > > >    * Converts the instance to a double array.
> > > > >
> > > > >    */
> > > > >
> > > > >   def toArray: Array[Double]
> > > > >
> > > > > Don't we need a template on float/double ? This will give us memory
> > > > > savings...
> > > > >
> > > > > Thanks.
> > > > >
> > > > > Deb
> > > > >
> > > >
> > >
> >
>

--001a11390f1c94cd6f04f8aed166--

From dev-return-7523-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May  5 22:53:19 2014
Return-Path: <dev-return-7523-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 374E511DC8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 May 2014 22:53:19 +0000 (UTC)
Received: (qmail 2215 invoked by uid 500); 5 May 2014 22:53:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2155 invoked by uid 500); 5 May 2014 22:53:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2144 invoked by uid 99); 5 May 2014 22:53:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 22:53:15 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 74.125.82.43 as permitted sender)
Received: from [74.125.82.43] (HELO mail-wg0-f43.google.com) (74.125.82.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 22:53:12 +0000
Received: by mail-wg0-f43.google.com with SMTP id l18so7305970wgh.14
        for <dev@spark.apache.org>; Mon, 05 May 2014 15:52:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=pFBbFh0dusTw+5djrdvD0+Q2vWgy6sySiudg2mAsDyI=;
        b=AbFxU7hErVzF8zOFh/gCsowlPClIJ/F4AoSn6qXX4bXXlWbVT0NotINP1wG45S0nxc
         oBuVwLo5m93SpDidf+a3HXJSMUii6871M7ll9jSm+9Bu44xoidZVYS1vFYyYRQHwbHUe
         4b9puOV2jXVH4zDUPbbPVF5bzGiLWEOaJlYE22XaOMezWyBdB2oLWu3H0VGUO8lcvtyM
         RjUHZ9DkpB7N7Nq9+B898fOu5eW5ApWfbSCesgUnrQiaJiXYDh634U9s3hnBwC+f2Y7b
         U2NFdXACFKzNkcQlwPocyu1RM4fV6rubgTvXFK+Mu4phF2WPy0F/YhGd63ePnkySejDh
         C5ag==
MIME-Version: 1.0
X-Received: by 10.194.119.34 with SMTP id kr2mr8985563wjb.34.1399330370150;
 Mon, 05 May 2014 15:52:50 -0700 (PDT)
Received: by 10.194.82.105 with HTTP; Mon, 5 May 2014 15:52:50 -0700 (PDT)
In-Reply-To: <CAEYYnxYo=9JwQpDQiNRvMp3n_tJLUOwWv_Mz28-P9VxW5QHAdg@mail.gmail.com>
References: <CA+B-+fzM9sxc+F7hxoWMo6hM6u64gMJMQSENAt7+GXRdTUeAfg@mail.gmail.com>
	<CAEYYnxYGGeTCtzGjN3+VRsRwB=ni7izCA21MXudn0Aq=kEaxtw@mail.gmail.com>
	<CA+B-+fxSfun1gzWTcm+m1jSyyxi0-CDjE4TkMY_0Vb2WkKAowA@mail.gmail.com>
	<CAEYYnxZMnAiBwNW8O7_moMbX8hBaZO1Gw=TY2gcMOhJrRHVb9Q@mail.gmail.com>
	<CALW2ey1E1tE9gsbs4ftv8J1JQseANMaUt-muGCK=KG3UDLRC7Q@mail.gmail.com>
	<CAEYYnxYo=9JwQpDQiNRvMp3n_tJLUOwWv_Mz28-P9VxW5QHAdg@mail.gmail.com>
Date: Mon, 5 May 2014 15:52:50 -0700
Message-ID: <CAJgQjQ-vhzF6EnKVU7bTPuUU22cyGOGymC3SC-Z4LQjxe93THQ@mail.gmail.com>
Subject: Re: mllib vector templates
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I fixed index type and value type to make things simple, especially
when we need to provide Java and Python APIs. For raw features and
feature transmations, we should allow generic types. -Xiangrui

On Mon, May 5, 2014 at 3:40 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> David,
>
> Could we use Int, Long, Float as the data feature spaces, and Double for
> optimizer?
>
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Mon, May 5, 2014 at 3:06 PM, David Hall <dlwh@cs.berkeley.edu> wrote:
>
>> Lbfgs and other optimizers would not work immediately, as they require
>> vector spaces over double. Otherwise it should work.
>> On May 5, 2014 3:03 PM, "DB Tsai" <dbtsai@stanford.edu> wrote:
>>
>> > Breeze could take any type (Int, Long, Double, and Float) in the matrix
>> > template.
>> >
>> >
>> > Sincerely,
>> >
>> > DB Tsai
>> > -------------------------------------------------------
>> > My Blog: https://www.dbtsai.com
>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>> >
>> >
>> > On Mon, May 5, 2014 at 2:56 PM, Debasish Das <debasish.das83@gmail.com
>> > >wrote:
>> >
>> > > Is this a breeze issue or breeze can take templates on float / double ?
>> > >
>> > > If breeze can take templates then it is a minor fix for Vectors.scala
>> > right
>> > > ?
>> > >
>> > > Thanks.
>> > > Deb
>> > >
>> > >
>> > > On Mon, May 5, 2014 at 2:45 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>> > >
>> > > > +1  Would be nice that we can use different type in Vector.
>> > > >
>> > > >
>> > > > Sincerely,
>> > > >
>> > > > DB Tsai
>> > > > -------------------------------------------------------
>> > > > My Blog: https://www.dbtsai.com
>> > > > LinkedIn: https://www.linkedin.com/in/dbtsai
>> > > >
>> > > >
>> > > > On Mon, May 5, 2014 at 2:41 PM, Debasish Das <
>> debasish.das83@gmail.com
>> > > > >wrote:
>> > > >
>> > > > > Hi,
>> > > > >
>> > > > > Why mllib vector is using double as default ?
>> > > > >
>> > > > > /**
>> > > > >
>> > > > >  * Represents a numeric vector, whose index type is Int and value
>> > type
>> > > is
>> > > > > Double.
>> > > > >
>> > > > >  */
>> > > > >
>> > > > > trait Vector extends Serializable {
>> > > > >
>> > > > >
>> > > > >   /**
>> > > > >
>> > > > >    * Size of the vector.
>> > > > >
>> > > > >    */
>> > > > >
>> > > > >   def size: Int
>> > > > >
>> > > > >
>> > > > >   /**
>> > > > >
>> > > > >    * Converts the instance to a double array.
>> > > > >
>> > > > >    */
>> > > > >
>> > > > >   def toArray: Array[Double]
>> > > > >
>> > > > > Don't we need a template on float/double ? This will give us memory
>> > > > > savings...
>> > > > >
>> > > > > Thanks.
>> > > > >
>> > > > > Deb
>> > > > >
>> > > >
>> > >
>> >
>>

From dev-return-7524-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May  5 23:06:21 2014
Return-Path: <dev-return-7524-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D376A11E0E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  5 May 2014 23:06:21 +0000 (UTC)
Received: (qmail 14916 invoked by uid 500); 5 May 2014 23:06:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14843 invoked by uid 500); 5 May 2014 23:06:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14760 invoked by uid 99); 5 May 2014 23:06:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 23:06:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of david.lw.hall@gmail.com designates 74.125.82.177 as permitted sender)
Received: from [74.125.82.177] (HELO mail-we0-f177.google.com) (74.125.82.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 05 May 2014 23:06:12 +0000
Received: by mail-we0-f177.google.com with SMTP id x48so2903088wes.36
        for <dev@spark.apache.org>; Mon, 05 May 2014 16:05:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=ynQk0KQlkKc/w9+NLezp767ASVsbWNUHqfpJQk3qnDU=;
        b=moAoKTyNpLBKXwCN9/DmnKToH4EhxCu77CsI23urfQql1YzEwOIJsQTuCazxWvs24q
         BvDjyOBzuwodeAoEn01HF4yQqAgqR0Rl5qS4HRxX23Hx518xfwi+evnnDO0DkmDUx6xP
         tYOw4/j1yeuDSL3cNqt1nJsuofqDSZq640ZfVggmsUu/6qVeaYJqHjn1A5sdRh33RP4K
         yumB/TjzllsIQBxYcZV2DCqHLfLDZZknPoNoM/oSmw9o6bihXNkb7RDTFmEcydRSJCvo
         rL02CQh9Y6LcH7S++iJhAClFvUP5hFroYRWtSSNecpNAaasc2fF9xc0jKmHFHXSi3+CZ
         QA2A==
MIME-Version: 1.0
X-Received: by 10.180.7.198 with SMTP id l6mr18153593wia.52.1399331149505;
 Mon, 05 May 2014 16:05:49 -0700 (PDT)
Sender: david.lw.hall@gmail.com
Received: by 10.227.232.207 with HTTP; Mon, 5 May 2014 16:05:49 -0700 (PDT)
In-Reply-To: <CAEYYnxYo=9JwQpDQiNRvMp3n_tJLUOwWv_Mz28-P9VxW5QHAdg@mail.gmail.com>
References: <CA+B-+fzM9sxc+F7hxoWMo6hM6u64gMJMQSENAt7+GXRdTUeAfg@mail.gmail.com>
	<CAEYYnxYGGeTCtzGjN3+VRsRwB=ni7izCA21MXudn0Aq=kEaxtw@mail.gmail.com>
	<CA+B-+fxSfun1gzWTcm+m1jSyyxi0-CDjE4TkMY_0Vb2WkKAowA@mail.gmail.com>
	<CAEYYnxZMnAiBwNW8O7_moMbX8hBaZO1Gw=TY2gcMOhJrRHVb9Q@mail.gmail.com>
	<CALW2ey1E1tE9gsbs4ftv8J1JQseANMaUt-muGCK=KG3UDLRC7Q@mail.gmail.com>
	<CAEYYnxYo=9JwQpDQiNRvMp3n_tJLUOwWv_Mz28-P9VxW5QHAdg@mail.gmail.com>
Date: Mon, 5 May 2014 16:05:49 -0700
X-Google-Sender-Auth: R0olwEo84h4O_vsr_0LrRUh5ko0
Message-ID: <CALW2ey2JBUY1VwTNHV4rirYyF0swcfwQamW2y--NJL5axo31cw@mail.gmail.com>
Subject: Re: mllib vector templates
From: David Hall <dlwh@cs.berkeley.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d044287c016ecf504f8af2cee
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d044287c016ecf504f8af2cee
Content-Type: text/plain; charset=UTF-8

On Mon, May 5, 2014 at 3:40 PM, DB Tsai <dbtsai@stanford.edu> wrote:

> David,
>
> Could we use Int, Long, Float as the data feature spaces, and Double for
> optimizer?
>

Yes. Breeze doesn't allow operations on mixed types, so you'd need to
convert the double vectors to Floats if you wanted, e.g. dot product with
the weights vector.

You might also be interested in FeatureVector, which is just a wrapper
around Array[Int] that emulates an indicator vector. It supports dot
products, axpy, etc.

-- David


>
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Mon, May 5, 2014 at 3:06 PM, David Hall <dlwh@cs.berkeley.edu> wrote:
>
> > Lbfgs and other optimizers would not work immediately, as they require
> > vector spaces over double. Otherwise it should work.
> > On May 5, 2014 3:03 PM, "DB Tsai" <dbtsai@stanford.edu> wrote:
> >
> > > Breeze could take any type (Int, Long, Double, and Float) in the matrix
> > > template.
> > >
> > >
> > > Sincerely,
> > >
> > > DB Tsai
> > > -------------------------------------------------------
> > > My Blog: https://www.dbtsai.com
> > > LinkedIn: https://www.linkedin.com/in/dbtsai
> > >
> > >
> > > On Mon, May 5, 2014 at 2:56 PM, Debasish Das <debasish.das83@gmail.com
> > > >wrote:
> > >
> > > > Is this a breeze issue or breeze can take templates on float /
> double ?
> > > >
> > > > If breeze can take templates then it is a minor fix for Vectors.scala
> > > right
> > > > ?
> > > >
> > > > Thanks.
> > > > Deb
> > > >
> > > >
> > > > On Mon, May 5, 2014 at 2:45 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> > > >
> > > > > +1  Would be nice that we can use different type in Vector.
> > > > >
> > > > >
> > > > > Sincerely,
> > > > >
> > > > > DB Tsai
> > > > > -------------------------------------------------------
> > > > > My Blog: https://www.dbtsai.com
> > > > > LinkedIn: https://www.linkedin.com/in/dbtsai
> > > > >
> > > > >
> > > > > On Mon, May 5, 2014 at 2:41 PM, Debasish Das <
> > debasish.das83@gmail.com
> > > > > >wrote:
> > > > >
> > > > > > Hi,
> > > > > >
> > > > > > Why mllib vector is using double as default ?
> > > > > >
> > > > > > /**
> > > > > >
> > > > > >  * Represents a numeric vector, whose index type is Int and value
> > > type
> > > > is
> > > > > > Double.
> > > > > >
> > > > > >  */
> > > > > >
> > > > > > trait Vector extends Serializable {
> > > > > >
> > > > > >
> > > > > >   /**
> > > > > >
> > > > > >    * Size of the vector.
> > > > > >
> > > > > >    */
> > > > > >
> > > > > >   def size: Int
> > > > > >
> > > > > >
> > > > > >   /**
> > > > > >
> > > > > >    * Converts the instance to a double array.
> > > > > >
> > > > > >    */
> > > > > >
> > > > > >   def toArray: Array[Double]
> > > > > >
> > > > > > Don't we need a template on float/double ? This will give us
> memory
> > > > > > savings...
> > > > > >
> > > > > > Thanks.
> > > > > >
> > > > > > Deb
> > > > > >
> > > > >
> > > >
> > >
> >
>

--f46d044287c016ecf504f8af2cee--

From dev-return-7525-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May  6 05:17:50 2014
Return-Path: <dev-return-7525-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BDF7E1160D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  6 May 2014 05:17:50 +0000 (UTC)
Received: (qmail 55688 invoked by uid 500); 6 May 2014 05:17:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55440 invoked by uid 500); 6 May 2014 05:17:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55417 invoked by uid 99); 6 May 2014 05:17:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 06 May 2014 05:17:36 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of scrapcodes@gmail.com designates 209.85.128.182 as permitted sender)
Received: from [209.85.128.182] (HELO mail-ve0-f182.google.com) (209.85.128.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 06 May 2014 05:17:32 +0000
Received: by mail-ve0-f182.google.com with SMTP id sa20so3781239veb.27
        for <dev@spark.apache.org>; Mon, 05 May 2014 22:17:09 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=KC83//qo5USLTWSEBK/2i7yO5n867yoDjEAEB7UV3rc=;
        b=tMqKmRxjL58ceClqIyQkzl+79T189F1r9XL8k7ngKlc3HcQ2xV5ZGmkJ9UhGxoYWfu
         sxeF3jR/S0TVA3AzQhUuzkWZiNE/WW3dDkjdcWw+ORD5A2ZZqRQy/LxU8JmtgIT1lTrQ
         n+qMgN5y6960+4Nvu4ctUR6zP5aFRjU/9B2pWDhmubXLCIEo34zI1CC389DCTZ3+qg5x
         JpBI08KtPQXTggEBgk8KK6VaJKs55O8OWeXNdFuoQx7OKAxv9xn2UV6II9ELx3smnpYi
         wuWo+XvaBV2dm4cDN9BbuJYg4hnB2aCusBpGGxwLbUcJv4FgWkK3Um/ab7oaQSK9ithg
         XNZw==
X-Received: by 10.58.116.175 with SMTP id jx15mr25148514veb.9.1399353429149;
 Mon, 05 May 2014 22:17:09 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.19.164 with HTTP; Mon, 5 May 2014 22:16:49 -0700 (PDT)
In-Reply-To: <1399305886301-6490.post@n3.nabble.com>
References: <1399279248689-6487.post@n3.nabble.com> <CAOYDGoAZ28jZBMia-mr3zEYJBHLDhDjPQCQSc2jtudYY_YnUvA@mail.gmail.com>
 <1399305886301-6490.post@n3.nabble.com>
From: Prashant Sharma <scrapcodes@gmail.com>
Date: Tue, 6 May 2014 10:46:49 +0530
Message-ID: <CAOYDGoC+9QiKGD=TTdoo5rNgNOEX61XpHEjH1E+2wmx-OCtexg@mail.gmail.com>
Subject: Re: Apache spark on 27gb wikipedia data
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b6dcbf60f4ded04f8b45cea
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6dcbf60f4ded04f8b45cea
Content-Type: text/plain; charset=UTF-8

Try tuning the options like memoryFraction and executorMemory found here :
http://spark.apache.org/docs/latest/configuration.html.

Thanks

Prashant Sharma


On Mon, May 5, 2014 at 9:34 PM, Ajay Nair <prodigyaj@gmail.com> wrote:

> Hi,
>
> Is there any way to overcome this error? I am running this from the
> spark-shell, is that the cause of concern ?
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Apache-spark-on-27gb-wikipedia-data-tp6487p6490.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--047d7b6dcbf60f4ded04f8b45cea--

From dev-return-7526-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May  6 05:54:04 2014
Return-Path: <dev-return-7526-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 00B971173D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  6 May 2014 05:54:04 +0000 (UTC)
Received: (qmail 6613 invoked by uid 500); 6 May 2014 05:53:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6534 invoked by uid 500); 6 May 2014 05:53:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6396 invoked by uid 99); 6 May 2014 05:53:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 06 May 2014 05:53:56 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of prabsmails@gmail.com designates 209.85.128.175 as permitted sender)
Received: from [209.85.128.175] (HELO mail-ve0-f175.google.com) (209.85.128.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 06 May 2014 05:53:51 +0000
Received: by mail-ve0-f175.google.com with SMTP id jw12so3134967veb.6
        for <multiple recipients>; Mon, 05 May 2014 22:53:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=hjnVZZg1Ar2D+7GU8O2g4c3lY9358f7VHaWPu29CVaI=;
        b=wZe5c5PH3nPCrvAx6CnbmDa1h6cnRr4OrCD3QAAVB97gIToBeVUATc5q9/XelbolOE
         cRP7TSarJzqPp/w9RBhnx9KrfW5VRWBRvFVoaRYn9SvV3Wvf1saSycdecNqSHvIo2+1f
         gMqzFrbmtqF93XE+d/MuKnstrQdU++68BoP+Gfq85Ui213BplR8Y/CsW9xoOqWDy3CLJ
         JOtyEITxx+mQzAgYcNnStBkTk+Vj6EO8BwvUtqa5bqs9Yz8qDlKWM1Ie6mOmm/cXLgo4
         xRtwnDlM17fsgbR2n1FIeENYXFaeZx2MzOK83nwOrUG0ZVpYKSCV9/t2MHhvuwMDD7wy
         Uvrw==
X-Received: by 10.58.96.36 with SMTP id dp4mr31961196veb.21.1399355611084;
 Mon, 05 May 2014 22:53:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.135.197 with HTTP; Mon, 5 May 2014 22:52:50 -0700 (PDT)
From: prabeesh k <prabsmails@gmail.com>
Date: Tue, 6 May 2014 11:22:50 +0530
Message-ID: <CAPdPcW1aL5i2MkmUbzR0RJNX3joWByKSCJTV=Kr2ktE+G+RLUQ@mail.gmail.com>
Subject: Better option to use Querying in Spark
To: dev <dev@spark.apache.org>, user@spark.apache.org
Content-Type: multipart/alternative; boundary=089e013a26e61d1fa804f8b4debb
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013a26e61d1fa804f8b4debb
Content-Type: text/plain; charset=UTF-8

Hi,

I have seen three different ways to query data from Spark

   1. Default SQL support(
   https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/sql/examples/HiveFromSpark.scala
   )
   2. Shark
   3. Blink DB

I would like know which one is more efficient

Regards.
prabeesh

--089e013a26e61d1fa804f8b4debb--

From dev-return-7527-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May  6 06:15:50 2014
Return-Path: <dev-return-7527-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 48677117B7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  6 May 2014 06:15:50 +0000 (UTC)
Received: (qmail 31794 invoked by uid 500); 6 May 2014 06:15:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31333 invoked by uid 500); 6 May 2014 06:15:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31310 invoked by uid 99); 6 May 2014 06:15:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 06 May 2014 06:15:28 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mayur.rustagi@gmail.com designates 209.85.212.180 as permitted sender)
Received: from [209.85.212.180] (HELO mail-wi0-f180.google.com) (209.85.212.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 06 May 2014 06:15:23 +0000
Received: by mail-wi0-f180.google.com with SMTP id hi2so320548wib.7
        for <multiple recipients>; Mon, 05 May 2014 23:15:02 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=LxX7hnFj38Q++R1ZdVT4yZ9dQW5nAyK17Am5ee0Lu1k=;
        b=XP9bs4WP21TJHYBeyu0FprR9T+hNxfv5qw1o8VA+qbCwbXn+BGW66iQhRG2+UX0MdO
         8Kkqz+H0h5s9PpIUgC2DikLb7qnC4QeEJh1r8Xh8RwNP6YP8DYK2OYuXf5wxqttt0BQ4
         G3lZPcwmVS6F3u69Y8Aqn5d3rPswSN2/nJpAdPlsjsiCTPN54gm67wLOLlVx5J9R8uH8
         g0HTkRF9gm8eesgPvOhmGysIbV6sS80kIBSBmoKOkQTZmG0i4nz9dELzFlfmUhZ/cNMX
         8+xq09QvU6TIemQgDa2KF+xKW84H57WM8odB0lIiuLt1k8QEI0UQSBkhGj2w42H1G4sz
         XPYw==
X-Received: by 10.180.96.225 with SMTP id dv1mr19321488wib.37.1399356902203;
 Mon, 05 May 2014 23:15:02 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.195.17.165 with HTTP; Mon, 5 May 2014 23:14:42 -0700 (PDT)
In-Reply-To: <CAPdPcW1aL5i2MkmUbzR0RJNX3joWByKSCJTV=Kr2ktE+G+RLUQ@mail.gmail.com>
References: <CAPdPcW1aL5i2MkmUbzR0RJNX3joWByKSCJTV=Kr2ktE+G+RLUQ@mail.gmail.com>
From: Mayur Rustagi <mayur.rustagi@gmail.com>
Date: Tue, 6 May 2014 11:44:42 +0530
Message-ID: <CAAqHKj54srhQ2rD7_C4uGA2OQ2Mqp4ZputS3GcvHxzTSE7B0Wg@mail.gmail.com>
Subject: Re: Better option to use Querying in Spark
To: dev <dev@spark.apache.org>
Cc: user@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d04428a6c11e8c504f8b52b3d
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04428a6c11e8c504f8b52b3d
Content-Type: text/plain; charset=UTF-8

All three have different usecases. If you are looking for more of a
warehouse you are better off with Shark.
SparkSQL is a way to query regular data in sql like syntax leveraging
columnar store.

BlinkDB is a experiment, meant to integrate with Shark in the long term.
Not meant for production usecase directly.


Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>



On Tue, May 6, 2014 at 11:22 AM, prabeesh k <prabsmails@gmail.com> wrote:

> Hi,
>
> I have seen three different ways to query data from Spark
>
>    1. Default SQL support(
>
> https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/sql/examples/HiveFromSpark.scala
>    )
>    2. Shark
>    3. Blink DB
>
> I would like know which one is more efficient
>
> Regards.
> prabeesh
>

--f46d04428a6c11e8c504f8b52b3d--

From dev-return-7512-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 10 23:07:58 2014
Return-Path: <dev-return-7512-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0BD0E12508
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 10 May 2014 23:07:58 +0000 (UTC)
Received: (qmail 87049 invoked by uid 500); 10 May 2014 22:40:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86227 invoked by uid 500); 10 May 2014 22:40:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83385 invoked by uid 99); 10 May 2014 22:39:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 10 May 2014 22:39:41 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=HTML_IMAGE_ONLY_16,HTML_IMAGE_RATIO_06,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of usman@platfora.com designates 209.85.212.169 as permitted sender)
Received: from [209.85.212.169] (HELO mail-wi0-f169.google.com) (209.85.212.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 10 May 2014 08:15:45 +0000
Received: by mail-wi0-f169.google.com with SMTP id hi2so3470023wib.2
        for <dev@spark.apache.org>; Sat, 10 May 2014 01:15:23 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=w3pdUNylCOWvsFpYXopNomidURFG7NZqc11femK4Eeo=;
        b=kQhJ4XJSbLY38LasZnw4dvwxdvmZrcAYrf8y8XYOi/bIRESNZoCcYJPP7JpJ0zM48c
         HgDiyLGCVmph9s7IqrX5C0SZq56cEv2YRD0iUNaUrkRoL3BRw2Q67t9F09AwrObzn1Mq
         zEfSvjcS6NrSSyTMTrbs1YGsRf9QITqZBoYWifd6WnGaXg9gr+m/q5UCsHKiwmN2nkPh
         Nvnn9mYXbxS5OCWSoQwfHfXAFIds2htkBfEfS5Gv+ZWtcvOYE/VLXHDccuRYUrDlutcu
         usvfnNriNJn2sD9LmvLM3UjTfZHuy03aeQUYP0967FvYYVY+KBP5K10/QKQ6vRrq8eQB
         YC1Q==
X-Gm-Message-State: ALoCoQlchPrg/tR7Ezq6WCT6Cr1qJuLHSU5EzH2e5W3X3JAjnsi91pTX2G2xnCqsKStq64lR4eq3
MIME-Version: 1.0
X-Received: by 10.180.91.114 with SMTP id cd18mr6883498wib.28.1399709723690;
 Sat, 10 May 2014 01:15:23 -0700 (PDT)
Received: by 10.216.241.130 with HTTP; Sat, 10 May 2014 01:15:23 -0700 (PDT)
Date: Sat, 10 May 2014 01:15:23 -0700
Message-ID: <CAG_e43wd_6Rr==pGz0KFEoyzJZdEkuM4tzsWHONsq8H7Y05xAg@mail.gmail.com>
Subject: Using spark 1.0.0 assembly in IntelliJ
From: Usman Ghani <usman@platfora.com>
To: dev@spark.apache.org
Content-Type: multipart/related; boundary=f46d043749addebf7404f9075035
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043749addebf7404f9075035
Content-Type: multipart/alternative; boundary=f46d043749addebf7004f9075034

--f46d043749addebf7004f9075034
Content-Type: text/plain; charset=UTF-8

*(Includes solution)*

I was having this weird issue where when I use a Spark 0.9.1 or earlier Jar
file in IntelliJ I can see its contents and IntelliJ can work with it. But
when I compile version 1.0.0 from source my jar is not being recognized by
IntelliJ even though I can build my project using ant from the command
line.

*Snapshot attached.*

[image: Inline image 1]

Basically IntelliJ on Mac always uses Apple JDK 1.6 to run. You have to
manually change it to run under 1.7 so it can look inside a 1.7 jar
compiled to 1.7 byte code. In case you run into this, use this link to
change the JVM under which intelliJ runs.

https://intellij-support.jetbrains.com/entries/23455956-Selecting-the-JDK-version-the-IDE-will-run-under

--f46d043749addebf7004f9075034
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div class=3D"gmail_quote"><i>(Includes solution)</i><br><=
div dir=3D"ltr"><div><br></div><div>I was having this weird issue where whe=
n I use a Spark 0.9.1 or earlier Jar file in IntelliJ I can see its content=
s and IntelliJ can work with it. But when I compile version 1.0.0 from sour=
ce my jar is not being recognized by IntelliJ even though I can build my pr=
oject using ant from the command line.=C2=A0</div>

<div><br></div><div><b>Snapshot attached.</b></div><div><br></div><div><img=
 src=3D"cid:ii_145e3d99ffa44c1e" alt=3D"Inline image 1" width=3D"482" heigh=
t=3D"256"><br></div><div><br></div><div><span style=3D"font-family:arial,sa=
ns-serif;font-size:13px">Basically IntelliJ on Mac always uses Apple JDK 1.=
6 to run. You have to manually change it to run under 1.7 so it can look in=
side a 1.7 jar compiled to 1.7 byte code. In case you run into this, use th=
is link to change the JVM under which intelliJ runs.</span><div style=3D"fo=
nt-family:arial,sans-serif;font-size:13px">
<br><div><a href=3D"https://intellij-support.jetbrains.com/entries/23455956=
-Selecting-the-JDK-version-the-IDE-will-run-under" target=3D"_blank">https:=
//intellij-support.jetbrains.com/entries/23455956-Selecting-the-JDK-version=
-the-IDE-will-run-under</a></div>
</div></div>
</div>
</div><br><div><br></div></div>

--f46d043749addebf7004f9075034--
--f46d043749addebf7404f9075035--

From dev-return-7522-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 11 03:02:32 2014
Return-Path: <dev-return-7522-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A938A1120F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 11 May 2014 03:02:32 +0000 (UTC)
Received: (qmail 47779 invoked by uid 500); 11 May 2014 03:02:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47696 invoked by uid 500); 11 May 2014 03:02:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47681 invoked by uid 99); 11 May 2014 03:02:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 11 May 2014 03:02:30 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of malouf.gary@gmail.com designates 209.85.216.169 as permitted sender)
Received: from [209.85.216.169] (HELO mail-qc0-f169.google.com) (209.85.216.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 11 May 2014 03:02:26 +0000
Received: by mail-qc0-f169.google.com with SMTP id e16so6399636qcx.28
        for <dev@spark.apache.org>; Sat, 10 May 2014 20:02:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=0gh/hsLAc1Y705nvHYDoHKa+4vLwsJPsi+vkUmO+qEQ=;
        b=KstAS2mZ7EtdAg5P5psLgo11h8ZNbRwk4O6hqKqNp8S3LwfAcor9toKm56Dm51ls9W
         X5vyQiry59Eb0eUt2posQcNfPMq4nXEZt4S2Yi26G7HFBauRDvF42Eea/JGYLwqYrPEl
         4FScYLFZJu+aX2+IvA3SfczSfsHjDggpYYgAFdSHsHKl/TAO3LCcIzUp8aaWIJDuq4Qe
         xlw6qowANxxMqiaQqC8TVbDXHPeX5qTiYU+Z46lnZ4Dd6Ji0naNTRgvoGDfDcTuSNxSA
         WZxhgRDjU4SYCDlxE+6sV4gExI38a8PnQdflcL2g53g+mToXnakTPJ49k7aCoHZmCYjS
         SLAg==
MIME-Version: 1.0
X-Received: by 10.224.55.6 with SMTP id s6mr26834259qag.7.1399777325507; Sat,
 10 May 2014 20:02:05 -0700 (PDT)
Received: by 10.140.23.85 with HTTP; Sat, 10 May 2014 20:02:05 -0700 (PDT)
In-Reply-To: <CAFboF2xJThe+5Jf8eAfvMMRmuUJW=ufXUoOn8XUMrKhktbE2cQ@mail.gmail.com>
References: <CAFboF2xJThe+5Jf8eAfvMMRmuUJW=ufXUoOn8XUMrKhktbE2cQ@mail.gmail.com>
Date: Sat, 10 May 2014 23:02:05 -0400
Message-ID: <CAGOvqiptNMhXrS4jLykYYO6Rzr8=jOQ67xmVUKU9nq3DTKKW5g@mail.gmail.com>
Subject: Re: Spark on Scala 2.11
From: Gary Malouf <malouf.gary@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0153705e4068b304f9170ec1
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0153705e4068b304f9170ec1
Content-Type: text/plain; charset=UTF-8

Considering the team just bumped to 2.10 in 0.9, I would be surprised if
this is a near term priority.


On Thu, May 8, 2014 at 9:33 PM, Anand Avati <avati@gluster.org> wrote:

> Is there an ongoing effort (or intent) to support Spark on Scala 2.11?
> Approximate timeline?
>
> Thanks
>

--089e0153705e4068b304f9170ec1--

From dev-return-7523-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 11 04:03:50 2014
Return-Path: <dev-return-7523-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A3FE311473
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 11 May 2014 04:03:50 +0000 (UTC)
Received: (qmail 31338 invoked by uid 500); 11 May 2014 03:35:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31058 invoked by uid 500); 11 May 2014 03:35:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30836 invoked by uid 99); 11 May 2014 03:35:48 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 11 May 2014 03:35:48 +0000
X-ASF-Spam-Status: No, hits=1.8 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_QUOTING
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.49 as permitted sender)
Received: from [209.85.216.49] (HELO mail-qa0-f49.google.com) (209.85.216.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 11 May 2014 03:35:44 +0000
Received: by mail-qa0-f49.google.com with SMTP id cm18so5663346qab.36
        for <dev@spark.apache.org>; Sat, 10 May 2014 20:35:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=UN0DecPd+F6F3Fgfh9lDSW5mVnt6HAox3O7EJAfw0Y0=;
        b=aeNm9IEtLTBA8z6zGPC10FkvcE1vKp7mkNP+ZR/yu3h/fzzQlwED3J4Q3qdmDL/ggf
         BKnC/G1eIxjUW9qjuXvHVyd1fLRsIik5tjSULb6KPJUr+zo5LLrSsswWlDxNB1Oz7AN+
         BIPszURogPVKAOgko4P+f/6q+j4O8RbHW6UCX5X+tXdcKBW3gn4BoXV+iDRe4M72QO07
         KQ9UUZjcZ63WPkcZvEWmYKgqRwyVdLW1H1CYC0vsY3qcIvy2WxgNPmQbCBILtcG+aBPq
         uC3bT5c/KBJ/D1an4h41T0gSqgrrRCpr5J+GosDlkAUrpJdCzy6nk3F9nGSNcZ/dWra9
         5MCQ==
MIME-Version: 1.0
X-Received: by 10.224.115.68 with SMTP id h4mr26978825qaq.35.1399779321319;
 Sat, 10 May 2014 20:35:21 -0700 (PDT)
Received: by 10.140.82.164 with HTTP; Sat, 10 May 2014 20:35:21 -0700 (PDT)
Date: Sat, 10 May 2014 20:35:21 -0700
Message-ID: <CA+B-+fzJMdScWmZWZct=Ds9J2P7uyzjxTnoMXEjBCxXNDukJ9g@mail.gmail.com>
Subject: LabeledPoint toString to dump LibSvm if SparseVector
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bdc7fc636151904f9178559
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc7fc636151904f9178559
Content-Type: text/plain; charset=UTF-8

Hi,

I need to change the toString on LabeledPoint to libsvm format so that I
can dump RDD[LabeledPoint] as a format that could be read by sparse
glmnet-R and other packages to benchmark mllib classification accuracy...

Basically I have to change the toString of LabeledPoint and toString of
SparseVector....

Should I add it as a PR or is it already being added ?

I added these functions toLibSvm in my internal util class for now...

def toLibSvm(labelPoint: LabeledPoint): String = {

    labelPoint.label.toString + " " + toLibSvm(labelPoint.features
.asInstanceOf[SparseVector])

  }

  def toLibSvm(features: SparseVector): String = {

    val indices = features.indices

    val values = features.values

    indices.zip(values).mkString(" ").replace(',', ':').replace("(", ""
).replace(")","")

  }
Thanks.
Deb



On Fri, May 9, 2014 at 10:09 PM, mateiz <git@git.apache.org> wrote:

> Github user mateiz commented on a diff in the pull request:
>
>     https://github.com/apache/spark/pull/685#discussion_r12502569
>
>     --- Diff:
> mllib/src/test/scala/org/apache/spark/mllib/linalg/VectorsSuite.scala ---
>     @@ -100,4 +100,27 @@ class VectorsSuite extends FunSuite {
>          assert(vec2(6) === 4.0)
>          assert(vec2(7) === 0.0)
>        }
>     +
>     +  test("parse vectors") {
>     +    val vectors = Seq(
>     +      Vectors.dense(Array.empty[Double]),
>     +      Vectors.dense(1.0),
>     +      Vectors.dense(1.0, 0.0, -2.0),
>     +      Vectors.sparse(0, Array.empty[Int], Array.empty[Double]),
>     +      Vectors.sparse(1, Array(0), Array(1.0)),
>     +      Vectors.sparse(3, Array(0, 2), Array(1.0, -2.0)))
>     +    vectors.foreach { v =>
>     +      val v1 = Vectors.parse(v.toString)
>     +      assert(v.getClass === v1.getClass)
>     +      assert(v === v1)
>     +    }
>     +
>     +    val malformatted = Seq("1", "[1,,]", "[1,2", "(1,[1,2])",
> "(1,[1],[2.0,1.0])")
>     +    malformatted.foreach { s =>
>     +      intercept[RuntimeException] {
>     --- End diff --
>
>     Should be Exception instead
>
>
> ---
> If your project is set up for it, you can reply to this email and have your
> reply appear on GitHub as well. If your project does not have this feature
> enabled and wishes so, or if the feature is enabled but not working, please
> contact infrastructure at infrastructure@apache.org or file a JIRA ticket
> with INFRA.
> ---
>

--047d7bdc7fc636151904f9178559--

From dev-return-7518-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 11 04:44:30 2014
Return-Path: <dev-return-7518-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0CEFC1166A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 11 May 2014 04:44:30 +0000 (UTC)
Received: (qmail 24651 invoked by uid 500); 10 May 2014 23:21:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28829 invoked by uid 500); 10 May 2014 23:05:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83391 invoked by uid 99); 10 May 2014 22:55:17 -0000
Received: from Unknown (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 10 May 2014 22:55:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.173] (HELO mail-vc0-f173.google.com) (209.85.220.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 May 2014 07:52:19 +0000
Received: by mail-vc0-f173.google.com with SMTP id il7so2761826vcb.4
        for <dev@spark.apache.org>; Thu, 08 May 2014 00:51:58 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=DUZ9hL+ej6ofL0KTGtcOOXuWGpYiWOOd/H9PMg5tsu4=;
        b=HZextAQPpDCnER/AOFx+Tv39nBIIBWD2hsSkMvIzpA2lftYD6jG9ltrAOCwyTdJ8J0
         ZVZ7+8an/bMD6JTJLK4fpXmsKlaOBmDL3sz0W0/KsTE9ahXZ5glVVmtaA8KoL0Y8Yi8D
         KqmytxTmFHaB5gl7EYNzOYGWX55qCvw75wGgHV03Wg3JV1BMbTYRrk+rCtXNRBvd9n59
         CeBFk5ApwOpyRHHZ/1Atxw6eDGo9L6cWdifn+37oK2GJ7yJ5waK8owwHd0seL+zx/RrX
         mDpAREgX6T+b8QAsT4qqorPlWLa6Yt+e4hdiXmCKXKg4oAKDACurYrigwY3ZFCWNenKP
         D3KQ==
X-Gm-Message-State: ALoCoQkjkhiMk7k/3sc5ksM/HoeNy+fZSBh2omdYBF1wpbfP9dGShCJ3ycQiW0ER+bebqILTn8EH
X-Received: by 10.52.37.48 with SMTP id v16mr1608041vdj.4.1399535518396;
        Thu, 08 May 2014 00:51:58 -0700 (PDT)
Received: from mail-vc0-f178.google.com (mail-vc0-f178.google.com [209.85.220.178])
        by mx.google.com with ESMTPSA id sb8sm348872vdc.26.2014.05.08.00.51.57
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 08 May 2014 00:51:57 -0700 (PDT)
Received: by mail-vc0-f178.google.com with SMTP id hu19so2793568vcb.37
        for <dev@spark.apache.org>; Thu, 08 May 2014 00:51:56 -0700 (PDT)
X-Received: by 10.58.66.195 with SMTP id h3mr133947vet.57.1399535516699; Thu,
 08 May 2014 00:51:56 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Thu, 8 May 2014 00:51:36 -0700 (PDT)
From: Andrew Ash <andrew@andrewash.com>
Date: Thu, 8 May 2014 00:51:36 -0700
Message-ID: <CA+-p3AGGAKA05e9MpvOgQpmj=opOgvw+zXVZVojYU8Yo9XZ4tg@mail.gmail.com>
Subject: Updating docs for running on Mesos
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b33934752d48704f8dec109
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b33934752d48704f8dec109
Content-Type: text/plain; charset=UTF-8

The docs for how to run Spark on Mesos have changed very little since
0.6.0, but setting it up is much easier now than then.  Does it make sense
to revamp with the below changes?


You no longer need to build mesos yourself as pre-built versions are
available from Mesosphere: http://mesosphere.io/downloads/

And the instructions guide you towards compiling your own distribution of
Spark, when you can use the prebuilt versions of Spark as well.


I'd like to split that portion of the documentation into two sections, a
build-from-scratch section and a use-prebuilt section.  The new outline
would look something like this:


*Running Spark on Mesos*

Installing Mesos
- using prebuilt (recommended)
 - pointer to mesosphere's packages
- from scratch
 - (similar to current)


Connecting Spark to Mesos
- loading distribution into an accessible location
- Spark settings

Mesos Run Modes
- (same as current)

Running Alongside Hadoop
- (trim this down)



Does that work for people?


Thanks!
Andrew


PS Basically all the same:

http://spark.apache.org/docs/0.6.0/running-on-mesos.html
http://spark.apache.org/docs/0.6.2/running-on-mesos.html
http://spark.apache.org/docs/0.7.3/running-on-mesos.html
http://spark.apache.org/docs/0.8.1/running-on-mesos.html
http://spark.apache.org/docs/0.9.1/running-on-mesos.html
https://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/running-on-mesos.html

--047d7b33934752d48704f8dec109--

From dev-return-7513-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 11 06:53:34 2014
Return-Path: <dev-return-7513-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 57DE011BF9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 11 May 2014 06:53:34 +0000 (UTC)
Received: (qmail 27436 invoked by uid 500); 10 May 2014 22:51:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14367 invoked by uid 500); 10 May 2014 22:50:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4667 invoked by uid 99); 10 May 2014 22:49:53 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 10 May 2014 22:49:53 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of anand.avati@gmail.com designates 209.85.214.177 as permitted sender)
Received: from [209.85.214.177] (HELO mail-ob0-f177.google.com) (209.85.214.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 May 2014 01:33:54 +0000
Received: by mail-ob0-f177.google.com with SMTP id gq1so3903176obb.8
        for <dev@spark.apache.org>; Thu, 08 May 2014 18:33:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:date:message-id:subject:from:to:content-type;
        bh=j7vq9+NKZDJvk6HH9GaVIX7/4TEKXJS/VAGHmplskPI=;
        b=Xu5UtPvZPIxQHRP7fnAwal8KyZPOqWQft7JqUd8kq+3OcMhN1ilJOKsN6Rt30EPfIf
         +I6J2THXjV6I4ndBqVQY7bADAgHMt7AZ04fR7ATapG0KeNzvkEIVkZHNk2k9KvSxAUse
         yv/BseUXnaqHJVnGUWJHCj8c33xpocgXjqLGVODz0RSPdoourWKXue1Z6XfzIUlS8ezi
         KbTX6brEShMDLWMPze3Fkuc8r80b5xIRaSCsP62KOZuRVG1goA7fG/kv3IH/+/OcY+Fh
         fsvWg1u4EozSom2DNURsrslfNVul0+ETASY6AXxp2DxeWtYs4FKPSR5IZd8PE5v6AzSW
         +otg==
MIME-Version: 1.0
X-Received: by 10.182.243.138 with SMTP id wy10mr108475obc.83.1399599210653;
 Thu, 08 May 2014 18:33:30 -0700 (PDT)
Sender: anand.avati@gmail.com
Received: by 10.60.165.68 with HTTP; Thu, 8 May 2014 18:33:30 -0700 (PDT)
Date: Thu, 8 May 2014 18:33:30 -0700
X-Google-Sender-Auth: bipWdgR3uOYI6TaqSNTA0y6IhFQ
Message-ID: <CAFboF2xJThe+5Jf8eAfvMMRmuUJW=ufXUoOn8XUMrKhktbE2cQ@mail.gmail.com>
Subject: Spark on Scala 2.11
From: Anand Avati <avati@gluster.org>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2a120c76dab04f8ed95a1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2a120c76dab04f8ed95a1
Content-Type: text/plain; charset=UTF-8

Is there an ongoing effort (or intent) to support Spark on Scala 2.11?
Approximate timeline?

Thanks

--001a11c2a120c76dab04f8ed95a1--

From dev-return-7515-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 11 12:41:05 2014
Return-Path: <dev-return-7515-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1C91F11883
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 11 May 2014 12:41:05 +0000 (UTC)
Received: (qmail 87601 invoked by uid 500); 10 May 2014 23:18:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60596 invoked by uid 500); 10 May 2014 23:00:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19149 invoked by uid 99); 10 May 2014 22:57:38 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 10 May 2014 22:57:38 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.219.49] (HELO mail-oa0-f49.google.com) (209.85.219.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 May 2014 16:55:29 +0000
Received: by mail-oa0-f49.google.com with SMTP id eb12so3395240oac.22
        for <dev@spark.apache.org>; Thu, 08 May 2014 09:55:05 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=Ohr0m8w3nNgzUMyIhWKAus33jkFN409bwb18Cbpe8K4=;
        b=JtuunJ/rPxwI4Bp6HhPijVMnEfWiJcDs6hU4O2amE4EaH2puHo7MglpU3xIuVBnLAH
         Pa4a/n1GVbfdUgkIUNfqqyw7diN69BE8FttfRuz+v0ER1N7IBa3j2myfRVDIRQnoHvws
         C/6KpMwmRvMro5EnKSjDBH8aWBnaHJDdC4JAdlpug5tdnZa3rJq/OrSb8KVzN7Lytx9a
         3yo3A/6LJl0hfwMS6+PPBPoVjWaxkmS2H7jFjr/3ORIXkbGaGZn3nijLWNcqYjHKARS2
         1COMBM5h3Qf81IhI9DzzP6FIr9+qD+p8f4FdGlOXzq1nPp3cWpunFntpP0T7DRupfQAZ
         Z49Q==
X-Gm-Message-State: ALoCoQl9K52OC7od8J2E70HUO96iS5jKtxOeuaZGZCDVq7j6vDWXxcCqZemCo3w08uSRkO3MJ/cg
MIME-Version: 1.0
X-Received: by 10.60.132.12 with SMTP id oq12mr6468327oeb.42.1399568105336;
 Thu, 08 May 2014 09:55:05 -0700 (PDT)
Received: by 10.182.184.40 with HTTP; Thu, 8 May 2014 09:55:05 -0700 (PDT)
Date: Thu, 8 May 2014 09:55:05 -0700
Message-ID: <CAJ3iqPQxeOKuWg2gz_zkitVy-8ZEYgCzy9O_5op2F4wh+j6XsA@mail.gmail.com>
Subject: Requirements of objects stored in RDDs
From: Soren Macbeth <soren@yieldbot.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b47286ec2041804f8e657f0
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b47286ec2041804f8e657f0
Content-Type: text/plain; charset=UTF-8

Hi,

What are the requirements of objects that are stored in RDDs?

I'm still struggling with an exception I've already posted about several
times. My questions are:

1) What interfaces are objects stored in RDDs expected to implement, if any?
2) Are collections (be they scala, java or otherwise) handled differently
than other objects?

The bug I'm hitting is when I try to use my clojure DSL (which wraps the
java api) with clojure collections, specifically
clojure.lang.PersistentVectors in my RDDs. Here is the exception message:

org.apache.spark.SparkException: Job aborted: Exception while deserializing
and fetching task: com.esotericsoftware.kryo.KryoException:
java.lang.IllegalArgumentException: Can not set final scala.collecti
on.convert.Wrappers field
scala.collection.convert.Wrappers$SeqWrapper.$outer to
clojure.lang.PersistentVector

Now, this same application works fine in local mode and tests, but it fails
when run under mesos. That would seem to me to point to something around
RDD partitioning for tasks, but I'm not sure.

I don't know much scala, but according to google, SeqWrapper is part of the
implicit JavaConversion functionality of scala collections. Under what
circumstances would spark be trying to wrap my RDD objects in scala
collections?

Finally - I'd like to point out that this is not a serialization issue with
my clojure collection objects. I have registered serializers for them and
have verified they serialize and deserialize perfectly well in spark.

One last note is that this failure occurs after all the tasks for finished
for a reduce stage and the results are returned to the driver.

TIA

--047d7b47286ec2041804f8e657f0--

From dev-return-7524-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 11 16:40:38 2014
Return-Path: <dev-return-7524-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 41CFB11F2D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 11 May 2014 16:40:38 +0000 (UTC)
Received: (qmail 17341 invoked by uid 500); 11 May 2014 16:40:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17279 invoked by uid 500); 11 May 2014 16:40:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17271 invoked by uid 99); 11 May 2014 16:40:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 11 May 2014 16:40:37 +0000
X-ASF-Spam-Status: No, hits=1.8 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_QUOTING
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.172 as permitted sender)
Received: from [209.85.216.172] (HELO mail-qc0-f172.google.com) (209.85.216.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 11 May 2014 16:40:31 +0000
Received: by mail-qc0-f172.google.com with SMTP id l6so6727263qcy.17
        for <dev@spark.incubator.apache.org>; Sun, 11 May 2014 09:40:11 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=JKDriVskO5/XTZP3XjtKqftfSkvbGEa9BezXBNe0rH4=;
        b=vrp5+bVzbTs9HGFqF4u+pO+GV2eDmxLQs77EVA++X6SoQz6VRtYU81e78jbdOPUMEy
         G4LmGB36+g6wzu5bsotB59A9h59U+ZIu+g2Ei6invbgH1he3ZwDFT8iPHOtKlt/VOIiY
         rSmE7HL7ZPhxcui8fXTXBSWFMtQaT5d1NRqs5nRpKU4LswrkynRv3JuMfMxOI0yrylvz
         WRC9UNxi5VkI8O8bP65bEXWh13XebVQQyTi19wB2aZmX2wCpz29p1cc2zy7WZOlaX5wa
         4Ipz2VbLPqvSV8riji69cmAfAdB2k7D1x2r/V6hVp5XKEWmiiuNsIdt3TG88JoWm03Ny
         BAmw==
MIME-Version: 1.0
X-Received: by 10.224.47.8 with SMTP id l8mr31670220qaf.24.1399826411032; Sun,
 11 May 2014 09:40:11 -0700 (PDT)
Received: by 10.140.82.164 with HTTP; Sun, 11 May 2014 09:40:10 -0700 (PDT)
Date: Sun, 11 May 2014 09:40:10 -0700
Message-ID: <CA+B-+fxoNS77N71R3y+8KXFN1DixPNU2Q-+Hwa=50Fzku2GgGw@mail.gmail.com>
Subject: LabeledPoint dump LibSVM if SparseVector
From: Debasish Das <debasish.das83@gmail.com>
To: dev <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2c3c8fa13de04f9227bd1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2c3c8fa13de04f9227bd1
Content-Type: text/plain; charset=UTF-8

Hi,

I need to change the toString on LabeledPoint to libsvm format so that I
can dump RDD[LabeledPoint] as a format that could be read by sparse
glmnet-R and other packages to benchmark mllib classification accuracy...

Basically I have to change the toString of LabeledPoint and toString of
SparseVector....

Should I add it as a PR or is it already being added ?

I added these functions toLibSvm in my internal util class for now...

def toLibSvm(labelPoint: LabeledPoint): String = {

    labelPoint.label.toString + " " +
toLibSvm(labelPoint.features.asInstanceOf[SparseVector])

  }

  def toLibSvm(features: SparseVector): String = {

    val indices = features.indices

    val values = features.values

    indices.zip(values).mkString("
").replace(',', ':').replace("(", "").replace(")","")

  }
Thanks.
Deb

--001a11c2c3c8fa13de04f9227bd1--

From dev-return-7520-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 11 20:56:39 2014
Return-Path: <dev-return-7520-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5E1A31169C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 11 May 2014 20:56:39 +0000 (UTC)
Received: (qmail 63562 invoked by uid 500); 10 May 2014 23:16:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60643 invoked by uid 500); 10 May 2014 23:08:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26119 invoked by uid 99); 10 May 2014 22:58:06 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 10 May 2014 22:58:05 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.219.43 as permitted sender)
Received: from [209.85.219.43] (HELO mail-oa0-f43.google.com) (209.85.219.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 May 2014 05:26:22 +0000
Received: by mail-oa0-f43.google.com with SMTP id l6so2498498oag.16
        for <dev@spark.incubator.apache.org>; Wed, 07 May 2014 22:25:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=MlL0olw+kq7CCOUt2mHYyBxUmaB0a/xFDWbt8uzQz/M=;
        b=OWF5wJXb3TTx7CqGIUEXjFChxv6RJKI3Y4ARrDNQBt4gc/gU5RkRELjT7c+1aBe1Ev
         cWUrvf6sJL+vKDpdU3bY0Y3kLjYm60a9HztybaET2E3HK+oOFZ9zdk3Vg9PIe9T6Q6MJ
         DTkaYarAS8L9RnFlYkc57EHzzcjFb4RXi5pUKrDX3+SXgGBZINNYJIioXpO6aj26+e9R
         zL+jM/Dtjac3Nqkj/iV8j8PCqDWokcuAegPb92gsli3o9VNcXm8F4bNCOqDhMiEayxwP
         MELzkyAW+4HtTd7McRTvZCJqLUoki9ijCPsqbHc21b2ZfWCCH2D0uvhr88aRL93qHcyr
         OCzw==
MIME-Version: 1.0
X-Received: by 10.60.124.227 with SMTP id ml3mr461492oeb.67.1399526758946;
 Wed, 07 May 2014 22:25:58 -0700 (PDT)
Received: by 10.182.246.164 with HTTP; Wed, 7 May 2014 22:25:58 -0700 (PDT)
In-Reply-To: <CA+B-+fx1DOwvzZttLypFBF6qg8M1X48hLL=UP__RVbJYNYCwiw@mail.gmail.com>
References: <CA+B-+fzM9sxc+F7hxoWMo6hM6u64gMJMQSENAt7+GXRdTUeAfg@mail.gmail.com>
	<CAEYYnxYGGeTCtzGjN3+VRsRwB=ni7izCA21MXudn0Aq=kEaxtw@mail.gmail.com>
	<CA+B-+fxSfun1gzWTcm+m1jSyyxi0-CDjE4TkMY_0Vb2WkKAowA@mail.gmail.com>
	<CAEYYnxZMnAiBwNW8O7_moMbX8hBaZO1Gw=TY2gcMOhJrRHVb9Q@mail.gmail.com>
	<CALW2ey1E1tE9gsbs4ftv8J1JQseANMaUt-muGCK=KG3UDLRC7Q@mail.gmail.com>
	<CAEYYnxYo=9JwQpDQiNRvMp3n_tJLUOwWv_Mz28-P9VxW5QHAdg@mail.gmail.com>
	<CALW2ey2JBUY1VwTNHV4rirYyF0swcfwQamW2y--NJL5axo31cw@mail.gmail.com>
	<CA+B-+fx1DOwvzZttLypFBF6qg8M1X48hLL=UP__RVbJYNYCwiw@mail.gmail.com>
Date: Wed, 7 May 2014 22:25:58 -0700
Message-ID: <CA+B-+fxNcFuQmdsQERiddQczHD6LYnKTx4QBV6p_QS=eE13n5g@mail.gmail.com>
Subject: Re: mllib vector templates
From: Debasish Das <debasish.das83@gmail.com>
To: dev <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=047d7b417997521cbc04f8dcb714
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b417997521cbc04f8dcb714
Content-Type: text/plain; charset=UTF-8

Hi,

I see ALS is still using Array[Int] but for other mllib algorithm we moved
to Vector[Double] so that it can support either dense and sparse formats...

ALS can stay in Array[Int] due to the Netflix format for input datasets
which is well defined but it helps if we move ALS to Vector[Double] as
well...that way all algorithms will be consistent...

The second issue is that toString on SparseVector does not write libsvm
format but something not very generic...can we change the
SparseVector.toString to write as libsvm output ? I am dumping a sample of
dataset to see how mllib glm compares with the glmnet-R package for QoR...

Thanks.
Deb

On Mon, May 5, 2014 at 4:05 PM, David Hall <dlwh@cs.berkeley.edu> wrote:
>
>> On Mon, May 5, 2014 at 3:40 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>
>> > David,
>> >
>> > Could we use Int, Long, Float as the data feature spaces, and Double for
>> > optimizer?
>> >
>>
>> Yes. Breeze doesn't allow operations on mixed types, so you'd need to
>> convert the double vectors to Floats if you wanted, e.g. dot product with
>> the weights vector.
>>
>> You might also be interested in FeatureVector, which is just a wrapper
>> around Array[Int] that emulates an indicator vector. It supports dot
>> products, axpy, etc.
>>
>> -- David
>>
>>
>> >
>> >
>> > Sincerely,
>> >
>> > DB Tsai
>> > -------------------------------------------------------
>> > My Blog: https://www.dbtsai.com
>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>> >
>> >
>> > On Mon, May 5, 2014 at 3:06 PM, David Hall <dlwh@cs.berkeley.edu>
>> wrote:
>> >
>> > > Lbfgs and other optimizers would not work immediately, as they require
>> > > vector spaces over double. Otherwise it should work.
>> > > On May 5, 2014 3:03 PM, "DB Tsai" <dbtsai@stanford.edu> wrote:
>> > >
>> > > > Breeze could take any type (Int, Long, Double, and Float) in the
>> matrix
>> > > > template.
>> > > >
>> > > >
>> > > > Sincerely,
>> > > >
>> > > > DB Tsai
>> > > > -------------------------------------------------------
>> > > > My Blog: https://www.dbtsai.com
>> > > > LinkedIn: https://www.linkedin.com/in/dbtsai
>> > > >
>> > > >
>> > > > On Mon, May 5, 2014 at 2:56 PM, Debasish Das <
>> debasish.das83@gmail.com
>> > > > >wrote:
>> > > >
>> > > > > Is this a breeze issue or breeze can take templates on float /
>> > double ?
>> > > > >
>> > > > > If breeze can take templates then it is a minor fix for
>> Vectors.scala
>> > > > right
>> > > > > ?
>> > > > >
>> > > > > Thanks.
>> > > > > Deb
>> > > > >
>> > > > >
>> > > > > On Mon, May 5, 2014 at 2:45 PM, DB Tsai <dbtsai@stanford.edu>
>> wrote:
>> > > > >
>> > > > > > +1  Would be nice that we can use different type in Vector.
>> > > > > >
>> > > > > >
>> > > > > > Sincerely,
>> > > > > >
>> > > > > > DB Tsai
>> > > > > > -------------------------------------------------------
>> > > > > > My Blog: https://www.dbtsai.com
>> > > > > > LinkedIn: https://www.linkedin.com/in/dbtsai
>> > > > > >
>> > > > > >
>> > > > > > On Mon, May 5, 2014 at 2:41 PM, Debasish Das <
>> > > debasish.das83@gmail.com
>> > > > > > >wrote:
>> > > > > >
>> > > > > > > Hi,
>> > > > > > >
>> > > > > > > Why mllib vector is using double as default ?
>> > > > > > >
>> > > > > > > /**
>> > > > > > >
>> > > > > > >  * Represents a numeric vector, whose index type is Int and
>> value
>> > > > type
>> > > > > is
>> > > > > > > Double.
>> > > > > > >
>> > > > > > >  */
>> > > > > > >
>> > > > > > > trait Vector extends Serializable {
>> > > > > > >
>> > > > > > >
>> > > > > > >   /**
>> > > > > > >
>> > > > > > >    * Size of the vector.
>> > > > > > >
>> > > > > > >    */
>> > > > > > >
>> > > > > > >   def size: Int
>> > > > > > >
>> > > > > > >
>> > > > > > >   /**
>> > > > > > >
>> > > > > > >    * Converts the instance to a double array.
>> > > > > > >
>> > > > > > >    */
>> > > > > > >
>> > > > > > >   def toArray: Array[Double]
>> > > > > > >
>> > > > > > > Don't we need a template on float/double ? This will give us
>> > memory
>> > > > > > > savings...
>> > > > > > >
>> > > > > > > Thanks.
>> > > > > > >
>> > > > > > > Deb
>> > > > > > >
>> > > > > >
>> > > > >
>> > > >
>> > >
>> >
>>
>
>

--047d7b417997521cbc04f8dcb714--

From dev-return-7525-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 11 21:09:07 2014
Return-Path: <dev-return-7525-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9F1A4116EF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 11 May 2014 21:09:07 +0000 (UTC)
Received: (qmail 55083 invoked by uid 500); 11 May 2014 21:09:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55024 invoked by uid 500); 11 May 2014 21:09:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55016 invoked by uid 99); 11 May 2014 21:09:02 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 11 May 2014 21:09:02 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.43 as permitted sender)
Received: from [209.85.220.43] (HELO mail-pa0-f43.google.com) (209.85.220.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 11 May 2014 21:08:58 +0000
Received: by mail-pa0-f43.google.com with SMTP id hz1so6942319pad.30
        for <dev@spark.apache.org>; Sun, 11 May 2014 14:08:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=Qca41a64mwMES66iUoAED1nOeRjP1JVLE3m3EsO4Deo=;
        b=0XV7a/ZG3aHleEdkFPzZ+aABzqGt3JRHxnifRvxH8O6jvZcOOT5cnoz9vi0WClNbEK
         NzUj3pkpBQ5fouVoQFdyrDgKnOXf8pW/Xo8jmSUba3lDdgr1hPY29GK8+/r98X3tlUTG
         +ZVqwdiW3xUxu4F5pwYSqW7tPPG3klip8LA6M69btEnkr/p5uELlTHljZ5eXS8cHK25e
         OI84XrEFy53Kejq6qVYZdxVA2l7wZ2RtMKttMXTyFzUGAlBrdRd9x1sP6w8R+60JDEbs
         JnextPNL/hONbttrJIqLNtbU3yUHgqDOsiIKodLgKyYbVnasCXfR2ZitTwXmf5KU2kjz
         xC4w==
X-Received: by 10.66.136.103 with SMTP id pz7mr48207069pab.140.1399842513600;
        Sun, 11 May 2014 14:08:33 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id lr3sm39615054pab.4.2014.05.11.14.08.30
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 11 May 2014 14:08:31 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: Spark on Scala 2.11
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAFboF2xJThe+5Jf8eAfvMMRmuUJW=ufXUoOn8XUMrKhktbE2cQ@mail.gmail.com>
Date: Sun, 11 May 2014 14:08:27 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <D86A1D6E-3345-42E8-A82F-E442D129BEF0@gmail.com>
References: <CAFboF2xJThe+5Jf8eAfvMMRmuUJW=ufXUoOn8XUMrKhktbE2cQ@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

We do want to support it eventually, possibly as early as Spark 1.1 =
(which we=92d cross-build on Scala 2.10 and 2.11). If someone wants to =
look at it before, feel free to do so! Scala 2.11 is very close to 2.10 =
so I think things will mostly work, except for possibly the REPL (which =
has require porting over code form the Scala REPL in each version).

Matei

On May 8, 2014, at 6:33 PM, Anand Avati <avati@gluster.org> wrote:

> Is there an ongoing effort (or intent) to support Spark on Scala 2.11?
> Approximate timeline?
>=20
> Thanks


From dev-return-7527-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 11 22:36:06 2014
Return-Path: <dev-return-7527-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3BA4B1197C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 11 May 2014 22:36:06 +0000 (UTC)
Received: (qmail 93137 invoked by uid 500); 11 May 2014 22:29:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93073 invoked by uid 500); 11 May 2014 22:29:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93065 invoked by uid 99); 11 May 2014 22:29:26 -0000
Received: from Unknown (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 11 May 2014 22:29:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [74.125.82.178] (HELO mail-we0-f178.google.com) (74.125.82.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 11 May 2014 22:29:21 +0000
Received: by mail-we0-f178.google.com with SMTP id u56so6010330wes.37
        for <dev@spark.apache.org>; Sun, 11 May 2014 15:29:00 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=Mbl5Q6rRzMLx0WWLn5N5mwHswpcbDaQnmREqnOKaaSM=;
        b=T3RlemVdWJxQfYpMCmh5KTC0Pr9dIX6JeQVyyX7eIr4Z8EnO7cPwi8mqOnSu2T5mpJ
         1xgGt8OSpZjVreGFEZ2SPqO790vgj7GyvHHQytQK9FF+7NwoeOs1CyCkKB8lyV80hLfp
         QrkWthDxBQvpKoPjscnP6ZHCMKv+SX6AGHQyoOLDstyWSaVr/jKayX/i+FJptk9GHomk
         5n7YDfS74crNku5hD11PgnvbDrdBQaZ0JV4rLNZHxMkoosfstKJde9WOBfSH9HnyNUjM
         pCndj0ZP3sSngMWTBfcnyokwb7iLk98tVCBR1FJ5ZM44yWI3msAmioQuTkv4dyjY4a+c
         0dTw==
X-Gm-Message-State: ALoCoQnmp+8dfpoAHnJn7pfgAyn9Nci/oFYBXPI7O0OdXG4L3p4U2go82Bb7h0wJnLo8EXlgOq0e
MIME-Version: 1.0
X-Received: by 10.194.60.4 with SMTP id d4mr18538784wjr.28.1399847340139; Sun,
 11 May 2014 15:29:00 -0700 (PDT)
Received: by 10.217.66.129 with HTTP; Sun, 11 May 2014 15:29:00 -0700 (PDT)
X-Originating-IP: [209.150.41.132]
In-Reply-To: <CAGOvqiptNMhXrS4jLykYYO6Rzr8=jOQ67xmVUKU9nq3DTKKW5g@mail.gmail.com>
References: <CAFboF2xJThe+5Jf8eAfvMMRmuUJW=ufXUoOn8XUMrKhktbE2cQ@mail.gmail.com>
	<CAGOvqiptNMhXrS4jLykYYO6Rzr8=jOQ67xmVUKU9nq3DTKKW5g@mail.gmail.com>
Date: Sun, 11 May 2014 18:29:00 -0400
Message-ID: <CANx3uAgLLmDXQ3OAt+-jgbjXKKMd37T3xyL2Gfn1vnQEOqkD3A@mail.gmail.com>
Subject: Re: Spark on Scala 2.11
From: Koert Kuipers <koert@tresata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bacb40e72fb7e04f9275b07
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bacb40e72fb7e04f9275b07
Content-Type: text/plain; charset=UTF-8

i believe matei has said before that he would like to crossbuild for 2.10
and 2.11, given that the difference is not as big as between 2.9 and 2.10.
but dont know when this would happen...


On Sat, May 10, 2014 at 11:02 PM, Gary Malouf <malouf.gary@gmail.com> wrote:

> Considering the team just bumped to 2.10 in 0.9, I would be surprised if
> this is a near term priority.
>
>
> On Thu, May 8, 2014 at 9:33 PM, Anand Avati <avati@gluster.org> wrote:
>
> > Is there an ongoing effort (or intent) to support Spark on Scala 2.11?
> > Approximate timeline?
> >
> > Thanks
> >
>

--047d7bacb40e72fb7e04f9275b07--

From dev-return-7528-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 11 23:28:38 2014
Return-Path: <dev-return-7528-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 28E8E11A4E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 11 May 2014 23:28:38 +0000 (UTC)
Received: (qmail 13998 invoked by uid 500); 11 May 2014 23:28:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13933 invoked by uid 500); 11 May 2014 23:28:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13922 invoked by uid 99); 11 May 2014 23:28:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 11 May 2014 23:28:37 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of andykonwinski@gmail.com designates 209.85.215.43 as permitted sender)
Received: from [209.85.215.43] (HELO mail-la0-f43.google.com) (209.85.215.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 11 May 2014 23:28:33 +0000
Received: by mail-la0-f43.google.com with SMTP id mc6so1353554lab.16
        for <dev@spark.apache.org>; Sun, 11 May 2014 16:28:11 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=bL3J5RaGimQmxLEqyK8GYvSLE7Hm1C30igMH9HFmtBE=;
        b=kLefCHF9D+nlw7ojG8H6//Ua1/qlvQg+14Pdk8Jij0yIVaFFpF1bdy+zSNk4uRo4ih
         6lOfHM5ME22sCEacNppvW8n7IWL+OyVcqzlwooC7KlrKYVfDJakpuhen7Ms443yCWV72
         ntn/s4Z7w/5pTdYGC8KNjEtiC+W7L9CSoLYCqUCPSnDyMB7+ay0WDSxXcNcNpLuYRo6D
         +gVzMpS417IgwFdI4URHoBlyvhBihLEsfrPiczOcmOORghMak0DVlwNTa4fD4DmK8Ivg
         dVykw6NVBKHKPB1wk0lq4LF8sCt4nRBjQTaBn8XHZsrgLLU+f7S1LThvHAFKx0k/Jv0a
         OoOw==
MIME-Version: 1.0
X-Received: by 10.152.23.136 with SMTP id m8mr10653536laf.2.1399850891489;
 Sun, 11 May 2014 16:28:11 -0700 (PDT)
Received: by 10.112.147.4 with HTTP; Sun, 11 May 2014 16:28:11 -0700 (PDT)
Received: by 10.112.147.4 with HTTP; Sun, 11 May 2014 16:28:11 -0700 (PDT)
In-Reply-To: <CA+-p3AGGAKA05e9MpvOgQpmj=opOgvw+zXVZVojYU8Yo9XZ4tg@mail.gmail.com>
References: <CA+-p3AGGAKA05e9MpvOgQpmj=opOgvw+zXVZVojYU8Yo9XZ4tg@mail.gmail.com>
Date: Sun, 11 May 2014 16:28:11 -0700
Message-ID: <CALEZFQx--MJTQ5vi14ELUyb1XUUwRJMaH19cX3XEsvO4ZN4gjA@mail.gmail.com>
Subject: Re: Updating docs for running on Mesos
From: Andy Konwinski <andykonwinski@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0158bf8820387804f9282f5e
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158bf8820387804f9282f5e
Content-Type: text/plain; charset=UTF-8

Thanks for suggesting this and volunteering to do it.

On May 11, 2014 3:32 AM, "Andrew Ash" <andrew@andrewash.com> wrote:
>
> The docs for how to run Spark on Mesos have changed very little since
> 0.6.0, but setting it up is much easier now than then.  Does it make sense
> to revamp with the below changes?
>
>
> You no longer need to build mesos yourself as pre-built versions are
> available from Mesosphere: http://mesosphere.io/downloads/
>
> And the instructions guide you towards compiling your own distribution of
> Spark, when you can use the prebuilt versions of Spark as well.
>
>
> I'd like to split that portion of the documentation into two sections, a
> build-from-scratch section and a use-prebuilt section.  The new outline
> would look something like this:
>
>
> *Running Spark on Mesos*
>
> Installing Mesos
> - using prebuilt (recommended)
>  - pointer to mesosphere's packages
> - from scratch
>  - (similar to current)
>
>
> Connecting Spark to Mesos
> - loading distribution into an accessible location
> - Spark settings
>
> Mesos Run Modes
> - (same as current)
>
> Running Alongside Hadoop
> - (trim this down)

What trimming do you have in mind here?

>
>
>
> Does that work for people?
>
>
> Thanks!
> Andrew
>
>
> PS Basically all the same:
>
> http://spark.apache.org/docs/0.6.0/running-on-mesos.html
> http://spark.apache.org/docs/0.6.2/running-on-mesos.html
> http://spark.apache.org/docs/0.7.3/running-on-mesos.html
> http://spark.apache.org/docs/0.8.1/running-on-mesos.html
> http://spark.apache.org/docs/0.9.1/running-on-mesos.html
>
https://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/running-on-mesos.html

--089e0158bf8820387804f9282f5e--

From dev-return-7526-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 12 00:03:31 2014
Return-Path: <dev-return-7526-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E7E6E11B10
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 May 2014 00:03:31 +0000 (UTC)
Received: (qmail 66852 invoked by uid 500); 11 May 2014 21:16:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66793 invoked by uid 500); 11 May 2014 21:16:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66785 invoked by uid 99); 11 May 2014 21:16:51 -0000
Received: from Unknown (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 11 May 2014 21:16:51 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.175 as permitted sender)
Received: from [209.85.214.175] (HELO mail-ob0-f175.google.com) (209.85.214.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 11 May 2014 21:16:47 +0000
Received: by mail-ob0-f175.google.com with SMTP id wo20so7236245obc.6
        for <dev@spark.apache.org>; Sun, 11 May 2014 14:16:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=HXc8p9+F2SukpSKuglcu6wEwUMuKoSPSOVFePbM3pHg=;
        b=hvVvT6RtEQZIbrBj3fJ/VvNcg2YQZOQOj9ER9EQ8aauyYyJyfBjbkpZcIF3yjvoqDD
         VaTY9NSj2dEUfo0u7pdO7rK5dEHHN3VJIusPjJeRonKvqYBQQpsAlV2SynIpDHVdvFFJ
         CsRuoNwcD1kAp6/jjbcauNj4VLfTsVQih+TwXO4/q3ZwWRwxVju7Bp/VSVsllAgxMSjL
         gBsHXNJB4n4LIxFJiljfP4+atHO5Hh7gvdjLgJq0zNarhXGo55br8nlPhgHBOGvVBgvm
         i2JcFLFYlgiwjRfdQn3sulhbRBS3yhndFnXrU19aWucAi39ppYAJsuWvgw6X7pX4RPj+
         PJDw==
MIME-Version: 1.0
X-Received: by 10.60.63.12 with SMTP id c12mr28802877oes.23.1399842986561;
 Sun, 11 May 2014 14:16:26 -0700 (PDT)
Received: by 10.182.212.2 with HTTP; Sun, 11 May 2014 14:16:26 -0700 (PDT)
In-Reply-To: <CA+-p3AGGAKA05e9MpvOgQpmj=opOgvw+zXVZVojYU8Yo9XZ4tg@mail.gmail.com>
References: <CA+-p3AGGAKA05e9MpvOgQpmj=opOgvw+zXVZVojYU8Yo9XZ4tg@mail.gmail.com>
Date: Sun, 11 May 2014 14:16:26 -0700
Message-ID: <CABPQxsvrhG5FX2d5VuBPZ3T5CuyBms5f+E5SFVOo2bzmB9Ay9A@mail.gmail.com>
Subject: Re: Updating docs for running on Mesos
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Andrew,

Updating these docs would be great! I think this would be a welcome change.

In terms of packaging, it would be good to mention the binaries
produced by the upstream project as well, in addition to Mesosphere.

- Patrick

On Thu, May 8, 2014 at 12:51 AM, Andrew Ash <andrew@andrewash.com> wrote:
> The docs for how to run Spark on Mesos have changed very little since
> 0.6.0, but setting it up is much easier now than then.  Does it make sense
> to revamp with the below changes?
>
>
> You no longer need to build mesos yourself as pre-built versions are
> available from Mesosphere: http://mesosphere.io/downloads/
>
> And the instructions guide you towards compiling your own distribution of
> Spark, when you can use the prebuilt versions of Spark as well.
>
>
> I'd like to split that portion of the documentation into two sections, a
> build-from-scratch section and a use-prebuilt section.  The new outline
> would look something like this:
>
>
> *Running Spark on Mesos*
>
> Installing Mesos
> - using prebuilt (recommended)
>  - pointer to mesosphere's packages
> - from scratch
>  - (similar to current)
>
>
> Connecting Spark to Mesos
> - loading distribution into an accessible location
> - Spark settings
>
> Mesos Run Modes
> - (same as current)
>
> Running Alongside Hadoop
> - (trim this down)
>
>
>
> Does that work for people?
>
>
> Thanks!
> Andrew
>
>
> PS Basically all the same:
>
> http://spark.apache.org/docs/0.6.0/running-on-mesos.html
> http://spark.apache.org/docs/0.6.2/running-on-mesos.html
> http://spark.apache.org/docs/0.7.3/running-on-mesos.html
> http://spark.apache.org/docs/0.8.1/running-on-mesos.html
> http://spark.apache.org/docs/0.9.1/running-on-mesos.html
> https://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/running-on-mesos.html

From dev-return-7529-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 12 10:28:36 2014
Return-Path: <dev-return-7529-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 239B210EC9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 May 2014 10:28:36 +0000 (UTC)
Received: (qmail 244 invoked by uid 500); 12 May 2014 10:01:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 137 invoked by uid 500); 12 May 2014 10:01:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99957 invoked by uid 99); 12 May 2014 10:01:56 -0000
Received: from Unknown (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 10:01:56 +0000
X-ASF-Spam-Status: No, hits=4.2 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HK_RANDOM_ENVFROM,HK_RANDOM_FROM,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of lwwcl1314@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 10:01:50 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <lwwcl1314@gmail.com>)
	id 1Wjn34-00080V-8k
	for dev@spark.incubator.apache.org; Mon, 12 May 2014 03:01:30 -0700
Date: Mon, 12 May 2014 03:01:30 -0700 (PDT)
From: fengshen <lwwcl1314@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1399888890234-6514.post@n3.nabble.com>
Subject: about spark interactive shell
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

i email to user list=EF=BC=8Cbut no body relpy me.
so, i email to this. i  hope relpy

I am now using spark in production. but I notice spark driver including rdd
and dag...=20
and the executors will try to register with the driver.=20
but  in my company the executors do not register with the client because of
Network Limited.
I think the driver should run on the cluster and client should  run on the
gateway.=20
Similar like:
<http://apache-spark-developers-list.1001551.n3.nabble.com/file/n6514/Spark=
-interactive_shell.jpg>=20

i hope we take about this and how to implement this.



--
View this message in context: http://apache-spark-developers-list.1001551.n=
3.nabble.com/about-spark-interactive-shell-tp6514.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.c=
om.

From dev-return-7530-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 12 13:23:38 2014
Return-Path: <dev-return-7530-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 83E1D1142B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 May 2014 13:23:38 +0000 (UTC)
Received: (qmail 58785 invoked by uid 500); 12 May 2014 13:16:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58727 invoked by uid 500); 12 May 2014 13:16:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58719 invoked by uid 99); 12 May 2014 13:16:58 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 13:16:58 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ju.han.felix@gmail.com designates 209.85.223.179 as permitted sender)
Received: from [209.85.223.179] (HELO mail-ie0-f179.google.com) (209.85.223.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 13:16:54 +0000
Received: by mail-ie0-f179.google.com with SMTP id rd18so2017100iec.10
        for <dev@spark.apache.org>; Mon, 12 May 2014 06:16:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=PgV4B+p51JQg4ElVS762hy7UeA93ARa+nVg9jGWduoI=;
        b=dPbsQR9g5kKiTTyFCOGtgfqheHZMQlv+8RE1pb+ZgONwLXVTeKJ7nK8s41q+lh9jGV
         aXY5Tw5IF97OWq1iopLYEKCv8D+g9BZdOh+1pHZblHOF3Pjfp256mzdyroANqucFgNlu
         rHjC67neUtuv8080Iqg6G0BCButTGosnrJm8fN4MUo6p9/GMe0TgpmBZmHZVw5qqDofO
         YcB+eR09KZ4rpL0oN228oVcEDNAF6JCSImxO68oy/KkHeHg/7C4oak87sDr4bVrfnGzT
         3wYgFL7vNZstsbtON2/pfctpcantraH4FQFDwa3LL3BT8VPn/xRVQg0Jgi9EIQWaZkBs
         p2ng==
MIME-Version: 1.0
X-Received: by 10.50.28.84 with SMTP id z20mr44429550igg.0.1399900590813; Mon,
 12 May 2014 06:16:30 -0700 (PDT)
Received: by 10.64.133.227 with HTTP; Mon, 12 May 2014 06:16:30 -0700 (PDT)
Date: Mon, 12 May 2014 15:16:30 +0200
Message-ID: <CA+ndhHpDB1D8eyk9V_wdu==s7JFrvqZhw9CHXiCYDwsUH-3+bA@mail.gmail.com>
Subject: [EC2] r3 instance type
From: Han JU <ju.han.felix@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e015387f26fb77a04f933c18f
X-Virus-Checked: Checked by ClamAV on apache.org

--089e015387f26fb77a04f933c18f
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi,

I'm modifying the ec2 script for the new r3 instance support, but there's a
problem with the instance storage.

For example, `r3.large` has a single 32GB SSD disk, the problem is that
it's a SSD with TRIM technology and is not automatically formatted and
mounted, `lsblk` gives me this after ec2_script's setup:

xvda    202:0    0   8G  0 disk
=E2=94=94=E2=94=80xvda1 202:1    0   8G  0 part /
xvdb    202:16   0  30G  0 disk

I think there's some workarounds of this problem, for example we could
treat it like an EBS device and check `/dev/xvdb` by using `blkid`, howver
this needs modifying the deployment script inside the AMI and I don't know
if it's the preferred way .

Some ideas or suggestions?

Thanks.
--=20
*JU Han*

Data Engineer @ Botify.com

+33 0619608888

--089e015387f26fb77a04f933c18f--

From dev-return-7531-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 12 15:14:19 2014
Return-Path: <dev-return-7531-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4E24C1176C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 May 2014 15:14:19 +0000 (UTC)
Received: (qmail 48435 invoked by uid 500); 12 May 2014 15:07:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48372 invoked by uid 500); 12 May 2014 15:07:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48364 invoked by uid 99); 12 May 2014 15:07:34 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 15:07:34 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_QUOTING
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 74.125.82.52 as permitted sender)
Received: from [74.125.82.52] (HELO mail-wg0-f52.google.com) (74.125.82.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 15:07:31 +0000
Received: by mail-wg0-f52.google.com with SMTP id l18so6877437wgh.23
        for <dev@spark.apache.org>; Mon, 12 May 2014 08:07:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=tCe22uuXPdnT2b1qWt7HCC5GyyWP6U0Avwv7iP16nps=;
        b=vIwcsoDAG11I6KytxxX+FLfDJXepzxnPf5y0nofaXSxQWbOkSSPSUA8pH6cWqQAowL
         jaUHSdhfX2Ta4tDGt+qbZSySXMjJf7ueQM0u4dpMaOe8vmhagQM0rWJbA6UVSN7wt6rG
         24TXVDOwkOw+mDwng78hS+d62tK7ZF4k+Tf1GSmlc4abVUOfY//u8YFfpUH4B0yU2oMr
         DlO/KejAccLwe2ticTybGFZ33PFIA0Dh1nCCuqrYaj6i+zRZkyo4ZysYY9AHOX6dzpVY
         oda4C0r2IkKClDCs5aJsY2a0mAezeg6Rh4d8VEU8ihCnnOVPYANZbo7xR2MZxpjojbhM
         daQA==
MIME-Version: 1.0
X-Received: by 10.180.228.42 with SMTP id sf10mr15863949wic.33.1399907228607;
 Mon, 12 May 2014 08:07:08 -0700 (PDT)
Received: by 10.194.187.80 with HTTP; Mon, 12 May 2014 08:07:08 -0700 (PDT)
In-Reply-To: <CA+B-+fxoNS77N71R3y+8KXFN1DixPNU2Q-+Hwa=50Fzku2GgGw@mail.gmail.com>
References: <CA+B-+fxoNS77N71R3y+8KXFN1DixPNU2Q-+Hwa=50Fzku2GgGw@mail.gmail.com>
Date: Mon, 12 May 2014 08:07:08 -0700
Message-ID: <CAJgQjQ_EcGDWwMftKRgCB=x7sg9rr_aTx6sfFB8cfBwyDG4DLA@mail.gmail.com>
Subject: Re: LabeledPoint dump LibSVM if SparseVector
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Deb,

There is a saveAsLibSVMFile in MLUtils now. Also, I submitted a PR for
standardizing text format of vectors and labeled point:
https://github.com/apache/spark/pull/685

Best,
Xiangrui

On Sun, May 11, 2014 at 9:40 AM, Debasish Das <debasish.das83@gmail.com> wrote:
> Hi,
>
> I need to change the toString on LabeledPoint to libsvm format so that I
> can dump RDD[LabeledPoint] as a format that could be read by sparse
> glmnet-R and other packages to benchmark mllib classification accuracy...
>
> Basically I have to change the toString of LabeledPoint and toString of
> SparseVector....
>
> Should I add it as a PR or is it already being added ?
>
> I added these functions toLibSvm in my internal util class for now...
>
> def toLibSvm(labelPoint: LabeledPoint): String = {
>
>     labelPoint.label.toString + " " +
> toLibSvm(labelPoint.features.asInstanceOf[SparseVector])
>
>   }
>
>   def toLibSvm(features: SparseVector): String = {
>
>     val indices = features.indices
>
>     val values = features.values
>
>     indices.zip(values).mkString("
> ").replace(',', ':').replace("(", "").replace(")","")
>
>   }
> Thanks.
> Deb

From dev-return-7532-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 12 15:14:55 2014
Return-Path: <dev-return-7532-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 865BD11772
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 May 2014 15:14:55 +0000 (UTC)
Received: (qmail 48726 invoked by uid 500); 12 May 2014 15:07:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48627 invoked by uid 500); 12 May 2014 15:07:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48380 invoked by uid 99); 12 May 2014 15:07:34 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 15:07:34 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_QUOTING
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.212.177 as permitted sender)
Received: from [209.85.212.177] (HELO mail-wi0-f177.google.com) (209.85.212.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 15:07:31 +0000
Received: by mail-wi0-f177.google.com with SMTP id f8so4655670wiw.16
        for <dev@spark.incubator.apache.org>; Mon, 12 May 2014 08:07:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=tCe22uuXPdnT2b1qWt7HCC5GyyWP6U0Avwv7iP16nps=;
        b=vIwcsoDAG11I6KytxxX+FLfDJXepzxnPf5y0nofaXSxQWbOkSSPSUA8pH6cWqQAowL
         jaUHSdhfX2Ta4tDGt+qbZSySXMjJf7ueQM0u4dpMaOe8vmhagQM0rWJbA6UVSN7wt6rG
         24TXVDOwkOw+mDwng78hS+d62tK7ZF4k+Tf1GSmlc4abVUOfY//u8YFfpUH4B0yU2oMr
         DlO/KejAccLwe2ticTybGFZ33PFIA0Dh1nCCuqrYaj6i+zRZkyo4ZysYY9AHOX6dzpVY
         oda4C0r2IkKClDCs5aJsY2a0mAezeg6Rh4d8VEU8ihCnnOVPYANZbo7xR2MZxpjojbhM
         daQA==
MIME-Version: 1.0
X-Received: by 10.180.228.42 with SMTP id sf10mr15863949wic.33.1399907228607;
 Mon, 12 May 2014 08:07:08 -0700 (PDT)
Received: by 10.194.187.80 with HTTP; Mon, 12 May 2014 08:07:08 -0700 (PDT)
In-Reply-To: <CA+B-+fxoNS77N71R3y+8KXFN1DixPNU2Q-+Hwa=50Fzku2GgGw@mail.gmail.com>
References: <CA+B-+fxoNS77N71R3y+8KXFN1DixPNU2Q-+Hwa=50Fzku2GgGw@mail.gmail.com>
Date: Mon, 12 May 2014 08:07:08 -0700
Message-ID: <CAJgQjQ_EcGDWwMftKRgCB=x7sg9rr_aTx6sfFB8cfBwyDG4DLA@mail.gmail.com>
Subject: Re: LabeledPoint dump LibSVM if SparseVector
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Deb,

There is a saveAsLibSVMFile in MLUtils now. Also, I submitted a PR for
standardizing text format of vectors and labeled point:
https://github.com/apache/spark/pull/685

Best,
Xiangrui

On Sun, May 11, 2014 at 9:40 AM, Debasish Das <debasish.das83@gmail.com> wrote:
> Hi,
>
> I need to change the toString on LabeledPoint to libsvm format so that I
> can dump RDD[LabeledPoint] as a format that could be read by sparse
> glmnet-R and other packages to benchmark mllib classification accuracy...
>
> Basically I have to change the toString of LabeledPoint and toString of
> SparseVector....
>
> Should I add it as a PR or is it already being added ?
>
> I added these functions toLibSvm in my internal util class for now...
>
> def toLibSvm(labelPoint: LabeledPoint): String = {
>
>     labelPoint.label.toString + " " +
> toLibSvm(labelPoint.features.asInstanceOf[SparseVector])
>
>   }
>
>   def toLibSvm(features: SparseVector): String = {
>
>     val indices = features.indices
>
>     val values = features.values
>
>     indices.zip(values).mkString("
> ").replace(',', ':').replace("(", "").replace(")","")
>
>   }
> Thanks.
> Deb

From dev-return-7516-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 12 15:40:31 2014
Return-Path: <dev-return-7516-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 62EA011883
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 May 2014 15:40:31 +0000 (UTC)
Received: (qmail 11363 invoked by uid 500); 10 May 2014 23:20:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 407 invoked by uid 500); 10 May 2014 23:03:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27731 invoked by uid 99); 10 May 2014 22:58:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 10 May 2014 22:58:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of anand.avati@gmail.com designates 209.85.214.176 as permitted sender)
Received: from [209.85.214.176] (HELO mail-ob0-f176.google.com) (209.85.214.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 09 May 2014 01:34:33 +0000
Received: by mail-ob0-f176.google.com with SMTP id wo20so4045188obc.35
        for <dev@spark.apache.org>; Thu, 08 May 2014 18:34:12 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:date:message-id:subject:from:to:content-type;
        bh=j7vq9+NKZDJvk6HH9GaVIX7/4TEKXJS/VAGHmplskPI=;
        b=Xu5UtPvZPIxQHRP7fnAwal8KyZPOqWQft7JqUd8kq+3OcMhN1ilJOKsN6Rt30EPfIf
         +I6J2THXjV6I4ndBqVQY7bADAgHMt7AZ04fR7ATapG0KeNzvkEIVkZHNk2k9KvSxAUse
         yv/BseUXnaqHJVnGUWJHCj8c33xpocgXjqLGVODz0RSPdoourWKXue1Z6XfzIUlS8ezi
         KbTX6brEShMDLWMPze3Fkuc8r80b5xIRaSCsP62KOZuRVG1goA7fG/kv3IH/+/OcY+Fh
         fsvWg1u4EozSom2DNURsrslfNVul0+ETASY6AXxp2DxeWtYs4FKPSR5IZd8PE5v6AzSW
         +otg==
MIME-Version: 1.0
X-Received: by 10.182.243.138 with SMTP id wy10mr108475obc.83.1399599210653;
 Thu, 08 May 2014 18:33:30 -0700 (PDT)
Sender: anand.avati@gmail.com
Received: by 10.60.165.68 with HTTP; Thu, 8 May 2014 18:33:30 -0700 (PDT)
Date: Thu, 8 May 2014 18:33:30 -0700
X-Google-Sender-Auth: bipWdgR3uOYI6TaqSNTA0y6IhFQ
Message-ID: <CAFboF2xJThe+5Jf8eAfvMMRmuUJW=ufXUoOn8XUMrKhktbE2cQ@mail.gmail.com>
Subject: Spark on Scala 2.11
From: Anand Avati <avati@gluster.org>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2a120c76dab04f8ed95a1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2a120c76dab04f8ed95a1
Content-Type: text/plain; charset=UTF-8

Is there an ongoing effort (or intent) to support Spark on Scala 2.11?
Approximate timeline?

Thanks

--001a11c2a120c76dab04f8ed95a1--

From dev-return-7533-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 12 18:41:30 2014
Return-Path: <dev-return-7533-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BE63611F01
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 May 2014 18:41:30 +0000 (UTC)
Received: (qmail 75655 invoked by uid 500); 12 May 2014 17:39:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75604 invoked by uid 500); 12 May 2014 17:39:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75596 invoked by uid 99); 12 May 2014 17:39:29 -0000
Received: from Unknown (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 17:39:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.214.172] (HELO mail-ob0-f172.google.com) (209.85.214.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 17:39:25 +0000
Received: by mail-ob0-f172.google.com with SMTP id wp18so8479479obc.17
        for <dev@spark.apache.org>; Mon, 12 May 2014 10:39:04 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=3dePrpU9GYyOpBKykD5iL5Bef5OYpdO5pGmtkQl0K3o=;
        b=YlhlnJKuDlVRoPUeQWpD2U0yq0EVr1OBO+bxXEO2OM/AVKH2PcnB4C78eI9hFtzyF7
         HYalpCxjKR5SSgwnJnmiYlyrnaeCN7NvKesQApe4PEL13Xf+qDLeWHpsIRoOGHdkzbjX
         dWkP/JEJww9+l3slM5+tEkom5uwEHOF04/l7IDfXai0fdbyT2JFSxsZRWGRsLSQA4vw5
         s2JXr/dW6zqQq4tAps0o/Ye8haB21LMwzcABwqR3FkO6aXbVC+FAf+nTI6eEw6ss9W0f
         Rbv+H3dtsDgJvPBhB/z59B+nDOaUpC/HpYNghvj1z83miyRftHwYeqbA2T8PdOACwl/X
         SAAg==
X-Gm-Message-State: ALoCoQlAGsLvI9CTAaMsRvZr2AszsD8Xf7AwnwfryirpFN06w2gl0ww/QKydEeDtTRsoUrh0Ul1C
MIME-Version: 1.0
X-Received: by 10.60.178.243 with SMTP id db19mr35693764oec.11.1399916344347;
 Mon, 12 May 2014 10:39:04 -0700 (PDT)
Received: by 10.182.184.40 with HTTP; Mon, 12 May 2014 10:39:04 -0700 (PDT)
Date: Mon, 12 May 2014 10:39:04 -0700
Message-ID: <CAJ3iqPSA0kVT7oncntcZecC1C3z4=_RZ=W1hyt=-FFQjZFj_3g@mail.gmail.com>
Subject: Bug is KryoSerializer under Mesos [work-around included]
From: Soren Macbeth <soren@yieldbot.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01176ac56bc3dc04f9376cda
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01176ac56bc3dc04f9376cda
Content-Type: text/plain; charset=UTF-8

I finally managed to track down the source of the kryo issues that I was
having under mesos.

What happens is the for a reason that I haven't tracked down yet, a handful
of the scala collection classes from chill-scala down get registered by the
mesos executors, but they do all get registered in the driver process.

This led to scala.Some classes which were serialized by the executors being
incorrectly deserialized as scala.collections.Wrappers$SeqWrapper in driver
during task deserialization, causing a KryoException.

I resolved this issue in my spark job by explicitly registering the classes
in my Registrator like so:


kryo.register(scala.collection.convert.Wrappers.IteratorWrapper.class);
      kryo.register(scala.collection.convert.Wrappers.SeqWrapper.class);
      kryo.register(scala.collection.convert.Wrappers.MapWrapper.class);
      kryo.register(scala.collection.convert.Wrappers.JListWrapper.class);
      kryo.register(scala.collection.convert.Wrappers.JMapWrapper.class);

Again, I'm not sure why they don't get registered in the mesos executors,
but I wanted to report wht I found as well as a workaround in case anyone
else hit this (extraordinarily frustrating) issue again.

Some interactive debugging note are available in this gist:

https://gist.github.com/sorenmacbeth/28707a7a973f7a1982dc

Cheers,
Soren

--089e01176ac56bc3dc04f9376cda--

From dev-return-7534-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 12 19:03:26 2014
Return-Path: <dev-return-7534-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 51ED311FA7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 May 2014 19:03:26 +0000 (UTC)
Received: (qmail 70818 invoked by uid 500); 12 May 2014 19:03:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70754 invoked by uid 500); 12 May 2014 19:03:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70746 invoked by uid 99); 12 May 2014 19:03:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 19:03:25 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.48 as permitted sender)
Received: from [209.85.220.48] (HELO mail-pa0-f48.google.com) (209.85.220.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 19:03:20 +0000
Received: by mail-pa0-f48.google.com with SMTP id rd3so9192829pab.7
        for <dev@spark.apache.org>; Mon, 12 May 2014 12:03:00 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=pnaLc6RMv+5xxef5/tI9GaqAGUL2mXE9P0BkyjgnlwE=;
        b=z/vKou694yfIpVyz1qGJkbI16FuZYAzUuzwIQBsCjqN+eMfqVdnA4dwyTR51SWxxZp
         PYNwxX2q79Lbjt4YhXF+tPiOcCEbKUfOQd8IWA1g/JakgCBvuv6WtXyQbpTXVCBlI/rD
         ssHI0hIoVhATFqH0cNfxxGxpJ20kEmpsYyLNgs6ljaZdi87J+JD4b6Ie+DW5PNN2xLc1
         qnbYHYdct6lT1w36RdyA78B0qUaf4PvUSpz1SsZuGGxvFJZqg0jUgWdQWJQsb0T95xEm
         Z9EfM1m2Tf7EwCHwTnBmXFRDOq9SA7BgbM+HMAlt8No/D0G7ws8jejLJ7526c/oeGiR3
         /RRA==
X-Received: by 10.66.102.4 with SMTP id fk4mr58251920pab.59.1399921380055;
        Mon, 12 May 2014 12:03:00 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id pl10sm24145122pbb.56.2014.05.12.12.02.58
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 12 May 2014 12:02:59 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: Bug is KryoSerializer under Mesos [work-around included]
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAJ3iqPSA0kVT7oncntcZecC1C3z4=_RZ=W1hyt=-FFQjZFj_3g@mail.gmail.com>
Date: Mon, 12 May 2014 12:02:57 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <AF52EE5D-2C7A-47F8-9533-8B91138A5AB8@gmail.com>
References: <CAJ3iqPSA0kVT7oncntcZecC1C3z4=_RZ=W1hyt=-FFQjZFj_3g@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Soren, are you sure that the JAR you used on the executors is for =
the right version of Spark? Maybe they=92re running an older version. =
The Kryo serializer should be initialized the same way on both.

Matei

On May 12, 2014, at 10:39 AM, Soren Macbeth <soren@yieldbot.com> wrote:

> I finally managed to track down the source of the kryo issues that I =
was
> having under mesos.
>=20
> What happens is the for a reason that I haven't tracked down yet, a =
handful
> of the scala collection classes from chill-scala down get registered =
by the
> mesos executors, but they do all get registered in the driver process.
>=20
> This led to scala.Some classes which were serialized by the executors =
being
> incorrectly deserialized as scala.collections.Wrappers$SeqWrapper in =
driver
> during task deserialization, causing a KryoException.
>=20
> I resolved this issue in my spark job by explicitly registering the =
classes
> in my Registrator like so:
>=20
>=20
> =
kryo.register(scala.collection.convert.Wrappers.IteratorWrapper.class);
>      =
kryo.register(scala.collection.convert.Wrappers.SeqWrapper.class);
>      =
kryo.register(scala.collection.convert.Wrappers.MapWrapper.class);
>      =
kryo.register(scala.collection.convert.Wrappers.JListWrapper.class);
>      =
kryo.register(scala.collection.convert.Wrappers.JMapWrapper.class);
>=20
> Again, I'm not sure why they don't get registered in the mesos =
executors,
> but I wanted to report wht I found as well as a workaround in case =
anyone
> else hit this (extraordinarily frustrating) issue again.
>=20
> Some interactive debugging note are available in this gist:
>=20
> https://gist.github.com/sorenmacbeth/28707a7a973f7a1982dc
>=20
> Cheers,
> Soren


From dev-return-7519-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 12 19:49:50 2014
Return-Path: <dev-return-7519-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A08C41116B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 May 2014 19:49:50 +0000 (UTC)
Received: (qmail 27218 invoked by uid 500); 10 May 2014 23:21:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49925 invoked by uid 500); 10 May 2014 23:07:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84500 invoked by uid 99); 10 May 2014 22:55:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 10 May 2014 22:55:22 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mark@clearstorydata.com designates 209.85.212.178 as permitted sender)
Received: from [209.85.212.178] (HELO mail-wi0-f178.google.com) (209.85.212.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 May 2014 21:17:05 +0000
Received: by mail-wi0-f178.google.com with SMTP id hm4so364268wib.17
        for <dev@spark.apache.org>; Thu, 08 May 2014 14:16:43 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=EWlc+n0wCUk8Ob+WzLEpLHJHwSMlBp+o6dtiYnprj40=;
        b=lfoM+qL2Of+qm3W1qsBgAWZYq7k2pw+Ux1uNlORAr1tW9bO7laEbJ7qDY9bb2It8gJ
         RC/4zrTvuMqZCo9c0uJYEjbo/rkvJ5q+uE/eSi+dJ49QYrnu36B7RXu7NqJXfqwelM7C
         hszp0MFVxzmqdKKSI2UPVohmbzvObFzu9wKMGgM5pmpocwr4gGnvKHXjtCTTgNuAJ1t+
         JMbkQIbtnL6giVFPgznQ2aO+lHtn1JvRNBJ+EqA7/zxPEUalydSOJV7d+91q3Aivr+tV
         udvMNR7DVgPS5UJ1qcEBil9f+IhA7ILh63TSJvSR60ziu2vn5uqXlmB9KPOwanek4HWc
         oSMg==
X-Gm-Message-State: ALoCoQmHtvwfNkyZ4exKYi2pUNaif1FsDfYBSRPwlOZbsdcBnakxR4+tmQIiBdtBnHAP1fbI4p/9
MIME-Version: 1.0
X-Received: by 10.194.161.168 with SMTP id xt8mr5048069wjb.35.1399583803214;
 Thu, 08 May 2014 14:16:43 -0700 (PDT)
Received: by 10.216.232.69 with HTTP; Thu, 8 May 2014 14:16:43 -0700 (PDT)
Date: Thu, 8 May 2014 14:16:43 -0700
Message-ID: <CAAsvFP=9PWfNP3ZQ4sXJV+R4VUpV3WzO2=eE+WrxDPDTs0rtpg@mail.gmail.com>
Subject: Any ideas on SPARK-1021?
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01419c6a6c9f0a04f8e9ff87
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01419c6a6c9f0a04f8e9ff87
Content-Type: text/plain; charset=UTF-8

I'm trying to decide whether attacking the underlying issue of
RangePartitioner running eager jobs in rangeBounds (i.e. SPARK-1021) is a
better option than a messy workaround for some async job-handling stuff
that I am working on.  It looks like there have been a couple of aborted
attempts to solve the problem, but no solution or clear path to one at this
point.

Has anybody made any further progress or does anyone have any new ideas on
how to proceed?

--089e01419c6a6c9f0a04f8e9ff87--

From dev-return-7538-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 12 22:54:47 2014
Return-Path: <dev-return-7538-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6C8D91177E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 May 2014 22:54:47 +0000 (UTC)
Received: (qmail 9734 invoked by uid 500); 12 May 2014 22:54:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9675 invoked by uid 500); 12 May 2014 22:54:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9667 invoked by uid 99); 12 May 2014 22:54:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 22:54:46 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [85.128.177.112] (HELO alu112.rev.netart.pl) (85.128.177.112)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 22:54:41 +0000
Received: from mail-yk0-f175.google.com (unknown [209.85.160.175])
	by laskowski.nazwa.pl (Postfix) with ESMTP id A28EA114EDC9
	for <dev@spark.apache.org>; Tue, 13 May 2014 00:54:19 +0200 (CEST)
Received: by mail-yk0-f175.google.com with SMTP id 131so6507028ykp.34
        for <dev@spark.apache.org>; Mon, 12 May 2014 15:54:18 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.236.26.72 with SMTP id b48mr46945488yha.59.1399935258116;
 Mon, 12 May 2014 15:54:18 -0700 (PDT)
Received: by 10.170.56.7 with HTTP; Mon, 12 May 2014 15:54:18 -0700 (PDT)
In-Reply-To: <D86A1D6E-3345-42E8-A82F-E442D129BEF0@gmail.com>
References: <CAFboF2xJThe+5Jf8eAfvMMRmuUJW=ufXUoOn8XUMrKhktbE2cQ@mail.gmail.com>
	<D86A1D6E-3345-42E8-A82F-E442D129BEF0@gmail.com>
Date: Tue, 13 May 2014 00:54:18 +0200
Message-ID: <CACo38_TfLMF4Zv0yBiSgFYCqa_CYwCF1TFC8wHzSqqs-K-KA+g@mail.gmail.com>
Subject: Re: Spark on Scala 2.11
From: Jacek Laskowski <jacek@japila.pl>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

On Sun, May 11, 2014 at 11:08 PM, Matei Zaharia <matei.zaharia@gmail.com> w=
rote:
> We do want to support it eventually, possibly as early as Spark 1.1 (whic=
h we=E2=80=99d cross-build on Scala 2.10 and 2.11). If someone wants to loo=
k at it before, feel free to do so! Scala 2.11 is very close to 2.10 so I t=
hink things will mostly work, except for possibly the REPL (which has requi=
re porting over code form the Scala REPL in each version).

Hi,

Would that be possible to have a JIRA issue for this (so I could have
a branch for the cross-build in sbt and give the task a try)?

Jacek

--=20
Jacek Laskowski | http://blog.japila.pl
"Never discourage anyone who continually makes progress, no matter how
slow." Plato

From dev-return-7537-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 12 23:34:09 2014
Return-Path: <dev-return-7537-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A3E87118A0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 May 2014 23:34:09 +0000 (UTC)
Received: (qmail 86855 invoked by uid 500); 12 May 2014 21:47:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86772 invoked by uid 500); 12 May 2014 21:47:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86340 invoked by uid 99); 12 May 2014 21:47:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 21:47:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of anand.avati@gmail.com designates 209.85.219.43 as permitted sender)
Received: from [209.85.219.43] (HELO mail-oa0-f43.google.com) (209.85.219.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 21:47:23 +0000
Received: by mail-oa0-f43.google.com with SMTP id l6so8979169oag.2
        for <dev@spark.apache.org>; Mon, 12 May 2014 14:47:03 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:date:message-id:subject:from:to:content-type;
        bh=YSWclzwxheP6l6QIZnRHQyrlM4I7CP2vtUtV9XsFwHI=;
        b=1Gp+RWbX7sArVr4FxrPRy6Jvw9BaqAnfzOrPSAii3GMMHCw6GGPsyUL2b0uA1RrPQI
         kv5NomONwg1Zk8IHSgEaGV5Wvl21CbJnYocCenrxIbauuVnNKXWKdjiehKhYTIHMq5t6
         f4SmQlopAF31+jAjQZCIhMSmo4qe+YrppQm0xZtBaPQa7lVguwzghvCBFzbuY2ACPLK2
         C3wNJ2Q6J5jIOLhfbdw3KyQ2X54VIerixGzLuYE+WZ/9+d22y8HIV/6wQRXMqUoLuA/8
         G2ft1DYY8Yj7MIoiMjSuutyMH3wtYwYQqZ9h5WU/kTMo2q5M7hwIflWCE8z7/uZa/8pX
         Ap7w==
MIME-Version: 1.0
X-Received: by 10.182.163.45 with SMTP id yf13mr7479920obb.66.1399931223091;
 Mon, 12 May 2014 14:47:03 -0700 (PDT)
Sender: anand.avati@gmail.com
Received: by 10.60.165.68 with HTTP; Mon, 12 May 2014 14:47:03 -0700 (PDT)
Date: Mon, 12 May 2014 14:47:03 -0700
X-Google-Sender-Auth: uT3dIBOrPyg5-NqLq4QAxw0FRtA
Message-ID: <CAFboF2yO2ZduPwCxv_h7ewyvOsGeBCSt897GnHs=u9g6Bju=VQ@mail.gmail.com>
Subject: Kryo not default?
From: Anand Avati <avati@gluster.org>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=e89a8f503b5c432f1504f93ae345
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f503b5c432f1504f93ae345
Content-Type: text/plain; charset=UTF-8

Hi,
Can someone share the reason why Kryo serializer is not the default? Is
there anything to be careful about (because of which it is not enabled by
default)?
Thanks!

--e89a8f503b5c432f1504f93ae345--

From dev-return-7535-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 12 23:54:26 2014
Return-Path: <dev-return-7535-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4BF401192A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 May 2014 23:54:26 +0000 (UTC)
Received: (qmail 94973 invoked by uid 500); 12 May 2014 21:07:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94915 invoked by uid 500); 12 May 2014 21:07:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94907 invoked by uid 99); 12 May 2014 21:07:46 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 21:07:46 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of shivaram@berkeley.edu designates 74.125.82.42 as permitted sender)
Received: from [74.125.82.42] (HELO mail-wg0-f42.google.com) (74.125.82.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 21:07:43 +0000
Received: by mail-wg0-f42.google.com with SMTP id y10so7298934wgg.13
        for <dev@spark.apache.org>; Mon, 12 May 2014 14:07:19 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:in-reply-to:references
         :date:message-id:subject:from:to:content-type;
        bh=yZ8NtFNivHUMIEm1uyQLehIwyXb6OIZGASB0QOK0Lxc=;
        b=chWMHinAZRAdOIlfVIXKlP6dPsDWQ8lb/+FIC0/miNe0RC2FpjjzS3tVpLQi/F2YlA
         p9pUwMTHLZk+axIyuM7O4KZbiE5WLVscnoyNuTsZjYm2ERtITFNsRPaRtn2o7IYp4a3E
         Phu4EcSbZUvrgO/HessypEn5wlwdGXdEj2eS9cvHRuSOJiHHs0re42nUG1vC8gVb9Ri1
         rXD5vY/HQ/Tbrjx1+XxPOX2QfEHBh3Tbbyk8zJaUA4LGu+SKHF8uKFpgW2owXl1+p5aK
         +TwkXXM2AIAIiSdNenn0LRaIaZnInuqmWYj8IYPuNU6lRRHQOarFpweJx7r/kXaTuX1p
         rWEg==
X-Gm-Message-State: ALoCoQmZAsIkTJe2SHZvOI5CJFnvMta6ioNAuhLC2Uz3DDoML3T+9G/OqW3wLjLndoAHxLo/DlXw
MIME-Version: 1.0
X-Received: by 10.180.14.233 with SMTP id s9mr17661706wic.53.1399928839559;
 Mon, 12 May 2014 14:07:19 -0700 (PDT)
Reply-To: shivaram@eecs.berkeley.edu
Received: by 10.216.92.11 with HTTP; Mon, 12 May 2014 14:07:19 -0700 (PDT)
In-Reply-To: <CA+ndhHpDB1D8eyk9V_wdu==s7JFrvqZhw9CHXiCYDwsUH-3+bA@mail.gmail.com>
References: <CA+ndhHpDB1D8eyk9V_wdu==s7JFrvqZhw9CHXiCYDwsUH-3+bA@mail.gmail.com>
Date: Mon, 12 May 2014 14:07:19 -0700
Message-ID: <CAKx7Bf8WGGkOD9NLdAwL3x4yLrGGYXYWf07b9booAN3YHn2uYg@mail.gmail.com>
Subject: Re: [EC2] r3 instance type
From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d040fa03c316d7804f93a554e
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d040fa03c316d7804f93a554e
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I ran into this a couple of days back as well. Yes, we need to check if
/dev/xvdb is formatted and if not create xfs or some such filesystem on it.
We will need to change the deployment script and you can do that (similar
to EBS volumes) at https://github.com/mesos/spark-ec2/blob/v2/setup-slave.s=
h


Thanks
Shivaram



On Mon, May 12, 2014 at 6:16 AM, Han JU <ju.han.felix@gmail.com> wrote:

> Hi,
>
> I'm modifying the ec2 script for the new r3 instance support, but there's=
 a
> problem with the instance storage.
>
> For example, `r3.large` has a single 32GB SSD disk, the problem is that
> it's a SSD with TRIM technology and is not automatically formatted and
> mounted, `lsblk` gives me this after ec2_script's setup:
>
> xvda    202:0    0   8G  0 disk
> =E2=94=94=E2=94=80xvda1 202:1    0   8G  0 part /
> xvdb    202:16   0  30G  0 disk
>
> I think there's some workarounds of this problem, for example we could
> treat it like an EBS device and check `/dev/xvdb` by using `blkid`, howve=
r
> this needs modifying the deployment script inside the AMI and I don't kno=
w
> if it's the preferred way .
>
> Some ideas or suggestions?
>
> Thanks.
> --
> *JU Han*
>
> Data Engineer @ Botify.com
>
> +33 0619608888
>

--f46d040fa03c316d7804f93a554e--

From dev-return-7536-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 12 23:56:28 2014
Return-Path: <dev-return-7536-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 92FD311937
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 May 2014 23:56:28 +0000 (UTC)
Received: (qmail 20323 invoked by uid 500); 12 May 2014 21:09:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20272 invoked by uid 500); 12 May 2014 21:09:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20264 invoked by uid 99); 12 May 2014 21:09:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 21:09:48 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of anand.avati@gmail.com designates 209.85.219.48 as permitted sender)
Received: from [209.85.219.48] (HELO mail-oa0-f48.google.com) (209.85.219.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 21:09:43 +0000
Received: by mail-oa0-f48.google.com with SMTP id i4so8946207oah.35
        for <dev@spark.apache.org>; Mon, 12 May 2014 14:09:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=L5mc/fl3I1vIxvOEF/vxKLoowSIfotVuFCR0ANRjEDw=;
        b=rnqodhC8I5p6C9DN3LTQr9he3pPF+s7+JuyHuZUfRG2n4Mj/a6CKPphdgqA6gQ2l+V
         RHj0IJQ7UeKDh3AxhG98UgeoeNqltXbBLXcIsG7fgWANWSkX17U/VJ+HigkLadpXqYU1
         5ryc1Tjl5fACYjKJocKZ2aQSEJOiGIPsQ5avYMNRLIooRGMPArsNoLgC6vGEEirNW8P2
         lynccpu59mYDhWBIV3VQOM5oXkTeE6gfhVtzoSSzKKgtZqE0m6f9/y6Sz7kvdIwKGOBY
         Nhz4TXCiwf++fNzh8Uc95xIKXx3Ms3M55qvFNU9Nv+rDne8FH0RGOIJnwVSjnijXqIht
         gdQg==
MIME-Version: 1.0
X-Received: by 10.60.92.170 with SMTP id cn10mr6846234oeb.76.1399928963218;
 Mon, 12 May 2014 14:09:23 -0700 (PDT)
Sender: anand.avati@gmail.com
Received: by 10.60.165.68 with HTTP; Mon, 12 May 2014 14:09:23 -0700 (PDT)
In-Reply-To: <D86A1D6E-3345-42E8-A82F-E442D129BEF0@gmail.com>
References: <CAFboF2xJThe+5Jf8eAfvMMRmuUJW=ufXUoOn8XUMrKhktbE2cQ@mail.gmail.com>
	<D86A1D6E-3345-42E8-A82F-E442D129BEF0@gmail.com>
Date: Mon, 12 May 2014 14:09:23 -0700
X-Google-Sender-Auth: jR15JUmjeRb8I2k9Mq3fuVWINo0
Message-ID: <CAFboF2ws2tGhYxea54TMFMKREC-n1F0mzD9na6NvZRjbnP4+Gg@mail.gmail.com>
Subject: Re: Spark on Scala 2.11
From: Anand Avati <avati@gluster.org>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b33d31e9043ef04f93a5c07
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b33d31e9043ef04f93a5c07
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Matei,
Thanks for confirming. I was looking specifically at the REPL part and how
it can be significantly simplified with 2.11 Scala, without having to
inherit a full copy of a refactored repl inside Spark. I am happy to
investigate/contribute a simpler 2.11 based REPL if this is were seen as a
priority (1.1 does not seem "too far" away.) However a 2.10 compatible
cross build would still require a separate (existing) REPL code for the
2.10 build, no?

Thanks.

On Sun, May 11, 2014 at 2:08 PM, Matei Zaharia <matei.zaharia@gmail.com>wro=
te:

> We do want to support it eventually, possibly as early as Spark 1.1 (whic=
h
> we=E2=80=99d cross-build on Scala 2.10 and 2.11). If someone wants to loo=
k at it
> before, feel free to do so! Scala 2.11 is very close to 2.10 so I think
> things will mostly work, except for possibly the REPL (which has require
> porting over code form the Scala REPL in each version).
>
> Matei
>
> On May 8, 2014, at 6:33 PM, Anand Avati <avati@gluster.org> wrote:
>
> > Is there an ongoing effort (or intent) to support Spark on Scala 2.11?
> > Approximate timeline?
> >
> > Thank
>

--047d7b33d31e9043ef04f93a5c07--

From dev-return-7540-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 00:15:42 2014
Return-Path: <dev-return-7540-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EA969119CD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 00:15:41 +0000 (UTC)
Received: (qmail 88547 invoked by uid 500); 12 May 2014 23:47:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88484 invoked by uid 500); 12 May 2014 23:47:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88476 invoked by uid 99); 12 May 2014 23:47:40 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 23:47:40 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dlieu.7@gmail.com designates 209.85.219.49 as permitted sender)
Received: from [209.85.219.49] (HELO mail-oa0-f49.google.com) (209.85.219.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 23:47:37 +0000
Received: by mail-oa0-f49.google.com with SMTP id eb12so9044893oac.36
        for <dev@spark.apache.org>; Mon, 12 May 2014 16:47:13 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=FRKeRQjEvtZ8zSBX1nDjJIWEYrVsFl3qEcYv1BPCgck=;
        b=jPKwltwvot7fyrldeBuAPC5PBCAnNYfCum/op4vgpte2rgwlqcwYoD9Qm1CqfCXW+J
         GVkUdCBRcmUKx65DOXHX8nsMHb9oJV5KCJnUPUdiFUi646Y3YY/Z0p8J2qVxvan5Wjji
         jFHhPkjQoU3ZpH8AqktfB5TXOShRoS2ldKLEIDVR/KHyw4XHOlVcxh/4JapUlmy77Gae
         bTXjQa2ZDwxlAXOQ6PZvZ68Hcsf/d9gmEzx0I2iM4R21h8TP4EKeFxmsc6k5ppxawisc
         iszXnNlphsM6OxaSwuj9SnBf6dv6an7AOS3QWuCpg0UR1s7YCet9P55i30gDFhWuf2zN
         I1ug==
MIME-Version: 1.0
X-Received: by 10.60.176.102 with SMTP id ch6mr12154762oec.37.1399938433580;
 Mon, 12 May 2014 16:47:13 -0700 (PDT)
Received: by 10.76.77.3 with HTTP; Mon, 12 May 2014 16:47:13 -0700 (PDT)
In-Reply-To: <CAFboF2yO2ZduPwCxv_h7ewyvOsGeBCSt897GnHs=u9g6Bju=VQ@mail.gmail.com>
References: <CAFboF2yO2ZduPwCxv_h7ewyvOsGeBCSt897GnHs=u9g6Bju=VQ@mail.gmail.com>
Date: Mon, 12 May 2014 16:47:13 -0700
Message-ID: <CAPud8Tp=F9K8-qes-mdYqLMqmmszxEkF8v0QWotEiSN60m9gyQ@mail.gmail.com>
Subject: Re: Kryo not default?
From: Dmitriy Lyubimov <dlieu.7@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01182fce0a830c04f93c918f
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01182fce0a830c04f93c918f
Content-Type: text/plain; charset=UTF-8

On Mon, May 12, 2014 at 2:47 PM, Anand Avati <avati@gluster.org> wrote:

> Hi,
> Can someone share the reason why Kryo serializer is not the default?

why should it be?

On top of it, the only way to serialize a closure into the backend (even
now) is java serialization (which means java serialization is required of
all closure attributes)


> Is
> there anything to be careful about (because of which it is not enabled by
> default)?
>

Yes. Kind of stems from above. There's still a number of api calls that use
closure attributes to serialize data to backend (see fold(), for example).
which means even if you enable kryo, some api still requires java
serialization of an attribute.

I fixed parallelize(), collect() and something else that i don't remember
already in that regard, but i think even up till now there's still a number
of apis lingering whose data parameters  wouldn't work with kryo.


> Thanks!
>

--089e01182fce0a830c04f93c918f--

From dev-return-7544-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 00:24:14 2014
Return-Path: <dev-return-7544-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 333D811A27
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 00:24:14 +0000 (UTC)
Received: (qmail 61416 invoked by uid 500); 12 May 2014 23:57:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61356 invoked by uid 500); 12 May 2014 23:57:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61348 invoked by uid 99); 12 May 2014 23:57:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 23:57:32 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.128.170] (HELO mail-ve0-f170.google.com) (209.85.128.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 23:57:28 +0000
Received: by mail-ve0-f170.google.com with SMTP id db11so9893239veb.29
        for <dev@spark.apache.org>; Mon, 12 May 2014 16:57:07 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=WHmdLUdrzAfjqQHWp2HZbYCbSOqYyWXw9dUFoZynQaU=;
        b=AA/Q7untUte3tP9+6OUIn7lnq1sTIVOnUP5YbSXupLqEhLpfApvUzId1+1K1FPr//0
         1YySTVeqD9sT04b2KbmzerAm3OTNkivV0t3oVa5mi+gTXLFoKq0mVnVoZRfyCDfKFJxL
         ekR15A08koZfpMcf+dVmmp2zXDiwAOwOu5lCnQbQ3qMmTuNybzTwrrYFoq4Q3aQ8ChTB
         /eHebtTKmXrrvMj8SJIxKlychn9F6RemFimFeBoenYEys4ZJRT9fjVdea2u9ZhJz4u2e
         16pFY4KiNrh2FSXvLcT5+gVxUz0AFu1ssNhLwBdq/Ql7mf8wLcbNKjJUEad9nAv6qt9c
         v5fw==
X-Gm-Message-State: ALoCoQlGskqVynsZyruKPUgi9s99vKjzcf5bBumIySvhSVkkceV6arGTcrMS/qFtl4Wqo+TolI8J
X-Received: by 10.58.111.163 with SMTP id ij3mr5507746veb.26.1399939027273;
        Mon, 12 May 2014 16:57:07 -0700 (PDT)
Received: from mail-ve0-f169.google.com (mail-ve0-f169.google.com [209.85.128.169])
        by mx.google.com with ESMTPSA id nh8sm416810veb.3.2014.05.12.16.57.06
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 12 May 2014 16:57:06 -0700 (PDT)
Received: by mail-ve0-f169.google.com with SMTP id jx11so10043415veb.28
        for <dev@spark.apache.org>; Mon, 12 May 2014 16:57:05 -0700 (PDT)
X-Received: by 10.52.53.101 with SMTP id a5mr21919492vdp.14.1399939025814;
 Mon, 12 May 2014 16:57:05 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Mon, 12 May 2014 16:56:45 -0700 (PDT)
In-Reply-To: <CAPh_B=b6D2irqADBZYu9tAbXNLCNXNU6ACzVBZfaD4PnF_T2QA@mail.gmail.com>
References: <CAFboF2yO2ZduPwCxv_h7ewyvOsGeBCSt897GnHs=u9g6Bju=VQ@mail.gmail.com>
 <CAPh_B=b6D2irqADBZYu9tAbXNLCNXNU6ACzVBZfaD4PnF_T2QA@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 12 May 2014 16:56:45 -0700
Message-ID: <CA+-p3AEtjE52rTKD9Qa4aCt9uWKAUN64snrQESJQq7Eoy0adUA@mail.gmail.com>
Subject: Re: Kryo not default?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01184ace575b7c04f93cb4f9
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01184ace575b7c04f93cb4f9
Content-Type: text/plain; charset=UTF-8

As an example of where it sometimes doesn't work, in older versions of Kryo
/ Chill the Joda LocalDate class didn't serialize properly --
https://groups.google.com/forum/#!topic/cascalog-user/35cdnNIamKU


On Mon, May 12, 2014 at 4:39 PM, Reynold Xin <rxin@databricks.com> wrote:

> The main reason is that it doesn't always work (e.g. sometimes application
> program has special serialization / externalization written already for
> Java which don't work in Kryo).
>
> On Mon, May 12, 2014 at 5:47 PM, Anand Avati <avati@gluster.org> wrote:
>
> > Hi,
> > Can someone share the reason why Kryo serializer is not the default? Is
> > there anything to be careful about (because of which it is not enabled by
> > default)?
> > Thanks!
> >
>

--089e01184ace575b7c04f93cb4f9--

From dev-return-7539-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 02:26:36 2014
Return-Path: <dev-return-7539-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1DBA811DE4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 02:26:36 +0000 (UTC)
Received: (qmail 42204 invoked by uid 500); 12 May 2014 23:39:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42140 invoked by uid 500); 12 May 2014 23:39:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42132 invoked by uid 99); 12 May 2014 23:39:56 -0000
Received: from Unknown (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 23:39:56 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.173] (HELO mail-qc0-f173.google.com) (209.85.216.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 23:39:52 +0000
Received: by mail-qc0-f173.google.com with SMTP id i8so8767999qcq.32
        for <dev@spark.apache.org>; Mon, 12 May 2014 16:39:31 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=O2EAwLPnFrsN4WE/uTorSMcuvwWwsYxMDKoFJToLQSo=;
        b=BmqAV8ziNQnDbIfUysSPUX+uwE+0NTH0TcWPbefrIpJWnri6jIz+8ULLKVvSCnF3lq
         jMUrjVGN/CcPMxKnipm1YGpN8cUjAU9vDFePg4IowWAi++3sWrij1XUUQGvoUeeqxULz
         cCZakCYC3cDlgpltFH9sEf9iXQIkCKM/hNW4sPKaOepEn1h8bY5dxLhKCdjhNnxzGsx/
         X9Tl5/7KxmQop12vEb00eO+XxQ/mw7wYntgSebtXo5TElDAm7ULu5ejw8KViqMAkdBHF
         PLW7L1uP5Cw3ZabBGBF4vNJr8ku6u8qAyyoywAazJTlibsQDYkUkKlP7e00kI2QnQg3l
         W2zA==
X-Gm-Message-State: ALoCoQmLK/Kt51c6OEWYd+9Dsc475M84QZ3gzcWM0+H9f7qyrR17ElYj9nQ7G51/5gJxexnRcoVS
X-Received: by 10.224.20.72 with SMTP id e8mr42362251qab.86.1399937971134;
 Mon, 12 May 2014 16:39:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.126.1 with HTTP; Mon, 12 May 2014 16:39:11 -0700 (PDT)
In-Reply-To: <CAFboF2yO2ZduPwCxv_h7ewyvOsGeBCSt897GnHs=u9g6Bju=VQ@mail.gmail.com>
References: <CAFboF2yO2ZduPwCxv_h7ewyvOsGeBCSt897GnHs=u9g6Bju=VQ@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 12 May 2014 19:39:11 -0400
Message-ID: <CAPh_B=b6D2irqADBZYu9tAbXNLCNXNU6ACzVBZfaD4PnF_T2QA@mail.gmail.com>
Subject: Re: Kryo not default?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c1bfea7a35da04f93c7535
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1bfea7a35da04f93c7535
Content-Type: text/plain; charset=UTF-8

The main reason is that it doesn't always work (e.g. sometimes application
program has special serialization / externalization written already for
Java which don't work in Kryo).

On Mon, May 12, 2014 at 5:47 PM, Anand Avati <avati@gluster.org> wrote:

> Hi,
> Can someone share the reason why Kryo serializer is not the default? Is
> there anything to be careful about (because of which it is not enabled by
> default)?
> Thanks!
>

--001a11c1bfea7a35da04f93c7535--

From dev-return-7542-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 02:41:47 2014
Return-Path: <dev-return-7542-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 17B8C11E53
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 02:41:47 +0000 (UTC)
Received: (qmail 71192 invoked by uid 500); 12 May 2014 23:55:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71131 invoked by uid 500); 12 May 2014 23:55:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71123 invoked by uid 99); 12 May 2014 23:55:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 23:55:06 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.53 as permitted sender)
Received: from [209.85.160.53] (HELO mail-pb0-f53.google.com) (209.85.160.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 23:55:01 +0000
Received: by mail-pb0-f53.google.com with SMTP id uo5so235850pbc.26
        for <dev@spark.apache.org>; Mon, 12 May 2014 16:54:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=8DsU1BYwXLG5Mmnk6OjeNV3p4BG/G4citkanz904Zvw=;
        b=0Yj5OBzW3UB+NDJgGUJdOa9cufGRS5AbS1Nlmtbgh/XH61UCZEGD3xaBXoQ5mO4CTO
         nV2nTvYUSVrgt2adt9Rbs985rTUwXD5dyUEN4j/u2OktvwCS4acaxesFxbpYLVXQ3fkI
         NRQwGHf+6Gs2cUR0ol2GWrbk6M16ez0Y+CzRT8dDy1Rv07jcEzMS3jnCxPnmr0z5i39v
         3mSu9rp7280delXin+phM7uYJn+IYlDL3jKyAchZuVoWGYpAkv5tHlvKSxt6mqG21I/Q
         35K+9jmxnbk4jB0F+T/lS9Y5j/b4oGgxfjfC3OZKGSngcIjJruuP8kOv+loQrSpWEhhl
         INXQ==
X-Received: by 10.68.129.132 with SMTP id nw4mr1295506pbb.46.1399938881545;
        Mon, 12 May 2014 16:54:41 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id gn3sm24884395pbc.32.2014.05.12.16.54.40
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 12 May 2014 16:54:40 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: Kryo not default?
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAFboF2yO2ZduPwCxv_h7ewyvOsGeBCSt897GnHs=u9g6Bju=VQ@mail.gmail.com>
Date: Mon, 12 May 2014 16:54:40 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <57107C03-3B2B-4B34-B959-FA80124E6CFF@gmail.com>
References: <CAFboF2yO2ZduPwCxv_h7ewyvOsGeBCSt897GnHs=u9g6Bju=VQ@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

It was just because it might not work with some user data types that are =
Serializable. But we should investigate it, as it=92s the easiest thing =
one can enable to improve performance.

Matei

On May 12, 2014, at 2:47 PM, Anand Avati <avati@gluster.org> wrote:

> Hi,
> Can someone share the reason why Kryo serializer is not the default? =
Is
> there anything to be careful about (because of which it is not enabled =
by
> default)?
> Thanks!


From dev-return-7545-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 03:14:47 2014
Return-Path: <dev-return-7545-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 20E5F11F9A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 03:14:47 +0000 (UTC)
Received: (qmail 52557 invoked by uid 500); 13 May 2014 01:28:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52493 invoked by uid 500); 13 May 2014 01:28:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52485 invoked by uid 99); 13 May 2014 01:28:06 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 01:28:06 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.52 as permitted sender)
Received: from [209.85.220.52] (HELO mail-pa0-f52.google.com) (209.85.220.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 01:28:02 +0000
Received: by mail-pa0-f52.google.com with SMTP id fa1so5423359pad.25
        for <dev@spark.apache.org>; Mon, 12 May 2014 18:27:38 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=UPQbc3VipT9NuDbfki4yTm4nXDk2HdbLmxWgNW1hCIg=;
        b=CotXVtstKfKkCsExUIDoPTFlnsvuQKRg1kmnnA8Y/hEC6bq0qW9n1lA9zqaBm1EuDy
         IT03GZspeWGLpkwkCPBfqkkhXgv+AATvdHDw1HyoGnbvNwP1ALUQJJwbKgl+1q55ULf6
         odZmVNK3UBsnCC1fX9MKssJC92KCb626TdmRLzhqL57ziK8pRllYE/TVI5G9WQYryhQ1
         PT6rRq6jpvp0V0KUjD0xcqwoysuaEt17Fmdxr4mYoBm2396A+y7vxNddo58cZsEL4n+E
         Y9tul0ROaefxw7ELvOBcFqMdi7A/0BUdjlHJVE0zS7rLK20BUj+HHQOgV1/6xyd60orO
         LK2g==
X-Received: by 10.68.134.101 with SMTP id pj5mr1675415pbb.62.1399944458830;
        Mon, 12 May 2014 18:27:38 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id yx3sm25144549pbb.6.2014.05.12.18.27.37
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 12 May 2014 18:27:38 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: Spark on Scala 2.11
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAFboF2ws2tGhYxea54TMFMKREC-n1F0mzD9na6NvZRjbnP4+Gg@mail.gmail.com>
Date: Mon, 12 May 2014 18:27:36 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <4D9AF729-4ACC-4B9E-B2DA-A6AD68C6C9B6@gmail.com>
References: <CAFboF2xJThe+5Jf8eAfvMMRmuUJW=ufXUoOn8XUMrKhktbE2cQ@mail.gmail.com> <D86A1D6E-3345-42E8-A82F-E442D129BEF0@gmail.com> <CAFboF2ws2tGhYxea54TMFMKREC-n1F0mzD9na6NvZRjbnP4+Gg@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

We can build the REPL separately for each version of Scala, or even give =
that package a different name in Scala 2.11.

Scala 2.11=92s REPL actually added two flags, -Yrepl-class-based and =
-Yrepl-outdir, that encompass the two modifications we made to the REPL =
(using classes instead of objects to wrap each line, and grabbing the =
files from some directory). So it might be possible to run it without =
modifications using just a simple wrapper class around it. That would =
definitely simplify things!

BTW did the non-REPL parts run fine on 2.11?

Matei

On May 12, 2014, at 2:09 PM, Anand Avati <avati@gluster.org> wrote:

> Matei,
> Thanks for confirming. I was looking specifically at the REPL part and =
how
> it can be significantly simplified with 2.11 Scala, without having to
> inherit a full copy of a refactored repl inside Spark. I am happy to
> investigate/contribute a simpler 2.11 based REPL if this is were seen =
as a
> priority (1.1 does not seem "too far" away.) However a 2.10 compatible
> cross build would still require a separate (existing) REPL code for =
the
> 2.10 build, no?
>=20
> Thanks.
>=20
> On Sun, May 11, 2014 at 2:08 PM, Matei Zaharia =
<matei.zaharia@gmail.com>wrote:
>=20
>> We do want to support it eventually, possibly as early as Spark 1.1 =
(which
>> we=92d cross-build on Scala 2.10 and 2.11). If someone wants to look =
at it
>> before, feel free to do so! Scala 2.11 is very close to 2.10 so I =
think
>> things will mostly work, except for possibly the REPL (which has =
require
>> porting over code form the Scala REPL in each version).
>>=20
>> Matei
>>=20
>> On May 8, 2014, at 6:33 PM, Anand Avati <avati@gluster.org> wrote:
>>=20
>>> Is there an ongoing effort (or intent) to support Spark on Scala =
2.11?
>>> Approximate timeline?
>>>=20
>>> Thank
>>=20


From dev-return-7541-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 03:54:30 2014
Return-Path: <dev-return-7541-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 38351110D4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 03:54:30 +0000 (UTC)
Received: (qmail 64444 invoked by uid 500); 12 May 2014 23:54:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64381 invoked by uid 500); 12 May 2014 23:54:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64373 invoked by uid 99); 12 May 2014 23:54:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 23:54:30 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.48 as permitted sender)
Received: from [209.85.220.48] (HELO mail-pa0-f48.google.com) (209.85.220.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 23:54:25 +0000
Received: by mail-pa0-f48.google.com with SMTP id rd3so9447459pab.35
        for <dev@spark.apache.org>; Mon, 12 May 2014 16:54:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=ubSmxKDNw+ArIGcMDs4vRFMA0nG/x++KKKTjSWYvulc=;
        b=jZFW/zc0sPvN+1gI7AITr4XzveUqYAD7zqasURqGuFAsc5uHSkC+Pa3p8p9cWBtq8g
         SBAnmjIGERG3SFSyHQJGUY5oEP0ys3Ecd9I6TbPo1WAgULY3ReHT6CTHNHm0mvslyOyX
         5SwEVxk3MdhdSKkssGjJB+nDxn/1oSsP1GDnLUJYJgxboQbCUpfGJTjtf69FEaYYWtDx
         z/jmIVbAS9xQVuOG+Oym9gow11jBK3klNHPl8ePytjmyynVgW+Oxu+aeWcxOCJhRmAfx
         aObT5hS2Onfj6pp3eoaOYO/OiW7H6ZPgmwmM7GPyOgP3g3LeUs9SggwBCGg1YLdKCib7
         yrQw==
X-Received: by 10.68.129.34 with SMTP id nt2mr1277003pbb.18.1399938845191;
        Mon, 12 May 2014 16:54:05 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id gn3sm24884395pbc.32.2014.05.12.16.54.02
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 12 May 2014 16:54:03 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: Spark on Scala 2.11
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CACo38_TfLMF4Zv0yBiSgFYCqa_CYwCF1TFC8wHzSqqs-K-KA+g@mail.gmail.com>
Date: Mon, 12 May 2014 16:54:01 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <33548912-E462-4CCA-A229-05B6AFAC1030@gmail.com>
References: <CAFboF2xJThe+5Jf8eAfvMMRmuUJW=ufXUoOn8XUMrKhktbE2cQ@mail.gmail.com> <D86A1D6E-3345-42E8-A82F-E442D129BEF0@gmail.com> <CACo38_TfLMF4Zv0yBiSgFYCqa_CYwCF1TFC8wHzSqqs-K-KA+g@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

Anyone can actually open a JIRA on =
https://issues.apache.org/jira/browse/SPARK. I=92ve created one for this =
now: https://issues.apache.org/jira/browse/SPARK-1812.

Matei

On May 12, 2014, at 3:54 PM, Jacek Laskowski <jacek@japila.pl> wrote:

> On Sun, May 11, 2014 at 11:08 PM, Matei Zaharia =
<matei.zaharia@gmail.com> wrote:
>> We do want to support it eventually, possibly as early as Spark 1.1 =
(which we=92d cross-build on Scala 2.10 and 2.11). If someone wants to =
look at it before, feel free to do so! Scala 2.11 is very close to 2.10 =
so I think things will mostly work, except for possibly the REPL (which =
has require porting over code form the Scala REPL in each version).
>=20
> Hi,
>=20
> Would that be possible to have a JIRA issue for this (so I could have
> a branch for the cross-build in sbt and give the task a try)?
>=20
> Jacek
>=20
> --=20
> Jacek Laskowski | http://blog.japila.pl
> "Never discourage anyone who continually makes progress, no matter how
> slow." Plato


From dev-return-7546-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 04:10:18 2014
Return-Path: <dev-return-7546-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0A78E1114D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 04:10:18 +0000 (UTC)
Received: (qmail 26691 invoked by uid 500); 13 May 2014 03:43:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26611 invoked by uid 500); 13 May 2014 03:43:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26559 invoked by uid 99); 13 May 2014 03:43:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 03:43:37 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.128.181] (HELO mail-ve0-f181.google.com) (209.85.128.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 03:43:32 +0000
Received: by mail-ve0-f181.google.com with SMTP id pa12so10079133veb.12
        for <dev@spark.apache.org>; Mon, 12 May 2014 20:43:11 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=tnN0dVwBKvP92ff5Wnn0R6fnpnzwh8IXgYy4XpF1rAk=;
        b=HFNTKbOxRXA1tyrBZsPALU0AJK33rD36AvIhWVj7IG9pZo4H4pWiWjfMY7zuHFeYtL
         suNki+dHXKNjdAwjBJ0m7nAxbgdtKhejDBnvI5XvOGQ7hXOXmqXeE/kkJrHJ57Ib00h1
         DQO+UqLvqFi5n9vxvBMMq97+rCIHvDwgARh6z9EoV5HIAkXjCLd7/uKkREdhCpYTjBgi
         glDFARdrnMM1ffdWpWRxXnPNYv5vrC45EfOZxlkZon5+TUPLpAE8kp7D6vCE1dG6QVHK
         aMWNA/QTHQ17Dq11NGJFA99d6P1wKwltMUzHXC9W/CW5xhvgohE4928XpVpYIsdrsZ1L
         utow==
X-Gm-Message-State: ALoCoQksPjE70RPOT30lww8eUEN36F/nyOnB36P8yL4mnegcTmmzjnIdambx179DC0+/AJilkPPq
X-Received: by 10.220.105.4 with SMTP id r4mr6324450vco.27.1399952591124;
        Mon, 12 May 2014 20:43:11 -0700 (PDT)
Received: from mail-ve0-f171.google.com (mail-ve0-f171.google.com [209.85.128.171])
        by mx.google.com with ESMTPSA id r10sm16284953veh.8.2014.05.12.20.43.09
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 12 May 2014 20:43:09 -0700 (PDT)
Received: by mail-ve0-f171.google.com with SMTP id oz11so10120536veb.30
        for <dev@spark.apache.org>; Mon, 12 May 2014 20:43:09 -0700 (PDT)
X-Received: by 10.52.37.130 with SMTP id y2mr3998673vdj.38.1399952589504; Mon,
 12 May 2014 20:43:09 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Mon, 12 May 2014 20:42:49 -0700 (PDT)
In-Reply-To: <CAAsvFP=9PWfNP3ZQ4sXJV+R4VUpV3WzO2=eE+WrxDPDTs0rtpg@mail.gmail.com>
References: <CAAsvFP=9PWfNP3ZQ4sXJV+R4VUpV3WzO2=eE+WrxDPDTs0rtpg@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 12 May 2014 20:42:49 -0700
Message-ID: <CA+-p3AHh+Y_VWpM18J=BPj8+o6KQ5Wxd6Jhw-mfUMmhYyZveNA@mail.gmail.com>
Subject: Re: Any ideas on SPARK-1021?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=bcaec51d20e0ccc96504f93fdc14
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec51d20e0ccc96504f93fdc14
Content-Type: text/plain; charset=UTF-8

This is the issue where .sortByKey() launches a cluster job when it
shouldn't because it's a transformation not an action.

https://issues.apache.org/jira/browse/SPARK-1021

I'd appreciate a fix too but don't currently have any thoughts on how to
proceed forward.

Andrew


On Thu, May 8, 2014 at 2:16 PM, Mark Hamstra <mark@clearstorydata.com>wrote:

> I'm trying to decide whether attacking the underlying issue of
> RangePartitioner running eager jobs in rangeBounds (i.e. SPARK-1021) is a
> better option than a messy workaround for some async job-handling stuff
> that I am working on.  It looks like there have been a couple of aborted
> attempts to solve the problem, but no solution or clear path to one at this
> point.
>
> Has anybody made any further progress or does anyone have any new ideas on
> how to proceed?
>

--bcaec51d20e0ccc96504f93fdc14--

From dev-return-7547-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 04:30:51 2014
Return-Path: <dev-return-7547-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B3206111B6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 04:30:51 +0000 (UTC)
Received: (qmail 19508 invoked by uid 500); 13 May 2014 04:24:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19452 invoked by uid 500); 13 May 2014 04:24:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19444 invoked by uid 99); 13 May 2014 04:24:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 04:24:10 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.178] (HELO mail-vc0-f178.google.com) (209.85.220.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 04:24:05 +0000
Received: by mail-vc0-f178.google.com with SMTP id hq16so7156309vcb.37
        for <dev@spark.apache.org>; Mon, 12 May 2014 21:23:44 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=hkv8N/4+rBnDiVDL160UlbpipCYqCbuWD+LUecHpqrw=;
        b=HcUVoCH5H1vSxT0W2mnELryUQGVD9jgznURqsnZ8ZZW12H1HBegWdNXb8nGeZ5Sj0+
         Z4cw2woKy/NSHTQsvlBoYExcnWfUvcLKdAROBnkwJ3VrDW/1ZDJapwLzxa7ovPinkFdR
         B30X2IuOfh5Zb5LsG+uvlldc/ShOwFQYZyxVxf1qhawkWqyLHsw+xOrvpAZYqnTYXVUu
         FjL17P/xWJBwaA5djfFmng8/v62+qR9ixzQ40hcoLQr/Gz+Eu2IcxmK8SCihvSib90eN
         Wj+G6BJqEQWoIQIJ9kr5bj+34Xmxk4lkPC4AJ0TR8l+ifHhofX1EALT7zc6rIv0gqGxy
         UPvg==
X-Gm-Message-State: ALoCoQkmemh5RpszbPY6jtDJBmdrE7ClC7xWulgTdeSDq4I/JJoAoICLvI6Q50M3MGWhfv20K1xB
X-Received: by 10.220.92.135 with SMTP id r7mr26663218vcm.11.1399955024482;
        Mon, 12 May 2014 21:23:44 -0700 (PDT)
Received: from mail-vc0-f176.google.com (mail-vc0-f176.google.com [209.85.220.176])
        by mx.google.com with ESMTPSA id l4sm26537417vdb.3.2014.05.12.21.23.43
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 12 May 2014 21:23:43 -0700 (PDT)
Received: by mail-vc0-f176.google.com with SMTP id lg15so9922328vcb.7
        for <dev@spark.apache.org>; Mon, 12 May 2014 21:23:42 -0700 (PDT)
X-Received: by 10.220.163.3 with SMTP id y3mr26777937vcx.7.1399955022895; Mon,
 12 May 2014 21:23:42 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Mon, 12 May 2014 21:23:20 -0700 (PDT)
In-Reply-To: <CAJ3iqPQxeOKuWg2gz_zkitVy-8ZEYgCzy9O_5op2F4wh+j6XsA@mail.gmail.com>
References: <CAJ3iqPQxeOKuWg2gz_zkitVy-8ZEYgCzy9O_5op2F4wh+j6XsA@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 12 May 2014 21:23:20 -0700
Message-ID: <CA+-p3AGo2YC-Ez=owoA1ScHTBuK2q6QA7=cUBVd0sDy9mjJ3YQ@mail.gmail.com>
Subject: Re: Requirements of objects stored in RDDs
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1133da66d760c004f9406d49
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133da66d760c004f9406d49
Content-Type: text/plain; charset=UTF-8

An RDD can hold objects of any type.  If you generally think of it as a
distributed Collection, then you won't ever be that far off.

As far as serialization, the contents of an RDD must be serializable.
 There are two serialization libraries you can use with Spark: normal Java
serialization or Kryo serialization.  See
https://spark.apache.org/docs/latest/tuning.html#data-serialization for
more details.

If you are using Java serialization then just implementing the Serializable
interface will work.  If you're using Kryo, then

The point that it works fine with local mode and tests but fails in Mesos,
that makes me think there's an issue with the Mesos cluster deployment.
 First, does it work properly in standalone mode?  Second, how are you
getting the Clojure libraries onto the Mesos executors?  Are they included
in your executor URI bundle, or otherwise passing a parameter that points
to the clojure jars?

Cheers,
Andrew


On Thu, May 8, 2014 at 9:55 AM, Soren Macbeth <soren@yieldbot.com> wrote:

> Hi,
>
> What are the requirements of objects that are stored in RDDs?
>
> I'm still struggling with an exception I've already posted about several
> times. My questions are:
>
> 1) What interfaces are objects stored in RDDs expected to implement, if
> any?
> 2) Are collections (be they scala, java or otherwise) handled differently
> than other objects?
>
> The bug I'm hitting is when I try to use my clojure DSL (which wraps the
> java api) with clojure collections, specifically
> clojure.lang.PersistentVectors in my RDDs. Here is the exception message:
>
> org.apache.spark.SparkException: Job aborted: Exception while deserializing
> and fetching task: com.esotericsoftware.kryo.KryoException:
> java.lang.IllegalArgumentException: Can not set final scala.collecti
> on.convert.Wrappers field
> scala.collection.convert.Wrappers$SeqWrapper.$outer to
> clojure.lang.PersistentVector
>
> Now, this same application works fine in local mode and tests, but it fails
> when run under mesos. That would seem to me to point to something around
> RDD partitioning for tasks, but I'm not sure.
>
> I don't know much scala, but according to google, SeqWrapper is part of the
> implicit JavaConversion functionality of scala collections. Under what
> circumstances would spark be trying to wrap my RDD objects in scala
> collections?
>
> Finally - I'd like to point out that this is not a serialization issue with
> my clojure collection objects. I have registered serializers for them and
> have verified they serialize and deserialize perfectly well in spark.
>
> One last note is that this failure occurs after all the tasks for finished
> for a reduce stage and the results are returned to the driver.
>
> TIA
>

--001a1133da66d760c004f9406d49--

From dev-return-7548-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 04:38:28 2014
Return-Path: <dev-return-7548-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 24198111D8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 04:38:28 +0000 (UTC)
Received: (qmail 37144 invoked by uid 500); 13 May 2014 04:31:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37089 invoked by uid 500); 13 May 2014 04:31:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37081 invoked by uid 99); 13 May 2014 04:31:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 04:31:48 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of anand.avati@gmail.com designates 209.85.219.41 as permitted sender)
Received: from [209.85.219.41] (HELO mail-oa0-f41.google.com) (209.85.219.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 04:31:43 +0000
Received: by mail-oa0-f41.google.com with SMTP id m1so9498753oag.0
        for <dev@spark.apache.org>; Mon, 12 May 2014 21:31:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=rwfUoW6xINehHO3SaiMymoEPsct1MtCJiQHBx9gqJjg=;
        b=gN5OxpQrEAAceFSpFeWoC5BdyzhEne/pGMHfDMHJpZhTSQoYKpPJkK4S23eVteVujn
         AWSLPXzdHXUuz6Gs1jkJTTfD4XfUspBRS6usoRlhD86H23mltkVazAsr75DtAW5zTsnj
         aUENdmQOw2/en686tQ0wkieeyoXkFgrMKJFVmsAgOEBHjTMEcBCMt5Qv5TkiDkcMPM14
         SFHU6sXrx86/rOIKZ7PZXTYplQ1D8oNx9Jx6dm860YSA+eHykENqz+EYIv5UOhI6CDVV
         nNpRJxZffHO2UwoPp7kOAkpnd+ls3SeG/4lgQxZsAdWiPEH6usCzm6G0TNvFQvn66/Za
         DXMQ==
MIME-Version: 1.0
X-Received: by 10.60.159.5 with SMTP id wy5mr9382052oeb.58.1399955483271; Mon,
 12 May 2014 21:31:23 -0700 (PDT)
Sender: anand.avati@gmail.com
Received: by 10.60.165.68 with HTTP; Mon, 12 May 2014 21:31:23 -0700 (PDT)
In-Reply-To: <4D9AF729-4ACC-4B9E-B2DA-A6AD68C6C9B6@gmail.com>
References: <CAFboF2xJThe+5Jf8eAfvMMRmuUJW=ufXUoOn8XUMrKhktbE2cQ@mail.gmail.com>
	<D86A1D6E-3345-42E8-A82F-E442D129BEF0@gmail.com>
	<CAFboF2ws2tGhYxea54TMFMKREC-n1F0mzD9na6NvZRjbnP4+Gg@mail.gmail.com>
	<4D9AF729-4ACC-4B9E-B2DA-A6AD68C6C9B6@gmail.com>
Date: Mon, 12 May 2014 21:31:23 -0700
X-Google-Sender-Auth: ZiULCfdTxURnqqtHR7VLirFw-Ss
Message-ID: <CAFboF2wc_S45WbfTcJHHUyRRmqccOf5Gom49EdMudKkX8pAkbg@mail.gmail.com>
Subject: Re: Spark on Scala 2.11
From: Anand Avati <avati@gluster.org>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bd7649e4828c704f9408909
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd7649e4828c704f9408909
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

On Mon, May 12, 2014 at 6:27 PM, Matei Zaharia <matei.zaharia@gmail.com>wro=
te:

> We can build the REPL separately for each version of Scala, or even give
> that package a different name in Scala 2.11.
>

OK.


> Scala 2.11=E2=80=99s REPL actually added two flags, -Yrepl-class-based an=
d
> -Yrepl-outdir, that encompass the two modifications we made to the REPL
> (using classes instead of objects to wrap each line, and grabbing the fil=
es
> from some directory). So it might be possible to run it without
> modifications using just a simple wrapper class around it. That would
> definitely simplify things!
>

Exactly. I have been tracking those changes in 2.11 as well. We would need
a simple wrapper around ILoop, set the Yreplclassbased flag, an HttpServer
to export the repl-outdir, bind sc into repl namespace and maybe little
more initialization (customizing repl init code in 2.11 is not as trivial
as the <=3D2.10 versions, but still doable.) Still, even with all this, it
should be much simpler than pulling in and refactoring everything like
today. I already have a prototype for this in my working tree, still needs
integration testing.


> BTW did the non-REPL parts run fine on 2.11?
>

Currently fighting to get all the dependencies in 2.11. Quick pointer where
I can get sources for akka-*-X.Y-shared-protobuf? Also, what's the smallest
set of dependencies to build the smallest testable subset of the project?

Thanks!


> Matei
>
> On May 12, 2014, at 2:09 PM, Anand Avati <avati@gluster.org> wrote:
>
> > Matei,
> > Thanks for confirming. I was looking specifically at the REPL part and
> how
> > it can be significantly simplified with 2.11 Scala, without having to
> > inherit a full copy of a refactored repl inside Spark. I am happy to
> > investigate/contribute a simpler 2.11 based REPL if this is were seen a=
s
> a
> > priority (1.1 does not seem "too far" away.) However a 2.10 compatible
> > cross build would still require a separate (existing) REPL code for the
> > 2.10 build, no?
> >
> > Thanks.
> >
> > On Sun, May 11, 2014 at 2:08 PM, Matei Zaharia <matei.zaharia@gmail.com
> >wrote:
> >
> >> We do want to support it eventually, possibly as early as Spark 1.1
> (which
> >> we=E2=80=99d cross-build on Scala 2.10 and 2.11). If someone wants to =
look at it
> >> before, feel free to do so! Scala 2.11 is very close to 2.10 so I thin=
k
> >> things will mostly work, except for possibly the REPL (which has requi=
re
> >> porting over code form the Scala REPL in each version).
> >>
> >> Matei
> >>
> >> On May 8, 2014, at 6:33 PM, Anand Avati <avati@gluster.org> wrote:
> >>
> >>> Is there an ongoing effort (or intent) to support Spark on Scala 2.11=
?
> >>> Approximate timeline?
> >>>
> >>> Thank
> >>
>
>

--047d7bd7649e4828c704f9408909--

From dev-return-7551-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 05:05:56 2014
Return-Path: <dev-return-7551-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 925B81129B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 05:05:56 +0000 (UTC)
Received: (qmail 86913 invoked by uid 500); 13 May 2014 04:58:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86857 invoked by uid 500); 13 May 2014 04:58:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86849 invoked by uid 99); 13 May 2014 04:58:35 -0000
Received: from Unknown (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 04:58:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.177] (HELO mail-vc0-f177.google.com) (209.85.220.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 04:58:31 +0000
Received: by mail-vc0-f177.google.com with SMTP id if17so5559512vcb.36
        for <dev@spark.apache.org>; Mon, 12 May 2014 21:58:10 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=HXyydxt9V9va8XR6MFo5XyuDKPJQya9F6o0AOxFJFlE=;
        b=HUOTWJFEfUgWRZbFpiF7k+UOhxSSvq+y1Bv8FVGd87X1GCZXTwYThZEaUHSKhRUsv3
         tgp1DDS9ljH2mkrso+CiIo8d873pLFqafg758aZfxKfaSdKLnXvu9xSqOFlrLp6HqkqR
         RvNVkbw5wcMX0JprQklt/jQVhaGvqYBl0N3+52yHgLnPxOGfkW0o4+tI5z5+P2/pkiWR
         XNeWXg7suz2gN8V0LAP8hdfL00Rf5mlUDd2kiiYEnJMkhuXJIhABibz+7YinZyJ/Zfo+
         kc9iRyt6r1cVwPncfcggKaY8gwfAOVIJONUIF7thQnAVrMfZcJPLZ5f17A27Ykht8mLx
         rRfw==
X-Gm-Message-State: ALoCoQlHb2JLx8SdFwVZV4ajSWK/E90Ohf4nrx7SUW2TjEUcBjt9mNdWJZAZyh7Pm8LS11lbbPoN
X-Received: by 10.52.183.228 with SMTP id ep4mr6012880vdc.30.1399957090432;
        Mon, 12 May 2014 21:58:10 -0700 (PDT)
Received: from mail-ve0-f169.google.com (mail-ve0-f169.google.com [209.85.128.169])
        by mx.google.com with ESMTPSA id bk6sm16457079vec.6.2014.05.12.21.58.09
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 12 May 2014 21:58:09 -0700 (PDT)
Received: by mail-ve0-f169.google.com with SMTP id jx11so10268537veb.0
        for <dev@spark.apache.org>; Mon, 12 May 2014 21:58:09 -0700 (PDT)
X-Received: by 10.220.105.4 with SMTP id r4mr6632579vco.27.1399957089091; Mon,
 12 May 2014 21:58:09 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Mon, 12 May 2014 21:57:49 -0700 (PDT)
In-Reply-To: <CABPQxsvrhG5FX2d5VuBPZ3T5CuyBms5f+E5SFVOo2bzmB9Ay9A@mail.gmail.com>
References: <CA+-p3AGGAKA05e9MpvOgQpmj=opOgvw+zXVZVojYU8Yo9XZ4tg@mail.gmail.com>
 <CABPQxsvrhG5FX2d5VuBPZ3T5CuyBms5f+E5SFVOo2bzmB9Ay9A@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 12 May 2014 21:57:49 -0700
Message-ID: <CA+-p3AG7VUZB45TadPJRvqCt=X6zY4Zip-7UogsyVCswsjkQFw@mail.gmail.com>
Subject: Re: Updating docs for running on Mesos
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0122f332ff079404f940e8c8
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0122f332ff079404f940e8c8
Content-Type: text/plain; charset=UTF-8

As far as I know, the upstream doesn't release binaries, only source code.
 The downloads page <https://mesos.apache.org/downloads/> for 0.18.0 only
has a source tarball.  Is there a binary release somewhere from Mesos that
I'm missing?


On Sun, May 11, 2014 at 2:16 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Andrew,
>
> Updating these docs would be great! I think this would be a welcome change.
>
> In terms of packaging, it would be good to mention the binaries
> produced by the upstream project as well, in addition to Mesosphere.
>
> - Patrick
>
> On Thu, May 8, 2014 at 12:51 AM, Andrew Ash <andrew@andrewash.com> wrote:
> > The docs for how to run Spark on Mesos have changed very little since
> > 0.6.0, but setting it up is much easier now than then.  Does it make
> sense
> > to revamp with the below changes?
> >
> >
> > You no longer need to build mesos yourself as pre-built versions are
> > available from Mesosphere: http://mesosphere.io/downloads/
> >
> > And the instructions guide you towards compiling your own distribution of
> > Spark, when you can use the prebuilt versions of Spark as well.
> >
> >
> > I'd like to split that portion of the documentation into two sections, a
> > build-from-scratch section and a use-prebuilt section.  The new outline
> > would look something like this:
> >
> >
> > *Running Spark on Mesos*
> >
> > Installing Mesos
> > - using prebuilt (recommended)
> >  - pointer to mesosphere's packages
> > - from scratch
> >  - (similar to current)
> >
> >
> > Connecting Spark to Mesos
> > - loading distribution into an accessible location
> > - Spark settings
> >
> > Mesos Run Modes
> > - (same as current)
> >
> > Running Alongside Hadoop
> > - (trim this down)
> >
> >
> >
> > Does that work for people?
> >
> >
> > Thanks!
> > Andrew
> >
> >
> > PS Basically all the same:
> >
> > http://spark.apache.org/docs/0.6.0/running-on-mesos.html
> > http://spark.apache.org/docs/0.6.2/running-on-mesos.html
> > http://spark.apache.org/docs/0.7.3/running-on-mesos.html
> > http://spark.apache.org/docs/0.8.1/running-on-mesos.html
> > http://spark.apache.org/docs/0.9.1/running-on-mesos.html
> >
> https://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/running-on-mesos.html
>

--089e0122f332ff079404f940e8c8--

From dev-return-7550-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 05:57:15 2014
Return-Path: <dev-return-7550-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 74C50113D7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 05:57:15 +0000 (UTC)
Received: (qmail 67782 invoked by uid 500); 13 May 2014 04:57:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67723 invoked by uid 500); 13 May 2014 04:57:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67715 invoked by uid 99); 13 May 2014 04:57:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 04:57:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.128.170] (HELO mail-ve0-f170.google.com) (209.85.128.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 04:57:10 +0000
Received: by mail-ve0-f170.google.com with SMTP id db11so10015241veb.15
        for <dev@spark.apache.org>; Mon, 12 May 2014 21:56:49 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=GSAUKGZ+A0RQZ02PX3bBZoPVGNNMx0hzSDlau8y1UkY=;
        b=PDcVgNyasXP7ljRlZamfvizwSvsRG3FDbrf6pkp4YbJ4JrnMb+oNGPokrbYjDMiKM9
         RQae9xAs0023aqZWPYMaYzdepSNzaXQAVKU6+CsKncQB1YU04Wrb6KAnf9Ti9gAqTNNC
         HY6GdKaGNqerZmo43AJoKHdV141cn0ct0qElBRI1ClBNYXp2hjDAK5QZ4bmCDEF9lCUI
         jsUKX364i8lH3v5yB6MJQFwQ49S+3bTBUrAbTWdeLNXT08ZrEqI6GyzMEck+xVRFb19i
         D+JdLY8wU8+YIzAU7p8/RVVMwLD7t6975NXOA5rDhgt3EcPFCn/MNk2lxUDQih7ki5vo
         syng==
X-Gm-Message-State: ALoCoQnby9gTHopHk3AK5+RlFUB3wQ/9DDP5dx7g3GbTAW7QGdJANvmmNVyhhD6PocTqIPuWNd18
X-Received: by 10.52.248.41 with SMTP id yj9mr16351674vdc.22.1399957009756;
        Mon, 12 May 2014 21:56:49 -0700 (PDT)
Received: from mail-ve0-f179.google.com (mail-ve0-f179.google.com [209.85.128.179])
        by mx.google.com with ESMTPSA id v7sm26648194vdj.7.2014.05.12.21.56.48
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 12 May 2014 21:56:48 -0700 (PDT)
Received: by mail-ve0-f179.google.com with SMTP id oy12so9909504veb.24
        for <dev@spark.apache.org>; Mon, 12 May 2014 21:56:48 -0700 (PDT)
X-Received: by 10.52.232.161 with SMTP id tp1mr4858936vdc.33.1399957008159;
 Mon, 12 May 2014 21:56:48 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Mon, 12 May 2014 21:56:28 -0700 (PDT)
In-Reply-To: <CALEZFQx--MJTQ5vi14ELUyb1XUUwRJMaH19cX3XEsvO4ZN4gjA@mail.gmail.com>
References: <CA+-p3AGGAKA05e9MpvOgQpmj=opOgvw+zXVZVojYU8Yo9XZ4tg@mail.gmail.com>
 <CALEZFQx--MJTQ5vi14ELUyb1XUUwRJMaH19cX3XEsvO4ZN4gjA@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 12 May 2014 21:56:28 -0700
Message-ID: <CA+-p3AGAReWPCndR2+YscJS3KS5rV-EhkSoCsy7307-eQjouBg@mail.gmail.com>
Subject: Re: Updating docs for running on Mesos
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0112ce182c1d9c04f940e46b
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0112ce182c1d9c04f940e46b
Content-Type: text/plain; charset=UTF-8

For trimming the Running Alongside Hadoop section I mostly think there
should be a separate Spark+HDFS section and have the CDH+HDP page be merged
into that one, but I supposed that's a separate docs change.


On Sun, May 11, 2014 at 4:28 PM, Andy Konwinski <andykonwinski@gmail.com>wrote:

> Thanks for suggesting this and volunteering to do it.
>
> On May 11, 2014 3:32 AM, "Andrew Ash" <andrew@andrewash.com> wrote:
> >
> > The docs for how to run Spark on Mesos have changed very little since
> > 0.6.0, but setting it up is much easier now than then.  Does it make
> sense
> > to revamp with the below changes?
> >
> >
> > You no longer need to build mesos yourself as pre-built versions are
> > available from Mesosphere: http://mesosphere.io/downloads/
> >
> > And the instructions guide you towards compiling your own distribution of
> > Spark, when you can use the prebuilt versions of Spark as well.
> >
> >
> > I'd like to split that portion of the documentation into two sections, a
> > build-from-scratch section and a use-prebuilt section.  The new outline
> > would look something like this:
> >
> >
> > *Running Spark on Mesos*
> >
> > Installing Mesos
> > - using prebuilt (recommended)
> >  - pointer to mesosphere's packages
> > - from scratch
> >  - (similar to current)
> >
> >
> > Connecting Spark to Mesos
> > - loading distribution into an accessible location
> > - Spark settings
> >
> > Mesos Run Modes
> > - (same as current)
> >
> > Running Alongside Hadoop
> > - (trim this down)
>
> What trimming do you have in mind here?
>
> >
> >
> >
> > Does that work for people?
> >
> >
> > Thanks!
> > Andrew
> >
> >
> > PS Basically all the same:
> >
> > http://spark.apache.org/docs/0.6.0/running-on-mesos.html
> > http://spark.apache.org/docs/0.6.2/running-on-mesos.html
> > http://spark.apache.org/docs/0.7.3/running-on-mesos.html
> > http://spark.apache.org/docs/0.8.1/running-on-mesos.html
> > http://spark.apache.org/docs/0.9.1/running-on-mesos.html
> >
>
> https://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/running-on-mesos.html
>

--089e0112ce182c1d9c04f940e46b--

From dev-return-7549-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 06:33:47 2014
Return-Path: <dev-return-7549-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3DCF3114C1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 06:33:47 +0000 (UTC)
Received: (qmail 51313 invoked by uid 500); 13 May 2014 04:47:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51253 invoked by uid 500); 13 May 2014 04:47:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51244 invoked by uid 99); 13 May 2014 04:47:05 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 04:47:05 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.128.175] (HELO mail-ve0-f175.google.com) (209.85.128.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 04:46:59 +0000
Received: by mail-ve0-f175.google.com with SMTP id jw12so9934666veb.6
        for <dev@spark.apache.org>; Mon, 12 May 2014 21:46:35 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=rr8S6eqQxkdHZ4t5wKBEQrl2NPUWs8hjl1F5JtU89xI=;
        b=PsaO1gHyeoZcoK97NhvL9NRYdFG8uqhjI4oHrg1KtnsN17RhGeBz65CFw3/GGZ1iLU
         weYSHYI6YNr5NMWcJxTKdTHprIWSELpXW5r0gxz8TzCfB8cwbPx7S+MvguucFXIPhMMN
         1tzuh+U0rT3mdiPPQQL8KDJdqcQUORNk/huz4ehDSnN8dZL7V+W6X7k9LGc0P8/17YP1
         bE1AVsoEO4KgEhhiWso75gMxvvFscIVi1Srqv+aDIVKqN4R0etQA3chN4elR1JRny5cY
         wXmzD8Cp2rgKz/F7RjCQSdb1rgn/RqMKcFF3IakId9MkRV97HlBn1G9qFP1TQmZuj4So
         qSSA==
X-Gm-Message-State: ALoCoQlflB4VtarIi/hGb6rehhrKONAdQQgH71Ee3Gf/AJl6tIuzPo8M7JeiuqCl3ARiBxH4Y8DV
X-Received: by 10.52.13.41 with SMTP id e9mr22822468vdc.21.1399956395329;
        Mon, 12 May 2014 21:46:35 -0700 (PDT)
Received: from mail-ve0-f177.google.com (mail-ve0-f177.google.com [209.85.128.177])
        by mx.google.com with ESMTPSA id us17sm26590114vdb.19.2014.05.12.21.46.34
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 12 May 2014 21:46:34 -0700 (PDT)
Received: by mail-ve0-f177.google.com with SMTP id db11so9881991veb.8
        for <dev@spark.apache.org>; Mon, 12 May 2014 21:46:33 -0700 (PDT)
X-Received: by 10.52.248.41 with SMTP id yj9mr16317078vdc.22.1399956393860;
 Mon, 12 May 2014 21:46:33 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Mon, 12 May 2014 21:46:13 -0700 (PDT)
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 12 May 2014 21:46:13 -0700
Message-ID: <CA+-p3AHXQZttzFG_TC0P2abupPthPY0EhcSxDAsa90Rvu45qFg@mail.gmail.com>
Subject: Preliminary Parquet numbers and including .count() in Catalyst
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113602aa8ea6c604f940bf9e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113602aa8ea6c604f940bf9e
Content-Type: text/plain; charset=UTF-8

Hi Spark devs,

First of all, huge congrats on the parquet integration with SparkSQL!  This
is an incredible direction forward and something I can see being very
broadly useful.

I was doing some preliminary tests to see how it works with one of my
workflows, and wanted to share some numbers that people might want to know
about.

I also wanted to point out that .count() doesn't seem integrated with the
rest of the optimization framework, and some big gains could be possible.


So, the numbers:

I took a table extracted from a SQL database and stored in HDFS:

   - 115 columns (several always-empty, mostly strings, some enums, some
   numbers)
   - 253,887,080 rows
   - 182,150,295,881 bytes (raw uncompressed)
   - 42,826,820,222 bytes (lzo compressed with .index file)

And I converted it to Parquet using SparkSQL's SchemaRDD.saveAsParquet()
call:

   - Converting from .lzo in HDFS to .parquet in HDFS took 635s using 42
   cores across 4 machines
   - 17,517,922,117 bytes (parquet per SparkSQL defaults)

So storing in parquet format vs lzo compresses the data down to less than
50% of the .lzo size, and under 10% of the raw uncompressed size.  Nice!


I then did some basic interactions on it:

*Row count*

   - LZO
      - lzoFile("/path/to/lzo").count
      - 31.632305953s
   - Parquet
      - sqlContext.parquetFile("/path/to/parquet").count
      - 289.129487003s

Reassembling rows from the separate column storage is clearly really
expensive.  Median task length is 33s vs 4s, and of that 33s in each task
(319 tasks total) about 1.75 seconds are spent in GC (inefficient object
allocation?)



*Count number of rows with a particular key:*

   - LZO
   - lzoFile("/path/to/lzo").filter(_.split("\\|")(0) == "1234567890").count
      - 73.988897511s
       - Parquet
   - sqlContext.parquetFile("/path/to/parquet").where('COL ===
      1234567890).count
      - 293.410470418s
       - Parquet (hand-tuned to count on just one column)
   - sqlContext.parquetFile("/path/to/parquet").where('COL ===
      1234567890).select('IDCOL).count
      - 1.160449187s

It looks like currently the .count() on parquet is handled incredibly
inefficiently and all the columns are materialized.  But if I select just
that relevant column and then count, then the column-oriented storage of
Parquet really shines.

There ought to be a potential optimization here such that a .count() on a
SchemaRDD backed by Parquet doesn't require re-assembling the rows, as
that's expensive.  I don't think .count() is handled specially in
SchemaRDDs, but it seems ripe for optimization.


*Count number of distinct values in a column*

   - LZO
   - lzoFile("/path/to/lzo").map(sel(0)).distinct.count
      - 115.582916866s
       - Parquet
   - sqlContext.parquetFile("/path/to/parquet").select('COL).distinct.count
      - 16.839004826 s

It turns out column selectivity is very useful!  I'm guessing that if I
could get byte counts read out of HDFS, that would just about match up with
the difference in read times.




Any thoughts on how to embed the knowledge of my hand-tuned additional
.select('IDCOL)
into Catalyst?


Thanks again for all the hard work and prep for the 1.0 release!

Andrew

--001a113602aa8ea6c604f940bf9e--

From dev-return-7553-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 06:49:26 2014
Return-Path: <dev-return-7553-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 70CCE11523
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 06:49:26 +0000 (UTC)
Received: (qmail 71692 invoked by uid 500); 13 May 2014 06:42:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71634 invoked by uid 500); 13 May 2014 06:42:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71626 invoked by uid 99); 13 May 2014 06:42:44 -0000
Received: from Unknown (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 06:42:44 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.41] (HELO mail-qg0-f41.google.com) (209.85.192.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 06:42:39 +0000
Received: by mail-qg0-f41.google.com with SMTP id j5so9004713qga.14
        for <dev@spark.apache.org>; Mon, 12 May 2014 23:42:19 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=lDQx74nBkJM4cz8YSsCs7g3aMmitO8zzQi8r3mXfX4Y=;
        b=Y7fT9HQGm0wHvefJBSE63+tSBZocM+mEQvVseUtShhllmW20201LavLPQEE/DrjYji
         XFYjInDuEnbw9+rjp0Z6iJB2+SjlIA3eqfkTarfekdmlvzzd4aIQ+p1wDDPl4EqYeYUu
         Kpjtov5ATEOUX3yLY4kgxLPH5mOZadOahTIee64qJhnd/1HREnvE5UQ8EPWnnhIY/Yt2
         ZiVXyDA6xAobWVUQmZ+suNcOz7Co7La5UKM8Fxl0FfvIx8jrHPqgaNQd2BWaTo3dn1D8
         MEPK0SEz18R+DNcKwpgFsKGJBNsfChDyX9eKDL5Dy9n2C42lrOD/FkDgPc6sBNex1iSp
         Bw6g==
X-Gm-Message-State: ALoCoQkwZFNn6FO/L6EIjY5XTHxLMkoYVKmOB7x2pcmhQDG88M/N/dt93hw73FvrUCCUiABZqIzP
X-Received: by 10.140.97.55 with SMTP id l52mr42516110qge.19.1399963339137;
 Mon, 12 May 2014 23:42:19 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.126.1 with HTTP; Mon, 12 May 2014 23:41:59 -0700 (PDT)
In-Reply-To: <CA+-p3AHXQZttzFG_TC0P2abupPthPY0EhcSxDAsa90Rvu45qFg@mail.gmail.com>
References: <CA+-p3AHXQZttzFG_TC0P2abupPthPY0EhcSxDAsa90Rvu45qFg@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 12 May 2014 23:41:59 -0700
Message-ID: <CAPh_B=a3eOi-VYbCvfCBHYY6t6vcaVWxqjUw=TqUAZp8oMfu4w@mail.gmail.com>
Subject: Re: Preliminary Parquet numbers and including .count() in Catalyst
To: dev@spark.apache.org
Cc: Michael Armbrust <michael@databricks.com>
Content-Type: multipart/alternative; boundary=001a113a98ce8740ae04f9425dd4
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a98ce8740ae04f9425dd4
Content-Type: text/plain; charset=UTF-8

Thanks for the experiments and analysis!

I think Michael already submitted a patch that avoids scanning all columns
for count(*) or count(1).


On Mon, May 12, 2014 at 9:46 PM, Andrew Ash <andrew@andrewash.com> wrote:

> Hi Spark devs,
>
> First of all, huge congrats on the parquet integration with SparkSQL!  This
> is an incredible direction forward and something I can see being very
> broadly useful.
>
> I was doing some preliminary tests to see how it works with one of my
> workflows, and wanted to share some numbers that people might want to know
> about.
>
> I also wanted to point out that .count() doesn't seem integrated with the
> rest of the optimization framework, and some big gains could be possible.
>
>
> So, the numbers:
>
> I took a table extracted from a SQL database and stored in HDFS:
>
>    - 115 columns (several always-empty, mostly strings, some enums, some
>    numbers)
>    - 253,887,080 rows
>    - 182,150,295,881 bytes (raw uncompressed)
>    - 42,826,820,222 bytes (lzo compressed with .index file)
>
> And I converted it to Parquet using SparkSQL's SchemaRDD.saveAsParquet()
> call:
>
>    - Converting from .lzo in HDFS to .parquet in HDFS took 635s using 42
>    cores across 4 machines
>    - 17,517,922,117 bytes (parquet per SparkSQL defaults)
>
> So storing in parquet format vs lzo compresses the data down to less than
> 50% of the .lzo size, and under 10% of the raw uncompressed size.  Nice!
>
>
> I then did some basic interactions on it:
>
> *Row count*
>
>    - LZO
>       - lzoFile("/path/to/lzo").count
>       - 31.632305953s
>    - Parquet
>       - sqlContext.parquetFile("/path/to/parquet").count
>       - 289.129487003s
>
> Reassembling rows from the separate column storage is clearly really
> expensive.  Median task length is 33s vs 4s, and of that 33s in each task
> (319 tasks total) about 1.75 seconds are spent in GC (inefficient object
> allocation?)
>
>
>
> *Count number of rows with a particular key:*
>
>    - LZO
>    - lzoFile("/path/to/lzo").filter(_.split("\\|")(0) ==
> "1234567890").count
>       - 73.988897511s
>        - Parquet
>    - sqlContext.parquetFile("/path/to/parquet").where('COL ===
>       1234567890).count
>       - 293.410470418s
>        - Parquet (hand-tuned to count on just one column)
>    - sqlContext.parquetFile("/path/to/parquet").where('COL ===
>       1234567890).select('IDCOL).count
>       - 1.160449187s
>
> It looks like currently the .count() on parquet is handled incredibly
> inefficiently and all the columns are materialized.  But if I select just
> that relevant column and then count, then the column-oriented storage of
> Parquet really shines.
>
> There ought to be a potential optimization here such that a .count() on a
> SchemaRDD backed by Parquet doesn't require re-assembling the rows, as
> that's expensive.  I don't think .count() is handled specially in
> SchemaRDDs, but it seems ripe for optimization.
>
>
> *Count number of distinct values in a column*
>
>    - LZO
>    - lzoFile("/path/to/lzo").map(sel(0)).distinct.count
>       - 115.582916866s
>        - Parquet
>    - sqlContext.parquetFile("/path/to/parquet").select('COL).distinct.count
>       - 16.839004826 s
>
> It turns out column selectivity is very useful!  I'm guessing that if I
> could get byte counts read out of HDFS, that would just about match up with
> the difference in read times.
>
>
>
>
> Any thoughts on how to embed the knowledge of my hand-tuned additional
> .select('IDCOL)
> into Catalyst?
>
>
> Thanks again for all the hard work and prep for the 1.0 release!
>
> Andrew
>

--001a113a98ce8740ae04f9425dd4--

From dev-return-7554-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 06:50:14 2014
Return-Path: <dev-return-7554-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 682AA11525
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 06:50:14 +0000 (UTC)
Received: (qmail 55730 invoked by uid 500); 13 May 2014 06:50:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55675 invoked by uid 500); 13 May 2014 06:50:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55667 invoked by uid 99); 13 May 2014 06:50:12 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 06:50:12 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.170] (HELO mail-vc0-f170.google.com) (209.85.220.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 06:50:08 +0000
Received: by mail-vc0-f170.google.com with SMTP id lf12so10145281vcb.15
        for <dev@spark.apache.org>; Mon, 12 May 2014 23:49:47 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=gkI+D3NZrEUrNuO60SDDF4G7NnBNWhMXc1wzmEL3WRo=;
        b=Pso32Qr1HV7ixkeDxzu64tdMw4rm0OhzkOmd0mVshxJ/q4Ouqdo1tYPtWdB9ylPZut
         IECbj4T3E4JHL5r+qsb1T4N+PvkGACYixy3Ur/ETv4ktvJ7t5xCKU/uxU+BdRYQyx63y
         cP6GVHfC7NN7inACVDJ8e7zGoZtW/XtI4FsTxailLCSPxWRy9aojKE8mUKNsL+FomdsH
         06JoZfjs+2qwx6y40mEF6Ody8rmIOpx8+mpWzsJ650A/WtlD4kgcEuh8bNJxKzvynNiN
         wVfJGyspoIa/YZ21q3HGTFC7XtfHdvQ40jALkINyYzkDptLeL6AgK/XOsxtER+mmnOrb
         twng==
X-Gm-Message-State: ALoCoQkT8VVd8Fbo5lequTJjGjkx9ce6HF7vQUeMcK3m6DgywCHCjntJS0RIoGF76x6C/B6O1TJY
X-Received: by 10.220.163.3 with SMTP id y3mr27375538vcx.7.1399963787535;
        Mon, 12 May 2014 23:49:47 -0700 (PDT)
Received: from mail-ve0-f173.google.com (mail-ve0-f173.google.com [209.85.128.173])
        by mx.google.com with ESMTPSA id l4sm27067557vdb.3.2014.05.12.23.49.46
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 12 May 2014 23:49:46 -0700 (PDT)
Received: by mail-ve0-f173.google.com with SMTP id pa12so10153611veb.18
        for <dev@spark.apache.org>; Mon, 12 May 2014 23:49:46 -0700 (PDT)
X-Received: by 10.220.59.65 with SMTP id k1mr27150601vch.22.1399963786042;
 Mon, 12 May 2014 23:49:46 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Mon, 12 May 2014 23:49:26 -0700 (PDT)
In-Reply-To: <CA+-p3AG7VUZB45TadPJRvqCt=X6zY4Zip-7UogsyVCswsjkQFw@mail.gmail.com>
References: <CA+-p3AGGAKA05e9MpvOgQpmj=opOgvw+zXVZVojYU8Yo9XZ4tg@mail.gmail.com>
 <CABPQxsvrhG5FX2d5VuBPZ3T5CuyBms5f+E5SFVOo2bzmB9Ay9A@mail.gmail.com> <CA+-p3AG7VUZB45TadPJRvqCt=X6zY4Zip-7UogsyVCswsjkQFw@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 12 May 2014 23:49:26 -0700
Message-ID: <CA+-p3AG+mVHFtek5S0L_kS-FhieBi6WrAF86iMUPQAZwfOwd-w@mail.gmail.com>
Subject: Re: Updating docs for running on Mesos
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c295022a68d504f94278de
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c295022a68d504f94278de
Content-Type: text/plain; charset=UTF-8

I have a draft of my proposed changes here:

https://github.com/apache/spark/pull/756
https://issues.apache.org/jira/browse/SPARK-1818

Thanks!
Andrew


On Mon, May 12, 2014 at 9:57 PM, Andrew Ash <andrew@andrewash.com> wrote:

> As far as I know, the upstream doesn't release binaries, only source code.
>  The downloads page <https://mesos.apache.org/downloads/> for 0.18.0 only
> has a source tarball.  Is there a binary release somewhere from Mesos that
> I'm missing?
>
>
> On Sun, May 11, 2014 at 2:16 PM, Patrick Wendell <pwendell@gmail.com>wrote:
>
>> Andrew,
>>
>> Updating these docs would be great! I think this would be a welcome
>> change.
>>
>> In terms of packaging, it would be good to mention the binaries
>> produced by the upstream project as well, in addition to Mesosphere.
>>
>> - Patrick
>>
>> On Thu, May 8, 2014 at 12:51 AM, Andrew Ash <andrew@andrewash.com> wrote:
>> > The docs for how to run Spark on Mesos have changed very little since
>> > 0.6.0, but setting it up is much easier now than then.  Does it make
>> sense
>> > to revamp with the below changes?
>> >
>> >
>> > You no longer need to build mesos yourself as pre-built versions are
>> > available from Mesosphere: http://mesosphere.io/downloads/
>> >
>> > And the instructions guide you towards compiling your own distribution
>> of
>> > Spark, when you can use the prebuilt versions of Spark as well.
>> >
>> >
>> > I'd like to split that portion of the documentation into two sections, a
>> > build-from-scratch section and a use-prebuilt section.  The new outline
>> > would look something like this:
>> >
>> >
>> > *Running Spark on Mesos*
>> >
>> > Installing Mesos
>> > - using prebuilt (recommended)
>> >  - pointer to mesosphere's packages
>> > - from scratch
>> >  - (similar to current)
>> >
>> >
>> > Connecting Spark to Mesos
>> > - loading distribution into an accessible location
>> > - Spark settings
>> >
>> > Mesos Run Modes
>> > - (same as current)
>> >
>> > Running Alongside Hadoop
>> > - (trim this down)
>> >
>> >
>> >
>> > Does that work for people?
>> >
>> >
>> > Thanks!
>> > Andrew
>> >
>> >
>> > PS Basically all the same:
>> >
>> > http://spark.apache.org/docs/0.6.0/running-on-mesos.html
>> > http://spark.apache.org/docs/0.6.2/running-on-mesos.html
>> > http://spark.apache.org/docs/0.7.3/running-on-mesos.html
>> > http://spark.apache.org/docs/0.8.1/running-on-mesos.html
>> > http://spark.apache.org/docs/0.9.1/running-on-mesos.html
>> >
>> https://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/running-on-mesos.html
>>
>
>

--001a11c295022a68d504f94278de--

From dev-return-7543-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 07:03:15 2014
Return-Path: <dev-return-7543-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 11F0A11577
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 07:03:15 +0000 (UTC)
Received: (qmail 13053 invoked by uid 500); 12 May 2014 23:56:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12993 invoked by uid 500); 12 May 2014 23:56:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12985 invoked by uid 99); 12 May 2014 23:56:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 23:56:35 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [85.128.177.112] (HELO alu112.rev.netart.pl) (85.128.177.112)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 May 2014 23:56:30 +0000
Received: from mail-yh0-f54.google.com (unknown [209.85.213.54])
	by laskowski.nazwa.pl (Postfix) with ESMTP id BFBC81159088
	for <dev@spark.apache.org>; Tue, 13 May 2014 01:56:08 +0200 (CEST)
Received: by mail-yh0-f54.google.com with SMTP id i57so6999774yha.13
        for <dev@spark.apache.org>; Mon, 12 May 2014 16:56:07 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.236.83.17 with SMTP id p17mr45130992yhe.122.1399938967640;
 Mon, 12 May 2014 16:56:07 -0700 (PDT)
Received: by 10.170.56.7 with HTTP; Mon, 12 May 2014 16:56:07 -0700 (PDT)
In-Reply-To: <33548912-E462-4CCA-A229-05B6AFAC1030@gmail.com>
References: <CAFboF2xJThe+5Jf8eAfvMMRmuUJW=ufXUoOn8XUMrKhktbE2cQ@mail.gmail.com>
	<D86A1D6E-3345-42E8-A82F-E442D129BEF0@gmail.com>
	<CACo38_TfLMF4Zv0yBiSgFYCqa_CYwCF1TFC8wHzSqqs-K-KA+g@mail.gmail.com>
	<33548912-E462-4CCA-A229-05B6AFAC1030@gmail.com>
Date: Tue, 13 May 2014 01:56:07 +0200
Message-ID: <CACo38_RXZ-mT8RN84apORk6tL1sQ9qN1PvEby2bwbKyOA=_XyA@mail.gmail.com>
Subject: Re: Spark on Scala 2.11
From: Jacek Laskowski <jacek@japila.pl>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks a lot!

Jacek

On Tue, May 13, 2014 at 1:54 AM, Matei Zaharia <matei.zaharia@gmail.com> wr=
ote:
> Anyone can actually open a JIRA on https://issues.apache.org/jira/browse/=
SPARK. I=E2=80=99ve created one for this now: https://issues.apache.org/jir=
a/browse/SPARK-1812.
>
> Matei
>
> On May 12, 2014, at 3:54 PM, Jacek Laskowski <jacek@japila.pl> wrote:
>
>> On Sun, May 11, 2014 at 11:08 PM, Matei Zaharia <matei.zaharia@gmail.com=
> wrote:
>>> We do want to support it eventually, possibly as early as Spark 1.1 (wh=
ich we=E2=80=99d cross-build on Scala 2.10 and 2.11). If someone wants to l=
ook at it before, feel free to do so! Scala 2.11 is very close to 2.10 so I=
 think things will mostly work, except for possibly the REPL (which has req=
uire porting over code form the Scala REPL in each version).
>>
>> Hi,
>>
>> Would that be possible to have a JIRA issue for this (so I could have
>> a branch for the cross-build in sbt and give the task a try)?
>>
>> Jacek
>>
>> --
>> Jacek Laskowski | http://blog.japila.pl
>> "Never discourage anyone who continually makes progress, no matter how
>> slow." Plato
>



--=20
Jacek Laskowski | http://blog.japila.pl
"Never discourage anyone who continually makes progress, no matter how
slow." Plato

From dev-return-7556-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 08:06:48 2014
Return-Path: <dev-return-7556-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0829D11764
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 08:06:48 +0000 (UTC)
Received: (qmail 1043 invoked by uid 500); 13 May 2014 08:00:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 888 invoked by uid 500); 13 May 2014 08:00:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 777 invoked by uid 99); 13 May 2014 08:00:08 -0000
Received: from Unknown (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 08:00:08 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.49 as permitted sender)
Received: from [209.85.220.49] (HELO mail-pa0-f49.google.com) (209.85.220.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 08:00:03 +0000
Received: by mail-pa0-f49.google.com with SMTP id lj1so9896599pab.8
        for <dev@spark.apache.org>; Tue, 13 May 2014 00:59:43 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=U6VZh9xreV8eIqcqqRXL2oBTQvhenfnBDrusqR7r2gM=;
        b=jY+9KvEGe8sZbZjKXOVYpuJ6hbEb0uItwbXqK02ZU1uItr821QfVLne1juV5/8VbBV
         Sv7r63ALr/FvOUH3WWPdm1FWpDgRSDVD7GDzknQESYyUk92swhZgH04WGBvrSw6BmTSF
         GJ8cvGxuCtMRi1sbk6N6qBABvaHkfyLN3IR91R17UAH58F93Dd1eew1hM60oKOoHI6iZ
         12kz/fUu4zMxX2hfVvl/e2a5rHAJbRMypASO3Sb5MAwXowP2eyfanSx7n3kqxfkNhS5G
         Lj/MsD+tXyxqMOMaCbzCBFwcq7711nmbLyuBnhQ1FdcsRN0GYInnKZKldWygWJXNQeRF
         EpkQ==
X-Received: by 10.68.202.74 with SMTP id kg10mr3403396pbc.163.1399967983341;
        Tue, 13 May 2014 00:59:43 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id iq10sm26774741pbc.14.2014.05.13.00.59.41
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 13 May 2014 00:59:42 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: Updating docs for running on Mesos
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAMc-71=z5eAvdEJ+O2FgbzxNCRDzZWWg9-ZrRKHA-wUOB2dGmQ@mail.gmail.com>
Date: Tue, 13 May 2014 00:59:42 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <95FAD9DA-C2A9-42E6-BDF3-9577B92E9B3A@gmail.com>
References: <CA+-p3AGGAKA05e9MpvOgQpmj=opOgvw+zXVZVojYU8Yo9XZ4tg@mail.gmail.com> <CABPQxsvrhG5FX2d5VuBPZ3T5CuyBms5f+E5SFVOo2bzmB9Ay9A@mail.gmail.com> <CA+-p3AG7VUZB45TadPJRvqCt=X6zY4Zip-7UogsyVCswsjkQFw@mail.gmail.com> <CAMc-71=z5eAvdEJ+O2FgbzxNCRDzZWWg9-ZrRKHA-wUOB2dGmQ@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

I=92ll ask the Mesos folks about this. Unfortunately it might be tough =
to link only to a company=92s builds; but we can perhaps include them in =
addition to instructions for building Mesos from Apache.

Matei

On May 12, 2014, at 11:55 PM, Gerard Maas <gerard.maas@gmail.com> wrote:

> Andrew,
>=20
> Mesosphere has binary releases here:
> http://mesosphere.io/downloads/
>=20
> (Anecdote: I actually burned a CPU building Mesos from source. No =
kidding -
> it was coming, as the laptop was crashing from time to time, but the =
mesos
> build was that one drop too much)
>=20
> kr, Gerard.
>=20
>=20
>=20
> On Tue, May 13, 2014 at 6:57 AM, Andrew Ash <andrew@andrewash.com> =
wrote:
>=20
>> As far as I know, the upstream doesn't release binaries, only source =
code.
>> The downloads page <https://mesos.apache.org/downloads/> for 0.18.0 =
only
>> has a source tarball.  Is there a binary release somewhere from Mesos =
that
>> I'm missing?
>>=20
>>=20
>> On Sun, May 11, 2014 at 2:16 PM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>=20
>>> Andrew,
>>>=20
>>> Updating these docs would be great! I think this would be a welcome
>> change.
>>>=20
>>> In terms of packaging, it would be good to mention the binaries
>>> produced by the upstream project as well, in addition to Mesosphere.
>>>=20
>>> - Patrick
>>>=20
>>> On Thu, May 8, 2014 at 12:51 AM, Andrew Ash <andrew@andrewash.com>
>> wrote:
>>>> The docs for how to run Spark on Mesos have changed very little =
since
>>>> 0.6.0, but setting it up is much easier now than then.  Does it =
make
>>> sense
>>>> to revamp with the below changes?
>>>>=20
>>>>=20
>>>> You no longer need to build mesos yourself as pre-built versions =
are
>>>> available from Mesosphere: http://mesosphere.io/downloads/
>>>>=20
>>>> And the instructions guide you towards compiling your own =
distribution
>> of
>>>> Spark, when you can use the prebuilt versions of Spark as well.
>>>>=20
>>>>=20
>>>> I'd like to split that portion of the documentation into two =
sections,
>> a
>>>> build-from-scratch section and a use-prebuilt section.  The new =
outline
>>>> would look something like this:
>>>>=20
>>>>=20
>>>> *Running Spark on Mesos*
>>>>=20
>>>> Installing Mesos
>>>> - using prebuilt (recommended)
>>>> - pointer to mesosphere's packages
>>>> - from scratch
>>>> - (similar to current)
>>>>=20
>>>>=20
>>>> Connecting Spark to Mesos
>>>> - loading distribution into an accessible location
>>>> - Spark settings
>>>>=20
>>>> Mesos Run Modes
>>>> - (same as current)
>>>>=20
>>>> Running Alongside Hadoop
>>>> - (trim this down)
>>>>=20
>>>>=20
>>>>=20
>>>> Does that work for people?
>>>>=20
>>>>=20
>>>> Thanks!
>>>> Andrew
>>>>=20
>>>>=20
>>>> PS Basically all the same:
>>>>=20
>>>> http://spark.apache.org/docs/0.6.0/running-on-mesos.html
>>>> http://spark.apache.org/docs/0.6.2/running-on-mesos.html
>>>> http://spark.apache.org/docs/0.7.3/running-on-mesos.html
>>>> http://spark.apache.org/docs/0.8.1/running-on-mesos.html
>>>> http://spark.apache.org/docs/0.9.1/running-on-mesos.html
>>>>=20
>>>=20
>> =
https://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/running-on-mesos.=
html
>>>=20
>>=20


From dev-return-7557-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 08:37:53 2014
Return-Path: <dev-return-7557-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 40B5511837
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 08:37:53 +0000 (UTC)
Received: (qmail 70350 invoked by uid 500); 13 May 2014 08:31:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70288 invoked by uid 500); 13 May 2014 08:31:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70279 invoked by uid 99); 13 May 2014 08:31:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 08:31:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.179] (HELO mail-vc0-f179.google.com) (209.85.220.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 08:31:07 +0000
Received: by mail-vc0-f179.google.com with SMTP id im17so10036vcb.24
        for <dev@spark.apache.org>; Tue, 13 May 2014 01:30:46 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=Z7qYcmYJOdH2MQ/JvHMcFKc9tOJIPKt30WcYVarsGYY=;
        b=kz+U0805YaEVzzJeq0/u5i+4RUcvIkm+FnRnCBnAmWuAR/0wnu3o2fGBARa9NL2Aae
         OoZjhPOMYUsh2bQD62zU2qXOK7EMjSsRQgCVApdE5/tz5va8ZAGdyCECvKM3UAvDDTg3
         dL+YF6G1pegZ/Zr6KHPdjqAIx4R6ZGmMULZXd3PTiXbiRgWUnv3+nPYvAdrwMelRX738
         YNf73Bg9H2pWVEvuqIkSOVNV1jL1JPNIzOamqpfhXonn4eWV3gC+PMt8D1QKmbnUMOEs
         PkR8yJTEf9OxCW7/9EOoGU1L1k02WPr1R9atLZstYrXNMLb+YAd1tmh85ogk0hSXEJUz
         6eUg==
X-Gm-Message-State: ALoCoQmFTgQEh6vmBI1mj+TNTBHeGMcXRDHG3tcww8PGUhX1TAZ4cwtXtaafUe6aK5jr6O6H+QJI
X-Received: by 10.52.37.130 with SMTP id y2mr4996236vdj.38.1399969846771;
        Tue, 13 May 2014 01:30:46 -0700 (PDT)
Received: from mail-vc0-f177.google.com (mail-vc0-f177.google.com [209.85.220.177])
        by mx.google.com with ESMTPSA id 10sm27422583vdu.9.2014.05.13.01.30.45
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 13 May 2014 01:30:45 -0700 (PDT)
Received: by mail-vc0-f177.google.com with SMTP id if17so13302vcb.8
        for <dev@spark.apache.org>; Tue, 13 May 2014 01:30:45 -0700 (PDT)
X-Received: by 10.58.220.161 with SMTP id px1mr28243692vec.13.1399969845310;
 Tue, 13 May 2014 01:30:45 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Tue, 13 May 2014 01:30:25 -0700 (PDT)
In-Reply-To: <95FAD9DA-C2A9-42E6-BDF3-9577B92E9B3A@gmail.com>
References: <CA+-p3AGGAKA05e9MpvOgQpmj=opOgvw+zXVZVojYU8Yo9XZ4tg@mail.gmail.com>
 <CABPQxsvrhG5FX2d5VuBPZ3T5CuyBms5f+E5SFVOo2bzmB9Ay9A@mail.gmail.com>
 <CA+-p3AG7VUZB45TadPJRvqCt=X6zY4Zip-7UogsyVCswsjkQFw@mail.gmail.com>
 <CAMc-71=z5eAvdEJ+O2FgbzxNCRDzZWWg9-ZrRKHA-wUOB2dGmQ@mail.gmail.com> <95FAD9DA-C2A9-42E6-BDF3-9577B92E9B3A@gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Tue, 13 May 2014 01:30:25 -0700
Message-ID: <CA+-p3AG9B9g96maxeMCRgWnOSF7Va2Wsr6S4n1KycAOp6qx82g@mail.gmail.com>
Subject: Re: Updating docs for running on Mesos
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bdc9e0253797604f943e122
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc9e0253797604f943e122
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Completely agree about preferring to link to the upstream project rather
than a company's -- the only reason I'm using mesosphere's now is that I
see no alternative from mesos.apache.org

I included instructions for both using Mesosphere's packages and building
from scratch in the PR: https://github.com/apache/spark/pull/756


On Tue, May 13, 2014 at 12:59 AM, Matei Zaharia <matei.zaharia@gmail.com>wr=
ote:

> I=E2=80=99ll ask the Mesos folks about this. Unfortunately it might be to=
ugh to
> link only to a company=E2=80=99s builds; but we can perhaps include them =
in
> addition to instructions for building Mesos from Apache.
>
> Matei
>
> On May 12, 2014, at 11:55 PM, Gerard Maas <gerard.maas@gmail.com> wrote:
>
> > Andrew,
> >
> > Mesosphere has binary releases here:
> > http://mesosphere.io/downloads/
> >
> > (Anecdote: I actually burned a CPU building Mesos from source. No
> kidding -
> > it was coming, as the laptop was crashing from time to time, but the
> mesos
> > build was that one drop too much)
> >
> > kr, Gerard.
> >
> >
> >
> > On Tue, May 13, 2014 at 6:57 AM, Andrew Ash <andrew@andrewash.com>
> wrote:
> >
> >> As far as I know, the upstream doesn't release binaries, only source
> code.
> >> The downloads page <https://mesos.apache.org/downloads/> for 0.18.0
> only
> >> has a source tarball.  Is there a binary release somewhere from Mesos
> that
> >> I'm missing?
> >>
> >>
> >> On Sun, May 11, 2014 at 2:16 PM, Patrick Wendell <pwendell@gmail.com>
> >> wrote:
> >>
> >>> Andrew,
> >>>
> >>> Updating these docs would be great! I think this would be a welcome
> >> change.
> >>>
> >>> In terms of packaging, it would be good to mention the binaries
> >>> produced by the upstream project as well, in addition to Mesosphere.
> >>>
> >>> - Patrick
> >>>
> >>> On Thu, May 8, 2014 at 12:51 AM, Andrew Ash <andrew@andrewash.com>
> >> wrote:
> >>>> The docs for how to run Spark on Mesos have changed very little sinc=
e
> >>>> 0.6.0, but setting it up is much easier now than then.  Does it make
> >>> sense
> >>>> to revamp with the below changes?
> >>>>
> >>>>
> >>>> You no longer need to build mesos yourself as pre-built versions are
> >>>> available from Mesosphere: http://mesosphere.io/downloads/
> >>>>
> >>>> And the instructions guide you towards compiling your own distributi=
on
> >> of
> >>>> Spark, when you can use the prebuilt versions of Spark as well.
> >>>>
> >>>>
> >>>> I'd like to split that portion of the documentation into two section=
s,
> >> a
> >>>> build-from-scratch section and a use-prebuilt section.  The new
> outline
> >>>> would look something like this:
> >>>>
> >>>>
> >>>> *Running Spark on Mesos*
> >>>>
> >>>> Installing Mesos
> >>>> - using prebuilt (recommended)
> >>>> - pointer to mesosphere's packages
> >>>> - from scratch
> >>>> - (similar to current)
> >>>>
> >>>>
> >>>> Connecting Spark to Mesos
> >>>> - loading distribution into an accessible location
> >>>> - Spark settings
> >>>>
> >>>> Mesos Run Modes
> >>>> - (same as current)
> >>>>
> >>>> Running Alongside Hadoop
> >>>> - (trim this down)
> >>>>
> >>>>
> >>>>
> >>>> Does that work for people?
> >>>>
> >>>>
> >>>> Thanks!
> >>>> Andrew
> >>>>
> >>>>
> >>>> PS Basically all the same:
> >>>>
> >>>> http://spark.apache.org/docs/0.6.0/running-on-mesos.html
> >>>> http://spark.apache.org/docs/0.6.2/running-on-mesos.html
> >>>> http://spark.apache.org/docs/0.7.3/running-on-mesos.html
> >>>> http://spark.apache.org/docs/0.8.1/running-on-mesos.html
> >>>> http://spark.apache.org/docs/0.9.1/running-on-mesos.html
> >>>>
> >>>
> >>
> https://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/running-on-mesos=
.html
> >>>
> >>
>
>

--047d7bdc9e0253797604f943e122--

From dev-return-7559-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 09:03:39 2014
Return-Path: <dev-return-7559-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 76364118DE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 09:03:39 +0000 (UTC)
Received: (qmail 36474 invoked by uid 500); 13 May 2014 08:36:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36416 invoked by uid 500); 13 May 2014 08:36:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36408 invoked by uid 99); 13 May 2014 08:36:59 -0000
Received: from Unknown (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 08:36:59 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.169 as permitted sender)
Received: from [209.85.214.169] (HELO mail-ob0-f169.google.com) (209.85.214.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 08:36:54 +0000
Received: by mail-ob0-f169.google.com with SMTP id vb8so15017obc.28
        for <dev@spark.apache.org>; Tue, 13 May 2014 01:36:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=v+OhO4kBbDcNogKKCvSUxgVXC/I0NkLvrRZJ8SmO6Vs=;
        b=p/XjKfowGa/4F+QKNLx+hPnE1f4nw0x3s2fY7MhNo2pgWhHGUKVQdmiDBx6aXVwEpK
         AsYvRKgKl1KOT1WA+0QxXkn6JMYZWEFtyQlT2UeeLyUkaBkjoL/ceYBV5hapP2O4kbEX
         ubZDSvrHJE+WdpL5w3iI24yEynSPFI0CR6OvrY+ZXv+lLS0jLvAcyCZ6uuXIDmtISn03
         KJoRL7f4cs6A+EdrN7MREtSd5h6fsXixox/tmp+giu7xpYs6RRQDUbos0ZEcbYyXhJRP
         psiEXnoMySCZPRVuC5BCydpsAo5ea1cnFOFWtZPAIPp9a44G9bddL2wGeMDgPMhEMPjO
         ESog==
MIME-Version: 1.0
X-Received: by 10.182.107.232 with SMTP id hf8mr1230636obb.75.1399970193451;
 Tue, 13 May 2014 01:36:33 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Tue, 13 May 2014 01:36:33 -0700 (PDT)
Date: Tue, 13 May 2014 01:36:33 -0700
Message-ID: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
Subject: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Please vote on releasing the following candidate as Apache Spark version 1.0.0!

The tag to be voted on is v1.0.0-rc5 (commit 18f0623):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=18f062303303824139998e8fc8f4158217b0dbc3

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc5/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1012/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/

Please vote on releasing this package as Apache Spark 1.0.0!

The vote is open until Friday, May 16, at 09:30 UTC and passes if a
majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.0.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== API Changes ==
We welcome users to compile Spark applications against 1.0. There are
a few API changes in this release. Here are links to the associated
upgrade guides - user facing changes have been kept as small as
possible.

changes to ML vector specification:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10

changes to the Java API:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark

changes to the streaming API:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x

changes to the GraphX API:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091

coGroup and related functions now return Iterable[T] instead of Seq[T]
==> Call toSeq on the result to restore the old behavior

SparkContext.jarOfClass returns Option[String] instead of Seq[String]
==> Call toSeq on the result to restore old behavior

From dev-return-7555-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 09:42:54 2014
Return-Path: <dev-return-7555-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B51DC119FA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 09:42:54 +0000 (UTC)
Received: (qmail 20145 invoked by uid 500); 13 May 2014 06:56:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20088 invoked by uid 500); 13 May 2014 06:56:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20080 invoked by uid 99); 13 May 2014 06:56:14 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 06:56:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of gerard.maas@gmail.com designates 209.85.212.172 as permitted sender)
Received: from [209.85.212.172] (HELO mail-wi0-f172.google.com) (209.85.212.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 06:56:10 +0000
Received: by mail-wi0-f172.google.com with SMTP id hi2so5679966wib.5
        for <dev@spark.apache.org>; Mon, 12 May 2014 23:55:48 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=d46mUR6WH+Wsd6U9qRUNLI3AhdqKT5yVPBupL0e6/+k=;
        b=pwsLIRyxDFIbw4m2zenc3FRGegTCNsNYXIM06vUGTr4F7z3OGXJYj7CqvGRMl946kE
         3WOLisSL7n4k1yM4eAp+3BJ1UJger33ezGlIJsfMmWDX3FcptR13tudYqhjjr1ZVqjbh
         md6WsIfw3GfUeLWT9I6VvR/RRTuyC3BgYTJt+6tP706dOTcVjUugoZ3sy0Jz0W+NTYhu
         ZWMnk8BqPmZKNWKsmIEd6W+eMLMULkK1bx1ZaglL6Idk70jp9SOYquIFL1SSaQx28r2e
         4JsSFDuCPM3PIVeYYCOoCrRTl2yJ3Z4qhfUgMaRoPi+L832EU3Ew8JrqOmgGYU3ua6o2
         1UZw==
X-Received: by 10.194.84.208 with SMTP id b16mr1412416wjz.55.1399964148056;
 Mon, 12 May 2014 23:55:48 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.173.2 with HTTP; Mon, 12 May 2014 23:55:17 -0700 (PDT)
In-Reply-To: <CA+-p3AG7VUZB45TadPJRvqCt=X6zY4Zip-7UogsyVCswsjkQFw@mail.gmail.com>
References: <CA+-p3AGGAKA05e9MpvOgQpmj=opOgvw+zXVZVojYU8Yo9XZ4tg@mail.gmail.com>
 <CABPQxsvrhG5FX2d5VuBPZ3T5CuyBms5f+E5SFVOo2bzmB9Ay9A@mail.gmail.com> <CA+-p3AG7VUZB45TadPJRvqCt=X6zY4Zip-7UogsyVCswsjkQFw@mail.gmail.com>
From: Gerard Maas <gerard.maas@gmail.com>
Date: Tue, 13 May 2014 08:55:17 +0200
Message-ID: <CAMc-71=z5eAvdEJ+O2FgbzxNCRDzZWWg9-ZrRKHA-wUOB2dGmQ@mail.gmail.com>
Subject: Re: Updating docs for running on Mesos
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bfcecacbe473a04f9428d4f
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfcecacbe473a04f9428d4f
Content-Type: text/plain; charset=UTF-8

Andrew,

Mesosphere has binary releases here:
http://mesosphere.io/downloads/

(Anecdote: I actually burned a CPU building Mesos from source. No kidding -
it was coming, as the laptop was crashing from time to time, but the mesos
build was that one drop too much)

kr, Gerard.



On Tue, May 13, 2014 at 6:57 AM, Andrew Ash <andrew@andrewash.com> wrote:

> As far as I know, the upstream doesn't release binaries, only source code.
>  The downloads page <https://mesos.apache.org/downloads/> for 0.18.0 only
> has a source tarball.  Is there a binary release somewhere from Mesos that
> I'm missing?
>
>
> On Sun, May 11, 2014 at 2:16 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> > Andrew,
> >
> > Updating these docs would be great! I think this would be a welcome
> change.
> >
> > In terms of packaging, it would be good to mention the binaries
> > produced by the upstream project as well, in addition to Mesosphere.
> >
> > - Patrick
> >
> > On Thu, May 8, 2014 at 12:51 AM, Andrew Ash <andrew@andrewash.com>
> wrote:
> > > The docs for how to run Spark on Mesos have changed very little since
> > > 0.6.0, but setting it up is much easier now than then.  Does it make
> > sense
> > > to revamp with the below changes?
> > >
> > >
> > > You no longer need to build mesos yourself as pre-built versions are
> > > available from Mesosphere: http://mesosphere.io/downloads/
> > >
> > > And the instructions guide you towards compiling your own distribution
> of
> > > Spark, when you can use the prebuilt versions of Spark as well.
> > >
> > >
> > > I'd like to split that portion of the documentation into two sections,
> a
> > > build-from-scratch section and a use-prebuilt section.  The new outline
> > > would look something like this:
> > >
> > >
> > > *Running Spark on Mesos*
> > >
> > > Installing Mesos
> > > - using prebuilt (recommended)
> > >  - pointer to mesosphere's packages
> > > - from scratch
> > >  - (similar to current)
> > >
> > >
> > > Connecting Spark to Mesos
> > > - loading distribution into an accessible location
> > > - Spark settings
> > >
> > > Mesos Run Modes
> > > - (same as current)
> > >
> > > Running Alongside Hadoop
> > > - (trim this down)
> > >
> > >
> > >
> > > Does that work for people?
> > >
> > >
> > > Thanks!
> > > Andrew
> > >
> > >
> > > PS Basically all the same:
> > >
> > > http://spark.apache.org/docs/0.6.0/running-on-mesos.html
> > > http://spark.apache.org/docs/0.6.2/running-on-mesos.html
> > > http://spark.apache.org/docs/0.7.3/running-on-mesos.html
> > > http://spark.apache.org/docs/0.8.1/running-on-mesos.html
> > > http://spark.apache.org/docs/0.9.1/running-on-mesos.html
> > >
> >
> https://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/running-on-mesos.html
> >
>

--047d7bfcecacbe473a04f9428d4f--

From dev-return-7561-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 12:41:31 2014
Return-Path: <dev-return-7561-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B81E611E97
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 12:41:31 +0000 (UTC)
Received: (qmail 50499 invoked by uid 500); 13 May 2014 12:14:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50431 invoked by uid 500); 13 May 2014 12:14:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50299 invoked by uid 99); 13 May 2014 12:14:50 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 12:14:50 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of gerard.maas@gmail.com designates 74.125.82.177 as permitted sender)
Received: from [74.125.82.177] (HELO mail-we0-f177.google.com) (74.125.82.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 12:14:46 +0000
Received: by mail-we0-f177.google.com with SMTP id x48so274402wes.22
        for <dev@spark.apache.org>; Tue, 13 May 2014 05:14:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=6k5CMTT7/Jer0Q082PPQ5F2B3Kch7bswCuvPGV/1kAI=;
        b=DOKx/HeGzClXwp6QemkmiHwkaDGB5qYyHyTsB8tu9qRJ5gZz9sk7U6WbaYcTLZ1UyX
         2w3AYG7ubr6TKIoCtyPaPU7zB/MAyAJaGvjihxRjktZokgT/PGKKgKiOzpWLmhcR8uxK
         McUEMhkIRvJ6e3/fGcp6592JhUcr1LKjFu4fVLFYwEJgykbiq10FYWv3X7EVWUbNr/6M
         J/y5F9mnRjC58K0Mm0gFa8EgUrJDutC+mvoQejfeLq7a9NXmKuZrCuN1A7U9DbAviM9a
         p/UMUkVTZskV9yOdvl+z4EGPCWysJCCW9w1akMp4WLpGdDPhZagH8s3rVIyrfeVeLncS
         W+xg==
X-Received: by 10.180.85.163 with SMTP id i3mr20897031wiz.14.1399983263683;
 Tue, 13 May 2014 05:14:23 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.173.2 with HTTP; Tue, 13 May 2014 05:13:53 -0700 (PDT)
In-Reply-To: <CA+-p3AGAReWPCndR2+YscJS3KS5rV-EhkSoCsy7307-eQjouBg@mail.gmail.com>
References: <CA+-p3AGGAKA05e9MpvOgQpmj=opOgvw+zXVZVojYU8Yo9XZ4tg@mail.gmail.com>
 <CALEZFQx--MJTQ5vi14ELUyb1XUUwRJMaH19cX3XEsvO4ZN4gjA@mail.gmail.com> <CA+-p3AGAReWPCndR2+YscJS3KS5rV-EhkSoCsy7307-eQjouBg@mail.gmail.com>
From: Gerard Maas <gerard.maas@gmail.com>
Date: Tue, 13 May 2014 14:13:53 +0200
Message-ID: <CAMc-71=7zDzBjEHW7LzbL_tBU9TSnQKgpinjV81vN2kmA1XS6g@mail.gmail.com>
Subject: Re: Updating docs for running on Mesos
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d0444eb091f9b9f04f947014e
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0444eb091f9b9f04f947014e
Content-Type: text/plain; charset=UTF-8

Great work!. I just left some comments in the PR. In summary, it would be
great to have more background on how Spark works on Mesos and how the
different elements interact. That will (hopefully) help understanding the
practicalities of the common assembly location (http/hdfs) and how the jobs
are distributed to the Mesos infrastructure.

Also, adding a chapter on troubleshooting (where we have spent most of our
time lately :-) would be a welcome addition.  I'm not use I've figured it
out completely as to attempt to contribute that myself.

-kr, Gerard.


On Tue, May 13, 2014 at 6:56 AM, Andrew Ash <andrew@andrewash.com> wrote:

> For trimming the Running Alongside Hadoop section I mostly think there
> should be a separate Spark+HDFS section and have the CDH+HDP page be merged
> into that one, but I supposed that's a separate docs change.
>
>
> On Sun, May 11, 2014 at 4:28 PM, Andy Konwinski <andykonwinski@gmail.com
> >wrote:
>
> > Thanks for suggesting this and volunteering to do it.
> >
> > On May 11, 2014 3:32 AM, "Andrew Ash" <andrew@andrewash.com> wrote:
> > >
> > > The docs for how to run Spark on Mesos have changed very little since
> > > 0.6.0, but setting it up is much easier now than then.  Does it make
> > sense
> > > to revamp with the below changes?
> > >
> > >
> > > You no longer need to build mesos yourself as pre-built versions are
> > > available from Mesosphere: http://mesosphere.io/downloads/
> > >
> > > And the instructions guide you towards compiling your own distribution
> of
> > > Spark, when you can use the prebuilt versions of Spark as well.
> > >
> > >
> > > I'd like to split that portion of the documentation into two sections,
> a
> > > build-from-scratch section and a use-prebuilt section.  The new outline
> > > would look something like this:
> > >
> > >
> > > *Running Spark on Mesos*
> > >
> > > Installing Mesos
> > > - using prebuilt (recommended)
> > >  - pointer to mesosphere's packages
> > > - from scratch
> > >  - (similar to current)
> > >
> > >
> > > Connecting Spark to Mesos
> > > - loading distribution into an accessible location
> > > - Spark settings
> > >
> > > Mesos Run Modes
> > > - (same as current)
> > >
> > > Running Alongside Hadoop
> > > - (trim this down)
> >
> > What trimming do you have in mind here?
> >
> > >
> > >
> > >
> > > Does that work for people?
> > >
> > >
> > > Thanks!
> > > Andrew
> > >
> > >
> > > PS Basically all the same:
> > >
> > > http://spark.apache.org/docs/0.6.0/running-on-mesos.html
> > > http://spark.apache.org/docs/0.6.2/running-on-mesos.html
> > > http://spark.apache.org/docs/0.7.3/running-on-mesos.html
> > > http://spark.apache.org/docs/0.8.1/running-on-mesos.html
> > > http://spark.apache.org/docs/0.9.1/running-on-mesos.html
> > >
> >
> >
> https://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/running-on-mesos.html
> >
>

--f46d0444eb091f9b9f04f947014e--

From dev-return-7560-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 12:57:58 2014
Return-Path: <dev-return-7560-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EEBD811F1F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 12:57:58 +0000 (UTC)
Received: (qmail 38985 invoked by uid 500); 13 May 2014 11:08:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38709 invoked by uid 500); 13 May 2014 11:08:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38613 invoked by uid 99); 13 May 2014 11:08:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 11:08:37 +0000
X-ASF-Spam-Status: No, hits=2.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.43 as permitted sender)
Received: from [209.85.192.43] (HELO mail-qg0-f43.google.com) (209.85.192.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 11:08:31 +0000
Received: by mail-qg0-f43.google.com with SMTP id 63so173551qgz.30
        for <dev@spark.apache.org>; Tue, 13 May 2014 04:08:11 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=2oqTqH9QyRJDPcEdmOBAKtbcCQ4Brd+xnOja/PkPMHA=;
        b=hRolPAwRhMiNi/a4Tbsg4xu9ioEm4+kKD5SmhM82C74XKO1yC32TP3aRVMYOWH8pRX
         mJ3Btpj7IlhXk1tE9LHgsVtdX6pYHN+Fu4caDNC2mVjJM5C6Eh7we5V9A+a7rvaZzrgI
         9DNHUC9IG5PgH/gNnwaaf+zW32+1u0oPyQEfvRBhWdsWHeA2PLofwZXM5AhOCz6mUwEo
         IoGejCGMwhMqUq4rw3uh9O5DNc1x4fYTnHsCDQSahzUgq5N7vI5xqkovyn3uol4pAXnX
         85C5XhJHjSWAXBGSiIsvNgZ5G4b1terxe7vfdMFqofhC3Cp+mLzLFpQyRTxaT3Sf2z0S
         r5iA==
MIME-Version: 1.0
X-Received: by 10.224.47.8 with SMTP id l8mr46433802qaf.24.1399979291320; Tue,
 13 May 2014 04:08:11 -0700 (PDT)
Received: by 10.140.82.164 with HTTP; Tue, 13 May 2014 04:08:11 -0700 (PDT)
Date: Tue, 13 May 2014 04:08:11 -0700
Message-ID: <CA+B-+fwL5OfJawkMNzn9BP4hiMVcPzgwR-a-7BE_cTOna27ufw@mail.gmail.com>
Subject: Multinomial Logistic Regression
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2c3c85a28c404f9461450
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2c3c85a28c404f9461450
Content-Type: text/plain; charset=UTF-8

Hi,

Is there a PR for multinomial logistic regression which does one-vs-all and
compare it to the other possibilities ?

@dbtsai in your strata presentation you used one vs all ? Did you add some
constraints on the fact that you penalize if mis-predicted labels are not
very far from the true label ?

Thanks.
Deb

--001a11c2c3c85a28c404f9461450--

From dev-return-7562-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 13:56:44 2014
Return-Path: <dev-return-7562-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 252EC1113A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 13:56:44 +0000 (UTC)
Received: (qmail 9105 invoked by uid 500); 13 May 2014 13:49:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9044 invoked by uid 500); 13 May 2014 13:49:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9036 invoked by uid 99); 13 May 2014 13:49:53 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 13:49:53 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.220.174 as permitted sender)
Received: from [209.85.220.174] (HELO mail-vc0-f174.google.com) (209.85.220.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 13:49:49 +0000
Received: by mail-vc0-f174.google.com with SMTP id lh14so463924vcb.5
        for <dev@spark.apache.org>; Tue, 13 May 2014 06:49:26 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=U/InXTLNjuzAUGhhwkRBQSKTVgJbQbO65ZQuj8RLOrk=;
        b=T9bY2P6kZIIQtEdwrRosz2KjyZKwV6WlfrLpmBo+MYmQnv0vPuoQQORw46xil8YxaZ
         9/KaUhJ5TU6qZ53gS5J7NewpAOXp1ALUxMqrtzhNHLoW+Huoj1/c6mNWVZgKR1EaF46U
         51uMi0TJTnVPlWIKaLvfWuAZbVqtRUWbRSTuAhywnVGTf+v7LCgdm+XZIgWhpC8auj1h
         bz1Dds2dmt6FCkc/JlYGFs1CVGt/+1FvCkWoKeviJCl6dO76m2C/3h+JKRPY+rvVAOqA
         ItVTAtWxVxPad7VT9SoJJwe4FUKXar/Wra6fnyCq1SZOAB92AU0UxxoN0HPOL44jPPVC
         9xMw==
X-Gm-Message-State: ALoCoQkzJAITtUg0vFhzxIPbBrWql3YazIeJEsD1LIAi9xyEZQetgw+J2TytqcjH41yJ4ieBV+Jd
X-Received: by 10.58.107.65 with SMTP id ha1mr29403289veb.1.1399988966330;
 Tue, 13 May 2014 06:49:26 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.111.69 with HTTP; Tue, 13 May 2014 06:49:06 -0700 (PDT)
In-Reply-To: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Tue, 13 May 2014 14:49:06 +0100
Message-ID: <CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Tue, May 13, 2014 at 9:36 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.0.0-rc5/

Good news is that the sigs, MD5 and SHA are all correct.

Tiny note: the Maven artifacts use SHA1, while the binary artifacts
use SHA512, which took me a bit of head-scratching to figure out.

If another RC comes out, I might suggest making it SHA1 everywhere?
But there is nothing wrong with these signatures and checksums.

Now to look at the contents...

From dev-return-7564-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 15:02:14 2014
Return-Path: <dev-return-7564-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CBD22113D6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 15:02:14 +0000 (UTC)
Received: (qmail 4807 invoked by uid 500); 13 May 2014 15:02:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4750 invoked by uid 500); 13 May 2014 15:02:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4742 invoked by uid 99); 13 May 2014 15:02:12 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 15:02:12 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.128.180 as permitted sender)
Received: from [209.85.128.180] (HELO mail-ve0-f180.google.com) (209.85.128.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 15:02:09 +0000
Received: by mail-ve0-f180.google.com with SMTP id db12so588841veb.39
        for <dev@spark.apache.org>; Tue, 13 May 2014 08:01:45 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=dbs7dOXBOQGixiT+Iri8csJ65yO2kp/DUSEWsZ0PsmI=;
        b=WSCUkQf2wpTC85YbVaUpQYrGpR41Y7ugxKxfjz0DvASafsB4mLsOTu24Z8xLqxGz0V
         wP4GiECsUOavrb13jGQFC3kIGaHkVQzf2bIsuKztPz6szIiJYFJZ+tM+b76qZ88vlhL2
         iU6qfB5FlUVfXNEdbsCYbR2BNtWN7vosahfsJXG2BiclMoeHpHXQzeUPlP2aREZbSIPJ
         5uzIJkP+xVuHYIs8iLNIyk6X9NZrDHJRiEGXXFRkiTpCCxdvWzMkPHUTWQwo15Vo0553
         ZA7J7FeZworsKiF8QXpzA3SyEgVtoTGNTcLA26Mn7utvXqEjptXmCiCgw3kydaJ6lTB+
         JvEA==
X-Gm-Message-State: ALoCoQnqsXgKtg2ORKGnq2/oFByPXH/dSPWMKMGtvICTbVu7ph+kqh8yiBXWKBfwT5Sbr7TNH6H+
X-Received: by 10.221.59.194 with SMTP id wp2mr696235vcb.59.1399993305714;
 Tue, 13 May 2014 08:01:45 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.111.69 with HTTP; Tue, 13 May 2014 08:01:25 -0700 (PDT)
In-Reply-To: <CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
 <CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Tue, 13 May 2014 16:01:25 +0100
Message-ID: <CAMAsSdKzLk=mRoH3kHtPW8dE4sK7W12WOUMY0jSJHGyeuWZLEw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Tue, May 13, 2014 at 2:49 PM, Sean Owen <sowen@cloudera.com> wrote:
> On Tue, May 13, 2014 at 9:36 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>> The release files, including signatures, digests, etc. can be found at:
>> http://people.apache.org/~pwendell/spark-1.0.0-rc5/
>
> Good news is that the sigs, MD5 and SHA are all correct.
>
> Tiny note: the Maven artifacts use SHA1, while the binary artifacts
> use SHA512, which took me a bit of head-scratching to figure out.
>
> If another RC comes out, I might suggest making it SHA1 everywhere?
> But there is nothing wrong with these signatures and checksums.
>
> Now to look at the contents...

This is a bit of drudgery that probably needs to be done too: a review
of the LICENSE and NOTICE file. Having dumped the licenses of
dependencies, I don't believe these reflect all of the software that's
going to be distributed in 1.0.

(Good news is there's no forbidden license stuff included AFAICT.)

And good news is that NOTICE can be auto-generated, largely, with a
Maven plugin. This can be done manually for now.

And there is a license plugin that will list all known licenses of
transitive dependencies so that LICENSE can be filled out fairly
easily.

What say? want a JIRA with details?

From dev-return-7567-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 16:56:00 2014
Return-Path: <dev-return-7567-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6C3AE11724
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 16:56:00 +0000 (UTC)
Received: (qmail 84919 invoked by uid 500); 13 May 2014 16:27:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84862 invoked by uid 500); 13 May 2014 16:27:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84854 invoked by uid 99); 13 May 2014 16:27:59 -0000
Received: from Unknown (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 16:27:59 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tstclair@redhat.com designates 209.132.183.37 as permitted sender)
Received: from [209.132.183.37] (HELO mx5-phx2.redhat.com) (209.132.183.37)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 16:27:55 +0000
Received: from zmail16.collab.prod.int.phx2.redhat.com (zmail16.collab.prod.int.phx2.redhat.com [10.5.83.18])
	by mx5-phx2.redhat.com (8.14.4/8.14.4) with ESMTP id s4DGRY1j015736;
	Tue, 13 May 2014 12:27:34 -0400
Date: Tue, 13 May 2014 12:27:34 -0400 (EDT)
From: Tim St Clair <tstclair@redhat.com>
To: dev@spark.apache.org
Cc: Dave Lester <dlester@twitter.com>
Message-ID: <1864015053.4704303.1399998454391.JavaMail.zimbra@redhat.com>
In-Reply-To: <95FAD9DA-C2A9-42E6-BDF3-9577B92E9B3A@gmail.com>
References: <CA+-p3AGGAKA05e9MpvOgQpmj=opOgvw+zXVZVojYU8Yo9XZ4tg@mail.gmail.com> <CABPQxsvrhG5FX2d5VuBPZ3T5CuyBms5f+E5SFVOo2bzmB9Ay9A@mail.gmail.com> <CA+-p3AG7VUZB45TadPJRvqCt=X6zY4Zip-7UogsyVCswsjkQFw@mail.gmail.com> <CAMc-71=z5eAvdEJ+O2FgbzxNCRDzZWWg9-ZrRKHA-wUOB2dGmQ@mail.gmail.com> <95FAD9DA-C2A9-42E6-BDF3-9577B92E9B3A@gmail.com>
Subject: Re: Updating docs for running on Mesos
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable
X-Originating-IP: [10.5.82.11]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - GC34 (Linux)/8.0.6_GA_5922)
Thread-Topic: Updating docs for running on Mesos
Thread-Index: gw7sV2GdqT4iGNTSeL5V6ofxFQ230g==
X-Virus-Checked: Checked by ClamAV on apache.org

Perhaps linking to a Mesos page, which then can list the various package in=
cantations.=20

Cheers,
Tim

----- Original Message -----
> From: "Matei Zaharia" <matei.zaharia@gmail.com>
> To: dev@spark.apache.org
> Sent: Tuesday, May 13, 2014 2:59:42 AM
> Subject: Re: Updating docs for running on Mesos
>=20
> I=E2=80=99ll ask the Mesos folks about this. Unfortunately it might be to=
ugh to link
> only to a company=E2=80=99s builds; but we can perhaps include them in ad=
dition to
> instructions for building Mesos from Apache.
>=20
> Matei
>=20
> On May 12, 2014, at 11:55 PM, Gerard Maas <gerard.maas@gmail.com> wrote:
>=20
> > Andrew,
> >=20
> > Mesosphere has binary releases here:
> > http://mesosphere.io/downloads/
> >=20
> > (Anecdote: I actually burned a CPU building Mesos from source. No kiddi=
ng -
> > it was coming, as the laptop was crashing from time to time, but the me=
sos
> > build was that one drop too much)
> >=20
> > kr, Gerard.
> >=20
> >=20
> >=20
> > On Tue, May 13, 2014 at 6:57 AM, Andrew Ash <andrew@andrewash.com> wrot=
e:
> >=20
> >> As far as I know, the upstream doesn't release binaries, only source c=
ode.
> >> The downloads page <https://mesos.apache.org/downloads/> for 0.18.0 on=
ly
> >> has a source tarball.  Is there a binary release somewhere from Mesos =
that
> >> I'm missing?
> >>=20
> >>=20
> >> On Sun, May 11, 2014 at 2:16 PM, Patrick Wendell <pwendell@gmail.com>
> >> wrote:
> >>=20
> >>> Andrew,
> >>>=20
> >>> Updating these docs would be great! I think this would be a welcome
> >> change.
> >>>=20
> >>> In terms of packaging, it would be good to mention the binaries
> >>> produced by the upstream project as well, in addition to Mesosphere.
> >>>=20
> >>> - Patrick
> >>>=20
> >>> On Thu, May 8, 2014 at 12:51 AM, Andrew Ash <andrew@andrewash.com>
> >> wrote:
> >>>> The docs for how to run Spark on Mesos have changed very little sinc=
e
> >>>> 0.6.0, but setting it up is much easier now than then.  Does it make
> >>> sense
> >>>> to revamp with the below changes?
> >>>>=20
> >>>>=20
> >>>> You no longer need to build mesos yourself as pre-built versions are
> >>>> available from Mesosphere: http://mesosphere.io/downloads/
> >>>>=20
> >>>> And the instructions guide you towards compiling your own distributi=
on
> >> of
> >>>> Spark, when you can use the prebuilt versions of Spark as well.
> >>>>=20
> >>>>=20
> >>>> I'd like to split that portion of the documentation into two section=
s,
> >> a
> >>>> build-from-scratch section and a use-prebuilt section.  The new outl=
ine
> >>>> would look something like this:
> >>>>=20
> >>>>=20
> >>>> *Running Spark on Mesos*
> >>>>=20
> >>>> Installing Mesos
> >>>> - using prebuilt (recommended)
> >>>> - pointer to mesosphere's packages
> >>>> - from scratch
> >>>> - (similar to current)
> >>>>=20
> >>>>=20
> >>>> Connecting Spark to Mesos
> >>>> - loading distribution into an accessible location
> >>>> - Spark settings
> >>>>=20
> >>>> Mesos Run Modes
> >>>> - (same as current)
> >>>>=20
> >>>> Running Alongside Hadoop
> >>>> - (trim this down)
> >>>>=20
> >>>>=20
> >>>>=20
> >>>> Does that work for people?
> >>>>=20
> >>>>=20
> >>>> Thanks!
> >>>> Andrew
> >>>>=20
> >>>>=20
> >>>> PS Basically all the same:
> >>>>=20
> >>>> http://spark.apache.org/docs/0.6.0/running-on-mesos.html
> >>>> http://spark.apache.org/docs/0.6.2/running-on-mesos.html
> >>>> http://spark.apache.org/docs/0.7.3/running-on-mesos.html
> >>>> http://spark.apache.org/docs/0.8.1/running-on-mesos.html
> >>>> http://spark.apache.org/docs/0.9.1/running-on-mesos.html
> >>>>=20
> >>>=20
> >> https://people.apache.org/~pwendell/spark-1.0.0-rc3-docs/running-on-me=
sos.html
> >>>=20
> >>=20
>=20
>=20

--=20
Cheers,
Tim
Freedom, Features, Friends, First -> Fedora
https://fedoraproject.org/wiki/SIGs/bigdata

From dev-return-7558-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 17:35:34 2014
Return-Path: <dev-return-7558-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B5C8F11862
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 17:35:34 +0000 (UTC)
Received: (qmail 14472 invoked by uid 500); 13 May 2014 08:35:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14413 invoked by uid 500); 13 May 2014 08:35:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14402 invoked by uid 99); 13 May 2014 08:35:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 08:35:34 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.128.173] (HELO mail-ve0-f173.google.com) (209.85.128.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 08:35:29 +0000
Received: by mail-ve0-f173.google.com with SMTP id pa12so15211veb.32
        for <dev@spark.apache.org>; Tue, 13 May 2014 01:35:08 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=SNX2tsZqL3UXpGVVvl+GrSPtKOeth7x0GUvxhXK/UW0=;
        b=ZW7T863bvq60rEfGGAQOIiyHMx2sALrm0ojwtYGQMNNFOG3/bDUs90VDC95EpVSjje
         wV3HlLpwGyNi87PugYyrPR11kd8ZuVpDUPRfbpliFhkWtIsM1B8PuBIm5kujfNHAeMAh
         ZaAiAJ/AhE+FLcjFAK3uLEZ2q//MIKlgd8NOTNVB7gbxSvbDRhXCt8N55pdcEb2oJZS5
         GwnOR25OfIZ9hf4TrmVszfoUIdXhVl4o8D2WylXBIspH9Xx+SzooxUSqmjlLfYRaxIp7
         sWsPcHW6qWeWHcncyOQwvjuQYmZSi/1mzi0sNr1tpINn3GKBnuvaXs7c2jg4bvjFlLD+
         wASA==
X-Gm-Message-State: ALoCoQmBj2fBDrs5t+qvc7TJduXSwMRTJH1DnQx/avdWqfYTl8sQj2s1F8p766wG6gTn+3NsQa50
X-Received: by 10.221.40.193 with SMTP id tr1mr6017580vcb.31.1399970108413;
        Tue, 13 May 2014 01:35:08 -0700 (PDT)
Received: from mail-ve0-f169.google.com (mail-ve0-f169.google.com [209.85.128.169])
        by mx.google.com with ESMTPSA id 6sm27454950vdy.1.2014.05.13.01.35.07
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 13 May 2014 01:35:07 -0700 (PDT)
Received: by mail-ve0-f169.google.com with SMTP id jx11so22798veb.0
        for <dev@spark.apache.org>; Tue, 13 May 2014 01:35:06 -0700 (PDT)
X-Received: by 10.220.191.134 with SMTP id dm6mr28211524vcb.16.1399970106808;
 Tue, 13 May 2014 01:35:06 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Tue, 13 May 2014 01:34:45 -0700 (PDT)
In-Reply-To: <CAPh_B=a3eOi-VYbCvfCBHYY6t6vcaVWxqjUw=TqUAZp8oMfu4w@mail.gmail.com>
References: <CA+-p3AHXQZttzFG_TC0P2abupPthPY0EhcSxDAsa90Rvu45qFg@mail.gmail.com>
 <CAPh_B=a3eOi-VYbCvfCBHYY6t6vcaVWxqjUw=TqUAZp8oMfu4w@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Tue, 13 May 2014 01:34:45 -0700
Message-ID: <CA+-p3AE-dh3Aog1hkJw3S-w0PDCiTcFfE4=0c-UhUgsOm_c-Aw@mail.gmail.com>
Subject: Re: Preliminary Parquet numbers and including .count() in Catalyst
To: dev@spark.apache.org
Cc: Michael Armbrust <michael@databricks.com>
Content-Type: multipart/alternative; boundary=089e0158a82ae99f9004f943f071
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158a82ae99f9004f943f071
Content-Type: text/plain; charset=UTF-8

These numbers were run on git commit 756c96 (a few days after the 1.0.0-rc3
tag).  Do you have a link to the patch that avoids scanning all columns for
count(*) or count(1)?  I'd like to give it a shot.

Andrew


On Mon, May 12, 2014 at 11:41 PM, Reynold Xin <rxin@databricks.com> wrote:

> Thanks for the experiments and analysis!
>
> I think Michael already submitted a patch that avoids scanning all columns
> for count(*) or count(1).
>
>
> On Mon, May 12, 2014 at 9:46 PM, Andrew Ash <andrew@andrewash.com> wrote:
>
> > Hi Spark devs,
> >
> > First of all, huge congrats on the parquet integration with SparkSQL!
>  This
> > is an incredible direction forward and something I can see being very
> > broadly useful.
> >
> > I was doing some preliminary tests to see how it works with one of my
> > workflows, and wanted to share some numbers that people might want to
> know
> > about.
> >
> > I also wanted to point out that .count() doesn't seem integrated with the
> > rest of the optimization framework, and some big gains could be possible.
> >
> >
> > So, the numbers:
> >
> > I took a table extracted from a SQL database and stored in HDFS:
> >
> >    - 115 columns (several always-empty, mostly strings, some enums, some
> >    numbers)
> >    - 253,887,080 rows
> >    - 182,150,295,881 bytes (raw uncompressed)
> >    - 42,826,820,222 bytes (lzo compressed with .index file)
> >
> > And I converted it to Parquet using SparkSQL's SchemaRDD.saveAsParquet()
> > call:
> >
> >    - Converting from .lzo in HDFS to .parquet in HDFS took 635s using 42
> >    cores across 4 machines
> >    - 17,517,922,117 bytes (parquet per SparkSQL defaults)
> >
> > So storing in parquet format vs lzo compresses the data down to less than
> > 50% of the .lzo size, and under 10% of the raw uncompressed size.  Nice!
> >
> >
> > I then did some basic interactions on it:
> >
> > *Row count*
> >
> >    - LZO
> >       - lzoFile("/path/to/lzo").count
> >       - 31.632305953s
> >    - Parquet
> >       - sqlContext.parquetFile("/path/to/parquet").count
> >       - 289.129487003s
> >
> > Reassembling rows from the separate column storage is clearly really
> > expensive.  Median task length is 33s vs 4s, and of that 33s in each task
> > (319 tasks total) about 1.75 seconds are spent in GC (inefficient object
> > allocation?)
> >
> >
> >
> > *Count number of rows with a particular key:*
> >
> >    - LZO
> >    - lzoFile("/path/to/lzo").filter(_.split("\\|")(0) ==
> > "1234567890").count
> >       - 73.988897511s
> >        - Parquet
> >    - sqlContext.parquetFile("/path/to/parquet").where('COL ===
> >       1234567890).count
> >       - 293.410470418s
> >        - Parquet (hand-tuned to count on just one column)
> >    - sqlContext.parquetFile("/path/to/parquet").where('COL ===
> >       1234567890).select('IDCOL).count
> >       - 1.160449187s
> >
> > It looks like currently the .count() on parquet is handled incredibly
> > inefficiently and all the columns are materialized.  But if I select just
> > that relevant column and then count, then the column-oriented storage of
> > Parquet really shines.
> >
> > There ought to be a potential optimization here such that a .count() on a
> > SchemaRDD backed by Parquet doesn't require re-assembling the rows, as
> > that's expensive.  I don't think .count() is handled specially in
> > SchemaRDDs, but it seems ripe for optimization.
> >
> >
> > *Count number of distinct values in a column*
> >
> >    - LZO
> >    - lzoFile("/path/to/lzo").map(sel(0)).distinct.count
> >       - 115.582916866s
> >        - Parquet
> >    -
> sqlContext.parquetFile("/path/to/parquet").select('COL).distinct.count
> >       - 16.839004826 s
> >
> > It turns out column selectivity is very useful!  I'm guessing that if I
> > could get byte counts read out of HDFS, that would just about match up
> with
> > the difference in read times.
> >
> >
> >
> >
> > Any thoughts on how to embed the knowledge of my hand-tuned additional
> > .select('IDCOL)
> > into Catalyst?
> >
> >
> > Thanks again for all the hard work and prep for the 1.0 release!
> >
> > Andrew
> >
>

--089e0158a82ae99f9004f943f071--

From dev-return-7568-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 18:49:36 2014
Return-Path: <dev-return-7568-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 506F211B31
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 18:49:36 +0000 (UTC)
Received: (qmail 36088 invoked by uid 500); 13 May 2014 17:00:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35937 invoked by uid 500); 13 May 2014 17:00:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35808 invoked by uid 99); 13 May 2014 17:00:15 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 17:00:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mark@clearstorydata.com designates 74.125.82.177 as permitted sender)
Received: from [74.125.82.177] (HELO mail-we0-f177.google.com) (74.125.82.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 17:00:11 +0000
Received: by mail-we0-f177.google.com with SMTP id x48so671282wes.8
        for <dev@spark.apache.org>; Tue, 13 May 2014 09:59:48 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=zsy5sbfR9CkZhVLsUSf1qmlSJ7M/JZLevjHeokEGMjI=;
        b=nCImYUUTqAyJfD+WcnejOzIGrNauDkCb8+/TTlOEpaM3DIX9COI0l5sl/PZQGTgLnW
         /DO/RH5u4UHWJdBHAI8/Xs6akpFrZ6hH81tWnmuIzrt3/scccC1O9CtaqHYtkfudLap1
         cMMPgW17tcnyhFu0iYp020JdWd7NYoGFAjLZFHO68pwm0K0qnFfTgVOsGA0+TNRKV8+M
         q2eqBgqFI/+3wGaBdI6lDLwZNZRSruaQ1KC4VZypUsi1GhlwSxbemfN9v6WFnaAqfaiQ
         bJxo4aZ6eFbzq+//e5R77/9orN7qa2UWhQY+atzr4YpK8u4TqTkK4k9UmywIPQpg3XX8
         AlHQ==
X-Gm-Message-State: ALoCoQmuumxppSuzKXXrwHzexcvihKfId+jZ21xcvw/RufVz+02klJTq3CCfP0LMVUHVF9of9g0M
MIME-Version: 1.0
X-Received: by 10.194.59.43 with SMTP id w11mr3633054wjq.65.1400000388240;
 Tue, 13 May 2014 09:59:48 -0700 (PDT)
Received: by 10.216.232.69 with HTTP; Tue, 13 May 2014 09:59:48 -0700 (PDT)
In-Reply-To: <CAMtqZeeAW6BeCiCfHbiCmKfMh=BnrO=+UM1oPe1VNgB2disQQw@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com>
	<CAMtqZeeAW6BeCiCfHbiCmKfMh=BnrO=+UM1oPe1VNgB2disQQw@mail.gmail.com>
Date: Tue, 13 May 2014 09:59:48 -0700
Message-ID: <CAAsvFPmATbyc=ij1mP2SAAtxrx3=fBncd5PDvau2pfS8Qnc8iw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bacb5b8d3ac3f04f94afdd9
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bacb5b8d3ac3f04f94afdd9
Content-Type: text/plain; charset=UTF-8

There were a few early/test RCs this cycle that were never put to a vote.


On Tue, May 13, 2014 at 8:07 AM, Nan Zhu <zhunanmcgill@gmail.com> wrote:

> just curious, where is rc4 VOTE?
>
> I searched my gmail but didn't find that?
>
>
>
>
> On Tue, May 13, 2014 at 9:49 AM, Sean Owen <sowen@cloudera.com> wrote:
>
> > On Tue, May 13, 2014 at 9:36 AM, Patrick Wendell <pwendell@gmail.com>
> > wrote:
> > > The release files, including signatures, digests, etc. can be found at:
> > > http://people.apache.org/~pwendell/spark-1.0.0-rc5/
> >
> > Good news is that the sigs, MD5 and SHA are all correct.
> >
> > Tiny note: the Maven artifacts use SHA1, while the binary artifacts
> > use SHA512, which took me a bit of head-scratching to figure out.
> >
> > If another RC comes out, I might suggest making it SHA1 everywhere?
> > But there is nothing wrong with these signatures and checksums.
> >
> > Now to look at the contents...
> >
>

--047d7bacb5b8d3ac3f04f94afdd9--

From dev-return-7571-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 20:55:43 2014
Return-Path: <dev-return-7571-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 26B5911F86
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 20:55:43 +0000 (UTC)
Received: (qmail 29700 invoked by uid 500); 13 May 2014 19:55:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 29643 invoked by uid 500); 13 May 2014 19:55:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 29635 invoked by uid 99); 13 May 2014 19:55:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 19:55:42 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.216.173 as permitted sender)
Received: from [209.85.216.173] (HELO mail-qc0-f173.google.com) (209.85.216.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 19:55:37 +0000
Received: by mail-qc0-f173.google.com with SMTP id i8so1173526qcq.32
        for <dev@spark.apache.org>; Tue, 13 May 2014 12:55:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=orUzt5tofwNzmE9RW9Y0NgEpfq+c7nyMO9UsSgRaydM=;
        b=UG8wSCNddWOEMDbVrYIgtcm4JZ8ReaSc8Wp+phTkU6DZXbJXSDAM4/OwErvABoU3a7
         xl/oGnvaoxYTylTzzMcTGeay2XOpetSV4DOO7w6GsM+97lXTnJ85np7H+U9W/uXVfqAT
         BhzkKP9dMkM14sRxiUmVnaf3qiaR/9at0K7kbak+DYjcZxLB9XNrXJu2uNsIk3QjEfST
         STZYrwO84r2vBDKi+KPOIkCubv/ghNBxiXoHihoCdT8jc6Fw7maTpr/ZUq0KkgxXntrZ
         /pRD4Mwm+giwXZllB0PuTjmdXVkeGIehiA5LTSn/dz605wk2qR4tUxhF4ppsklJ90qk2
         eSEA==
X-Received: by 10.229.97.71 with SMTP id k7mr15043851qcn.4.1400010917194;
        Tue, 13 May 2014 12:55:17 -0700 (PDT)
Received: from [192.168.2.11] (MTRLPQ02-1177746539.sdsl.bell.ca. [70.50.252.107])
        by mx.google.com with ESMTPSA id j110sm9898090qga.40.2014.05.13.12.55.16
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Tue, 13 May 2014 12:55:16 -0700 (PDT)
Date: Tue, 13 May 2014 16:02:55 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Message-ID: <816FFA3A070E464EB2F5D6A80CAE8BB7@gmail.com>
In-Reply-To: <CAAsvFPmATbyc=ij1mP2SAAtxrx3=fBncd5PDvau2pfS8Qnc8iw@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
 <CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com>
 <CAMtqZeeAW6BeCiCfHbiCmKfMh=BnrO=+UM1oPe1VNgB2disQQw@mail.gmail.com>
 <CAAsvFPmATbyc=ij1mP2SAAtxrx3=fBncd5PDvau2pfS8Qnc8iw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53727a6f_643c9869_52f3"
X-Virus-Checked: Checked by ClamAV on apache.org

--53727a6f_643c9869_52f3
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

Ah, I see, thanks 

-- 
Nan Zhu


On Tuesday, May 13, 2014 at 12:59 PM, Mark Hamstra wrote:

> There were a few early/test RCs this cycle that were never put to a vote.
> 
> 
> On Tue, May 13, 2014 at 8:07 AM, Nan Zhu <zhunanmcgill@gmail.com (mailto:zhunanmcgill@gmail.com)> wrote:
> 
> > just curious, where is rc4 VOTE?
> > 
> > I searched my gmail but didn't find that?
> > 
> > 
> > 
> > 
> > On Tue, May 13, 2014 at 9:49 AM, Sean Owen <sowen@cloudera.com (mailto:sowen@cloudera.com)> wrote:
> > 
> > > On Tue, May 13, 2014 at 9:36 AM, Patrick Wendell <pwendell@gmail.com (mailto:pwendell@gmail.com)>
> > > wrote:
> > > > The release files, including signatures, digests, etc. can be found at:
> > > > http://people.apache.org/~pwendell/spark-1.0.0-rc5/
> > > > 
> > > 
> > > 
> > > Good news is that the sigs, MD5 and SHA are all correct.
> > > 
> > > Tiny note: the Maven artifacts use SHA1, while the binary artifacts
> > > use SHA512, which took me a bit of head-scratching to figure out.
> > > 
> > > If another RC comes out, I might suggest making it SHA1 everywhere?
> > > But there is nothing wrong with these signatures and checksums.
> > > 
> > > Now to look at the contents... 


--53727a6f_643c9869_52f3--


From dev-return-7566-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 22:34:03 2014
Return-Path: <dev-return-7566-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 64C12112AC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 22:34:03 +0000 (UTC)
Received: (qmail 18183 invoked by uid 500); 13 May 2014 15:27:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18126 invoked by uid 500); 13 May 2014 15:27:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18118 invoked by uid 99); 13 May 2014 15:27:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 15:27:23 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of michaelmalak@yahoo.com designates 72.30.239.17 as permitted sender)
Received: from [72.30.239.17] (HELO nm38-vm1.bullet.mail.bf1.yahoo.com) (72.30.239.17)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 15:27:15 +0000
Received: from [98.139.212.152] by nm38.bullet.mail.bf1.yahoo.com with NNFMP; 13 May 2014 15:26:54 -0000
Received: from [98.139.212.250] by tm9.bullet.mail.bf1.yahoo.com with NNFMP; 13 May 2014 15:26:54 -0000
Received: from [127.0.0.1] by omp1059.mail.bf1.yahoo.com with NNFMP; 13 May 2014 15:26:54 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 407138.44618.bm@omp1059.mail.bf1.yahoo.com
Received: (qmail 58529 invoked by uid 60001); 13 May 2014 15:26:54 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.com; s=s1024; t=1399994814; bh=QX/LsCURlRvZ+y3YlGinMF9kfRDgCkOdkwaqO9fjrig=; h=Message-ID:Date:From:Reply-To:Subject:To:MIME-Version:Content-Type:Content-Transfer-Encoding; b=n/AoZDI9NfKT61z86IvO/cgv7MrYEO90XJ8gcrgWy3iMDAIkl6RyddWh310jGC2FupZd5mJxg70JxpieUptBC/jQM7ari51j0R3S29tVH3yt/YUtQ+CVIlMEM2WBExXgLNGBn8bVlf9pn4uMnjlfqrJAjieU2yxuhrJSZf97M2w=
X-YMail-OSG: fuliYsEVM1m4Hwt7rAsz4qfbsis93OGwC2nnD4d8CG_Wrh3
 eB9hC5LNHRJBCCVNUazrcuZruE80O8s49R7Z8RYbv4.wXUHQWkIic5cdEWZ3
 qlJZ8FnC1DE09Seyh5qxFH7H9gq6Tiut_mn64GS0yUONvDYO.P8AWnBkyeLq
 p_.VzjKM5UoeZ6vOdiEXCV3n3r5GsKwdM28_i3MixHSQN19OVT7_qzT3Jt9Q
 p0jltNA_VBh79a7TzKve2TEJ_uRP7gWRyv963QJmhdsJm9s3oVeh_BySxdUS
 FD2F9CaiI5zVkMoPbOYesWgttujQED6.QTDjGtg3p4MEnEtkS1qb2nN6SjvV
 a1Z1yj1RMnpOAgK57LsNoBr80g1Qp9_XU1cssJKzTyt9_25HrF0O.ZDQTfeZ
 k5aYGcjS5YNTHSanzayUgWYSaxhYSC3gRrDj4QC37xa8clKARPoIcLKA0qxH
 QugOYJVAEdIjM5noPRWk1CjK7TdDQh2nbo8JNRmRz55Q70bOB6nshfg07Z.L
 OD6JLoF.yZyGMWfDgyxz048oxaAfQwy5ybDfpCQDHR8XGUSt2NUsa
Received: from [148.87.67.200] by web160805.mail.bf1.yahoo.com via HTTP; Tue, 13 May 2014 08:26:54 PDT
X-Rocket-MIMEInfo: 002.001,UmVwb3N0aW5nIGhlcmUgb24gZGV2IHNpbmNlIEkgZGlkbid0IHNlZSBhIHJlc3BvbnNlIG9uIHVzZXI6CgpJJ20gc2VlaW5nIGRpZmZlcmVudCBTZXJpYWxpemFibGUgYmVoYXZpb3IgaW4gU3BhcmsgU2hlbGwgdnMuIFNjYWxhIFNoZWxsLiBJbiB0aGUgU3BhcmsgU2hlbGwsIGVxdWFscygpIGZhaWxzIHdoZW4gSSB1c2UgdGhlIGNhbm9uaWNhbCBlcXVhbHMoKSBwYXR0ZXJuIG9mIG1hdGNoe30sIGJ1dCB3b3JrcyB3aGVuIEkgc3Vic2l0dXRlIHdpdGggaXNJbnN0YW5jZU9mW10uIEkgYW0gdXNpbmcgU3BhcmsBMAEBAQE-
X-Mailer: YahooMailWebService/0.8.188.663
Message-ID: <1399994814.54033.YahooMailNeo@web160805.mail.bf1.yahoo.com>
Date: Tue, 13 May 2014 08:26:54 -0700 (PDT)
From: Michael Malak <michaelmalak@yahoo.com>
Reply-To: Michael Malak <michaelmalak@yahoo.com>
Subject:  Serializable different behavior Spark Shell vs. Scala Shell
To: "dev@spark.apache.org" <dev@spark.apache.org>
MIME-Version: 1.0
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Reposting here on dev since I didn't see a response on user:=0A=0AI'm seein=
g different Serializable behavior in Spark Shell vs. Scala Shell. In the Sp=
ark Shell, equals() fails when I use the canonical equals() pattern of matc=
h{}, but works when I subsitute with isInstanceOf[]. I am using Spark 0.9.0=
/Scala 2.10.3.=0A=0AIs this a bug?=0A=0ASpark Shell (equals uses match{})=
=0A=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=0A=0Aclass C(val s:String) extends Serializable=
 {=0A=A0 override def equals(o: Any) =3D o match {=0A=A0=A0=A0 case that: C=
 =3D> that.s =3D=3D s=0A=A0=A0=A0 case _ =3D> false=0A=A0 }=0A}=0A=0Aval x =
=3D new C("a")=0Aval bos =3D new java.io.ByteArrayOutputStream()=0Aval out =
=3D new java.io.ObjectOutputStream(bos)=0Aout.writeObject(x);=0Aval b =3D b=
os.toByteArray();=0Aout.close=0Abos.close=0Aval y =3D new java.io.ObjectInp=
utStream(new java.io.ByteArrayInputStream(b)).readObject().asInstanceOf[C]=
=0Ax.equals(y)=0A=0Ares: Boolean =3D false=0A=0ASpark Shell (equals uses is=
InstanceOf[])=0A=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=0A=0Aclass =
C(val s:String) extends Serializable {=0A=A0 override def equals(o: Any) =
=3D if (o.isInstanceOf[C]) (o.asInstanceOf[C].s =3D=3D s) else false=0A}=0A=
=0Aval x =3D new C("a")=0Aval bos =3D new java.io.ByteArrayOutputStream()=
=0Aval out =3D new java.io.ObjectOutputStream(bos)=0Aout.writeObject(x);=0A=
val b =3D bos.toByteArray();=0Aout.close=0Abos.close=0Aval y =3D new java.i=
o.ObjectInputStream(new java.io.ByteArrayInputStream(b)).readObject().asIns=
tanceOf[C]=0Ax.equals(y)=0A=0Ares: Boolean =3D true=0A=0AScala Shell (equal=
s uses match{})=0A=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=0A=0Aclass C(val s:String) exten=
ds Serializable {=0A=A0 override def equals(o: Any) =3D o match {=0A=A0=A0=
=A0 case that: C =3D> that.s =3D=3D s=0A=A0=A0=A0 case _ =3D> false=0A=A0 }=
=0A}=0A=0Aval x =3D new C("a")=0Aval bos =3D new java.io.ByteArrayOutputStr=
eam()=0Aval out =3D new java.io.ObjectOutputStream(bos)=0Aout.writeObject(x=
);=0Aval b =3D bos.toByteArray();=0Aout.close=0Abos.close=0Aval y =3D new j=
ava.io.ObjectInputStream(new java.io.ByteArrayInputStream(b)).readObject().=
asInstanceOf[C]=0Ax.equals(y)=0A=0Ares: Boolean =3D true

From dev-return-7575-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 13 23:17:34 2014
Return-Path: <dev-return-7575-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 43F7C1142B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 13 May 2014 23:17:34 +0000 (UTC)
Received: (qmail 76310 invoked by uid 500); 13 May 2014 22:17:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76166 invoked by uid 500); 13 May 2014 22:17:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76012 invoked by uid 99); 13 May 2014 22:17:34 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 22:17:34 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of michaelmalak@yahoo.com designates 98.139.213.158 as permitted sender)
Received: from [98.139.213.158] (HELO nm2-vm1.bullet.mail.bf1.yahoo.com) (98.139.213.158)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 22:17:28 +0000
Received: from [98.139.215.141] by nm2.bullet.mail.bf1.yahoo.com with NNFMP; 13 May 2014 22:17:04 -0000
Received: from [98.139.212.245] by tm12.bullet.mail.bf1.yahoo.com with NNFMP; 13 May 2014 22:17:04 -0000
Received: from [127.0.0.1] by omp1054.mail.bf1.yahoo.com with NNFMP; 13 May 2014 22:17:04 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 820231.97482.bm@omp1054.mail.bf1.yahoo.com
Received: (qmail 59473 invoked by uid 60001); 13 May 2014 22:17:04 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.com; s=s1024; t=1400019424; bh=yUk9+KMZ98uGn+GF8c2cLN4XpXvr4mTnvSTWq3aRUNM=; h=References:Message-ID:Date:From:Reply-To:Subject:To:In-Reply-To:MIME-Version:Content-Type:Content-Transfer-Encoding; b=r6HDnUTl/Lehywa3hX/4cMdsIh36HKISNW4EbajTXUm9h7JIXMCumTOPgWcIDttEyzrEXT9O+CahbnL0yFQ9gu1Hk18vCWF4bdgH2gxKDQENcWISsnj1GdUVlpSnqLFC8n8FFaDvaj7LjSouva8UcJUTePYeFbFN374nRMtYwLg=
X-YMail-OSG: kgTEG_wVM1nD4SIJVOvsV1bqaAhsBDN7xwR4PL1cVxWtwcb
 sSyyHn618JqhDchMqwZT7kz8WuiH9i.aNn00CsUt7iePhjF606.o0XNYOwJp
 .0xIlFrwzgKEEpVjs.csk8s_mV4hNP8Y38lv98N_7MX1wN1D.DUe3HRvfyf2
 1q9fSYBB2x015B2tuyGVtKP20cFVhCpqquJI_DO782ulkqdl4TWXV1Cqd1MN
 FMR5oDhA_NFQ3x.BBPpARuF.YUAcTSXjzNX8IQm1.sWfp1HPV0ZGLD.U_z1y
 6Lb3X_sqz_tWM9azQOFmNoR6JMEjNo6mYZeCmanz93SMnBfWKPU_7XxdG0Io
 tOLPsUEFj.21F.SxUXmFIYz0eZoYk3OHhHgGUZEu5QEHxxg9BG4BIBCkdA9W
 xnWhHci9fXH6d14rajppPObHEFq9c_N6iHcW6jmjlIkVun5TAbEouSMAFPhz
 IvF.JOklYpHS6BVcpp4yWFI7QFVrg3sNfHlCOHoySmy57xr5HKPEpqzwnr1l
 1glgoNVQtNrXK6twMyYGszAUsrU39Z42Zs4rTDJj0ANnWkmTvlw--
Received: from [148.87.67.200] by web160801.mail.bf1.yahoo.com via HTTP; Tue, 13 May 2014 15:17:04 PDT
X-Rocket-MIMEInfo: 002.001,VGhhbmsgeW91IGZvciB5b3VyIGludmVzdGlnYXRpb24gaW50byB0aGlzIQoKSnVzdCBmb3IgY29tcGxldGVuZXNzLCBJJ3ZlIGNvbmZpcm1lZCBpdCdzIGEgcHJvYmxlbSBvbmx5IGluIFJFUEwsIG5vdCBpbiBjb21waWxlZCBTcGFyayBwcm9ncmFtcy4KCkJ1dCB3aXRoaW4gUkVQTCwgYSBkaXJlY3QgY29uc2VxdWVuY2Ugb2Ygbm9uLXNhbWUgY2xhc3NlcyBhZnRlciBzZXJpYWxpemF0aW9uL2Rlc2VyaWFsaXphdGlvbiBhbHNvIG1lYW5zIHRoYXQgbG9va3VwKCkgZG9lc24ndCB3b3JrOgoKc2NhbGE.IGNsYXMBMAEBAQE-
X-Mailer: YahooMailWebService/0.8.188.663
References: <1399994814.54033.YahooMailNeo@web160805.mail.bf1.yahoo.com> <CAFboF2ytAUFpBtz3indSvqq3kX_w2vdkGraNQGVf8oA=a0-JEQ@mail.gmail.com>
Message-ID: <1400019424.80629.YahooMailNeo@web160801.mail.bf1.yahoo.com>
Date: Tue, 13 May 2014 15:17:04 -0700 (PDT)
From: Michael Malak <michaelmalak@yahoo.com>
Reply-To: Michael Malak <michaelmalak@yahoo.com>
Subject: Re: Serializable different behavior Spark Shell vs. Scala Shell
To: "dev@spark.apache.org" <dev@spark.apache.org>
In-Reply-To: <CAFboF2ytAUFpBtz3indSvqq3kX_w2vdkGraNQGVf8oA=a0-JEQ@mail.gmail.com>
MIME-Version: 1.0
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Thank you for your investigation into this!=0A=0AJust for completeness, I'v=
e confirmed it's a problem only in REPL, not in compiled Spark programs.=0A=
=0ABut within REPL, a direct consequence of non-same classes after serializ=
ation/deserialization also means that lookup() doesn't work:=0A=0Ascala> cl=
ass C(val s:String) extends Serializable {=0A=A0=A0=A0=A0 |=A0=A0 override =
def equals(o: Any) =3D if (o.isInstanceOf[C]) o.asInstanceOf[C].s =3D=3D s =
else false=0A=A0=A0=A0=A0 |=A0=A0 override def toString =3D s=0A=A0=A0=A0=
=A0 | }=0Adefined class C=0A=0Ascala> val r =3D sc.parallelize(Array((new C=
("a"),11),(new C("a"),12)))=0Ar: org.apache.spark.rdd.RDD[(C, Int)] =3D Par=
allelCollectionRDD[3] at parallelize at <console>:14=0A=0Ascala> r.lookup(n=
ew C("a"))=0A<console>:17: error: type mismatch;=0A=A0found=A0=A0 : C=0A=A0=
required: C=0A=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0 r.lookup(new C("a"))=
=0A=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0=A0 ^=0A=
=0A=0A=0AOn Tuesday, May 13, 2014 3:05 PM, Anand Avati <avati@gluster.org> =
wrote:=0A=0AOn Tue, May 13, 2014 at 8:26 AM, Michael Malak <michaelmalak@ya=
hoo.com> wrote:=0A=0AReposting here on dev since I didn't see a response on=
 user:=0A>=0A>I'm seeing different Serializable behavior in Spark Shell vs.=
 Scala Shell. In the Spark Shell, equals() fails when I use the canonical e=
quals() pattern of match{}, but works when I subsitute with isInstanceOf[].=
 I am using Spark 0.9.0/Scala 2.10.3.=0A>=0A>Is this a bug?=0A>=0A>Spark Sh=
ell (equals uses match{})=0A>=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=0A>=0A>class C(val s=
:String) extends Serializable {=0A>=A0 override def equals(o: Any) =3D o ma=
tch {=0A>=A0=A0=A0 case that: C =3D> that.s =3D=3D s=0A>=A0=A0=A0 case _ =
=3D> false=0A>=A0 }=0A>}=0A>=0A>val x =3D new C("a")=0A>val bos =3D new jav=
a.io.ByteArrayOutputStream()=0A>val out =3D new java.io.ObjectOutputStream(=
bos)=0A>out.writeObject(x);=0A>val b =3D bos.toByteArray();=0A>out.close=0A=
>bos.close=0A>val y =3D new java.io.ObjectInputStream(new java.io.ByteArray=
InputStream(b)).readObject().asInstanceOf[C]=0A>x.equals(y)=0A>=0A>res: Boo=
lean =3D false=0A>=0A>Spark Shell (equals uses isInstanceOf[])=0A>=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=0A>=0A>class C(val s:String) extends S=
erializable {=0A>=A0 override def equals(o: Any) =3D if (o.isInstanceOf[C])=
 (o.asInstanceOf[C].s =3D=3D s) else false=0A>}=0A>=0A>val x =3D new C("a")=
=0A>val bos =3D new java.io.ByteArrayOutputStream()=0A>val out =3D new java=
.io.ObjectOutputStream(bos)=0A>out.writeObject(x);=0A>val b =3D bos.toByteA=
rray();=0A>out.close=0A>bos.close=0A>val y =3D new java.io.ObjectInputStrea=
m(new java.io.ByteArrayInputStream(b)).readObject().asInstanceOf[C]=0A>x.eq=
uals(y)=0A>=0A>res: Boolean =3D true=0A>=0A>Scala Shell (equals uses match{=
})=0A>=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=0A>=0A>class C(val s:String) extends Seriali=
zable {=0A>=A0 override def equals(o: Any) =3D o match {=0A>=A0=A0=A0 case =
that: C =3D> that.s =3D=3D s=0A>=A0=A0=A0 case _ =3D> false=0A>=A0 }=0A>}=
=0A>=0A>val x =3D new C("a")=0A>val bos =3D new java.io.ByteArrayOutputStre=
am()=0A>val out =3D new java.io.ObjectOutputStream(bos)=0A>out.writeObject(=
x);=0A>val b =3D bos.toByteArray();=0A>out.close=0A>bos.close=0A>val y =3D =
new java.io.ObjectInputStream(new java.io.ByteArrayInputStream(b)).readObje=
ct().asInstanceOf[C]=0A>x.equals(y)=0A>=0A>res: Boolean =3D true=0A>=0A=0A=
=0AHmm. I see that this can be reproduced without Spark in Scala 2.11, with=
 and without -Yrepl-class-based command line flag to the repl. Spark's REPL=
 has the effective behavior of 2.11's -Yrepl-class-based flag. Inspecting t=
he byte code generated, it appears -Yrepl-class-based results in the creati=
on of "$outer" field in the generated classes (including class C). The firs=
t case match in equals() is resulting code along the lines of (simplified):=
=0A=0Aif (o isinstanceof Cstr && this.$outer =3D=3D that.$outer) { // do st=
ring compare // }=0A=0A$outer is the synthetic field object to the outer ob=
ject in which the object was created (in this case, the repl environment). =
Now obviously, when x is taken through the bytestream and deserialized, it =
would have a new $outer created (it may have deserialized in a different jv=
m or machine for all we know). So the $outer's mismatching is expected. How=
ever I'm still trying to understand why the outers need to be the same for =
the case match.

From dev-return-7578-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 14 01:04:18 2014
Return-Path: <dev-return-7578-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 351B8117E7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 May 2014 01:04:18 +0000 (UTC)
Received: (qmail 47519 invoked by uid 500); 14 May 2014 00:04:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47461 invoked by uid 500); 14 May 2014 00:04:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47342 invoked by uid 99); 14 May 2014 00:04:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 00:04:17 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 00:04:13 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <madhu@madhu.com>)
	id 1WkMfo-0001HR-0v
	for dev@spark.incubator.apache.org; Tue, 13 May 2014 17:03:52 -0700
Date: Tue, 13 May 2014 17:03:52 -0700 (PDT)
From: Madhu <madhu@madhu.com>
To: dev@spark.incubator.apache.org
Message-ID: <1400025832017-6558.post@n3.nabble.com>
In-Reply-To: <1400022674823-6555.post@n3.nabble.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com> <1400022674823-6555.post@n3.nabble.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I built rc5 using sbt/sbt assembly on Linux without any problems.
There used to be an sbt.cmd for Windows build, has that been deprecated?
If so, I can document the Windows build steps that worked for me.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-0-0-rc5-tp6542p6558.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7573-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 14 01:06:11 2014
Return-Path: <dev-return-7573-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5F4C2117ED
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 May 2014 01:06:11 +0000 (UTC)
Received: (qmail 85251 invoked by uid 500); 13 May 2014 21:06:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85189 invoked by uid 500); 13 May 2014 21:06:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85181 invoked by uid 99); 13 May 2014 21:06:11 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 21:06:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of anand.avati@gmail.com designates 209.85.219.53 as permitted sender)
Received: from [209.85.219.53] (HELO mail-oa0-f53.google.com) (209.85.219.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 21:06:06 +0000
Received: by mail-oa0-f53.google.com with SMTP id m1so1087077oag.26
        for <dev@spark.apache.org>; Tue, 13 May 2014 14:05:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=blzrSDHtFXTxlAng7HkhQ2G0XqWzwTwkfiLe9MOJ1wc=;
        b=T4l/zCXZWxAfhEdrHhH7AJCrWR6GeTwbZs/CVC/SLJKtXDzOAcJNE9kOrGj8tIMGfq
         p8nmDiCUp1ynvi32I0EkXij4aQ+6S6Xzlg1bOLMpDxC2SSlftuF2ElphO05fGmhXMIP9
         Z96PTJVMn61Zota3JGzjlvCUfwXFmxVcKAgOKK5bEVOgxrs7SjMmzC82Qpg2R9mh80mk
         N2/cHAz7Kthnd7jQCcTbptaXH67M9FppY9oo6M97KiFjk550CXPQueFBowWadSsh3ban
         K9MCBF44jHQ/lU7XrJr+FAyrNBCkJvZ3/KCwE6XFlry1nVMJqJ1O4mjPJpQQyRTXVnd/
         oGNw==
MIME-Version: 1.0
X-Received: by 10.182.144.161 with SMTP id sn1mr5764625obb.82.1400015142784;
 Tue, 13 May 2014 14:05:42 -0700 (PDT)
Sender: anand.avati@gmail.com
Received: by 10.60.165.68 with HTTP; Tue, 13 May 2014 14:05:42 -0700 (PDT)
In-Reply-To: <1399994814.54033.YahooMailNeo@web160805.mail.bf1.yahoo.com>
References: <1399994814.54033.YahooMailNeo@web160805.mail.bf1.yahoo.com>
Date: Tue, 13 May 2014 14:05:42 -0700
X-Google-Sender-Auth: NBJqTxeQp5dUL7FCHcR_B7df8dg
Message-ID: <CAFboF2ytAUFpBtz3indSvqq3kX_w2vdkGraNQGVf8oA=a0-JEQ@mail.gmail.com>
Subject: Re: Serializable different behavior Spark Shell vs. Scala Shell
From: Anand Avati <avati@gluster.org>
To: dev@spark.apache.org, Michael Malak <michaelmalak@yahoo.com>
Content-Type: multipart/alternative; boundary=089e0158ac7844166a04f94e6d6b
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158ac7844166a04f94e6d6b
Content-Type: text/plain; charset=UTF-8

On Tue, May 13, 2014 at 8:26 AM, Michael Malak <michaelmalak@yahoo.com>wrote:

> Reposting here on dev since I didn't see a response on user:
>
> I'm seeing different Serializable behavior in Spark Shell vs. Scala Shell.
> In the Spark Shell, equals() fails when I use the canonical equals()
> pattern of match{}, but works when I subsitute with isInstanceOf[]. I am
> using Spark 0.9.0/Scala 2.10.3.
>
> Is this a bug?
>
> Spark Shell (equals uses match{})
> =================================
>
> class C(val s:String) extends Serializable {
>   override def equals(o: Any) = o match {
>     case that: C => that.s == s
>     case _ => false
>   }
> }
>
> val x = new C("a")
> val bos = new java.io.ByteArrayOutputStream()
> val out = new java.io.ObjectOutputStream(bos)
> out.writeObject(x);
> val b = bos.toByteArray();
> out.close
> bos.close
> val y = new java.io.ObjectInputStream(new
> java.io.ByteArrayInputStream(b)).readObject().asInstanceOf[C]
> x.equals(y)
>
> res: Boolean = false
>
> Spark Shell (equals uses isInstanceOf[])
> ========================================
>
> class C(val s:String) extends Serializable {
>   override def equals(o: Any) = if (o.isInstanceOf[C])
> (o.asInstanceOf[C].s == s) else false
> }
>
> val x = new C("a")
> val bos = new java.io.ByteArrayOutputStream()
> val out = new java.io.ObjectOutputStream(bos)
> out.writeObject(x);
> val b = bos.toByteArray();
> out.close
> bos.close
> val y = new java.io.ObjectInputStream(new
> java.io.ByteArrayInputStream(b)).readObject().asInstanceOf[C]
> x.equals(y)
>
> res: Boolean = true
>
> Scala Shell (equals uses match{})
> =================================
>
> class C(val s:String) extends Serializable {
>   override def equals(o: Any) = o match {
>     case that: C => that.s == s
>     case _ => false
>   }
> }
>
> val x = new C("a")
> val bos = new java.io.ByteArrayOutputStream()
> val out = new java.io.ObjectOutputStream(bos)
> out.writeObject(x);
> val b = bos.toByteArray();
> out.close
> bos.close
> val y = new java.io.ObjectInputStream(new
> java.io.ByteArrayInputStream(b)).readObject().asInstanceOf[C]
> x.equals(y)
>
> res: Boolean = true
>


Hmm. I see that this can be reproduced without Spark in Scala 2.11, with
and without -Yrepl-class-based command line flag to the repl. Spark's REPL
has the effective behavior of 2.11's -Yrepl-class-based flag. Inspecting
the byte code generated, it appears -Yrepl-class-based results in the
creation of "$outer" field in the generated classes (including class C).
The first case match in equals() is resulting code along the lines of
(simplified):

if (o isinstanceof Cstr && this.$outer == that.$outer) { // do string
compare // }

$outer is the synthetic field object to the outer object in which the
object was created (in this case, the repl environment). Now obviously,
when x is taken through the bytestream and deserialized, it would have a
new $outer created (it may have deserialized in a different jvm or machine
for all we know). So the $outer's mismatching is expected. However I'm
still trying to understand why the outers need to be the same for the case
match.

--089e0158ac7844166a04f94e6d6b--

From dev-return-7580-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 14 01:15:55 2014
Return-Path: <dev-return-7580-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9F34A11839
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 May 2014 01:15:55 +0000 (UTC)
Received: (qmail 52338 invoked by uid 500); 14 May 2014 01:09:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52136 invoked by uid 500); 14 May 2014 01:09:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52128 invoked by uid 99); 14 May 2014 01:09:14 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 01:09:14 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.216.176 as permitted sender)
Received: from [209.85.216.176] (HELO mail-qc0-f176.google.com) (209.85.216.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 01:09:10 +0000
Received: by mail-qc0-f176.google.com with SMTP id r5so1657233qcx.7
        for <dev@spark.apache.org>; Tue, 13 May 2014 18:08:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=TkHgdxhhfG3R6JZuUj1Hp2h5u941nVgT3AdHu55BLtE=;
        b=y3qej95v+qrB7eUy6n6imVh7jk4arI85atZq+qsfaKsSGsgWPHpf2CpQPJNOp2wIkf
         nRBbUcrxkUstLTXdeu+qw/MleYFSaZXdb1abMGiBDJA5Tuh+txdDnVel0TjESBSYBBOm
         QNNy34vS+TFH+b57ST5ZkzVySutPWPVbh8aIWlT8od60uFOoOcE/cSinSwVbrYSMRUgr
         05Tq3IAbenjb+4q3pHFrUyVRq8NShFNx076fxEDcC/UIk5O+BP8FL0KgoX1cjTpTDI6N
         m5Iq+Al8E/BNO803DWxB2h8zzbGYIL4i5ScgGLGDEV0kchgoYLxrMyql3dL1DA7sNPrJ
         1fPA==
X-Received: by 10.140.29.226 with SMTP id b89mr956848qgb.48.1400029727138;
        Tue, 13 May 2014 18:08:47 -0700 (PDT)
Received: from [192.168.2.13] ([69.157.95.72])
        by mx.google.com with ESMTPSA id m18sm399737qax.47.2014.05.13.18.08.46
        for <multiple recipients>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Tue, 13 May 2014 18:08:46 -0700 (PDT)
Date: Tue, 13 May 2014 21:16:27 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Cc: dev@spark.incubator.apache.org
Message-ID: <89B7B6B3F2944CB1AABA6BB9AF1D128E@gmail.com>
In-Reply-To: <1400025832017-6558.post@n3.nabble.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
 <1400022674823-6555.post@n3.nabble.com>
 <1400025832017-6558.post@n3.nabble.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="5372c3eb_5ff87e05_52f3"
X-Virus-Checked: Checked by ClamAV on apache.org

--5372c3eb_5ff87e05_52f3
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

+1, replaced rc3 with rc5, all applications are working fine

Best, 

-- 
Nan Zhu


On Tuesday, May 13, 2014 at 8:03 PM, Madhu wrote:

> I built rc5 using sbt/sbt assembly on Linux without any problems.
> There used to be an sbt.cmd for Windows build, has that been deprecated?
> If so, I can document the Windows build steps that worked for me.
> 
> 
> 
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-0-0-rc5-tp6542p6558.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com (http://Nabble.com).
> 
> 



--5372c3eb_5ff87e05_52f3--


From dev-return-7582-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 14 01:44:00 2014
Return-Path: <dev-return-7582-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 41664118E9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 May 2014 01:44:00 +0000 (UTC)
Received: (qmail 52542 invoked by uid 500); 14 May 2014 01:15:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52480 invoked by uid 500); 14 May 2014 01:15:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52472 invoked by uid 99); 14 May 2014 01:15:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 01:15:59 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 01:15:55 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <madhu@madhu.com>)
	id 1WkNnB-0005Yv-Io
	for dev@spark.incubator.apache.org; Tue, 13 May 2014 18:15:33 -0700
Date: Tue, 13 May 2014 18:15:33 -0700 (PDT)
From: Madhu <madhu@madhu.com>
To: dev@spark.incubator.apache.org
Message-ID: <1400030133573-6560.post@n3.nabble.com>
In-Reply-To: <tencent_5EABC2696C9E4E8610F1CE38@qq.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com> <CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com> <CAMtqZeeAW6BeCiCfHbiCmKfMh=BnrO=+UM1oPe1VNgB2disQQw@mail.gmail.com> <tencent_5EABC2696C9E4E8610F1CE38@qq.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I just built rc5 on Windows 7 and tried to reproduce the problem described in

https://issues.apache.org/jira/browse/SPARK-1712

It works on my machine:

14/05/13 21:06:47 INFO DAGScheduler: Stage 1 (sum at <console>:17) finished
in 4.548 s
14/05/13 21:06:47 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks
have all completed, from pool
14/05/13 21:06:47 INFO SparkContext: Job finished: sum at <console>:17, took
4.814991993 s
res1: Double = 5.000005E11

I used all defaults, no config files were changed.
Not sure if that makes a difference...



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-0-0-rc5-tp6542p6560.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7577-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 14 01:49:35 2014
Return-Path: <dev-return-7577-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9DA4311904
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 May 2014 01:49:35 +0000 (UTC)
Received: (qmail 47520 invoked by uid 500); 13 May 2014 23:02:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47396 invoked by uid 500); 13 May 2014 23:02:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47284 invoked by uid 99); 13 May 2014 23:02:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 23:02:55 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.182] (HELO mail-vc0-f182.google.com) (209.85.220.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 23:02:50 +0000
Received: by mail-vc0-f182.google.com with SMTP id la4so1447049vcb.27
        for <dev@spark.apache.org>; Tue, 13 May 2014 16:02:29 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=Ar1uG/Kf5TAR19f9EQbbdLkFnm2/bBXNMKqnUXMq2I8=;
        b=BZxmHaTBYKOK3bIiA8sKt1TArkGrMgInXd+b+qrBrJBTlukIoOSt7h/k3hKs9TcJRy
         NRYhEUHWk9LS/8/TwJ/NPrbUnZWoXhzh4ugtuRYH23wHk04cP24RnWAex3KV4/B2iZIW
         A4br0Taol1OdNszyWX56C8Ofez53CqMQ9MpTBQdF0ftm76/ZnnU6Ytpt6u+ffOFJWYPy
         sHdWeXlPI4RaXIfKaS1N15iEcAYSzF01J/KY7VHvKbcwI6aCLVk3ZiCrH488NMVBTCPq
         9qbs5dMVWfjso1Mnb5TgdyErWcbf+8oGYMCMfqKnRwUsZMIHJFQrwaEeTyBZQ7TbyFAR
         JLBQ==
X-Gm-Message-State: ALoCoQmoEv2ljM84SJRLl6/C8Q1+EBNrBbCbnrhpk7iZ10Qsfocf3Q5ZNgMY5g4plRfrk1i3w4qh
X-Received: by 10.58.161.101 with SMTP id xr5mr3748915veb.36.1400022149345;
        Tue, 13 May 2014 16:02:29 -0700 (PDT)
Received: from mail-ve0-f178.google.com (mail-ve0-f178.google.com [209.85.128.178])
        by mx.google.com with ESMTPSA id sb8sm30737023vdc.26.2014.05.13.16.02.28
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 13 May 2014 16:02:28 -0700 (PDT)
Received: by mail-ve0-f178.google.com with SMTP id sa20so1390679veb.9
        for <dev@spark.apache.org>; Tue, 13 May 2014 16:02:27 -0700 (PDT)
X-Received: by 10.52.153.229 with SMTP id vj5mr8290405vdb.34.1400022147718;
 Tue, 13 May 2014 16:02:27 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Tue, 13 May 2014 16:02:07 -0700 (PDT)
In-Reply-To: <1400020245.91345.YahooMailNeo@web160805.mail.bf1.yahoo.com>
References: <1400020245.91345.YahooMailNeo@web160805.mail.bf1.yahoo.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Tue, 13 May 2014 16:02:07 -0700
Message-ID: <CA+-p3AGHi6=c=C=7ePBaN+yHtAh0H6ASmVpNTMNJ-E76cxREUQ@mail.gmail.com>
Subject: Re: Class-based key in groupByKey?
To: dev@spark.apache.org, Michael Malak <michaelmalak@yahoo.com>
Content-Type: multipart/alternative; boundary=bcaec51b1aa9cae38304f9500e7a
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec51b1aa9cae38304f9500e7a
Content-Type: text/plain; charset=UTF-8

In Scala, if you override .equals() you also need to override .hashCode(),
just like in Java:

http://www.scala-lang.org/api/2.10.3/index.html#scala.AnyRef

I suspect if your .hashCode() delegates to just the hashcode of s then
you'd be good.


On Tue, May 13, 2014 at 3:30 PM, Michael Malak <michaelmalak@yahoo.com>wrote:

> Is it permissible to use a custom class (as opposed to e.g. the built-in
> String or Int) for the key in groupByKey? It doesn't seem to be working for
> me on Spark 0.9.0/Scala 2.10.3:
>
> import org.apache.spark.SparkContext
> import org.apache.spark.SparkContext._
>
> class C(val s:String) extends Serializable {
>   override def equals(o: Any) = if (o.isInstanceOf[C]) o.asInstanceOf[C].s
> == s else false
>   override def toString = s
> }
>
> object SimpleApp {
>   def main(args: Array[String]) {
>     val sc = new SparkContext("local", "Simple App", null, null)
>     val r1 = sc.parallelize(Array((new C("a"),11),(new C("a"),12)))
>     println("r1=" + r1.groupByKey.collect.mkString(";"))
>     val r2 = sc.parallelize(Array(("a",11),("a",12)))
>     println("r2=" + r2.groupByKey.collect.mkString(";"))
>   }
> }
>
>
> Output
> ======
> r1=(a,ArrayBuffer(11));(a,ArrayBuffer(12))
> r2=(a,ArrayBuffer(11, 12))
>

--bcaec51b1aa9cae38304f9500e7a--

From dev-return-7570-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 14 02:02:42 2014
Return-Path: <dev-return-7570-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A478D1196D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 May 2014 02:02:42 +0000 (UTC)
Received: (qmail 19320 invoked by uid 500); 13 May 2014 18:56:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19263 invoked by uid 500); 13 May 2014 18:56:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19255 invoked by uid 99); 13 May 2014 18:56:02 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 18:56:02 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.212.181] (HELO mail-wi0-f181.google.com) (209.85.212.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 18:55:59 +0000
Received: by mail-wi0-f181.google.com with SMTP id n15so1164614wiw.2
        for <dev@spark.apache.org>; Tue, 13 May 2014 11:55:34 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=pLM/iyaGXeEU/Rt6gaXwCASOhevZL8unS/0la7HvdFM=;
        b=BFBYylU9kDFlVz0GahMv4jfvpPJc/UPU6Q0wMDqoQsCIsBT0nHTeGqQD8r7IGuW8ja
         ZwJkR5JsGbSfnFSE/fVn7HdzPzZHahJ4COVLOBAqYFy11h/RP4rLUzc1lijJeX1H4BdY
         hHHPO4/pWpao5IEr7QktPLftC7/iH8NoTRsNg0A4r/3BVFv87/CkMnOKYNJeuB5vhCoO
         MorTWLDXp7kxDqOPTsnYZ5E12qBQvmJkUvWECWz+AZb5fySSPo2lCjKlnNflfUsiT8xk
         tNdj2xfhiNw+OIeaUQ2+LVrSi+8eY+PEcY/I5cVjcaOJIAL89mITa2uhju55WQsdT9Pw
         Pu/Q==
X-Gm-Message-State: ALoCoQlcYswVTFrgB8BCU2dXxTYB6nDcb+9C8LKb+SzBVRgDL/roGvN/7rIPbffRU29es7Mmwl6R
MIME-Version: 1.0
X-Received: by 10.194.60.4 with SMTP id d4mr28693408wjr.28.1400007334915; Tue,
 13 May 2014 11:55:34 -0700 (PDT)
Received: by 10.180.88.97 with HTTP; Tue, 13 May 2014 11:55:34 -0700 (PDT)
In-Reply-To: <CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com>
Date: Tue, 13 May 2014 11:55:34 -0700
Message-ID: <CAMJOb8=9Fc+Z=zLcmyiT6uNScneiMcQDXyi_kmWcknAY8Lzr4g@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Andrew Or <andrew@databricks.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bacb40ee1849e04f94c9be2
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bacb40ee1849e04f94c9be2
Content-Type: text/plain; charset=UTF-8

+1


2014-05-13 6:49 GMT-07:00 Sean Owen <sowen@cloudera.com>:

> On Tue, May 13, 2014 at 9:36 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > The release files, including signatures, digests, etc. can be found at:
> > http://people.apache.org/~pwendell/spark-1.0.0-rc5/
>
> Good news is that the sigs, MD5 and SHA are all correct.
>
> Tiny note: the Maven artifacts use SHA1, while the binary artifacts
> use SHA512, which took me a bit of head-scratching to figure out.
>
> If another RC comes out, I might suggest making it SHA1 everywhere?
> But there is nothing wrong with these signatures and checksums.
>
> Now to look at the contents...
>

--047d7bacb40ee1849e04f94c9be2--

From dev-return-7576-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 14 02:31:17 2014
Return-Path: <dev-return-7576-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 65FFE11A2F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 May 2014 02:31:17 +0000 (UTC)
Received: (qmail 38685 invoked by uid 500); 13 May 2014 22:31:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38620 invoked by uid 500); 13 May 2014 22:31:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38612 invoked by uid 99); 13 May 2014 22:31:16 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 22:31:16 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of michaelmalak@yahoo.com designates 216.109.115.191 as permitted sender)
Received: from [216.109.115.191] (HELO nm49-vm4.bullet.mail.bf1.yahoo.com) (216.109.115.191)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 22:31:09 +0000
Received: from [98.139.215.143] by nm49.bullet.mail.bf1.yahoo.com with NNFMP; 13 May 2014 22:30:45 -0000
Received: from [98.139.212.241] by tm14.bullet.mail.bf1.yahoo.com with NNFMP; 13 May 2014 22:30:45 -0000
Received: from [127.0.0.1] by omp1050.mail.bf1.yahoo.com with NNFMP; 13 May 2014 22:30:45 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 903491.11335.bm@omp1050.mail.bf1.yahoo.com
Received: (qmail 95050 invoked by uid 60001); 13 May 2014 22:30:45 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.com; s=s1024; t=1400020245; bh=0MfEuNoL5mOw3Fl8bejY5pzh5J17f0J17iDsZM+VUls=; h=Message-ID:Date:From:Reply-To:Subject:To:MIME-Version:Content-Type:Content-Transfer-Encoding; b=u4QR8MrE+CEaDu6p+jCbxnrGJ9kMbF+CyAiMFMiUzt9q8kwfBm9HoX4J4Lm67F+Acx+USaKSLyqCK1S4WzjHxi0L9Ld0O54h81wst9hpJSdGIiab+dgsHVg/GL9y3iOLAicUfz2awoHirkKju6O5q5Ie5WJGyyjt7A0zXUudCWs=
X-YMail-OSG: l8rJxQYVM1lJvxvxn4Pu4fY_NORyzXzV5IL66.BDQAkZKJ2
 njd.lUcAjsIObztoyy_VIa9zQl2YaK54z2NNpiMFLI1dCJ0J4SjSx39gc04v
 iri33BzEJ8bcordv3RS1OlUG_fGsx1dGALfR56p3yVS3UVy3er54Qw6TxQY.
 9YTgR2CuWJ_jecozXiIUtPTQXxwumQU0TF2UoiXDAV8uEq.A_xWemCjPX0.i
 softu4geluDUrZbm7r2aX_o5IBZyKMrtHg0fMvBWqLb8XBAHaDmbQVOgGziP
 Sq3HahpmIt2aZbtwEh.5bAKb06Ukt.sCth2BETZP.d7CTJz0Gfu7brWpPptK
 qsLWicn3i9LVGiuw2juNZ.tvNR6IQiDppSAWRv1wISmJkjL5u5EXNdTuOPjk
 w7t74UT0DE.ctIZd6WKiS1tKIuQr39iw_TfctAA5CDqAHOA.fNozdPn3AcqU
 gjQmYihJ9icBZROVcUHDFqPlVyDOMwK8JSatd8faBpBoSAfeq1v9mnJIqREZ
 DpHxX5GpzTM0mSWvJ2HrCsrJvGyg2UxKfp4beed2ryn_IvhBGoHpi
Received: from [148.87.67.200] by web160805.mail.bf1.yahoo.com via HTTP; Tue, 13 May 2014 15:30:45 PDT
X-Rocket-MIMEInfo: 002.001,SXMgaXQgcGVybWlzc2libGUgdG8gdXNlIGEgY3VzdG9tIGNsYXNzIChhcyBvcHBvc2VkIHRvIGUuZy4gdGhlIGJ1aWx0LWluIFN0cmluZyBvciBJbnQpIGZvciB0aGUga2V5IGluIGdyb3VwQnlLZXk_IEl0IGRvZXNuJ3Qgc2VlbSB0byBiZSB3b3JraW5nIGZvciBtZSBvbiBTcGFyayAwLjkuMC9TY2FsYSAyLjEwLjM6CgppbXBvcnQgb3JnLmFwYWNoZS5zcGFyay5TcGFya0NvbnRleHQKaW1wb3J0IG9yZy5hcGFjaGUuc3BhcmsuU3BhcmtDb250ZXh0Ll8KCmNsYXNzIEModmFsIHM6U3RyaW5nKSBleHRlbmRzIFMBMAEBAQE-
X-Mailer: YahooMailWebService/0.8.188.663
Message-ID: <1400020245.91345.YahooMailNeo@web160805.mail.bf1.yahoo.com>
Date: Tue, 13 May 2014 15:30:45 -0700 (PDT)
From: Michael Malak <michaelmalak@yahoo.com>
Reply-To: Michael Malak <michaelmalak@yahoo.com>
Subject: Class-based key in groupByKey?
To: "dev@spark.apache.org" <dev@spark.apache.org>
MIME-Version: 1.0
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Is it permissible to use a custom class (as opposed to e.g. the built-in St=
ring or Int) for the key in groupByKey? It doesn't seem to be working for m=
e on Spark 0.9.0/Scala 2.10.3:=0A=0Aimport org.apache.spark.SparkContext=0A=
import org.apache.spark.SparkContext._=0A=0Aclass C(val s:String) extends S=
erializable {=0A=A0 override def equals(o: Any) =3D if (o.isInstanceOf[C]) =
o.asInstanceOf[C].s =3D=3D s else false=0A=A0 override def toString =3D s=
=0A}=0A=0Aobject SimpleApp {=0A=A0 def main(args: Array[String]) {=0A=A0=A0=
=A0 val sc =3D new SparkContext("local", "Simple App", null, null)=0A=A0=A0=
=A0 val r1 =3D sc.parallelize(Array((new C("a"),11),(new C("a"),12)))=0A=A0=
=A0=A0 println("r1=3D" + r1.groupByKey.collect.mkString(";"))=0A=A0=A0=A0 v=
al r2 =3D sc.parallelize(Array(("a",11),("a",12)))=0A=A0=A0=A0 println("r2=
=3D" + r2.groupByKey.collect.mkString(";"))=0A=A0 }=0A}=0A=0A=0AOutput=0A=
=3D=3D=3D=3D=3D=3D=0Ar1=3D(a,ArrayBuffer(11));(a,ArrayBuffer(12))=0Ar2=3D(a=
,ArrayBuffer(11, 12))

From dev-return-7585-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 14 04:17:25 2014
Return-Path: <dev-return-7585-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 240CC11D10
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 May 2014 04:17:25 +0000 (UTC)
Received: (qmail 28140 invoked by uid 500); 14 May 2014 04:10:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28086 invoked by uid 500); 14 May 2014 04:10:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28078 invoked by uid 99); 14 May 2014 04:10:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 04:10:04 +0000
X-ASF-Spam-Status: No, hits=4.6 required=10.0
	tests=FROM_EXCESS_BASE64,HTML_MESSAGE,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of witgo@qq.com does not designate 54.207.19.206 as permitted sender)
Received: from [54.207.19.206] (HELO smtpbgbr1.qq.com) (54.207.19.206)
    by apache.org (qpsmtpd/0.29) with SMTP; Wed, 14 May 2014 04:10:00 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=qq.com; s=s201307;
	t=1400040574; bh=yhCYpzHH0kXl4KcVK6Dnl0xdq7J6KuQIlNUnjOQ7Kik=;
	h=X-QQ-FEAT:X-QQ-SSF:X-HAS-ATTACH:X-QQ-BUSINESS-ORIGIN:
	 X-Originating-IP:In-Reply-To:References:X-QQ-STYLE:X-QQ-mid:From:To:Subject:Mime-Version:Content-Type:Content-Transfer-Encoding:Date:
	 X-Priority:Message-ID:X-QQ-MIME:X-Mailer:X-QQ-Mailer:
	 X-QQ-ReplyHash:X-QQ-SENDSIZE;
	b=ygIOHCkQCSlCL3qDq/LWVLVLwoKyq2JFDfeJtm6HSHo2PK77OR6OOsVg8M0+iCzZp
	 UoSfqumAuUsRl99ApiFBFfM1+QPxthEo9VUHl7MHav64r7Ylo5Ur1zNOIOEV7q//gL
	 y4KE+VaWy5rrVVUMqh0ZGYGAWOdzoe0gJiB3g434=
X-QQ-FEAT: sWDmDJh0Z91CP7bvIabXFHBMLF9jxFwuu3R+JPLPPXRvIXXjAARvt0MdzDkT0
	BqAPY2oPgTBwcHj4Enu3g6XvjC1wGEX3/qO4wcczqGbLXuhxTlCel6NwymVHkIUqGqqG7gB
	LHL2TzVnAq6xud9MJ6amg8O9aJOe4rq+hZD1OxA=
X-QQ-SSF: 000000000000001000000000000000M
X-HAS-ATTACH: no
X-QQ-BUSINESS-ORIGIN: 2
X-Originating-IP: 219.142.170.212
In-Reply-To: <1400030133573-6560.post@n3.nabble.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com> <CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com> <CAMtqZeeAW6BeCiCfHbiCmKfMh=BnrO=+UM1oPe1VNgB2disQQw@mail.gmail.com> <tencent_5EABC2696C9E4E8610F1CE38@qq.com>
	<1400030133573-6560.post@n3.nabble.com>
X-QQ-STYLE: 
X-QQ-mid: webmail421t1400040573t7718666
From: "=?ISO-8859-1?B?d2l0Z28=?=" <witgo@qq.com>
To: "=?ISO-8859-1?B?ZGV2?=" <dev@spark.apache.org>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
Mime-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_5372EC7D_08960228_494E6C21"
Content-Transfer-Encoding: 8Bit
Date: Wed, 14 May 2014 12:09:33 +0800
X-Priority: 3
Message-ID: <tencent_4A0DC42353D5C43B6806EC6F@qq.com>
X-QQ-MIME: TCMime 1.0 by Tencent
X-Mailer: QQMail 2.x
X-QQ-Mailer: QQMail 2.x
X-QQ-ReplyHash: 1361412881
X-QQ-SENDSIZE: 520
X-QQ-Bgrelay: 1
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_5372EC7D_08960228_494E6C21
Content-Type: text/plain;
	charset="ISO-8859-1"
Content-Transfer-Encoding: base64

WW91IG5lZWQgdG8gc2V0OiANCnNwYXJrLmFra2EuZnJhbWVTaXplICAgICAgICAgNQ0Kc3Bh
cmsuZGVmYXVsdC5wYXJhbGxlbGlzbSAgICAxDQoNCg0KDQoNCg0KLS0tLS0tLS0tLS0tLS0t
LS0tIE9yaWdpbmFsIC0tLS0tLS0tLS0tLS0tLS0tLQ0KRnJvbTogICJNYWRodSI7PG1hZGh1
QG1hZGh1LmNvbT47DQpEYXRlOiAgV2VkLCBNYXkgMTQsIDIwMTQgMDk6MTUgQU0NClRvOiAg
ImRldiI8ZGV2QHNwYXJrLmluY3ViYXRvci5hcGFjaGUub3JnPjsgDQoNClN1YmplY3Q6ICBS
ZTogW1ZPVEVdIFJlbGVhc2UgQXBhY2hlIFNwYXJrIDEuMC4wIChyYzUpDQoNCg0KDQpJIGp1
c3QgYnVpbHQgcmM1IG9uIFdpbmRvd3MgNyBhbmQgdHJpZWQgdG8gcmVwcm9kdWNlIHRoZSBw
cm9ibGVtIGRlc2NyaWJlZCBpbg0KDQpodHRwczovL2lzc3Vlcy5hcGFjaGUub3JnL2ppcmEv
YnJvd3NlL1NQQVJLLTE3MTINCg0KSXQgd29ya3Mgb24gbXkgbWFjaGluZToNCg0KMTQvMDUv
MTMgMjE6MDY6NDcgSU5GTyBEQUdTY2hlZHVsZXI6IFN0YWdlIDEgKHN1bSBhdCA8Y29uc29s
ZT46MTcpIGZpbmlzaGVkDQppbiA0LjU0OCBzDQoxNC8wNS8xMyAyMTowNjo0NyBJTkZPIFRh
c2tTY2hlZHVsZXJJbXBsOiBSZW1vdmVkIFRhc2tTZXQgMS4wLCB3aG9zZSB0YXNrcw0KaGF2
ZSBhbGwgY29tcGxldGVkLCBmcm9tIHBvb2wNCjE0LzA1LzEzIDIxOjA2OjQ3IElORk8gU3Bh
cmtDb250ZXh0OiBKb2IgZmluaXNoZWQ6IHN1bSBhdCA8Y29uc29sZT46MTcsIHRvb2sNCjQu
ODE0OTkxOTkzIHMNCnJlczE6IERvdWJsZSA9IDUuMDAwMDA1RTExDQoNCkkgdXNlZCBhbGwg
ZGVmYXVsdHMsIG5vIGNvbmZpZyBmaWxlcyB3ZXJlIGNoYW5nZWQuDQpOb3Qgc3VyZSBpZiB0
aGF0IG1ha2VzIGEgZGlmZmVyZW5jZS4uLg0KDQoNCg0KLS0NClZpZXcgdGhpcyBtZXNzYWdl
IGluIGNvbnRleHQ6IGh0dHA6Ly9hcGFjaGUtc3BhcmstZGV2ZWxvcGVycy1saXN0LjEwMDE1
NTEubjMubmFiYmxlLmNvbS9WT1RFLVJlbGVhc2UtQXBhY2hlLVNwYXJrLTEtMC0wLXJjNS10
cDY1NDJwNjU2MC5odG1sDQpTZW50IGZyb20gdGhlIEFwYWNoZSBTcGFyayBEZXZlbG9wZXJz
IExpc3QgbWFpbGluZyBsaXN0IGFyY2hpdmUgYXQgTmFiYmxlLmNvbS4NCi4=

------=_NextPart_5372EC7D_08960228_494E6C21--




From dev-return-7579-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 14 04:33:26 2014
Return-Path: <dev-return-7579-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 228A711D7A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 May 2014 04:33:26 +0000 (UTC)
Received: (qmail 44458 invoked by uid 500); 14 May 2014 00:33:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44398 invoked by uid 500); 14 May 2014 00:33:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44390 invoked by uid 99); 14 May 2014 00:33:26 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 00:33:26 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=FROM_EXCESS_BASE64,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of witgo@qq.com designates 184.105.67.99 as permitted sender)
Received: from [184.105.67.99] (HELO smtpbg299.qq.com) (184.105.67.99)
    by apache.org (qpsmtpd/0.29) with SMTP; Wed, 14 May 2014 00:33:22 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=qq.com; s=s201307;
	t=1400027576; bh=bF7V/3Ax3bHJ9zIfbiy4wgjiLQHY+uPDHwvpfrUpm1A=;
	h=X-QQ-FEAT:X-QQ-SSF:X-HAS-ATTACH:X-QQ-BUSINESS-ORIGIN:
	 X-Originating-IP:In-Reply-To:References:X-QQ-STYLE:X-QQ-mid:From:To:Subject:Mime-Version:Content-Type:Content-Transfer-Encoding:Date:
	 X-Priority:Message-ID:X-QQ-MIME:X-Mailer:X-QQ-Mailer:
	 X-QQ-ReplyHash:X-QQ-SENDSIZE:X-QQ-FName:X-QQ-LocalIP;
	b=k6tSV70olO6JDD8tgyfN6JfFWsW3FPrCP4beZOA8pQXZnc7msp+dHDL5sOxqHZPvH
	 5gEw5ZUmxX8FG1hNK4bCCViIOP/sXk17PO+/7erbx0wiwV6lpmdBoU0pq8xZo8vNTn
	 LUPctT7ejG6ms6aHguuQAsLZdDSqzOVEp412YibY=
X-QQ-FEAT: daG0sHxrCVSyCbgYw3C35Mk+PmVsXXI9tRRe4dpnZ1cdwrBQ/v1wIJEH5aTsb
	yYwFRwmkdFmO34KY+l61Mb+LXGnF5IkwtlMAHgcPWfyA74e06q7uRs7UygMeLGaYftpEX+/
	rj8rM2iekBwh9ZVV+XQ9jl1n/jD/
X-QQ-SSF: 000000000000001000000000000000M
X-HAS-ATTACH: no
X-QQ-BUSINESS-ORIGIN: 2
X-Originating-IP: 103.242.25.144
In-Reply-To: <CABPQxssAXupM4faQD1zm5x4yObufMa7dq1SpU3dwciQV671sNQ@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com>
	<CAMtqZeeAW6BeCiCfHbiCmKfMh=BnrO=+UM1oPe1VNgB2disQQw@mail.gmail.com>
	<CABPQxssAXupM4faQD1zm5x4yObufMa7dq1SpU3dwciQV671sNQ@mail.gmail.com>
X-QQ-STYLE: 
X-QQ-mid: webmail421t1400027573t7912821
From: "=?ISO-8859-1?B?d2l0Z28=?=" <witgo@qq.com>
To: "=?ISO-8859-1?B?UGF0cmljayBXZW5kZWxs?=" <dev@spark.apache.org>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
Mime-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_5372B9B5_08EC9910_524EE588"
Content-Transfer-Encoding: 8Bit
Date: Wed, 14 May 2014 08:32:53 +0800
X-Priority: 3
Message-ID: <tencent_5EABC2696C9E4E8610F1CE38@qq.com>
X-QQ-MIME: TCMime 1.0 by Tencent
X-Mailer: QQMail 2.x
X-QQ-Mailer: QQMail 2.x
X-QQ-ReplyHash: 903496705
X-QQ-SENDSIZE: 520
X-QQ-FName: A5B228804CD849FD9243187787BD3B38
X-QQ-LocalIP: 112.95.241.173
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_5372B9B5_08EC9910_524EE588
Content-Type: text/plain;
	charset="ISO-8859-1"
Content-Transfer-Encoding: base64

LTEgDQpUaGUgZm9sbG93aW5nIGJ1ZyBzaG91bGQgYmUgZml4ZWQ6IA0KaHR0cHM6Ly9pc3N1
ZXMuYXBhY2hlLm9yZy9qaXJhL2Jyb3dzZS9TUEFSSy0xODE3DQpodHRwczovL2lzc3Vlcy5h
cGFjaGUub3JnL2ppcmEvYnJvd3NlL1NQQVJLLTE3MTINCg0KDQotLS0tLS0tLS0tLS0tLS0t
LS0gT3JpZ2luYWwgLS0tLS0tLS0tLS0tLS0tLS0tDQpGcm9tOiAgIlBhdHJpY2sgV2VuZGVs
bCI7PHB3ZW5kZWxsQGdtYWlsLmNvbT47DQpEYXRlOiAgV2VkLCBNYXkgMTQsIDIwMTQgMDQ6
MDcgQU0NClRvOiAgImRldkBzcGFyay5hcGFjaGUub3JnIjxkZXZAc3BhcmsuYXBhY2hlLm9y
Zz47IA0KDQpTdWJqZWN0OiAgUmU6IFtWT1RFXSBSZWxlYXNlIEFwYWNoZSBTcGFyayAxLjAu
MCAocmM1KQ0KDQoNCg0KSGV5IGFsbCAtIHRoZXJlIHdlcmUgc29tZSBlYXJsaWVyIFJDJ3Mg
dGhhdCB3ZXJlIG5vdCBwcmVzZW50ZWQgdG8gdGhlDQpkZXYgbGlzdCBiZWNhdXNlIGlzc3Vl
cyB3ZXJlIGZvdW5kIHdpdGggdGhlbS4gQWxzbywgdGhlcmUgc2VlbXMgdG8gYmUNCnNvbWUg
aXNzdWVzIHdpdGggdGhlIHJlbGlhYmlsaXR5IG9mIHRoZSBkZXYgbGlzdCBlLW1haWwuIEp1
c3QgYSBoZWFkcw0KdXAuDQoNCkknbGwgbGVhZCB3aXRoIGEgKzEgZm9yIHRoaXMuDQoNCk9u
IFR1ZSwgTWF5IDEzLCAyMDE0IGF0IDg6MDcgQU0sIE5hbiBaaHUgPHpodW5hbm1jZ2lsbEBn
bWFpbC5jb20+IHdyb3RlOg0KPiBqdXN0IGN1cmlvdXMsIHdoZXJlIGlzIHJjNCBWT1RFPw0K
Pg0KPiBJIHNlYXJjaGVkIG15IGdtYWlsIGJ1dCBkaWRuJ3QgZmluZCB0aGF0Pw0KPg0KPg0K
Pg0KPg0KPiBPbiBUdWUsIE1heSAxMywgMjAxNCBhdCA5OjQ5IEFNLCBTZWFuIE93ZW4gPHNv
d2VuQGNsb3VkZXJhLmNvbT4gd3JvdGU6DQo+DQo+PiBPbiBUdWUsIE1heSAxMywgMjAxNCBh
dCA5OjM2IEFNLCBQYXRyaWNrIFdlbmRlbGwgPHB3ZW5kZWxsQGdtYWlsLmNvbT4NCj4+IHdy
b3RlOg0KPj4gPiBUaGUgcmVsZWFzZSBmaWxlcywgaW5jbHVkaW5nIHNpZ25hdHVyZXMsIGRp
Z2VzdHMsIGV0Yy4gY2FuIGJlIGZvdW5kIGF0Og0KPj4gPiBodHRwOi8vcGVvcGxlLmFwYWNo
ZS5vcmcvfnB3ZW5kZWxsL3NwYXJrLTEuMC4wLXJjNS8NCj4+DQo+PiBHb29kIG5ld3MgaXMg
dGhhdCB0aGUgc2lncywgTUQ1IGFuZCBTSEEgYXJlIGFsbCBjb3JyZWN0Lg0KPj4NCj4+IFRp
bnkgbm90ZTogdGhlIE1hdmVuIGFydGlmYWN0cyB1c2UgU0hBMSwgd2hpbGUgdGhlIGJpbmFy
eSBhcnRpZmFjdHMNCj4+IHVzZSBTSEE1MTIsIHdoaWNoIHRvb2sgbWUgYSBiaXQgb2YgaGVh
ZC1zY3JhdGNoaW5nIHRvIGZpZ3VyZSBvdXQuDQo+Pg0KPj4gSWYgYW5vdGhlciBSQyBjb21l
cyBvdXQsIEkgbWlnaHQgc3VnZ2VzdCBtYWtpbmcgaXQgU0hBMSBldmVyeXdoZXJlPw0KPj4g
QnV0IHRoZXJlIGlzIG5vdGhpbmcgd3Jvbmcgd2l0aCB0aGVzZSBzaWduYXR1cmVzIGFuZCBj
aGVja3N1bXMuDQo+Pg0KPj4gTm93IHRvIGxvb2sgYXQgdGhlIGNvbnRlbnRzLi4uDQo+Pg0K
Lg==

------=_NextPart_5372B9B5_08EC9910_524EE588--


From dev-return-7569-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 14 05:46:09 2014
Return-Path: <dev-return-7569-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4994511F8C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 May 2014 05:46:09 +0000 (UTC)
Received: (qmail 51996 invoked by uid 500); 13 May 2014 18:39:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51939 invoked by uid 500); 13 May 2014 18:39:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51931 invoked by uid 99); 13 May 2014 18:39:29 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 18:39:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.81 as permitted sender)
Received: from [171.67.219.81] (HELO smtp.stanford.edu) (171.67.219.81)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 18:39:25 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id BC1AC213B1
	for <dev@spark.apache.org>; Tue, 13 May 2014 11:39:01 -0700 (PDT)
Received: from mail-qg0-f43.google.com (mail-qg0-f43.google.com [209.85.192.43])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 89CEA213F6
	for <dev@spark.apache.org>; Tue, 13 May 2014 11:39:00 -0700 (PDT)
Received: by mail-qg0-f43.google.com with SMTP id 63so991274qgz.2
        for <dev@spark.apache.org>; Tue, 13 May 2014 11:38:59 -0700 (PDT)
X-Gm-Message-State: ALoCoQkBADzOM0c5zqXtEoZO0Zf8XLcrjDboivbvzWRzZ8lKYYS4WreuTDMX1UC2b+cFIJm2ZG8E
MIME-Version: 1.0
X-Received: by 10.140.92.200 with SMTP id b66mr46931904qge.41.1400006339639;
 Tue, 13 May 2014 11:38:59 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Tue, 13 May 2014 11:38:59 -0700 (PDT)
In-Reply-To: <CA+B-+fwL5OfJawkMNzn9BP4hiMVcPzgwR-a-7BE_cTOna27ufw@mail.gmail.com>
References: <CA+B-+fwL5OfJawkMNzn9BP4hiMVcPzgwR-a-7BE_cTOna27ufw@mail.gmail.com>
Date: Tue, 13 May 2014 11:38:59 -0700
Message-ID: <CAEYYnxYirMYDUTa7HhCUo__Wdd5StemXQTh-bc=VhWfAHspYtQ@mail.gmail.com>
Subject: Re: Multinomial Logistic Regression
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113a332c8fe00504f94c600e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a332c8fe00504f94c600e
Content-Type: text/plain; charset=UTF-8

Hi Deb,

For K possible outcomes in multinomial logistic regression,  we can have
K-1 independent binary logistic regression models, in which one outcome is
chosen as a "pivot" and then the other K-1 outcomes are separately
regressed against the pivot outcome. See my presentation for technical
detail http://www.slideshare.net/dbtsai/2014-0501-mlor

Since mllib only supports one linear model per classification model, there
will be some infrastructure work to support MLOR in mllib. But if you want
to implement yourself with the L-BFGS solver in mllib, you can follow the
equation in my slide, and it will not be too difficult.

I can give you the gradient method for multinomial logistic regression, you
just need to put the K-1 intercepts in the right place.

  def computeGradient(y: Double, x: Array[Double], lambda: Double, w:
Array[Array[Double]], b: Array[Double],
    gradient: Array[Double]): (Double, Int) = {
    val classes = b.length + 1
    val yy = y.toInt

    def alpha(i: Int): Int = {
      if (i == 0) 1 else 0
    }

    def delta(i: Int, j: Int): Int = {
      if (i == j) 1 else 0
    }

    var denominator: Double = 1.0
    val numerators: Array[Double] = Array.ofDim[Double](b.length)

    var predicted = 1

    {
      var i = 0
      var j = 0
      var acc: Double = 0
      while (i < b.length) {
        acc = b(i)
        j = 0
        while (j < x.length) {
          acc += x(j) * w(i)(j)
          j += 1
        }
        numerators(i) = math.exp(acc)
        if (i > 0 && numerators(i) > numerators(predicted - 1)) {
          predicted = i + 1
        }
        denominator += numerators(i)
        i += 1
      }
      if (numerators(predicted - 1) < 1) {
        predicted = 0
      }
    }

    {
      // gradient has dim of (classes-1) * (x.length+1)
      var i = 0
      var m1: Int = 0
      var l1: Int = 0
      while (i < (classes - 1) * (x.length + 1)) {
        m1 = i % (x.length + 1) // m0 is intercept
        l1 = (i - m1) / (x.length + 1) // l + 1 is class
        if (m1 == 0) {
          gradient(i) += (1 - alpha(yy)) * delta(yy, l1 + 1) -
numerators(l1) / denominator
        } else {
          gradient(i) += ((1 - alpha(yy)) * delta(yy, l1 + 1) -
numerators(l1) / denominator) * x(m1 - 1)
        }
        i += 1
      }
    }
    val loglike: Double = math.round(y).toInt match {
      case 0 => math.log(1.0 / denominator)
      case _ => math.log(numerators(math.round(y - 1).toInt) / denominator)
    }
    (loglike, predicted)
  }



Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Tue, May 13, 2014 at 4:08 AM, Debasish Das <debasish.das83@gmail.com>wrote:

> Hi,
>
> Is there a PR for multinomial logistic regression which does one-vs-all and
> compare it to the other possibilities ?
>
> @dbtsai in your strata presentation you used one vs all ? Did you add some
> constraints on the fact that you penalize if mis-predicted labels are not
> very far from the true label ?
>
> Thanks.
> Deb
>

--001a113a332c8fe00504f94c600e--

From dev-return-7584-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 14 06:11:26 2014
Return-Path: <dev-return-7584-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D928911066
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 May 2014 06:11:26 +0000 (UTC)
Received: (qmail 9798 invoked by uid 500); 14 May 2014 03:24:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9742 invoked by uid 500); 14 May 2014 03:24:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9734 invoked by uid 99); 14 May 2014 03:24:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 03:24:46 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.170] (HELO mail-vc0-f170.google.com) (209.85.220.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 03:24:41 +0000
Received: by mail-vc0-f170.google.com with SMTP id lf12so1705853vcb.1
        for <dev@spark.apache.org>; Tue, 13 May 2014 20:24:20 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=xxXDHDj/wm2bwt6Rarc8pyXhQj5bizz/nmXGoEOjISo=;
        b=A0Xr808DAHrtu+KMvN2kuanLIujgVct8/1lBtGpu95A0we2mVfr7H8lnMTgbxgjb87
         d1mEOkcZnaehsvCgq8EfXDmdQ8ud9kwMbF8TM6gr9gAjK58GNVQs1ekJy5KU5QigLrXD
         DahF76IKPhbay7BnOS4VJE3kXL9JmiEnNBtphtPHeJBYjFB7JgSL6+BOAp8z/6k9CORD
         Q0EitavCPHka8D1Ba/Kr4x/3p98vdJ12q0RcK6t0txDZfa4CqL8X2Ijmge6YJwet5P53
         UD+BrE4htVs0IkWdan8bgmAEoPTY+/rsBzp4MK+fAhcQ+Z5kaluDaZYa6Xh89SJbtgpd
         2vpg==
X-Gm-Message-State: ALoCoQk1mmz7h8nRNWNgWQGnQeeaWhu68T2047g8TS+6eyBmWa6aqv6+xKK/MY2zV4daMHFUhtrw
X-Received: by 10.221.27.8 with SMTP id ro8mr791592vcb.30.1400037860622;
        Tue, 13 May 2014 20:24:20 -0700 (PDT)
Received: from mail-vc0-f173.google.com (mail-vc0-f173.google.com [209.85.220.173])
        by mx.google.com with ESMTPSA id l4sm980233vdb.3.2014.05.13.20.24.19
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 13 May 2014 20:24:19 -0700 (PDT)
Received: by mail-vc0-f173.google.com with SMTP id il7so1674315vcb.4
        for <dev@spark.apache.org>; Tue, 13 May 2014 20:24:18 -0700 (PDT)
X-Received: by 10.220.81.194 with SMTP id y2mr729910vck.29.1400037858915; Tue,
 13 May 2014 20:24:18 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Tue, 13 May 2014 20:23:58 -0700 (PDT)
In-Reply-To: <CAAswR-49jXVdPiGW=e1ivg0V3otKfmu0kGeLSrMNeQhy_XC2KA@mail.gmail.com>
References: <CA+-p3AHXQZttzFG_TC0P2abupPthPY0EhcSxDAsa90Rvu45qFg@mail.gmail.com>
 <CAAswR-49jXVdPiGW=e1ivg0V3otKfmu0kGeLSrMNeQhy_XC2KA@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Tue, 13 May 2014 20:23:58 -0700
Message-ID: <CA+-p3AECrkBcNWUTNq5T2ttnhYm-xeP2LVMObrjcb+VgwDuPBQ@mail.gmail.com>
Subject: Re: Preliminary Parquet numbers and including .count() in Catalyst
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2d86040bd2804f953b7fb
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2d86040bd2804f953b7fb
Content-Type: text/plain; charset=UTF-8

Thanks for filing -- I'm keeping my eye out for updates on that ticket.

Cheers!
Andrew


On Tue, May 13, 2014 at 2:40 PM, Michael Armbrust <michael@databricks.com>wrote:

> >
> > It looks like currently the .count() on parquet is handled incredibly
> > inefficiently and all the columns are materialized.  But if I select just
> > that relevant column and then count, then the column-oriented storage of
> > Parquet really shines.
> >
> > There ought to be a potential optimization here such that a .count() on a
> > SchemaRDD backed by Parquet doesn't require re-assembling the rows, as
> > that's expensive.  I don't think .count() is handled specially in
> > SchemaRDDs, but it seems ripe for optimization.
> >
>
> Yeah, you are right.  Thanks for pointing this out!
>
> If you call .count() that is just the native Spark count, which is not
> aware of the potential optimizations.  We could just override count() in a
> schema RDD to be something like
> "groupBy()(Count(Literal(1))).collect().head.getInt(0)"
>
> Here is a JIRA: SPARK-1822 - SchemaRDD.count() should use the
> optimizer.<https://issues.apache.org/jira/browse/SPARK-1822>
>
> Michael
>

--001a11c2d86040bd2804f953b7fb--

From dev-return-7563-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 14 06:53:03 2014
Return-Path: <dev-return-7563-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1E3AE111AC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 May 2014 06:53:03 +0000 (UTC)
Received: (qmail 57126 invoked by uid 500); 13 May 2014 14:53:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57066 invoked by uid 500); 13 May 2014 14:53:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57058 invoked by uid 99); 13 May 2014 14:53:02 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 14:53:02 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.44 as permitted sender)
Received: from [209.85.220.44] (HELO mail-pa0-f44.google.com) (209.85.220.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 14:52:59 +0000
Received: by mail-pa0-f44.google.com with SMTP id ld10so377272pab.3
        for <dev@spark.apache.org>; Tue, 13 May 2014 07:52:35 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=4kJTGrVI8qI4carWNlPhmLmEWFlknq4be67MyCroQlI=;
        b=XKy7mDBCXyQQnSsovRrriDAyDuKsFcvufzCw29pmQLRHv7otGasvqsYK15/UYNskH8
         PqSdeBRl5I5LuQOJvrULgr2KaXSFIly7MHGalIgVGRzNyjRzm2NFbVYUQq3wBnFPGWSz
         pRt4fXwaTXp+5sImk44kX35TiHukwyGtFVd4HePJRlOBbFJ1zzbjC4rNAqQ3tNcq86ZA
         GGqkr7rw/EoH/m1rcRd2jH47H+7AJJ0nYpiSyFPPxUYO3CpS6u6IeqeKIvuIdgbyTSln
         IDXrf0yTy6kJRPpOcjnN+5AtcTXsKEnD4Cw6Krk5zEuwEWXf7r4gJ4ryZbjhhnvzBMos
         Oa2w==
X-Received: by 10.68.197.99 with SMTP id it3mr5732083pbc.37.1399992755421;
        Tue, 13 May 2014 07:52:35 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id ay3sm25292062pbb.62.2014.05.13.07.52.31
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 13 May 2014 07:52:32 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com>
Date: Tue, 13 May 2014 07:52:30 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <C40F430D-0753-426A-BCCB-93709F966B04@gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com> <CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

SHA-1 is being end-of-lived so I=92d actually say switch to 512 for all =
of them instead.

On May 13, 2014, at 6:49 AM, Sean Owen <sowen@cloudera.com> wrote:

> On Tue, May 13, 2014 at 9:36 AM, Patrick Wendell <pwendell@gmail.com> =
wrote:
>> The release files, including signatures, digests, etc. can be found =
at:
>> http://people.apache.org/~pwendell/spark-1.0.0-rc5/
>=20
> Good news is that the sigs, MD5 and SHA are all correct.
>=20
> Tiny note: the Maven artifacts use SHA1, while the binary artifacts
> use SHA512, which took me a bit of head-scratching to figure out.
>=20
> If another RC comes out, I might suggest making it SHA1 everywhere?
> But there is nothing wrong with these signatures and checksums.
>=20
> Now to look at the contents...


From dev-return-7586-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 14 07:09:33 2014
Return-Path: <dev-return-7586-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 53CC311215
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 May 2014 07:09:33 +0000 (UTC)
Received: (qmail 35072 invoked by uid 500); 14 May 2014 07:02:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35015 invoked by uid 500); 14 May 2014 07:02:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35007 invoked by uid 99); 14 May 2014 07:02:53 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 07:02:53 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.176 as permitted sender)
Received: from [209.85.214.176] (HELO mail-ob0-f176.google.com) (209.85.214.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 07:02:50 +0000
Received: by mail-ob0-f176.google.com with SMTP id wo20so1648434obc.7
        for <dev@spark.apache.org>; Wed, 14 May 2014 00:02:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=MUSvKqMSK9C7HnaGcjc8UQOFSCJwtUw6MDJQC77E1GQ=;
        b=DjO8GX3hkTqgBqDYNfe+qpnLqGothnbDVGuX+IT4m/pVTbq07iZQDqHLUikVoTfW5k
         TDof5tehkwvKGLTvE8ie3D+QlFNqVvaXx9Or0Vta4xWYPHtup71qJuSG5i6nNLFOucW3
         cFouj5JSqs6YVitZwKSXNCN5MiK7+lkTpnQ1LiWERC3FAVkuVAXk3EtTwhPCTLWmo/J3
         wyWW2cUATEHODqayVD6tYUX1KgP3dWzpsBdqkgk/VkYIW4KrWKgordIUbPlC0VCSxoJc
         ogb9hW0uRoruwgJQotl37zAvx7Up+RRa+0ImZQV3zMi+XpdDk6Weij0HM4a5RpJU9T/P
         f2Xw==
MIME-Version: 1.0
X-Received: by 10.182.163.45 with SMTP id yf13mr1669877obb.66.1400050946749;
 Wed, 14 May 2014 00:02:26 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Wed, 14 May 2014 00:02:26 -0700 (PDT)
In-Reply-To: <tencent_5EABC2696C9E4E8610F1CE38@qq.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com>
	<CAMtqZeeAW6BeCiCfHbiCmKfMh=BnrO=+UM1oPe1VNgB2disQQw@mail.gmail.com>
	<CABPQxssAXupM4faQD1zm5x4yObufMa7dq1SpU3dwciQV671sNQ@mail.gmail.com>
	<tencent_5EABC2696C9E4E8610F1CE38@qq.com>
Date: Wed, 14 May 2014 00:02:26 -0700
Message-ID: <CABPQxstBPL6goZaR1F3pwZR=04yS8nCq9hxNC7Yq=yT1j8vEvQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey @witgo - those bugs are not severe enough to block the release,
but it would be nice to get them fixed.

At this point we are focused on severe bugs with an immediate fix, or
regressions from previous versions of Spark. Anything that misses this
release will get merged into the branch-1.0 branch and make it into
the 1.0.1 release, so people will have access to it.

On Tue, May 13, 2014 at 5:32 PM, witgo <witgo@qq.com> wrote:
> -1
> The following bug should be fixed:
> https://issues.apache.org/jira/browse/SPARK-1817
> https://issues.apache.org/jira/browse/SPARK-1712
>
>
> ------------------ Original ------------------
> From:  "Patrick Wendell";<pwendell@gmail.com>;
> Date:  Wed, May 14, 2014 04:07 AM
> To:  "dev@spark.apache.org"<dev@spark.apache.org>;
>
> Subject:  Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
>
>
>
> Hey all - there were some earlier RC's that were not presented to the
> dev list because issues were found with them. Also, there seems to be
> some issues with the reliability of the dev list e-mail. Just a heads
> up.
>
> I'll lead with a +1 for this.
>
> On Tue, May 13, 2014 at 8:07 AM, Nan Zhu <zhunanmcgill@gmail.com> wrote:
>> just curious, where is rc4 VOTE?
>>
>> I searched my gmail but didn't find that?
>>
>>
>>
>>
>> On Tue, May 13, 2014 at 9:49 AM, Sean Owen <sowen@cloudera.com> wrote:
>>
>>> On Tue, May 13, 2014 at 9:36 AM, Patrick Wendell <pwendell@gmail.com>
>>> wrote:
>>> > The release files, including signatures, digests, etc. can be found at:
>>> > http://people.apache.org/~pwendell/spark-1.0.0-rc5/
>>>
>>> Good news is that the sigs, MD5 and SHA are all correct.
>>>
>>> Tiny note: the Maven artifacts use SHA1, while the binary artifacts
>>> use SHA512, which took me a bit of head-scratching to figure out.
>>>
>>> If another RC comes out, I might suggest making it SHA1 everywhere?
>>> But there is nothing wrong with these signatures and checksums.
>>>
>>> Now to look at the contents...
>>>
> .

From dev-return-7572-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 14 07:14:24 2014
Return-Path: <dev-return-7572-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1C1E911230
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 May 2014 07:14:24 +0000 (UTC)
Received: (qmail 35359 invoked by uid 500); 13 May 2014 20:07:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35305 invoked by uid 500); 13 May 2014 20:07:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35297 invoked by uid 99); 13 May 2014 20:07:44 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 20:07:44 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.41 as permitted sender)
Received: from [209.85.219.41] (HELO mail-oa0-f41.google.com) (209.85.219.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 20:07:41 +0000
Received: by mail-oa0-f41.google.com with SMTP id m1so1047238oag.28
        for <dev@spark.apache.org>; Tue, 13 May 2014 13:07:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=JhIUEmRHQOdHJDNN170jlRn9/pOQ6Nuqr9n4iWf91n0=;
        b=FjC8LwZk5z5nOIDpPyEYn8osmBurOIMRj1K3nQHWbOCD0sbT43R09F9SNGO/48LUJb
         0nR7j/3J5GIVjSa2yyEPBW4YqqKbJ4wWrMLN4e8e2znU4UflkngNZX9WlpPT793mKHi1
         a7cBUuaLewvXEzYGMNbCX+gNIMd4UeR2P7IKSyzymarlfKExXuProu2n54l9UKrfnb/a
         oLB9YF3NBu4tgXJ65C8e2ETHOJnWPVW4Xg7dkHidwL5TOwXuKLH3JkMAfBO8OjFRQJEP
         BnMlGQijl2liZfoXcf+QCD0pCwswJCC1KEcGhZWY9eEdBnsTX03vYTwzb+9GcgzPeHJB
         +yZQ==
MIME-Version: 1.0
X-Received: by 10.60.54.38 with SMTP id g6mr5610155oep.79.1400011635231; Tue,
 13 May 2014 13:07:15 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Tue, 13 May 2014 13:07:15 -0700 (PDT)
In-Reply-To: <CAMtqZeeAW6BeCiCfHbiCmKfMh=BnrO=+UM1oPe1VNgB2disQQw@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com>
	<CAMtqZeeAW6BeCiCfHbiCmKfMh=BnrO=+UM1oPe1VNgB2disQQw@mail.gmail.com>
Date: Tue, 13 May 2014 13:07:15 -0700
Message-ID: <CABPQxssAXupM4faQD1zm5x4yObufMa7dq1SpU3dwciQV671sNQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey all - there were some earlier RC's that were not presented to the
dev list because issues were found with them. Also, there seems to be
some issues with the reliability of the dev list e-mail. Just a heads
up.

I'll lead with a +1 for this.

On Tue, May 13, 2014 at 8:07 AM, Nan Zhu <zhunanmcgill@gmail.com> wrote:
> just curious, where is rc4 VOTE?
>
> I searched my gmail but didn't find that?
>
>
>
>
> On Tue, May 13, 2014 at 9:49 AM, Sean Owen <sowen@cloudera.com> wrote:
>
>> On Tue, May 13, 2014 at 9:36 AM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>> > The release files, including signatures, digests, etc. can be found at:
>> > http://people.apache.org/~pwendell/spark-1.0.0-rc5/
>>
>> Good news is that the sigs, MD5 and SHA are all correct.
>>
>> Tiny note: the Maven artifacts use SHA1, while the binary artifacts
>> use SHA512, which took me a bit of head-scratching to figure out.
>>
>> If another RC comes out, I might suggest making it SHA1 everywhere?
>> But there is nothing wrong with these signatures and checksums.
>>
>> Now to look at the contents...
>>

From dev-return-7552-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 14 07:17:37 2014
Return-Path: <dev-return-7552-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1C60E11248
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 May 2014 07:17:37 +0000 (UTC)
Received: (qmail 36413 invoked by uid 500); 13 May 2014 06:17:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36353 invoked by uid 500); 13 May 2014 06:17:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36345 invoked by uid 99); 13 May 2014 06:17:37 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 06:17:37 +0000
X-ASF-Spam-Status: No, hits=1.0 required=10.0
	tests=SPF_SOFTFAIL
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of taeyun.kim@innowireless.co.kr does not designate 59.12.193.80 as permitted sender)
Received: from [59.12.193.80] (HELO mail.innowireless.co.kr) (59.12.193.80)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 06:17:32 +0000
Received: from INNOC358 (218.154.29.110) by mail.innowireless.co.kr
 (59.12.193.80) with Microsoft SMTP Server id 8.2.255.0; Tue, 13 May 2014
 15:09:47 +0900
From: innowireless TaeYun Kim <taeyun.kim@innowireless.co.kr>
To: <dev@spark.apache.org>
Subject: Is this supported? : Spark on Windows, Hadoop YARN on Linux.
Date: Tue, 13 May 2014 15:17:46 +0900
Message-ID: <000001cf6e73$0eea6350$2cbf29f0$@innowireless.co.kr>
MIME-Version: 1.0
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: 7bit
X-Mailer: Microsoft Outlook 14.0
Thread-Index: Ac9ucZweEYJ/4jU2TSaV4l4IV8IrHQ==
Content-Language: ko
X-Virus-Checked: Checked by ClamAV on apache.org

I'm trying to run spark-shell on Windows that uses Hadoop YARN on Linux.
Specifically, the environment is as follows:

- Client
  - OS: Windows 7
  - Spark version: 1.0.0-SNAPSHOT (git cloned 2014.5.8)
- Server
  - Platform: hortonworks sandbox 2.1

I has to modify the spark source code to apply
https://issues.apache.org/jira/browse/YARN-1824, so that the cross-platform
issues can be addressed. (that is, change $() to $$(), File.pathSeparator to
ApplicationConstants.CLASS_PATH_SEPARATOR).
Seeing this, I suspect that the Spark code for now is not prepared to
support cross-platform submit, that is, Spark on Windows -> Hadoop YARN on
Linux.

Anyways, after the modification and some configuration tweak, at least the
yarn-client mode spark-shell submitted from Windows 7 seems to try to start.
But the ApplicationManager fails to register.
Yarn server log is as follows: ('owner' is the user name of the Windows 7
machine.)

Log Type: stderr
Log Length: 1356
log4j:WARN No appenders could be found for logger
(org.apache.hadoop.util.Shell).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for
more info.
14/05/12 01:13:54 INFO YarnSparkHadoopUtil: Using Spark's default log4j
profile: org/apache/spark/log4j-defaults.properties
14/05/12 01:13:54 INFO SecurityManager: Changing view acls to: yarn,owner
14/05/12 01:13:54 INFO SecurityManager: SecurityManager: authentication
disabled; ui acls disabled; users with view permissions: Set(yarn, owner)
14/05/12 01:13:55 INFO Slf4jLogger: Slf4jLogger started
14/05/12 01:13:56 INFO Remoting: Starting remoting
14/05/12 01:13:56 INFO Remoting: Remoting started; listening on addresses
:[akka.tcp://sparkYarnAM@sandbox.hortonworks.com:47074]
14/05/12 01:13:56 INFO Remoting: Remoting now listens on addresses:
[akka.tcp://sparkYarnAM@sandbox.hortonworks.com:47074]
14/05/12 01:13:56 INFO RMProxy: Connecting to ResourceManager at
/0.0.0.0:8030
14/05/12 01:13:56 INFO ExecutorLauncher: ApplicationAttemptId:
appattempt_1399856448891_0018_000001
14/05/12 01:13:56 INFO ExecutorLauncher: Registering the ApplicationMaster
14/05/12 01:13:56 WARN Client: Exception encountered while connecting to the
server : org.apache.hadoop.security.AccessControlException: Client cannot
authenticate via:[TOKEN]

How can I handle this error?
Or, should I give up and use Linux for my client machine?
(I want to use Windows for client, since for me it's more comfortable to
develop applications.)
BTW, I'm a newbie for Spark and Hadoop.

Thanks in advance.



From dev-return-7565-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 14 09:54:22 2014
Return-Path: <dev-return-7565-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 24039116C8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 May 2014 09:54:22 +0000 (UTC)
Received: (qmail 16008 invoked by uid 500); 13 May 2014 15:07:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15956 invoked by uid 500); 13 May 2014 15:07:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15948 invoked by uid 99); 13 May 2014 15:07:39 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 15:07:39 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.192.54 as permitted sender)
Received: from [209.85.192.54] (HELO mail-qg0-f54.google.com) (209.85.192.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 15:07:36 +0000
Received: by mail-qg0-f54.google.com with SMTP id q108so556550qgd.27
        for <dev@spark.apache.org>; Tue, 13 May 2014 08:07:13 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=nheBriVtn9uDQsHhaq1lY49Sk/CNiGilxBHstqQqyq0=;
        b=VufgsnLWT+xdIz94ekreFBDHW49oAQ4bVzhdYGfUVzKpXDmFvlirBIJm3B8HFOlYdu
         ubBDq6/6ELLpspvGHSmSE7vq4I/0uLyBzMK1TFIqeHYknopjun248lXmmhwtWrrbaoSc
         k++beKWiobPdSS1d+ZoX8Ukbx2NtfM3GbpWKsTRNvL8RNRkWW0qtxNCwdkLlv8Q1WvVf
         kdq8tI1Gk6DSU/llpFrs7GomRM4jnfVuR0Ngd1HgotAVBUynJCT6x0/kzslua2wp370g
         a96GM/+9yepgbvAPI3ZTQb9PScdnzj/E4nMF0lIxVtsPRZ6Fdum2dRmUn7DTYqGWlwaR
         b8dg==
MIME-Version: 1.0
X-Received: by 10.140.51.74 with SMTP id t68mr45588645qga.50.1399993633103;
 Tue, 13 May 2014 08:07:13 -0700 (PDT)
Received: by 10.140.92.55 with HTTP; Tue, 13 May 2014 08:07:12 -0700 (PDT)
In-Reply-To: <CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com>
Date: Tue, 13 May 2014 11:07:12 -0400
Message-ID: <CAMtqZeeAW6BeCiCfHbiCmKfMh=BnrO=+UM1oPe1VNgB2disQQw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Nan Zhu <zhunanmcgill@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1135235e307a8004f9496b6c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1135235e307a8004f9496b6c
Content-Type: text/plain; charset=UTF-8

just curious, where is rc4 VOTE?

I searched my gmail but didn't find that?




On Tue, May 13, 2014 at 9:49 AM, Sean Owen <sowen@cloudera.com> wrote:

> On Tue, May 13, 2014 at 9:36 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > The release files, including signatures, digests, etc. can be found at:
> > http://people.apache.org/~pwendell/spark-1.0.0-rc5/
>
> Good news is that the sigs, MD5 and SHA are all correct.
>
> Tiny note: the Maven artifacts use SHA1, while the binary artifacts
> use SHA512, which took me a bit of head-scratching to figure out.
>
> If another RC comes out, I might suggest making it SHA1 everywhere?
> But there is nothing wrong with these signatures and checksums.
>
> Now to look at the contents...
>

--001a1135235e307a8004f9496b6c--

From dev-return-7587-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 14 10:04:59 2014
Return-Path: <dev-return-7587-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 42CB01170E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 May 2014 10:04:59 +0000 (UTC)
Received: (qmail 56561 invoked by uid 500); 14 May 2014 07:18:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 56504 invoked by uid 500); 14 May 2014 07:18:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56496 invoked by uid 99); 14 May 2014 07:18:19 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 07:18:19 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=FROM_EXCESS_BASE64,HTML_MESSAGE,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of witgo@qq.com designates 14.17.44.36 as permitted sender)
Received: from [14.17.44.36] (HELO smtpbg341.qq.com) (14.17.44.36)
    by apache.org (qpsmtpd/0.29) with SMTP; Wed, 14 May 2014 07:18:15 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=qq.com; s=s201307;
	t=1400051867; bh=WiCMRN6+kCpa7WYbcSWn0QDhI8SlIgU+17Bhz+PXAKs=;
	h=X-QQ-FEAT:X-QQ-SSF:X-HAS-ATTACH:X-QQ-BUSINESS-ORIGIN:
	 X-Originating-IP:In-Reply-To:References:X-QQ-STYLE:X-QQ-mid:From:To:Subject:Mime-Version:Content-Type:Content-Transfer-Encoding:Date:
	 X-Priority:Message-ID:X-QQ-MIME:X-Mailer:X-QQ-Mailer:
	 X-QQ-ReplyHash:X-QQ-SENDSIZE;
	b=oY7fHEDTs6/0jmXT1QP/b/64+baYuiiNhkjLz1KBcNqe1uJGVYCbY6DM8jVcc/0PO
	 KFTRzvwRRekHigyOGx8tlnRzWUPPUN6PzTLOSxcP1xS0ScXnWnocOQMWH+MPm274J+
	 Ld/WSD2nTmSFz3E75paRGxBCUfFqfAW8ovBiTXAg=
X-QQ-FEAT: X26KN2quZSeIOR4cjWlxufoRuJbmCmqEVvlqGGm6fGdl8grtnZEYKiqhKKvxu
	x7YGOUO9DawR4NDxeZHkJXwEju4qezKhcrFZhDv8yS57u/kbmBnTPO76nrWY/Mzb4h0VfRM
	y+wldhf5yUX78hJZa8GXaYF8YRABVuh2C7/odD8=
X-QQ-SSF: 000000000000001000000000000000M
X-HAS-ATTACH: no
X-QQ-BUSINESS-ORIGIN: 2
X-Originating-IP: 219.142.170.212
In-Reply-To: <CABPQxstBPL6goZaR1F3pwZR=04yS8nCq9hxNC7Yq=yT1j8vEvQ@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com>
	<CAMtqZeeAW6BeCiCfHbiCmKfMh=BnrO=+UM1oPe1VNgB2disQQw@mail.gmail.com>
	<CABPQxssAXupM4faQD1zm5x4yObufMa7dq1SpU3dwciQV671sNQ@mail.gmail.com>
	<tencent_5EABC2696C9E4E8610F1CE38@qq.com>
	<CABPQxstBPL6goZaR1F3pwZR=04yS8nCq9hxNC7Yq=yT1j8vEvQ@mail.gmail.com>
X-QQ-STYLE: 
X-QQ-mid: webmail421t1400051866t336276
From: "=?ISO-8859-1?B?d2l0Z28=?=" <witgo@qq.com>
To: "=?ISO-8859-1?B?UGF0cmljayBXZW5kZWxs?=" <dev@spark.apache.org>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
Mime-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_5373189A_08DDA3A0_2F141827"
Content-Transfer-Encoding: 8Bit
Date: Wed, 14 May 2014 15:17:46 +0800
X-Priority: 3
Message-ID: <tencent_4CAB53D66DED27DB54B1317C@qq.com>
X-QQ-MIME: TCMime 1.0 by Tencent
X-Mailer: QQMail 2.x
X-QQ-Mailer: QQMail 2.x
X-QQ-ReplyHash: 168042532
X-QQ-SENDSIZE: 520
X-QQ-Bgrelay: 1
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_5373189A_08DDA3A0_2F141827
Content-Type: text/plain;
	charset="ISO-8859-1"
Content-Transfer-Encoding: base64
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------=_NextPart_5373189A_08DDA3A0_2F141827--




From dev-return-7588-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 14 11:55:59 2014
Return-Path: <dev-return-7588-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5126C11A18
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 May 2014 11:55:59 +0000 (UTC)
Received: (qmail 61361 invoked by uid 500); 14 May 2014 07:55:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61301 invoked by uid 500); 14 May 2014 07:55:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61293 invoked by uid 99); 14 May 2014 07:55:59 -0000
Received: from Unknown (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 07:55:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.219.44] (HELO mail-oa0-f44.google.com) (209.85.219.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 07:55:55 +0000
Received: by mail-oa0-f44.google.com with SMTP id o6so1752046oag.31
        for <dev@spark.apache.org>; Wed, 14 May 2014 00:55:34 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=vTpEEMj5dDOBIyrXUqvJsm3NfcaQYUzZXPXznl+DCp0=;
        b=HBOx3UE7uoxZJ4dJlaiqg1AENqfJRUP7+oAiQSnVuDqZrkXR1znFRko3sktjTB5IbM
         guyKCc2RIB5xbbYgJ8Y6qthdWxmFqvFK979X4yF8M/G73y1+M+AoJ+71JI6Ds5jNGq8t
         RZXrtYtIKmky183vEdPMkGk3wrkALl0a6YIXWg+wdh0MA0o5eDmL7K2FttL630t27+e+
         hWoqD43YrWRE+OY+w47CpAn2vrmBtMhYiQ5zIDwJt4gPMliIIeNtsmmI6ixqOWE6lzEk
         0BlsESPd3qnY0Teova3Pqh/R7OvheQdQsgGvJHNNmzAzg7msK0KuZECpqPvRmuiiXT5I
         gdyw==
X-Gm-Message-State: ALoCoQm2vOUeF35AjaWvgBa2IV7iAehx/Modk6JuKkG7TmMyk3EVUYpTzHv4sA1KAncFEVJZ5UWl
MIME-Version: 1.0
X-Received: by 10.60.50.197 with SMTP id e5mr1828884oeo.39.1400054134146; Wed,
 14 May 2014 00:55:34 -0700 (PDT)
Received: by 10.182.184.40 with HTTP; Wed, 14 May 2014 00:55:34 -0700 (PDT)
In-Reply-To: <AF52EE5D-2C7A-47F8-9533-8B91138A5AB8@gmail.com>
References: <CAJ3iqPSA0kVT7oncntcZecC1C3z4=_RZ=W1hyt=-FFQjZFj_3g@mail.gmail.com>
	<AF52EE5D-2C7A-47F8-9533-8B91138A5AB8@gmail.com>
Date: Wed, 14 May 2014 00:55:34 -0700
Message-ID: <CAJ3iqPSpR4fdQSFPbqtU8CgVhLmtA54etsG3NXH+9ePjeBK_QA@mail.gmail.com>
Subject: Re: Bug is KryoSerializer under Mesos [work-around included]
From: Soren Macbeth <soren@yieldbot.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2ed6c551c8b04f95781de
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ed6c551c8b04f95781de
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Matei,

Yes, I'm 100% positive the jar on the executors is the same version. I am
building everything and deploying myself. Additionally, while debugging the
issue, I forked spark's git repo and added additional logging, which I
could see in the driver and executors. These debugging jars exhibited the
same behaviour.

I agree that the code being called in the executors and the driver *should*
be the same, but I found that not to be the case. I forgot to mention that
this issue only exhibits it's self under mesos; running in local mode or a
standalone cluster (with a single worker, all processes running on my
laptop) I wasn't able to reproduce the issue.

The classes in question should get registered by chill-scala's
AllScalaRegistrar here:

https://github.com/twitter/chill/blob/0.3.6/chill-scala/src/main/scala/com/=
twitter/chill/ScalaKryoInstantiator.scala#L166

but, for a reason I haven't tracked down, they don't in my mesos executor.
I don't really have a way to test if this is an issue specific only to my
mesos cluster, or if it exhibits in all mesos clusters.

fwiw, I am running spark-0.9.1 with hadoop 2.0.0-mr1-cdh4.6.0 under mesos
0.18.1


On Mon, May 12, 2014 at 12:02 PM, Matei Zaharia <matei.zaharia@gmail.com>wr=
ote:

> Hey Soren, are you sure that the JAR you used on the executors is for the
> right version of Spark? Maybe they=E2=80=99re running an older version. T=
he Kryo
> serializer should be initialized the same way on both.
>
> Matei
>
> On May 12, 2014, at 10:39 AM, Soren Macbeth <soren@yieldbot.com> wrote:
>
> > I finally managed to track down the source of the kryo issues that I wa=
s
> > having under mesos.
> >
> > What happens is the for a reason that I haven't tracked down yet, a
> handful
> > of the scala collection classes from chill-scala down get registered by
> the
> > mesos executors, but they do all get registered in the driver process.
> >
> > This led to scala.Some classes which were serialized by the executors
> being
> > incorrectly deserialized as scala.collections.Wrappers$SeqWrapper in
> driver
> > during task deserialization, causing a KryoException.
> >
> > I resolved this issue in my spark job by explicitly registering the
> classes
> > in my Registrator like so:
> >
> >
> > kryo.register(scala.collection.convert.Wrappers.IteratorWrapper.class);
> >      kryo.register(scala.collection.convert.Wrappers.SeqWrapper.class);
> >      kryo.register(scala.collection.convert.Wrappers.MapWrapper.class);
> >      kryo.register(scala.collection.convert.Wrappers.JListWrapper.class=
);
> >      kryo.register(scala.collection.convert.Wrappers.JMapWrapper.class)=
;
> >
> > Again, I'm not sure why they don't get registered in the mesos executor=
s,
> > but I wanted to report wht I found as well as a workaround in case anyo=
ne
> > else hit this (extraordinarily frustrating) issue again.
> >
> > Some interactive debugging note are available in this gist:
> >
> > https://gist.github.com/sorenmacbeth/28707a7a973f7a1982dc
> >
> > Cheers,
> > Soren
>
>

--001a11c2ed6c551c8b04f95781de--

From dev-return-7591-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 14 20:05:23 2014
Return-Path: <dev-return-7591-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1F63811A22
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 14 May 2014 20:05:23 +0000 (UTC)
Received: (qmail 61791 invoked by uid 500); 14 May 2014 20:05:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61730 invoked by uid 500); 14 May 2014 20:05:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61722 invoked by uid 99); 14 May 2014 20:05:12 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 20:05:12 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.176 as permitted sender)
Received: from [209.85.214.176] (HELO mail-ob0-f176.google.com) (209.85.214.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 20:05:09 +0000
Received: by mail-ob0-f176.google.com with SMTP id wo20so71360obc.35
        for <dev@spark.apache.org>; Wed, 14 May 2014 13:04:46 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=5PkoqtSwr1IJjYN63d77EEyWUA9tVsIHTHgBm6eKB30=;
        b=ZzrbHoFlfMAKeqk0+sZB8vsEhWihW7GBqwTH5lqqbkgeMkKE2fNn6DEEuGQJIuDgec
         CP8j4PhDrUuN1z5m750YCTMvxJV8Se0r95n4K5i+3U7bfaHonSJXEZdfIuUJjJ1sC/pw
         P3xJb+/ET2vSWur86rxqrXFGDJeo9d/LlDCM4DMTiqSJSbwAMBVIm6wOs6bIUyMQ8b4F
         k1DoqzCZcYT4rW7UK255TEN0ZoSLqSB8Xb2mX9NlRni2Au2cC/U+r7Oo+RI9Ud1A6sMf
         ebvuB1byL6gapfXptYRDLenLjOLz9XLeKEmFm6KBLQd7dtkoxxN+PSSMnUflJBsnM7AI
         F9qQ==
MIME-Version: 1.0
X-Received: by 10.182.144.161 with SMTP id sn1mr4470100obb.82.1400097886266;
 Wed, 14 May 2014 13:04:46 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Wed, 14 May 2014 13:04:46 -0700 (PDT)
Date: Wed, 14 May 2014 13:04:46 -0700
Message-ID: <CABPQxsvwxU4jzQQ2EkiN0t-Vt7mW53hspkFayoFd7r8q+JzLDg@mail.gmail.com>
Subject: [RESULT] [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

This vote is cancelled in favor of rc6.

On Wed, May 14, 2014 at 1:04 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> I'm cancelling this vote in favor of rc6.
>
> On Tue, May 13, 2014 at 8:01 AM, Sean Owen <sowen@cloudera.com> wrote:
>> On Tue, May 13, 2014 at 2:49 PM, Sean Owen <sowen@cloudera.com> wrote:
>>> On Tue, May 13, 2014 at 9:36 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>>>> The release files, including signatures, digests, etc. can be found at:
>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc5/
>>>
>>> Good news is that the sigs, MD5 and SHA are all correct.
>>>
>>> Tiny note: the Maven artifacts use SHA1, while the binary artifacts
>>> use SHA512, which took me a bit of head-scratching to figure out.
>>>
>>> If another RC comes out, I might suggest making it SHA1 everywhere?
>>> But there is nothing wrong with these signatures and checksums.
>>>
>>> Now to look at the contents...
>>
>> This is a bit of drudgery that probably needs to be done too: a review
>> of the LICENSE and NOTICE file. Having dumped the licenses of
>> dependencies, I don't believe these reflect all of the software that's
>> going to be distributed in 1.0.
>>
>> (Good news is there's no forbidden license stuff included AFAICT.)
>>
>> And good news is that NOTICE can be auto-generated, largely, with a
>> Maven plugin. This can be done manually for now.
>>
>> And there is a license plugin that will list all known licenses of
>> transitive dependencies so that LICENSE can be filled out fairly
>> easily.
>>
>> What say? want a JIRA with details?

From dev-return-7517-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 15 00:06:58 2014
Return-Path: <dev-return-7517-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E88EC11C42
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 May 2014 00:06:58 +0000 (UTC)
Received: (qmail 85800 invoked by uid 500); 10 May 2014 23:10:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5553 invoked by uid 500); 10 May 2014 23:03:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37849 invoked by uid 99); 10 May 2014 22:58:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 10 May 2014 22:58:47 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.178] (HELO mail-qc0-f178.google.com) (209.85.216.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 08 May 2014 19:58:45 +0000
Received: by mail-qc0-f178.google.com with SMTP id l6so3430799qcy.23
        for <dev@spark.apache.org>; Thu, 08 May 2014 12:58:24 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=8C9uG/5+Kz3PnjdrP5ciA3vr/PVNOonZkwAgxFy8MGw=;
        b=gWI6P86LxXxuCWLhaFyY02L6CrpSGZU7GLbvMLWfvz4vNVKogScxFCio2gYaCF2GWd
         gnisKsw+0KTNXK2n/imEl3CGnmDG/V5O0eqBztGLljSqBlZEOX1vbizYp4r0X57DvplL
         ajBTc4LiP6EDbjbVflO/T+rX6N3EYW372h2XOiWDHYwNquIc4INYPnASJ9Y2g9mjs4hx
         P/P2SNK9l903wTguWHsf+DvGycYvMCPpWZVxa//ln+/SXmF9rFPmwtKCPIRFtghr51v0
         neF99W+jAMOOz5P+X3v3InXlfH6JDRcPtDzMrmbRSFqK2mAl4ipBbiA/pv47XjU9G/pS
         hxyw==
X-Gm-Message-State: ALoCoQnpVJSvy/oAM1FDRSgpw0UYSrQMjbsZw/OYCxROvcq7chNwGOQkH3B7JhNCh9Wb/I2OQ1zb
X-Received: by 10.224.20.72 with SMTP id e8mr8184522qab.86.1399579104654; Thu,
 08 May 2014 12:58:24 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.229.169.132 with HTTP; Thu, 8 May 2014 12:58:04 -0700 (PDT)
X-Originating-IP: [49.206.0.67]
In-Reply-To: <CADKBdnoymVB4QS62VoXj12gKeGsRazeu+TxvMugvqHMoZR4wfQ@mail.gmail.com>
References: <CADKBdnrrnJ-9qGAeohdq4oYAZ=hjGoYpu2Lvh9wZnFiprn9D2Q@mail.gmail.com>
 <CAAswR-5aUGtmi2KQbTPhNAuTSuabStwm9+QByi8epBXxPo7zmA@mail.gmail.com> <CADKBdnoymVB4QS62VoXj12gKeGsRazeu+TxvMugvqHMoZR4wfQ@mail.gmail.com>
From: Rohit Rai <rohit@tuplejump.com>
Date: Fri, 9 May 2014 01:28:04 +0530
Message-ID: <CAFRXrPrAaotodYb6BNJgx98EZp_7ky-2=2pYCQ4aZiz3FNqc=Q@mail.gmail.com>
Subject: Re: Problem creating objects through reflection
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c1bfea5e452104f8e8e767
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1bfea5e452104f8e8e767
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Piotr,

The easiest solution to this for now is to write all your code (including
the case class) inside a Object and the execution part in a method in the
object. Then you can call the method on the spark shell to executed your
code.

Cheers,
Rohit


*Founder & CEO, **Tuplejump, Inc.*
____________________________
www.tuplejump.com
*The Data Engineering Platform*


On Fri, Apr 25, 2014 at 12:58 PM, Piotr Ko=C5=82aczkowski
<pkolaczk@datastax.com>wrote:

> Yeah, this is related.
>
> From
> https://groups.google.com/forum/#!msg/spark-users/bwAmbUgxWrA/HwP4Nv4adfE=
J
> :
> "This is a limitation that will hopefully go away in Scala 2.10 or 2.10 .=
1,
> when we'll use macros to remove the need to do this. (Or more generally i=
f
> we get some changes in the Scala interpreter to do something smarter in
> this case.) "
>
> We're using Spark 0.9.0, Scala 2.10.3 and the limitation is there. Any
> ideas when it is going to be fixed?
>
> The workaround with embedding everything inside a singleton object does n=
ot
> work for me, because nested classes defined there are still inner  and
> require additional argument to the constructor (when invoked by
> reflection).
>
> If I only had some reliable way to obtain a reference to that outer objec=
t
> by reflection, we could somehow workaround it. E.g. saving it in some
> singleton object, etc. However, a proper fix would be to make non-inner
> classes properly non-inner.
>
> Thanks,
> Piotr
>
>
>
>
> 2014-04-25 0:13 GMT+02:00 Michael Armbrust <michael@databricks.com>:
>
> > The Spark REPL is slightly modified from the normal Scala REPL to preve=
nt
> > work from being done twice when closures are deserialized on the worker=
s.
> >  I'm not sure exactly why this causes your problem, but its probably
> worth
> > filing a JIRA about it.
> >
> > Here is another issues with classes defined in the REPL.  Not sure if i=
t
> is
> > related, but I'd be curious if the workaround helps you:
> > https://issues.apache.org/jira/browse/SPARK-1199
> >
> > Michael
> >
> >
> > On Thu, Apr 24, 2014 at 3:14 AM, Piotr Ko=C5=82aczkowski
> > <pkolaczk@datastax.com>wrote:
> >
> > > Hi,
> > >
> > > I'm working on Cassandra-Spark integration and I hit a pretty severe
> > > problem. One of the provided functionality is mapping Cassandra rows
> into
> > > objects of user-defined classes. E.g. like this:
> > >
> > > class MyRow(val key: String, val data: Int)
> > > sc.cassandraTable("keyspace", "table").select("key", "data").as[MyRow=
]
> >  //
> > > returns CassandraRDD[MyRow]
> > >
> > > In this example CassandraRDD creates MyRow instances by reflection,
> i.e.
> > > matches selected fields from Cassandra table and passes them to the
> > > constructor.
> > >
> > > Unfortunately this does not work in Spark REPL.
> > > Turns out any class declared on the REPL is an inner classes, and to =
be
> > > successfully created, it needs a reference to the outer object, even
> > though
> > > it doesn't really use anything from the outer context.
> > >
> > > scala> class SomeClass
> > > defined class SomeClass
> > >
> > > scala> classOf[SomeClass].getConstructors()(0)
> > > res11: java.lang.reflect.Constructor[_] =3D public
> > > $iwC$$iwC$SomeClass($iwC$$iwC)
> > >
> > > I tried passing a null as a temporary workaround, and it also doesn't
> > work
> > > - I get NPE.
> > > How can I get a reference to the current outer object representing th=
e
> > > context of the current line?
> > >
> > > Also, plain non-spark Scala REPL doesn't exhibit this behaviour - and
> > > classes declared on the REPL are proper top-most classes, not inner
> ones.
> > > Why?
> > >
> > > Thanks,
> > > Piotr
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > > --
> > > Piotr Kolaczkowski, Lead Software Engineer
> > > pkolaczk@datastax.com
> > >
> > > 777 Mariners Island Blvd., Suite 510
> > > San Mateo, CA 94404
> > >
> >
>
>
>
> --
> Piotr Kolaczkowski, Lead Software Engineer
> pkolaczk@datastax.com
>
> 777 Mariners Island Blvd., Suite 510
> San Mateo, CA 94404
>

--001a11c1bfea5e452104f8e8e767--

From dev-return-7521-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 15 01:01:48 2014
Return-Path: <dev-return-7521-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0C5F511F88
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 May 2014 01:01:48 +0000 (UTC)
Received: (qmail 99781 invoked by uid 500); 10 May 2014 23:26:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55821 invoked by uid 500); 10 May 2014 23:15:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11241 invoked by uid 99); 10 May 2014 22:57:08 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 10 May 2014 22:57:08 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.214.169 as permitted sender)
Received: from [209.85.214.169] (HELO mail-ob0-f169.google.com) (209.85.214.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 07 May 2014 18:58:02 +0000
Received: by mail-ob0-f169.google.com with SMTP id vb8so1776108obc.28
        for <dev@spark.apache.org>; Wed, 07 May 2014 11:57:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=MvQM5PQn2ndO6MKWHqFfynqZJ41wlDgfXYLnVqb/Q/w=;
        b=TX+0vBdJybK5ktuQXj1rUVTAxDpdjKWE3Hil65pbv5hQsRcSIPhHByabmCYjn7UexA
         WV2DaLZctZzCGOpRdAquOLv+Fl3IC8COlHTERQrjhueqd8Xhnx762Gf9L50lBgrWnYAi
         6Tc73Sw/IfTZiW4Lrct7eWKx9QIdbHLQpN8h6wLg89zV5m7+N5IQrnTnnpmVArZMEOAY
         +j7NmHZpBV2eihALXdvhmlukH/pJ9DbbLESpdI6Hncb7QI3RqPjzhnMl19HWDH6YL/rm
         kLciwZCtJ9pC6b/0FF0TcjJGYHRtzgJV8kVhTZ9ETJXcz1RY78Aynhi1uJAekt0oWzR3
         MQMw==
MIME-Version: 1.0
X-Received: by 10.60.176.9 with SMTP id ce9mr18065127oec.55.1399489062149;
 Wed, 07 May 2014 11:57:42 -0700 (PDT)
Received: by 10.182.246.164 with HTTP; Wed, 7 May 2014 11:57:42 -0700 (PDT)
Date: Wed, 7 May 2014 11:57:42 -0700
Message-ID: <CA+B-+fzBzfS9d_DBL2g1tBA+o-MgfOY1aspDjP8wG6P9E0S7+w@mail.gmail.com>
Subject: Sparse vector toLibSvm API
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0118226c6aa98304f8d3f090
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0118226c6aa98304f8d3f090
Content-Type: text/plain; charset=UTF-8

Hi,

In the sparse vector the toString API is as follows:

 override def toString: String = {

    "(" + size + "," + indices.zip(values).mkString("[", "," ,"]") + ")"

  }

Does it make sense to keep it consistent with libsvm format ?

What does each line of libsvm format looks like ?

Thanks.

Deb

--089e0118226c6aa98304f8d3f090--

From dev-return-7592-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 15 01:22:08 2014
Return-Path: <dev-return-7592-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ECBFD110A4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 May 2014 01:22:08 +0000 (UTC)
Received: (qmail 72912 invoked by uid 500); 14 May 2014 20:28:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72862 invoked by uid 500); 14 May 2014 20:28:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72854 invoked by uid 99); 14 May 2014 20:28:50 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 20:28:50 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.170 as permitted sender)
Received: from [209.85.216.170] (HELO mail-qc0-f170.google.com) (209.85.216.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 20:28:47 +0000
Received: by mail-qc0-f170.google.com with SMTP id i8so180844qcq.1
        for <dev@spark.apache.org>; Wed, 14 May 2014 13:28:24 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=M6rj68Nze2uz9Z/eN2SzaOhHmYIbhbAU1e3osEIsxww=;
        b=nL3fLPjW1/OoXD0Ot98Qf6LjIZNTyiOCU46yPOb2rAla5jJvc21Mwn3mdNBEOvustO
         FVIL7rAkY/Nhjw0l2CRNOAjN/OUuUdE/tFrSDghiPOjpiG0jV/iqGgAdYj4/Swzsjwbf
         Do8pW7m/x6KvkKabseE7DYLgqdjJVYocv5wRBsVZGqo0F/pTqUyslA7IZorSO8tWCXhI
         HF1YdLL7xP9/j9O+Wd7UCSbTf4qDOrlI4lhhZm/v7LeQ5Q0FoPNIH4vnwPJtMUPX8qT8
         Y21iyOkbV2fZE49QQBPcln7GUO9e46iGIqPQB73a9NXDTS2xc0DLgno5mC4H4znx2+j1
         Ic9Q==
X-Gm-Message-State: ALoCoQnkpIEdQi1v3aPDTKDUA4Bm6/RxAaazUHn+0RCdyuaOCdgOf1M0GFaSSXSmTpa8JODYzdWd
MIME-Version: 1.0
X-Received: by 10.224.49.67 with SMTP id u3mr7182617qaf.63.1400099304123; Wed,
 14 May 2014 13:28:24 -0700 (PDT)
Received: by 10.140.106.202 with HTTP; Wed, 14 May 2014 13:28:24 -0700 (PDT)
In-Reply-To: <tencent_4A0DC42353D5C43B6806EC6F@qq.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com>
	<CAMtqZeeAW6BeCiCfHbiCmKfMh=BnrO=+UM1oPe1VNgB2disQQw@mail.gmail.com>
	<tencent_5EABC2696C9E4E8610F1CE38@qq.com>
	<1400030133573-6560.post@n3.nabble.com>
	<tencent_4A0DC42353D5C43B6806EC6F@qq.com>
Date: Wed, 14 May 2014 13:28:24 -0700
Message-ID: <CACBYxK+tyorAkUdL4XGBX=KD4LonBqvanm91pW8B8P=QHPOuFg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2ef80ac3e5904f9620592
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ef80ac3e5904f9620592
Content-Type: text/plain; charset=UTF-8

+1 (non-binding)

* Built the release from source.
* Compiled Java and Scala apps that interact with HDFS against it.
* Ran them in local mode.
* Ran them against a pseudo-distributed YARN cluster in both yarn-client
mode and yarn-cluster mode.


On Tue, May 13, 2014 at 9:09 PM, witgo <witgo@qq.com> wrote:

> You need to set:
> spark.akka.frameSize         5
> spark.default.parallelism    1
>
>
>
>
>
> ------------------ Original ------------------
> From:  "Madhu";<madhu@madhu.com>;
> Date:  Wed, May 14, 2014 09:15 AM
> To:  "dev"<dev@spark.incubator.apache.org>;
>
> Subject:  Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
>
>
>
> I just built rc5 on Windows 7 and tried to reproduce the problem described
> in
>
> https://issues.apache.org/jira/browse/SPARK-1712
>
> It works on my machine:
>
> 14/05/13 21:06:47 INFO DAGScheduler: Stage 1 (sum at <console>:17) finished
> in 4.548 s
> 14/05/13 21:06:47 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks
> have all completed, from pool
> 14/05/13 21:06:47 INFO SparkContext: Job finished: sum at <console>:17,
> took
> 4.814991993 s
> res1: Double = 5.000005E11
>
> I used all defaults, no config files were changed.
> Not sure if that makes a difference...
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-0-0-rc5-tp6542p6560.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
> .

--001a11c2ef80ac3e5904f9620592--

From dev-return-7574-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 15 01:56:53 2014
Return-Path: <dev-return-7574-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3C34611203
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 May 2014 01:56:53 +0000 (UTC)
Received: (qmail 59399 invoked by uid 500); 13 May 2014 21:40:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59340 invoked by uid 500); 13 May 2014 21:40:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59332 invoked by uid 99); 13 May 2014 21:40:52 -0000
Received: from Unknown (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 21:40:52 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 13 May 2014 21:40:47 +0000
Received: by mail-qg0-f50.google.com with SMTP id z60so1409415qgd.9
        for <dev@spark.apache.org>; Tue, 13 May 2014 14:40:26 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=1dHg/Ifdei7dalilZZQQFucENJBNNC4k+QnErb55MYM=;
        b=Mj3ldfRUyZDDfl5+e0y4F+LArifbQvsqgAyIV+vSv09jgNLjmm9UIPnqtMqAvdoMie
         J2TOVNCqFF1WXyEoWEICw7REs9xoFk+eR6BahqAI8DY/YixgRtgguXWHHAaiekeN+kya
         JjTPOP5SjgGmLMLZy0wja50UKbxKOnCByxuT+aYb0C+Wevv4D7/5yKqt1H7STovE3xfD
         TFdX3n1CpCpaFjc8HdUe8+t7DZ/0vffXAJvi2rJh3cD0BEi/qpbuJOyi283WzUuZ6s5O
         kJtDWB1qp7STjmULUAQG0fdnEBKdaU7Ic7/fIEOi6Dt24kHTfe/Byx/NVFXw7nStkIx6
         N2Kw==
X-Gm-Message-State: ALoCoQlm1ZSPhjx91gHOuUdrVV2XybOq/kh0A4rtn1PNirb2BHBj7uaTF2tIS3yswSeCTJj9Hyby
X-Received: by 10.140.100.198 with SMTP id s64mr24212703qge.106.1400017226239;
 Tue, 13 May 2014 14:40:26 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.209.73 with HTTP; Tue, 13 May 2014 14:40:06 -0700 (PDT)
In-Reply-To: <CA+-p3AHXQZttzFG_TC0P2abupPthPY0EhcSxDAsa90Rvu45qFg@mail.gmail.com>
References: <CA+-p3AHXQZttzFG_TC0P2abupPthPY0EhcSxDAsa90Rvu45qFg@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Tue, 13 May 2014 14:40:06 -0700
Message-ID: <CAAswR-49jXVdPiGW=e1ivg0V3otKfmu0kGeLSrMNeQhy_XC2KA@mail.gmail.com>
Subject: Re: Preliminary Parquet numbers and including .count() in Catalyst
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c16ba873233d04f94ee985
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c16ba873233d04f94ee985
Content-Type: text/plain; charset=UTF-8

>
> It looks like currently the .count() on parquet is handled incredibly
> inefficiently and all the columns are materialized.  But if I select just
> that relevant column and then count, then the column-oriented storage of
> Parquet really shines.
>
> There ought to be a potential optimization here such that a .count() on a
> SchemaRDD backed by Parquet doesn't require re-assembling the rows, as
> that's expensive.  I don't think .count() is handled specially in
> SchemaRDDs, but it seems ripe for optimization.
>

Yeah, you are right.  Thanks for pointing this out!

If you call .count() that is just the native Spark count, which is not
aware of the potential optimizations.  We could just override count() in a
schema RDD to be something like
"groupBy()(Count(Literal(1))).collect().head.getInt(0)"

Here is a JIRA: SPARK-1822 - SchemaRDD.count() should use the
optimizer.<https://issues.apache.org/jira/browse/SPARK-1822>

Michael

--001a11c16ba873233d04f94ee985--

From dev-return-7581-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 15 02:01:58 2014
Return-Path: <dev-return-7581-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8470C11246
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 May 2014 02:01:58 +0000 (UTC)
Received: (qmail 52748 invoked by uid 500); 14 May 2014 01:09:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52605 invoked by uid 500); 14 May 2014 01:09:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52313 invoked by uid 99); 14 May 2014 01:09:15 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 01:09:15 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.216.175 as permitted sender)
Received: from [209.85.216.175] (HELO mail-qc0-f175.google.com) (209.85.216.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 01:09:10 +0000
Received: by mail-qc0-f175.google.com with SMTP id w7so1669806qcr.34
        for <dev@spark.incubator.apache.org>; Tue, 13 May 2014 18:08:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=TkHgdxhhfG3R6JZuUj1Hp2h5u941nVgT3AdHu55BLtE=;
        b=y3qej95v+qrB7eUy6n6imVh7jk4arI85atZq+qsfaKsSGsgWPHpf2CpQPJNOp2wIkf
         nRBbUcrxkUstLTXdeu+qw/MleYFSaZXdb1abMGiBDJA5Tuh+txdDnVel0TjESBSYBBOm
         QNNy34vS+TFH+b57ST5ZkzVySutPWPVbh8aIWlT8od60uFOoOcE/cSinSwVbrYSMRUgr
         05Tq3IAbenjb+4q3pHFrUyVRq8NShFNx076fxEDcC/UIk5O+BP8FL0KgoX1cjTpTDI6N
         m5Iq+Al8E/BNO803DWxB2h8zzbGYIL4i5ScgGLGDEV0kchgoYLxrMyql3dL1DA7sNPrJ
         1fPA==
X-Received: by 10.140.29.226 with SMTP id b89mr956848qgb.48.1400029727138;
        Tue, 13 May 2014 18:08:47 -0700 (PDT)
Received: from [192.168.2.13] ([69.157.95.72])
        by mx.google.com with ESMTPSA id m18sm399737qax.47.2014.05.13.18.08.46
        for <multiple recipients>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Tue, 13 May 2014 18:08:46 -0700 (PDT)
Date: Tue, 13 May 2014 21:16:27 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Cc: dev@spark.incubator.apache.org
Message-ID: <89B7B6B3F2944CB1AABA6BB9AF1D128E@gmail.com>
In-Reply-To: <1400025832017-6558.post@n3.nabble.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
 <1400022674823-6555.post@n3.nabble.com>
 <1400025832017-6558.post@n3.nabble.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="5372c3eb_5ff87e05_52f3"
X-Virus-Checked: Checked by ClamAV on apache.org

--5372c3eb_5ff87e05_52f3
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

+1, replaced rc3 with rc5, all applications are working fine

Best, 

-- 
Nan Zhu


On Tuesday, May 13, 2014 at 8:03 PM, Madhu wrote:

> I built rc5 using sbt/sbt assembly on Linux without any problems.
> There used to be an sbt.cmd for Windows build, has that been deprecated?
> If so, I can document the Windows build steps that worked for me.
> 
> 
> 
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-0-0-rc5-tp6542p6558.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com (http://Nabble.com).
> 
> 



--5372c3eb_5ff87e05_52f3--


From dev-return-7593-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 15 02:17:14 2014
Return-Path: <dev-return-7593-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B90F6112F4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 May 2014 02:17:14 +0000 (UTC)
Received: (qmail 22560 invoked by uid 500); 14 May 2014 21:09:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22370 invoked by uid 500); 14 May 2014 21:09:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22351 invoked by uid 99); 14 May 2014 21:09:57 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 21:09:57 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of michaelmalak@yahoo.com designates 98.139.213.161 as permitted sender)
Received: from [98.139.213.161] (HELO nm24-vm0.bullet.mail.bf1.yahoo.com) (98.139.213.161)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 21:09:46 +0000
Received: from [66.196.81.172] by nm24.bullet.mail.bf1.yahoo.com with NNFMP; 14 May 2014 21:09:21 -0000
Received: from [98.139.212.216] by tm18.bullet.mail.bf1.yahoo.com with NNFMP; 14 May 2014 21:09:21 -0000
Received: from [127.0.0.1] by omp1025.mail.bf1.yahoo.com with NNFMP; 14 May 2014 21:09:21 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 921352.16068.bm@omp1025.mail.bf1.yahoo.com
Received: (qmail 93315 invoked by uid 60001); 14 May 2014 21:09:21 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.com; s=s1024; t=1400101761; bh=eYS1C99x3DFHwjWdyFhvvaiOhDRr92rmc7FE9MdQ/E8=; h=Message-ID:Date:From:Reply-To:Subject:To:MIME-Version:Content-Type; b=uaBB8RS1Nfcwz2pZ4K+w2yaCP9SuKsVn0+bblfoP8W2a47ehTdIDFhEhcs5hZNO1zUgAepEWWmmTgyQmbdVcYys2P02NfJZ3BNBXMNnGJKrQZ6SRMRVmYClCTLazVNY2NRQ+qbEsbsuR7VGFjpSSsIYhnUgLSSk2Cj6aqlWoHAg=
X-YMail-OSG: j1clRDYVM1lI_5Qhqnu85v5huVpCaV7cGYAz9w_AG6lFWh5
 lxBB6SONZOeinjmpH0T6Ii0dsLj_p.4cVc.jf0OFQr6hnRK8NedPNzlYN2DE
 RXnx0272oOmetHLpw5p8Xn8vv4bJs_ag6q7uG0ZWNaRqWoN8A7lCikK63Fhf
 NSzsnZ4fda_yx8iG59v8ClQqZKGe2gPrRjP9av7.Pytxa5QVkVgVu0jZqCHq
 mS2k2F77KnePovJj4W3g_EBw.MVfJ_YoR3D5bdKjQPS0IemuJlkVqtE.zpof
 zew2HUG2saOHu9jq_JNsWoUqb9Zn0l_EkocSB1mQKHSHvcf_RaIVGwBA_hqP
 2e36GBqNhrCG9jlUGQ5fUy0MPgKm7p1J2Byjpiz4t57016XyGfooh6Z56Ai1
 T6f_hIMXVT0iqbqdtjHzMv1CIi0o36uxbsxJ.Nq7a.erhfRxyTCekFlQSpCZ
 gaE9ex4br01cBoRSLrYuuwYjvihGyKW9tjO0jlHVyFdsokKdWS3O8ojCWF2S
 xALIKcB2S_ExCyk4RS_ssMGft8CFb67pc5IuNEw75RLGPHRs-
Received: from [148.87.67.199] by web160805.mail.bf1.yahoo.com via HTTP; Wed, 14 May 2014 14:09:21 PDT
X-Rocket-MIMEInfo: 002.001,V2hlbiB1c2luZyBtYXAoKSBhbmQgbG9va3VwKCkgaW4gY29uanVuY3Rpb24sIEkgZ2V0IGFuIGV4Y2VwdGlvbiAoZWFjaCBpbmRlcGVuZGVudGx5IHdvcmtzIGZpbmUpLiBJJ20gdXNpbmcgU3BhcmsgMC45LjAvU2NhbGEgMi4xMC4zCgp2YWwgYSA9IHNjLnBhcmFsbGVsaXplKEFycmF5KDExKSkKdmFsIG0gPSBzYy5wYXJhbGxlbGl6ZShBcnJheSgoMTEsMjEpKSkKYS5tYXAobS5sb29rdXAoXykoMCkpLmNvbGxlY3QKCjE0LzA1LzE0IDE1OjAzOjM1IEVSUk9SIEV4ZWN1dG9yOiBFeGNlcHRpb24gaW4gdGFzayABMAEBAQE-
X-Mailer: YahooMailWebService/0.8.188.663
Message-ID: <1400101761.53812.YahooMailNeo@web160805.mail.bf1.yahoo.com>
Date: Wed, 14 May 2014 14:09:21 -0700 (PDT)
From: Michael Malak <michaelmalak@yahoo.com>
Reply-To: Michael Malak <michaelmalak@yahoo.com>
Subject: map() + lookup() exception
To: dev@spark.apache.org
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="-185498972-722528763-1400101761=:53812"
X-Virus-Checked: Checked by ClamAV on apache.org

---185498972-722528763-1400101761=:53812
Content-Type: text/plain; charset=us-ascii

When using map() and lookup() in conjunction, I get an exception (each independently works fine). I'm using Spark 0.9.0/Scala 2.10.3

val a = sc.parallelize(Array(11))
val m = sc.parallelize(Array((11,21)))
a.map(m.lookup(_)(0)).collect

14/05/14 15:03:35 ERROR Executor: Exception in task ID 23
scala.MatchError: null
at org.apache.spark.rdd.PairRDDFunctions.lookup(PairRDDFunctions.scala:551)

I'm able to work around it with:

a.map((_,0)).join(m).map(_._2._2).collect
---185498972-722528763-1400101761=:53812--

From dev-return-7589-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 15 02:42:04 2014
Return-Path: <dev-return-7589-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0B69F11543
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 May 2014 02:42:04 +0000 (UTC)
Received: (qmail 24581 invoked by uid 500); 14 May 2014 20:02:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24525 invoked by uid 500); 14 May 2014 20:02:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24517 invoked by uid 99); 14 May 2014 20:02:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 20:02:36 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.50 as permitted sender)
Received: from [209.85.219.50] (HELO mail-oa0-f50.google.com) (209.85.219.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 20:02:31 +0000
Received: by mail-oa0-f50.google.com with SMTP id i7so67762oag.37
        for <dev@spark.apache.org>; Wed, 14 May 2014 13:02:10 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=ulGq4v2WY02WLNkkC5Nivfa3vsB6MLWWZBvCN3teBGs=;
        b=kZrQQtD/jiAU4fIzEG3liq8OaAmvjBGcPQffOS5vxMnkeFGAXT+aHvo3VUCsRYeTXg
         6nDpZUf6VLTqN/zguz9Cxk+icZWduRBK77ddXxIFvlgZ1vklSFGfRZjSyF0KLiolpuOw
         YgVFOIXvGUYX6u2Dw+VAwwQt8nfF8J4jm/PKDlzzdRCM17Hk293KZypQ0wDWnZR2Geg6
         /CvcNepqmadju30UUaikvAxO/F4AwOL1O42O1vS9iHQpgiLOB4u1HY3BPLlOkgBkdb2W
         HYrsgkwjtKSG89pIb0ibNmWed1vigMwaAq8EGKbE55WQZ+NkfLHhybLwEYPxwelxJRUm
         cvIA==
MIME-Version: 1.0
X-Received: by 10.60.102.198 with SMTP id fq6mr5720743oeb.6.1400097730105;
 Wed, 14 May 2014 13:02:10 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Wed, 14 May 2014 13:02:10 -0700 (PDT)
Date: Wed, 14 May 2014 13:02:10 -0700
Message-ID: <CABPQxst79373EAcpmFemjukod+9veP_F=EeRZWch84_=fo1xTQ@mail.gmail.com>
Subject: [VOTE] Release Apache Spark 1.0.0 (rc6)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Please vote on releasing the following candidate as Apache Spark version 1.0.0!

This patch has a few minor fixes on top of rc5. I've also built the
binary artifacts with Hive support enabled so people can test this
configuration. When we release 1.0 we might just release both vanilla
and Hive-enabled binaries.

The tag to be voted on is v1.0.0-rc6 (commit 54133a):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=54133abdce0246f6643a1112a5204afb2c4caa82

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc6/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachestratos-1011

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc6-docs/

Please vote on releasing this package as Apache Spark 1.0.0!

The vote is open until Saturday, May 17, at 20:58 UTC and passes if
amajority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.0.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== API Changes ==
We welcome users to compile Spark applications against 1.0. There are
a few API changes in this release. Here are links to the associated
upgrade guides - user facing changes have been kept as small as
possible.

changes to ML vector specification:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10

changes to the Java API:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark

changes to the streaming API:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x

changes to the GraphX API:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091

coGroup and related functions now return Iterable[T] instead of Seq[T]
==> Call toSeq on the result to restore the old behavior

SparkContext.jarOfClass returns Option[String] instead of Seq[String]
==> Call toSeq on the result to restore old behavior

From dev-return-7583-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 15 03:02:56 2014
Return-Path: <dev-return-7583-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 332D3116CE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 May 2014 03:02:56 +0000 (UTC)
Received: (qmail 33370 invoked by uid 500); 14 May 2014 03:06:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33266 invoked by uid 500); 14 May 2014 03:06:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33258 invoked by uid 99); 14 May 2014 03:06:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 03:06:15 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.53 as permitted sender)
Received: from [209.85.220.53] (HELO mail-pa0-f53.google.com) (209.85.220.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 03:06:09 +0000
Received: by mail-pa0-f53.google.com with SMTP id kp14so1051322pab.26
        for <dev@spark.apache.org>; Tue, 13 May 2014 20:05:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=wg1P4F3oYtQXhVwF7ZwG+ysy7IcC8vDAmWTOid8wCi0=;
        b=ORF7rKVb1wnBZS2nkxk1BHmQBdlmXdu46RZFxzTcGprImjSz0t+fwKf4QtGjbQ7gK+
         2QU7eWjFipAFSUixx1/labFRv1Lv59GntHDO+M4+mRU5o6mzBII4QsNMqA26qQY2K1L3
         ZS8Mgf2fpJBnutcZWDpMxvAzpFOxIucf8fi4ZvDDpdqLdb8Cs5iOjlFJAzljmSVWTDDe
         YoxaErepNVSGFhJ4VbpM70sqUVBfOCKeqU7oydIZlLPPjQm0RsrrwbBEFGxoGIVjc0nx
         6orIb7VmQCLurg+IU6zq0bxV9n0GhN0CbK2fWnzXb+Rhl0c+HzYMnK3P2DN/kFrhf+kM
         cEBg==
X-Received: by 10.66.66.66 with SMTP id d2mr1109580pat.24.1400036748995;
        Tue, 13 May 2014 20:05:48 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id fu12sm1869010pad.42.2014.05.13.20.05.46
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 13 May 2014 20:05:47 -0700 (PDT)
Content-Type: text/plain; charset=iso-8859-1
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: Class-based key in groupByKey?
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <1400020245.91345.YahooMailNeo@web160805.mail.bf1.yahoo.com>
Date: Tue, 13 May 2014 20:05:43 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <E26F4A22-7F6B-4092-A94F-DE36A21289D4@gmail.com>
References: <1400020245.91345.YahooMailNeo@web160805.mail.bf1.yahoo.com>
To: dev@spark.apache.org,
 Michael Malak <michaelmalak@yahoo.com>
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

Your key needs to implement hashCode in addition to equals.

Matei

On May 13, 2014, at 3:30 PM, Michael Malak <michaelmalak@yahoo.com> =
wrote:

> Is it permissible to use a custom class (as opposed to e.g. the =
built-in String or Int) for the key in groupByKey? It doesn't seem to be =
working for me on Spark 0.9.0/Scala 2.10.3:
>=20
> import org.apache.spark.SparkContext
> import org.apache.spark.SparkContext._
>=20
> class C(val s:String) extends Serializable {
>   override def equals(o: Any) =3D if (o.isInstanceOf[C]) =
o.asInstanceOf[C].s =3D=3D s else false
>   override def toString =3D s
> }
>=20
> object SimpleApp {
>   def main(args: Array[String]) {
>     val sc =3D new SparkContext("local", "Simple App", null, null)
>     val r1 =3D sc.parallelize(Array((new C("a"),11),(new C("a"),12)))
>     println("r1=3D" + r1.groupByKey.collect.mkString(";"))
>     val r2 =3D sc.parallelize(Array(("a",11),("a",12)))
>     println("r2=3D" + r2.groupByKey.collect.mkString(";"))
>   }
> }
>=20
>=20
> Output
> =3D=3D=3D=3D=3D=3D
> r1=3D(a,ArrayBuffer(11));(a,ArrayBuffer(12))
> r2=3D(a,ArrayBuffer(11, 12))


From dev-return-7590-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 15 03:22:37 2014
Return-Path: <dev-return-7590-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DA82711850
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 May 2014 03:22:36 +0000 (UTC)
Received: (qmail 57315 invoked by uid 500); 14 May 2014 20:04:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57255 invoked by uid 500); 14 May 2014 20:04:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57196 invoked by uid 99); 14 May 2014 20:04:51 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 20:04:51 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.171 as permitted sender)
Received: from [209.85.214.171] (HELO mail-ob0-f171.google.com) (209.85.214.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 14 May 2014 20:04:48 +0000
Received: by mail-ob0-f171.google.com with SMTP id wn1so73155obc.30
        for <dev@spark.apache.org>; Wed, 14 May 2014 13:04:25 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=VKTjlGKyZtJtlvur4gf9VlRCDaZGmZPF6MqU69yc4TI=;
        b=q/UayMjJw3SqSzhv1t3OivcO+wj3OC2pt7ws8DPAdRG2mVJUExz+iD++Mgaf3rWbZ/
         iwmsnzQYTxbnmCBkOZ1pC9mRIZ47bqn3tiW7MOmi4sYfmClRoLC78HFPZPbjeaTYF6N4
         Ogx8YeXnzVSKVnDgHBhG9MQ9CdYdPJZi/TqN46v/dtvy/UYP5w+aMT+eYmgI1PEOkb7j
         Xkb8jte9PrLYXAQSuurRXe6wcw1bugYA/EWNxPJ6C4O1ImxTwOxtJZx9p+Mfs2Qax1YA
         1kVZXeT6T5io4T7URmEMIsExkJzLFKna9XEXmB/pf++e7yW3XLS9P0tTDbZDcM0U4Ij1
         CI4Q==
MIME-Version: 1.0
X-Received: by 10.60.179.80 with SMTP id de16mr5579477oec.69.1400097864901;
 Wed, 14 May 2014 13:04:24 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Wed, 14 May 2014 13:04:24 -0700 (PDT)
In-Reply-To: <CAMAsSdKzLk=mRoH3kHtPW8dE4sK7W12WOUMY0jSJHGyeuWZLEw@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com>
	<CAMAsSdKzLk=mRoH3kHtPW8dE4sK7W12WOUMY0jSJHGyeuWZLEw@mail.gmail.com>
Date: Wed, 14 May 2014 13:04:24 -0700
Message-ID: <CABPQxss74o-u94MJNN5dg_cRNdDx0nVeCOz5fZM=CCG9VKbVfg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I'm cancelling this vote in favor of rc6.

On Tue, May 13, 2014 at 8:01 AM, Sean Owen <sowen@cloudera.com> wrote:
> On Tue, May 13, 2014 at 2:49 PM, Sean Owen <sowen@cloudera.com> wrote:
>> On Tue, May 13, 2014 at 9:36 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>>> The release files, including signatures, digests, etc. can be found at:
>>> http://people.apache.org/~pwendell/spark-1.0.0-rc5/
>>
>> Good news is that the sigs, MD5 and SHA are all correct.
>>
>> Tiny note: the Maven artifacts use SHA1, while the binary artifacts
>> use SHA512, which took me a bit of head-scratching to figure out.
>>
>> If another RC comes out, I might suggest making it SHA1 everywhere?
>> But there is nothing wrong with these signatures and checksums.
>>
>> Now to look at the contents...
>
> This is a bit of drudgery that probably needs to be done too: a review
> of the LICENSE and NOTICE file. Having dumped the licenses of
> dependencies, I don't believe these reflect all of the software that's
> going to be distributed in 1.0.
>
> (Good news is there's no forbidden license stuff included AFAICT.)
>
> And good news is that NOTICE can be auto-generated, largely, with a
> Maven plugin. This can be done manually for now.
>
> And there is a license plugin that will list all known licenses of
> transitive dependencies so that LICENSE can be filled out fairly
> easily.
>
> What say? want a JIRA with details?

From dev-return-7514-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 15 05:26:49 2014
Return-Path: <dev-return-7514-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F257F11FBD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 May 2014 05:26:48 +0000 (UTC)
Received: (qmail 30463 invoked by uid 500); 10 May 2014 22:58:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89707 invoked by uid 500); 10 May 2014 22:55:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65762 invoked by uid 99); 10 May 2014 22:54:11 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 10 May 2014 22:54:11 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.219.43 as permitted sender)
Received: from [209.85.219.43] (HELO mail-oa0-f43.google.com) (209.85.219.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 07 May 2014 18:32:22 +0000
Received: by mail-oa0-f43.google.com with SMTP id l6so1748484oag.2
        for <dev@spark.apache.org>; Wed, 07 May 2014 11:31:58 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=J5l/NQlz6uCkgnPHBZyAU33znvgZoTzDuzz03Pr0Qso=;
        b=ysyeJxqw+5oG9y9etRz5dMeusD1u1W+gHfE0hho9DCGIy5cmxYik+pNHt5/uDdPCW0
         vqH4wXwcx/tLKkLQZ21Bm2BCs1NfGmKRDVZECikiK3YlSU/UQDCu0KmX3dUuRevhL4nA
         tHBtEsr4OU0ATtPm4md8XW92OgZhO2F+Dy7hMgwhXZtH2/P8dhWaWihbQRfHC8OujQx7
         HbB4tlYbZKTNlCr96GEtsPL0vNEcARd8iXIPvCVRlq0UnPgEMmWn1pKbp47c5pskmhoa
         RzBwQ+yaABwFeIDk5+dC6zPA8URGBsmkqxIute0/ALuBCWFpZ++/Ysp4FbnE7PNSfVPb
         lJ9g==
MIME-Version: 1.0
X-Received: by 10.182.22.227 with SMTP id h3mr47684787obf.36.1399487518447;
 Wed, 07 May 2014 11:31:58 -0700 (PDT)
Received: by 10.182.246.164 with HTTP; Wed, 7 May 2014 11:31:58 -0700 (PDT)
In-Reply-To: <CALW2ey2JBUY1VwTNHV4rirYyF0swcfwQamW2y--NJL5axo31cw@mail.gmail.com>
References: <CA+B-+fzM9sxc+F7hxoWMo6hM6u64gMJMQSENAt7+GXRdTUeAfg@mail.gmail.com>
	<CAEYYnxYGGeTCtzGjN3+VRsRwB=ni7izCA21MXudn0Aq=kEaxtw@mail.gmail.com>
	<CA+B-+fxSfun1gzWTcm+m1jSyyxi0-CDjE4TkMY_0Vb2WkKAowA@mail.gmail.com>
	<CAEYYnxZMnAiBwNW8O7_moMbX8hBaZO1Gw=TY2gcMOhJrRHVb9Q@mail.gmail.com>
	<CALW2ey1E1tE9gsbs4ftv8J1JQseANMaUt-muGCK=KG3UDLRC7Q@mail.gmail.com>
	<CAEYYnxYo=9JwQpDQiNRvMp3n_tJLUOwWv_Mz28-P9VxW5QHAdg@mail.gmail.com>
	<CALW2ey2JBUY1VwTNHV4rirYyF0swcfwQamW2y--NJL5axo31cw@mail.gmail.com>
Date: Wed, 7 May 2014 11:31:58 -0700
Message-ID: <CA+B-+fx1DOwvzZttLypFBF6qg8M1X48hLL=UP__RVbJYNYCwiw@mail.gmail.com>
Subject: Re: mllib vector templates
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2fd6c686f1c04f8d39449
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2fd6c686f1c04f8d39449
Content-Type: text/plain; charset=UTF-8

Hi,

I see ALS is still using Array[Int] but for other mllib algorithm we moved
to Vector[Double] so that it can support either dense and sparse formats...

I know ALS can stay in Array[Int] due to the Netflix format for input
datasets which is well defined but it helps if we move ALS to
Vector[Double] as well...that way all algorithms will be consistent...

Does it make sense ?

Thanks.
Deb



On Mon, May 5, 2014 at 4:05 PM, David Hall <dlwh@cs.berkeley.edu> wrote:

> On Mon, May 5, 2014 at 3:40 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>
> > David,
> >
> > Could we use Int, Long, Float as the data feature spaces, and Double for
> > optimizer?
> >
>
> Yes. Breeze doesn't allow operations on mixed types, so you'd need to
> convert the double vectors to Floats if you wanted, e.g. dot product with
> the weights vector.
>
> You might also be interested in FeatureVector, which is just a wrapper
> around Array[Int] that emulates an indicator vector. It supports dot
> products, axpy, etc.
>
> -- David
>
>
> >
> >
> > Sincerely,
> >
> > DB Tsai
> > -------------------------------------------------------
> > My Blog: https://www.dbtsai.com
> > LinkedIn: https://www.linkedin.com/in/dbtsai
> >
> >
> > On Mon, May 5, 2014 at 3:06 PM, David Hall <dlwh@cs.berkeley.edu> wrote:
> >
> > > Lbfgs and other optimizers would not work immediately, as they require
> > > vector spaces over double. Otherwise it should work.
> > > On May 5, 2014 3:03 PM, "DB Tsai" <dbtsai@stanford.edu> wrote:
> > >
> > > > Breeze could take any type (Int, Long, Double, and Float) in the
> matrix
> > > > template.
> > > >
> > > >
> > > > Sincerely,
> > > >
> > > > DB Tsai
> > > > -------------------------------------------------------
> > > > My Blog: https://www.dbtsai.com
> > > > LinkedIn: https://www.linkedin.com/in/dbtsai
> > > >
> > > >
> > > > On Mon, May 5, 2014 at 2:56 PM, Debasish Das <
> debasish.das83@gmail.com
> > > > >wrote:
> > > >
> > > > > Is this a breeze issue or breeze can take templates on float /
> > double ?
> > > > >
> > > > > If breeze can take templates then it is a minor fix for
> Vectors.scala
> > > > right
> > > > > ?
> > > > >
> > > > > Thanks.
> > > > > Deb
> > > > >
> > > > >
> > > > > On Mon, May 5, 2014 at 2:45 PM, DB Tsai <dbtsai@stanford.edu>
> wrote:
> > > > >
> > > > > > +1  Would be nice that we can use different type in Vector.
> > > > > >
> > > > > >
> > > > > > Sincerely,
> > > > > >
> > > > > > DB Tsai
> > > > > > -------------------------------------------------------
> > > > > > My Blog: https://www.dbtsai.com
> > > > > > LinkedIn: https://www.linkedin.com/in/dbtsai
> > > > > >
> > > > > >
> > > > > > On Mon, May 5, 2014 at 2:41 PM, Debasish Das <
> > > debasish.das83@gmail.com
> > > > > > >wrote:
> > > > > >
> > > > > > > Hi,
> > > > > > >
> > > > > > > Why mllib vector is using double as default ?
> > > > > > >
> > > > > > > /**
> > > > > > >
> > > > > > >  * Represents a numeric vector, whose index type is Int and
> value
> > > > type
> > > > > is
> > > > > > > Double.
> > > > > > >
> > > > > > >  */
> > > > > > >
> > > > > > > trait Vector extends Serializable {
> > > > > > >
> > > > > > >
> > > > > > >   /**
> > > > > > >
> > > > > > >    * Size of the vector.
> > > > > > >
> > > > > > >    */
> > > > > > >
> > > > > > >   def size: Int
> > > > > > >
> > > > > > >
> > > > > > >   /**
> > > > > > >
> > > > > > >    * Converts the instance to a double array.
> > > > > > >
> > > > > > >    */
> > > > > > >
> > > > > > >   def toArray: Array[Double]
> > > > > > >
> > > > > > > Don't we need a template on float/double ? This will give us
> > memory
> > > > > > > savings...
> > > > > > >
> > > > > > > Thanks.
> > > > > > >
> > > > > > > Deb
> > > > > > >
> > > > > >
> > > > >
> > > >
> > >
> >
>

--001a11c2fd6c686f1c04f8d39449--

From dev-return-7594-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 15 11:25:32 2014
Return-Path: <dev-return-7594-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 19F0F11756
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 15 May 2014 11:25:32 +0000 (UTC)
Received: (qmail 58705 invoked by uid 500); 15 May 2014 05:19:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58633 invoked by uid 500); 15 May 2014 05:19:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58610 invoked by uid 99); 15 May 2014 05:19:43 -0000
Received: from Unknown (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 May 2014 05:19:43 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.192.50 as permitted sender)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 May 2014 05:19:23 +0000
Received: by mail-qg0-f50.google.com with SMTP id z60so905623qgd.37
        for <dev@spark.apache.org>; Wed, 14 May 2014 22:19:02 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=mjsGjnFgLnSPJ4XaMHXfOSMf2lIbyyOyndOID9aVgeQ=;
        b=xxp1JuPq5VtJhaThmdb8i3qs2zh8Rcj2oRewKvXkzvL8Kp6EaTWUBtsF0dkqzMEvwz
         UCkBLuZZ3X6TUvZs/n4frWH/tyG3TytW+rBsO31ny/x83qnD383rJJ12S5fvs1pX5MQ8
         W0/QAHaib+bXCJip7UR8JwIOT9Hs7aCEale321nPZd2SmnEX1xhzXifEOJ/7JTdMNrG3
         zrpxt4Y0x4UYPlzcy6in/AnDdgcGRm6vLYPkyxCAGQKUjMlX8PglYK0dMGnxnQem3xGE
         hnG7VdJzK9TSc/8niJiWWEGu774IymGIo/C76Wl7dc4lw6MliJqICpOR2Jmn98S28EZ2
         9kEw==
MIME-Version: 1.0
X-Received: by 10.140.21.101 with SMTP id 92mr11495856qgk.57.1400131142687;
 Wed, 14 May 2014 22:19:02 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Wed, 14 May 2014 22:19:02 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Wed, 14 May 2014 22:19:02 -0700 (PDT)
In-Reply-To: <JIRA.12713182.1399581846384.322578.1399759998378@arcas>
References: <JIRA.12713182.1399581846384@arcas>
	<JIRA.12713182.1399581846384.322578.1399759998378@arcas>
Date: Thu, 15 May 2014 10:49:02 +0530
Message-ID: <CAJiQeYKjQ5wufVYVQk0nco7NaMGsHqT1L046O6=UhLmF05zk4w@mail.gmail.com>
Subject: Re: [jira] [Created] (SPARK-1767) Prefer HDFS-cached replicas when
 scheduling data-local tasks
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c130e666211904f9696fbf
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c130e666211904f9696fbf
Content-Type: text/plain; charset=UTF-8

Hi Sandy,

  I assume you are referring to caching added to datanodes via new caching
api via NN ? (To preemptively mmap blocks).

I have not looked in detail, but does NN tell us about this in block
locations?
If yes, we can simply make those process local instead of node local for
executors on that node.

This would simply be a change to hadoop based rdd partitioning (what makes
it tricky is to expose currently 'alive' executors to partition)

Thanks
Mridul
On 15-May-2014 3:49 am, "Sandy Ryza (JIRA)" <jira@apache.org> wrote:

> Sandy Ryza created SPARK-1767:
> ---------------------------------
>
>              Summary: Prefer HDFS-cached replicas when scheduling
> data-local tasks
>                  Key: SPARK-1767
>                  URL: https://issues.apache.org/jira/browse/SPARK-1767
>              Project: Spark
>           Issue Type: Improvement
>           Components: Spark Core
>     Affects Versions: 1.0.0
>             Reporter: Sandy Ryza
>
>
>
>
>
>
> --
> This message was sent by Atlassian JIRA
> (v6.2#6252)
>

--001a11c130e666211904f9696fbf--

From dev-return-7596-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 14:28:21 2014
Return-Path: <dev-return-7596-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D63EB11218
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 14:28:21 +0000 (UTC)
Received: (qmail 241 invoked by uid 500); 16 May 2014 10:59:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75948 invoked by uid 500); 16 May 2014 10:48:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58959 invoked by uid 99); 16 May 2014 10:34:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 10:34:55 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.174 as permitted sender)
Received: from [209.85.214.174] (HELO mail-ob0-f174.google.com) (209.85.214.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 09:16:51 +0000
Received: by mail-ob0-f174.google.com with SMTP id uz6so2642036obc.5
        for <dev@spark.apache.org>; Fri, 16 May 2014 02:16:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=0ln6Hd0NERrH3KmLUsjeD5KPuRewDMd2goAu9RAVnzk=;
        b=rpOJNeclvv6Xdb3mLPp6S4SfhamytJwf7lZfNSwbTVjhmKTv+bsnOUPNZoTmi9tCnr
         BRvMBcjLYmbbsbKi6YEeIpeIuyZG9p9pVjSt6RdObLSCnHEzVHkHvvf4xasS+251S94K
         w/hhkmyWQGLcQDDzWeQcBCHyLbHp6Xr3GG+vUnhQnO1l7Fa+aTDkMhWNHqaFww0XP8Yq
         62hxHFSik9LNmU06XpYYHLiLVjdwW7MR1W/y5tylQt5jYbkczBXaJdS2WMb4/tn+8hFY
         3Q8mfDMvZEe7Wk7pdwKzSqCSh+j9X6NxrYI0E3awzg3eQMB8h0lbUVkZTjW4wHuKgiBx
         0SXQ==
MIME-Version: 1.0
X-Received: by 10.182.163.45 with SMTP id yf13mr15703653obb.66.1400231790982;
 Fri, 16 May 2014 02:16:30 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Fri, 16 May 2014 02:16:30 -0700 (PDT)
Date: Fri, 16 May 2014 02:16:30 -0700
Message-ID: <CABPQxsvAw5x0d7mpXW5OePKpCg0DMmuQVDwkrE+UMk55EMf8EQ@mail.gmail.com>
Subject: [VOTE] Release Apache Spark 1.0.0 (rc8)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

[Due to ASF e-mail outage, I'm not if anyone will actually receive this.]

Please vote on releasing the following candidate as Apache Spark version 1.0.0!
This has only minor changes on top of rc7.

The tag to be voted on is v1.0.0-rc8 (commit 80eea0f):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=80eea0f111c06260ffaa780d2f3f7facd09c17bc

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc8/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1016/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/

Please vote on releasing this package as Apache Spark 1.0.0!

The vote is open until Monday, May 19, at 10:15 UTC and passes if a
majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.0.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== API Changes ==
We welcome users to compile Spark applications against 1.0. There are
a few API changes in this release. Here are links to the associated
upgrade guides - user facing changes have been kept as small as
possible.

changes to ML vector specification:
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#from-09-to-10

changes to the Java API:
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark

changes to the streaming API:
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x

changes to the GraphX API:
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091

coGroup and related functions now return Iterable[T] instead of Seq[T]
==> Call toSeq on the result to restore the old behavior

SparkContext.jarOfClass returns Option[String] instead of Seq[String]
==> Call toSeq on the result to restore old behavior

From dev-return-7601-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 14:29:13 2014
Return-Path: <dev-return-7601-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 804261139F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 14:29:13 +0000 (UTC)
Received: (qmail 72208 invoked by uid 500); 16 May 2014 11:19:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15985 invoked by uid 500); 16 May 2014 11:03:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89691 invoked by uid 99); 16 May 2014 10:54:30 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 10:54:30 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 74.125.82.172 as permitted sender)
Received: from [74.125.82.172] (HELO mail-we0-f172.google.com) (74.125.82.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 03:34:03 +0000
Received: by mail-we0-f172.google.com with SMTP id k48so1989688wev.3
        for <dev@spark.incubator.apache.org>; Thu, 15 May 2014 20:33:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=QaRgNMAtOYSEFd1qVZH/aV8aklO07BKimVxChGrxHYQ=;
        b=xxbLOcOoS27ROiYKwYhGgdLvep88uMwmLQh3olj2QdFKa93WQFrKIu/2le11hNS7x5
         rGUQf6ph0StuGzpR7PWi2qil1Y6i2O3JVpEMYw9TsktCDFDv90VSPZGgrqppWf7SwClS
         s1arZbc1rAEifZvED1n/R2G9YmO9A+Z0DFdbz23kMw5K1ccKF2yQp2IUzlUJVClHVh7i
         jC6V0TzXAvOeILlp5uapO8JE+naVKkzeGxgsJmixw6Qjv4TNW6sSvOnnjJrd3ZV7R7tC
         izxAX3k1dwCgpmd1d16wBa0biDQ85Q78VAkTLPLxJ94inlVUwRSlcfddbeLTgA4pe+QC
         8h7A==
MIME-Version: 1.0
X-Received: by 10.180.73.201 with SMTP id n9mr11456802wiv.45.1400211220925;
 Thu, 15 May 2014 20:33:40 -0700 (PDT)
Received: by 10.194.187.20 with HTTP; Thu, 15 May 2014 20:33:40 -0700 (PDT)
In-Reply-To: <CA+B-+fxNcFuQmdsQERiddQczHD6LYnKTx4QBV6p_QS=eE13n5g@mail.gmail.com>
References: <CA+B-+fzM9sxc+F7hxoWMo6hM6u64gMJMQSENAt7+GXRdTUeAfg@mail.gmail.com>
	<CAEYYnxYGGeTCtzGjN3+VRsRwB=ni7izCA21MXudn0Aq=kEaxtw@mail.gmail.com>
	<CA+B-+fxSfun1gzWTcm+m1jSyyxi0-CDjE4TkMY_0Vb2WkKAowA@mail.gmail.com>
	<CAEYYnxZMnAiBwNW8O7_moMbX8hBaZO1Gw=TY2gcMOhJrRHVb9Q@mail.gmail.com>
	<CALW2ey1E1tE9gsbs4ftv8J1JQseANMaUt-muGCK=KG3UDLRC7Q@mail.gmail.com>
	<CAEYYnxYo=9JwQpDQiNRvMp3n_tJLUOwWv_Mz28-P9VxW5QHAdg@mail.gmail.com>
	<CALW2ey2JBUY1VwTNHV4rirYyF0swcfwQamW2y--NJL5axo31cw@mail.gmail.com>
	<CA+B-+fx1DOwvzZttLypFBF6qg8M1X48hLL=UP__RVbJYNYCwiw@mail.gmail.com>
	<CA+B-+fxNcFuQmdsQERiddQczHD6LYnKTx4QBV6p_QS=eE13n5g@mail.gmail.com>
Date: Thu, 15 May 2014 20:33:40 -0700
Message-ID: <CAJgQjQ9uCtnxJUVU9o4sWfOwc78WfFmxbrMS=j2QffvonFkU=Q@mail.gmail.com>
Subject: Re: mllib vector templates
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I submitted a PR for standardizing the text format for vectors and
labeled data: https://github.com/apache/spark/pull/685

Once it gets merged, saveAsTextFile and loading should be consistent.
I didn't choose LibSVM as the default format because two reasons:

1) It doesn't contain feature dimension info in the record. We need to
scan the dataset to get that info.
2) It saves index:value tuples. Putting indices together can help data
compression. Same for value if there are many binary features.

Best,
Xiangrui

On Wed, May 7, 2014 at 10:25 PM, Debasish Das <debasish.das83@gmail.com> wrote:
> Hi,
>
> I see ALS is still using Array[Int] but for other mllib algorithm we moved
> to Vector[Double] so that it can support either dense and sparse formats...
>
> ALS can stay in Array[Int] due to the Netflix format for input datasets
> which is well defined but it helps if we move ALS to Vector[Double] as
> well...that way all algorithms will be consistent...
>
> The second issue is that toString on SparseVector does not write libsvm
> format but something not very generic...can we change the
> SparseVector.toString to write as libsvm output ? I am dumping a sample of
> dataset to see how mllib glm compares with the glmnet-R package for QoR...
>
> Thanks.
> Deb
>
> On Mon, May 5, 2014 at 4:05 PM, David Hall <dlwh@cs.berkeley.edu> wrote:
>>
>>> On Mon, May 5, 2014 at 3:40 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>
>>> > David,
>>> >
>>> > Could we use Int, Long, Float as the data feature spaces, and Double for
>>> > optimizer?
>>> >
>>>
>>> Yes. Breeze doesn't allow operations on mixed types, so you'd need to
>>> convert the double vectors to Floats if you wanted, e.g. dot product with
>>> the weights vector.
>>>
>>> You might also be interested in FeatureVector, which is just a wrapper
>>> around Array[Int] that emulates an indicator vector. It supports dot
>>> products, axpy, etc.
>>>
>>> -- David
>>>
>>>
>>> >
>>> >
>>> > Sincerely,
>>> >
>>> > DB Tsai
>>> > -------------------------------------------------------
>>> > My Blog: https://www.dbtsai.com
>>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>>> >
>>> >
>>> > On Mon, May 5, 2014 at 3:06 PM, David Hall <dlwh@cs.berkeley.edu>
>>> wrote:
>>> >
>>> > > Lbfgs and other optimizers would not work immediately, as they require
>>> > > vector spaces over double. Otherwise it should work.
>>> > > On May 5, 2014 3:03 PM, "DB Tsai" <dbtsai@stanford.edu> wrote:
>>> > >
>>> > > > Breeze could take any type (Int, Long, Double, and Float) in the
>>> matrix
>>> > > > template.
>>> > > >
>>> > > >
>>> > > > Sincerely,
>>> > > >
>>> > > > DB Tsai
>>> > > > -------------------------------------------------------
>>> > > > My Blog: https://www.dbtsai.com
>>> > > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>> > > >
>>> > > >
>>> > > > On Mon, May 5, 2014 at 2:56 PM, Debasish Das <
>>> debasish.das83@gmail.com
>>> > > > >wrote:
>>> > > >
>>> > > > > Is this a breeze issue or breeze can take templates on float /
>>> > double ?
>>> > > > >
>>> > > > > If breeze can take templates then it is a minor fix for
>>> Vectors.scala
>>> > > > right
>>> > > > > ?
>>> > > > >
>>> > > > > Thanks.
>>> > > > > Deb
>>> > > > >
>>> > > > >
>>> > > > > On Mon, May 5, 2014 at 2:45 PM, DB Tsai <dbtsai@stanford.edu>
>>> wrote:
>>> > > > >
>>> > > > > > +1  Would be nice that we can use different type in Vector.
>>> > > > > >
>>> > > > > >
>>> > > > > > Sincerely,
>>> > > > > >
>>> > > > > > DB Tsai
>>> > > > > > -------------------------------------------------------
>>> > > > > > My Blog: https://www.dbtsai.com
>>> > > > > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>> > > > > >
>>> > > > > >
>>> > > > > > On Mon, May 5, 2014 at 2:41 PM, Debasish Das <
>>> > > debasish.das83@gmail.com
>>> > > > > > >wrote:
>>> > > > > >
>>> > > > > > > Hi,
>>> > > > > > >
>>> > > > > > > Why mllib vector is using double as default ?
>>> > > > > > >
>>> > > > > > > /**
>>> > > > > > >
>>> > > > > > >  * Represents a numeric vector, whose index type is Int and
>>> value
>>> > > > type
>>> > > > > is
>>> > > > > > > Double.
>>> > > > > > >
>>> > > > > > >  */
>>> > > > > > >
>>> > > > > > > trait Vector extends Serializable {
>>> > > > > > >
>>> > > > > > >
>>> > > > > > >   /**
>>> > > > > > >
>>> > > > > > >    * Size of the vector.
>>> > > > > > >
>>> > > > > > >    */
>>> > > > > > >
>>> > > > > > >   def size: Int
>>> > > > > > >
>>> > > > > > >
>>> > > > > > >   /**
>>> > > > > > >
>>> > > > > > >    * Converts the instance to a double array.
>>> > > > > > >
>>> > > > > > >    */
>>> > > > > > >
>>> > > > > > >   def toArray: Array[Double]
>>> > > > > > >
>>> > > > > > > Don't we need a template on float/double ? This will give us
>>> > memory
>>> > > > > > > savings...
>>> > > > > > >
>>> > > > > > > Thanks.
>>> > > > > > >
>>> > > > > > > Deb
>>> > > > > > >
>>> > > > > >
>>> > > > >
>>> > > >
>>> > >
>>> >
>>>
>>
>>

From dev-return-7599-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 16:17:50 2014
Return-Path: <dev-return-7599-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 86EFB11AE3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 16:17:50 +0000 (UTC)
Received: (qmail 45384 invoked by uid 500); 16 May 2014 11:14:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98547 invoked by uid 500); 16 May 2014 10:59:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79258 invoked by uid 99); 16 May 2014 10:50:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 10:50:01 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 209.85.212.179 as permitted sender)
Received: from [209.85.212.179] (HELO mail-wi0-f179.google.com) (209.85.212.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 03:34:50 +0000
Received: by mail-wi0-f179.google.com with SMTP id bs8so270234wib.12
        for <dev@spark.incubator.apache.org>; Thu, 15 May 2014 20:34:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=SeKZuFfBPrZhG4l+ZEJ/BGxcE2vj7/XmwYCZtF/wjl8=;
        b=qvW+8t3fw6g6m0C6XPF44iJLa/MBkhEyrBOyIGXoe2Eu7ucCdOZFgUgCEkmgyevgXG
         jaEsAUiN3DxruCqjcivljpnTWO/BjiDW3xFLmV4w4LFLjh7EY6jSDs06k0WLdjbdl3iR
         eYMpHqYDwb/18MNf2mcoAnssuD979bICDfSZ8o0cffXZ6Cxzydc2YQLV9N3ecVU9jI59
         B7tuH+STLLZSuZ65/TgJI4Vgs30mdwKUs6oPdgkGZFb55xZY2iKGMMmbeoUxVk5hXF07
         2epylHNWsZ903Yp0rzvdizv6MCSbDBQxmuvb9Hng5vC9mOhppfZxw7KZIikjfTIGY9GF
         DdbA==
MIME-Version: 1.0
X-Received: by 10.180.211.116 with SMTP id nb20mr11411288wic.5.1400211269376;
 Thu, 15 May 2014 20:34:29 -0700 (PDT)
Received: by 10.194.187.20 with HTTP; Thu, 15 May 2014 20:34:29 -0700 (PDT)
In-Reply-To: <CAJgQjQ9uCtnxJUVU9o4sWfOwc78WfFmxbrMS=j2QffvonFkU=Q@mail.gmail.com>
References: <CA+B-+fzM9sxc+F7hxoWMo6hM6u64gMJMQSENAt7+GXRdTUeAfg@mail.gmail.com>
	<CAEYYnxYGGeTCtzGjN3+VRsRwB=ni7izCA21MXudn0Aq=kEaxtw@mail.gmail.com>
	<CA+B-+fxSfun1gzWTcm+m1jSyyxi0-CDjE4TkMY_0Vb2WkKAowA@mail.gmail.com>
	<CAEYYnxZMnAiBwNW8O7_moMbX8hBaZO1Gw=TY2gcMOhJrRHVb9Q@mail.gmail.com>
	<CALW2ey1E1tE9gsbs4ftv8J1JQseANMaUt-muGCK=KG3UDLRC7Q@mail.gmail.com>
	<CAEYYnxYo=9JwQpDQiNRvMp3n_tJLUOwWv_Mz28-P9VxW5QHAdg@mail.gmail.com>
	<CALW2ey2JBUY1VwTNHV4rirYyF0swcfwQamW2y--NJL5axo31cw@mail.gmail.com>
	<CA+B-+fx1DOwvzZttLypFBF6qg8M1X48hLL=UP__RVbJYNYCwiw@mail.gmail.com>
	<CA+B-+fxNcFuQmdsQERiddQczHD6LYnKTx4QBV6p_QS=eE13n5g@mail.gmail.com>
	<CAJgQjQ9uCtnxJUVU9o4sWfOwc78WfFmxbrMS=j2QffvonFkU=Q@mail.gmail.com>
Date: Thu, 15 May 2014 20:34:29 -0700
Message-ID: <CAJgQjQ8wqtP44_J0n=wDfArsOAbQ+b7b2J_TwKkDVObGahYtWw@mail.gmail.com>
Subject: Re: mllib vector templates
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

3) It is not designed for dense feature vectors.

On Thu, May 15, 2014 at 8:33 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
> I submitted a PR for standardizing the text format for vectors and
> labeled data: https://github.com/apache/spark/pull/685
>
> Once it gets merged, saveAsTextFile and loading should be consistent.
> I didn't choose LibSVM as the default format because two reasons:
>
> 1) It doesn't contain feature dimension info in the record. We need to
> scan the dataset to get that info.
> 2) It saves index:value tuples. Putting indices together can help data
> compression. Same for value if there are many binary features.
>
> Best,
> Xiangrui
>
> On Wed, May 7, 2014 at 10:25 PM, Debasish Das <debasish.das83@gmail.com> wrote:
>> Hi,
>>
>> I see ALS is still using Array[Int] but for other mllib algorithm we moved
>> to Vector[Double] so that it can support either dense and sparse formats...
>>
>> ALS can stay in Array[Int] due to the Netflix format for input datasets
>> which is well defined but it helps if we move ALS to Vector[Double] as
>> well...that way all algorithms will be consistent...
>>
>> The second issue is that toString on SparseVector does not write libsvm
>> format but something not very generic...can we change the
>> SparseVector.toString to write as libsvm output ? I am dumping a sample of
>> dataset to see how mllib glm compares with the glmnet-R package for QoR...
>>
>> Thanks.
>> Deb
>>
>> On Mon, May 5, 2014 at 4:05 PM, David Hall <dlwh@cs.berkeley.edu> wrote:
>>>
>>>> On Mon, May 5, 2014 at 3:40 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>>
>>>> > David,
>>>> >
>>>> > Could we use Int, Long, Float as the data feature spaces, and Double for
>>>> > optimizer?
>>>> >
>>>>
>>>> Yes. Breeze doesn't allow operations on mixed types, so you'd need to
>>>> convert the double vectors to Floats if you wanted, e.g. dot product with
>>>> the weights vector.
>>>>
>>>> You might also be interested in FeatureVector, which is just a wrapper
>>>> around Array[Int] that emulates an indicator vector. It supports dot
>>>> products, axpy, etc.
>>>>
>>>> -- David
>>>>
>>>>
>>>> >
>>>> >
>>>> > Sincerely,
>>>> >
>>>> > DB Tsai
>>>> > -------------------------------------------------------
>>>> > My Blog: https://www.dbtsai.com
>>>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>> >
>>>> >
>>>> > On Mon, May 5, 2014 at 3:06 PM, David Hall <dlwh@cs.berkeley.edu>
>>>> wrote:
>>>> >
>>>> > > Lbfgs and other optimizers would not work immediately, as they require
>>>> > > vector spaces over double. Otherwise it should work.
>>>> > > On May 5, 2014 3:03 PM, "DB Tsai" <dbtsai@stanford.edu> wrote:
>>>> > >
>>>> > > > Breeze could take any type (Int, Long, Double, and Float) in the
>>>> matrix
>>>> > > > template.
>>>> > > >
>>>> > > >
>>>> > > > Sincerely,
>>>> > > >
>>>> > > > DB Tsai
>>>> > > > -------------------------------------------------------
>>>> > > > My Blog: https://www.dbtsai.com
>>>> > > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>> > > >
>>>> > > >
>>>> > > > On Mon, May 5, 2014 at 2:56 PM, Debasish Das <
>>>> debasish.das83@gmail.com
>>>> > > > >wrote:
>>>> > > >
>>>> > > > > Is this a breeze issue or breeze can take templates on float /
>>>> > double ?
>>>> > > > >
>>>> > > > > If breeze can take templates then it is a minor fix for
>>>> Vectors.scala
>>>> > > > right
>>>> > > > > ?
>>>> > > > >
>>>> > > > > Thanks.
>>>> > > > > Deb
>>>> > > > >
>>>> > > > >
>>>> > > > > On Mon, May 5, 2014 at 2:45 PM, DB Tsai <dbtsai@stanford.edu>
>>>> wrote:
>>>> > > > >
>>>> > > > > > +1  Would be nice that we can use different type in Vector.
>>>> > > > > >
>>>> > > > > >
>>>> > > > > > Sincerely,
>>>> > > > > >
>>>> > > > > > DB Tsai
>>>> > > > > > -------------------------------------------------------
>>>> > > > > > My Blog: https://www.dbtsai.com
>>>> > > > > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>> > > > > >
>>>> > > > > >
>>>> > > > > > On Mon, May 5, 2014 at 2:41 PM, Debasish Das <
>>>> > > debasish.das83@gmail.com
>>>> > > > > > >wrote:
>>>> > > > > >
>>>> > > > > > > Hi,
>>>> > > > > > >
>>>> > > > > > > Why mllib vector is using double as default ?
>>>> > > > > > >
>>>> > > > > > > /**
>>>> > > > > > >
>>>> > > > > > >  * Represents a numeric vector, whose index type is Int and
>>>> value
>>>> > > > type
>>>> > > > > is
>>>> > > > > > > Double.
>>>> > > > > > >
>>>> > > > > > >  */
>>>> > > > > > >
>>>> > > > > > > trait Vector extends Serializable {
>>>> > > > > > >
>>>> > > > > > >
>>>> > > > > > >   /**
>>>> > > > > > >
>>>> > > > > > >    * Size of the vector.
>>>> > > > > > >
>>>> > > > > > >    */
>>>> > > > > > >
>>>> > > > > > >   def size: Int
>>>> > > > > > >
>>>> > > > > > >
>>>> > > > > > >   /**
>>>> > > > > > >
>>>> > > > > > >    * Converts the instance to a double array.
>>>> > > > > > >
>>>> > > > > > >    */
>>>> > > > > > >
>>>> > > > > > >   def toArray: Array[Double]
>>>> > > > > > >
>>>> > > > > > > Don't we need a template on float/double ? This will give us
>>>> > memory
>>>> > > > > > > savings...
>>>> > > > > > >
>>>> > > > > > > Thanks.
>>>> > > > > > >
>>>> > > > > > > Deb
>>>> > > > > > >
>>>> > > > > >
>>>> > > > >
>>>> > > >
>>>> > >
>>>> >
>>>>
>>>
>>>

From dev-return-7611-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 17:16:43 2014
Return-Path: <dev-return-7611-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2E62011DDF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 17:16:43 +0000 (UTC)
Received: (qmail 60607 invoked by uid 500); 16 May 2014 16:08:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44827 invoked by uid 500); 16 May 2014 15:43:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33244 invoked by uid 99); 16 May 2014 15:40:20 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 15:40:20 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yuzhihong@gmail.com designates 209.85.160.176 as permitted sender)
Received: from [209.85.160.176] (HELO mail-yk0-f176.google.com) (209.85.160.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 15:40:17 +0000
Received: by mail-yk0-f176.google.com with SMTP id q9so2274323ykb.35
        for <dev@spark.apache.org>; Fri, 16 May 2014 08:39:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=RpGaXEKKf0BNqXaZf1YZRlFBWowuQuqmyDmSdLcp4KQ=;
        b=CiL8qvlXVGwODqPDbwefd9c9euaALDS4sNQ6A0g5h1MMmWY9frWmwXYhq/ZYSESIUr
         gkDYxoRZdtx7eLmrvTHERTKwQbjG3hYPGCfOa+QpKc/aLwh+hC5RHP6aPlngDjPL04uI
         GIroNEQzKkPlKHxsmxCxSsmXgitjR173Po7/BUxdq5+R7Zm7mZHxUi8ADjCVXGlaHODH
         CkdsaxDbdJzRQJP1p0G4JMl6zPgil9S5VQhZhNhNRv8lrEv2skBBTKJvB3xi6gGGfH7b
         Vv/2z23mf+U7KYy93MrAm+DB61P99t3x6c5i3/cZTtbiyQxC9jpDCcs7iCpubx8MZ3ox
         w9tg==
MIME-Version: 1.0
X-Received: by 10.236.10.82 with SMTP id 58mr26764577yhu.118.1400254793657;
 Fri, 16 May 2014 08:39:53 -0700 (PDT)
Received: by 10.170.37.144 with HTTP; Fri, 16 May 2014 08:39:53 -0700 (PDT)
In-Reply-To: <CAMJOb8k_6d_0ixRQz0yFcjt59y42pW+rw_NsJUCHiG7aOmcn2A@mail.gmail.com>
References: <CAMJOb8k_6d_0ixRQz0yFcjt59y42pW+rw_NsJUCHiG7aOmcn2A@mail.gmail.com>
Date: Fri, 16 May 2014 08:39:53 -0700
Message-ID: <CALte62wG+m6kN8ZysuSPTeXGPukrmyvEYxhXeAc6ccoj4NTVrA@mail.gmail.com>
Subject: Re: (test)
From: Ted Yu <yuzhihong@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1137feb49232f804f9863976
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1137feb49232f804f9863976
Content-Type: text/plain; charset=UTF-8

Yes.


On Thu, May 15, 2014 at 10:34 AM, Andrew Or <andrew@databricks.com> wrote:

> Apache has been having some problems lately. Do you guys see this message?
>

--001a1137feb49232f804f9863976--

From dev-return-7610-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 17:21:07 2014
Return-Path: <dev-return-7610-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4A63711E01
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 17:21:07 +0000 (UTC)
Received: (qmail 63306 invoked by uid 500); 16 May 2014 16:08:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40902 invoked by uid 500); 16 May 2014 15:43:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85425 invoked by uid 99); 16 May 2014 15:40:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 15:40:56 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 15:40:51 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 345E8102138
	for <dev@spark.apache.org>; Fri, 16 May 2014 08:40:31 -0700 (PDT)
Received: from mail-qg0-f42.google.com (mail-qg0-f42.google.com [209.85.192.42])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id A4151102137
	for <dev@spark.apache.org>; Fri, 16 May 2014 08:40:30 -0700 (PDT)
Received: by mail-qg0-f42.google.com with SMTP id q107so4549949qgd.29
        for <dev@spark.apache.org>; Fri, 16 May 2014 08:40:29 -0700 (PDT)
X-Gm-Message-State: ALoCoQm+wCIHsRut/vyuJa1DeRX0NV3anxDio/3pviA4tkTcxJkgaJGC+vlrW+DQt9COC2fNgltU
MIME-Version: 1.0
X-Received: by 10.224.15.137 with SMTP id k9mr14388797qaa.104.1400254829789;
 Fri, 16 May 2014 08:40:29 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Fri, 16 May 2014 08:40:29 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Fri, 16 May 2014 08:40:29 -0700 (PDT)
In-Reply-To: <CAMJOb8k_6d_0ixRQz0yFcjt59y42pW+rw_NsJUCHiG7aOmcn2A@mail.gmail.com>
References: <CAMJOb8k_6d_0ixRQz0yFcjt59y42pW+rw_NsJUCHiG7aOmcn2A@mail.gmail.com>
Date: Fri, 16 May 2014 08:40:29 -0700
Message-ID: <CAEYYnxZBGT6ma=r_5SsJ7Eiv8UVFbqc9fibLSSSWjvGYo+ix+w@mail.gmail.com>
Subject: Re: (test)
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bdc912eb9a40404f9863bad
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc912eb9a40404f9863bad
Content-Type: text/plain; charset=UTF-8

Yes.
On May 16, 2014 8:39 AM, "Andrew Or" <andrew@databricks.com> wrote:

> Apache has been having some problems lately. Do you guys see this message?
>

--047d7bdc912eb9a40404f9863bad--

From dev-return-7613-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 17:23:32 2014
Return-Path: <dev-return-7613-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6F33511E18
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 17:23:32 +0000 (UTC)
Received: (qmail 71260 invoked by uid 500); 16 May 2014 16:58:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66482 invoked by uid 500); 16 May 2014 16:58:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49660 invoked by uid 99); 16 May 2014 16:42:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 16:42:09 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of glenn.strycker@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 16:41:55 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <glenn.strycker@gmail.com>)
	id 1WlLCQ-0004Y0-NQ
	for dev@spark.incubator.apache.org; Fri, 16 May 2014 09:41:34 -0700
Date: Fri, 16 May 2014 09:41:34 -0700 (PDT)
From: GlennStrycker <glenn.strycker@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1400258494709-6593.post@n3.nabble.com>
Subject: Scala examples for Spark do not work as written in documentation
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

On the webpage http://spark.apache.org/examples.html, there is an example
written as

val count = spark.parallelize(1 to NUM_SAMPLES).map(i =>
  val x = Math.random()
  val y = Math.random()
  if (x*x + y*y < 1) 1 else 0
).reduce(_ + _)
println("Pi is roughly " + 4.0 * count / NUM_SAMPLES)

This does not execute in Spark, which gives me an error:
<console>:2: error: illegal start of simple expression
         val x = Math.random()
         ^

If I rewrite the query slightly, adding in {}, it works:

val count = spark.parallelize(1 to 10000).map(i =>
   {
   val x = Math.random()
   val y = Math.random()
   if (x*x + y*y < 1) 1 else 0
   }
).reduce(_ + _)
println("Pi is roughly " + 4.0 * count / 10000.0)





--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Scala-examples-for-Spark-do-not-work-as-written-in-documentation-tp6593.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7606-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 17:31:56 2014
Return-Path: <dev-return-7606-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 150B511F2D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 17:31:56 +0000 (UTC)
Received: (qmail 93980 invoked by uid 500); 16 May 2014 11:50:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78176 invoked by uid 500); 16 May 2014 11:37:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85304 invoked by uid 99); 16 May 2014 11:20:58 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 11:20:58 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.216.182 as permitted sender)
Received: from [209.85.216.182] (HELO mail-qc0-f182.google.com) (209.85.216.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 May 2014 17:18:40 +0000
Received: by mail-qc0-f182.google.com with SMTP id e16so2276985qcx.41
        for <dev@spark.apache.org>; Thu, 15 May 2014 10:18:16 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=Tm0y/9MO2v8u5su4Q9RONoqfmomyNWbZe2DV3plDUu4=;
        b=fzQpP26Wy7qrZISFGBC4MAIjJ3dsCTw7at/2ajufkHtiAW0oAyNU5xYCgzbDleKLDt
         ExLn9NqFLjFOoR/lGgK5X8AnKtXdgnkunD47jm13AYRVU+XMroRkU6ddtL26Rd2hwVyJ
         DAbaCfx0C6Nl03e9VTp+g8zCJW/yrJ9VNvx6kmvevJ24Jm1focZIWKP9rjtMVGVtMox+
         ZE8TPcQi4gUuR/IM00L8MwfrJOHcafyHq1h90XjM6tn+IflCIsDKIe4bVodUNwDpaAOc
         Zaq9tZZZrEBCR2j4WyeOouTDP/N9tKhhth+eKVbFJXLYYWIA2Cw/CWNNefwdDpm4375G
         aCKg==
MIME-Version: 1.0
X-Received: by 10.140.82.7 with SMTP id g7mr16708690qgd.74.1400174296502; Thu,
 15 May 2014 10:18:16 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Thu, 15 May 2014 10:18:16 -0700 (PDT)
In-Reply-To: <CABPQxst79373EAcpmFemjukod+9veP_F=EeRZWch84_=fo1xTQ@mail.gmail.com>
References: <CABPQxst79373EAcpmFemjukod+9veP_F=EeRZWch84_=fo1xTQ@mail.gmail.com>
Date: Thu, 15 May 2014 22:48:16 +0530
Message-ID: <CAJiQeYLxzKRwuH8qvmHR10DtkzboHQyj_hDE1_-CXum3CwzRTg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc6)
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

So was rc5 cancelled ? Did not see a note indicating that or why ... [1]

- Mridul


[1] could have easily missed it in the email storm though !

On Thu, May 15, 2014 at 1:32 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.0.0!
>
> This patch has a few minor fixes on top of rc5. I've also built the
> binary artifacts with Hive support enabled so people can test this
> configuration. When we release 1.0 we might just release both vanilla
> and Hive-enabled binaries.
>
> The tag to be voted on is v1.0.0-rc6 (commit 54133a):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=54133abdce0246f6643a1112a5204afb2c4caa82
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.0.0-rc6/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachestratos-1011
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.0.0-rc6-docs/
>
> Please vote on releasing this package as Apache Spark 1.0.0!
>
> The vote is open until Saturday, May 17, at 20:58 UTC and passes if
> amajority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.0.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == API Changes ==
> We welcome users to compile Spark applications against 1.0. There are
> a few API changes in this release. Here are links to the associated
> upgrade guides - user facing changes have been kept as small as
> possible.
>
> changes to ML vector specification:
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10
>
> changes to the Java API:
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>
> changes to the streaming API:
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>
> changes to the GraphX API:
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>
> coGroup and related functions now return Iterable[T] instead of Seq[T]
> ==> Call toSeq on the result to restore the old behavior
>
> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> ==> Call toSeq on the result to restore old behavior

From dev-return-7597-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 17:35:54 2014
Return-Path: <dev-return-7597-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5071711141
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 17:35:54 +0000 (UTC)
Received: (qmail 96200 invoked by uid 500); 16 May 2014 10:57:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76701 invoked by uid 500); 16 May 2014 10:48:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58932 invoked by uid 99); 16 May 2014 10:34:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 10:34:55 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of henry.saputra@gmail.com designates 74.125.82.177 as permitted sender)
Received: from [74.125.82.177] (HELO mail-we0-f177.google.com) (74.125.82.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 08:05:54 +0000
Received: by mail-we0-f177.google.com with SMTP id x48so2155589wes.22
        for <dev@spark.apache.org>; Fri, 16 May 2014 01:05:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=9Nb66tbeUawK0otE2Dr/TFQI/prrcDm1bVt94cisMEI=;
        b=aAumKE7GhpLC8iBfrelg0dylqp/BEs530vDOsI9bXsiGHn4OisPHgaQP7ACv7FMi/7
         n74h1r5oZXfdR2uP7huB1Sxp6Sy5+uYqmOcNfBkXyeHSaoINpCs4GuisHe7jY1b6O58Z
         cDPnsl6AaQK4C5f0idM17u8ezmlTqVqst1Cp8+0dFE97Ruzt2fHIpTL8K7I3ufx0cIKV
         5Lop98MER3OidE0FKRrermquNHxI4LcheOMBPGFF1+Fk54Ygk/iklepRtLhVUhJl2aC7
         NGRNV1IZ1WLuioqZFdflidL6dOQZeW5NsdvQtGpM15ECvDQe4qrJERHJIhMsXu3S0B6X
         BPiQ==
MIME-Version: 1.0
X-Received: by 10.194.1.164 with SMTP id 4mr12432171wjn.17.1400227531480; Fri,
 16 May 2014 01:05:31 -0700 (PDT)
Received: by 10.216.165.71 with HTTP; Fri, 16 May 2014 01:05:31 -0700 (PDT)
In-Reply-To: <CACBYxK+tyorAkUdL4XGBX=KD4LonBqvanm91pW8B8P=QHPOuFg@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com>
	<CAMtqZeeAW6BeCiCfHbiCmKfMh=BnrO=+UM1oPe1VNgB2disQQw@mail.gmail.com>
	<tencent_5EABC2696C9E4E8610F1CE38@qq.com>
	<1400030133573-6560.post@n3.nabble.com>
	<tencent_4A0DC42353D5C43B6806EC6F@qq.com>
	<CACBYxK+tyorAkUdL4XGBX=KD4LonBqvanm91pW8B8P=QHPOuFg@mail.gmail.com>
Date: Fri, 16 May 2014 01:05:31 -0700
Message-ID: <CALuGr6ZGqnYk+ZvfmcGuB8XjAQsBGBUsppm+5Q+FONETbnZt0w@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Henry Saputra <henry.saputra@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

HI Sandy,

Just curious if the Vote is for rc5 or rc6? Gmail shows me that you
replied to the rc5 thread.

Thanks,

- Henry

On Wed, May 14, 2014 at 1:28 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
> +1 (non-binding)
>
> * Built the release from source.
> * Compiled Java and Scala apps that interact with HDFS against it.
> * Ran them in local mode.
> * Ran them against a pseudo-distributed YARN cluster in both yarn-client
> mode and yarn-cluster mode.
>
>
> On Tue, May 13, 2014 at 9:09 PM, witgo <witgo@qq.com> wrote:
>
>> You need to set:
>> spark.akka.frameSize         5
>> spark.default.parallelism    1
>>
>>
>>
>>
>>
>> ------------------ Original ------------------
>> From:  "Madhu";<madhu@madhu.com>;
>> Date:  Wed, May 14, 2014 09:15 AM
>> To:  "dev"<dev@spark.incubator.apache.org>;
>>
>> Subject:  Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
>>
>>
>>
>> I just built rc5 on Windows 7 and tried to reproduce the problem described
>> in
>>
>> https://issues.apache.org/jira/browse/SPARK-1712
>>
>> It works on my machine:
>>
>> 14/05/13 21:06:47 INFO DAGScheduler: Stage 1 (sum at <console>:17) finished
>> in 4.548 s
>> 14/05/13 21:06:47 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks
>> have all completed, from pool
>> 14/05/13 21:06:47 INFO SparkContext: Job finished: sum at <console>:17,
>> took
>> 4.814991993 s
>> res1: Double = 5.000005E11
>>
>> I used all defaults, no config files were changed.
>> Not sure if that makes a difference...
>>
>>
>>
>> --
>> View this message in context:
>> http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-0-0-rc5-tp6542p6560.html
>> Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com.
>> .

From dev-return-7598-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 19:25:11 2014
Return-Path: <dev-return-7598-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2F47311B6C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 19:25:11 +0000 (UTC)
Received: (qmail 18334 invoked by uid 500); 16 May 2014 11:04:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88005 invoked by uid 500); 16 May 2014 10:54:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71150 invoked by uid 99); 16 May 2014 10:45:58 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 10:45:58 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.212.169 as permitted sender)
Received: from [209.85.212.169] (HELO mail-wi0-f169.google.com) (209.85.212.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 03:34:52 +0000
Received: by mail-wi0-f169.google.com with SMTP id hi2so1530790wib.4
        for <dev@spark.apache.org>; Thu, 15 May 2014 20:34:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=SeKZuFfBPrZhG4l+ZEJ/BGxcE2vj7/XmwYCZtF/wjl8=;
        b=qvW+8t3fw6g6m0C6XPF44iJLa/MBkhEyrBOyIGXoe2Eu7ucCdOZFgUgCEkmgyevgXG
         jaEsAUiN3DxruCqjcivljpnTWO/BjiDW3xFLmV4w4LFLjh7EY6jSDs06k0WLdjbdl3iR
         eYMpHqYDwb/18MNf2mcoAnssuD979bICDfSZ8o0cffXZ6Cxzydc2YQLV9N3ecVU9jI59
         B7tuH+STLLZSuZ65/TgJI4Vgs30mdwKUs6oPdgkGZFb55xZY2iKGMMmbeoUxVk5hXF07
         2epylHNWsZ903Yp0rzvdizv6MCSbDBQxmuvb9Hng5vC9mOhppfZxw7KZIikjfTIGY9GF
         DdbA==
MIME-Version: 1.0
X-Received: by 10.180.211.116 with SMTP id nb20mr11411288wic.5.1400211269376;
 Thu, 15 May 2014 20:34:29 -0700 (PDT)
Received: by 10.194.187.20 with HTTP; Thu, 15 May 2014 20:34:29 -0700 (PDT)
In-Reply-To: <CAJgQjQ9uCtnxJUVU9o4sWfOwc78WfFmxbrMS=j2QffvonFkU=Q@mail.gmail.com>
References: <CA+B-+fzM9sxc+F7hxoWMo6hM6u64gMJMQSENAt7+GXRdTUeAfg@mail.gmail.com>
	<CAEYYnxYGGeTCtzGjN3+VRsRwB=ni7izCA21MXudn0Aq=kEaxtw@mail.gmail.com>
	<CA+B-+fxSfun1gzWTcm+m1jSyyxi0-CDjE4TkMY_0Vb2WkKAowA@mail.gmail.com>
	<CAEYYnxZMnAiBwNW8O7_moMbX8hBaZO1Gw=TY2gcMOhJrRHVb9Q@mail.gmail.com>
	<CALW2ey1E1tE9gsbs4ftv8J1JQseANMaUt-muGCK=KG3UDLRC7Q@mail.gmail.com>
	<CAEYYnxYo=9JwQpDQiNRvMp3n_tJLUOwWv_Mz28-P9VxW5QHAdg@mail.gmail.com>
	<CALW2ey2JBUY1VwTNHV4rirYyF0swcfwQamW2y--NJL5axo31cw@mail.gmail.com>
	<CA+B-+fx1DOwvzZttLypFBF6qg8M1X48hLL=UP__RVbJYNYCwiw@mail.gmail.com>
	<CA+B-+fxNcFuQmdsQERiddQczHD6LYnKTx4QBV6p_QS=eE13n5g@mail.gmail.com>
	<CAJgQjQ9uCtnxJUVU9o4sWfOwc78WfFmxbrMS=j2QffvonFkU=Q@mail.gmail.com>
Date: Thu, 15 May 2014 20:34:29 -0700
Message-ID: <CAJgQjQ8wqtP44_J0n=wDfArsOAbQ+b7b2J_TwKkDVObGahYtWw@mail.gmail.com>
Subject: Re: mllib vector templates
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

3) It is not designed for dense feature vectors.

On Thu, May 15, 2014 at 8:33 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
> I submitted a PR for standardizing the text format for vectors and
> labeled data: https://github.com/apache/spark/pull/685
>
> Once it gets merged, saveAsTextFile and loading should be consistent.
> I didn't choose LibSVM as the default format because two reasons:
>
> 1) It doesn't contain feature dimension info in the record. We need to
> scan the dataset to get that info.
> 2) It saves index:value tuples. Putting indices together can help data
> compression. Same for value if there are many binary features.
>
> Best,
> Xiangrui
>
> On Wed, May 7, 2014 at 10:25 PM, Debasish Das <debasish.das83@gmail.com> wrote:
>> Hi,
>>
>> I see ALS is still using Array[Int] but for other mllib algorithm we moved
>> to Vector[Double] so that it can support either dense and sparse formats...
>>
>> ALS can stay in Array[Int] due to the Netflix format for input datasets
>> which is well defined but it helps if we move ALS to Vector[Double] as
>> well...that way all algorithms will be consistent...
>>
>> The second issue is that toString on SparseVector does not write libsvm
>> format but something not very generic...can we change the
>> SparseVector.toString to write as libsvm output ? I am dumping a sample of
>> dataset to see how mllib glm compares with the glmnet-R package for QoR...
>>
>> Thanks.
>> Deb
>>
>> On Mon, May 5, 2014 at 4:05 PM, David Hall <dlwh@cs.berkeley.edu> wrote:
>>>
>>>> On Mon, May 5, 2014 at 3:40 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>>
>>>> > David,
>>>> >
>>>> > Could we use Int, Long, Float as the data feature spaces, and Double for
>>>> > optimizer?
>>>> >
>>>>
>>>> Yes. Breeze doesn't allow operations on mixed types, so you'd need to
>>>> convert the double vectors to Floats if you wanted, e.g. dot product with
>>>> the weights vector.
>>>>
>>>> You might also be interested in FeatureVector, which is just a wrapper
>>>> around Array[Int] that emulates an indicator vector. It supports dot
>>>> products, axpy, etc.
>>>>
>>>> -- David
>>>>
>>>>
>>>> >
>>>> >
>>>> > Sincerely,
>>>> >
>>>> > DB Tsai
>>>> > -------------------------------------------------------
>>>> > My Blog: https://www.dbtsai.com
>>>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>> >
>>>> >
>>>> > On Mon, May 5, 2014 at 3:06 PM, David Hall <dlwh@cs.berkeley.edu>
>>>> wrote:
>>>> >
>>>> > > Lbfgs and other optimizers would not work immediately, as they require
>>>> > > vector spaces over double. Otherwise it should work.
>>>> > > On May 5, 2014 3:03 PM, "DB Tsai" <dbtsai@stanford.edu> wrote:
>>>> > >
>>>> > > > Breeze could take any type (Int, Long, Double, and Float) in the
>>>> matrix
>>>> > > > template.
>>>> > > >
>>>> > > >
>>>> > > > Sincerely,
>>>> > > >
>>>> > > > DB Tsai
>>>> > > > -------------------------------------------------------
>>>> > > > My Blog: https://www.dbtsai.com
>>>> > > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>> > > >
>>>> > > >
>>>> > > > On Mon, May 5, 2014 at 2:56 PM, Debasish Das <
>>>> debasish.das83@gmail.com
>>>> > > > >wrote:
>>>> > > >
>>>> > > > > Is this a breeze issue or breeze can take templates on float /
>>>> > double ?
>>>> > > > >
>>>> > > > > If breeze can take templates then it is a minor fix for
>>>> Vectors.scala
>>>> > > > right
>>>> > > > > ?
>>>> > > > >
>>>> > > > > Thanks.
>>>> > > > > Deb
>>>> > > > >
>>>> > > > >
>>>> > > > > On Mon, May 5, 2014 at 2:45 PM, DB Tsai <dbtsai@stanford.edu>
>>>> wrote:
>>>> > > > >
>>>> > > > > > +1  Would be nice that we can use different type in Vector.
>>>> > > > > >
>>>> > > > > >
>>>> > > > > > Sincerely,
>>>> > > > > >
>>>> > > > > > DB Tsai
>>>> > > > > > -------------------------------------------------------
>>>> > > > > > My Blog: https://www.dbtsai.com
>>>> > > > > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>> > > > > >
>>>> > > > > >
>>>> > > > > > On Mon, May 5, 2014 at 2:41 PM, Debasish Das <
>>>> > > debasish.das83@gmail.com
>>>> > > > > > >wrote:
>>>> > > > > >
>>>> > > > > > > Hi,
>>>> > > > > > >
>>>> > > > > > > Why mllib vector is using double as default ?
>>>> > > > > > >
>>>> > > > > > > /**
>>>> > > > > > >
>>>> > > > > > >  * Represents a numeric vector, whose index type is Int and
>>>> value
>>>> > > > type
>>>> > > > > is
>>>> > > > > > > Double.
>>>> > > > > > >
>>>> > > > > > >  */
>>>> > > > > > >
>>>> > > > > > > trait Vector extends Serializable {
>>>> > > > > > >
>>>> > > > > > >
>>>> > > > > > >   /**
>>>> > > > > > >
>>>> > > > > > >    * Size of the vector.
>>>> > > > > > >
>>>> > > > > > >    */
>>>> > > > > > >
>>>> > > > > > >   def size: Int
>>>> > > > > > >
>>>> > > > > > >
>>>> > > > > > >   /**
>>>> > > > > > >
>>>> > > > > > >    * Converts the instance to a double array.
>>>> > > > > > >
>>>> > > > > > >    */
>>>> > > > > > >
>>>> > > > > > >   def toArray: Array[Double]
>>>> > > > > > >
>>>> > > > > > > Don't we need a template on float/double ? This will give us
>>>> > memory
>>>> > > > > > > savings...
>>>> > > > > > >
>>>> > > > > > > Thanks.
>>>> > > > > > >
>>>> > > > > > > Deb
>>>> > > > > > >
>>>> > > > > >
>>>> > > > >
>>>> > > >
>>>> > >
>>>> >
>>>>
>>>
>>>

From dev-return-7607-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 19:25:24 2014
Return-Path: <dev-return-7607-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9DDEE11CAE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 19:25:24 +0000 (UTC)
Received: (qmail 92690 invoked by uid 500); 16 May 2014 11:49:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83865 invoked by uid 500); 16 May 2014 11:38:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64344 invoked by uid 99); 16 May 2014 11:19:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 11:19:09 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.212.181] (HELO mail-wi0-f181.google.com) (209.85.212.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 May 2014 17:35:05 +0000
Received: by mail-wi0-f181.google.com with SMTP id n15so4623631wiw.2
        for <dev@spark.apache.org>; Thu, 15 May 2014 10:34:44 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=rsuVSU0jrZRWiO+6ssu0Nhm/P4zUBT9UPgDnFzmIN4M=;
        b=IuaJXr1TI4doeZ3YQG1Hfv6JHmqeF7gdYqqVwyJk5+/ZhNjeRtqjmYAa9+Et2ro6MH
         3GZA5jYQ1PxwQlVhdnxcRx0oFaUbZNEZHmwOM6eX/c/0VMESSpP44BbLrapNdWtm0AeO
         k3xHOzx5HpjlOCFdIbJ8FqY8RsZMrfISTM8a/ln6kNTbfxnASxCL6/8FPeprdtErAX5e
         L1ygZkTrLHnM1UeAzTn7T3CtsyVYYnkFzhJXtg5oAzmmMte4Q63CUhGc55Kea4zShM0Y
         325yyKkQtYVXw6xwU1rkjNg0sUMtG0ggdekFG/7zTjJD5/Egg6/6U2/agoj7EmYDu+i6
         ynqA==
X-Gm-Message-State: ALoCoQnlzID30RyQK3vGUkCgY7DRRQxdqYcWAAeK8HwTXKljWyd+1wkAhggELvGuge8phXcOfoDV
MIME-Version: 1.0
X-Received: by 10.194.157.68 with SMTP id wk4mr9683004wjb.42.1400175284404;
 Thu, 15 May 2014 10:34:44 -0700 (PDT)
Received: by 10.180.88.97 with HTTP; Thu, 15 May 2014 10:34:44 -0700 (PDT)
Date: Thu, 15 May 2014 10:34:44 -0700
Message-ID: <CAMJOb8k_6d_0ixRQz0yFcjt59y42pW+rw_NsJUCHiG7aOmcn2A@mail.gmail.com>
Subject: (test)
From: Andrew Or <andrew@databricks.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0112cbfc7358a804f973b624
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0112cbfc7358a804f973b624
Content-Type: text/plain; charset=UTF-8

Apache has been having some problems lately. Do you guys see this message?

--089e0112cbfc7358a804f973b624--

From dev-return-7616-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 20:09:35 2014
Return-Path: <dev-return-7616-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D278A11F93
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 20:09:35 +0000 (UTC)
Received: (qmail 14859 invoked by uid 500); 16 May 2014 18:38:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9701 invoked by uid 500); 16 May 2014 18:38:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5208 invoked by uid 99); 16 May 2014 18:38:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 18:38:22 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ilikerps@gmail.com designates 209.85.192.53 as permitted sender)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 18:38:17 +0000
Received: by mail-qg0-f53.google.com with SMTP id f51so4848908qge.12
        for <dev@spark.apache.org>; Fri, 16 May 2014 11:37:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=VIaBc/3l7HPIqBJuZDCUUeUVyX1ciy4DOjLqKdpAkok=;
        b=DfsQCp1wNhkMTrtyljmKQXgbHdVDZXMiR/2ZaJblrhdDKPDRr53g92Hxxl3yPu4iVM
         iTuvQcSPky5IFZZIBxlBjKoVexDSvadekPDFdeQ+iZOCp07eG3JV9Qy5ZbZhKtXnPqUW
         XiuS/bXP/nRyGg+0Wl/c131sPeCfrG0rwLy0YKFjc1UWpiQQlQlfH6T+bI4I3Zj1x27J
         Iki0sLY0FStiWlHCpSKOq2d4h/Q2a1IHGVZtxuvbqzNhhQtcd5K3y2FO5EUn3G8e18Ot
         JVRypafaHmHymSDq9JFwBleP94AoNYRRYBjX238mDETWyJI/wM0N68ZBqLq9HUAppJrt
         QQIw==
X-Received: by 10.224.98.141 with SMTP id q13mr26082091qan.64.1400265476300;
 Fri, 16 May 2014 11:37:56 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.31.35 with HTTP; Fri, 16 May 2014 11:37:36 -0700 (PDT)
In-Reply-To: <CALuGr6bpepzhFNV3ery=0067xRO8FP9S2gVZf9SDQ54T652s6g@mail.gmail.com>
References: <CABPQxsu13Y1TnF_NhVZrYQG=DP94hfyvwCaTa7DAGp4zgHGiJg@mail.gmail.com>
 <CABPQxst+WESyBGi-fTdFXjVVmrVkvduE_1Uf6Zh+PbgYPbGCgA@mail.gmail.com> <CALuGr6bpepzhFNV3ery=0067xRO8FP9S2gVZf9SDQ54T652s6g@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Fri, 16 May 2014 11:37:36 -0700
Message-ID: <CANGvG8oFfLM-nXtxUCFQKwRCyuZ0cK+mpCWCG_PyFA32C_5r=Q@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc7)
To: dev@spark.apache.org, Henry Saputra <henry.saputra@gmail.com>
Content-Type: multipart/alternative; boundary=089e0149cc484e669504f988b645
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149cc484e669504f988b645
Content-Type: text/plain; charset=UTF-8

It was, but due to the apache infra issues, some may not have received the
email yet...

On Fri, May 16, 2014 at 10:48 AM, Henry Saputra <henry.saputra@gmail.com>wrote:

> Hi Patrick,
>
> Just want to make sure that VOTE for rc6 also cancelled?
>
>
> Thanks,
>
> Henry
>
> On Thu, May 15, 2014 at 1:15 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > I'll start the voting with a +1.
> >
> > On Thu, May 15, 2014 at 1:14 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >> Please vote on releasing the following candidate as Apache Spark
> version 1.0.0!
> >>
> >> This patch has minor documentation changes and fixes on top of rc6.
> >>
> >> The tag to be voted on is v1.0.0-rc7 (commit 9212b3e):
> >>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=9212b3e5bb5545ccfce242da8d89108e6fb1c464
> >>
> >> The release files, including signatures, digests, etc. can be found at:
> >> http://people.apache.org/~pwendell/spark-1.0.0-rc7/
> >>
> >> Release artifacts are signed with the following key:
> >> https://people.apache.org/keys/committer/pwendell.asc
> >>
> >> The staging repository for this release can be found at:
> >> https://repository.apache.org/content/repositories/orgapachespark-1015
> >>
> >> The documentation corresponding to this release can be found at:
> >> http://people.apache.org/~pwendell/spark-1.0.0-rc7-docs/
> >>
> >> Please vote on releasing this package as Apache Spark 1.0.0!
> >>
> >> The vote is open until Sunday, May 18, at 09:12 UTC and passes if a
> >> majority of at least 3 +1 PMC votes are cast.
> >>
> >> [ ] +1 Release this package as Apache Spark 1.0.0
> >> [ ] -1 Do not release this package because ...
> >>
> >> To learn more about Apache Spark, please see
> >> http://spark.apache.org/
> >>
> >> == API Changes ==
> >> We welcome users to compile Spark applications against 1.0. There are
> >> a few API changes in this release. Here are links to the associated
> >> upgrade guides - user facing changes have been kept as small as
> >> possible.
> >>
> >> changes to ML vector specification:
> >>
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10
> >>
> >> changes to the Java API:
> >>
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
> >>
> >> changes to the streaming API:
> >>
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
> >>
> >> changes to the GraphX API:
> >>
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
> >>
> >> coGroup and related functions now return Iterable[T] instead of Seq[T]
> >> ==> Call toSeq on the result to restore the old behavior
> >>
> >> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> >> ==> Call toSeq on the result to restore old behavior
>

--089e0149cc484e669504f988b645--

From dev-return-7615-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 20:17:15 2014
Return-Path: <dev-return-7615-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 962751105D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 20:17:15 +0000 (UTC)
Received: (qmail 50247 invoked by uid 500); 16 May 2014 18:13:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48476 invoked by uid 500); 16 May 2014 18:13:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36092 invoked by uid 99); 16 May 2014 17:54:25 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 17:54:25 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of henry.saputra@gmail.com designates 74.125.82.52 as permitted sender)
Received: from [74.125.82.52] (HELO mail-wg0-f52.google.com) (74.125.82.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 17:48:54 +0000
Received: by mail-wg0-f52.google.com with SMTP id l18so5313472wgh.35
        for <dev@spark.apache.org>; Fri, 16 May 2014 10:48:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=EIXtBkNsS66ZBYo6kPfFGnvMmbd2F7Z0pVzIMRli1oA=;
        b=ebt2xCn137NZfpHd5hnUrF4mrKRHLNeKaaRiA6Kpvn1xUS/3aExx0GGs/Xv8w/8xMy
         Xdbhm8fTTgBvgsWNbJ6I/UkTRQKX5shLrwEqMIA18c0ZLhJiHOFhgCzpGJ5rtx6ZAXHO
         i6e1RJWXTH69ulRqF7qLdepE580lIUeikaiZPET6bTycvJ4gYL0+OfeT9FcYBs2qo8D3
         +2pz6iAVZp+KRK8eLEbCvhwiS7O94czWHx44dE4h0ra+XGQ2+uP3977lZUr2P3A8pyE5
         IZ8mNkSqbmJTiPnQTX6R3MH12ugRCcgL+WSmyCC3BdFjfGF4vZ0jD9Km3SuH1JYO2chn
         E6cg==
MIME-Version: 1.0
X-Received: by 10.180.228.42 with SMTP id sf10mr37222733wic.33.1400262511287;
 Fri, 16 May 2014 10:48:31 -0700 (PDT)
Received: by 10.216.165.71 with HTTP; Fri, 16 May 2014 10:48:31 -0700 (PDT)
In-Reply-To: <CABPQxst+WESyBGi-fTdFXjVVmrVkvduE_1Uf6Zh+PbgYPbGCgA@mail.gmail.com>
References: <CABPQxsu13Y1TnF_NhVZrYQG=DP94hfyvwCaTa7DAGp4zgHGiJg@mail.gmail.com>
	<CABPQxst+WESyBGi-fTdFXjVVmrVkvduE_1Uf6Zh+PbgYPbGCgA@mail.gmail.com>
Date: Fri, 16 May 2014 10:48:31 -0700
Message-ID: <CALuGr6bpepzhFNV3ery=0067xRO8FP9S2gVZf9SDQ54T652s6g@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc7)
From: Henry Saputra <henry.saputra@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Patrick,

Just want to make sure that VOTE for rc6 also cancelled?


Thanks,

Henry

On Thu, May 15, 2014 at 1:15 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> I'll start the voting with a +1.
>
> On Thu, May 15, 2014 at 1:14 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>> Please vote on releasing the following candidate as Apache Spark version 1.0.0!
>>
>> This patch has minor documentation changes and fixes on top of rc6.
>>
>> The tag to be voted on is v1.0.0-rc7 (commit 9212b3e):
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=9212b3e5bb5545ccfce242da8d89108e6fb1c464
>>
>> The release files, including signatures, digests, etc. can be found at:
>> http://people.apache.org/~pwendell/spark-1.0.0-rc7/
>>
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/pwendell.asc
>>
>> The staging repository for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1015
>>
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~pwendell/spark-1.0.0-rc7-docs/
>>
>> Please vote on releasing this package as Apache Spark 1.0.0!
>>
>> The vote is open until Sunday, May 18, at 09:12 UTC and passes if a
>> majority of at least 3 +1 PMC votes are cast.
>>
>> [ ] +1 Release this package as Apache Spark 1.0.0
>> [ ] -1 Do not release this package because ...
>>
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>>
>> == API Changes ==
>> We welcome users to compile Spark applications against 1.0. There are
>> a few API changes in this release. Here are links to the associated
>> upgrade guides - user facing changes have been kept as small as
>> possible.
>>
>> changes to ML vector specification:
>> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10
>>
>> changes to the Java API:
>> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>>
>> changes to the streaming API:
>> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>>
>> changes to the GraphX API:
>> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>>
>> coGroup and related functions now return Iterable[T] instead of Seq[T]
>> ==> Call toSeq on the result to restore the old behavior
>>
>> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
>> ==> Call toSeq on the result to restore old behavior

From dev-return-7621-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 20:23:36 2014
Return-Path: <dev-return-7621-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 24ECA110F7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 20:23:36 +0000 (UTC)
Received: (qmail 40004 invoked by uid 500); 16 May 2014 19:50:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27662 invoked by uid 500); 16 May 2014 19:49:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22424 invoked by uid 99); 16 May 2014 19:44:08 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 19:44:08 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mark@clearstorydata.com designates 209.85.212.172 as permitted sender)
Received: from [209.85.212.172] (HELO mail-wi0-f172.google.com) (209.85.212.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 19:38:25 +0000
Received: by mail-wi0-f172.google.com with SMTP id hi2so1477251wib.17
        for <dev@spark.apache.org>; Fri, 16 May 2014 12:38:02 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=PTShhGrL+Jhq9kP1sfL5oxG4SHBxHicBWaGnh59x3AE=;
        b=mI2pwL4438wqmKtkn6DCkbe5UWriCnz5jvKpvsdgv6J+ZTTiseVRc6zZyTtw9jCc7/
         grLuV/4aWxP1hOU92jyJZkkomaxB+1a32Lt8UfDFhqu6R3Ym18AF3XRMv+g15bvkCgin
         ulH/vEDlyyeYEWR52k49fXnR85eEJbmXxOrf5ByGi2aj7etvwkcnM+8Bn5+MZh5SZO9s
         aOR+qoQD0QF0vuy7jzhZZ8A/YfmoKaZMzFRmzl67JZpAlmka+sI/kiLRDh7foQoqAAWI
         +kIIh8r9mNYYBKEFGE7x8mMeZLOoXgtF4Z5is0ff2bo+NpU+LeCMOcm2n68V/g1Cj22z
         Am1g==
X-Gm-Message-State: ALoCoQnTHKYEAlY6pxYttkAZmnsGwQHC7Eod7wPhGkSk2lRxrtKZF++D5880Rk+Xj4uL53XfFjWu
MIME-Version: 1.0
X-Received: by 10.180.210.170 with SMTP id mv10mr6425837wic.27.1400269082618;
 Fri, 16 May 2014 12:38:02 -0700 (PDT)
Received: by 10.216.232.69 with HTTP; Fri, 16 May 2014 12:38:02 -0700 (PDT)
In-Reply-To: <CAAsvFPk36+vXkZijCdZGxJHRSJcvPon3BopPNacNxXvDB=FF0Q@mail.gmail.com>
References: <1400258494709-6593.post@n3.nabble.com>
	<CAAsvFPk36+vXkZijCdZGxJHRSJcvPon3BopPNacNxXvDB=FF0Q@mail.gmail.com>
Date: Fri, 16 May 2014 12:38:02 -0700
Message-ID: <CAAsvFP=8e079xR0JrHEQ2tM56t-Ag2Lt8JqthUum0veMcEOd7g@mail.gmail.com>
Subject: Re: Scala examples for Spark do not work as written in documentation
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c37b544287c804f9898df5
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c37b544287c804f9898df5
Content-Type: text/plain; charset=UTF-8

Sorry, looks like an extra line got inserted in there.  One more try:

val count = spark.parallelize(1 to NUM_SAMPLES).map { _ =>
  val x = Math.random()
  val y = Math.random()
  if (x*x + y*y < 1) 1 else 0
}.reduce(_ + _)



On Fri, May 16, 2014 at 12:36 PM, Mark Hamstra <mark@clearstorydata.com>wrote:

> Actually, the better way to write the multi-line closure would be:
>
> val count = spark.parallelize(1 to NUM_SAMPLES).map { _ =>
>
>   val x = Math.random()
>   val y = Math.random()
>   if (x*x + y*y < 1) 1 else 0
> }.reduce(_ + _)
>
>
> On Fri, May 16, 2014 at 9:41 AM, GlennStrycker <glenn.strycker@gmail.com>wrote:
>
>> On the webpage http://spark.apache.org/examples.html, there is an example
>> written as
>>
>> val count = spark.parallelize(1 to NUM_SAMPLES).map(i =>
>>   val x = Math.random()
>>   val y = Math.random()
>>   if (x*x + y*y < 1) 1 else 0
>> ).reduce(_ + _)
>> println("Pi is roughly " + 4.0 * count / NUM_SAMPLES)
>>
>> This does not execute in Spark, which gives me an error:
>> <console>:2: error: illegal start of simple expression
>>          val x = Math.random()
>>          ^
>>
>> If I rewrite the query slightly, adding in {}, it works:
>>
>> val count = spark.parallelize(1 to 10000).map(i =>
>>    {
>>    val x = Math.random()
>>    val y = Math.random()
>>    if (x*x + y*y < 1) 1 else 0
>>    }
>> ).reduce(_ + _)
>> println("Pi is roughly " + 4.0 * count / 10000.0)
>>
>>
>>
>>
>>
>> --
>> View this message in context:
>> http://apache-spark-developers-list.1001551.n3.nabble.com/Scala-examples-for-Spark-do-not-work-as-written-in-documentation-tp6593.html
>> Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com.
>>
>
>

--001a11c37b544287c804f9898df5--

From dev-return-7614-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 20:25:28 2014
Return-Path: <dev-return-7614-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 05C541119B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 20:25:28 +0000 (UTC)
Received: (qmail 46525 invoked by uid 500); 16 May 2014 18:13:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44402 invoked by uid 500); 16 May 2014 18:13:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36126 invoked by uid 99); 16 May 2014 17:54:26 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 17:54:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nravi@cloudera.com designates 209.85.219.50 as permitted sender)
Received: from [209.85.219.50] (HELO mail-oa0-f50.google.com) (209.85.219.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 17:38:48 +0000
Received: by mail-oa0-f50.google.com with SMTP id i7so3379378oag.23
        for <dev@spark.apache.org>; Fri, 16 May 2014 10:38:24 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=ukp5xFL5k79rrnmrzGD6dURdLB2H4o5lFzh9Zyjsp8E=;
        b=fJuWEXe3ppXMLkaGdmv5i7r8VE8Wgb+mKM8SesM6BRV/aaRw2hzl5DOswJQpctM/gg
         1jIJW2ElNfu0ZkEMa9oCRaPSX0W00YmE7PuayENL5ArO1gYnDoxdbT3h0GFMa1Q9UwQq
         cY73todCAa9ZCD32jmBJ2nPqOrlJBTMqiaoUMLJPh0AaCWOJdv1dqgl51AEVUtM2mOyJ
         wqN7USpW2Bjm8TSuPfESOa49QnHkAO0PHTHpwT2/Yt3QF8IInU5H7bdNItK9IdNypSrn
         6ATzj1bMtA+MIh+AvJlx+h2u4Uvw/OG4AT5sbUsfLyzK/ZfJsZNayj00mxw/hpLn97sg
         Jpew==
X-Gm-Message-State: ALoCoQk3PdDcpuiWTe2WLKSF5oxmkPrKH4bD5YrPy+OwpPHv20AynjnAKE1HEsii8tK8GjYTI7fO
X-Received: by 10.60.133.81 with SMTP id pa17mr18949750oeb.35.1400261904676;
 Fri, 16 May 2014 10:38:24 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.182.210.193 with HTTP; Fri, 16 May 2014 10:38:03 -0700 (PDT)
In-Reply-To: <CAEYYnxZBGT6ma=r_5SsJ7Eiv8UVFbqc9fibLSSSWjvGYo+ix+w@mail.gmail.com>
References: <CAMJOb8k_6d_0ixRQz0yFcjt59y42pW+rw_NsJUCHiG7aOmcn2A@mail.gmail.com>
 <CAEYYnxZBGT6ma=r_5SsJ7Eiv8UVFbqc9fibLSSSWjvGYo+ix+w@mail.gmail.com>
From: Nishkam Ravi <nravi@cloudera.com>
Date: Fri, 16 May 2014 10:38:03 -0700
Message-ID: <CACfA1zVEOy=bWceH+26nTX5R23-Fmx8wVHh2u7kJPFcp=yE2SQ@mail.gmail.com>
Subject: Re: (test)
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b4728006bc29604f987e1af
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b4728006bc29604f987e1af
Content-Type: text/plain; charset=UTF-8

Yes.


On Fri, May 16, 2014 at 8:40 AM, DB Tsai <dbtsai@stanford.edu> wrote:

> Yes.
> On May 16, 2014 8:39 AM, "Andrew Or" <andrew@databricks.com> wrote:
>
> > Apache has been having some problems lately. Do you guys see this
> message?
> >
>

--047d7b4728006bc29604f987e1af--

From dev-return-7612-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 20:41:32 2014
Return-Path: <dev-return-7612-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2783511287
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 20:41:32 +0000 (UTC)
Received: (qmail 97313 invoked by uid 500); 16 May 2014 16:33:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72978 invoked by uid 500); 16 May 2014 16:08:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26513 invoked by uid 99); 16 May 2014 15:49:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 15:49:43 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tgraves_cs@yahoo.com designates 98.139.212.166 as permitted sender)
Received: from [98.139.212.166] (HELO nm7.bullet.mail.bf1.yahoo.com) (98.139.212.166)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 15:39:43 +0000
Received: from [66.196.81.170] by nm7.bullet.mail.bf1.yahoo.com with NNFMP; 16 May 2014 15:39:22 -0000
Received: from [98.139.212.236] by tm16.bullet.mail.bf1.yahoo.com with NNFMP; 16 May 2014 15:39:22 -0000
Received: from [127.0.0.1] by omp1045.mail.bf1.yahoo.com with NNFMP; 16 May 2014 15:39:22 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 133218.75745.bm@omp1045.mail.bf1.yahoo.com
Received: (qmail 58216 invoked by uid 60001); 16 May 2014 15:39:21 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.com; s=s1024; t=1400254761; bh=bBmLZWICL22BMXKMjRJUJLW3JAMz2WJibdoZsnEy8Gs=; h=References:Message-ID:Date:From:Reply-To:Subject:To:In-Reply-To:MIME-Version:Content-Type; b=a8RGqYiGSta5MG6QdN1ID2w9GFfRefo+aD8CY2NOybgLPQ4sOpbjBsuUkoA2q5xTmuMrItylQE0CQ5sU4CQVkXhUjQxhjcVt7878Ej9UcYGbDh4k2zS9CGtYHwn1PqQT/6Mu0etV2S+SmWF2Gh3hKJL1qlr6x166CkHPXgY0BMI=
X-YMail-OSG: OI0NGyMVM1l5RHUIPBHP5K0JqRDWk.ognnQFTFE2cVfm0v2
 T__0o64JcM7DkU2d7dmKLn2lNsqs.B4xf2SNvx3uB_Tmc7P87sdkLvCY6XRC
 4Z3AShbMzDdKU7_RlAFAwO_ACUl5luqtR_dYmfTVgwfIsgmlxAB2VqZ1r0si
 bVDHvZ3upyN5enm.WaRHoFKvkpebGWqqiRQaBGlD75ZNp7k55lwNP2x3dHbM
 ZGdN6kfqi.GMvz.OBziyr_p5jufInyrza0_568OTiFKQe8F0Y7043bEQ.wW3
 KQGZ0vkak7nTp54wYX4hZVIcy1SE9Umj.zFt.mIbwmbSsBBtSBOsgkXOBgJv
 5xinnYHH.gMbZT6xHf6EMX8WiXkKppPhQXGRL6RzPL9YYcl4ZL8VpczbcNV9
 xtK_j_biiU8vqb7hDqgr3Kn0WdE6R.xRe15683gPcG.4wBp5gcZgRT29sF20
 HplSYmzOXM97hzd7Ki_1KwE2vOK5rdi8u8dErljgApIWyK8_zSgqb_dRkFjL
 BRJrO6CHea98I32Qop_2mp7GMwzm4aPLtADLPQ97._gXZi969V5GQ6NmNtS.
 lahw9xc5VVaPPndjgUEDLkWvYfjPIk9JTYTAaxr4w1s3aW5PTfN6kYwbsGg-
 -
Received: from [204.11.79.50] by web140102.mail.bf1.yahoo.com via HTTP; Fri, 16 May 2014 08:39:21 PDT
X-Rocket-MIMEInfo: 002.001,WWVzLCByYzUgYW5kIHJjNiB3ZXJlIGNhbmNlbGxlZC4gVGhlcmUgaXMgbm93IGFuIHJjNy4gwqAgVW5mb3J0dW5hdGVseSB0aGUgQXBhY2hlIG1haWxpbmcgbGlzdCBpc3N1ZSBoYXMgY2F1c2VkIGxvdHMgb2YgZW1haWxzIG5vdCB0byBjb21lIHRocm91Z2guCgpIZXJlIGlzIHRoZSBkZXRhaWxzIChob3BlZnVsbHkgaXQgZ29lcyB0aHJvdWdoKToKClBsZWFzZSB2b3RlIG9uIHJlbGVhc2luZyB0aGUgZm9sbG93aW5nIGNhbmRpZGF0ZSBhcyBBcGFjaGUgU3BhcmsgdmVyc2lvbiAxLjAuMCEKClRoaXMgcGF0Y2gBMAEBAQE-
X-Mailer: YahooMailWebService/0.8.188.663
References: <CABPQxst79373EAcpmFemjukod+9veP_F=EeRZWch84_=fo1xTQ@mail.gmail.com> <CAJiQeYLxzKRwuH8qvmHR10DtkzboHQyj_hDE1_-CXum3CwzRTg@mail.gmail.com> 
Message-ID: <1400254761.4737.YahooMailNeo@web140102.mail.bf1.yahoo.com>
Date: Fri, 16 May 2014 08:39:21 -0700 (PDT)
From: Tom Graves <tgraves_cs@yahoo.com>
Reply-To: Tom Graves <tgraves_cs@yahoo.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc6)
To: "dev@spark.apache.org" <dev@spark.apache.org>
In-Reply-To: <CAJiQeYLxzKRwuH8qvmHR10DtkzboHQyj_hDE1_-CXum3CwzRTg@mail.gmail.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="-559291896-1169171447-1400254761=:4737"
X-Virus-Checked: Checked by ClamAV on apache.org

---559291896-1169171447-1400254761=:4737
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

Yes, rc5 and rc6 were cancelled. There is now an rc7. =A0 Unfortunately the=
 Apache mailing list issue has caused lots of emails not to come through.=
=0A=0AHere is the details (hopefully it goes through):=0A=0APlease vote on =
releasing the following candidate as Apache Spark version 1.0.0!=0A=0AThis =
patch has minor documentation changes and fixes on top of rc6.=0A=0AThe tag=
 to be voted on is v1.0.0-rc7 (commit 9212b3e):=0Ahttps://git-wip-us.apache=
.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D9212b3e5bb5545ccfce242da8d89108=
e6fb1c464=0A=0AThe release files, including signatures, digests, etc. can b=
e found at:=0Ahttp://people.apache.org/~pwendell/spark-1.0.0-rc7/=0A=0ARele=
ase artifacts are signed with the following key:=0Ahttps://people.apache.or=
g/keys/committer/pwendell.asc=0A=0AThe staging repository for this release =
can be found at:=0Ahttps://repository.apache.org/content/repositories/orgap=
achespark-1015=0A=0AThe documentation corresponding to this release can be =
found at:=0Ahttp://people.apache.org/~pwendell/spark-1.0.0-rc7-docs/=0A=0AP=
lease vote on releasing this package as Apache Spark 1.0.0!=0A=0AThe vote i=
s open until Sunday, May 18, at 09:12 UTC and passes if a=0Amajority of at =
least 3 +1 PMC votes are cast.=0A=0A[ ] +1 Release this package as Apache S=
park 1.0.0=0A[ ] -1 Do not release this package because ...=0A=0ATo learn m=
ore about Apache Spark, please see=0Ahttp://spark.apache.org/=0A=0A=3D=3D A=
PI Changes =3D=3D=0AWe welcome users to compile Spark applications against =
1.0. There are=0Aa few API changes in this release. Here are links to the a=
ssociated=0Aupgrade guides - user facing changes have been kept as small as=
=0Apossible.=0A=0Achanges to ML vector specification:=0Ahttp://people.apach=
e.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10=0A=0Ach=
anges to the Java API:=0Ahttp://people.apache.org/~pwendell/spark-1.0.0-rc5=
-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark=
=0A=0Achanges to the streaming API:=0Ahttp://people.apache.org/~pwendell/sp=
ark-1.0.0-rc5-docs/streaming-programming-guide.html#migration-guide-from-09=
1-or-below-to-1x=0A=0Achanges to the GraphX API:=0Ahttp://people.apache.org=
/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-guide.html#upgrade-guide=
-from-spark-091=0A=0AcoGroup and related functions now return Iterable[T] i=
nstead of Seq[T]=0A=3D=3D> Call toSeq on the result to restore the old beha=
vior=0A=0ASparkContext.jarOfClass returns Option[String] instead of Seq[Str=
ing]=0A=3D=3D> Call toSeq on the result to restore old behavior=0A=0A=0ATom=
=0A=0A=0AOn Friday, May 16, 2014 10:22 AM, Mridul Muralidharan <mridul@gmai=
l.com> wrote:=0A =0A=0A=0ASo was rc5 cancelled ? Did not see a note indicat=
ing that or why ... [1]=0A=0A- Mridul=0A=0A=0A[1] could have easily missed =
it in the email storm though !=0A=0A=0AOn Thu, May 15, 2014 at 1:32 AM, Pat=
rick Wendell <pwendell@gmail.com> wrote:=0A> Please vote on releasing the f=
ollowing candidate as Apache Spark version=0A 1.0.0!=0A>=0A> This patch has=
 a few minor fixes on top of rc5. I've also built the=0A> binary artifacts =
with Hive support enabled so people can test this=0A> configuration. When w=
e release 1.0 we might just release both vanilla=0A> and Hive-enabled binar=
ies.=0A>=0A> The tag to be voted on is v1.0.0-rc6 (commit 54133a):=0A> http=
s://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D54133abdce=
0246f6643a1112a5204afb2c4caa82=0A>=0A> The release files, including signatu=
res, digests, etc. can be found at:=0A> http://people.apache.org/~pwendell/=
spark-1.0.0-rc6/=0A>=0A> Release artifacts are signed with the following ke=
y:=0A> https://people.apache.org/keys/committer/pwendell.asc=0A>=0A> The st=
aging repository for this release can be found at:=0A> https://repository.a=
pache.org/content/repositories/orgapachestratos-1011=0A>=0A> The documentat=
ion corresponding to this release can be found at:=0A> http://people.apache=
.org/~pwendell/spark-1.0.0-rc6-docs/=0A>=0A> Please vote on releasing this =
package as Apache Spark 1.0.0!=0A>=0A> The vote is open until Saturday, May=
 17, at 20:58 UTC and passes if=0A> amajority of at least 3 +1 PMC votes ar=
e cast.=0A>=0A> [ ] +1 Release this package as Apache Spark 1.0.0=0A> [ ] -=
1 Do not release this package because ...=0A>=0A> To learn more about Apach=
e Spark, please see=0A> http://spark.apache.org/=0A>=0A> =3D=3D API Changes=
 =3D=3D=0A> We welcome users to compile Spark applications against 1.0. The=
re are=0A> a few API changes in this release. Here are links to the associa=
ted=0A> upgrade guides - user facing changes have been kept as small as=0A>=
=0A possible.=0A>=0A> changes to ML vector specification:=0A> http://people=
.apache.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10=
=0A>=0A> changes to the Java API:=0A> http://people.apache.org/~pwendell/sp=
ark-1.0.0-rc5-docs/java-programming-guide.html#upgrading-from-pre-10-versio=
ns-of-spark=0A>=0A> changes to the streaming API:=0A> http://people.apache.=
org/~pwendell/spark-1.0.0-rc5-docs/streaming-programming-guide.html#migrati=
on-guide-from-091-or-below-to-1x=0A>=0A> changes to the GraphX API:=0A> htt=
p://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-gui=
de.html#upgrade-guide-from-spark-091=0A>=0A> coGroup and related functions =
now return Iterable[T] instead of Seq[T]=0A> =3D=3D> Call toSeq on the resu=
lt to restore the old behavior=0A>=0A> SparkContext.jarOfClass returns Opti=
on[String] instead of Seq[String]=0A> =3D=3D> Call toSeq on the result to r=
estore old behavior
---559291896-1169171447-1400254761=:4737--

From dev-return-7617-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 20:46:28 2014
Return-Path: <dev-return-7617-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 43C2111301
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 20:46:28 +0000 (UTC)
Received: (qmail 76007 invoked by uid 500); 16 May 2014 19:24:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54437 invoked by uid 500); 16 May 2014 19:03:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10132 invoked by uid 99); 16 May 2014 18:58:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 18:58:46 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 18:58:42 +0000
Received: by mail-qg0-f53.google.com with SMTP id f51so4889411qge.12
        for <dev@spark.apache.org>; Fri, 16 May 2014 11:58:21 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=4x2CIsu0h7G7RA/B5ctfEJnKNRcQWwgZQuwr9NMVJ/E=;
        b=R5nABXa4mKEkXrijfeEsSWU19XTBlJl/7ZMjoRo20o6cd4NZAlXY3YuFJr4mxsoXBb
         w+IJwc4hgQtNXYJaG8vUA9T11/JSwhMz5OjbGs99y23Wr7UZ97I3tT7DEVNTDhXxIS/Z
         bAaKQC+Vo/2LhZ9zomXTejRBtqHIS9VtUwz1fuS7U7xsexrvyLeN/EQaPw58wfII9xtU
         t6qo2Sa/2EvI2Og5FJdVLmTZUaZ8hsHHlk0zXBcK6OWa+1r8+G7EYzK01t8DRj5tjKlb
         wvBwPgrAmw7SAylmqUSPCXWEJJArs7W19yCfkyCQhcQdgKet35DDbrYpKNgJDdhFtPDr
         1JKA==
X-Gm-Message-State: ALoCoQlHBionFSw8CZPkrz49td5I7okmCiQ+AvzXGE7lIHN//eI4zZk34TgwSPIFE4FMBx8uNyI6
X-Received: by 10.224.151.82 with SMTP id b18mr26070546qaw.27.1400266701354;
 Fri, 16 May 2014 11:58:21 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.126.1 with HTTP; Fri, 16 May 2014 11:58:01 -0700 (PDT)
In-Reply-To: <1400258494709-6593.post@n3.nabble.com>
References: <1400258494709-6593.post@n3.nabble.com>
From: Reynold Xin <rxin@databricks.com>
Date: Fri, 16 May 2014 11:58:01 -0700
Message-ID: <CAPh_B=b=7aOdLrXLeifpDGAB8eZaKF=272vW+E70TZaAN6GsHA@mail.gmail.com>
Subject: Re: Scala examples for Spark do not work as written in documentation
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0149ced6534cb704f988ff03
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149ced6534cb704f988ff03
Content-Type: text/plain; charset=UTF-8

Thanks for pointing it out. We should update the website to fix the code.

val count = spark.parallelize(1 to NUM_SAMPLES).map { i =>
  val x = Math.random()
  val y = Math.random()
  if (x*x + y*y < 1) 1 else 0
}.reduce(_ + _)
println("Pi is roughly " + 4.0 * count / NUM_SAMPLES)



On Fri, May 16, 2014 at 9:41 AM, GlennStrycker <glenn.strycker@gmail.com>wrote:

> On the webpage http://spark.apache.org/examples.html, there is an example
> written as
>
> val count = spark.parallelize(1 to NUM_SAMPLES).map(i =>
>   val x = Math.random()
>   val y = Math.random()
>   if (x*x + y*y < 1) 1 else 0
> ).reduce(_ + _)
> println("Pi is roughly " + 4.0 * count / NUM_SAMPLES)
>
> This does not execute in Spark, which gives me an error:
> <console>:2: error: illegal start of simple expression
>          val x = Math.random()
>          ^
>
> If I rewrite the query slightly, adding in {}, it works:
>
> val count = spark.parallelize(1 to 10000).map(i =>
>    {
>    val x = Math.random()
>    val y = Math.random()
>    if (x*x + y*y < 1) 1 else 0
>    }
> ).reduce(_ + _)
> println("Pi is roughly " + 4.0 * count / 10000.0)
>
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Scala-examples-for-Spark-do-not-work-as-written-in-documentation-tp6593.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--089e0149ced6534cb704f988ff03--

From dev-return-7623-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 21:13:43 2014
Return-Path: <dev-return-7623-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 005A71176E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 21:13:43 +0000 (UTC)
Received: (qmail 94224 invoked by uid 500); 16 May 2014 20:40:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60739 invoked by uid 500); 16 May 2014 20:16:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60389 invoked by uid 99); 16 May 2014 20:10:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 20:10:47 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mark@clearstorydata.com designates 209.85.212.180 as permitted sender)
Received: from [209.85.212.180] (HELO mail-wi0-f180.google.com) (209.85.212.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 20:10:37 +0000
Received: by mail-wi0-f180.google.com with SMTP id hi2so1542030wib.7
        for <dev@spark.apache.org>; Fri, 16 May 2014 13:10:15 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=4BmwK+BUfKHCdKEW1pEGP0xEnuHuhLxlbeRuQySNIxs=;
        b=NKXpBKFOHATjQQClw+6rQkp8Qw4dLpiMyUjKS8tkyfps5srXL6v++G/NeTd4RwJFG9
         yi+QpA1BOU2A4cYWBCPHFRMrIga5LAk0AfJ0/jg/86XBFv+R/1AnZFT1My7KGXJBhrev
         CA45sHDyXvfSVDWMDFoSAV7f5kTckJ0ZabID+iGW/LYMEQSOh2E3eQdRcn5NBRAaB9B/
         eZHYZA1iWLdiKjHMP3By401JsDNhSS5zjZ4w2Ju2gJpiEb9YL0alGTTGsohfH2vvCdPS
         g2kIBiS8P38r6KqVW5JZRuUjZmCO56y1EmX2f+vwKmJg9wDgA6LP78nnxW+jSpJK1vrq
         B6dQ==
X-Gm-Message-State: ALoCoQn/4QnOkohMAT5Ev7Xaedcwv6lQPTQazvBLdY6TrxxGsgAL5uH7ooWKHbJOv6K4XVpYum0Y
MIME-Version: 1.0
X-Received: by 10.194.242.66 with SMTP id wo2mr15624001wjc.37.1400271015578;
 Fri, 16 May 2014 13:10:15 -0700 (PDT)
Received: by 10.216.232.69 with HTTP; Fri, 16 May 2014 13:10:15 -0700 (PDT)
In-Reply-To: <CALuGr6b6HbRd34vN4=uvWDKj-UKJMUs6seJUo2u0J-OgMomdyg@mail.gmail.com>
References: <CABPQxsu13Y1TnF_NhVZrYQG=DP94hfyvwCaTa7DAGp4zgHGiJg@mail.gmail.com>
	<CABPQxst+WESyBGi-fTdFXjVVmrVkvduE_1Uf6Zh+PbgYPbGCgA@mail.gmail.com>
	<CALuGr6bpepzhFNV3ery=0067xRO8FP9S2gVZf9SDQ54T652s6g@mail.gmail.com>
	<CANGvG8oFfLM-nXtxUCFQKwRCyuZ0cK+mpCWCG_PyFA32C_5r=Q@mail.gmail.com>
	<CALuGr6b6HbRd34vN4=uvWDKj-UKJMUs6seJUo2u0J-OgMomdyg@mail.gmail.com>
Date: Fri, 16 May 2014 13:10:15 -0700
Message-ID: <CAAsvFPmdYk63D+UmAZ7rw9SKA7rsMGzqDBryPrz=_VF=5MGkRw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc7)
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e013d13ba79379204f98a00ab
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013d13ba79379204f98a00ab
Content-Type: text/plain; charset=UTF-8

Sorry for the duplication, but I think this is the current VOTE candidate
-- we're not voting on rc8 yet?

+1, but just barely.  We've got quite a number of outstanding bugs
identified, and many of them have fixes in progress.  I'd hate to see those
efforts get lost in a post-1.0.0 flood of new features targeted at 1.1.0 --
in other words, I'd like to see 1.0.1 retain a high priority relative to
1.1.0.

Looking through the unresolved JIRAs, it doesn't look like any of the
identified bugs are show-stoppers or strictly regressions (although I will
note that one that I have in progress, SPARK-1749, is a bug that we
introduced with recent work -- it's not strictly a regression because we
had equally bad but different behavior when the DAGScheduler exceptions
weren't previously being handled at all vs. being slightly mis-handled
now), so I'm not currently seeing a reason not to release.


On Fri, May 16, 2014 at 11:42 AM, Henry Saputra <henry.saputra@gmail.com>wrote:

> Ah ok, thanks Aaron
>
> Just to make sure we VOTE the right RC.
>
> Thanks,
>
> Henry
>
> On Fri, May 16, 2014 at 11:37 AM, Aaron Davidson <ilikerps@gmail.com>
> wrote:
> > It was, but due to the apache infra issues, some may not have received
> the
> > email yet...
> >
> > On Fri, May 16, 2014 at 10:48 AM, Henry Saputra <henry.saputra@gmail.com
> >
> > wrote:
> >>
> >> Hi Patrick,
> >>
> >> Just want to make sure that VOTE for rc6 also cancelled?
> >>
> >>
> >> Thanks,
> >>
> >> Henry
> >>
> >> On Thu, May 15, 2014 at 1:15 AM, Patrick Wendell <pwendell@gmail.com>
> >> wrote:
> >> > I'll start the voting with a +1.
> >> >
> >> > On Thu, May 15, 2014 at 1:14 AM, Patrick Wendell <pwendell@gmail.com>
> >> > wrote:
> >> >> Please vote on releasing the following candidate as Apache Spark
> >> >> version 1.0.0!
> >> >>
> >> >> This patch has minor documentation changes and fixes on top of rc6.
> >> >>
> >> >> The tag to be voted on is v1.0.0-rc7 (commit 9212b3e):
> >> >>
> >> >>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=9212b3e5bb5545ccfce242da8d89108e6fb1c464
> >> >>
> >> >> The release files, including signatures, digests, etc. can be found
> at:
> >> >> http://people.apache.org/~pwendell/spark-1.0.0-rc7/
> >> >>
> >> >> Release artifacts are signed with the following key:
> >> >> https://people.apache.org/keys/committer/pwendell.asc
> >> >>
> >> >> The staging repository for this release can be found at:
> >> >>
> https://repository.apache.org/content/repositories/orgapachespark-1015
> >> >>
> >> >> The documentation corresponding to this release can be found at:
> >> >> http://people.apache.org/~pwendell/spark-1.0.0-rc7-docs/
> >> >>
> >> >> Please vote on releasing this package as Apache Spark 1.0.0!
> >> >>
> >> >> The vote is open until Sunday, May 18, at 09:12 UTC and passes if a
> >> >> majority of at least 3 +1 PMC votes are cast.
> >> >>
> >> >> [ ] +1 Release this package as Apache Spark 1.0.0
> >> >> [ ] -1 Do not release this package because ...
> >> >>
> >> >> To learn more about Apache Spark, please see
> >> >> http://spark.apache.org/
> >> >>
> >> >> == API Changes ==
> >> >> We welcome users to compile Spark applications against 1.0. There are
> >> >> a few API changes in this release. Here are links to the associated
> >> >> upgrade guides - user facing changes have been kept as small as
> >> >> possible.
> >> >>
> >> >> changes to ML vector specification:
> >> >>
> >> >>
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10
> >> >>
> >> >> changes to the Java API:
> >> >>
> >> >>
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
> >> >>
> >> >> changes to the streaming API:
> >> >>
> >> >>
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
> >> >>
> >> >> changes to the GraphX API:
> >> >>
> >> >>
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
> >> >>
> >> >> coGroup and related functions now return Iterable[T] instead of
> Seq[T]
> >> >> ==> Call toSeq on the result to restore the old behavior
> >> >>
> >> >> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> >> >> ==> Call toSeq on the result to restore old behavior
> >
> >
>

--089e013d13ba79379204f98a00ab--

From dev-return-7622-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 21:13:44 2014
Return-Path: <dev-return-7622-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E035F11771
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 21:13:43 +0000 (UTC)
Received: (qmail 4150 invoked by uid 500); 16 May 2014 20:14:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37457 invoked by uid 500); 16 May 2014 19:50:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31333 invoked by uid 99); 16 May 2014 19:37:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 19:37:05 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mark@clearstorydata.com designates 74.125.82.52 as permitted sender)
Received: from [74.125.82.52] (HELO mail-wg0-f52.google.com) (74.125.82.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 19:37:00 +0000
Received: by mail-wg0-f52.google.com with SMTP id l18so5431027wgh.35
        for <dev@spark.apache.org>; Fri, 16 May 2014 12:36:38 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=uzspb3E3F1ueEkSQSdyO0925QnUzdLWbcQGhDTBvhvE=;
        b=i/aacUele12o5rtPn4yqWwSswx+760X2ReXLkqPM8mQIptP2M5RuxhOBDct+ACl+60
         ZFeEz9I3FBK6wM5J0O9fAbidT23X9be4SqwD+lLaNzDeYgAV/xyQAqRCP4f2T6H9u7GQ
         5XHjNfsIzYdTLh4EZE/SqVbEX9YmVvB1nTKLcTFk+BSND+TNwICL17tSwTIYNaAtY9nD
         TzgrykABe1HVu2XuWCGyO29ZhfzbeSENr1maTaKFt47HftBvoepGETHhVRBIBL1iygZg
         a0k3aBNRPQdQ7NoDJwFKqvdE0cP9Mp0c6oxtaP/eWwVBbZEe6jXhTipeC8RKGJ0nEwEH
         oRlg==
X-Gm-Message-State: ALoCoQl5dTNGbrInXAhhjmRvqG86cs8HuOlkcH53OjCjvY9HEpU3VC9AB5pel4+UARY4RoCO73CV
MIME-Version: 1.0
X-Received: by 10.180.76.179 with SMTP id l19mr15385689wiw.43.1400268998582;
 Fri, 16 May 2014 12:36:38 -0700 (PDT)
Received: by 10.216.232.69 with HTTP; Fri, 16 May 2014 12:36:38 -0700 (PDT)
In-Reply-To: <1400258494709-6593.post@n3.nabble.com>
References: <1400258494709-6593.post@n3.nabble.com>
Date: Fri, 16 May 2014 12:36:38 -0700
Message-ID: <CAAsvFPk36+vXkZijCdZGxJHRSJcvPon3BopPNacNxXvDB=FF0Q@mail.gmail.com>
Subject: Re: Scala examples for Spark do not work as written in documentation
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d0438938b4041b504f98988b4
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0438938b4041b504f98988b4
Content-Type: text/plain; charset=UTF-8

Actually, the better way to write the multi-line closure would be:

val count = spark.parallelize(1 to NUM_SAMPLES).map { _ =>
  val x = Math.random()
  val y = Math.random()
  if (x*x + y*y < 1) 1 else 0
}.reduce(_ + _)


On Fri, May 16, 2014 at 9:41 AM, GlennStrycker <glenn.strycker@gmail.com>wrote:

> On the webpage http://spark.apache.org/examples.html, there is an example
> written as
>
> val count = spark.parallelize(1 to NUM_SAMPLES).map(i =>
>   val x = Math.random()
>   val y = Math.random()
>   if (x*x + y*y < 1) 1 else 0
> ).reduce(_ + _)
> println("Pi is roughly " + 4.0 * count / NUM_SAMPLES)
>
> This does not execute in Spark, which gives me an error:
> <console>:2: error: illegal start of simple expression
>          val x = Math.random()
>          ^
>
> If I rewrite the query slightly, adding in {}, it works:
>
> val count = spark.parallelize(1 to 10000).map(i =>
>    {
>    val x = Math.random()
>    val y = Math.random()
>    if (x*x + y*y < 1) 1 else 0
>    }
> ).reduce(_ + _)
> println("Pi is roughly " + 4.0 * count / 10000.0)
>
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Scala-examples-for-Spark-do-not-work-as-written-in-documentation-tp6593.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--f46d0438938b4041b504f98988b4--

From dev-return-7624-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 21:57:06 2014
Return-Path: <dev-return-7624-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 133D6119B6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 21:57:06 +0000 (UTC)
Received: (qmail 89567 invoked by uid 500); 16 May 2014 20:39:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80089 invoked by uid 500); 16 May 2014 20:39:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56981 invoked by uid 99); 16 May 2014 20:28:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 20:28:46 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of glenn.strycker@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 20:28:42 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <glenn.strycker@gmail.com>)
	id 1WlOjt-00034L-KI
	for dev@spark.incubator.apache.org; Fri, 16 May 2014 13:28:21 -0700
Date: Fri, 16 May 2014 13:28:21 -0700 (PDT)
From: GlennStrycker <glenn.strycker@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1400272101615-6606.post@n3.nabble.com>
Subject: reduce only removes duplicates, cannot be arbitrary function
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I am attempting to write a mapreduce job on a graph object to take an edge
list and return a new edge list.  Unfortunately I find that the current
function is

def reduce(f: (T, T) => T): T

not

def reduce(f: (T1, T2) => T3): T


I see this because the following 2 commands give different results for the
final number, which should be the same (tempMappedRDD is a MappedRDD of the
form (Edge,1), and I found the the A and B here are (1,4) and (7,3) )

tempMappedRDD.reduce( (A,B) => (Edge(A._1.srcId, A._1.dstId,
A._1.dstId.toInt), 1) )  // (Edge(1,4,4),1)
tempMappedRDD.reduce( (A,B) => (Edge(A._1.srcId, B._1.dstId,
A._1.dstId.toInt), 1) )  // (Edge(1,3,3),1)

why is the 3rd digit above a '3' in the second line, and not a '4'?  Does it
have something to do with toInt?

the really weird thing is that it is only for A, since the following
commands work correctly:

tempMappedRDD.reduce( (A,B) => (Edge(B._1.srcId, B._1.dstId,
B._1.dstId.toInt), 1) )  // (Edge(7,3,3),1)
tempMappedRDD.reduce( (A,B) => (Edge(B._1.srcId, A._1.dstId,
B._1.dstId.toInt), 1) )  // (Edge(7,4,3),1)




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/reduce-only-removes-duplicates-cannot-be-arbitrary-function-tp6606.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7603-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 22:18:53 2014
Return-Path: <dev-return-7603-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 71EDE11E85
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 22:18:53 +0000 (UTC)
Received: (qmail 93409 invoked by uid 500); 16 May 2014 11:39:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9278 invoked by uid 500); 16 May 2014 11:23:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46962 invoked by uid 99); 16 May 2014 11:14:44 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 11:14:44 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.41 as permitted sender)
Received: from [209.85.219.41] (HELO mail-oa0-f41.google.com) (209.85.219.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 May 2014 17:23:36 +0000
Received: by mail-oa0-f41.google.com with SMTP id m1so1668279oag.28
        for <dev@spark.apache.org>; Thu, 15 May 2014 10:23:13 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=nGGZcr/S9NCty1iSWy2SjpnJuWhjwAcqfTMceVB7a70=;
        b=po7DmvWdapB5mNMwZjaQAM5SuKHECwZ6AeirWVIIk5NxsUAyfl7GcsPVhoElbu+9/K
         XrEMvjDh3BfIscUJ0fXdRZ+svTnmgdfiTB3gKdefIvsIi9W4GRd+aTjQQrJsm1mLpsCO
         19AiitixAdkovKwEj+m88cD4Vz2kL6fsx7qnbmzjfWpFTDP4PK7TAjNikScxaGczxys0
         yER4RGhXSapR1riBpTMtPapZekB/+nA9CXamCc0LSSayuswdnTJisGZFxiVVpDJXJ/l6
         vWxhJsDvM1eIn0EuQTTAEmVSaahAtGk2sM5jdOTHkxBiK75ZbW3sqysPCck6rCfSNO4T
         GMwA==
MIME-Version: 1.0
X-Received: by 10.182.163.45 with SMTP id yf13mr11588543obb.66.1400174592843;
 Thu, 15 May 2014 10:23:12 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Thu, 15 May 2014 10:23:12 -0700 (PDT)
In-Reply-To: <tencent_4CAB53D66DED27DB54B1317C@qq.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com>
	<CAMtqZeeAW6BeCiCfHbiCmKfMh=BnrO=+UM1oPe1VNgB2disQQw@mail.gmail.com>
	<CABPQxssAXupM4faQD1zm5x4yObufMa7dq1SpU3dwciQV671sNQ@mail.gmail.com>
	<tencent_5EABC2696C9E4E8610F1CE38@qq.com>
	<CABPQxstBPL6goZaR1F3pwZR=04yS8nCq9hxNC7Yq=yT1j8vEvQ@mail.gmail.com>
	<tencent_4CAB53D66DED27DB54B1317C@qq.com>
Date: Thu, 15 May 2014 10:23:12 -0700
Message-ID: <CABPQxsvVOkU_XiKdFe+D2DP4gA_MjGP+SohjbUnB8ptQrYQ=FA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks for your feedback. Since it's not a regression, it won't block
the release.

On Wed, May 14, 2014 at 12:17 AM, witgo <witgo@qq.com> wrote:
> SPARK-1817 will cause users to get incorrect results  and RDD.zip is common usage .
> This should be the highest priority. I think we should fix the bug,and should also test the previous release
> ------------------ Original ------------------
> From:  "Patrick Wendell";<pwendell@gmail.com>;
> Date:  Wed, May 14, 2014 03:02 PM
> To:  "dev@spark.apache.org"<dev@spark.apache.org>;
>
> Subject:  Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
>
>
>
> Hey @witgo - those bugs are not severe enough to block the release,
> but it would be nice to get them fixed.
>
> At this point we are focused on severe bugs with an immediate fix, or
> regressions from previous versions of Spark. Anything that misses this
> release will get merged into the branch-1.0 branch and make it into
> the 1.0.1 release, so people will have access to it.
>
> On Tue, May 13, 2014 at 5:32 PM, witgo <witgo@qq.com> wrote:
>> -1
>> The following bug should be fixed:
>> https://issues.apache.org/jira/browse/SPARK-1817
>> https://issues.apache.org/jira/browse/SPARK-1712
>>
>>
>> ------------------ Original ------------------
>> From:  "Patrick Wendell";<pwendell@gmail.com>;
>> Date:  Wed, May 14, 2014 04:07 AM
>> To:  "dev@spark.apache.org"<dev@spark.apache.org>;
>>
>> Subject:  Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
>>
>>
>>
>> Hey all - there were some earlier RC's that were not presented to the
>> dev list because issues were found with them. Also, there seems to be
>> some issues with the reliability of the dev list e-mail. Just a heads
>> up.
>>
>> I'll lead with a +1 for this.
>>
>> On Tue, May 13, 2014 at 8:07 AM, Nan Zhu <zhunanmcgill@gmail.com> wrote:
>>> just curious, where is rc4 VOTE?
>>>
>>> I searched my gmail but didn't find that?
>>>
>>>
>>>
>>>
>>> On Tue, May 13, 2014 at 9:49 AM, Sean Owen <sowen@cloudera.com> wrote:
>>>
>>>> On Tue, May 13, 2014 at 9:36 AM, Patrick Wendell <pwendell@gmail.com>
>>>> wrote:
>>>> > The release files, including signatures, digests, etc. can be found at:
>>>> > http://people.apache.org/~pwendell/spark-1.0.0-rc5/
>>>>
>>>> Good news is that the sigs, MD5 and SHA are all correct.
>>>>
>>>> Tiny note: the Maven artifacts use SHA1, while the binary artifacts
>>>> use SHA512, which took me a bit of head-scratching to figure out.
>>>>
>>>> If another RC comes out, I might suggest making it SHA1 everywhere?
>>>> But there is nothing wrong with these signatures and checksums.
>>>>
>>>> Now to look at the contents...
>>>>
>> .
> .

From dev-return-7609-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 22:38:46 2014
Return-Path: <dev-return-7609-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DCEFE11FEF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 22:38:46 +0000 (UTC)
Received: (qmail 21206 invoked by uid 500); 16 May 2014 11:53:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36703 invoked by uid 500); 16 May 2014 11:44:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62944 invoked by uid 99); 16 May 2014 11:19:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 11:19:01 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.44 as permitted sender)
Received: from [209.85.219.44] (HELO mail-oa0-f44.google.com) (209.85.219.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 May 2014 08:14:46 +0000
Received: by mail-oa0-f44.google.com with SMTP id o6so840676oag.31
        for <dev@spark.apache.org>; Thu, 15 May 2014 01:14:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=XeDnVH/45UeJ9wo5y4TdrfHQ1AnTLjfCpxdZFBZTCnM=;
        b=Am33cZqCIN2ByTnUoRu3MOPDZ7i2dU/qNlc2bE0irTsUsus9PzIAGCPRjHbMo2VyB2
         60baifgGjUxCJoVel4dGvxwPcP861lXmw3qmR9EueTqZBTChAlpNQ1eUJL7AKeSLVlNw
         pZSmeZhPF/3FAZy9j0I0TfF3tyllrCAA11ihpwlhr/DuZyXUfpXEE8nWJExq/rYaJIl/
         nurVLPjRxXuG6ao8oMjw4MDs5bMmIXzRw+I1mspKq7CmfVEDaXeMcqfrBWM5Ar8KvvIq
         r8bQ9vLD+3wItzcds6i8+nxr59J3XfqsNM5ZpLgybYAZ7dc1iu0cUP4vrpnogwCm2w/1
         Xc+g==
MIME-Version: 1.0
X-Received: by 10.182.102.99 with SMTP id fn3mr8281066obb.57.1400141666384;
 Thu, 15 May 2014 01:14:26 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Thu, 15 May 2014 01:14:26 -0700 (PDT)
Date: Thu, 15 May 2014 01:14:26 -0700
Message-ID: <CABPQxsu13Y1TnF_NhVZrYQG=DP94hfyvwCaTa7DAGp4zgHGiJg@mail.gmail.com>
Subject: [VOTE] Release Apache Spark 1.0.0 (rc7)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Please vote on releasing the following candidate as Apache Spark version 1.0.0!

This patch has minor documentation changes and fixes on top of rc6.

The tag to be voted on is v1.0.0-rc7 (commit 9212b3e):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=9212b3e5bb5545ccfce242da8d89108e6fb1c464

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc7/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1015

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc7-docs/

Please vote on releasing this package as Apache Spark 1.0.0!

The vote is open until Sunday, May 18, at 09:12 UTC and passes if a
majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.0.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== API Changes ==
We welcome users to compile Spark applications against 1.0. There are
a few API changes in this release. Here are links to the associated
upgrade guides - user facing changes have been kept as small as
possible.

changes to ML vector specification:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10

changes to the Java API:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark

changes to the streaming API:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x

changes to the GraphX API:
http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091

coGroup and related functions now return Iterable[T] instead of Seq[T]
==> Call toSeq on the result to restore the old behavior

SparkContext.jarOfClass returns Option[String] instead of Seq[String]
==> Call toSeq on the result to restore old behavior

From dev-return-7602-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 22:51:44 2014
Return-Path: <dev-return-7602-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 58B84115A3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 22:51:44 +0000 (UTC)
Received: (qmail 79749 invoked by uid 500); 16 May 2014 11:38:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8899 invoked by uid 500); 16 May 2014 11:23:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57837 invoked by uid 99); 16 May 2014 11:18:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 11:18:17 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of liqingyang1985@gmail.com designates 209.85.212.176 as permitted sender)
Received: from [209.85.212.176] (HELO mail-wi0-f176.google.com) (209.85.212.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 May 2014 08:16:05 +0000
Received: by mail-wi0-f176.google.com with SMTP id n15so9356475wiw.3
        for <dev@spark.apache.org>; Thu, 15 May 2014 01:15:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=e0zMCJDwdpm9/2cf0lx5LeRortMsM2q85d2V/IuoaZ4=;
        b=Rx0gWMJANSS50ILgG5DaeC6GJE7LnH+9TIbpy11DBuNe63//OZEhnh8QPXm12972Fh
         d2s1ijuimiJsL64cGm3KcycYGuIxAdWFx1foJJ11Aj0XVPxiR4Coy/yMPh/oDsUen/0w
         6jcqoblI0PIag0Mc3Cz2YE2aV7GjQFul31oXbJN0BtUBhX7fKqHq8r4RJYLV3a0zgiwH
         gWadkr0jVJZlsHWKtDFmSVln+bJQINkQYG1prLbB5QYPIUUxsE2h3C/wYOLQ/NhffYQF
         YpYvshsLVyYaBPYb9VFcPXO0JhSmqQMfAJas1yhmImaP1ITckKozrx2BXZ7IJb+TK9co
         PVHw==
MIME-Version: 1.0
X-Received: by 10.180.11.9 with SMTP id m9mr29909278wib.51.1400141744155; Thu,
 15 May 2014 01:15:44 -0700 (PDT)
Received: by 10.194.61.39 with HTTP; Thu, 15 May 2014 01:15:44 -0700 (PDT)
Date: Thu, 15 May 2014 16:15:44 +0800
Message-ID: <CABDsqqb64pL9ENMv0YVoo2GRKWQQe=jCb5TLZV4yBeRwg2ZyTA@mail.gmail.com>
Subject: can RDD be shared across mutil spark applications?
From: qingyang li <liqingyang1985@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c258dc4bb09904f96be721
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c258dc4bb09904f96be721
Content-Type: text/plain; charset=UTF-8



--001a11c258dc4bb09904f96be721--

From dev-return-7625-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 22:57:28 2014
Return-Path: <dev-return-7625-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1D2D411A0E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 22:57:28 +0000 (UTC)
Received: (qmail 99019 invoked by uid 500); 16 May 2014 21:04:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85769 invoked by uid 500); 16 May 2014 20:39:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71900 invoked by uid 99); 16 May 2014 20:33:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 20:33:50 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of glenn.strycker@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 20:33:45 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <glenn.strycker@gmail.com>)
	id 1WlOon-0003xj-BZ
	for dev@spark.incubator.apache.org; Fri, 16 May 2014 13:33:25 -0700
Date: Fri, 16 May 2014 13:33:25 -0700 (PDT)
From: GlennStrycker <glenn.strycker@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1400272405344-6607.post@n3.nabble.com>
In-Reply-To: <CAAsvFPk36+vXkZijCdZGxJHRSJcvPon3BopPNacNxXvDB=FF0Q@mail.gmail.com>
References: <1400258494709-6593.post@n3.nabble.com> <CAAsvFPk36+vXkZijCdZGxJHRSJcvPon3BopPNacNxXvDB=FF0Q@mail.gmail.com>
Subject: Re: Scala examples for Spark do not work as written in
 documentation
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Why does the reduce function only work on sums of keys of the same type and
does not support other functional forms?

I am having trouble in another example where instead of 1s and 0s, the
output of the map function is something like A=(1,2) and B=(3,4).  I need a
reduce function that can return something complicated based on reduce( (A,B)
=> (arbitrary fcn1 of A and B, arbitrary fcn2 of A and B) ), but I am only
getting reduce( (A,B) => (arbitrary fcn1 of A, arbitrary fcn2 of A) ).

See
http://apache-spark-developers-list.1001551.n3.nabble.com/reduce-only-removes-duplicates-cannot-be-arbitrary-function-td6606.html




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Scala-examples-for-Spark-do-not-work-as-written-in-documentation-tp6593p6607.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7595-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 23:12:21 2014
Return-Path: <dev-return-7595-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2A6B611B81
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 23:12:21 +0000 (UTC)
Received: (qmail 2725 invoked by uid 500); 16 May 2014 10:59:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74788 invoked by uid 500); 16 May 2014 10:48:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51938 invoked by uid 99); 16 May 2014 10:25:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 10:25:03 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 08:46:56 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 62B00101F89
	for <dev@spark.apache.org>; Fri, 16 May 2014 01:46:36 -0700 (PDT)
Received: from mail-qc0-f182.google.com (mail-qc0-f182.google.com [209.85.216.182])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id CD81A101F88
	for <dev@spark.apache.org>; Fri, 16 May 2014 01:46:35 -0700 (PDT)
Received: by mail-qc0-f182.google.com with SMTP id e16so3812549qcx.27
        for <dev@spark.apache.org>; Fri, 16 May 2014 01:46:35 -0700 (PDT)
X-Gm-Message-State: ALoCoQlf67ZY2ct/KTzHNnZLhMMhybX25ojE7m7w9hdSykcXl6tIxfq/bMjpRzvHOrFcR+k3SwEw
MIME-Version: 1.0
X-Received: by 10.224.62.17 with SMTP id v17mr20854587qah.6.1400229995023;
 Fri, 16 May 2014 01:46:35 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Fri, 16 May 2014 01:46:34 -0700 (PDT)
Date: Fri, 16 May 2014 01:46:34 -0700
Message-ID: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
Subject: Calling external classes added by sc.addJar needs to be through reflection
From: DB Tsai <dbtsai@stanford.edu>
To: user@spark.apache.org, dev@spark.apache.org, 
	Xiangrui Meng <mengxr@gmail.com>
Content-Type: multipart/alternative; boundary=089e015372227526cf04f980734c
X-Virus-Checked: Checked by ClamAV on apache.org

--089e015372227526cf04f980734c
Content-Type: text/plain; charset=UTF-8

Finally find a way out of the ClassLoader maze! It took me some times to
understand how it works; I think it worths to document it in a separated
thread.

We're trying to add external utility.jar which contains CSVRecordParser,
and we added the jar to executors through sc.addJar APIs.

If the instance of CSVRecordParser is created without reflection, it
raises *ClassNotFound
Exception*.

data.mapPartitions(lines => {
    val csvParser = new CSVRecordParser((delimiter.charAt(0))
    lines.foreach(line => {
      val lineElems = csvParser.parseLine(line)
    })
    ...
    ...
 )


If the instance of CSVRecordParser is created through reflection, it works.

data.mapPartitions(lines => {
    val loader = Thread.currentThread.getContextClassLoader
    val CSVRecordParser =
        loader.loadClass("com.alpine.hadoop.ext.CSVRecordParser")

    val csvParser = CSVRecordParser.getConstructor(Character.TYPE)
        .newInstance(delimiter.charAt(0).asInstanceOf[Character])

    val parseLine = CSVRecordParser
        .getDeclaredMethod("parseLine", classOf[String])

    lines.foreach(line => {
       val lineElems = parseLine.invoke(csvParser,
line).asInstanceOf[Array[String]]
    })
    ...
    ...
 )


This is identical to this question,
http://stackoverflow.com/questions/7452411/thread-currentthread-setcontextclassloader-without-using-reflection

It's not intuitive for users to load external classes through reflection,
but couple available solutions including 1) messing around
systemClassLoader by calling systemClassLoader.addURI through reflection or
2) forking another JVM to add jars into classpath before bootstrap loader
are very tricky.

Any thought on fixing it properly?

@Xiangrui,
netlib-java jniloader is loaded from netlib-java through reflection, so
this problem will not be seen.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai

--089e015372227526cf04f980734c--

From dev-return-7605-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 23:12:22 2014
Return-Path: <dev-return-7605-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ADD7C11BB7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 23:12:22 +0000 (UTC)
Received: (qmail 58450 invoked by uid 500); 16 May 2014 11:46:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70283 invoked by uid 500); 16 May 2014 11:36:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63864 invoked by uid 99); 16 May 2014 11:19:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 11:19:06 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.52 as permitted sender)
Received: from [209.85.219.52] (HELO mail-oa0-f52.google.com) (209.85.219.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 May 2014 08:15:15 +0000
Received: by mail-oa0-f52.google.com with SMTP id eb12so844876oac.11
        for <dev@spark.apache.org>; Thu, 15 May 2014 01:14:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=fcQ4QFBQnJM/rEaZMKOU2f5uEfqtKI8cu5bvbSRfj0M=;
        b=nyRKbGp58KVmXxqxiE0dxGN5a2m3y/sh/5par8/SZbFnlAYa4WxxCcK/TsmvEOroF1
         AWy2RkLTeT4V2za7ZrprVlfBBSjaeKkS2PnbO7tFDEcO95CriGfiYx+1YG129aKPJrOq
         1pFGNxKy31d8f8CS/Jzs/XKEj8YJaxVYpphPG9E61s3WJpj6eUAfyByB1YL3a+jONwU/
         QYWwSjGLEr8Cl8jghUESNoU+D+8UvTPhppKXbJx070cX4sPHFIm6mEGr9Ao9CNCOzdfL
         4IiiEI2p69ZrQw17IX0M0B9uEGyNDavO2X9Z/p70GLj/hFRnbt6brZWCiiwS99cw6A+W
         58/Q==
MIME-Version: 1.0
X-Received: by 10.182.74.234 with SMTP id x10mr8351660obv.1.1400141695376;
 Thu, 15 May 2014 01:14:55 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Thu, 15 May 2014 01:14:55 -0700 (PDT)
Date: Thu, 15 May 2014 01:14:55 -0700
Message-ID: <CABPQxss5ni5a2w5iX71UAeB7f=-kZjtqzEhRog-gFSdjouJ-JQ@mail.gmail.com>
Subject: [RESULT][VOTE] Release Apache Spark 1.0.0 (rc6)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

This vote is cancelled in favor of rc7.

On Wed, May 14, 2014 at 1:02 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.0.0!
>
> This patch has a few minor fixes on top of rc5. I've also built the
> binary artifacts with Hive support enabled so people can test this
> configuration. When we release 1.0 we might just release both vanilla
> and Hive-enabled binaries.
>
> The tag to be voted on is v1.0.0-rc6 (commit 54133a):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=54133abdce0246f6643a1112a5204afb2c4caa82
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.0.0-rc6/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachestratos-1011
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.0.0-rc6-docs/
>
> Please vote on releasing this package as Apache Spark 1.0.0!
>
> The vote is open until Saturday, May 17, at 20:58 UTC and passes if
> amajority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.0.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == API Changes ==
> We welcome users to compile Spark applications against 1.0. There are
> a few API changes in this release. Here are links to the associated
> upgrade guides - user facing changes have been kept as small as
> possible.
>
> changes to ML vector specification:
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10
>
> changes to the Java API:
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>
> changes to the streaming API:
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>
> changes to the GraphX API:
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>
> coGroup and related functions now return Iterable[T] instead of Seq[T]
> ==> Call toSeq on the result to restore the old behavior
>
> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> ==> Call toSeq on the result to restore old behavior

From dev-return-7604-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 23:12:26 2014
Return-Path: <dev-return-7604-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2662111C81
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 23:12:26 +0000 (UTC)
Received: (qmail 42926 invoked by uid 500); 16 May 2014 11:45:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26559 invoked by uid 500); 16 May 2014 11:27:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52574 invoked by uid 99); 16 May 2014 11:16:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 11:16:56 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.43 as permitted sender)
Received: from [209.85.219.43] (HELO mail-oa0-f43.google.com) (209.85.219.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 May 2014 17:25:49 +0000
Received: by mail-oa0-f43.google.com with SMTP id l6so1653914oag.16
        for <dev@spark.apache.org>; Thu, 15 May 2014 10:25:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=yH20Ho99Iwwn12JidEbVDrFKaAWbYovWF8wpW7ZeA08=;
        b=ymzwZmzMPNpgsleyWoL6M+0Ox5VHd8mXt8hwVyLb5WRLJiOI30/NeEUuGflk91gnn8
         qwex6P8mYVeYT6PCA8ccAKr998Aq9fbAEl5VBuD83Vit0nOIpzcM/tccwvLQd/HJMU6b
         F/20wm7T36rklXFqQSlxAbgWS5GLsauSBY4Cv79fPNnIB+m7T0+3xF7KbZvSgtSmT7cG
         lOb8NomMRCGM5+DQ63Q+N59b0Yr+skEXKdLS1ua4KjFCTOofvNjqJDmkd+KUQGNswPXW
         Hx5gXO0+e6E1W7CLMMKhjd5o0obcwbOdnPOoo7F5BOLEiucwbSMcO+BA3Mk02RDe269G
         RxBQ==
MIME-Version: 1.0
X-Received: by 10.60.179.138 with SMTP id dg10mr11447439oec.13.1400174729108;
 Thu, 15 May 2014 10:25:29 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Thu, 15 May 2014 10:25:28 -0700 (PDT)
In-Reply-To: <CABPQxsvVOkU_XiKdFe+D2DP4gA_MjGP+SohjbUnB8ptQrYQ=FA@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAMAsSdLm+aH-KsE0FCNixXv5vwCd6WSb3cA3+kfmK1PfnLVYiQ@mail.gmail.com>
	<CAMtqZeeAW6BeCiCfHbiCmKfMh=BnrO=+UM1oPe1VNgB2disQQw@mail.gmail.com>
	<CABPQxssAXupM4faQD1zm5x4yObufMa7dq1SpU3dwciQV671sNQ@mail.gmail.com>
	<tencent_5EABC2696C9E4E8610F1CE38@qq.com>
	<CABPQxstBPL6goZaR1F3pwZR=04yS8nCq9hxNC7Yq=yT1j8vEvQ@mail.gmail.com>
	<tencent_4CAB53D66DED27DB54B1317C@qq.com>
	<CABPQxsvVOkU_XiKdFe+D2DP4gA_MjGP+SohjbUnB8ptQrYQ=FA@mail.gmail.com>
Date: Thu, 15 May 2014 10:25:28 -0700
Message-ID: <CABPQxsudzjrsfpW1tmdjeXhbgxDE05=RqosD2kX=YmjOy4AQBQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Everyone,

Just a heads up - I've sent other release candidates to the list, but
they appear to be getting swallowed (i.e. they are not on nabble). I
think there is an issue with Apache mail servers.

I'm going to keep trying... if you get duplicate e-mails I apologize in advance.

On Thu, May 15, 2014 at 10:23 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> Thanks for your feedback. Since it's not a regression, it won't block
> the release.
>
> On Wed, May 14, 2014 at 12:17 AM, witgo <witgo@qq.com> wrote:
>> SPARK-1817 will cause users to get incorrect results  and RDD.zip is common usage .
>> This should be the highest priority. I think we should fix the bug,and should also test the previous release
>> ------------------ Original ------------------
>> From:  "Patrick Wendell";<pwendell@gmail.com>;
>> Date:  Wed, May 14, 2014 03:02 PM
>> To:  "dev@spark.apache.org"<dev@spark.apache.org>;
>>
>> Subject:  Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
>>
>>
>>
>> Hey @witgo - those bugs are not severe enough to block the release,
>> but it would be nice to get them fixed.
>>
>> At this point we are focused on severe bugs with an immediate fix, or
>> regressions from previous versions of Spark. Anything that misses this
>> release will get merged into the branch-1.0 branch and make it into
>> the 1.0.1 release, so people will have access to it.
>>
>> On Tue, May 13, 2014 at 5:32 PM, witgo <witgo@qq.com> wrote:
>>> -1
>>> The following bug should be fixed:
>>> https://issues.apache.org/jira/browse/SPARK-1817
>>> https://issues.apache.org/jira/browse/SPARK-1712
>>>
>>>
>>> ------------------ Original ------------------
>>> From:  "Patrick Wendell";<pwendell@gmail.com>;
>>> Date:  Wed, May 14, 2014 04:07 AM
>>> To:  "dev@spark.apache.org"<dev@spark.apache.org>;
>>>
>>> Subject:  Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
>>>
>>>
>>>
>>> Hey all - there were some earlier RC's that were not presented to the
>>> dev list because issues were found with them. Also, there seems to be
>>> some issues with the reliability of the dev list e-mail. Just a heads
>>> up.
>>>
>>> I'll lead with a +1 for this.
>>>
>>> On Tue, May 13, 2014 at 8:07 AM, Nan Zhu <zhunanmcgill@gmail.com> wrote:
>>>> just curious, where is rc4 VOTE?
>>>>
>>>> I searched my gmail but didn't find that?
>>>>
>>>>
>>>>
>>>>
>>>> On Tue, May 13, 2014 at 9:49 AM, Sean Owen <sowen@cloudera.com> wrote:
>>>>
>>>>> On Tue, May 13, 2014 at 9:36 AM, Patrick Wendell <pwendell@gmail.com>
>>>>> wrote:
>>>>> > The release files, including signatures, digests, etc. can be found at:
>>>>> > http://people.apache.org/~pwendell/spark-1.0.0-rc5/
>>>>>
>>>>> Good news is that the sigs, MD5 and SHA are all correct.
>>>>>
>>>>> Tiny note: the Maven artifacts use SHA1, while the binary artifacts
>>>>> use SHA512, which took me a bit of head-scratching to figure out.
>>>>>
>>>>> If another RC comes out, I might suggest making it SHA1 everywhere?
>>>>> But there is nothing wrong with these signatures and checksums.
>>>>>
>>>>> Now to look at the contents...
>>>>>
>>> .
>> .

From dev-return-7600-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 23:12:27 2014
Return-Path: <dev-return-7600-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4A16011CAC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 23:12:27 +0000 (UTC)
Received: (qmail 42999 invoked by uid 500); 16 May 2014 11:12:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4214 invoked by uid 500); 16 May 2014 11:00:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71429 invoked by uid 99); 16 May 2014 10:46:36 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 10:46:35 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 74.125.82.177 as permitted sender)
Received: from [74.125.82.177] (HELO mail-we0-f177.google.com) (74.125.82.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 03:34:03 +0000
Received: by mail-we0-f177.google.com with SMTP id x48so1909949wes.8
        for <dev@spark.apache.org>; Thu, 15 May 2014 20:33:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=QaRgNMAtOYSEFd1qVZH/aV8aklO07BKimVxChGrxHYQ=;
        b=xxbLOcOoS27ROiYKwYhGgdLvep88uMwmLQh3olj2QdFKa93WQFrKIu/2le11hNS7x5
         rGUQf6ph0StuGzpR7PWi2qil1Y6i2O3JVpEMYw9TsktCDFDv90VSPZGgrqppWf7SwClS
         s1arZbc1rAEifZvED1n/R2G9YmO9A+Z0DFdbz23kMw5K1ccKF2yQp2IUzlUJVClHVh7i
         jC6V0TzXAvOeILlp5uapO8JE+naVKkzeGxgsJmixw6Qjv4TNW6sSvOnnjJrd3ZV7R7tC
         izxAX3k1dwCgpmd1d16wBa0biDQ85Q78VAkTLPLxJ94inlVUwRSlcfddbeLTgA4pe+QC
         8h7A==
MIME-Version: 1.0
X-Received: by 10.180.73.201 with SMTP id n9mr11456802wiv.45.1400211220925;
 Thu, 15 May 2014 20:33:40 -0700 (PDT)
Received: by 10.194.187.20 with HTTP; Thu, 15 May 2014 20:33:40 -0700 (PDT)
In-Reply-To: <CA+B-+fxNcFuQmdsQERiddQczHD6LYnKTx4QBV6p_QS=eE13n5g@mail.gmail.com>
References: <CA+B-+fzM9sxc+F7hxoWMo6hM6u64gMJMQSENAt7+GXRdTUeAfg@mail.gmail.com>
	<CAEYYnxYGGeTCtzGjN3+VRsRwB=ni7izCA21MXudn0Aq=kEaxtw@mail.gmail.com>
	<CA+B-+fxSfun1gzWTcm+m1jSyyxi0-CDjE4TkMY_0Vb2WkKAowA@mail.gmail.com>
	<CAEYYnxZMnAiBwNW8O7_moMbX8hBaZO1Gw=TY2gcMOhJrRHVb9Q@mail.gmail.com>
	<CALW2ey1E1tE9gsbs4ftv8J1JQseANMaUt-muGCK=KG3UDLRC7Q@mail.gmail.com>
	<CAEYYnxYo=9JwQpDQiNRvMp3n_tJLUOwWv_Mz28-P9VxW5QHAdg@mail.gmail.com>
	<CALW2ey2JBUY1VwTNHV4rirYyF0swcfwQamW2y--NJL5axo31cw@mail.gmail.com>
	<CA+B-+fx1DOwvzZttLypFBF6qg8M1X48hLL=UP__RVbJYNYCwiw@mail.gmail.com>
	<CA+B-+fxNcFuQmdsQERiddQczHD6LYnKTx4QBV6p_QS=eE13n5g@mail.gmail.com>
Date: Thu, 15 May 2014 20:33:40 -0700
Message-ID: <CAJgQjQ9uCtnxJUVU9o4sWfOwc78WfFmxbrMS=j2QffvonFkU=Q@mail.gmail.com>
Subject: Re: mllib vector templates
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I submitted a PR for standardizing the text format for vectors and
labeled data: https://github.com/apache/spark/pull/685

Once it gets merged, saveAsTextFile and loading should be consistent.
I didn't choose LibSVM as the default format because two reasons:

1) It doesn't contain feature dimension info in the record. We need to
scan the dataset to get that info.
2) It saves index:value tuples. Putting indices together can help data
compression. Same for value if there are many binary features.

Best,
Xiangrui

On Wed, May 7, 2014 at 10:25 PM, Debasish Das <debasish.das83@gmail.com> wrote:
> Hi,
>
> I see ALS is still using Array[Int] but for other mllib algorithm we moved
> to Vector[Double] so that it can support either dense and sparse formats...
>
> ALS can stay in Array[Int] due to the Netflix format for input datasets
> which is well defined but it helps if we move ALS to Vector[Double] as
> well...that way all algorithms will be consistent...
>
> The second issue is that toString on SparseVector does not write libsvm
> format but something not very generic...can we change the
> SparseVector.toString to write as libsvm output ? I am dumping a sample of
> dataset to see how mllib glm compares with the glmnet-R package for QoR...
>
> Thanks.
> Deb
>
> On Mon, May 5, 2014 at 4:05 PM, David Hall <dlwh@cs.berkeley.edu> wrote:
>>
>>> On Mon, May 5, 2014 at 3:40 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>
>>> > David,
>>> >
>>> > Could we use Int, Long, Float as the data feature spaces, and Double for
>>> > optimizer?
>>> >
>>>
>>> Yes. Breeze doesn't allow operations on mixed types, so you'd need to
>>> convert the double vectors to Floats if you wanted, e.g. dot product with
>>> the weights vector.
>>>
>>> You might also be interested in FeatureVector, which is just a wrapper
>>> around Array[Int] that emulates an indicator vector. It supports dot
>>> products, axpy, etc.
>>>
>>> -- David
>>>
>>>
>>> >
>>> >
>>> > Sincerely,
>>> >
>>> > DB Tsai
>>> > -------------------------------------------------------
>>> > My Blog: https://www.dbtsai.com
>>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>>> >
>>> >
>>> > On Mon, May 5, 2014 at 3:06 PM, David Hall <dlwh@cs.berkeley.edu>
>>> wrote:
>>> >
>>> > > Lbfgs and other optimizers would not work immediately, as they require
>>> > > vector spaces over double. Otherwise it should work.
>>> > > On May 5, 2014 3:03 PM, "DB Tsai" <dbtsai@stanford.edu> wrote:
>>> > >
>>> > > > Breeze could take any type (Int, Long, Double, and Float) in the
>>> matrix
>>> > > > template.
>>> > > >
>>> > > >
>>> > > > Sincerely,
>>> > > >
>>> > > > DB Tsai
>>> > > > -------------------------------------------------------
>>> > > > My Blog: https://www.dbtsai.com
>>> > > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>> > > >
>>> > > >
>>> > > > On Mon, May 5, 2014 at 2:56 PM, Debasish Das <
>>> debasish.das83@gmail.com
>>> > > > >wrote:
>>> > > >
>>> > > > > Is this a breeze issue or breeze can take templates on float /
>>> > double ?
>>> > > > >
>>> > > > > If breeze can take templates then it is a minor fix for
>>> Vectors.scala
>>> > > > right
>>> > > > > ?
>>> > > > >
>>> > > > > Thanks.
>>> > > > > Deb
>>> > > > >
>>> > > > >
>>> > > > > On Mon, May 5, 2014 at 2:45 PM, DB Tsai <dbtsai@stanford.edu>
>>> wrote:
>>> > > > >
>>> > > > > > +1  Would be nice that we can use different type in Vector.
>>> > > > > >
>>> > > > > >
>>> > > > > > Sincerely,
>>> > > > > >
>>> > > > > > DB Tsai
>>> > > > > > -------------------------------------------------------
>>> > > > > > My Blog: https://www.dbtsai.com
>>> > > > > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>> > > > > >
>>> > > > > >
>>> > > > > > On Mon, May 5, 2014 at 2:41 PM, Debasish Das <
>>> > > debasish.das83@gmail.com
>>> > > > > > >wrote:
>>> > > > > >
>>> > > > > > > Hi,
>>> > > > > > >
>>> > > > > > > Why mllib vector is using double as default ?
>>> > > > > > >
>>> > > > > > > /**
>>> > > > > > >
>>> > > > > > >  * Represents a numeric vector, whose index type is Int and
>>> value
>>> > > > type
>>> > > > > is
>>> > > > > > > Double.
>>> > > > > > >
>>> > > > > > >  */
>>> > > > > > >
>>> > > > > > > trait Vector extends Serializable {
>>> > > > > > >
>>> > > > > > >
>>> > > > > > >   /**
>>> > > > > > >
>>> > > > > > >    * Size of the vector.
>>> > > > > > >
>>> > > > > > >    */
>>> > > > > > >
>>> > > > > > >   def size: Int
>>> > > > > > >
>>> > > > > > >
>>> > > > > > >   /**
>>> > > > > > >
>>> > > > > > >    * Converts the instance to a double array.
>>> > > > > > >
>>> > > > > > >    */
>>> > > > > > >
>>> > > > > > >   def toArray: Array[Double]
>>> > > > > > >
>>> > > > > > > Don't we need a template on float/double ? This will give us
>>> > memory
>>> > > > > > > savings...
>>> > > > > > >
>>> > > > > > > Thanks.
>>> > > > > > >
>>> > > > > > > Deb
>>> > > > > > >
>>> > > > > >
>>> > > > >
>>> > > >
>>> > >
>>> >
>>>
>>
>>

From dev-return-7608-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 23:12:29 2014
Return-Path: <dev-return-7608-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E532D11CF1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 23:12:29 +0000 (UTC)
Received: (qmail 92849 invoked by uid 500); 16 May 2014 11:49:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96710 invoked by uid 500); 16 May 2014 11:40:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49132 invoked by uid 99); 16 May 2014 11:14:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 11:14:59 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.53 as permitted sender)
Received: from [209.85.219.53] (HELO mail-oa0-f53.google.com) (209.85.219.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 15 May 2014 08:15:45 +0000
Received: by mail-oa0-f53.google.com with SMTP id m1so834449oag.40
        for <dev@spark.apache.org>; Thu, 15 May 2014 01:15:25 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=LZ6NNstwp+Lks/myXxaumyN1CfikKHokotG5Ti7tdQg=;
        b=y/Kg8TKRpmxyabsdpaaRtBZRNxjv4gKrwGdYm0h2B2A9T5hS5/KxY4ljrSlWl7JC8Y
         ZJbT9ruc91plALFLnGRLIfNAhh4llkrj/k8l53gqdVd4WgabugGk/mXZ96VMKyRTJp5A
         cdQPqhrEQYJ4gjaYUcGkuIXzxC3nYsKQteLp1yOpdZmcXnNTcXf5k/olpe5QK0hhloMT
         z/CG1MFFyGpjJOaIk/CFPbWFOkzkdASn1+ApFp8O/0MYOZiW94NiP2YjojFKaVe4KYpe
         /CIGGFzGUdDh8WJ7ENGDZxfXfpyMPgbkB/s7GqPAEHCDzBfbuIOztiX7pn1PqFugYF7k
         LSvQ==
MIME-Version: 1.0
X-Received: by 10.60.102.198 with SMTP id fq6mr8522626oeb.6.1400141725504;
 Thu, 15 May 2014 01:15:25 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Thu, 15 May 2014 01:15:25 -0700 (PDT)
In-Reply-To: <CABPQxsu13Y1TnF_NhVZrYQG=DP94hfyvwCaTa7DAGp4zgHGiJg@mail.gmail.com>
References: <CABPQxsu13Y1TnF_NhVZrYQG=DP94hfyvwCaTa7DAGp4zgHGiJg@mail.gmail.com>
Date: Thu, 15 May 2014 01:15:25 -0700
Message-ID: <CABPQxst+WESyBGi-fTdFXjVVmrVkvduE_1Uf6Zh+PbgYPbGCgA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc7)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I'll start the voting with a +1.

On Thu, May 15, 2014 at 1:14 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.0.0!
>
> This patch has minor documentation changes and fixes on top of rc6.
>
> The tag to be voted on is v1.0.0-rc7 (commit 9212b3e):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=9212b3e5bb5545ccfce242da8d89108e6fb1c464
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.0.0-rc7/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1015
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.0.0-rc7-docs/
>
> Please vote on releasing this package as Apache Spark 1.0.0!
>
> The vote is open until Sunday, May 18, at 09:12 UTC and passes if a
> majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.0.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == API Changes ==
> We welcome users to compile Spark applications against 1.0. There are
> a few API changes in this release. Here are links to the associated
> upgrade guides - user facing changes have been kept as small as
> possible.
>
> changes to ML vector specification:
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10
>
> changes to the Java API:
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>
> changes to the streaming API:
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>
> changes to the GraphX API:
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>
> coGroup and related functions now return Iterable[T] instead of Seq[T]
> ==> Call toSeq on the result to restore the old behavior
>
> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> ==> Call toSeq on the result to restore old behavior

From dev-return-7618-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 23:12:35 2014
Return-Path: <dev-return-7618-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 663FE11D24
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 23:12:35 +0000 (UTC)
Received: (qmail 74444 invoked by uid 500); 16 May 2014 19:24:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61518 invoked by uid 500); 16 May 2014 19:03:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 3106 invoked by uid 99); 16 May 2014 18:46:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 18:46:32 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.170] (HELO mail-qc0-f170.google.com) (209.85.216.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 18:46:28 +0000
Received: by mail-qc0-f170.google.com with SMTP id i8so5086673qcq.1
        for <dev@spark.apache.org>; Fri, 16 May 2014 11:46:07 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=um8iJvS4EF5fxXLTVv28lDINcYZkdGGkJmpjVnhJDWk=;
        b=mmRtXLFzdtE644VLjVaqupi1bBnN+OBBXF3H0R26BY35ePCIYK3ohhdazRI81MYzYE
         5oNCijzKIk0zlTOXG0buTOicAx4M1rFKoSuN1ofXBXBV1JDZzDn3lH1rK6jSO7KyX5ko
         jeGpGTAbc7pMH4elGDrr1095S/T7mq/PpFacIHkEHTu+UJ6tjcZ2p6oiLsnBOuJlDNF0
         VSHPH7F7j6+UK1KDfDbkgnMiRxFtWPrW5VwYk2sku1l7phpvCPvflGjY4f42jlVQRGA2
         YP4gmYvwEfEhExgYtOifvfSiu6NoukWrs+5ubGRb1lnaI/NJTE5WoXQqXITJK0ULlTsa
         omIw==
X-Gm-Message-State: ALoCoQmn//Ea68PLDElKr9kic6oxmC4b6ReRLeJPG5/RSee4vXKQWKb0NDyqKZ4+dnfEs5VpJPD0
X-Received: by 10.224.49.67 with SMTP id u3mr26018656qaf.63.1400265967898;
 Fri, 16 May 2014 11:46:07 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.126.1 with HTTP; Fri, 16 May 2014 11:45:47 -0700 (PDT)
In-Reply-To: <CACfA1zVEOy=bWceH+26nTX5R23-Fmx8wVHh2u7kJPFcp=yE2SQ@mail.gmail.com>
References: <CAMJOb8k_6d_0ixRQz0yFcjt59y42pW+rw_NsJUCHiG7aOmcn2A@mail.gmail.com>
 <CAEYYnxZBGT6ma=r_5SsJ7Eiv8UVFbqc9fibLSSSWjvGYo+ix+w@mail.gmail.com> <CACfA1zVEOy=bWceH+26nTX5R23-Fmx8wVHh2u7kJPFcp=yE2SQ@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Fri, 16 May 2014 11:45:47 -0700
Message-ID: <CAPh_B=aAQM8q=TR90BRh_QN5S2nkxb5B4MyDr4+D=DTLGFCsNQ@mail.gmail.com>
Subject: Re: (test)
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2ef809ba5f504f988d352
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ef809ba5f504f988d352
Content-Type: text/plain; charset=UTF-8

I didn't see the original message, but only a reply.


On Fri, May 16, 2014 at 10:38 AM, Nishkam Ravi <nravi@cloudera.com> wrote:

> Yes.
>
>
> On Fri, May 16, 2014 at 8:40 AM, DB Tsai <dbtsai@stanford.edu> wrote:
>
> > Yes.
> > On May 16, 2014 8:39 AM, "Andrew Or" <andrew@databricks.com> wrote:
> >
> > > Apache has been having some problems lately. Do you guys see this
> > message?
> > >
> >
>

--001a11c2ef809ba5f504f988d352--

From dev-return-7619-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 23:27:28 2014
Return-Path: <dev-return-7619-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 501F811EB9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 23:27:28 +0000 (UTC)
Received: (qmail 66237 invoked by uid 500); 16 May 2014 19:03:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62044 invoked by uid 500); 16 May 2014 19:03:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82123 invoked by uid 99); 16 May 2014 18:42:47 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 18:42:47 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of henry.saputra@gmail.com designates 74.125.82.46 as permitted sender)
Received: from [74.125.82.46] (HELO mail-wg0-f46.google.com) (74.125.82.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 18:42:43 +0000
Received: by mail-wg0-f46.google.com with SMTP id n12so5266139wgh.17
        for <dev@spark.apache.org>; Fri, 16 May 2014 11:42:20 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=OZp+eoz6Nw9PbqC4CZzihI1goLS8KTCmR64WmZE3AoI=;
        b=ZDPKe7C24owcp+4p96Ss2nU/6BaH3YzDpdS6ToOaNoPbou/Oyshaw3UUB+jt9k8lui
         v2cE4c0dDERx68b01JB7XJMmXshN3LFDDe08N/zxf+Vx3GMWYiGnwrGD1V3J2NmpUKNj
         0N3NzNsVU1v3WFQaYIcxfLo28DdL7NMlnk1jmF5yNPczUurlMke6xwqkFmGlRLl/H6U9
         35c8KLdD290D48CUJEpW0GsZw0LudKWsWmAJcsbCMktRqoH9eKweO7RAaFXT39d5KxWe
         VmIf/fe3otbq12tQ/YH7ccOh7MMupSkzHg5hGE/nlaISnnT1ValluNbV0jhj2wtiINuj
         RiaQ==
MIME-Version: 1.0
X-Received: by 10.194.63.176 with SMTP id h16mr15522734wjs.6.1400265740316;
 Fri, 16 May 2014 11:42:20 -0700 (PDT)
Received: by 10.216.165.71 with HTTP; Fri, 16 May 2014 11:42:20 -0700 (PDT)
In-Reply-To: <CANGvG8oFfLM-nXtxUCFQKwRCyuZ0cK+mpCWCG_PyFA32C_5r=Q@mail.gmail.com>
References: <CABPQxsu13Y1TnF_NhVZrYQG=DP94hfyvwCaTa7DAGp4zgHGiJg@mail.gmail.com>
	<CABPQxst+WESyBGi-fTdFXjVVmrVkvduE_1Uf6Zh+PbgYPbGCgA@mail.gmail.com>
	<CALuGr6bpepzhFNV3ery=0067xRO8FP9S2gVZf9SDQ54T652s6g@mail.gmail.com>
	<CANGvG8oFfLM-nXtxUCFQKwRCyuZ0cK+mpCWCG_PyFA32C_5r=Q@mail.gmail.com>
Date: Fri, 16 May 2014 11:42:20 -0700
Message-ID: <CALuGr6b6HbRd34vN4=uvWDKj-UKJMUs6seJUo2u0J-OgMomdyg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc7)
From: Henry Saputra <henry.saputra@gmail.com>
To: Aaron Davidson <ilikerps@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Ah ok, thanks Aaron

Just to make sure we VOTE the right RC.

Thanks,

Henry

On Fri, May 16, 2014 at 11:37 AM, Aaron Davidson <ilikerps@gmail.com> wrote:
> It was, but due to the apache infra issues, some may not have received the
> email yet...
>
> On Fri, May 16, 2014 at 10:48 AM, Henry Saputra <henry.saputra@gmail.com>
> wrote:
>>
>> Hi Patrick,
>>
>> Just want to make sure that VOTE for rc6 also cancelled?
>>
>>
>> Thanks,
>>
>> Henry
>>
>> On Thu, May 15, 2014 at 1:15 AM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>> > I'll start the voting with a +1.
>> >
>> > On Thu, May 15, 2014 at 1:14 AM, Patrick Wendell <pwendell@gmail.com>
>> > wrote:
>> >> Please vote on releasing the following candidate as Apache Spark
>> >> version 1.0.0!
>> >>
>> >> This patch has minor documentation changes and fixes on top of rc6.
>> >>
>> >> The tag to be voted on is v1.0.0-rc7 (commit 9212b3e):
>> >>
>> >> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=9212b3e5bb5545ccfce242da8d89108e6fb1c464
>> >>
>> >> The release files, including signatures, digests, etc. can be found at:
>> >> http://people.apache.org/~pwendell/spark-1.0.0-rc7/
>> >>
>> >> Release artifacts are signed with the following key:
>> >> https://people.apache.org/keys/committer/pwendell.asc
>> >>
>> >> The staging repository for this release can be found at:
>> >> https://repository.apache.org/content/repositories/orgapachespark-1015
>> >>
>> >> The documentation corresponding to this release can be found at:
>> >> http://people.apache.org/~pwendell/spark-1.0.0-rc7-docs/
>> >>
>> >> Please vote on releasing this package as Apache Spark 1.0.0!
>> >>
>> >> The vote is open until Sunday, May 18, at 09:12 UTC and passes if a
>> >> majority of at least 3 +1 PMC votes are cast.
>> >>
>> >> [ ] +1 Release this package as Apache Spark 1.0.0
>> >> [ ] -1 Do not release this package because ...
>> >>
>> >> To learn more about Apache Spark, please see
>> >> http://spark.apache.org/
>> >>
>> >> == API Changes ==
>> >> We welcome users to compile Spark applications against 1.0. There are
>> >> a few API changes in this release. Here are links to the associated
>> >> upgrade guides - user facing changes have been kept as small as
>> >> possible.
>> >>
>> >> changes to ML vector specification:
>> >>
>> >> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10
>> >>
>> >> changes to the Java API:
>> >>
>> >> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>> >>
>> >> changes to the streaming API:
>> >>
>> >> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>> >>
>> >> changes to the GraphX API:
>> >>
>> >> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>> >>
>> >> coGroup and related functions now return Iterable[T] instead of Seq[T]
>> >> ==> Call toSeq on the result to restore the old behavior
>> >>
>> >> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
>> >> ==> Call toSeq on the result to restore old behavior
>
>

From dev-return-7620-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 23:27:28 2014
Return-Path: <dev-return-7620-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5954511EBB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 23:27:28 +0000 (UTC)
Received: (qmail 78955 invoked by uid 500); 16 May 2014 19:24:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69189 invoked by uid 500); 16 May 2014 19:03:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99705 invoked by uid 99); 16 May 2014 18:45:33 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 18:45:33 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ilikerps@gmail.com designates 209.85.216.178 as permitted sender)
Received: from [209.85.216.178] (HELO mail-qc0-f178.google.com) (209.85.216.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 18:45:29 +0000
Received: by mail-qc0-f178.google.com with SMTP id l6so5037089qcy.9
        for <dev@spark.apache.org>; Fri, 16 May 2014 11:45:09 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=4ZpOJN3yVFsNY+n435Oyvj+T5UVNkVVi8X+gq8Uvbjs=;
        b=tK7iIupv4WJCjHK6yjVqY9y5BN9TrxK4NWD7/N0oE5zq76cuX5WLLdEiHwUpCQHi5T
         WpyqqhfK50pPEArvSvwLCG+GjTOjanDMtAGIuUgLI7mI9ZJsusDTW84VQ4TxuKjLm7Zb
         MkRbnRbMK44Fsv9TuciClLhEdjTmb9thFFN6fZ21Uz2QeSp0vhbFbe2KjygcvE5uT52m
         1zmnfFps0cF+Ffa2v2csJsoiYqZkBqxlbMTiMxtidCry2cLqZxy7Pnz7Z73f8cDhsCpe
         5SFqSKypza5d6pccbkSPoolwJXNMREGYGQYeBLHR5xiGVu7liGlVvoxJiYJn0Et+z4vN
         3h3g==
X-Received: by 10.140.93.198 with SMTP id d64mr27084748qge.1.1400265909066;
 Fri, 16 May 2014 11:45:09 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.31.35 with HTTP; Fri, 16 May 2014 11:44:48 -0700 (PDT)
In-Reply-To: <CACfA1zVEOy=bWceH+26nTX5R23-Fmx8wVHh2u7kJPFcp=yE2SQ@mail.gmail.com>
References: <CAMJOb8k_6d_0ixRQz0yFcjt59y42pW+rw_NsJUCHiG7aOmcn2A@mail.gmail.com>
 <CAEYYnxZBGT6ma=r_5SsJ7Eiv8UVFbqc9fibLSSSWjvGYo+ix+w@mail.gmail.com> <CACfA1zVEOy=bWceH+26nTX5R23-Fmx8wVHh2u7kJPFcp=yE2SQ@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Fri, 16 May 2014 11:44:48 -0700
Message-ID: <CANGvG8pqz0=YK+=0DChtYagVsq8buxoCk5JJKDHZX1NPSa3aLw@mail.gmail.com>
Subject: Re: (test)
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11393ace19e42904f988d0c3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11393ace19e42904f988d0c3
Content-Type: text/plain; charset=UTF-8

No. Only 3 of the responses.


On Fri, May 16, 2014 at 10:38 AM, Nishkam Ravi <nravi@cloudera.com> wrote:

> Yes.
>
>
> On Fri, May 16, 2014 at 8:40 AM, DB Tsai <dbtsai@stanford.edu> wrote:
>
> > Yes.
> > On May 16, 2014 8:39 AM, "Andrew Or" <andrew@databricks.com> wrote:
> >
> > > Apache has been having some problems lately. Do you guys see this
> > message?
> > >
> >
>

--001a11393ace19e42904f988d0c3--

From dev-return-7629-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 23:44:27 2014
Return-Path: <dev-return-7629-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A2B9C111CD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 23:44:27 +0000 (UTC)
Received: (qmail 59140 invoked by uid 500); 16 May 2014 23:12:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53662 invoked by uid 500); 16 May 2014 23:12:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91631 invoked by uid 99); 16 May 2014 23:00:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 23:00:51 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.192.47 as permitted sender)
Received: from [209.85.192.47] (HELO mail-qg0-f47.google.com) (209.85.192.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 23:00:40 +0000
Received: by mail-qg0-f47.google.com with SMTP id j107so5233828qga.6
        for <dev@spark.apache.org>; Fri, 16 May 2014 16:00:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=hr6AMaOBM9oS5TwQOSOoH90MJ/fqG8lU0SQK2vZaodk=;
        b=flfnvfly209Y9cOo7TFP1q1dapNbKAx7eXgRQptAvanfoForFEI62dg6aFOvubtXdF
         cPZLk33IOLdUS0k8P4R72esyADcdfufmhjJGxpxbmIafDhGkUAW9Rb7Y7eKIsehRLX06
         /tpZRW96yUteCyFRRgsFMmpvv9aP3NBSH+1qe8iTgwIuYkQkh317zi4yvI0wYXwG6Mb3
         Xst1VbupfX3dvD8lkVq1ruIqG3FYg5at3tn/rl5m0NARAyM5EIM7aOU6Ik7Uidtf7K/Y
         fcp1IxkEwjJAZ0OHsohKDbP9NInrYvBLJPhnGgDwT2VL3fDM6mUa0zqsj4vgdzNOXWfL
         P22A==
MIME-Version: 1.0
X-Received: by 10.140.82.7 with SMTP id g7mr28535144qgd.74.1400281217246; Fri,
 16 May 2014 16:00:17 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Fri, 16 May 2014 16:00:17 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Fri, 16 May 2014 16:00:17 -0700 (PDT)
In-Reply-To: <JIRA.12714626.1400189782796.363092.1400238099511@arcas>
References: <JIRA.12714626.1400189782796@arcas>
	<JIRA.12714626.1400189782796.363092.1400238099511@arcas>
Date: Sat, 17 May 2014 04:30:17 +0530
Message-ID: <CAJiQeY+zsu3cfX1zyA=qK1KO8hirddmjGSB7HohTTA_a9jk3wQ@mail.gmail.com>
Subject: Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c115868a475904f98c6024
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c115868a475904f98c6024
Content-Type: text/plain; charset=UTF-8

Effectively this is persist without fault tolerance.
Failure of any node means complete lack of fault tolerance.
I would be very skeptical of truncating lineage if it is not reliable.
 On 17-May-2014 3:49 am, "Xiangrui Meng (JIRA)" <jira@apache.org> wrote:

> Xiangrui Meng created SPARK-1855:
> ------------------------------------
>
>              Summary: Provide memory-and-local-disk RDD checkpointing
>                  Key: SPARK-1855
>                  URL: https://issues.apache.org/jira/browse/SPARK-1855
>              Project: Spark
>           Issue Type: New Feature
>           Components: MLlib, Spark Core
>     Affects Versions: 1.0.0
>             Reporter: Xiangrui Meng
>
>
> Checkpointing is used to cut long lineage while maintaining fault
> tolerance. The current implementation is HDFS-based. Using the BlockRDD we
> can create in-memory-and-local-disk (with replication) checkpoints that are
> not as reliable as HDFS-based solution but faster.
>
> It can help applications that require many iterations.
>
>
>
> --
> This message was sent by Atlassian JIRA
> (v6.2#6252)
>

--001a11c115868a475904f98c6024--

From dev-return-7626-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 23:57:25 2014
Return-Path: <dev-return-7626-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0852811252
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 23:57:25 +0000 (UTC)
Received: (qmail 51279 invoked by uid 500); 16 May 2014 22:51:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22609 invoked by uid 500); 16 May 2014 22:38:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56823 invoked by uid 99); 16 May 2014 22:19:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 22:19:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mark@clearstorydata.com designates 74.125.82.180 as permitted sender)
Received: from [74.125.82.180] (HELO mail-we0-f180.google.com) (74.125.82.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 19:50:21 +0000
Received: by mail-we0-f180.google.com with SMTP id t61so3071120wes.11
        for <dev@spark.apache.org>; Fri, 16 May 2014 12:50:00 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=kilz0U+mXiiYP0bypoZMKdej2nZEKHJ4jrcudnNd+Wo=;
        b=Ds8Pg8rYEIxJyNGgvOITFlwtxC2DM3WNHyi4yP5gepG/CMOobFcduG6jB939+IPGP+
         Hdgx1w68pyN9Zz9q5yM6sUygRa5rvAMqmpLCNNh5sPLc/STK+Qdq0elPOad4PdBcmijw
         Smn2W3Ych1urdBx8bnBZzpHUyUBYQlkWNPTDvjfRQRTn0/PxUQ2ikLvu7DMqydy8bxGq
         FZ9Bq3HbpVjbNPQwlng1QszHUEM8+vmRUHw2RofRtXbjm5eMtJwerafSoDdLAS3r47jV
         BHtOA72dD0EDNoYRHUoP1sdWZRIna6iLSgdow3FQfcn/K2wj2xz0v636B1ji8AIoqRma
         qDZg==
X-Gm-Message-State: ALoCoQnPEDDf+y8AAADm0JWKq4eyOdotpKl62QBXwfzDRoTup2DGdKP+teYJ29ocHKxW4h8z4Czd
MIME-Version: 1.0
X-Received: by 10.194.157.68 with SMTP id wk4mr15667317wjb.42.1400269800144;
 Fri, 16 May 2014 12:50:00 -0700 (PDT)
Received: by 10.216.232.69 with HTTP; Fri, 16 May 2014 12:50:00 -0700 (PDT)
In-Reply-To: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
Date: Fri, 16 May 2014 12:50:00 -0700
Message-ID: <CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0112cbfc07177604f989b8a3
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0112cbfc07177604f989b8a3
Content-Type: text/plain; charset=UTF-8

+1, but just barely.  We've got quite a number of outstanding bugs
identified, and many of them have fixes in progress.  I'd hate to see those
efforts get lost in a post-1.0.0 flood of new features targeted at 1.1.0 --
in other words, I'd like to see 1.0.1 retain a high priority relative to
1.1.0.

Looking through the unresolved JIRAs, it doesn't look like any of the
identified bugs are show-stoppers or strictly regressions (although I will
note that one that I have in progress, SPARK-1749, is a bug that we
introduced with recent work -- it's not strictly a regression because we
had equally bad but different behavior when the DAGScheduler exceptions
weren't previously being handled at all vs. being slightly mis-handled
now), so I'm not currently seeing a reason not to release.


On Tue, May 13, 2014 at 1:36 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> Please vote on releasing the following candidate as Apache Spark version
> 1.0.0!
>
> The tag to be voted on is v1.0.0-rc5 (commit 18f0623):
>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=18f062303303824139998e8fc8f4158217b0dbc3
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.0.0-rc5/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1012/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/
>
> Please vote on releasing this package as Apache Spark 1.0.0!
>
> The vote is open until Friday, May 16, at 09:30 UTC and passes if a
> majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.0.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == API Changes ==
> We welcome users to compile Spark applications against 1.0. There are
> a few API changes in this release. Here are links to the associated
> upgrade guides - user facing changes have been kept as small as
> possible.
>
> changes to ML vector specification:
>
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10
>
> changes to the Java API:
>
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>
> changes to the streaming API:
>
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>
> changes to the GraphX API:
>
> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>
> coGroup and related functions now return Iterable[T] instead of Seq[T]
> ==> Call toSeq on the result to restore the old behavior
>
> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> ==> Call toSeq on the result to restore old behavior
>

--089e0112cbfc07177604f989b8a3--

From dev-return-7627-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 16 23:57:30 2014
Return-Path: <dev-return-7627-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 264B6112C3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 16 May 2014 23:57:30 +0000 (UTC)
Received: (qmail 76840 invoked by uid 500); 16 May 2014 22:56:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75117 invoked by uid 500); 16 May 2014 22:56:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54040 invoked by uid 99); 16 May 2014 22:54:57 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 22:54:57 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.182 as permitted sender)
Received: from [209.85.214.182] (HELO mail-ob0-f182.google.com) (209.85.214.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 22:47:08 +0000
Received: by mail-ob0-f182.google.com with SMTP id wn1so3710459obc.41
        for <dev@spark.apache.org>; Fri, 16 May 2014 15:46:48 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=K64y5LohY9GevkNFF7ho8WM+3MpEtTaDTkWF0koKEDk=;
        b=ksmAVBHWcuzIG9RyqNlqgkSvSHOACSVcvNVPT1fBu/54pyNPIG4tLduh35ay4tQSe9
         2+fefwKh2nD8jVTUmxQn+qG3mTiZ4AE5E35iINsyLPSQOea9tRTzVvpFjuRf9U4LRIz5
         lOMCnnjIKqjZtRWbnzubOWTFbGyan8tjClZR/kcJT/D/W44pvfYVcde+OCa1t3O76pSN
         W5gO/rbZzZX3L85PIFLv6IVlqBXGuHlf/XUcLzNFmNSt4uw7kIcdaBRJW7+cXnOX7tvd
         m1BWsYiRxF9kdzLrGDpsSmTsXkq2dLDUoj/GBPO33fgp4ZX/tl0OGr/nTE+ej5RfyT4N
         +qAg==
MIME-Version: 1.0
X-Received: by 10.60.44.243 with SMTP id h19mr20375850oem.46.1400280407846;
 Fri, 16 May 2014 15:46:47 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Fri, 16 May 2014 15:46:47 -0700 (PDT)
In-Reply-To: <CAAsvFPmdYk63D+UmAZ7rw9SKA7rsMGzqDBryPrz=_VF=5MGkRw@mail.gmail.com>
References: <CABPQxsu13Y1TnF_NhVZrYQG=DP94hfyvwCaTa7DAGp4zgHGiJg@mail.gmail.com>
	<CABPQxst+WESyBGi-fTdFXjVVmrVkvduE_1Uf6Zh+PbgYPbGCgA@mail.gmail.com>
	<CALuGr6bpepzhFNV3ery=0067xRO8FP9S2gVZf9SDQ54T652s6g@mail.gmail.com>
	<CANGvG8oFfLM-nXtxUCFQKwRCyuZ0cK+mpCWCG_PyFA32C_5r=Q@mail.gmail.com>
	<CALuGr6b6HbRd34vN4=uvWDKj-UKJMUs6seJUo2u0J-OgMomdyg@mail.gmail.com>
	<CAAsvFPmdYk63D+UmAZ7rw9SKA7rsMGzqDBryPrz=_VF=5MGkRw@mail.gmail.com>
Date: Fri, 16 May 2014 15:46:47 -0700
Message-ID: <CABPQxssubrQkoDj5s9+EFjqeM07nLPTXu5eO1Ryt2n92sEvfUw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc7)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey all,

My vote threads seem to be running about 24 hours behind and/or
getting swallowed by infra e-mail.

I sent RC8 yesterday and we might send one tonight as well. I'll make
sure to close all existing ones

There have been only small "polish" changes in the recent RC's since
RC5. So testing any off these should be pretty equivalent. I'll make
sure I close all the other threads by tonight.

- Patrick

On Fri, May 16, 2014 at 1:10 PM, Mark Hamstra <mark@clearstorydata.com> wrote:
> Sorry for the duplication, but I think this is the current VOTE candidate
> -- we're not voting on rc8 yet?
>
> +1, but just barely.  We've got quite a number of outstanding bugs
> identified, and many of them have fixes in progress.  I'd hate to see those
> efforts get lost in a post-1.0.0 flood of new features targeted at 1.1.0 --
> in other words, I'd like to see 1.0.1 retain a high priority relative to
> 1.1.0.
>
> Looking through the unresolved JIRAs, it doesn't look like any of the
> identified bugs are show-stoppers or strictly regressions (although I will
> note that one that I have in progress, SPARK-1749, is a bug that we
> introduced with recent work -- it's not strictly a regression because we
> had equally bad but different behavior when the DAGScheduler exceptions
> weren't previously being handled at all vs. being slightly mis-handled
> now), so I'm not currently seeing a reason not to release.
>
>
> On Fri, May 16, 2014 at 11:42 AM, Henry Saputra <henry.saputra@gmail.com>wrote:
>
>> Ah ok, thanks Aaron
>>
>> Just to make sure we VOTE the right RC.
>>
>> Thanks,
>>
>> Henry
>>
>> On Fri, May 16, 2014 at 11:37 AM, Aaron Davidson <ilikerps@gmail.com>
>> wrote:
>> > It was, but due to the apache infra issues, some may not have received
>> the
>> > email yet...
>> >
>> > On Fri, May 16, 2014 at 10:48 AM, Henry Saputra <henry.saputra@gmail.com
>> >
>> > wrote:
>> >>
>> >> Hi Patrick,
>> >>
>> >> Just want to make sure that VOTE for rc6 also cancelled?
>> >>
>> >>
>> >> Thanks,
>> >>
>> >> Henry
>> >>
>> >> On Thu, May 15, 2014 at 1:15 AM, Patrick Wendell <pwendell@gmail.com>
>> >> wrote:
>> >> > I'll start the voting with a +1.
>> >> >
>> >> > On Thu, May 15, 2014 at 1:14 AM, Patrick Wendell <pwendell@gmail.com>
>> >> > wrote:
>> >> >> Please vote on releasing the following candidate as Apache Spark
>> >> >> version 1.0.0!
>> >> >>
>> >> >> This patch has minor documentation changes and fixes on top of rc6.
>> >> >>
>> >> >> The tag to be voted on is v1.0.0-rc7 (commit 9212b3e):
>> >> >>
>> >> >>
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=9212b3e5bb5545ccfce242da8d89108e6fb1c464
>> >> >>
>> >> >> The release files, including signatures, digests, etc. can be found
>> at:
>> >> >> http://people.apache.org/~pwendell/spark-1.0.0-rc7/
>> >> >>
>> >> >> Release artifacts are signed with the following key:
>> >> >> https://people.apache.org/keys/committer/pwendell.asc
>> >> >>
>> >> >> The staging repository for this release can be found at:
>> >> >>
>> https://repository.apache.org/content/repositories/orgapachespark-1015
>> >> >>
>> >> >> The documentation corresponding to this release can be found at:
>> >> >> http://people.apache.org/~pwendell/spark-1.0.0-rc7-docs/
>> >> >>
>> >> >> Please vote on releasing this package as Apache Spark 1.0.0!
>> >> >>
>> >> >> The vote is open until Sunday, May 18, at 09:12 UTC and passes if a
>> >> >> majority of at least 3 +1 PMC votes are cast.
>> >> >>
>> >> >> [ ] +1 Release this package as Apache Spark 1.0.0
>> >> >> [ ] -1 Do not release this package because ...
>> >> >>
>> >> >> To learn more about Apache Spark, please see
>> >> >> http://spark.apache.org/
>> >> >>
>> >> >> == API Changes ==
>> >> >> We welcome users to compile Spark applications against 1.0. There are
>> >> >> a few API changes in this release. Here are links to the associated
>> >> >> upgrade guides - user facing changes have been kept as small as
>> >> >> possible.
>> >> >>
>> >> >> changes to ML vector specification:
>> >> >>
>> >> >>
>> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/mllib-guide.html#from-09-to-10
>> >> >>
>> >> >> changes to the Java API:
>> >> >>
>> >> >>
>> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>> >> >>
>> >> >> changes to the streaming API:
>> >> >>
>> >> >>
>> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>> >> >>
>> >> >> changes to the GraphX API:
>> >> >>
>> >> >>
>> http://people.apache.org/~pwendell/spark-1.0.0-rc5-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>> >> >>
>> >> >> coGroup and related functions now return Iterable[T] instead of
>> Seq[T]
>> >> >> ==> Call toSeq on the result to restore the old behavior
>> >> >>
>> >> >> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
>> >> >> ==> Call toSeq on the result to restore old behavior
>> >
>> >
>>

From dev-return-7628-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 00:12:33 2014
Return-Path: <dev-return-7628-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1770A113A7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 00:12:33 +0000 (UTC)
Received: (qmail 17028 invoked by uid 500); 16 May 2014 23:27:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52590 invoked by uid 500); 16 May 2014 23:12:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45612 invoked by uid 99); 16 May 2014 22:58:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 22:58:05 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mark@clearstorydata.com designates 209.85.212.173 as permitted sender)
Received: from [209.85.212.173] (HELO mail-wi0-f173.google.com) (209.85.212.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 16 May 2014 22:57:45 +0000
Received: by mail-wi0-f173.google.com with SMTP id bs8so1679130wib.12
        for <dev@spark.apache.org>; Fri, 16 May 2014 15:57:21 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=Uqt7mb0fKwfBsGJIaU1DaLztqwskn4cKiPWox2dgyXQ=;
        b=Uz4iYCxBgaOdBHa3I9A0dfmTKLU31saiT+QbGAIYf3aDuV+nc7DtTNAAyUvHHdFw/c
         EaiKy5bjs0+HOuec6pCkc31LQiAxXn58P5bziFtvmP1MppKx5DQ6Sf0l814fC1uGfVNJ
         ZzPfdA3U8Egqiwevf1oivLJd03jUBlMZKV/Qy/l0VIKwisVZrPAV0Ut3aeULyXOKVBWN
         jBvw4nn6ltd098Q6V0m1DBH/cYu8p9bUFUFASOFVkEgDJq9iS+UOXVUihZRj9p391TcB
         dlIt39Ql2xOEGN9r5PGMcZQJy2p/d0KB32bNlf+e+rgulcTjs+saewGhGR1wE1apN/Ov
         Yw4Q==
X-Gm-Message-State: ALoCoQnjJ2I33yg38cWMA/sUruH4V+GUOEh8C8KXsH+qBr92ZVWeRfXmfXisSDin3YtXvER1hpvK
MIME-Version: 1.0
X-Received: by 10.180.106.1 with SMTP id gq1mr365672wib.45.1400281041625; Fri,
 16 May 2014 15:57:21 -0700 (PDT)
Received: by 10.216.232.69 with HTTP; Fri, 16 May 2014 15:57:21 -0700 (PDT)
In-Reply-To: <CABPQxsvAw5x0d7mpXW5OePKpCg0DMmuQVDwkrE+UMk55EMf8EQ@mail.gmail.com>
References: <CABPQxsvAw5x0d7mpXW5OePKpCg0DMmuQVDwkrE+UMk55EMf8EQ@mail.gmail.com>
Date: Fri, 16 May 2014 15:57:21 -0700
Message-ID: <CAAsvFPnn0+4kawZrgJ-znLA+gmup+cjgp1vGpU1k1KsN9Lfk9Q@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc8)
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d0443069a12807d04f98c5607
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0443069a12807d04f98c5607
Content-Type: text/plain; charset=UTF-8

+1


On Fri, May 16, 2014 at 2:16 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> [Due to ASF e-mail outage, I'm not if anyone will actually receive this.]
>
> Please vote on releasing the following candidate as Apache Spark version
> 1.0.0!
> This has only minor changes on top of rc7.
>
> The tag to be voted on is v1.0.0-rc8 (commit 80eea0f):
>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=80eea0f111c06260ffaa780d2f3f7facd09c17bc
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.0.0-rc8/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1016/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/
>
> Please vote on releasing this package as Apache Spark 1.0.0!
>
> The vote is open until Monday, May 19, at 10:15 UTC and passes if a
> majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.0.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == API Changes ==
> We welcome users to compile Spark applications against 1.0. There are
> a few API changes in this release. Here are links to the associated
> upgrade guides - user facing changes have been kept as small as
> possible.
>
> changes to ML vector specification:
>
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#from-09-to-10
>
> changes to the Java API:
>
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>
> changes to the streaming API:
>
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>
> changes to the GraphX API:
>
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>
> coGroup and related functions now return Iterable[T] instead of Seq[T]
> ==> Call toSeq on the result to restore the old behavior
>
> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> ==> Call toSeq on the result to restore old behavior
>

--f46d0443069a12807d04f98c5607--

From dev-return-7630-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 02:27:21 2014
Return-Path: <dev-return-7630-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E33E511880
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 02:27:21 +0000 (UTC)
Received: (qmail 45556 invoked by uid 500); 17 May 2014 01:42:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63578 invoked by uid 500); 17 May 2014 01:27:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59326 invoked by uid 99); 17 May 2014 01:23:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 01:23:00 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.179] (HELO mail-qc0-f179.google.com) (209.85.216.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 01:22:57 +0000
Received: by mail-qc0-f179.google.com with SMTP id x3so5490755qcv.38
        for <dev@spark.apache.org>; Fri, 16 May 2014 18:22:34 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=Wzns1mGtPwgJzFCRNd2JyM8/oW1biBryiaY4g0Lfujg=;
        b=H9Rr5IM3K67LOxzG9g/K6riXRSGAgdi6mGIKvoSckhGLFrXvFnwoL9RY65QyD/CI6X
         VSwl9rUIHoGHCRaZ4k2GcqmryVsIpUR4Mqnnw8gqSQyKX+ORQ+O+AGxWHUp8PziGbJgA
         9jLPQwM6gax8w6BF+o+v1ycSVninIWgCA5lTw2ZI1j51dUpj28+pxKLaWHLfOGeDkTyW
         i+ydGPlQ3Sjtv7P0tR5dwybPrrgJ2caLkpoRv8a5hbaG50HFhj1UQZlfintlBfwwruVG
         r63uM50HpF4l35jA/exHKr2RuS8fZnO31nUYmt+S4Vpq88o1e0dFawE59Mv4MJUg7519
         OiwQ==
X-Gm-Message-State: ALoCoQnqNWf7mNZBh/DGGY3VvTi58e4qEXdVJJtsorM8XpInoOvRScLUtBZSLX8qqRgw5i4W6U0O
X-Received: by 10.224.95.73 with SMTP id c9mr28904956qan.68.1400289753997;
 Fri, 16 May 2014 18:22:33 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.209.73 with HTTP; Fri, 16 May 2014 18:22:13 -0700 (PDT)
In-Reply-To: <CAAsvFPnn0+4kawZrgJ-znLA+gmup+cjgp1vGpU1k1KsN9Lfk9Q@mail.gmail.com>
References: <CABPQxsvAw5x0d7mpXW5OePKpCg0DMmuQVDwkrE+UMk55EMf8EQ@mail.gmail.com>
 <CAAsvFPnn0+4kawZrgJ-znLA+gmup+cjgp1vGpU1k1KsN9Lfk9Q@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Fri, 16 May 2014 18:22:13 -0700
Message-ID: <CAAswR-52zpbMMTmwhdZsLkGRdc1mtKyf4O5GFzngEWqNH19_Pg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc8)
To: dev@spark.apache.org
Cc: Patrick Wendell <pwendell@gmail.com>
Content-Type: multipart/alternative; boundary=001a11c347aa5ebb9904f98e5df1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c347aa5ebb9904f98e5df1
Content-Type: text/plain; charset=UTF-8

-1

We found a regression in the way configuration is passed to executors.

https://issues.apache.org/jira/browse/SPARK-1864
https://github.com/apache/spark/pull/808

Michael


On Fri, May 16, 2014 at 3:57 PM, Mark Hamstra <mark@clearstorydata.com>wrote:

> +1
>
>
> On Fri, May 16, 2014 at 2:16 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> > [Due to ASF e-mail outage, I'm not if anyone will actually receive this.]
> >
> > Please vote on releasing the following candidate as Apache Spark version
> > 1.0.0!
> > This has only minor changes on top of rc7.
> >
> > The tag to be voted on is v1.0.0-rc8 (commit 80eea0f):
> >
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=80eea0f111c06260ffaa780d2f3f7facd09c17bc
> >
> > The release files, including signatures, digests, etc. can be found at:
> > http://people.apache.org/~pwendell/spark-1.0.0-rc8/
> >
> > Release artifacts are signed with the following key:
> > https://people.apache.org/keys/committer/pwendell.asc
> >
> > The staging repository for this release can be found at:
> > https://repository.apache.org/content/repositories/orgapachespark-1016/
> >
> > The documentation corresponding to this release can be found at:
> > http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/
> >
> > Please vote on releasing this package as Apache Spark 1.0.0!
> >
> > The vote is open until Monday, May 19, at 10:15 UTC and passes if a
> > majority of at least 3 +1 PMC votes are cast.
> >
> > [ ] +1 Release this package as Apache Spark 1.0.0
> > [ ] -1 Do not release this package because ...
> >
> > To learn more about Apache Spark, please see
> > http://spark.apache.org/
> >
> > == API Changes ==
> > We welcome users to compile Spark applications against 1.0. There are
> > a few API changes in this release. Here are links to the associated
> > upgrade guides - user facing changes have been kept as small as
> > possible.
> >
> > changes to ML vector specification:
> >
> >
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#from-09-to-10
> >
> > changes to the Java API:
> >
> >
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
> >
> > changes to the streaming API:
> >
> >
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
> >
> > changes to the GraphX API:
> >
> >
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
> >
> > coGroup and related functions now return Iterable[T] instead of Seq[T]
> > ==> Call toSeq on the result to restore the old behavior
> >
> > SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> > ==> Call toSeq on the result to restore old behavior
> >
>

--001a11c347aa5ebb9904f98e5df1--

From dev-return-7631-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 02:58:25 2014
Return-Path: <dev-return-7631-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9BBA111965
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 02:58:25 +0000 (UTC)
Received: (qmail 78032 invoked by uid 500); 17 May 2014 02:42:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49738 invoked by uid 500); 17 May 2014 02:26:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48868 invoked by uid 99); 17 May 2014 02:23:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 02:23:01 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 74.125.82.169 as permitted sender)
Received: from [74.125.82.169] (HELO mail-we0-f169.google.com) (74.125.82.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 02:22:57 +0000
Received: by mail-we0-f169.google.com with SMTP id u56so3386846wes.0
        for <dev@spark.apache.org>; Fri, 16 May 2014 19:22:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=+hZcxYu1Bo7pCDHM6c8pGIHRt4OikHn2mtk8ksZ26ks=;
        b=wp3pVjnPULN4o5biGl7lQaClMprHptVlnC8ZMsXPIKnO67fLfZ/BXe/L+sub4cD5aS
         uO4llYnUhlHDumjS1dK+BpaxyssqgP30sIoVs3B5VIGDr4XrEOT8IqCwlgSOU7gjEWgd
         vFwsCMagu7KLcqmhao9Azj30ww4mwfAH7m6bZGg/aE7/A5PAMJ4yYa5ePcPQORjMvShr
         VbzJMBVRF24w0PkZrDhZeS7zQ/7otMRmmzromySRZA7Gdp7p/LsONfMNJkBQvpvJL0m3
         FI8kPuZcvbV66xO+ZApo+k4DZ+rVBvN4m/dpKFGTP4Il8YQnru1c/UUyklZNb/tKsaQE
         Ifzg==
MIME-Version: 1.0
X-Received: by 10.194.175.70 with SMTP id by6mr17083671wjc.3.1400293354156;
 Fri, 16 May 2014 19:22:34 -0700 (PDT)
Received: by 10.194.187.20 with HTTP; Fri, 16 May 2014 19:22:34 -0700 (PDT)
In-Reply-To: <CAJiQeY+zsu3cfX1zyA=qK1KO8hirddmjGSB7HohTTA_a9jk3wQ@mail.gmail.com>
References: <JIRA.12714626.1400189782796@arcas>
	<JIRA.12714626.1400189782796.363092.1400238099511@arcas>
	<CAJiQeY+zsu3cfX1zyA=qK1KO8hirddmjGSB7HohTTA_a9jk3wQ@mail.gmail.com>
Date: Fri, 16 May 2014 19:22:34 -0700
Message-ID: <CAJgQjQ_1qrz4=05E=Wwj+O4UqKHwFKWQQiZosKbPjvwocMz9mw@mail.gmail.com>
Subject: Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

With 3x replication, we should be able to achieve fault tolerance.
This checkPointed RDD can be cleared if we have another in-memory
checkPointed RDD down the line. It can avoid hitting disk if we have
enough memory to use. We need to investigate more to find a good
solution. -Xiangrui

On Fri, May 16, 2014 at 4:00 PM, Mridul Muralidharan <mridul@gmail.com> wrote:
> Effectively this is persist without fault tolerance.
> Failure of any node means complete lack of fault tolerance.
> I would be very skeptical of truncating lineage if it is not reliable.
>  On 17-May-2014 3:49 am, "Xiangrui Meng (JIRA)" <jira@apache.org> wrote:
>
>> Xiangrui Meng created SPARK-1855:
>> ------------------------------------
>>
>>              Summary: Provide memory-and-local-disk RDD checkpointing
>>                  Key: SPARK-1855
>>                  URL: https://issues.apache.org/jira/browse/SPARK-1855
>>              Project: Spark
>>           Issue Type: New Feature
>>           Components: MLlib, Spark Core
>>     Affects Versions: 1.0.0
>>             Reporter: Xiangrui Meng
>>
>>
>> Checkpointing is used to cut long lineage while maintaining fault
>> tolerance. The current implementation is HDFS-based. Using the BlockRDD we
>> can create in-memory-and-local-disk (with replication) checkpoints that are
>> not as reliable as HDFS-based solution but faster.
>>
>> It can help applications that require many iterations.
>>
>>
>>
>> --
>> This message was sent by Atlassian JIRA
>> (v6.2#6252)
>>

From dev-return-7632-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 08:23:53 2014
Return-Path: <dev-return-7632-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1095111F52
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 08:23:53 +0000 (UTC)
Received: (qmail 62470 invoked by uid 500); 17 May 2014 08:09:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30571 invoked by uid 500); 17 May 2014 07:57:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26352 invoked by uid 99); 17 May 2014 07:51:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 07:51:48 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.169 as permitted sender)
Received: from [209.85.214.169] (HELO mail-ob0-f169.google.com) (209.85.214.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 07:51:45 +0000
Received: by mail-ob0-f169.google.com with SMTP id vb8so4020494obc.14
        for <dev@spark.apache.org>; Sat, 17 May 2014 00:51:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=yoHgMHOW68Ax2OHke23L8AIxehj42kP/pVM/ZqFk1m4=;
        b=T7h7UXEGsbXBMMVHWCngbrnZy2l33UiIWtp7AJ0EBRaKo7HLsIdWpiqqDryRCnAila
         7N4sshGZHy5RT5ieDk+vRAdxy07QvB5T7M1KyBZvAVofkl21kze3mHtg0V9HqmC0btLk
         vgobq/z0/V7AAjFwHqwmSxEY6UZHOIorAubnd/UIDQVLUryBZqxAKwlpzk4F8cHJN8qd
         SZ3VPjOkSeKZ7ev3NDAo8Dz4eD2Iwh85Nq6Z1J89HsguMnZOXVzAWMPw/PknKPkAJl/8
         YL7L62BImYlbuFklK3hPHkPPCKCtG5EGxZG9FJO4pjsFMdSlNyE2gqXnoPnRFkiWwjID
         FWSQ==
MIME-Version: 1.0
X-Received: by 10.182.163.45 with SMTP id yf13mr22383930obb.66.1400313082196;
 Sat, 17 May 2014 00:51:22 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Sat, 17 May 2014 00:51:22 -0700 (PDT)
In-Reply-To: <CAAswR-52zpbMMTmwhdZsLkGRdc1mtKyf4O5GFzngEWqNH19_Pg@mail.gmail.com>
References: <CABPQxsvAw5x0d7mpXW5OePKpCg0DMmuQVDwkrE+UMk55EMf8EQ@mail.gmail.com>
	<CAAsvFPnn0+4kawZrgJ-znLA+gmup+cjgp1vGpU1k1KsN9Lfk9Q@mail.gmail.com>
	<CAAswR-52zpbMMTmwhdZsLkGRdc1mtKyf4O5GFzngEWqNH19_Pg@mail.gmail.com>
Date: Sat, 17 May 2014 00:51:22 -0700
Message-ID: <CABPQxsvT=iqud0iOLc_95XwL5Ut6Z-1BKgZMxJR=kJ9dXOFFhQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc8)
From: Patrick Wendell <pwendell@gmail.com>
To: Michael Armbrust <michael@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Due to the issue discovered by Michael, this vote is cancelled in favor of rc9.

On Fri, May 16, 2014 at 6:22 PM, Michael Armbrust
<michael@databricks.com> wrote:
> -1
>
> We found a regression in the way configuration is passed to executors.
>
> https://issues.apache.org/jira/browse/SPARK-1864
> https://github.com/apache/spark/pull/808
>
> Michael
>
>
> On Fri, May 16, 2014 at 3:57 PM, Mark Hamstra <mark@clearstorydata.com>
> wrote:
>>
>> +1
>>
>>
>> On Fri, May 16, 2014 at 2:16 AM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>> > [Due to ASF e-mail outage, I'm not if anyone will actually receive
>> > this.]
>> >
>> > Please vote on releasing the following candidate as Apache Spark version
>> > 1.0.0!
>> > This has only minor changes on top of rc7.
>> >
>> > The tag to be voted on is v1.0.0-rc8 (commit 80eea0f):
>> >
>> >
>> > https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=80eea0f111c06260ffaa780d2f3f7facd09c17bc
>> >
>> > The release files, including signatures, digests, etc. can be found at:
>> > http://people.apache.org/~pwendell/spark-1.0.0-rc8/
>> >
>> > Release artifacts are signed with the following key:
>> > https://people.apache.org/keys/committer/pwendell.asc
>> >
>> > The staging repository for this release can be found at:
>> > https://repository.apache.org/content/repositories/orgapachespark-1016/
>> >
>> > The documentation corresponding to this release can be found at:
>> > http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/
>> >
>> > Please vote on releasing this package as Apache Spark 1.0.0!
>> >
>> > The vote is open until Monday, May 19, at 10:15 UTC and passes if a
>> > majority of at least 3 +1 PMC votes are cast.
>> >
>> > [ ] +1 Release this package as Apache Spark 1.0.0
>> > [ ] -1 Do not release this package because ...
>> >
>> > To learn more about Apache Spark, please see
>> > http://spark.apache.org/
>> >
>> > == API Changes ==
>> > We welcome users to compile Spark applications against 1.0. There are
>> > a few API changes in this release. Here are links to the associated
>> > upgrade guides - user facing changes have been kept as small as
>> > possible.
>> >
>> > changes to ML vector specification:
>> >
>> >
>> > http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#from-09-to-10
>> >
>> > changes to the Java API:
>> >
>> >
>> > http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>> >
>> > changes to the streaming API:
>> >
>> >
>> > http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>> >
>> > changes to the GraphX API:
>> >
>> >
>> > http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>> >
>> > coGroup and related functions now return Iterable[T] instead of Seq[T]
>> > ==> Call toSeq on the result to restore the old behavior
>> >
>> > SparkContext.jarOfClass returns Option[String] instead of Seq[String]
>> > ==> Call toSeq on the result to restore old behavior
>> >
>
>

From dev-return-7635-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 08:34:53 2014
Return-Path: <dev-return-7635-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C6DA111F8C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 08:34:53 +0000 (UTC)
Received: (qmail 13696 invoked by uid 500); 17 May 2014 08:34:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64803 invoked by uid 500); 17 May 2014 08:09:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49420 invoked by uid 99); 17 May 2014 07:59:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 07:59:03 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.181 as permitted sender)
Received: from [209.85.214.181] (HELO mail-ob0-f181.google.com) (209.85.214.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 07:58:58 +0000
Received: by mail-ob0-f181.google.com with SMTP id wm4so4062721obc.40
        for <dev@spark.apache.org>; Sat, 17 May 2014 00:58:38 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=MO+uvwmnMCdtXaULOkDPjfeJmK7jUB20FuMnI0HhXA0=;
        b=HsfXZ9hhHXQc7rEW63IA3QvlMDHHEgUl2ZFABIBWdgx790OYQHB4qyvejOh7L2cD77
         cCEV2hLrQ8Wk/6BRNcXd8Idc0/uflA72grOZAvMUNRpQh9ZxuJGFUtpnlwgY3ffmnsj6
         /oAbGA3g1qgdPg56kibCehJavkCmArJ42Ttj3M2mV/TVhDdIdD5XjVj5HOwQZTd89rPy
         aYAWSm2yMmdRN4oDl3KkCGn+R7vA3oc61GqFkkZjOerz/7nmx8EIxMmAphp9zzS9slHD
         +h20tiujb5xzvI8RJwH6CfUc7gZNE/k36QrbQzaUM1K3hmapT+bmrsltRJ3QfeoBMdy2
         0GNg==
MIME-Version: 1.0
X-Received: by 10.60.179.138 with SMTP id dg10mr22236717oec.13.1400313518142;
 Sat, 17 May 2014 00:58:38 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Sat, 17 May 2014 00:58:38 -0700 (PDT)
In-Reply-To: <CABPQxstaxcbGSQTqUxSSUQfRpjQs=BqRkrEVvzOXbXZo8kzShg@mail.gmail.com>
References: <CABPQxstaxcbGSQTqUxSSUQfRpjQs=BqRkrEVvzOXbXZo8kzShg@mail.gmail.com>
Date: Sat, 17 May 2014 00:58:38 -0700
Message-ID: <CABPQxstbwN64rQvx6kUoESkO4GJOohYSAO+s_F85wFWjb6T1zw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc9)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I'll start the voting with a +1.

On Sat, May 17, 2014 at 12:58 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.0.0!
> This has one bug fix and one minor feature on top of rc8:
> SPARK-1864: https://github.com/apache/spark/pull/808
> SPARK-1808: https://github.com/apache/spark/pull/799
>
> The tag to be voted on is v1.0.0-rc9 (commit 920f947):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=920f947eb5a22a679c0c3186cf69ee75f6041c75
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.0.0-rc9/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1017/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.0.0-rc9-docs/
>
> Please vote on releasing this package as Apache Spark 1.0.0!
>
> The vote is open until Tuesday, May 20, at 08:56 UTC and passes if
> amajority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.0.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == API Changes ==
> We welcome users to compile Spark applications against 1.0. There are
> a few API changes in this release. Here are links to the associated
> upgrade guides - user facing changes have been kept as small as
> possible.
>
> changes to ML vector specification:
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#from-09-to-10
>
> changes to the Java API:
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>
> changes to the streaming API:
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>
> changes to the GraphX API:
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>
> coGroup and related functions now return Iterable[T] instead of Seq[T]
> ==> Call toSeq on the result to restore the old behavior
>
> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> ==> Call toSeq on the result to restore old behavior

From dev-return-7634-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 08:34:55 2014
Return-Path: <dev-return-7634-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DC8DB11FA3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 08:34:55 +0000 (UTC)
Received: (qmail 64676 invoked by uid 500); 17 May 2014 08:09:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62701 invoked by uid 500); 17 May 2014 08:09:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49399 invoked by uid 99); 17 May 2014 07:58:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 07:58:36 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.176 as permitted sender)
Received: from [209.85.214.176] (HELO mail-ob0-f176.google.com) (209.85.214.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 07:58:33 +0000
Received: by mail-ob0-f176.google.com with SMTP id wo20so4017735obc.21
        for <dev@spark.apache.org>; Sat, 17 May 2014 00:58:09 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=29efVXNuCrnirXvPjduOhk+vlTB+H8O6AzOc5mcASnQ=;
        b=lu7i4wL6/dygvhx769k4TV74sbcF5sVR61c0YxZKlC47dJTbH4E9tMdyXx5G7QH2SU
         kzwbpnsAxw8x67wZhIa6qV691Zvbplggym6lYfHgDQ+VdOx8paBzPvaxkY9clZ/EXxOI
         rLhjWOzVZoxvLx9Mhp2D16m2A6ZOYBp4xAZEEBlYvN0DkVfeFOlXN7jUL0YB3j5OG9ow
         WmWRMwK4AO0tz/X4o3i5XtGifDA70BgsefdYApfDUJlSMmFjq2+3Xj3xZIEarvq13aFB
         z0JOJcC4wLWUaXQbbzF2OOcny8JgvB87msep+BLheFrjd0VG36FD25q/zPQSVQeOhYVt
         V+3w==
MIME-Version: 1.0
X-Received: by 10.182.33.99 with SMTP id q3mr22460779obi.33.1400313489652;
 Sat, 17 May 2014 00:58:09 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Sat, 17 May 2014 00:58:09 -0700 (PDT)
Date: Sat, 17 May 2014 00:58:09 -0700
Message-ID: <CABPQxstaxcbGSQTqUxSSUQfRpjQs=BqRkrEVvzOXbXZo8kzShg@mail.gmail.com>
Subject: [VOTE] Release Apache Spark 1.0.0 (rc9)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Please vote on releasing the following candidate as Apache Spark version 1.0.0!
This has one bug fix and one minor feature on top of rc8:
SPARK-1864: https://github.com/apache/spark/pull/808
SPARK-1808: https://github.com/apache/spark/pull/799

The tag to be voted on is v1.0.0-rc9 (commit 920f947):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=920f947eb5a22a679c0c3186cf69ee75f6041c75

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc9/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1017/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.0.0-rc9-docs/

Please vote on releasing this package as Apache Spark 1.0.0!

The vote is open until Tuesday, May 20, at 08:56 UTC and passes if
amajority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.0.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== API Changes ==
We welcome users to compile Spark applications against 1.0. There are
a few API changes in this release. Here are links to the associated
upgrade guides - user facing changes have been kept as small as
possible.

changes to ML vector specification:
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#from-09-to-10

changes to the Java API:
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark

changes to the streaming API:
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x

changes to the GraphX API:
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091

coGroup and related functions now return Iterable[T] instead of Seq[T]
==> Call toSeq on the result to restore the old behavior

SparkContext.jarOfClass returns Option[String] instead of Seq[String]
==> Call toSeq on the result to restore old behavior

From dev-return-7633-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 08:49:54 2014
Return-Path: <dev-return-7633-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4BBFC11FC8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 08:49:54 +0000 (UTC)
Received: (qmail 61537 invoked by uid 500); 17 May 2014 08:09:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32052 invoked by uid 500); 17 May 2014 07:57:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26367 invoked by uid 99); 17 May 2014 07:52:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 07:52:11 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.175 as permitted sender)
Received: from [209.85.214.175] (HELO mail-ob0-f175.google.com) (209.85.214.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 07:52:07 +0000
Received: by mail-ob0-f175.google.com with SMTP id wo20so4005485obc.20
        for <dev@spark.apache.org>; Sat, 17 May 2014 00:51:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=PE3SHsfTSaQ8gU7r25oLHXrmpT1UKr1QQzRrnUVjmoE=;
        b=k3Bds/FJCMh9Cs9I8T0tTgCdvPR82cNAB5Nf2TpKAZVq3A5+13ifPAIl/zwvG2Z5DZ
         ZyLgZTyzXrGn8i35ozTB1ptPOE9L5wHAVUf1arDCNwg4b5BAPw5U8b6VXEMuHRGw0A8D
         ek7Fetd0lzR78oBi8qms1KVZkeAPWnNG6UM1k6ueOoHbFCPVHGMHNKwFNTjWHCQ/NgG5
         M9eVwVPJHIc+y++DAqAag5PU7qwYc6wnZX5I7O4U9nuySyflu7oCtpv4z+Mp3Jmx2ThF
         rfry2FB7IhyOtfgnGlqsD16ht2X3gdg2qB0iVOpe2eekHpYblkfmwQIa2CrZRiK1JJXc
         c0+w==
MIME-Version: 1.0
X-Received: by 10.60.54.38 with SMTP id g6mr625703oep.79.1400313106994; Sat,
 17 May 2014 00:51:46 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Sat, 17 May 2014 00:51:46 -0700 (PDT)
Date: Sat, 17 May 2014 00:51:46 -0700
Message-ID: <CABPQxss8iSDeugeeONoQpoff-sDKPc3TwamP2p-XpYt+bntWnA@mail.gmail.com>
Subject: [RESULT] [VOTE] Release Apache Spark 1.0.0 (rc8)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Cancelled in favor of rc9.

On Sat, May 17, 2014 at 12:51 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> Due to the issue discovered by Michael, this vote is cancelled in favor of rc9.
>
> On Fri, May 16, 2014 at 6:22 PM, Michael Armbrust
> <michael@databricks.com> wrote:
>> -1
>>
>> We found a regression in the way configuration is passed to executors.
>>
>> https://issues.apache.org/jira/browse/SPARK-1864
>> https://github.com/apache/spark/pull/808
>>
>> Michael
>>
>>
>> On Fri, May 16, 2014 at 3:57 PM, Mark Hamstra <mark@clearstorydata.com>
>> wrote:
>>>
>>> +1
>>>
>>>
>>> On Fri, May 16, 2014 at 2:16 AM, Patrick Wendell <pwendell@gmail.com>
>>> wrote:
>>>
>>> > [Due to ASF e-mail outage, I'm not if anyone will actually receive
>>> > this.]
>>> >
>>> > Please vote on releasing the following candidate as Apache Spark version
>>> > 1.0.0!
>>> > This has only minor changes on top of rc7.
>>> >
>>> > The tag to be voted on is v1.0.0-rc8 (commit 80eea0f):
>>> >
>>> >
>>> > https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=80eea0f111c06260ffaa780d2f3f7facd09c17bc
>>> >
>>> > The release files, including signatures, digests, etc. can be found at:
>>> > http://people.apache.org/~pwendell/spark-1.0.0-rc8/
>>> >
>>> > Release artifacts are signed with the following key:
>>> > https://people.apache.org/keys/committer/pwendell.asc
>>> >
>>> > The staging repository for this release can be found at:
>>> > https://repository.apache.org/content/repositories/orgapachespark-1016/
>>> >
>>> > The documentation corresponding to this release can be found at:
>>> > http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/
>>> >
>>> > Please vote on releasing this package as Apache Spark 1.0.0!
>>> >
>>> > The vote is open until Monday, May 19, at 10:15 UTC and passes if a
>>> > majority of at least 3 +1 PMC votes are cast.
>>> >
>>> > [ ] +1 Release this package as Apache Spark 1.0.0
>>> > [ ] -1 Do not release this package because ...
>>> >
>>> > To learn more about Apache Spark, please see
>>> > http://spark.apache.org/
>>> >
>>> > == API Changes ==
>>> > We welcome users to compile Spark applications against 1.0. There are
>>> > a few API changes in this release. Here are links to the associated
>>> > upgrade guides - user facing changes have been kept as small as
>>> > possible.
>>> >
>>> > changes to ML vector specification:
>>> >
>>> >
>>> > http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#from-09-to-10
>>> >
>>> > changes to the Java API:
>>> >
>>> >
>>> > http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>>> >
>>> > changes to the streaming API:
>>> >
>>> >
>>> > http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>>> >
>>> > changes to the GraphX API:
>>> >
>>> >
>>> > http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>>> >
>>> > coGroup and related functions now return Iterable[T] instead of Seq[T]
>>> > ==> Call toSeq on the result to restore the old behavior
>>> >
>>> > SparkContext.jarOfClass returns Option[String] instead of Seq[String]
>>> > ==> Call toSeq on the result to restore old behavior
>>> >
>>
>>

From dev-return-7636-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 09:24:56 2014
Return-Path: <dev-return-7636-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 29DE311045
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 09:24:56 +0000 (UTC)
Received: (qmail 34155 invoked by uid 500); 17 May 2014 08:59:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16934 invoked by uid 500); 17 May 2014 08:34:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13079 invoked by uid 99); 17 May 2014 08:30:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 08:30:16 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.192.42 as permitted sender)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 08:30:12 +0000
Received: by mail-qg0-f42.google.com with SMTP id q107so5890112qgd.1
        for <dev@spark.apache.org>; Sat, 17 May 2014 01:29:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=furQClHzfkKBUUm6nzlTjREKFSKw5HasuxZOPjklj5c=;
        b=pg5nfmnXQOhi113sCYa+LyMOTWHBfLLoiir/iEdPaCpc2gWm+YItbmVPr1qiUaFrXN
         cXqlEQuKAUJdga2YXVEAvPIAYtlGI+YrIKgxPDMZnWlgJm3pefyV6B/z7BO4/MfM2DsI
         PKhTynNk8wfsBAIKgZGZTaI/yBqpOdRomdzAPr6iwcZkCggeGfli+n3fppztybcSm66c
         KB9ZW3wZMv4Tb+mgpWoKOwIAv5nEnV/wMXb1E4QGi/MIA05QLE1OdMwQmMum+UdXqi6O
         vZvsdnQTns4QMewRquR803KFhIb9PS4Unuy7tsJBSM54tKk84m7zc2UsU0Fvo2Ca6evY
         uwbg==
MIME-Version: 1.0
X-Received: by 10.224.60.137 with SMTP id p9mr29737731qah.92.1400315389663;
 Sat, 17 May 2014 01:29:49 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Sat, 17 May 2014 01:29:49 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Sat, 17 May 2014 01:29:49 -0700 (PDT)
In-Reply-To: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
Date: Sat, 17 May 2014 13:59:49 +0530
Message-ID: <CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1133d8945fe07004f994552d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133d8945fe07004f994552d
Content-Type: text/plain; charset=UTF-8

Can you try moving your mapPartitions to another class/object which is
referenced only after sc.addJar ?

I would suspect CNFEx is coming while loading the class containing
mapPartitions before addJars is executed.

In general though, dynamic loading of classes means you use reflection to
instantiate it since expectation is you don't know which implementation
provides the interface ... If you statically know it apriori, you bundle it
in your classpath.

Regards
Mridul
On 17-May-2014 7:28 am, "DB Tsai" <dbtsai@stanford.edu> wrote:

> Finally find a way out of the ClassLoader maze! It took me some times to
> understand how it works; I think it worths to document it in a separated
> thread.
>
> We're trying to add external utility.jar which contains CSVRecordParser,
> and we added the jar to executors through sc.addJar APIs.
>
> If the instance of CSVRecordParser is created without reflection, it
> raises *ClassNotFound
> Exception*.
>
> data.mapPartitions(lines => {
>     val csvParser = new CSVRecordParser((delimiter.charAt(0))
>     lines.foreach(line => {
>       val lineElems = csvParser.parseLine(line)
>     })
>     ...
>     ...
>  )
>
>
> If the instance of CSVRecordParser is created through reflection, it works.
>
> data.mapPartitions(lines => {
>     val loader = Thread.currentThread.getContextClassLoader
>     val CSVRecordParser =
>         loader.loadClass("com.alpine.hadoop.ext.CSVRecordParser")
>
>     val csvParser = CSVRecordParser.getConstructor(Character.TYPE)
>         .newInstance(delimiter.charAt(0).asInstanceOf[Character])
>
>     val parseLine = CSVRecordParser
>         .getDeclaredMethod("parseLine", classOf[String])
>
>     lines.foreach(line => {
>        val lineElems = parseLine.invoke(csvParser,
> line).asInstanceOf[Array[String]]
>     })
>     ...
>     ...
>  )
>
>
> This is identical to this question,
>
> http://stackoverflow.com/questions/7452411/thread-currentthread-setcontextclassloader-without-using-reflection
>
> It's not intuitive for users to load external classes through reflection,
> but couple available solutions including 1) messing around
> systemClassLoader by calling systemClassLoader.addURI through reflection or
> 2) forking another JVM to add jars into classpath before bootstrap loader
> are very tricky.
>
> Any thought on fixing it properly?
>
> @Xiangrui,
> netlib-java jniloader is loaded from netlib-java through reflection, so
> this problem will not be seen.
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>

--001a1133d8945fe07004f994552d--

From dev-return-7637-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 09:33:00 2014
Return-Path: <dev-return-7637-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B62A21106F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 09:33:00 +0000 (UTC)
Received: (qmail 49797 invoked by uid 500); 17 May 2014 09:24:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35537 invoked by uid 500); 17 May 2014 08:59:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26276 invoked by uid 99); 17 May 2014 08:37:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 08:37:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.216.181 as permitted sender)
Received: from [209.85.216.181] (HELO mail-qc0-f181.google.com) (209.85.216.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 08:37:17 +0000
Received: by mail-qc0-f181.google.com with SMTP id m20so5842824qcx.26
        for <dev@spark.apache.org>; Sat, 17 May 2014 01:36:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=B+nCbkevJR5nqlu7YEriuOYk0kNOg8/Dqnt0ti7KYpM=;
        b=SnVB8mvJHQakBloZ2OZxzWzhk90ij+rTHZt/OE1PnzOewhlLsdSAtf/kTMuwcEnrPs
         XL1hDknPlKbjdaV2zrlOrsjVfxd/3vYMCbJe5KeB1IndKqKqRGxM0MpPXRND8rK9KSNl
         bPR0a9GzpqEEP3CsdAM0PHiQ77YYgJv/50Axgbeu9wneWnKlxZLKPXNS15duHIATwfBr
         grJqAqBlv+w5n34PBjEEZpAjcPEvg5ckRg8gRcBRYkYMy9w5dGAQScgJcXmB30K3GMf3
         UwPR1U9Qt01GkNvdxauWsfFNhRfeYIWQxEwmVl+R9+QVO6p0ZYqO+Lb9qljCWuARZPMr
         Gb9w==
MIME-Version: 1.0
X-Received: by 10.140.94.179 with SMTP id g48mr30990201qge.58.1400315813756;
 Sat, 17 May 2014 01:36:53 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Sat, 17 May 2014 01:36:53 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Sat, 17 May 2014 01:36:53 -0700 (PDT)
In-Reply-To: <CAJgQjQ_1qrz4=05E=Wwj+O4UqKHwFKWQQiZosKbPjvwocMz9mw@mail.gmail.com>
References: <JIRA.12714626.1400189782796@arcas>
	<JIRA.12714626.1400189782796.363092.1400238099511@arcas>
	<CAJiQeY+zsu3cfX1zyA=qK1KO8hirddmjGSB7HohTTA_a9jk3wQ@mail.gmail.com>
	<CAJgQjQ_1qrz4=05E=Wwj+O4UqKHwFKWQQiZosKbPjvwocMz9mw@mail.gmail.com>
Date: Sat, 17 May 2014 14:06:53 +0530
Message-ID: <CAJiQeY+eGxSzM-6C_poKCY53Gkixg85rdXiXXx+qi0z0bvCrrQ@mail.gmail.com>
Subject: Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113ab3eca707de04f9946e66
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ab3eca707de04f9946e66
Content-Type: text/plain; charset=UTF-8

We don't have 3x replication in spark :-)
And if we use replicated storagelevel, while decreasing odds of failure, it
does not eliminate it (since we are not doing a great job with replication
anyway from fault tolerance point of view).
Also it does take a nontrivial performance hit with replicated levels.

Regards,
Mridul
 On 17-May-2014 8:16 am, "Xiangrui Meng" <mengxr@gmail.com> wrote:

> With 3x replication, we should be able to achieve fault tolerance.
> This checkPointed RDD can be cleared if we have another in-memory
> checkPointed RDD down the line. It can avoid hitting disk if we have
> enough memory to use. We need to investigate more to find a good
> solution. -Xiangrui
>
> On Fri, May 16, 2014 at 4:00 PM, Mridul Muralidharan <mridul@gmail.com>
> wrote:
> > Effectively this is persist without fault tolerance.
> > Failure of any node means complete lack of fault tolerance.
> > I would be very skeptical of truncating lineage if it is not reliable.
> >  On 17-May-2014 3:49 am, "Xiangrui Meng (JIRA)" <jira@apache.org> wrote:
> >
> >> Xiangrui Meng created SPARK-1855:
> >> ------------------------------------
> >>
> >>              Summary: Provide memory-and-local-disk RDD checkpointing
> >>                  Key: SPARK-1855
> >>                  URL: https://issues.apache.org/jira/browse/SPARK-1855
> >>              Project: Spark
> >>           Issue Type: New Feature
> >>           Components: MLlib, Spark Core
> >>     Affects Versions: 1.0.0
> >>             Reporter: Xiangrui Meng
> >>
> >>
> >> Checkpointing is used to cut long lineage while maintaining fault
> >> tolerance. The current implementation is HDFS-based. Using the BlockRDD
> we
> >> can create in-memory-and-local-disk (with replication) checkpoints that
> are
> >> not as reliable as HDFS-based solution but faster.
> >>
> >> It can help applications that require many iterations.
> >>
> >>
> >>
> >> --
> >> This message was sent by Atlassian JIRA
> >> (v6.2#6252)
> >>
>

--001a113ab3eca707de04f9946e66--

From dev-return-7638-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 09:57:58 2014
Return-Path: <dev-return-7638-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5088D1109B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 09:57:58 +0000 (UTC)
Received: (qmail 70002 invoked by uid 500); 17 May 2014 09:49:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50498 invoked by uid 500); 17 May 2014 09:24:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44454 invoked by uid 99); 17 May 2014 09:19:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 09:19:00 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.220.178 as permitted sender)
Received: from [209.85.220.178] (HELO mail-vc0-f178.google.com) (209.85.220.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 09:18:56 +0000
Received: by mail-vc0-f178.google.com with SMTP id hq16so7407598vcb.37
        for <dev@spark.apache.org>; Sat, 17 May 2014 02:18:35 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=haVuDfTLQLobG506rqS8lRRVpJedxjWkQK0+7/4CLos=;
        b=JvF9S9OVnLszTCkG9CoWSgco1h1O8QNPI5Pkz8ezshxeKlEazlYPIouO3hEe9NaZwj
         LU9o8+9nS4mEjhd7upOMGOyAT9dIVgSbTbmAju0BqGEv+6agF1VIJjwJRwaS3A4h8Aed
         brZWLFc2Mqi2KEKQWj2KMH83d+LZv0pKwJ9z+xvusknxwAE+zxmiau/gYEwzPbV1+q15
         QRTAnrz0qv+U3RBIOJJ2U35OP+tErWYaIDRorHRPxGr3K38wweCHpNn4OEFuzcNR1dIR
         YJ4eKWDlT9c5B5o6HlJd2kwJ89GSW/90nk9w1crPFOBmJpeq9pEDT2BHgnUKTYYpGoli
         7ksw==
X-Gm-Message-State: ALoCoQkEDtLjvYCGQG3t0lMP2Aw/Umy6Nhwe/ll4qKADDuWGvNI7/QxwFuiH4/1Yw9ZUoaDoQNkd
X-Received: by 10.52.228.134 with SMTP id si6mr1400175vdc.5.1400318315533;
 Sat, 17 May 2014 02:18:35 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.111.69 with HTTP; Sat, 17 May 2014 02:18:15 -0700 (PDT)
In-Reply-To: <CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
 <CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Sat, 17 May 2014 10:18:15 +0100
Message-ID: <CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On this note, non-binding commentary:

Releases happen in local minima of change, usually created by
internally enforced code freeze. Spark is incredibly busy now due to
external factors -- recently a TLP, recently discovered by a large new
audience, ease of contribution enabled by Github. It's getting like
the first year of mainstream battle-testing in a month. It's been very
hard to freeze anything! I see a number of non-trivial issues being
reported, and I don't think it has been possible to triage all of
them, even.

Given the high rate of change, my instinct would have been to release
0.10.0 now. But won't it always be very busy? I do think the rate of
significant issues will slow down.

Version ain't nothing but a number, but if it has any meaning it's the
semantic versioning meaning. 1.0 imposes extra handicaps around
striving to maintain backwards-compatibility. That may end up being
bent to fit in important changes that are going to be required in this
continuing period of change. Hadoop does this all the time
unfortunately and gets away with it, I suppose -- minor version
releases are really major. (On the other extreme, HBase is at 0.98 and
quite production-ready.)

Just consider this a second vote for focus on fixes and 1.0.x rather
than new features and 1.x. I think there are a few steps that could
streamline triage of this flood of contributions, and make all of this
easier, but that's for another thread.


On Fri, May 16, 2014 at 8:50 PM, Mark Hamstra <mark@clearstorydata.com> wrote:
> +1, but just barely.  We've got quite a number of outstanding bugs
> identified, and many of them have fixes in progress.  I'd hate to see those
> efforts get lost in a post-1.0.0 flood of new features targeted at 1.1.0 --
> in other words, I'd like to see 1.0.1 retain a high priority relative to
> 1.1.0.
>
> Looking through the unresolved JIRAs, it doesn't look like any of the
> identified bugs are show-stoppers or strictly regressions (although I will
> note that one that I have in progress, SPARK-1749, is a bug that we
> introduced with recent work -- it's not strictly a regression because we
> had equally bad but different behavior when the DAGScheduler exceptions
> weren't previously being handled at all vs. being slightly mis-handled
> now), so I'm not currently seeing a reason not to release.

From dev-return-7639-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 12:03:49 2014
Return-Path: <dev-return-7639-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 664E111289
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 12:03:49 +0000 (UTC)
Received: (qmail 84362 invoked by uid 500); 17 May 2014 11:29:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82529 invoked by uid 500); 17 May 2014 11:29:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81015 invoked by uid 99); 17 May 2014 11:27:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 11:27:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.216.175 as permitted sender)
Received: from [209.85.216.175] (HELO mail-qc0-f175.google.com) (209.85.216.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 11:27:12 +0000
Received: by mail-qc0-f175.google.com with SMTP id w7so6010186qcr.6
        for <dev@spark.apache.org>; Sat, 17 May 2014 04:26:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=9mEFQkFiFPn7t99R8xOYwOJSz4AnBQ8LiWK5zUkivMo=;
        b=aMzO8+KhBMYGhps9lQeAbuuj9RFpXVEb+4JwBqvXs3qS3ql4JLboAFxSGvA9i3rjRC
         WojjI6+Dd4pA1JdUW1/6N2WFAwFnGWwygtKU7z/hzydXvI/02rylZ2vFgbZfiuCGMxxK
         B1j7+jFjsmAk2io+3Skm35O0HUFo6BYidBrC4/Wd5xKeSU2btezLlg+wDsXkSuknOPfa
         wkDMEnff8OCsn18yzda4d5NlEVrxK73T3G4N0iLBlFb3VmYFZrw55pEgYMHR9unLqzPl
         l1kSnPp0gURthPLj0L+rWNM0UilbRzbSs8Ne0Zcim6MwtYMiFXXwsdh42McGAw2ty0nT
         4aLQ==
MIME-Version: 1.0
X-Received: by 10.224.60.137 with SMTP id p9mr30696143qah.92.1400326009385;
 Sat, 17 May 2014 04:26:49 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Sat, 17 May 2014 04:26:49 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Sat, 17 May 2014 04:26:49 -0700 (PDT)
In-Reply-To: <CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
Date: Sat, 17 May 2014 16:56:49 +0530
Message-ID: <CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1133d8945bf81604f996cec9
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133d8945bf81604f996cec9
Content-Type: text/plain; charset=UTF-8

I had echoed similar sentiments a while back when there was a discussion
around 0.10 vs 1.0 ... I would have preferred 0.10 to stabilize the api
changes, add missing functionality, go through a hardening release before
1.0

But the community preferred a 1.0 :-)

Regards,
Mridul

On 17-May-2014 3:19 pm, "Sean Owen" <sowen@cloudera.com> wrote:
>
> On this note, non-binding commentary:
>
> Releases happen in local minima of change, usually created by
> internally enforced code freeze. Spark is incredibly busy now due to
> external factors -- recently a TLP, recently discovered by a large new
> audience, ease of contribution enabled by Github. It's getting like
> the first year of mainstream battle-testing in a month. It's been very
> hard to freeze anything! I see a number of non-trivial issues being
> reported, and I don't think it has been possible to triage all of
> them, even.
>
> Given the high rate of change, my instinct would have been to release
> 0.10.0 now. But won't it always be very busy? I do think the rate of
> significant issues will slow down.
>
> Version ain't nothing but a number, but if it has any meaning it's the
> semantic versioning meaning. 1.0 imposes extra handicaps around
> striving to maintain backwards-compatibility. That may end up being
> bent to fit in important changes that are going to be required in this
> continuing period of change. Hadoop does this all the time
> unfortunately and gets away with it, I suppose -- minor version
> releases are really major. (On the other extreme, HBase is at 0.98 and
> quite production-ready.)
>
> Just consider this a second vote for focus on fixes and 1.0.x rather
> than new features and 1.x. I think there are a few steps that could
> streamline triage of this flood of contributions, and make all of this
> easier, but that's for another thread.
>
>
> On Fri, May 16, 2014 at 8:50 PM, Mark Hamstra <mark@clearstorydata.com>
wrote:
> > +1, but just barely.  We've got quite a number of outstanding bugs
> > identified, and many of them have fixes in progress.  I'd hate to see
those
> > efforts get lost in a post-1.0.0 flood of new features targeted at
1.1.0 --
> > in other words, I'd like to see 1.0.1 retain a high priority relative to
> > 1.1.0.
> >
> > Looking through the unresolved JIRAs, it doesn't look like any of the
> > identified bugs are show-stoppers or strictly regressions (although I
will
> > note that one that I have in progress, SPARK-1749, is a bug that we
> > introduced with recent work -- it's not strictly a regression because we
> > had equally bad but different behavior when the DAGScheduler exceptions
> > weren't previously being handled at all vs. being slightly mis-handled
> > now), so I'm not currently seeing a reason not to release.

--001a1133d8945bf81604f996cec9--

From dev-return-7640-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 12:20:04 2014
Return-Path: <dev-return-7640-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 175CC112CF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 12:20:04 +0000 (UTC)
Received: (qmail 17816 invoked by uid 500); 17 May 2014 11:54:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15497 invoked by uid 500); 17 May 2014 11:54:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12673 invoked by uid 99); 17 May 2014 11:42:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 11:42:06 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.192.48 as permitted sender)
Received: from [209.85.192.48] (HELO mail-qg0-f48.google.com) (209.85.192.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 11:42:00 +0000
Received: by mail-qg0-f48.google.com with SMTP id i50so5879933qgf.7
        for <dev@spark.apache.org>; Sat, 17 May 2014 04:41:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=stk8T3qO+qgH1Z8CxHse1hYFZwVFLxGZJ/RuMIaeKdg=;
        b=CK+5dOyZSNsq7pvDiCaS200Jg/AfPd01/tvRCjTQh0f/pOLeL+WbFt1ojTg8VC3/Bi
         FaLzn7kAit6Ro9qoidfTiRhXqyaxvmFwifZQgUhsPGp3cupnuFrzXn0Xao2/JH8QXinK
         8mwAGBMI3K6EFe8R0d3c5TD+bhQ4sjqDbFlIDkE9thqDcWE4u1kkJL5vIoNlQncjaXbI
         7XEVwp1wIGcx0yVWza3FU7gJwIU2YS6hy2CMgZBtMC+1vDZ+z6/L9ytweKQAZ0QAJd/O
         0PWdU4djUKYw99w9Z0Z8tLShG3qx4HJrBtmpNQfJ4Wmccbs1louvryIf6C4R3mrocgRb
         Huzg==
MIME-Version: 1.0
X-Received: by 10.140.84.168 with SMTP id l37mr32378184qgd.104.1400326899931;
 Sat, 17 May 2014 04:41:39 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Sat, 17 May 2014 04:41:39 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Sat, 17 May 2014 04:41:39 -0700 (PDT)
In-Reply-To: <JIRA.12714951.1400320946136.374466.1400320994592@arcas>
References: <JIRA.12714951.1400320946136@arcas>
	<JIRA.12714951.1400320946136.374466.1400320994592@arcas>
Date: Sat, 17 May 2014 17:11:39 +0530
Message-ID: <CAJiQeYLnEL+6dWiGfoim-V=vPydJ_TTFZ6VQ_jUgmuyVT8w=dg@mail.gmail.com>
Subject: Re: [jira] [Created] (SPARK-1867) Spark Documentation Error causes
 java.lang.IllegalStateException: unread block data
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c1170870a11204f9970354
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1170870a11204f9970354
Content-Type: text/plain; charset=UTF-8

I suspect this is an issue we have fixed internally here as part of a
larger change - the issue we fixed was not a config issue but bugs in spark.

Unfortunately we plan to contribute this as part of 1.1

Regards,
Mridul
On 17-May-2014 4:09 pm, "sam (JIRA)" <jira@apache.org> wrote:

> sam created SPARK-1867:
> --------------------------
>
>              Summary: Spark Documentation Error causes
> java.lang.IllegalStateException: unread block data
>                  Key: SPARK-1867
>                  URL: https://issues.apache.org/jira/browse/SPARK-1867
>              Project: Spark
>           Issue Type: Bug
>             Reporter: sam
>
>
> I've employed two System Administrators on a contract basis (for quite a
> bit of money), and both contractors have independently hit the following
> exception.  What we are doing is:
>
> 1. Installing Spark 0.9.1 according to the documentation on the website,
> along with CDH4 (and another cluster with CDH5) distros of hadoop/hdfs.
> 2. Building a fat jar with a Spark app with sbt then trying to run it on
> the cluster
>
> I've also included code snippets, and sbt deps at the bottom.
>
> When I've Googled this, there seems to be two somewhat vague responses:
> a) Mismatching spark versions on nodes/user code
> b) Need to add more jars to the SparkConf
>
> Now I know that (b) is not the problem having successfully run the same
> code on other clusters while only including one jar (it's a fat jar).
>
> But I have no idea how to check for (a) - it appears Spark doesn't have
> any version checks or anything - it would be nice if it checked versions
> and threw a "mismatching version exception: you have user code using
> version X and node Y has version Z".
>
> I would be very grateful for advice on this.
>
> The exception:
>
> Exception in thread "main" org.apache.spark.SparkException: Job aborted:
> Task 0.0:1 failed 32 times (most recent failure: Exception failure:
> java.lang.IllegalStateException: unread block data)
>         at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1020)
>         at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1018)
>         at
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>         at
> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>         at org.apache.spark.scheduler.DAGScheduler.org
> $apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1018)
>         at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
>         at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
>         at scala.Option.foreach(Option.scala:236)
>         at
> org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:604)
>         at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:190)
>         at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>         at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>         at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>         at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>         at
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>         at
> scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>         at
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>         at
> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>         at
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
> 14/05/16 18:05:31 INFO scheduler.TaskSetManager: Loss was due to
> java.lang.IllegalStateException: unread block data [duplicate 59]
>
> My code snippet:
>
> val conf = new SparkConf()
>                .setMaster(clusterMaster)
>                .setAppName(appName)
>                .setSparkHome(sparkHome)
>                .setJars(SparkContext.jarOfClass(this.getClass))
>
> println("count = " + new SparkContext(conf).textFile(someHdfsPath).count())
>
> My SBT dependencies:
>
> // relevant
> "org.apache.spark" % "spark-core_2.10" % "0.9.1",
> "org.apache.hadoop" % "hadoop-client" % "2.3.0-mr1-cdh5.0.0",
>
> // standard, probably unrelated
> "com.github.seratch" %% "awscala" % "[0.2,)",
> "org.scalacheck" %% "scalacheck" % "1.10.1" % "test",
> "org.specs2" %% "specs2" % "1.14" % "test",
> "org.scala-lang" % "scala-reflect" % "2.10.3",
> "org.scalaz" %% "scalaz-core" % "7.0.5",
> "net.minidev" % "json-smart" % "1.2"
>
>
>
> --
> This message was sent by Atlassian JIRA
> (v6.2#6252)
>

--001a11c1170870a11204f9970354--

From dev-return-7642-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 16:38:56 2014
Return-Path: <dev-return-7642-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8928E117A6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 16:38:56 +0000 (UTC)
Received: (qmail 61828 invoked by uid 500); 17 May 2014 16:05:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60720 invoked by uid 500); 17 May 2014 16:05:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56384 invoked by uid 99); 17 May 2014 15:53:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 15:53:07 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mark@clearstorydata.com designates 74.125.82.181 as permitted sender)
Received: from [74.125.82.181] (HELO mail-we0-f181.google.com) (74.125.82.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 15:53:02 +0000
Received: by mail-we0-f181.google.com with SMTP id w61so3722158wes.40
        for <dev@spark.apache.org>; Sat, 17 May 2014 08:52:39 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=ldFDbLwsJAw2pN2Lu9V4xVeb2Vcbx2adFs+DpJ6CpQE=;
        b=QYZ4b6Zkpp9S7m1SKwUSmWv/JjopzHj1yzjemLHGANHVwDYBFCAKo1rg8C/Y6PKqXZ
         0CJX0BOPFRGDAEQyrkDKd2D2ip7OToE1z9OUnL19dFWkEOlf1TYs6hy13EMs1WkIWifc
         daTO8/rdt4VVvakobp1FoOFRg+cDXJeOO7qmvO5VIWdYLelzB5QSy2z67ZgdkB7/wvvd
         gRXoUFznrR/8ypNrrHhmWuTLTJyniJCD3FQZLT7DBRFhx/0REW+BD3D3Q6ziWhyNlbJ8
         xITYmu2nLK4cCEQKIjjKN80J3SiIW2gGLdkgMLg9Ofa4GMS9xJJUW5vAmImkzJvz5ddd
         9QTg==
X-Gm-Message-State: ALoCoQmLBuDf47ThuIYd9HDfysOpNkA/fE4Q0Htwr/M9gO9Aej6ZKWQgW76MQGozMJwg5NUlgKDL
MIME-Version: 1.0
X-Received: by 10.180.210.170 with SMTP id mv10mr3650832wic.27.1400341959492;
 Sat, 17 May 2014 08:52:39 -0700 (PDT)
Received: by 10.216.232.69 with HTTP; Sat, 17 May 2014 08:52:39 -0700 (PDT)
In-Reply-To: <CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
Date: Sat, 17 May 2014 08:52:39 -0700
Message-ID: <CAAsvFP=gm_bQc38_DaDbYNUk_2NQxoA=3rqVRCGt1gM1i=Zeug@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c37b540f5b0604f99a85d9
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c37b540f5b0604f99a85d9
Content-Type: text/plain; charset=UTF-8

Which of the unresolved bugs in spark-core do you think will require an
API-breaking change to fix?  If there are none of those, then we are still
essentially on track for a 1.0.0 release.

The number of contributions and pace of change now is quite high, but I
don't think that waiting for the pace to slow before releasing 1.0 is
viable.  If Spark's short history is any guide to its near future, the pace
will not slow by any significant amount for any noteworthy length of time,
but rather will continue to increase.  What we need to be aiming for, I
think, is to have the great majority of those new contributions being made
to MLLlib, GraphX, SparkSQL and other areas of the code that we have
clearly marked as not frozen in 1.x. I think we are already seeing that,
but if I am just not recognizing breakage of our semantic versioning
guarantee that will be forced on us by some pending changes, now would be a
good time to set me straight.


On Sat, May 17, 2014 at 4:26 AM, Mridul Muralidharan <mridul@gmail.com>wrote:

> I had echoed similar sentiments a while back when there was a discussion
> around 0.10 vs 1.0 ... I would have preferred 0.10 to stabilize the api
> changes, add missing functionality, go through a hardening release before
> 1.0
>
> But the community preferred a 1.0 :-)
>
> Regards,
> Mridul
>
> On 17-May-2014 3:19 pm, "Sean Owen" <sowen@cloudera.com> wrote:
> >
> > On this note, non-binding commentary:
> >
> > Releases happen in local minima of change, usually created by
> > internally enforced code freeze. Spark is incredibly busy now due to
> > external factors -- recently a TLP, recently discovered by a large new
> > audience, ease of contribution enabled by Github. It's getting like
> > the first year of mainstream battle-testing in a month. It's been very
> > hard to freeze anything! I see a number of non-trivial issues being
> > reported, and I don't think it has been possible to triage all of
> > them, even.
> >
> > Given the high rate of change, my instinct would have been to release
> > 0.10.0 now. But won't it always be very busy? I do think the rate of
> > significant issues will slow down.
> >
> > Version ain't nothing but a number, but if it has any meaning it's the
> > semantic versioning meaning. 1.0 imposes extra handicaps around
> > striving to maintain backwards-compatibility. That may end up being
> > bent to fit in important changes that are going to be required in this
> > continuing period of change. Hadoop does this all the time
> > unfortunately and gets away with it, I suppose -- minor version
> > releases are really major. (On the other extreme, HBase is at 0.98 and
> > quite production-ready.)
> >
> > Just consider this a second vote for focus on fixes and 1.0.x rather
> > than new features and 1.x. I think there are a few steps that could
> > streamline triage of this flood of contributions, and make all of this
> > easier, but that's for another thread.
> >
> >
> > On Fri, May 16, 2014 at 8:50 PM, Mark Hamstra <mark@clearstorydata.com>
> wrote:
> > > +1, but just barely.  We've got quite a number of outstanding bugs
> > > identified, and many of them have fixes in progress.  I'd hate to see
> those
> > > efforts get lost in a post-1.0.0 flood of new features targeted at
> 1.1.0 --
> > > in other words, I'd like to see 1.0.1 retain a high priority relative
> to
> > > 1.1.0.
> > >
> > > Looking through the unresolved JIRAs, it doesn't look like any of the
> > > identified bugs are show-stoppers or strictly regressions (although I
> will
> > > note that one that I have in progress, SPARK-1749, is a bug that we
> > > introduced with recent work -- it's not strictly a regression because
> we
> > > had equally bad but different behavior when the DAGScheduler exceptions
> > > weren't previously being handled at all vs. being slightly mis-handled
> > > now), so I'm not currently seeing a reason not to release.
>

--001a11c37b540f5b0604f99a85d9--

From dev-return-7641-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 16:38:57 2014
Return-Path: <dev-return-7641-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AD921117AF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 16:38:56 +0000 (UTC)
Received: (qmail 83399 invoked by uid 500); 17 May 2014 16:30:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58716 invoked by uid 500); 17 May 2014 16:05:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56440 invoked by uid 99); 17 May 2014 15:54:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 15:54:01 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mark@clearstorydata.com designates 74.125.82.181 as permitted sender)
Received: from [74.125.82.181] (HELO mail-we0-f181.google.com) (74.125.82.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 15:53:56 +0000
Received: by mail-we0-f181.google.com with SMTP id w61so3833792wes.12
        for <dev@spark.apache.org>; Sat, 17 May 2014 08:53:33 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=7H68ocDPmdpbeeW34F5ahZHSOjv41twGugacYEUNroU=;
        b=cI82WXuPsFSUVVrs+m8rM1iwHE0f29JaAy6FIRGpZ1xaPQJNPb2Yg8bXbCYoHHTTTI
         iBPrGnRPZ4c+UW8bsH4FCp047OBSImmEyg9bmluq/0kUi9VjYlFehWYxBdvXkmbjDU0u
         P7pXopHp+nmr/3r4Sc/3bsUiScFCvGr2oC9kSSSgSdgIRe2ly/fsxMxgrmuLfbI4W8+a
         g7U1ABF7LpV+T6CQkWWEm7Y0E99j1ddUJjtjcY9x/kRKJnm2hrq2ZFEKIhUYun6e4U0X
         ZunSx2nP8ciBDnMqiC5FRookbxdcGM0AX6dsVnerMn2V+8MxJ7qO9h6tVpC3lAbaiy+3
         P+/Q==
X-Gm-Message-State: ALoCoQklGfeSVz0HYTFAkj0y1gSbXM1aFBPjZtNpuzqZbNubfDvgTpqEv4oo6xcVInOOLsi4itBJ
MIME-Version: 1.0
X-Received: by 10.194.77.50 with SMTP id p18mr196669wjw.68.1400342013722; Sat,
 17 May 2014 08:53:33 -0700 (PDT)
Received: by 10.216.232.69 with HTTP; Sat, 17 May 2014 08:53:33 -0700 (PDT)
In-Reply-To: <CABPQxstbwN64rQvx6kUoESkO4GJOohYSAO+s_F85wFWjb6T1zw@mail.gmail.com>
References: <CABPQxstaxcbGSQTqUxSSUQfRpjQs=BqRkrEVvzOXbXZo8kzShg@mail.gmail.com>
	<CABPQxstbwN64rQvx6kUoESkO4GJOohYSAO+s_F85wFWjb6T1zw@mail.gmail.com>
Date: Sat, 17 May 2014 08:53:33 -0700
Message-ID: <CAAsvFPn2RwpFMz7ajy=t8GSjiKSPqARgDtPqXk007Cd7_xvA-Q@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc9)
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bfcfc624adb5a04f99a88ba
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfcfc624adb5a04f99a88ba
Content-Type: text/plain; charset=UTF-8

+1


On Sat, May 17, 2014 at 12:58 AM, Patrick Wendell <pwendell@gmail.com>wrote:

> I'll start the voting with a +1.
>
> On Sat, May 17, 2014 at 12:58 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > Please vote on releasing the following candidate as Apache Spark version
> 1.0.0!
> > This has one bug fix and one minor feature on top of rc8:
> > SPARK-1864: https://github.com/apache/spark/pull/808
> > SPARK-1808: https://github.com/apache/spark/pull/799
> >
> > The tag to be voted on is v1.0.0-rc9 (commit 920f947):
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=920f947eb5a22a679c0c3186cf69ee75f6041c75
> >
> > The release files, including signatures, digests, etc. can be found at:
> > http://people.apache.org/~pwendell/spark-1.0.0-rc9/
> >
> > Release artifacts are signed with the following key:
> > https://people.apache.org/keys/committer/pwendell.asc
> >
> > The staging repository for this release can be found at:
> > https://repository.apache.org/content/repositories/orgapachespark-1017/
> >
> > The documentation corresponding to this release can be found at:
> > http://people.apache.org/~pwendell/spark-1.0.0-rc9-docs/
> >
> > Please vote on releasing this package as Apache Spark 1.0.0!
> >
> > The vote is open until Tuesday, May 20, at 08:56 UTC and passes if
> > amajority of at least 3 +1 PMC votes are cast.
> >
> > [ ] +1 Release this package as Apache Spark 1.0.0
> > [ ] -1 Do not release this package because ...
> >
> > To learn more about Apache Spark, please see
> > http://spark.apache.org/
> >
> > == API Changes ==
> > We welcome users to compile Spark applications against 1.0. There are
> > a few API changes in this release. Here are links to the associated
> > upgrade guides - user facing changes have been kept as small as
> > possible.
> >
> > changes to ML vector specification:
> >
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#from-09-to-10
> >
> > changes to the Java API:
> >
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
> >
> > changes to the streaming API:
> >
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
> >
> > changes to the GraphX API:
> >
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
> >
> > coGroup and related functions now return Iterable[T] instead of Seq[T]
> > ==> Call toSeq on the result to restore the old behavior
> >
> > SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> > ==> Call toSeq on the result to restore old behavior
>

--047d7bfcfc624adb5a04f99a88ba--

From dev-return-7643-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 16:55:06 2014
Return-Path: <dev-return-7643-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A9D94117D1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 16:55:06 +0000 (UTC)
Received: (qmail 84121 invoked by uid 500); 17 May 2014 16:30:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61867 invoked by uid 500); 17 May 2014 16:05:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45174 invoked by uid 99); 17 May 2014 15:45:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 15:45:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.128.173] (HELO mail-ve0-f173.google.com) (209.85.128.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 15:45:16 +0000
Received: by mail-ve0-f173.google.com with SMTP id pa12so4617090veb.32
        for <dev@spark.apache.org>; Sat, 17 May 2014 08:44:52 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=19JgZ1v4tlVU4UW1mSWNjBX7WmO/Xx4Yixc0eXqTwzY=;
        b=bcjtNWBWHqkUKgvYeYtRlrRaT5jeFkqeu8/v1xE1iBPaCPkhcMDXr681ymBo9Dxkha
         mG24b3erDNM3DEvezEWYhxeeMrhwmmGPWr9W6iXaxhagNSfDGInC0MYYQ/s7Pn7IGMrx
         H+pHij3MOmcH2pO7t4/2rC3j4kngIMI9Bdz4qI2gn8b0vZsmsB6GYHlW6LwFBE3WTn5l
         zaSPJo8xJ7O/Slzzd7DPXx/k01GsXHes1yxBvylrB7ZEQTbxosHBYO9YJHNVfqHKYHdF
         8LD245moI/ZuvTnAYf5AoTYZk0DjlStmcqK8MKC3Wh/OkQebYZjkISLP423ol01maqUc
         Nm4A==
X-Gm-Message-State: ALoCoQlfrcMGU7tI8IsTeKrNgN8YhokZiefMk1tMoCuI1919onb4eK3+yPuRMe6FthyRM2od/Yey
X-Received: by 10.58.161.101 with SMTP id xr5mr7139093veb.36.1400341492596;
        Sat, 17 May 2014 08:44:52 -0700 (PDT)
Received: from mail-ve0-f181.google.com (mail-ve0-f181.google.com [209.85.128.181])
        by mx.google.com with ESMTPSA id p1sm20647429vdp.17.2014.05.17.08.44.51
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sat, 17 May 2014 08:44:51 -0700 (PDT)
Received: by mail-ve0-f181.google.com with SMTP id pa12so4489097veb.26
        for <dev@spark.apache.org>; Sat, 17 May 2014 08:44:50 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.53.5.194 with SMTP id co2mr2758875vdd.48.1400341490925; Sat,
 17 May 2014 08:44:50 -0700 (PDT)
Received: by 10.220.109.65 with HTTP; Sat, 17 May 2014 08:44:50 -0700 (PDT)
Received: by 10.220.109.65 with HTTP; Sat, 17 May 2014 08:44:50 -0700 (PDT)
In-Reply-To: <CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
Date: Sat, 17 May 2014 08:44:50 -0700
Message-ID: <CA+-p3AEL6ct-bJwkBSXm+wTLBHEbBoGnnGOJGAjWHvQogDs-4g@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Andrew Ash <andrew@andrewash.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1133eb9021871504f99a69f6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133eb9021871504f99a69f6
Content-Type: text/plain; charset=UTF-8

+1 on the next release feeling more like a 0.10 than a 1.0
On May 17, 2014 4:38 AM, "Mridul Muralidharan" <mridul@gmail.com> wrote:

> I had echoed similar sentiments a while back when there was a discussion
> around 0.10 vs 1.0 ... I would have preferred 0.10 to stabilize the api
> changes, add missing functionality, go through a hardening release before
> 1.0
>
> But the community preferred a 1.0 :-)
>
> Regards,
> Mridul
>
> On 17-May-2014 3:19 pm, "Sean Owen" <sowen@cloudera.com> wrote:
> >
> > On this note, non-binding commentary:
> >
> > Releases happen in local minima of change, usually created by
> > internally enforced code freeze. Spark is incredibly busy now due to
> > external factors -- recently a TLP, recently discovered by a large new
> > audience, ease of contribution enabled by Github. It's getting like
> > the first year of mainstream battle-testing in a month. It's been very
> > hard to freeze anything! I see a number of non-trivial issues being
> > reported, and I don't think it has been possible to triage all of
> > them, even.
> >
> > Given the high rate of change, my instinct would have been to release
> > 0.10.0 now. But won't it always be very busy? I do think the rate of
> > significant issues will slow down.
> >
> > Version ain't nothing but a number, but if it has any meaning it's the
> > semantic versioning meaning. 1.0 imposes extra handicaps around
> > striving to maintain backwards-compatibility. That may end up being
> > bent to fit in important changes that are going to be required in this
> > continuing period of change. Hadoop does this all the time
> > unfortunately and gets away with it, I suppose -- minor version
> > releases are really major. (On the other extreme, HBase is at 0.98 and
> > quite production-ready.)
> >
> > Just consider this a second vote for focus on fixes and 1.0.x rather
> > than new features and 1.x. I think there are a few steps that could
> > streamline triage of this flood of contributions, and make all of this
> > easier, but that's for another thread.
> >
> >
> > On Fri, May 16, 2014 at 8:50 PM, Mark Hamstra <mark@clearstorydata.com>
> wrote:
> > > +1, but just barely.  We've got quite a number of outstanding bugs
> > > identified, and many of them have fixes in progress.  I'd hate to see
> those
> > > efforts get lost in a post-1.0.0 flood of new features targeted at
> 1.1.0 --
> > > in other words, I'd like to see 1.0.1 retain a high priority relative
> to
> > > 1.1.0.
> > >
> > > Looking through the unresolved JIRAs, it doesn't look like any of the
> > > identified bugs are show-stoppers or strictly regressions (although I
> will
> > > note that one that I have in progress, SPARK-1749, is a bug that we
> > > introduced with recent work -- it's not strictly a regression because
> we
> > > had equally bad but different behavior when the DAGScheduler exceptions
> > > weren't previously being handled at all vs. being slightly mis-handled
> > > now), so I'm not currently seeing a reason not to release.
>

--001a1133eb9021871504f99a69f6--

From dev-return-7645-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 17:20:12 2014
Return-Path: <dev-return-7645-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1739511864
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 17:20:12 +0000 (UTC)
Received: (qmail 80918 invoked by uid 500); 17 May 2014 17:20:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48683 invoked by uid 500); 17 May 2014 16:55:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45601 invoked by uid 99); 17 May 2014 16:45:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 16:45:23 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.192.48 as permitted sender)
Received: from [209.85.192.48] (HELO mail-qg0-f48.google.com) (209.85.192.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 16:45:18 +0000
Received: by mail-qg0-f48.google.com with SMTP id i50so6274155qgf.35
        for <dev@spark.apache.org>; Sat, 17 May 2014 09:44:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=ywZ4W9RuT7NnNhKM1g64N/etVt/M2UDIiQSrMRNrWQU=;
        b=yBUtIkeZIYsxTkhMyIhylQlcCMei+Xi7RIwyVBehP77SXnnhi8CsQwC01XD2FFbssj
         82QXVjiKTbMv4XjBGLQrRf2hYV7b1ezpno55+efNtW0gmYQrDZedDidiE8zgAVwgbZjB
         jKAIXr6dwTj7sCNsnBPIfcPbhHrrQbrGLycn5ZThJEo5CotrG98Rqk+Nw6EJ20cJ3yOj
         1H8lvND3hwm6TMy7t0Hn2u8WK8MEt6Hi7dzJGYQ1RDxMPeZoDHAGfCQRr2bHY+h8hDcU
         3FYKdrvG5APtEF4XM8A2IJLVIpu0e4mn57shFqKM0+iVYD6gs8z1kZlwImMX2NjniVoe
         jB1w==
MIME-Version: 1.0
X-Received: by 10.140.21.101 with SMTP id 92mr33759518qgk.57.1400345097615;
 Sat, 17 May 2014 09:44:57 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Sat, 17 May 2014 09:44:57 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Sat, 17 May 2014 09:44:57 -0700 (PDT)
In-Reply-To: <CAAsvFP=gm_bQc38_DaDbYNUk_2NQxoA=3rqVRCGt1gM1i=Zeug@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
	<CAAsvFP=gm_bQc38_DaDbYNUk_2NQxoA=3rqVRCGt1gM1i=Zeug@mail.gmail.com>
Date: Sat, 17 May 2014 22:14:57 +0530
Message-ID: <CAJiQeYLW4UV+wf++_kozHiYb0vK_50GS3marjKp6EFgAXGfn-A@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c130e61b3da804f99b4042
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c130e61b3da804f99b4042
Content-Type: text/plain; charset=UTF-8

We made incompatible api changes whose impact we don't know yet completely
: both from implementation and usage point of view.

We had the option of getting real-world feedback from the user community if
we had gone to 0.10 but the spark developers seemed to be in a hurry to get
to 1.0 - so I made my opinion known but left it to the wisdom of larger
group of committers to decide ... I did not think it was critical enough to
do a binding -1 on.

Regards
Mridul
On 17-May-2014 9:43 pm, "Mark Hamstra" <mark@clearstorydata.com> wrote:

> Which of the unresolved bugs in spark-core do you think will require an
> API-breaking change to fix?  If there are none of those, then we are still
> essentially on track for a 1.0.0 release.
>
> The number of contributions and pace of change now is quite high, but I
> don't think that waiting for the pace to slow before releasing 1.0 is
> viable.  If Spark's short history is any guide to its near future, the pace
> will not slow by any significant amount for any noteworthy length of time,
> but rather will continue to increase.  What we need to be aiming for, I
> think, is to have the great majority of those new contributions being made
> to MLLlib, GraphX, SparkSQL and other areas of the code that we have
> clearly marked as not frozen in 1.x. I think we are already seeing that,
> but if I am just not recognizing breakage of our semantic versioning
> guarantee that will be forced on us by some pending changes, now would be a
> good time to set me straight.
>
>
> On Sat, May 17, 2014 at 4:26 AM, Mridul Muralidharan <mridul@gmail.com
> >wrote:
>
> > I had echoed similar sentiments a while back when there was a discussion
> > around 0.10 vs 1.0 ... I would have preferred 0.10 to stabilize the api
> > changes, add missing functionality, go through a hardening release before
> > 1.0
> >
> > But the community preferred a 1.0 :-)
> >
> > Regards,
> > Mridul
> >
> > On 17-May-2014 3:19 pm, "Sean Owen" <sowen@cloudera.com> wrote:
> > >
> > > On this note, non-binding commentary:
> > >
> > > Releases happen in local minima of change, usually created by
> > > internally enforced code freeze. Spark is incredibly busy now due to
> > > external factors -- recently a TLP, recently discovered by a large new
> > > audience, ease of contribution enabled by Github. It's getting like
> > > the first year of mainstream battle-testing in a month. It's been very
> > > hard to freeze anything! I see a number of non-trivial issues being
> > > reported, and I don't think it has been possible to triage all of
> > > them, even.
> > >
> > > Given the high rate of change, my instinct would have been to release
> > > 0.10.0 now. But won't it always be very busy? I do think the rate of
> > > significant issues will slow down.
> > >
> > > Version ain't nothing but a number, but if it has any meaning it's the
> > > semantic versioning meaning. 1.0 imposes extra handicaps around
> > > striving to maintain backwards-compatibility. That may end up being
> > > bent to fit in important changes that are going to be required in this
> > > continuing period of change. Hadoop does this all the time
> > > unfortunately and gets away with it, I suppose -- minor version
> > > releases are really major. (On the other extreme, HBase is at 0.98 and
> > > quite production-ready.)
> > >
> > > Just consider this a second vote for focus on fixes and 1.0.x rather
> > > than new features and 1.x. I think there are a few steps that could
> > > streamline triage of this flood of contributions, and make all of this
> > > easier, but that's for another thread.
> > >
> > >
> > > On Fri, May 16, 2014 at 8:50 PM, Mark Hamstra <mark@clearstorydata.com
> >
> > wrote:
> > > > +1, but just barely.  We've got quite a number of outstanding bugs
> > > > identified, and many of them have fixes in progress.  I'd hate to see
> > those
> > > > efforts get lost in a post-1.0.0 flood of new features targeted at
> > 1.1.0 --
> > > > in other words, I'd like to see 1.0.1 retain a high priority relative
> > to
> > > > 1.1.0.
> > > >
> > > > Looking through the unresolved JIRAs, it doesn't look like any of the
> > > > identified bugs are show-stoppers or strictly regressions (although I
> > will
> > > > note that one that I have in progress, SPARK-1749, is a bug that we
> > > > introduced with recent work -- it's not strictly a regression because
> > we
> > > > had equally bad but different behavior when the DAGScheduler
> exceptions
> > > > weren't previously being handled at all vs. being slightly
> mis-handled
> > > > now), so I'm not currently seeing a reason not to release.
> >
>

--001a11c130e61b3da804f99b4042--

From dev-return-7644-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 17:20:13 2014
Return-Path: <dev-return-7644-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 29C0911869
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 17:20:12 +0000 (UTC)
Received: (qmail 49668 invoked by uid 500); 17 May 2014 16:55:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46233 invoked by uid 500); 17 May 2014 16:55:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35726 invoked by uid 99); 17 May 2014 16:39:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 16:39:23 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.128.171 as permitted sender)
Received: from [209.85.128.171] (HELO mail-ve0-f171.google.com) (209.85.128.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 16:39:17 +0000
Received: by mail-ve0-f171.google.com with SMTP id oz11so4643569veb.30
        for <dev@spark.apache.org>; Sat, 17 May 2014 09:38:57 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=9at9YMn/8QSLxvDBGq3iyNFCAOzESL+0AifZ9jQcXyA=;
        b=OLhdMrA+OJjFvSVHDzaSIxuQBckunycehk5YvUFm+V0/GQ8HCf8bSdTt7KIjUMWCxp
         i3H3Vi/cK5BZoE9RENXJ0iQw7qwuc4aAbaJXWnosXj6vrn/iQtigRXdDpTZ0Fm28OC3E
         6GtpG+rPU28/AkQGxCTDCT08W2fStGQSD2zHUDPrFDG3OOg5ljD8p0kgEC6I1RNTavxm
         H1EiVXs8gyo7jbMcqnOK1/au8CnAwUyJC+2YIjLFP09XMRKUQvqXxqNHhpC06KigqVI4
         n52SA+eeH0XnuW8SIcyJd6yWaLd2MFzE5nYwETfaYbL9rtAEiQt4IjTmLRnqnDkxv766
         6L9w==
X-Gm-Message-State: ALoCoQk1YnI0FrzplL/qEBkkjQrdcsoI90V30RUqNAR/kuX6rQZ5snxIYCnZgWk1wZyMuejaoKfF
X-Received: by 10.220.94.146 with SMTP id z18mr676050vcm.40.1400344737152;
 Sat, 17 May 2014 09:38:57 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.111.69 with HTTP; Sat, 17 May 2014 09:38:36 -0700 (PDT)
In-Reply-To: <CAAsvFP=gm_bQc38_DaDbYNUk_2NQxoA=3rqVRCGt1gM1i=Zeug@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
 <CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
 <CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
 <CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com> <CAAsvFP=gm_bQc38_DaDbYNUk_2NQxoA=3rqVRCGt1gM1i=Zeug@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Sat, 17 May 2014 17:38:36 +0100
Message-ID: <CAMAsSdJms7jg2zgoX41T0wkGC6h+e5BPEsXUt5qQBbAN15zxiA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Sat, May 17, 2014 at 4:52 PM, Mark Hamstra <mark@clearstorydata.com> wrote:
> Which of the unresolved bugs in spark-core do you think will require an
> API-breaking change to fix?  If there are none of those, then we are still
> essentially on track for a 1.0.0 release.

I don't have a particular one in mind, but look at
https://issues.apache.org/jira/browse/SPARK-1817?filter=12327229 for
example. There are 10 issues marked blocker or critical, that are
targeted at Core / 1.0.0 (or unset). Many are probably not critical,
not for 1.0, or wouldn't require a big change to fix. But has this
been reviewed then -- can you tell? I'd be happy for someone to tell
me to stop worrying, yeah, there's nothing too big here.


> The number of contributions and pace of change now is quite high, but I
> don't think that waiting for the pace to slow before releasing 1.0 is
> viable.  If Spark's short history is any guide to its near future, the pace
> will not slow by any significant amount for any noteworthy length of time,

I think we'd agree core is the most important part. I'd humbly suggest
fixes and improvements to core remain exceptionally important after
1.0 and there is a long line of proposed changes, most good. Would be
great to really burn that down. Maybe that is the kind of thing I
personally would have preferred to see before a 1.0, but it's not up
to me and there are other factors at work here. I don't object
strongly or anything.

From dev-return-7646-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 17:53:13 2014
Return-Path: <dev-return-7646-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5B5F5118CE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 17:53:13 +0000 (UTC)
Received: (qmail 84245 invoked by uid 500); 17 May 2014 17:20:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81915 invoked by uid 500); 17 May 2014 17:20:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79688 invoked by uid 99); 17 May 2014 17:08:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 17:08:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.212.173] (HELO mail-wi0-f173.google.com) (209.85.212.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 17:08:35 +0000
Received: by mail-wi0-f173.google.com with SMTP id bs8so2324670wib.6
        for <dev@spark.apache.org>; Sat, 17 May 2014 10:08:13 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=e1EKjNINlDvGDvvJzgn9bBz9anvm8feWW2LDSDdnOYE=;
        b=MBII741UHgCrPWo4jtpr1JTb3i57eWqu2my9UJHLePD7twxVv7YImjmg3DbtzaxRxz
         WLVVKG2rOQno7U0x5bPjZKlIqM2vPh8Wak9dKXTCTUyjQlh+ewQA8xmnyufRWZaOmeWY
         vHloKy9PY7d4pIcLR2JianzpEjo7FfITYsdGeGvpCPCbpnvPOsNFd5K/D3+UvsHIsbyH
         qBKuCk+ZbStSB1t9eXDsYK+Ecs14G28enUWvt3IrAzQozWFI6HjikE1ZbUWNjmZtnshC
         1L7SMCJzSGD7Y3Raq8gnQUtfZS8nJeHS9ey1I4DHTehIDs0JE3jho5930BfzwaQCqtoL
         Z4sg==
X-Gm-Message-State: ALoCoQmsUS91XeWJ3vhp0lXlhDnyc+qKbstJptb1EhRqMarCxSM0L4jccdld8r3JXh9E345ibrAU
MIME-Version: 1.0
X-Received: by 10.180.107.97 with SMTP id hb1mr4052597wib.20.1400346493668;
 Sat, 17 May 2014 10:08:13 -0700 (PDT)
Received: by 10.180.88.97 with HTTP; Sat, 17 May 2014 10:08:13 -0700 (PDT)
In-Reply-To: <CAAsvFPn2RwpFMz7ajy=t8GSjiKSPqARgDtPqXk007Cd7_xvA-Q@mail.gmail.com>
References: <CABPQxstaxcbGSQTqUxSSUQfRpjQs=BqRkrEVvzOXbXZo8kzShg@mail.gmail.com>
	<CABPQxstbwN64rQvx6kUoESkO4GJOohYSAO+s_F85wFWjb6T1zw@mail.gmail.com>
	<CAAsvFPn2RwpFMz7ajy=t8GSjiKSPqARgDtPqXk007Cd7_xvA-Q@mail.gmail.com>
Date: Sat, 17 May 2014 10:08:13 -0700
Message-ID: <CAMJOb8nTR38Uf50xVxv5O+OrBkO0ELDFYiHrziBVm9qknAgpYA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc9)
From: Andrew Or <andrew@databricks.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=e89a8f2354f15163e704f99b93a8
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f2354f15163e704f99b93a8
Content-Type: text/plain; charset=UTF-8

+1


2014-05-17 8:53 GMT-07:00 Mark Hamstra <mark@clearstorydata.com>:

> +1
>
>
> On Sat, May 17, 2014 at 12:58 AM, Patrick Wendell <pwendell@gmail.com
> >wrote:
>
> > I'll start the voting with a +1.
> >
> > On Sat, May 17, 2014 at 12:58 AM, Patrick Wendell <pwendell@gmail.com>
> > wrote:
> > > Please vote on releasing the following candidate as Apache Spark
> version
> > 1.0.0!
> > > This has one bug fix and one minor feature on top of rc8:
> > > SPARK-1864: https://github.com/apache/spark/pull/808
> > > SPARK-1808: https://github.com/apache/spark/pull/799
> > >
> > > The tag to be voted on is v1.0.0-rc9 (commit 920f947):
> > >
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=920f947eb5a22a679c0c3186cf69ee75f6041c75
> > >
> > > The release files, including signatures, digests, etc. can be found at:
> > > http://people.apache.org/~pwendell/spark-1.0.0-rc9/
> > >
> > > Release artifacts are signed with the following key:
> > > https://people.apache.org/keys/committer/pwendell.asc
> > >
> > > The staging repository for this release can be found at:
> > >
> https://repository.apache.org/content/repositories/orgapachespark-1017/
> > >
> > > The documentation corresponding to this release can be found at:
> > > http://people.apache.org/~pwendell/spark-1.0.0-rc9-docs/
> > >
> > > Please vote on releasing this package as Apache Spark 1.0.0!
> > >
> > > The vote is open until Tuesday, May 20, at 08:56 UTC and passes if
> > > amajority of at least 3 +1 PMC votes are cast.
> > >
> > > [ ] +1 Release this package as Apache Spark 1.0.0
> > > [ ] -1 Do not release this package because ...
> > >
> > > To learn more about Apache Spark, please see
> > > http://spark.apache.org/
> > >
> > > == API Changes ==
> > > We welcome users to compile Spark applications against 1.0. There are
> > > a few API changes in this release. Here are links to the associated
> > > upgrade guides - user facing changes have been kept as small as
> > > possible.
> > >
> > > changes to ML vector specification:
> > >
> >
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#from-09-to-10
> > >
> > > changes to the Java API:
> > >
> >
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
> > >
> > > changes to the streaming API:
> > >
> >
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
> > >
> > > changes to the GraphX API:
> > >
> >
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
> > >
> > > coGroup and related functions now return Iterable[T] instead of Seq[T]
> > > ==> Call toSeq on the result to restore the old behavior
> > >
> > > SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> > > ==> Call toSeq on the result to restore old behavior
> >
>

--e89a8f2354f15163e704f99b93a8--

From dev-return-7647-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 18:10:15 2014
Return-Path: <dev-return-7647-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A82551192B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 18:10:15 +0000 (UTC)
Received: (qmail 31003 invoked by uid 500); 17 May 2014 18:10:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8914 invoked by uid 500); 17 May 2014 17:45:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99648 invoked by uid 99); 17 May 2014 17:36:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 17:36:10 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mark@clearstorydata.com designates 74.125.82.180 as permitted sender)
Received: from [74.125.82.180] (HELO mail-we0-f180.google.com) (74.125.82.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 17:36:05 +0000
Received: by mail-we0-f180.google.com with SMTP id t61so3784471wes.25
        for <dev@spark.apache.org>; Sat, 17 May 2014 10:35:42 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=+eKMMmx6IHpZUDcHaaBGQ0C/AbFtF0Zsxtfj3Ep4sh8=;
        b=D5mx0cgkofVTMVitylIoPrZIfPCMVU8g7YwVH+Kyy5uD4Kz3hl+/2/DJJWxHytyzjo
         +QLLtZOHHGIp/HX64EdMyA3WTsY8FjZtQFeg8+V571k8AZYsyNWoO3c+bWUQB2UmnUDJ
         5Z9JH2LfBhWQ0xRPP8Pi1PS49H2xH2guZ460lTDfRIbZ2tGmvM0/dN+PaxeYCszJkcpp
         PIrSNB/Ec3BUDN9fIFj6pPqhk7+a9ZRWNoLJIUANss1xRSPtYljQ9kbaXRM47BTmNB5a
         jvSI5uFrq0JOm2HPbK8OPKVmihau+ikMKdnUKJPogTikonHZMhEv6bTLwhz/nijV+gry
         OiCw==
X-Gm-Message-State: ALoCoQn+JqmKMBLGkMU1oCWIF8YVzH3kdrUHf/W1g9mFiO43G15h6iq/mA+mxCycBRNM+DeldIfN
MIME-Version: 1.0
X-Received: by 10.180.74.203 with SMTP id w11mr3986525wiv.27.1400348142681;
 Sat, 17 May 2014 10:35:42 -0700 (PDT)
Received: by 10.216.232.69 with HTTP; Sat, 17 May 2014 10:35:42 -0700 (PDT)
In-Reply-To: <CAJiQeYLW4UV+wf++_kozHiYb0vK_50GS3marjKp6EFgAXGfn-A@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
	<CAAsvFP=gm_bQc38_DaDbYNUk_2NQxoA=3rqVRCGt1gM1i=Zeug@mail.gmail.com>
	<CAJiQeYLW4UV+wf++_kozHiYb0vK_50GS3marjKp6EFgAXGfn-A@mail.gmail.com>
Date: Sat, 17 May 2014 10:35:42 -0700
Message-ID: <CAAsvFPku_gueRLTmXE-yRjAGGcG-_jSvRGvP==RWSazro7zwvA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d043891ff9b83a704f99bf543
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043891ff9b83a704f99bf543
Content-Type: text/plain; charset=UTF-8

That is a past issue that we don't need to be re-opening now.  The present
issue, and what I am asking, is which pending bug fixes does anyone
anticipate will require breaking the public API guaranteed in rc9?


On Sat, May 17, 2014 at 9:44 AM, Mridul Muralidharan <mridul@gmail.com>wrote:

> We made incompatible api changes whose impact we don't know yet completely
> : both from implementation and usage point of view.
>
> We had the option of getting real-world feedback from the user community if
> we had gone to 0.10 but the spark developers seemed to be in a hurry to get
> to 1.0 - so I made my opinion known but left it to the wisdom of larger
> group of committers to decide ... I did not think it was critical enough to
> do a binding -1 on.
>
> Regards
> Mridul
> On 17-May-2014 9:43 pm, "Mark Hamstra" <mark@clearstorydata.com> wrote:
>
> > Which of the unresolved bugs in spark-core do you think will require an
> > API-breaking change to fix?  If there are none of those, then we are
> still
> > essentially on track for a 1.0.0 release.
> >
> > The number of contributions and pace of change now is quite high, but I
> > don't think that waiting for the pace to slow before releasing 1.0 is
> > viable.  If Spark's short history is any guide to its near future, the
> pace
> > will not slow by any significant amount for any noteworthy length of
> time,
> > but rather will continue to increase.  What we need to be aiming for, I
> > think, is to have the great majority of those new contributions being
> made
> > to MLLlib, GraphX, SparkSQL and other areas of the code that we have
> > clearly marked as not frozen in 1.x. I think we are already seeing that,
> > but if I am just not recognizing breakage of our semantic versioning
> > guarantee that will be forced on us by some pending changes, now would
> be a
> > good time to set me straight.
> >
> >
> > On Sat, May 17, 2014 at 4:26 AM, Mridul Muralidharan <mridul@gmail.com
> > >wrote:
> >
> > > I had echoed similar sentiments a while back when there was a
> discussion
> > > around 0.10 vs 1.0 ... I would have preferred 0.10 to stabilize the api
> > > changes, add missing functionality, go through a hardening release
> before
> > > 1.0
> > >
> > > But the community preferred a 1.0 :-)
> > >
> > > Regards,
> > > Mridul
> > >
> > > On 17-May-2014 3:19 pm, "Sean Owen" <sowen@cloudera.com> wrote:
> > > >
> > > > On this note, non-binding commentary:
> > > >
> > > > Releases happen in local minima of change, usually created by
> > > > internally enforced code freeze. Spark is incredibly busy now due to
> > > > external factors -- recently a TLP, recently discovered by a large
> new
> > > > audience, ease of contribution enabled by Github. It's getting like
> > > > the first year of mainstream battle-testing in a month. It's been
> very
> > > > hard to freeze anything! I see a number of non-trivial issues being
> > > > reported, and I don't think it has been possible to triage all of
> > > > them, even.
> > > >
> > > > Given the high rate of change, my instinct would have been to release
> > > > 0.10.0 now. But won't it always be very busy? I do think the rate of
> > > > significant issues will slow down.
> > > >
> > > > Version ain't nothing but a number, but if it has any meaning it's
> the
> > > > semantic versioning meaning. 1.0 imposes extra handicaps around
> > > > striving to maintain backwards-compatibility. That may end up being
> > > > bent to fit in important changes that are going to be required in
> this
> > > > continuing period of change. Hadoop does this all the time
> > > > unfortunately and gets away with it, I suppose -- minor version
> > > > releases are really major. (On the other extreme, HBase is at 0.98
> and
> > > > quite production-ready.)
> > > >
> > > > Just consider this a second vote for focus on fixes and 1.0.x rather
> > > > than new features and 1.x. I think there are a few steps that could
> > > > streamline triage of this flood of contributions, and make all of
> this
> > > > easier, but that's for another thread.
> > > >
> > > >
> > > > On Fri, May 16, 2014 at 8:50 PM, Mark Hamstra <
> mark@clearstorydata.com
> > >
> > > wrote:
> > > > > +1, but just barely.  We've got quite a number of outstanding bugs
> > > > > identified, and many of them have fixes in progress.  I'd hate to
> see
> > > those
> > > > > efforts get lost in a post-1.0.0 flood of new features targeted at
> > > 1.1.0 --
> > > > > in other words, I'd like to see 1.0.1 retain a high priority
> relative
> > > to
> > > > > 1.1.0.
> > > > >
> > > > > Looking through the unresolved JIRAs, it doesn't look like any of
> the
> > > > > identified bugs are show-stoppers or strictly regressions
> (although I
> > > will
> > > > > note that one that I have in progress, SPARK-1749, is a bug that we
> > > > > introduced with recent work -- it's not strictly a regression
> because
> > > we
> > > > > had equally bad but different behavior when the DAGScheduler
> > exceptions
> > > > > weren't previously being handled at all vs. being slightly
> > mis-handled
> > > > > now), so I'm not currently seeing a reason not to release.
> > >
> >
>

--f46d043891ff9b83a704f99bf543--

From dev-return-7648-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 18:18:15 2014
Return-Path: <dev-return-7648-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5A19711949
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 18:18:15 +0000 (UTC)
Received: (qmail 32693 invoked by uid 500); 17 May 2014 18:10:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10807 invoked by uid 500); 17 May 2014 17:45:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99352 invoked by uid 99); 17 May 2014 17:32:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 17:32:53 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of kanzhangemail@gmail.com designates 209.85.223.180 as permitted sender)
Received: from [209.85.223.180] (HELO mail-ie0-f180.google.com) (209.85.223.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 17:32:49 +0000
Received: by mail-ie0-f180.google.com with SMTP id tp5so864054ieb.11
        for <dev@spark.apache.org>; Sat, 17 May 2014 10:32:25 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=KTFUHN+QPQKqT1QsZLMhVJcuCplmbBSbM//zOPrzabg=;
        b=dOcnZ6c+b+h2Ky4CxwF1SgIYqWLQGvLpHSiSjSeQmuz6Wg6yyCkF0lsSpQqmsAIBL2
         RSURl3WOfGBMzdz+dh1vnTI+qzdLSqInWikCTiiDtgHLnbPThnMkSBtqyBX9t51OkyKe
         cSmEL3a+oEPf2TML8p5/+zWmIU8EUKfafsFKaQApDasQ8dJkqSxWwfgx2FiIgx/BT40f
         ME2mgYwC1/8ut5KIvV9zvyIW23ICv9tx5Llw8ow4DzjbjD90dkYU6GC1Hb/PSFXYzXdZ
         2DGBv3AQN5jG/0z/Wr6L9VNWo3PhhpQJmjA5f1hjakFDFzEJzqZOsE1iO5dIwHsgql/r
         Jdbw==
MIME-Version: 1.0
X-Received: by 10.50.4.70 with SMTP id i6mr5672516igi.40.1400347945580; Sat,
 17 May 2014 10:32:25 -0700 (PDT)
Sender: kanzhangemail@gmail.com
Received: by 10.64.70.6 with HTTP; Sat, 17 May 2014 10:32:25 -0700 (PDT)
In-Reply-To: <CA+-p3AEL6ct-bJwkBSXm+wTLBHEbBoGnnGOJGAjWHvQogDs-4g@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
	<CA+-p3AEL6ct-bJwkBSXm+wTLBHEbBoGnnGOJGAjWHvQogDs-4g@mail.gmail.com>
Date: Sat, 17 May 2014 10:32:25 -0700
X-Google-Sender-Auth: DXmZ5yYxq1GuF5As_A7dKJE5Y-w
Message-ID: <CALRHqP-jLhJM3_5qCTWzrbU+GpmVsd2DFtvMXsH4-PGuT6X8ww@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Kan Zhang <kzhang@apache.org>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c32a88dbbdd704f99be90c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c32a88dbbdd704f99be90c
Content-Type: text/plain; charset=UTF-8

+1 on the running commentary here, non-binding of course :-)


On Sat, May 17, 2014 at 8:44 AM, Andrew Ash <andrew@andrewash.com> wrote:

> +1 on the next release feeling more like a 0.10 than a 1.0
> On May 17, 2014 4:38 AM, "Mridul Muralidharan" <mridul@gmail.com> wrote:
>
> > I had echoed similar sentiments a while back when there was a discussion
> > around 0.10 vs 1.0 ... I would have preferred 0.10 to stabilize the api
> > changes, add missing functionality, go through a hardening release before
> > 1.0
> >
> > But the community preferred a 1.0 :-)
> >
> > Regards,
> > Mridul
> >
> > On 17-May-2014 3:19 pm, "Sean Owen" <sowen@cloudera.com> wrote:
> > >
> > > On this note, non-binding commentary:
> > >
> > > Releases happen in local minima of change, usually created by
> > > internally enforced code freeze. Spark is incredibly busy now due to
> > > external factors -- recently a TLP, recently discovered by a large new
> > > audience, ease of contribution enabled by Github. It's getting like
> > > the first year of mainstream battle-testing in a month. It's been very
> > > hard to freeze anything! I see a number of non-trivial issues being
> > > reported, and I don't think it has been possible to triage all of
> > > them, even.
> > >
> > > Given the high rate of change, my instinct would have been to release
> > > 0.10.0 now. But won't it always be very busy? I do think the rate of
> > > significant issues will slow down.
> > >
> > > Version ain't nothing but a number, but if it has any meaning it's the
> > > semantic versioning meaning. 1.0 imposes extra handicaps around
> > > striving to maintain backwards-compatibility. That may end up being
> > > bent to fit in important changes that are going to be required in this
> > > continuing period of change. Hadoop does this all the time
> > > unfortunately and gets away with it, I suppose -- minor version
> > > releases are really major. (On the other extreme, HBase is at 0.98 and
> > > quite production-ready.)
> > >
> > > Just consider this a second vote for focus on fixes and 1.0.x rather
> > > than new features and 1.x. I think there are a few steps that could
> > > streamline triage of this flood of contributions, and make all of this
> > > easier, but that's for another thread.
> > >
> > >
> > > On Fri, May 16, 2014 at 8:50 PM, Mark Hamstra <mark@clearstorydata.com
> >
> > wrote:
> > > > +1, but just barely.  We've got quite a number of outstanding bugs
> > > > identified, and many of them have fixes in progress.  I'd hate to see
> > those
> > > > efforts get lost in a post-1.0.0 flood of new features targeted at
> > 1.1.0 --
> > > > in other words, I'd like to see 1.0.1 retain a high priority relative
> > to
> > > > 1.1.0.
> > > >
> > > > Looking through the unresolved JIRAs, it doesn't look like any of the
> > > > identified bugs are show-stoppers or strictly regressions (although I
> > will
> > > > note that one that I have in progress, SPARK-1749, is a bug that we
> > > > introduced with recent work -- it's not strictly a regression because
> > we
> > > > had equally bad but different behavior when the DAGScheduler
> exceptions
> > > > weren't previously being handled at all vs. being slightly
> mis-handled
> > > > now), so I'm not currently seeing a reason not to release.
> >
>

--001a11c32a88dbbdd704f99be90c--

From dev-return-7650-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 19:25:15 2014
Return-Path: <dev-return-7650-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 41CAE11A33
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 19:25:15 +0000 (UTC)
Received: (qmail 40181 invoked by uid 500); 17 May 2014 19:25:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2419 invoked by uid 500); 17 May 2014 19:00:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99110 invoked by uid 99); 17 May 2014 18:59:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 18:59:54 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.216.176 as permitted sender)
Received: from [209.85.216.176] (HELO mail-qc0-f176.google.com) (209.85.216.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 18:59:49 +0000
Received: by mail-qc0-f176.google.com with SMTP id r5so6556102qcx.35
        for <dev@spark.apache.org>; Sat, 17 May 2014 11:59:25 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=HPSOtIrgIdh1dZRNxU9EdnQ2CpF6JOhK9ldK5zzK47g=;
        b=KmaQs/axCEJi8l3AChINJYmacrFo7+mi0UCWPlueZHVD48seUaq/7V8szk9rKOpMx8
         Q4Xr7KSI7nC/eI7HoqMCtCXiC0l8FZd1DjGeXesdwvGdG4gFleUTonx3HLZEAXTYAAdf
         dhidktZhEVrOq1mm7JjxpebifZ6+/bfFsntvK/RC1ylsYiPhZqG34/m0RIou8r18Zbwz
         155pjsI0khlZ6tSRtiynqSBnUHrzpZGGaj3QcL/LiaqOFbRzJ34TbzmpJghqIj+fPkAb
         zhU+G/f/tCGoTZlvqYqo1EbERNJlLylGqahfX8q+Qct10RyVk1yVju2wmqZKbQioCXZx
         EgdQ==
MIME-Version: 1.0
X-Received: by 10.224.79.143 with SMTP id p15mr34175147qak.57.1400353165755;
 Sat, 17 May 2014 11:59:25 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Sat, 17 May 2014 11:59:25 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Sat, 17 May 2014 11:59:25 -0700 (PDT)
In-Reply-To: <CAAsvFPku_gueRLTmXE-yRjAGGcG-_jSvRGvP==RWSazro7zwvA@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
	<CAAsvFP=gm_bQc38_DaDbYNUk_2NQxoA=3rqVRCGt1gM1i=Zeug@mail.gmail.com>
	<CAJiQeYLW4UV+wf++_kozHiYb0vK_50GS3marjKp6EFgAXGfn-A@mail.gmail.com>
	<CAAsvFPku_gueRLTmXE-yRjAGGcG-_jSvRGvP==RWSazro7zwvA@mail.gmail.com>
Date: Sun, 18 May 2014 00:29:25 +0530
Message-ID: <CAJiQeY+ifxMt_Q=kNCMRCdbrnhEfezFMZyjM_U_LW=5X02Fu2g@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bf0e568014d6904f99d2134
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bf0e568014d6904f99d2134
Content-Type: text/plain; charset=UTF-8

On 17-May-2014 11:40 pm, "Mark Hamstra" <mark@clearstorydata.com> wrote:
>
> That is a past issue that we don't need to be re-opening now.  The present

Huh ? If we need to revisit based on changed circumstances, we must - the
scope of changes introduced in this release was definitely not anticipated
when 1.0 vs 0.10 discussion happened.

If folks are worried about stability of core; it is a valid concern IMO.

Having said that, I am still ok with going to 1.0; but if a conversation
starts about need for 1.0 vs going to 0.10 I want to hear more and possibly
allay the concerns and not try to muzzle the discussion.


Regards
Mridul

> issue, and what I am asking, is which pending bug fixes does anyone
> anticipate will require breaking the public API guaranteed in rc9
>
>
> On Sat, May 17, 2014 at 9:44 AM, Mridul Muralidharan <mridul@gmail.com
>wrote:
>
> > We made incompatible api changes whose impact we don't know yet
completely
> > : both from implementation and usage point of view.
> >
> > We had the option of getting real-world feedback from the user
community if
> > we had gone to 0.10 but the spark developers seemed to be in a hurry to
get
> > to 1.0 - so I made my opinion known but left it to the wisdom of larger
> > group of committers to decide ... I did not think it was critical
enough to
> > do a binding -1 on.
> >
> > Regards
> > Mridul
> > On 17-May-2014 9:43 pm, "Mark Hamstra" <mark@clearstorydata.com> wrote:
> >
> > > Which of the unresolved bugs in spark-core do you think will require
an
> > > API-breaking change to fix?  If there are none of those, then we are
> > still
> > > essentially on track for a 1.0.0 release.
> > >
> > > The number of contributions and pace of change now is quite high, but
I
> > > don't think that waiting for the pace to slow before releasing 1.0 is
> > > viable.  If Spark's short history is any guide to its near future, the
> > pace
> > > will not slow by any significant amount for any noteworthy length of
> > time,
> > > but rather will continue to increase.  What we need to be aiming for,
I
> > > think, is to have the great majority of those new contributions being
> > made
> > > to MLLlib, GraphX, SparkSQL and other areas of the code that we have
> > > clearly marked as not frozen in 1.x. I think we are already seeing
that,
> > > but if I am just not recognizing breakage of our semantic versioning
> > > guarantee that will be forced on us by some pending changes, now would
> > be a
> > > good time to set me straight.
> > >
> > >
> > > On Sat, May 17, 2014 at 4:26 AM, Mridul Muralidharan <mridul@gmail.com
> > > >wrote:
> > >
> > > > I had echoed similar sentiments a while back when there was a
> > discussion
> > > > around 0.10 vs 1.0 ... I would have preferred 0.10 to stabilize the
api
> > > > changes, add missing functionality, go through a hardening release
> > before
> > > > 1.0
> > > >
> > > > But the community preferred a 1.0 :-)
> > > >
> > > > Regards,
> > > > Mridul
> > > >
> > > > On 17-May-2014 3:19 pm, "Sean Owen" <sowen@cloudera.com> wrote:
> > > > >
> > > > > On this note, non-binding commentary:
> > > > >
> > > > > Releases happen in local minima of change, usually created by
> > > > > internally enforced code freeze. Spark is incredibly busy now due
to
> > > > > external factors -- recently a TLP, recently discovered by a large
> > new
> > > > > audience, ease of contribution enabled by Github. It's getting
like
> > > > > the first year of mainstream battle-testing in a month. It's been
> > very
> > > > > hard to freeze anything! I see a number of non-trivial issues
being
> > > > > reported, and I don't think it has been possible to triage all of
> > > > > them, even.
> > > > >
> > > > > Given the high rate of change, my instinct would have been to
release
> > > > > 0.10.0 now. But won't it always be very busy? I do think the rate
of
> > > > > significant issues will slow down.
> > > > >
> > > > > Version ain't nothing but a number, but if it has any meaning it's
> > the
> > > > > semantic versioning meaning. 1.0 imposes extra handicaps around
> > > > > striving to maintain backwards-compatibility. That may end up
being
> > > > > bent to fit in important changes that are going to be required in
> > this
> > > > > continuing period of change. Hadoop does this all the time
> > > > > unfortunately and gets away with it, I suppose -- minor version
> > > > > releases are really major. (On the other extreme, HBase is at 0.98
> > and
> > > > > quite production-ready.)
> > > > >
> > > > > Just consider this a second vote for focus on fixes and 1.0.x
rather
> > > > > than new features and 1.x. I think there are a few steps that
could
> > > > > streamline triage of this flood of contributions, and make all of
> > this
> > > > > easier, but that's for another thread.
> > > > >
> > > > >
> > > > > On Fri, May 16, 2014 at 8:50 PM, Mark Hamstra <
> > mark@clearstorydata.com
> > > >
> > > > wrote:
> > > > > > +1, but just barely.  We've got quite a number of outstanding
bugs
> > > > > > identified, and many of them have fixes in progress.  I'd hate
to
> > see
> > > > those
> > > > > > efforts get lost in a post-1.0.0 flood of new features targeted
at
> > > > 1.1.0 --
> > > > > > in other words, I'd like to see 1.0.1 retain a high priority
> > relative
> > > > to
> > > > > > 1.1.0.
> > > > > >
> > > > > > Looking through the unresolved JIRAs, it doesn't look like any
of
> > the
> > > > > > identified bugs are show-stoppers or strictly regressions
> > (although I
> > > > will
> > > > > > note that one that I have in progress, SPARK-1749, is a bug
that we
> > > > > > introduced with recent work -- it's not strictly a regression
> > because
> > > > we
> > > > > > had equally bad but different behavior when the DAGScheduler
> > > exceptions
> > > > > > weren't previously being handled at all vs. being slightly
> > > mis-handled
> > > > > > now), so I'm not currently seeing a reason not to release.
> > > >
> > >
> >

--047d7bf0e568014d6904f99d2134--

From dev-return-7649-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 19:25:15 2014
Return-Path: <dev-return-7649-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D2CFB11A40
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 19:25:15 +0000 (UTC)
Received: (qmail 2310 invoked by uid 500); 17 May 2014 19:00:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 56611 invoked by uid 500); 17 May 2014 18:35:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51655 invoked by uid 99); 17 May 2014 18:32:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 18:32:28 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.176 as permitted sender)
Received: from [209.85.216.176] (HELO mail-qc0-f176.google.com) (209.85.216.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 18:32:23 +0000
Received: by mail-qc0-f176.google.com with SMTP id r5so6532891qcx.35
        for <dev@spark.apache.org>; Sat, 17 May 2014 11:32:02 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=lR+ocq2H2hDb04hg7/WQy6AOV+z64EHksiraGdpj7ds=;
        b=hxMczlihJ0lzKKt/BvyS8gSOMIg9JUKN5BwWluGq8HNpioDMtfCG0+Kq0lM9BdvsPK
         tvEWNBMHOB+yLTrU58wpDdhwZ0NVyTM20fU0wJUU6JHSvH8yL72QXFCduaAIBtGJ9qsw
         X4pprYlykOzm3K7z2dDy2pbxi6WEF8eAzlGuNqNLU0vTT+inOHbtr0FPcdJKzPtYId20
         VQmiBbZfGrdMMJZBBfSjKq+AYUxPG+Ub66faA+5b9suNWw1goPxlblO3a9kEmtsbeU5c
         Rz/aoXSbGWVj7u6Gwzbd1CUSfOwxTwjggBEKb92yoyNMsNhInZad8YkffZSGj8eh4KGp
         7O0Q==
X-Gm-Message-State: ALoCoQmFLAr9w0ETp0/NvZkB5xTdtBJWYsDNtgYFdN3lQMFMJee4lhPhYEJtfe7VsLFZryYIRj6z
MIME-Version: 1.0
X-Received: by 10.224.49.67 with SMTP id u3mr34092222qaf.63.1400351522687;
 Sat, 17 May 2014 11:32:02 -0700 (PDT)
Received: by 10.140.106.202 with HTTP; Sat, 17 May 2014 11:32:02 -0700 (PDT)
In-Reply-To: <CAMJOb8nTR38Uf50xVxv5O+OrBkO0ELDFYiHrziBVm9qknAgpYA@mail.gmail.com>
References: <CABPQxstaxcbGSQTqUxSSUQfRpjQs=BqRkrEVvzOXbXZo8kzShg@mail.gmail.com>
	<CABPQxstbwN64rQvx6kUoESkO4GJOohYSAO+s_F85wFWjb6T1zw@mail.gmail.com>
	<CAAsvFPn2RwpFMz7ajy=t8GSjiKSPqARgDtPqXk007Cd7_xvA-Q@mail.gmail.com>
	<CAMJOb8nTR38Uf50xVxv5O+OrBkO0ELDFYiHrziBVm9qknAgpYA@mail.gmail.com>
Date: Sat, 17 May 2014 11:32:02 -0700
Message-ID: <CACBYxKLR3xE4FSqZQwrjmP+bhhqy_mAat0hHSu+1pOBEVTttrA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc9)
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2ef801221cb04f99cbfee
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ef801221cb04f99cbfee
Content-Type: text/plain; charset=UTF-8

+1

Reran my tests from rc5:

* Built the release from source.
* Compiled Java and Scala apps that interact with HDFS against it.
* Ran them in local mode.
* Ran them against a pseudo-distributed YARN cluster in both yarn-client
mode and yarn-cluster mode.


On Sat, May 17, 2014 at 10:08 AM, Andrew Or <andrew@databricks.com> wrote:

> +1
>
>
> 2014-05-17 8:53 GMT-07:00 Mark Hamstra <mark@clearstorydata.com>:
>
> > +1
> >
> >
> > On Sat, May 17, 2014 at 12:58 AM, Patrick Wendell <pwendell@gmail.com
> > >wrote:
> >
> > > I'll start the voting with a +1.
> > >
> > > On Sat, May 17, 2014 at 12:58 AM, Patrick Wendell <pwendell@gmail.com>
> > > wrote:
> > > > Please vote on releasing the following candidate as Apache Spark
> > version
> > > 1.0.0!
> > > > This has one bug fix and one minor feature on top of rc8:
> > > > SPARK-1864: https://github.com/apache/spark/pull/808
> > > > SPARK-1808: https://github.com/apache/spark/pull/799
> > > >
> > > > The tag to be voted on is v1.0.0-rc9 (commit 920f947):
> > > >
> > >
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=920f947eb5a22a679c0c3186cf69ee75f6041c75
> > > >
> > > > The release files, including signatures, digests, etc. can be found
> at:
> > > > http://people.apache.org/~pwendell/spark-1.0.0-rc9/
> > > >
> > > > Release artifacts are signed with the following key:
> > > > https://people.apache.org/keys/committer/pwendell.asc
> > > >
> > > > The staging repository for this release can be found at:
> > > >
> > https://repository.apache.org/content/repositories/orgapachespark-1017/
> > > >
> > > > The documentation corresponding to this release can be found at:
> > > > http://people.apache.org/~pwendell/spark-1.0.0-rc9-docs/
> > > >
> > > > Please vote on releasing this package as Apache Spark 1.0.0!
> > > >
> > > > The vote is open until Tuesday, May 20, at 08:56 UTC and passes if
> > > > amajority of at least 3 +1 PMC votes are cast.
> > > >
> > > > [ ] +1 Release this package as Apache Spark 1.0.0
> > > > [ ] -1 Do not release this package because ...
> > > >
> > > > To learn more about Apache Spark, please see
> > > > http://spark.apache.org/
> > > >
> > > > == API Changes ==
> > > > We welcome users to compile Spark applications against 1.0. There are
> > > > a few API changes in this release. Here are links to the associated
> > > > upgrade guides - user facing changes have been kept as small as
> > > > possible.
> > > >
> > > > changes to ML vector specification:
> > > >
> > >
> >
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#from-09-to-10
> > > >
> > > > changes to the Java API:
> > > >
> > >
> >
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
> > > >
> > > > changes to the streaming API:
> > > >
> > >
> >
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
> > > >
> > > > changes to the GraphX API:
> > > >
> > >
> >
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
> > > >
> > > > coGroup and related functions now return Iterable[T] instead of
> Seq[T]
> > > > ==> Call toSeq on the result to restore the old behavior
> > > >
> > > > SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> > > > ==> Call toSeq on the result to restore old behavior
> > >
> >
>

--001a11c2ef801221cb04f99cbfee--

From dev-return-7651-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 20:15:18 2014
Return-Path: <dev-return-7651-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 10EF211B0F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 20:15:18 +0000 (UTC)
Received: (qmail 76915 invoked by uid 500); 17 May 2014 20:15:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57933 invoked by uid 500); 17 May 2014 19:50:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55555 invoked by uid 99); 17 May 2014 19:43:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 19:43:33 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mark@clearstorydata.com designates 74.125.82.48 as permitted sender)
Received: from [74.125.82.48] (HELO mail-wg0-f48.google.com) (74.125.82.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 19:43:28 +0000
Received: by mail-wg0-f48.google.com with SMTP id b13so6358228wgh.7
        for <dev@spark.apache.org>; Sat, 17 May 2014 12:43:04 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=fTwkLGEbFSsQhiPzP76S2l4wo5TFwwtPzbjTINuKzPQ=;
        b=OjOxDsJ/1NUT/BQ2/6oHMIapsv1GchcMZ9cQ3NnCk+QdT7HrBA+4uwijSEZ3qC2dkL
         Ot+JXPHJwslYLp7wak4vOpKmlojOXysBCwecOg62m3+c9QwJ8/KWF5c6mRJQ7Y9Mb3f6
         CaSpGUzsz04KAX50GZ84qqFW1kZHS2k8FL5Mqeb8JGbGsX9+8JXnjxKLPVOp6JxAxzvC
         8HiVacGyFEZzhoOjMCWjgWFYs6+SCGc0RXh7ep6XzsfoibgEEX8NZnm/1RLYEVtYLlKF
         HC8LmobAZBEdUtIlxbPWW3w3FmLpT9LVs83kKimfBKW+7w6YnXSdmNBM94ql+UyePUT5
         a4jA==
X-Gm-Message-State: ALoCoQk6zvFlZO19eSOkrjpoQlkXp9xKSAZDHPCGcEhSLiLo/Fxg0YLYL2geI8U1thUx3qOOMeEn
MIME-Version: 1.0
X-Received: by 10.194.90.107 with SMTP id bv11mr20555266wjb.11.1400355784870;
 Sat, 17 May 2014 12:43:04 -0700 (PDT)
Received: by 10.216.232.69 with HTTP; Sat, 17 May 2014 12:43:04 -0700 (PDT)
In-Reply-To: <CAJiQeY+ifxMt_Q=kNCMRCdbrnhEfezFMZyjM_U_LW=5X02Fu2g@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
	<CAAsvFP=gm_bQc38_DaDbYNUk_2NQxoA=3rqVRCGt1gM1i=Zeug@mail.gmail.com>
	<CAJiQeYLW4UV+wf++_kozHiYb0vK_50GS3marjKp6EFgAXGfn-A@mail.gmail.com>
	<CAAsvFPku_gueRLTmXE-yRjAGGcG-_jSvRGvP==RWSazro7zwvA@mail.gmail.com>
	<CAJiQeY+ifxMt_Q=kNCMRCdbrnhEfezFMZyjM_U_LW=5X02Fu2g@mail.gmail.com>
Date: Sat, 17 May 2014 12:43:04 -0700
Message-ID: <CAAsvFPkAt=Q2-gLrXLzqV_NVeaxmFTpWwXFxusqhzZ+h3u7Szg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7beb9ec01de30e04f99dbde4
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7beb9ec01de30e04f99dbde4
Content-Type: text/plain; charset=UTF-8

I'm not trying to muzzle the discussion.  All I am saying is that we don't
need to have the same discussion about 0.10 vs. 1.0 that we already had.
 If you can tell me about specific changes in the current release candidate
that occasion new arguments for why a 1.0 release is an unacceptable idea,
then I'm listening.


On Sat, May 17, 2014 at 11:59 AM, Mridul Muralidharan <mridul@gmail.com>wrote:

> On 17-May-2014 11:40 pm, "Mark Hamstra" <mark@clearstorydata.com> wrote:
> >
> > That is a past issue that we don't need to be re-opening now.  The
> present
>
> Huh ? If we need to revisit based on changed circumstances, we must - the
> scope of changes introduced in this release was definitely not anticipated
> when 1.0 vs 0.10 discussion happened.
>
> If folks are worried about stability of core; it is a valid concern IMO.
>
> Having said that, I am still ok with going to 1.0; but if a conversation
> starts about need for 1.0 vs going to 0.10 I want to hear more and possibly
> allay the concerns and not try to muzzle the discussion.
>
>
> Regards
> Mridul
>
> > issue, and what I am asking, is which pending bug fixes does anyone
> > anticipate will require breaking the public API guaranteed in rc9
> >
> >
> > On Sat, May 17, 2014 at 9:44 AM, Mridul Muralidharan <mridul@gmail.com
> >wrote:
> >
> > > We made incompatible api changes whose impact we don't know yet
> completely
> > > : both from implementation and usage point of view.
> > >
> > > We had the option of getting real-world feedback from the user
> community if
> > > we had gone to 0.10 but the spark developers seemed to be in a hurry to
> get
> > > to 1.0 - so I made my opinion known but left it to the wisdom of larger
> > > group of committers to decide ... I did not think it was critical
> enough to
> > > do a binding -1 on.
> > >
> > > Regards
> > > Mridul
> > > On 17-May-2014 9:43 pm, "Mark Hamstra" <mark@clearstorydata.com>
> wrote:
> > >
> > > > Which of the unresolved bugs in spark-core do you think will require
> an
> > > > API-breaking change to fix?  If there are none of those, then we are
> > > still
> > > > essentially on track for a 1.0.0 release.
> > > >
> > > > The number of contributions and pace of change now is quite high, but
> I
> > > > don't think that waiting for the pace to slow before releasing 1.0 is
> > > > viable.  If Spark's short history is any guide to its near future,
> the
> > > pace
> > > > will not slow by any significant amount for any noteworthy length of
> > > time,
> > > > but rather will continue to increase.  What we need to be aiming for,
> I
> > > > think, is to have the great majority of those new contributions being
> > > made
> > > > to MLLlib, GraphX, SparkSQL and other areas of the code that we have
> > > > clearly marked as not frozen in 1.x. I think we are already seeing
> that,
> > > > but if I am just not recognizing breakage of our semantic versioning
> > > > guarantee that will be forced on us by some pending changes, now
> would
> > > be a
> > > > good time to set me straight.
> > > >
> > > >
> > > > On Sat, May 17, 2014 at 4:26 AM, Mridul Muralidharan <
> mridul@gmail.com
> > > > >wrote:
> > > >
> > > > > I had echoed similar sentiments a while back when there was a
> > > discussion
> > > > > around 0.10 vs 1.0 ... I would have preferred 0.10 to stabilize the
> api
> > > > > changes, add missing functionality, go through a hardening release
> > > before
> > > > > 1.0
> > > > >
> > > > > But the community preferred a 1.0 :-)
> > > > >
> > > > > Regards,
> > > > > Mridul
> > > > >
> > > > > On 17-May-2014 3:19 pm, "Sean Owen" <sowen@cloudera.com> wrote:
> > > > > >
> > > > > > On this note, non-binding commentary:
> > > > > >
> > > > > > Releases happen in local minima of change, usually created by
> > > > > > internally enforced code freeze. Spark is incredibly busy now due
> to
> > > > > > external factors -- recently a TLP, recently discovered by a
> large
> > > new
> > > > > > audience, ease of contribution enabled by Github. It's getting
> like
> > > > > > the first year of mainstream battle-testing in a month. It's been
> > > very
> > > > > > hard to freeze anything! I see a number of non-trivial issues
> being
> > > > > > reported, and I don't think it has been possible to triage all of
> > > > > > them, even.
> > > > > >
> > > > > > Given the high rate of change, my instinct would have been to
> release
> > > > > > 0.10.0 now. But won't it always be very busy? I do think the rate
> of
> > > > > > significant issues will slow down.
> > > > > >
> > > > > > Version ain't nothing but a number, but if it has any meaning
> it's
> > > the
> > > > > > semantic versioning meaning. 1.0 imposes extra handicaps around
> > > > > > striving to maintain backwards-compatibility. That may end up
> being
> > > > > > bent to fit in important changes that are going to be required in
> > > this
> > > > > > continuing period of change. Hadoop does this all the time
> > > > > > unfortunately and gets away with it, I suppose -- minor version
> > > > > > releases are really major. (On the other extreme, HBase is at
> 0.98
> > > and
> > > > > > quite production-ready.)
> > > > > >
> > > > > > Just consider this a second vote for focus on fixes and 1.0.x
> rather
> > > > > > than new features and 1.x. I think there are a few steps that
> could
> > > > > > streamline triage of this flood of contributions, and make all of
> > > this
> > > > > > easier, but that's for another thread.
> > > > > >
> > > > > >
> > > > > > On Fri, May 16, 2014 at 8:50 PM, Mark Hamstra <
> > > mark@clearstorydata.com
> > > > >
> > > > > wrote:
> > > > > > > +1, but just barely.  We've got quite a number of outstanding
> bugs
> > > > > > > identified, and many of them have fixes in progress.  I'd hate
> to
> > > see
> > > > > those
> > > > > > > efforts get lost in a post-1.0.0 flood of new features targeted
> at
> > > > > 1.1.0 --
> > > > > > > in other words, I'd like to see 1.0.1 retain a high priority
> > > relative
> > > > > to
> > > > > > > 1.1.0.
> > > > > > >
> > > > > > > Looking through the unresolved JIRAs, it doesn't look like any
> of
> > > the
> > > > > > > identified bugs are show-stoppers or strictly regressions
> > > (although I
> > > > > will
> > > > > > > note that one that I have in progress, SPARK-1749, is a bug
> that we
> > > > > > > introduced with recent work -- it's not strictly a regression
> > > because
> > > > > we
> > > > > > > had equally bad but different behavior when the DAGScheduler
> > > > exceptions
> > > > > > > weren't previously being handled at all vs. being slightly
> > > > mis-handled
> > > > > > > now), so I'm not currently seeing a reason not to release.
> > > > >
> > > >
> > >
>

--047d7beb9ec01de30e04f99dbde4--

From dev-return-7652-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 21:05:21 2014
Return-Path: <dev-return-7652-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0843E11BD7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 21:05:21 +0000 (UTC)
Received: (qmail 89291 invoked by uid 500); 17 May 2014 20:40:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79269 invoked by uid 500); 17 May 2014 20:15:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70687 invoked by uid 99); 17 May 2014 19:50:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 19:50:33 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.47 as permitted sender)
Received: from [209.85.160.47] (HELO mail-pb0-f47.google.com) (209.85.160.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 19:50:28 +0000
Received: by mail-pb0-f47.google.com with SMTP id rp16so4019688pbb.6
        for <dev@spark.apache.org>; Sat, 17 May 2014 12:50:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=from:content-type:message-id:mime-version:subject:date:references
         :to:in-reply-to;
        bh=ESYqFTu/ewJwAfVMxnQxeUzs1xuJhS7BiD4AZj5jjPo=;
        b=c2VhXH2RXX0nYh3OQerSHf6rDt1SpM4CXVqOCk5KWx8w53ospGjwI+FGXglOIDAckl
         UeNeifgeUz///QwXCuNhVvnRHlNsv4RAGv9io1DNySvmr+G2zkcslFwTWM1OzPCJzoEk
         bQJUSNR9IfMlIRPOE7uws64xetm6hurhdCQGRYfOvSzMxDR1lNzCpZLL4r3iCyCfKn7n
         RNONXt/KY6BLBwvc+dTRvAh/KZhjiIWekbmjxdbo3nJRLhQ09+E5YmY9TqE5MCD+WlQz
         Ihw/X4juWgS2cjE7epzLfqnk3PvTpTrUlNO47ihaPaLWN9AdzY5N5jR5FhV7eR9X5bK+
         bROQ==
X-Received: by 10.68.106.194 with SMTP id gw2mr31044574pbb.85.1400356208101;
        Sat, 17 May 2014 12:50:08 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id ov4sm21354952pbc.46.2014.05.17.12.50.03
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sat, 17 May 2014 12:50:05 -0700 (PDT)
From: Matei Zaharia <matei.zaharia@gmail.com>
Content-Type: multipart/alternative; boundary="Apple-Mail=_7A40E965-7562-45AD-9455-FFCF7C86FA62"
Message-Id: <75094F23-AACD-4427-AE5B-A25AC02189A8@gmail.com>
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
Date: Sat, 17 May 2014 12:50:01 -0700
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com> <CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com> <CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com> <CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com> <CA+-p3AEL6ct-bJwkBSXm+wTLBHEbBoGnnGOJGAjWHvQogDs-4g@mail.gmail.com> <CALRHqP-jLhJM3_5qCTWzrbU+GpmVsd2DFtvMXsH4-PGuT6X8ww@mail.gmail.com>
To: dev@spark.apache.org
In-Reply-To: <CALRHqP-jLhJM3_5qCTWzrbU+GpmVsd2DFtvMXsH4-PGuT6X8ww@mail.gmail.com>
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_7A40E965-7562-45AD-9455-FFCF7C86FA62
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=windows-1252

As others have said, the 1.0 milestone is about API stability, not about =
saying =93we=92ve eliminated all bugs=94. The sooner you declare 1.0, =
the sooner users can confidently build on Spark, knowing that the =
application they build today will still run on Spark 1.9.9 three years =
from now. This is something that I=92ve seen done badly (and experienced =
the effects thereof) in other big data projects, such as MapReduce and =
even YARN. The result is that you annoy users, you end up with a =
fragmented userbase where everyone is building against a different =
version, and you drastically slow down development.

With a project as fast-growing as fast-growing as Spark in particular, =
there will be new bugs discovered and reported continuously, especially =
in the non-core components. Look at the graph of # of contributors in =
time to Spark: https://www.ohloh.net/p/apache-spark (bottom-most graph; =
=93commits=94 changed when we started merging each patch as a single =
commit). This is not slowing down, and we need to have the culture now =
that we treat API stability and release numbers at the level expected =
for a 1.0 project instead of having people come in and randomly change =
the API.

I=92ll also note that the issues marked =93blocker=94 were marked so by =
their reporters, since the reporter can set the priority. I don=92t =
consider stuff like parallelize() not partitioning ranges in the same =
way as other collections a blocker =97 it=92s a bug, it would be good to =
fix it, but it only affects a small number of use cases. Of course if we =
find a real blocker (in particular a regression from a previous version, =
or a feature that=92s just completely broken), we will delay the release =
for that, but at some point you have to say =93okay, this fix will go =
into the next maintenance release=94. Maybe we need to write a clear =
policy for what the issue priorities mean.

Finally, I believe it=92s much better to have a culture where you can =
make releases on a regular schedule, and have the option to make a =
maintenance release in 3-4 days if you find new bugs, than one where you =
pile up stuff into each release. This is what much large project than =
us, like Linux, do, and it=92s the only way to avoid indefinite stalling =
with a large contributor base. In the worst case, if you find a new bug =
that warrants immediate release, it goes into 1.0.1 a week after 1.0.0 =
(we can vote on 1.0.1 in three days with just your bug fix in it). And =
if you find an API that you=92d like to improve, just add a new one and =
maybe deprecate the old one =97 at some point we have to respect our =
users and let them know that code they write today will still run =
tomorrow.

Matei

On May 17, 2014, at 10:32 AM, Kan Zhang <kzhang@apache.org> wrote:

> +1 on the running commentary here, non-binding of course :-)
>=20
>=20
> On Sat, May 17, 2014 at 8:44 AM, Andrew Ash <andrew@andrewash.com> =
wrote:
>=20
>> +1 on the next release feeling more like a 0.10 than a 1.0
>> On May 17, 2014 4:38 AM, "Mridul Muralidharan" <mridul@gmail.com> =
wrote:
>>=20
>>> I had echoed similar sentiments a while back when there was a =
discussion
>>> around 0.10 vs 1.0 ... I would have preferred 0.10 to stabilize the =
api
>>> changes, add missing functionality, go through a hardening release =
before
>>> 1.0
>>>=20
>>> But the community preferred a 1.0 :-)
>>>=20
>>> Regards,
>>> Mridul
>>>=20
>>> On 17-May-2014 3:19 pm, "Sean Owen" <sowen@cloudera.com> wrote:
>>>>=20
>>>> On this note, non-binding commentary:
>>>>=20
>>>> Releases happen in local minima of change, usually created by
>>>> internally enforced code freeze. Spark is incredibly busy now due =
to
>>>> external factors -- recently a TLP, recently discovered by a large =
new
>>>> audience, ease of contribution enabled by Github. It's getting like
>>>> the first year of mainstream battle-testing in a month. It's been =
very
>>>> hard to freeze anything! I see a number of non-trivial issues being
>>>> reported, and I don't think it has been possible to triage all of
>>>> them, even.
>>>>=20
>>>> Given the high rate of change, my instinct would have been to =
release
>>>> 0.10.0 now. But won't it always be very busy? I do think the rate =
of
>>>> significant issues will slow down.
>>>>=20
>>>> Version ain't nothing but a number, but if it has any meaning it's =
the
>>>> semantic versioning meaning. 1.0 imposes extra handicaps around
>>>> striving to maintain backwards-compatibility. That may end up being
>>>> bent to fit in important changes that are going to be required in =
this
>>>> continuing period of change. Hadoop does this all the time
>>>> unfortunately and gets away with it, I suppose -- minor version
>>>> releases are really major. (On the other extreme, HBase is at 0.98 =
and
>>>> quite production-ready.)
>>>>=20
>>>> Just consider this a second vote for focus on fixes and 1.0.x =
rather
>>>> than new features and 1.x. I think there are a few steps that could
>>>> streamline triage of this flood of contributions, and make all of =
this
>>>> easier, but that's for another thread.
>>>>=20
>>>>=20
>>>> On Fri, May 16, 2014 at 8:50 PM, Mark Hamstra =
<mark@clearstorydata.com
>>>=20
>>> wrote:
>>>>> +1, but just barely.  We've got quite a number of outstanding bugs
>>>>> identified, and many of them have fixes in progress.  I'd hate to =
see
>>> those
>>>>> efforts get lost in a post-1.0.0 flood of new features targeted at
>>> 1.1.0 --
>>>>> in other words, I'd like to see 1.0.1 retain a high priority =
relative
>>> to
>>>>> 1.1.0.
>>>>>=20
>>>>> Looking through the unresolved JIRAs, it doesn't look like any of =
the
>>>>> identified bugs are show-stoppers or strictly regressions =
(although I
>>> will
>>>>> note that one that I have in progress, SPARK-1749, is a bug that =
we
>>>>> introduced with recent work -- it's not strictly a regression =
because
>>> we
>>>>> had equally bad but different behavior when the DAGScheduler
>> exceptions
>>>>> weren't previously being handled at all vs. being slightly
>> mis-handled
>>>>> now), so I'm not currently seeing a reason not to release.
>>>=20
>>=20


--Apple-Mail=_7A40E965-7562-45AD-9455-FFCF7C86FA62--

From dev-return-7653-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 21:13:21 2014
Return-Path: <dev-return-7653-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DBC1611BF6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 21:13:20 +0000 (UTC)
Received: (qmail 9469 invoked by uid 500); 17 May 2014 21:05:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8585 invoked by uid 500); 17 May 2014 21:05:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1471 invoked by uid 99); 17 May 2014 20:41:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 20:41:06 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.216.180 as permitted sender)
Received: from [209.85.216.180] (HELO mail-qc0-f180.google.com) (209.85.216.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 20:41:01 +0000
Received: by mail-qc0-f180.google.com with SMTP id i17so6687522qcy.11
        for <dev@spark.apache.org>; Sat, 17 May 2014 13:40:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=crdhMp13ScFT5rl611XQnSIQkNttioWL0RILgFq0OCs=;
        b=dwE4hXAnAb588aDALJh67Q+LIgVqnTiwu1/kOr4u2kS9PljSNUr/euFscIrhLzB40l
         uRa8Li3oicMTmNDMNQ9JbD3aNfu+6GpiUNEAymwEzcwD7P7W40rRq0HZap5AkL0TMGPy
         uloDazliKVcAu94fxkopznFZh4b9r7LVDW9JnScz2f4+E4sxSNZISbouczMltuvS0MN1
         YWToeDZ93jiVtwZ8UASidavhyPB590JAJ+vvyB3ae74UIOuCeWz1CGO0RpiJqY3ix68y
         1UP+ALaR2uwXu+EdnH6B/p8vhYlW261zk4w+nkZpjFWb5A4chS/L60wIXS7HTF05ZBf0
         tMSQ==
MIME-Version: 1.0
X-Received: by 10.224.79.143 with SMTP id p15mr34708148qak.57.1400359240595;
 Sat, 17 May 2014 13:40:40 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Sat, 17 May 2014 13:40:40 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Sat, 17 May 2014 13:40:40 -0700 (PDT)
In-Reply-To: <CAAsvFPkAt=Q2-gLrXLzqV_NVeaxmFTpWwXFxusqhzZ+h3u7Szg@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
	<CAAsvFP=gm_bQc38_DaDbYNUk_2NQxoA=3rqVRCGt1gM1i=Zeug@mail.gmail.com>
	<CAJiQeYLW4UV+wf++_kozHiYb0vK_50GS3marjKp6EFgAXGfn-A@mail.gmail.com>
	<CAAsvFPku_gueRLTmXE-yRjAGGcG-_jSvRGvP==RWSazro7zwvA@mail.gmail.com>
	<CAJiQeY+ifxMt_Q=kNCMRCdbrnhEfezFMZyjM_U_LW=5X02Fu2g@mail.gmail.com>
	<CAAsvFPkAt=Q2-gLrXLzqV_NVeaxmFTpWwXFxusqhzZ+h3u7Szg@mail.gmail.com>
Date: Sun, 18 May 2014 02:10:40 +0530
Message-ID: <CAJiQeYJxymFF0Q_XTHRXZxo2vaJ9gnzGe4RnDn1Ya8CmzLQ6=A@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bf0e56817ffc504f99e8bca
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bf0e56817ffc504f99e8bca
Content-Type: text/plain; charset=UTF-8

On 18-May-2014 1:45 am, "Mark Hamstra" <mark@clearstorydata.com> wrote:
>
> I'm not trying to muzzle the discussion.  All I am saying is that we don't
> need to have the same discussion about 0.10 vs. 1.0 that we already had.

Agreed, no point in repeating the same discussion ... I am also trying to
understand what the concerns are.

Specifically though, the scope of 1.0 (in terms of changes) went up quite a
bit - a lot of which are new changes and features; not just the initially
envisioned api changes and stability fixes.

If this is raising concerns, particularly since lot of users are depending
on stability of spark interfaces (api, env, scripts, behavior); I want to
understand better what they are - and if they are legitimately serious
enough, we will need to revisit decision to go to 1.0 instead of 0.10 ...
I hope we don't need to though given how late we are in dev cycle

Regards
Mridul

>  If you can tell me about specific changes in the current release
candidate
> that occasion new arguments for why a 1.0 release is an unacceptable idea,
> then I'm listening.
>
>
> On Sat, May 17, 2014 at 11:59 AM, Mridul Muralidharan <mridul@gmail.com
>wrote:
>
> > On 17-May-2014 11:40 pm, "Mark Hamstra" <mark@clearstorydata.com> wrote:
> > >
> > > That is a past issue that we don't need to be re-opening now.  The
> > present
> >
> > Huh ? If we need to revisit based on changed circumstances, we must -
the
> > scope of changes introduced in this release was definitely not
anticipated
> > when 1.0 vs 0.10 discussion happened.
> >
> > If folks are worried about stability of core; it is a valid concern IMO.
> >
> > Having said that, I am still ok with going to 1.0; but if a conversation
> > starts about need for 1.0 vs going to 0.10 I want to hear more and
possibly
> > allay the concerns and not try to muzzle the discussion.
> >
> >
> > Regards
> > Mridul
> >
> > > issue, and what I am asking, is which pending bug fixes does anyone
> > > anticipate will require breaking the public API guaranteed in rc9
> > >
> > >
> > > On Sat, May 17, 2014 at 9:44 AM, Mridul Muralidharan <mridul@gmail.com
> > >wrote:
> > >
> > > > We made incompatible api changes whose impact we don't know yet
> > completely
> > > > : both from implementation and usage point of view.
> > > >
> > > > We had the option of getting real-world feedback from the user
> > community if
> > > > we had gone to 0.10 but the spark developers seemed to be in a
hurry to
> > get
> > > > to 1.0 - so I made my opinion known but left it to the wisdom of
larger
> > > > group of committers to decide ... I did not think it was critical
> > enough to
> > > > do a binding -1 on.
> > > >
> > > > Regards
> > > > Mridul
> > > > On 17-May-2014 9:43 pm, "Mark Hamstra" <mark@clearstorydata.com>
> > wrote:
> > > >
> > > > > Which of the unresolved bugs in spark-core do you think will
require
> > an
> > > > > API-breaking change to fix?  If there are none of those, then we
are
> > > > still
> > > > > essentially on track for a 1.0.0 release.
> > > > >
> > > > > The number of contributions and pace of change now is quite high,
but
> > I
> > > > > don't think that waiting for the pace to slow before releasing
1.0 is
> > > > > viable.  If Spark's short history is any guide to its near future,
> > the
> > > > pace
> > > > > will not slow by any significant amount for any noteworthy length
of
> > > > time,
> > > > > but rather will continue to increase.  What we need to be aiming
for,
> > I
> > > > > think, is to have the great majority of those new contributions
being
> > > > made
> > > > > to MLLlib, GraphX, SparkSQL and other areas of the code that we
have
> > > > > clearly marked as not frozen in 1.x. I think we are already seeing
> > that,
> > > > > but if I am just not recognizing breakage of our semantic
versioning
> > > > > guarantee that will be forced on us by some pending changes, now
> > would
> > > > be a
> > > > > good time to set me straight.
> > > > >
> > > > >
> > > > > On Sat, May 17, 2014 at 4:26 AM, Mridul Muralidharan <
> > mridul@gmail.com
> > > > > >wrote:
> > > > >
> > > > > > I had echoed similar sentiments a while back when there was a
> > > > discussion
> > > > > > around 0.10 vs 1.0 ... I would have preferred 0.10 to stabilize
the
> > api
> > > > > > changes, add missing functionality, go through a hardening
release
> > > > before
> > > > > > 1.0
> > > > > >
> > > > > > But the community preferred a 1.0 :-)
> > > > > >
> > > > > > Regards,
> > > > > > Mridul
> > > > > >
> > > > > > On 17-May-2014 3:19 pm, "Sean Owen" <sowen@cloudera.com> wrote:
> > > > > > >
> > > > > > > On this note, non-binding commentary:
> > > > > > >
> > > > > > > Releases happen in local minima of change, usually created by
> > > > > > > internally enforced code freeze. Spark is incredibly busy now
due
> > to
> > > > > > > external factors -- recently a TLP, recently discovered by a
> > large
> > > > new
> > > > > > > audience, ease of contribution enabled by Github. It's getting
> > like
> > > > > > > the first year of mainstream battle-testing in a month. It's
been
> > > > very
> > > > > > > hard to freeze anything! I see a number of non-trivial issues
> > being
> > > > > > > reported, and I don't think it has been possible to triage
all of
> > > > > > > them, even.
> > > > > > >
> > > > > > > Given the high rate of change, my instinct would have been to
> > release
> > > > > > > 0.10.0 now. But won't it always be very busy? I do think the
rate
> > of
> > > > > > > significant issues will slow down.
> > > > > > >
> > > > > > > Version ain't nothing but a number, but if it has any meaning
> > it's
> > > > the
> > > > > > > semantic versioning meaning. 1.0 imposes extra handicaps
around
> > > > > > > striving to maintain backwards-compatibility. That may end up
> > being
> > > > > > > bent to fit in important changes that are going to be
required in
> > > > this
> > > > > > > continuing period of change. Hadoop does this all the time
> > > > > > > unfortunately and gets away with it, I suppose -- minor
version
> > > > > > > releases are really major. (On the other extreme, HBase is at
> > 0.98
> > > > and
> > > > > > > quite production-ready.)
> > > > > > >
> > > > > > > Just consider this a second vote for focus on fixes and 1.0.x
> > rather
> > > > > > > than new features and 1.x. I think there are a few steps that
> > could
> > > > > > > streamline triage of this flood of contributions, and make
all of
> > > > this
> > > > > > > easier, but that's for another thread.
> > > > > > >
> > > > > > >
> > > > > > > On Fri, May 16, 2014 at 8:50 PM, Mark Hamstra <
> > > > mark@clearstorydata.com
> > > > > >
> > > > > > wrote:
> > > > > > > > +1, but just barely.  We've got quite a number of
outstanding
> > bugs
> > > > > > > > identified, and many of them have fixes in progress.  I'd
hate
> > to
> > > > see
> > > > > > those
> > > > > > > > efforts get lost in a post-1.0.0 flood of new features
targeted
> > at
> > > > > > 1.1.0 --
> > > > > > > > in other words, I'd like to see 1.0.1 retain a high priority
> > > > relative
> > > > > > to
> > > > > > > > 1.1.0.
> > > > > > > >
> > > > > > > > Looking through the unresolved JIRAs, it doesn't look like
any
> > of
> > > > the
> > > > > > > > identified bugs are show-stoppers or strictly regressions
> > > > (although I
> > > > > > will
> > > > > > > > note that one that I have in progress, SPARK-1749, is a bug
> > that we
> > > > > > > > introduced with recent work -- it's not strictly a
regression
> > > > because
> > > > > > we
> > > > > > > > had equally bad but different behavior when the DAGScheduler
> > > > > exceptions
> > > > > > > > weren't previously being handled at all vs. being slightly
> > > > > mis-handled
> > > > > > > > now), so I'm not currently seeing a reason not to release.
> > > > > >
> > > > >
> > > >
> >

--047d7bf0e56817ffc504f99e8bca--

From dev-return-7655-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 22:03:22 2014
Return-Path: <dev-return-7655-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2186111C6F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 22:03:22 +0000 (UTC)
Received: (qmail 48898 invoked by uid 500); 17 May 2014 21:55:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28971 invoked by uid 500); 17 May 2014 21:30:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27349 invoked by uid 99); 17 May 2014 21:21:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 21:21:07 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.41 as permitted sender)
Received: from [209.85.220.41] (HELO mail-pa0-f41.google.com) (209.85.220.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 21:21:01 +0000
Received: by mail-pa0-f41.google.com with SMTP id lj1so4025737pab.14
        for <dev@spark.apache.org>; Sat, 17 May 2014 14:20:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=kHWWWd2KxsiWhP9YxTEob0roa+QEGoIc1og88qVV9WM=;
        b=efMT2iAtRtnKYibMEYa+4pmBOEH/tGGHWLLC07UdeA6HZF/q8dtTM61jd+rXiKgy5T
         J+MCcRcKdhLtCJU7uY2N1FET/ZkBpvTJ8bwzRdQD2M4wCroA6KIk5FhVb7/k5ByYFqSh
         3x9/hBMRnghheN+uKhTF3zpW0u5J+i/bi1vF0wMW8XS7hh4OFDjyTVeCtA3WsbilhwAI
         6IpDXLNOUzjtFWaKfuN6JuGxvZIWeMoQZKiqWgQGkNTI08X2GA+kQWmdce9O7PmIQ1Dp
         Lb5hje49EIDlKDX6qnkHNdOlrPPI/XFdXb+461+rpVS4pePHhJ9LCPAMahq6pKphgAKE
         /Llw==
X-Received: by 10.69.19.225 with SMTP id gx1mr31542063pbd.34.1400361640909;
        Sat, 17 May 2014 14:20:40 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id gj9sm21593581pbc.7.2014.05.17.14.20.37
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sat, 17 May 2014 14:20:38 -0700 (PDT)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAJiQeY+eGxSzM-6C_poKCY53Gkixg85rdXiXXx+qi0z0bvCrrQ@mail.gmail.com>
Date: Sat, 17 May 2014 14:20:35 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <6EF34F88-F79D-49B0-92FD-C421FBB0B6ED@gmail.com>
References: <JIRA.12714626.1400189782796@arcas> <JIRA.12714626.1400189782796.363092.1400238099511@arcas> <CAJiQeY+zsu3cfX1zyA=qK1KO8hirddmjGSB7HohTTA_a9jk3wQ@mail.gmail.com> <CAJgQjQ_1qrz4=05E=Wwj+O4UqKHwFKWQQiZosKbPjvwocMz9mw@mail.gmail.com> <CAJiQeY+eGxSzM-6C_poKCY53Gkixg85rdXiXXx+qi0z0bvCrrQ@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

We do actually have replicated StorageLevels in Spark. You can use =
MEMORY_AND_DISK_2 or construct your own StorageLevel with your own =
custom replication factor.

BTW you guys should probably have this discussion on the JIRA rather =
than the dev list; I think the replies somehow ended up on the dev list.

Matei

On May 17, 2014, at 1:36 AM, Mridul Muralidharan <mridul@gmail.com> =
wrote:

> We don't have 3x replication in spark :-)
> And if we use replicated storagelevel, while decreasing odds of =
failure, it
> does not eliminate it (since we are not doing a great job with =
replication
> anyway from fault tolerance point of view).
> Also it does take a nontrivial performance hit with replicated levels.
>=20
> Regards,
> Mridul
> On 17-May-2014 8:16 am, "Xiangrui Meng" <mengxr@gmail.com> wrote:
>=20
>> With 3x replication, we should be able to achieve fault tolerance.
>> This checkPointed RDD can be cleared if we have another in-memory
>> checkPointed RDD down the line. It can avoid hitting disk if we have
>> enough memory to use. We need to investigate more to find a good
>> solution. -Xiangrui
>>=20
>> On Fri, May 16, 2014 at 4:00 PM, Mridul Muralidharan =
<mridul@gmail.com>
>> wrote:
>>> Effectively this is persist without fault tolerance.
>>> Failure of any node means complete lack of fault tolerance.
>>> I would be very skeptical of truncating lineage if it is not =
reliable.
>>> On 17-May-2014 3:49 am, "Xiangrui Meng (JIRA)" <jira@apache.org> =
wrote:
>>>=20
>>>> Xiangrui Meng created SPARK-1855:
>>>> ------------------------------------
>>>>=20
>>>>             Summary: Provide memory-and-local-disk RDD =
checkpointing
>>>>                 Key: SPARK-1855
>>>>                 URL: =
https://issues.apache.org/jira/browse/SPARK-1855
>>>>             Project: Spark
>>>>          Issue Type: New Feature
>>>>          Components: MLlib, Spark Core
>>>>    Affects Versions: 1.0.0
>>>>            Reporter: Xiangrui Meng
>>>>=20
>>>>=20
>>>> Checkpointing is used to cut long lineage while maintaining fault
>>>> tolerance. The current implementation is HDFS-based. Using the =
BlockRDD
>> we
>>>> can create in-memory-and-local-disk (with replication) checkpoints =
that
>> are
>>>> not as reliable as HDFS-based solution but faster.
>>>>=20
>>>> It can help applications that require many iterations.
>>>>=20
>>>>=20
>>>>=20
>>>> --
>>>> This message was sent by Atlassian JIRA
>>>> (v6.2#6252)
>>>>=20
>>=20


From dev-return-7654-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 22:03:22 2014
Return-Path: <dev-return-7654-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 71D0A11C81
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 22:03:22 +0000 (UTC)
Received: (qmail 30290 invoked by uid 500); 17 May 2014 21:30:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28111 invoked by uid 500); 17 May 2014 21:30:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27428 invoked by uid 99); 17 May 2014 21:22:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 21:22:43 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.50 as permitted sender)
Received: from [209.85.220.50] (HELO mail-pa0-f50.google.com) (209.85.220.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 21:22:37 +0000
Received: by mail-pa0-f50.google.com with SMTP id fb1so4055436pad.9
        for <dev@spark.apache.org>; Sat, 17 May 2014 14:22:13 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=7OqkDI2CH6KUd361gYhP150RSNGmneUVlAm1YHnzSag=;
        b=GRHDAjICn5VgD8fIuN7XA9nIU68gGEvPgvnrxOlp9TlK2bcofgyqzK6+hA7TX1HIFs
         ED0pHSEYXKS05zVR0zJcbkvG50wCmw47pA0soVfYWX40YLB/eB0pgS2zHABqPLTcbWwT
         glfTUhL1qR7ohilmP7b2/owpuAnNgMiozsLmA4nS87SUl1wdotsWaK8SxfVKnR0oWnnC
         nayec2ieh26YHyNkIWiQnVqaCnBGGResQZ3Emwlgk6dj+KGdkhKdrueOf9ugix2v79Ma
         poAXnquhnWIg38T9q6JhqV9S3jiGpFK2EyLefcPmkde0iGrcorToCaJz77HNXzvFLDGC
         UzgA==
X-Received: by 10.66.231.40 with SMTP id td8mr31267049pac.103.1400361733475;
        Sat, 17 May 2014 14:22:13 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id xk1sm52523454pac.21.2014.05.17.14.22.02
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sat, 17 May 2014 14:22:09 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <6EF34F88-F79D-49B0-92FD-C421FBB0B6ED@gmail.com>
Date: Sat, 17 May 2014 14:22:01 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <76860AFC-D1D1-4D4C-AB36-34F17E2A3ECC@gmail.com>
References: <JIRA.12714626.1400189782796@arcas> <JIRA.12714626.1400189782796.363092.1400238099511@arcas> <CAJiQeY+zsu3cfX1zyA=qK1KO8hirddmjGSB7HohTTA_a9jk3wQ@mail.gmail.com> <CAJgQjQ_1qrz4=05E=Wwj+O4UqKHwFKWQQiZosKbPjvwocMz9mw@mail.gmail.com> <CAJiQeY+eGxSzM-6C_poKCY53Gkixg85rdXiXXx+qi0z0bvCrrQ@mail.gmail.com> <6EF34F88-F79D-49B0-92FD-C421FBB0B6ED@gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

BTW for what it=92s worth I agree this is a good option to add, the only =
tricky thing will be making sure the checkpoint blocks are not =
garbage-collected by the block store. I don=92t think they will be =
though.

Matei
On May 17, 2014, at 2:20 PM, Matei Zaharia <matei.zaharia@gmail.com> =
wrote:

> We do actually have replicated StorageLevels in Spark. You can use =
MEMORY_AND_DISK_2 or construct your own StorageLevel with your own =
custom replication factor.
>=20
> BTW you guys should probably have this discussion on the JIRA rather =
than the dev list; I think the replies somehow ended up on the dev list.
>=20
> Matei
>=20
> On May 17, 2014, at 1:36 AM, Mridul Muralidharan <mridul@gmail.com> =
wrote:
>=20
>> We don't have 3x replication in spark :-)
>> And if we use replicated storagelevel, while decreasing odds of =
failure, it
>> does not eliminate it (since we are not doing a great job with =
replication
>> anyway from fault tolerance point of view).
>> Also it does take a nontrivial performance hit with replicated =
levels.
>>=20
>> Regards,
>> Mridul
>> On 17-May-2014 8:16 am, "Xiangrui Meng" <mengxr@gmail.com> wrote:
>>=20
>>> With 3x replication, we should be able to achieve fault tolerance.
>>> This checkPointed RDD can be cleared if we have another in-memory
>>> checkPointed RDD down the line. It can avoid hitting disk if we have
>>> enough memory to use. We need to investigate more to find a good
>>> solution. -Xiangrui
>>>=20
>>> On Fri, May 16, 2014 at 4:00 PM, Mridul Muralidharan =
<mridul@gmail.com>
>>> wrote:
>>>> Effectively this is persist without fault tolerance.
>>>> Failure of any node means complete lack of fault tolerance.
>>>> I would be very skeptical of truncating lineage if it is not =
reliable.
>>>> On 17-May-2014 3:49 am, "Xiangrui Meng (JIRA)" <jira@apache.org> =
wrote:
>>>>=20
>>>>> Xiangrui Meng created SPARK-1855:
>>>>> ------------------------------------
>>>>>=20
>>>>>            Summary: Provide memory-and-local-disk RDD =
checkpointing
>>>>>                Key: SPARK-1855
>>>>>                URL: =
https://issues.apache.org/jira/browse/SPARK-1855
>>>>>            Project: Spark
>>>>>         Issue Type: New Feature
>>>>>         Components: MLlib, Spark Core
>>>>>   Affects Versions: 1.0.0
>>>>>           Reporter: Xiangrui Meng
>>>>>=20
>>>>>=20
>>>>> Checkpointing is used to cut long lineage while maintaining fault
>>>>> tolerance. The current implementation is HDFS-based. Using the =
BlockRDD
>>> we
>>>>> can create in-memory-and-local-disk (with replication) checkpoints =
that
>>> are
>>>>> not as reliable as HDFS-based solution but faster.
>>>>>=20
>>>>> It can help applications that require many iterations.
>>>>>=20
>>>>>=20
>>>>>=20
>>>>> --
>>>>> This message was sent by Atlassian JIRA
>>>>> (v6.2#6252)
>>>>>=20
>>>=20
>=20


From dev-return-7656-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 22:20:22 2014
Return-Path: <dev-return-7656-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BD16711CCD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 22:20:22 +0000 (UTC)
Received: (qmail 51222 invoked by uid 500); 17 May 2014 21:55:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31604 invoked by uid 500); 17 May 2014 21:30:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19280 invoked by uid 99); 17 May 2014 21:06:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 21:06:06 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.216.177 as permitted sender)
Received: from [209.85.216.177] (HELO mail-qc0-f177.google.com) (209.85.216.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 21:06:03 +0000
Received: by mail-qc0-f177.google.com with SMTP id i17so6514410qcy.8
        for <dev@spark.apache.org>; Sat, 17 May 2014 14:05:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=mXdd19KJQmvq3gzoySUn325evRmqGgFSCSGPBcni/RQ=;
        b=rSlsiczydmWvx1SQ/moEEVI3wA1TtYDWLPQszRC10etriD1AbN0oSrjiWwvzA+34Vv
         nrbDtmypl1EKssWIWNPiGCSVHjkQRBNgwWR/qlwG328Lm5cQHVw1xqcrovspONN+IaEc
         /WXu40KEHPXNsK8vpBgf5Ax6aorpb5HSIELQfRDw1yJjRr0stk5Uu/v8UnjhtfqSJsqy
         FDrhxCyNuSOgy3e1rjf1drBU6Hf+XySKClXUMpVcLR8DjTIS6NdezBYt13yrB+Xfuq7i
         kzhYYuvrj9T+7U0fixWf7p6CDAa5t1RuKiJfdvE8qnDYcjPF49Zlk3ErC2AJfXzEh13I
         sD9Q==
MIME-Version: 1.0
X-Received: by 10.140.81.74 with SMTP id e68mr34953327qgd.77.1400360739714;
 Sat, 17 May 2014 14:05:39 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Sat, 17 May 2014 14:05:39 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Sat, 17 May 2014 14:05:39 -0700 (PDT)
In-Reply-To: <75094F23-AACD-4427-AE5B-A25AC02189A8@gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
	<CA+-p3AEL6ct-bJwkBSXm+wTLBHEbBoGnnGOJGAjWHvQogDs-4g@mail.gmail.com>
	<CALRHqP-jLhJM3_5qCTWzrbU+GpmVsd2DFtvMXsH4-PGuT6X8ww@mail.gmail.com>
	<75094F23-AACD-4427-AE5B-A25AC02189A8@gmail.com>
Date: Sun, 18 May 2014 02:35:39 +0530
Message-ID: <CAJiQeY+Sy4EUKhM8ztsSfxaNoOG_73b=s7W_u5YRBPk-JegUDA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c11d1272bef104f99ee41c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c11d1272bef104f99ee41c
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I would make the case for interface stability not just api stability.
Particularly given that we have significantly changed some of our
interfaces, I want to ensure developers/users are not seeing red flags.

Bugs and code stability can be addressed in minor releases if found, but
behavioral change and/or interface changes would be a much more invasive
issue for our users.

Regards
Mridul
On 18-May-2014 2:19 am, "Matei Zaharia" <matei.zaharia@gmail.com> wrote:

> As others have said, the 1.0 milestone is about API stability, not about
> saying =E2=80=9Cwe=E2=80=99ve eliminated all bugs=E2=80=9D. The sooner yo=
u declare 1.0, the sooner
> users can confidently build on Spark, knowing that the application they
> build today will still run on Spark 1.9.9 three years from now. This is
> something that I=E2=80=99ve seen done badly (and experienced the effects =
thereof)
> in other big data projects, such as MapReduce and even YARN. The result i=
s
> that you annoy users, you end up with a fragmented userbase where everyon=
e
> is building against a different version, and you drastically slow down
> development.
>
> With a project as fast-growing as fast-growing as Spark in particular,
> there will be new bugs discovered and reported continuously, especially i=
n
> the non-core components. Look at the graph of # of contributors in time t=
o
> Spark: https://www.ohloh.net/p/apache-spark (bottom-most graph; =E2=80=9C=
commits=E2=80=9D
> changed when we started merging each patch as a single commit). This is n=
ot
> slowing down, and we need to have the culture now that we treat API
> stability and release numbers at the level expected for a 1.0 project
> instead of having people come in and randomly change the API.
>
> I=E2=80=99ll also note that the issues marked =E2=80=9Cblocker=E2=80=9D w=
ere marked so by their
> reporters, since the reporter can set the priority. I don=E2=80=99t consi=
der stuff
> like parallelize() not partitioning ranges in the same way as other
> collections a blocker =E2=80=94 it=E2=80=99s a bug, it would be good to f=
ix it, but it only
> affects a small number of use cases. Of course if we find a real blocker
> (in particular a regression from a previous version, or a feature that=E2=
=80=99s
> just completely broken), we will delay the release for that, but at some
> point you have to say =E2=80=9Cokay, this fix will go into the next maint=
enance
> release=E2=80=9D. Maybe we need to write a clear policy for what the issu=
e
> priorities mean.
>
> Finally, I believe it=E2=80=99s much better to have a culture where you c=
an make
> releases on a regular schedule, and have the option to make a maintenance
> release in 3-4 days if you find new bugs, than one where you pile up stuf=
f
> into each release. This is what much large project than us, like Linux, d=
o,
> and it=E2=80=99s the only way to avoid indefinite stalling with a large c=
ontributor
> base. In the worst case, if you find a new bug that warrants immediate
> release, it goes into 1.0.1 a week after 1.0.0 (we can vote on 1.0.1 in
> three days with just your bug fix in it). And if you find an API that you=
=E2=80=99d
> like to improve, just add a new one and maybe deprecate the old one =E2=
=80=94 at
> some point we have to respect our users and let them know that code they
> write today will still run tomorrow.
>
> Matei
>
> On May 17, 2014, at 10:32 AM, Kan Zhang <kzhang@apache.org> wrote:
>
> > +1 on the running commentary here, non-binding of course :-)
> >
> >
> > On Sat, May 17, 2014 at 8:44 AM, Andrew Ash <andrew@andrewash.com>
> wrote:
> >
> >> +1 on the next release feeling more like a 0.10 than a 1.0
> >> On May 17, 2014 4:38 AM, "Mridul Muralidharan" <mridul@gmail.com>
> wrote:
> >>
> >>> I had echoed similar sentiments a while back when there was a
> discussion
> >>> around 0.10 vs 1.0 ... I would have preferred 0.10 to stabilize the a=
pi
> >>> changes, add missing functionality, go through a hardening release
> before
> >>> 1.0
> >>>
> >>> But the community preferred a 1.0 :-)
> >>>
> >>> Regards,
> >>> Mridul
> >>>
> >>> On 17-May-2014 3:19 pm, "Sean Owen" <sowen@cloudera.com> wrote:
> >>>>
> >>>> On this note, non-binding commentary:
> >>>>
> >>>> Releases happen in local minima of change, usually created by
> >>>> internally enforced code freeze. Spark is incredibly busy now due to
> >>>> external factors -- recently a TLP, recently discovered by a large n=
ew
> >>>> audience, ease of contribution enabled by Github. It's getting like
> >>>> the first year of mainstream battle-testing in a month. It's been ve=
ry
> >>>> hard to freeze anything! I see a number of non-trivial issues being
> >>>> reported, and I don't think it has been possible to triage all of
> >>>> them, even.
> >>>>
> >>>> Given the high rate of change, my instinct would have been to releas=
e
> >>>> 0.10.0 now. But won't it always be very busy? I do think the rate of
> >>>> significant issues will slow down.
> >>>>
> >>>> Version ain't nothing but a number, but if it has any meaning it's t=
he
> >>>> semantic versioning meaning. 1.0 imposes extra handicaps around
> >>>> striving to maintain backwards-compatibility. That may end up being
> >>>> bent to fit in important changes that are going to be required in th=
is
> >>>> continuing period of change. Hadoop does this all the time
> >>>> unfortunately and gets away with it, I suppose -- minor version
> >>>> releases are really major. (On the other extreme, HBase is at 0.98 a=
nd
> >>>> quite production-ready.)
> >>>>
> >>>> Just consider this a second vote for focus on fixes and 1.0.x rather
> >>>> than new features and 1.x. I think there are a few steps that could
> >>>> streamline triage of this flood of contributions, and make all of th=
is
> >>>> easier, but that's for another thread.
> >>>>
> >>>>
> >>>> On Fri, May 16, 2014 at 8:50 PM, Mark Hamstra <
> mark@clearstorydata.com
> >>>
> >>> wrote:
> >>>>> +1, but just barely.  We've got quite a number of outstanding bugs
> >>>>> identified, and many of them have fixes in progress.  I'd hate to s=
ee
> >>> those
> >>>>> efforts get lost in a post-1.0.0 flood of new features targeted at
> >>> 1.1.0 --
> >>>>> in other words, I'd like to see 1.0.1 retain a high priority relati=
ve
> >>> to
> >>>>> 1.1.0.
> >>>>>
> >>>>> Looking through the unresolved JIRAs, it doesn't look like any of t=
he
> >>>>> identified bugs are show-stoppers or strictly regressions (although=
 I
> >>> will
> >>>>> note that one that I have in progress, SPARK-1749, is a bug that we
> >>>>> introduced with recent work -- it's not strictly a regression becau=
se
> >>> we
> >>>>> had equally bad but different behavior when the DAGScheduler
> >> exceptions
> >>>>> weren't previously being handled at all vs. being slightly
> >> mis-handled
> >>>>> now), so I'm not currently seeing a reason not to release.
> >>>
> >>
>
>

--001a11c11d1272bef104f99ee41c--

From dev-return-7658-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 23:18:31 2014
Return-Path: <dev-return-7658-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C24EA11D75
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 23:18:31 +0000 (UTC)
Received: (qmail 34417 invoked by uid 500); 17 May 2014 23:10:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34345 invoked by uid 500); 17 May 2014 23:10:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33953 invoked by uid 99); 17 May 2014 23:09:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 23:09:01 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of michaelmalak@yahoo.com designates 216.109.114.191 as permitted sender)
Received: from [216.109.114.191] (HELO nm42-vm4.bullet.mail.bf1.yahoo.com) (216.109.114.191)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 23:08:57 +0000
Received: from [66.196.81.174] by nm42.bullet.mail.bf1.yahoo.com with NNFMP; 17 May 2014 23:08:33 -0000
Received: from [98.139.212.216] by tm20.bullet.mail.bf1.yahoo.com with NNFMP; 17 May 2014 23:08:33 -0000
Received: from [127.0.0.1] by omp1025.mail.bf1.yahoo.com with NNFMP; 17 May 2014 23:08:33 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 465334.73458.bm@omp1025.mail.bf1.yahoo.com
Received: (qmail 68309 invoked by uid 60001); 17 May 2014 23:08:33 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.com; s=s1024; t=1400368113; bh=Z+lsvN4f3q2SeeK/sIfeS+XB38ZlTlS3wD7DdYSJc64=; h=References:Message-ID:Date:From:Reply-To:Subject:To:In-Reply-To:MIME-Version:Content-Type; b=sJKZ2OxHLeg/YV31RoogtUMZgzH761uPsLCRBURdLWssLmKKxPMl0sllgsVq+BGnmST2vch4b6d7JnhCL4vavCICPTGhHRomnRWGMkK2xiUVmyNeyQlf2j2RhGYsOJEq0c0151cU3702jJERGGmtQ4w581H85izssA9ftKtqe5k=
X-YMail-OSG: A.mect0VM1mhA9hC4ITbUVQBF3KMipWtYBeQz1Z2ZTwAzuK
 sWxu_npx_aYnxlzn06PwfenliJY7OV4Bv5qd_Ur_hOdHvNwZBnTN8U8WjpAj
 lBIwkR4BBPMVQOrQIasDde3iHvdruDoyc_.DPWNdK7lsB8c8l3OwQkp8fmMo
 FX0ECy7o2ZGjXZeiYYjK7S1vtp_ahqsXdPjAz5ocL8XIfoskwNyHk0NXx53F
 ZC00wNdZIyfVeI8wF3Ex5Vb0nWtfiok3.gJ657xlFXevd3oeUFoPvo8EGyJj
 hMXNDy7bxTcA2siZYuj_Zrg6a7uht2YtJLaH3KsXkmIGvQ1ALygs8i7P6.Co
 m7Kfh9bap6mF.ev1Ob.FIbKykFyawbj9O5pw20_MhbeYSKFxF6NkHTI4rwzH
 xJEpryP1.ohstdPO1PJmf.xFewpuIBhIyDsXDMt.NCipPxWSmpGEqi7joNAy
 ZbmICPKwLypctiT8rpcZgcAYURsoypyM5eZqp2N5nyTHQTXA2mkx0.VnEASL
 0aLLF7RBSQ6_Xobf9KpYGwjV9ueITlskvk0wLLgn98dn1QZEs1xJItdKJku3
 LcTM2Xs2pI48Og.R0sKRDTUKL1Pv7e1TJRETUjMRYH_092jYmfqt4EASluwr
 VQhbM
Received: from [174.29.199.177] by web160801.mail.bf1.yahoo.com via HTTP; Sat, 17 May 2014 16:08:33 PDT
X-Rocket-MIMEInfo: 002.001,V2hpbGUgZGV2ZWxvcGVycyBtYXkgYXBwcmVjaWF0ZSAiMS4wID09IEFQSSBzdGFiaWxpdHksIiBJJ20gbm90IHN1cmUgdGhhdCB3aWxsIGJlIHRoZSB1bmRlcnN0YW5kaW5nIG9mIHRoZSBWUCB3aG8gZ2l2ZXMgdGhlIGdyZWVuIGxpZ2h0IHRvIGEgU3BhcmstYmFzZWQgZGV2ZWxvcG1lbnQgZWZmb3J0LgoKSSBmZWFyIGEgYnVnIHRoYXQgc2lsZW50bHkgcHJvZHVjZXMgZXJyb25lb3VzIHJlc3VsdHMgd2lsbCBiZSBwZXJjZWl2ZWQgbGlrZSB0aGUgRkRJViBidWcsIGJ1dCBpbiB0aGlzIGNhc2Ugd2l0aG91dCABMAEBAQE-
X-Mailer: YahooMailWebService/0.8.188.663
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com> <CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com> <CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com> <CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com> <CA+-p3AEL6ct-bJwkBSXm+wTLBHEbBoGnnGOJGAjWHvQogDs-4g@mail.gmail.com> <CALRHqP-jLhJM3_5qCTWzrbU+GpmVsd2DFtvMXsH4-PGuT6X8ww@mail.gmail.com> <75094F23-AACD-4427-AE5B-A25AC02189A8@gmail.com>
Message-ID: <1400368113.48296.YahooMailNeo@web160801.mail.bf1.yahoo.com>
Date: Sat, 17 May 2014 16:08:33 -0700 (PDT)
From: Michael Malak <michaelmalak@yahoo.com>
Reply-To: Michael Malak <michaelmalak@yahoo.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
To: "dev@spark.apache.org" <dev@spark.apache.org>
In-Reply-To: <75094F23-AACD-4427-AE5B-A25AC02189A8@gmail.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="1146854128-636912239-1400368113=:48296"
X-Virus-Checked: Checked by ClamAV on apache.org

--1146854128-636912239-1400368113=:48296
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

While developers may appreciate "1.0 =3D=3D API stability," I'm not sure th=
at will be the understanding of the VP who gives the green light to a Spark=
-based development effort.=0A=0AI fear a bug that silently produces erroneo=
us results will be perceived like the FDIV bug, but in this case without th=
e momentum of an existing large installed base and with a number of "compet=
itors" (GridGain, H20, Stratosphere). Despite the stated intention of API s=
tability, the perception (which becomes the reality) of "1.0" is that it's =
ready for production use -- not bullet-proof, but also not with known silen=
t generation of erroneous results. Exceptions and crashes are much more tol=
erated than silent corruption of data. The result may be a reputation of th=
e Spark team unconcerned about data integrity.=0A=0AI ran into (and submitt=
ed)=C2=A0https://issues.apache.org/jira/browse/SPARK-1817=C2=A0due to the l=
ack of zipWithIndex(). zip() with a self-created partitioned range was the =
way I was trying to number with IDs a collection of nodes in preparation fo=
r the GraphX constructor. For the record, it was a frequent Spark committer=
 who escalated it to "blocker"; I did not submit it as such. Partitioning a=
 Scala range isn't just a toy example; it has a real-life use.=0A=0AI also =
wonder about the REPL. Cloudera, for example, touts it as key to making Spa=
rk a "crossover tool" that Data Scientists can also use. The REPL can be co=
nsidered an API of sorts -- not a traditional Scala or Java API, of course,=
 but the "API" that a human data analyst would use. With the Scala REPL exh=
ibiting some of the same bad behaviors as the Spark REPL, there is a questi=
on of whether the Spark REPL can even be fixed. If the Spark REPL has to be=
 eliminated after 1.0 due to an inability to repair it, that would constitu=
te API instability.=0A=0A=0A=C2=A0=0AOn Saturday, May 17, 2014 2:49 PM, Mat=
ei Zaharia <matei.zaharia@gmail.com> wrote:=0A =0AAs others have said, the =
1.0 milestone is about API stability, not about saying =E2=80=9Cwe=E2=80=99=
ve eliminated all bugs=E2=80=9D. The sooner you declare 1.0, the sooner use=
rs can confidently build on Spark, knowing that the application they build =
today will still run on Spark 1.9.9 three years from now. This is something=
 that I=E2=80=99ve seen done badly (and experienced the effects thereof) in=
 other big data projects, such as MapReduce and even YARN. The result is th=
at you annoy users, you end up with a fragmented userbase where everyone is=
 building against a different version, and you drastically slow down develo=
pment.=0A=0AWith a project as fast-growing as fast-growing as Spark in part=
icular, there will be new bugs discovered and reported continuously, especi=
ally in the non-core components. Look at the graph of # of contributors in =
time to Spark: https://www.ohloh.net/p/apache-spark (bottom-most graph; =E2=
=80=9Ccommits=E2=80=9D changed when we started merging each patch as a sing=
le commit). This is not slowing down, and we need to have the culture now t=
hat we treat API stability and release numbers at the level expected for a =
1.0 project instead of having people come in and randomly change the API.=
=0A=0AI=E2=80=99ll also note that the issues marked =E2=80=9Cblocker=E2=80=
=9D were marked so by their reporters, since the reporter can set the prior=
ity. I don=E2=80=99t consider stuff like parallelize() not partitioning ran=
ges in the same way as other collections a blocker =E2=80=94 it=E2=80=99s a=
 bug, it would be good to fix it, but it only affects a small number of use=
 cases. Of course if we find a real blocker (in particular a regression fro=
m a previous version, or a feature that=E2=80=99s just completely broken), =
we will delay the release for that, but at some point you have to say =E2=
=80=9Cokay, this fix will go into the next maintenance release=E2=80=9D. Ma=
ybe we need to write a clear policy for what the issue priorities mean.=0A=
=0AFinally, I believe it=E2=80=99s much better to have a culture where you =
can make releases on a regular schedule, and have the option to make a main=
tenance release in 3-4 days if you find new bugs, than one where you pile u=
p stuff into each release. This is what much large project than us, like Li=
nux, do, and it=E2=80=99s the only way to avoid indefinite stalling with a =
large contributor base. In the worst case, if you find a new bug that warra=
nts immediate release, it goes into 1.0.1 a week after 1.0.0 (we can vote o=
n 1.0.1 in three days with just your bug fix in it). And if you find an API=
 that you=E2=80=99d like to improve, just add a new one and maybe deprecate=
 the old one =E2=80=94 at some point we have to respect our users and let t=
hem know that code they write today will still run tomorrow.=0A=0AMatei=0A=
=0A=0AOn May 17, 2014, at 10:32 AM, Kan Zhang <kzhang@apache.org> wrote:=0A=
=0A> +1 on the running commentary here, non-binding of course :-)=0A> =0A> =
=0A> On Sat, May 17, 2014 at 8:44 AM, Andrew Ash <andrew@andrewash.com> wro=
te:=0A> =0A>> +1 on the next release feeling more like a 0.10 than a 1.0=0A=
>> On May 17, 2014 4:38 AM, "Mridul Muralidharan" <mridul@gmail.com> wrote:=
=0A>> =0A>>> I had echoed similar sentiments a while back when there was a =
discussion=0A>>> around 0.10 vs 1.0 ... I would have preferred 0.10 to stab=
ilize the api=0A>>> changes, add missing functionality, go through a harden=
ing release before=0A>>> 1.0=0A>>> =0A>>> But the community preferred a 1.0=
 :-)=0A>>> =0A>>> Regards,=0A>>> Mridul=0A>>> =0A>>> On 17-May-2014 3:19 pm=
, "Sean Owen" <sowen@cloudera.com> wrote:=0A>>>> =0A>>>> On this note, non-=
binding commentary:=0A>>>> =0A>>>> Releases happen in local minima of chang=
e, usually created by=0A>>>> internally enforced code freeze. Spark is incr=
edibly busy now due to=0A>>>> external factors -- recently a TLP, recently =
discovered by a large new=0A>>>> audience, ease of contribution enabled by =
Github. It's getting like=0A>>>> the first year of mainstream battle-testin=
g in a month. It's been very=0A>>>> hard to freeze anything! I see a number=
 of non-trivial issues being=0A>>>> reported, and I don't think it has been=
 possible to triage all of=0A>>>> them, even.=0A>>>> =0A>>>> Given the high=
 rate of change, my instinct would have been to release=0A>>>> 0.10.0 now. =
But won't it always be very busy? I do think the rate of=0A>>>> significant=
 issues will slow down.=0A>>>> =0A>>>> Version ain't nothing but a number, =
but if it has any meaning it's the=0A>>>> semantic versioning meaning. 1.0 =
imposes extra handicaps around=0A>>>> striving to maintain backwards-compat=
ibility. That may end up being=0A>>>> bent to fit in important changes that=
 are going to be required in this=0A>>>> continuing period of change. Hadoo=
p does this all the time=0A>>>> unfortunately and gets away with it, I supp=
ose -- minor version=0A>>>> releases are really major. (On the other extrem=
e, HBase is at 0.98 and=0A>>>> quite production-ready.)=0A>>>> =0A>>>> Just=
 consider this a second vote for focus on fixes and 1.0.x rather=0A>>>> tha=
n new features and 1.x. I think there are a few steps that could=0A>>>> str=
eamline triage of this flood of contributions, and make all of this=0A>>>> =
easier, but that's for another thread.=0A>>>> =0A>>>> =0A>>>> On Fri, May 1=
6, 2014 at 8:50 PM, Mark Hamstra <mark@clearstorydata.com=0A>>> =0A>>> wrot=
e:=0A>>>>> +1, but just barely.=C2=A0 We've got quite a number of outstandi=
ng bugs=0A>>>>> identified, and many of them have fixes in progress.=C2=A0 =
I'd hate to see=0A>>> those=0A>>>>> efforts get lost in a post-1.0.0 flood =
of new features targeted at=0A>>> 1.1.0 --=0A>>>>> in other words, I'd like=
 to see 1.0.1 retain a high priority relative=0A>>> to=0A>>>>> 1.1.0.=0A>>>=
>> =0A>>>>> Looking through the unresolved JIRAs, it doesn't look like any =
of the=0A>>>>> identified bugs are show-stoppers or strictly regressions (a=
lthough I=0A>>> will=0A>>>>> note that one that I have in progress, SPARK-1=
749, is a bug that we=0A>>>>> introduced with recent work -- it's not stric=
tly a regression because=0A>>> we=0A>>>>> had equally bad but different beh=
avior when the DAGScheduler=0A>> exceptions=0A>>>>> weren't previously bein=
g handled at all vs. being slightly=0A>> mis-handled=0A>>>>> now), so I'm n=
ot currently seeing a reason not to release.=0A>>> =0A>> 
--1146854128-636912239-1400368113=:48296--

From dev-return-7657-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 17 23:35:24 2014
Return-Path: <dev-return-7657-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F1F8E11D92
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 17 May 2014 23:35:24 +0000 (UTC)
Received: (qmail 51522 invoked by uid 500); 17 May 2014 23:35:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34072 invoked by uid 500); 17 May 2014 23:10:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33671 invoked by uid 99); 17 May 2014 23:02:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 23:02:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mark@clearstorydata.com designates 74.125.82.47 as permitted sender)
Received: from [74.125.82.47] (HELO mail-wg0-f47.google.com) (74.125.82.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 23:02:40 +0000
Received: by mail-wg0-f47.google.com with SMTP id x12so6287442wgg.6
        for <dev@spark.apache.org>; Sat, 17 May 2014 16:02:17 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=n7f5rGFYTNqU6ZMzZIJilxSVXgvhL9jUlWeZUHhgzFg=;
        b=As0M73XqhybDSEb4aqFUHtwQAa3Bmw7xt45aaQaDOqEb0ORavGX5JuagwKlRcJW6Su
         Z/kWjmQmQiqMMNQodq3fIvKEIj1fZeId66KwGmzhOqtzTmEwyQVp2wUeLMWNRVt7pSlr
         noP9XziWdpEbSxx7THw02CtjBwvm3DfwNvIjtRmKEI0IBGa75tJ879ywSma9IEgEx0GQ
         lvGL94/zxHe1k/Ie50bU/XH4VHmeiPCkctCPnAZuLjWbcUyJsceLFytIISrprjqkhlqb
         CjxYKwcj5VlBgDlIK2DoYs1VRw34hQt2XPJg0peI0/X4tmp7mH+fLUibNSHusg5ZYlfR
         Te2g==
X-Gm-Message-State: ALoCoQlePL6SIcoCycI7n5ssZHv0qdSqeo3kVVLzu1gZUavjVLxp3lPja5tM3kUasvPeoO3Hr+Dn
MIME-Version: 1.0
X-Received: by 10.180.98.231 with SMTP id el7mr5515396wib.1.1400367737306;
 Sat, 17 May 2014 16:02:17 -0700 (PDT)
Received: by 10.216.232.69 with HTTP; Sat, 17 May 2014 16:02:17 -0700 (PDT)
In-Reply-To: <CAJiQeY+Sy4EUKhM8ztsSfxaNoOG_73b=s7W_u5YRBPk-JegUDA@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
	<CA+-p3AEL6ct-bJwkBSXm+wTLBHEbBoGnnGOJGAjWHvQogDs-4g@mail.gmail.com>
	<CALRHqP-jLhJM3_5qCTWzrbU+GpmVsd2DFtvMXsH4-PGuT6X8ww@mail.gmail.com>
	<75094F23-AACD-4427-AE5B-A25AC02189A8@gmail.com>
	<CAJiQeY+Sy4EUKhM8ztsSfxaNoOG_73b=s7W_u5YRBPk-JegUDA@mail.gmail.com>
Date: Sat, 17 May 2014 16:02:17 -0700
Message-ID: <CAAsvFPm9OeOd7v2wsevLpKdqaCjfQuAA21r8itVu4-c2vft2OA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d04428d2a899c8504f9a08562
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04428d2a899c8504f9a08562
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I don't understand.  We never said that interfaces wouldn't change from 0.9
to 1.0.  What we are committing to is stability going forward from the
1.0.0 baseline.  Nobody is disputing that backward-incompatible behavior or
interface changes would be an issue post-1.0.0.  The question is whether
there is anything apparent now that is expected to require such disruptive
changes if we were to commit to the current release candidate as our
guaranteed 1.0.0 baseline.


On Sat, May 17, 2014 at 2:05 PM, Mridul Muralidharan <mridul@gmail.com>wrot=
e:

> I would make the case for interface stability not just api stability.
> Particularly given that we have significantly changed some of our
> interfaces, I want to ensure developers/users are not seeing red flags.
>
> Bugs and code stability can be addressed in minor releases if found, but
> behavioral change and/or interface changes would be a much more invasive
> issue for our users.
>
> Regards
> Mridul
> On 18-May-2014 2:19 am, "Matei Zaharia" <matei.zaharia@gmail.com> wrote:
>
> > As others have said, the 1.0 milestone is about API stability, not abou=
t
> > saying =E2=80=9Cwe=E2=80=99ve eliminated all bugs=E2=80=9D. The sooner =
you declare 1.0, the
> sooner
> > users can confidently build on Spark, knowing that the application they
> > build today will still run on Spark 1.9.9 three years from now. This is
> > something that I=E2=80=99ve seen done badly (and experienced the effect=
s thereof)
> > in other big data projects, such as MapReduce and even YARN. The result
> is
> > that you annoy users, you end up with a fragmented userbase where
> everyone
> > is building against a different version, and you drastically slow down
> > development.
> >
> > With a project as fast-growing as fast-growing as Spark in particular,
> > there will be new bugs discovered and reported continuously, especially
> in
> > the non-core components. Look at the graph of # of contributors in time
> to
> > Spark: https://www.ohloh.net/p/apache-spark (bottom-most graph;
> =E2=80=9Ccommits=E2=80=9D
> > changed when we started merging each patch as a single commit). This is
> not
> > slowing down, and we need to have the culture now that we treat API
> > stability and release numbers at the level expected for a 1.0 project
> > instead of having people come in and randomly change the API.
> >
> > I=E2=80=99ll also note that the issues marked =E2=80=9Cblocker=E2=80=9D=
 were marked so by their
> > reporters, since the reporter can set the priority. I don=E2=80=99t con=
sider
> stuff
> > like parallelize() not partitioning ranges in the same way as other
> > collections a blocker =E2=80=94 it=E2=80=99s a bug, it would be good to=
 fix it, but it
> only
> > affects a small number of use cases. Of course if we find a real blocke=
r
> > (in particular a regression from a previous version, or a feature that=
=E2=80=99s
> > just completely broken), we will delay the release for that, but at som=
e
> > point you have to say =E2=80=9Cokay, this fix will go into the next mai=
ntenance
> > release=E2=80=9D. Maybe we need to write a clear policy for what the is=
sue
> > priorities mean.
> >
> > Finally, I believe it=E2=80=99s much better to have a culture where you=
 can make
> > releases on a regular schedule, and have the option to make a maintenan=
ce
> > release in 3-4 days if you find new bugs, than one where you pile up
> stuff
> > into each release. This is what much large project than us, like Linux,
> do,
> > and it=E2=80=99s the only way to avoid indefinite stalling with a large
> contributor
> > base. In the worst case, if you find a new bug that warrants immediate
> > release, it goes into 1.0.1 a week after 1.0.0 (we can vote on 1.0.1 in
> > three days with just your bug fix in it). And if you find an API that
> you=E2=80=99d
> > like to improve, just add a new one and maybe deprecate the old one =E2=
=80=94 at
> > some point we have to respect our users and let them know that code the=
y
> > write today will still run tomorrow.
> >
> > Matei
> >
> > On May 17, 2014, at 10:32 AM, Kan Zhang <kzhang@apache.org> wrote:
> >
> > > +1 on the running commentary here, non-binding of course :-)
> > >
> > >
> > > On Sat, May 17, 2014 at 8:44 AM, Andrew Ash <andrew@andrewash.com>
> > wrote:
> > >
> > >> +1 on the next release feeling more like a 0.10 than a 1.0
> > >> On May 17, 2014 4:38 AM, "Mridul Muralidharan" <mridul@gmail.com>
> > wrote:
> > >>
> > >>> I had echoed similar sentiments a while back when there was a
> > discussion
> > >>> around 0.10 vs 1.0 ... I would have preferred 0.10 to stabilize the
> api
> > >>> changes, add missing functionality, go through a hardening release
> > before
> > >>> 1.0
> > >>>
> > >>> But the community preferred a 1.0 :-)
> > >>>
> > >>> Regards,
> > >>> Mridul
> > >>>
> > >>> On 17-May-2014 3:19 pm, "Sean Owen" <sowen@cloudera.com> wrote:
> > >>>>
> > >>>> On this note, non-binding commentary:
> > >>>>
> > >>>> Releases happen in local minima of change, usually created by
> > >>>> internally enforced code freeze. Spark is incredibly busy now due =
to
> > >>>> external factors -- recently a TLP, recently discovered by a large
> new
> > >>>> audience, ease of contribution enabled by Github. It's getting lik=
e
> > >>>> the first year of mainstream battle-testing in a month. It's been
> very
> > >>>> hard to freeze anything! I see a number of non-trivial issues bein=
g
> > >>>> reported, and I don't think it has been possible to triage all of
> > >>>> them, even.
> > >>>>
> > >>>> Given the high rate of change, my instinct would have been to
> release
> > >>>> 0.10.0 now. But won't it always be very busy? I do think the rate =
of
> > >>>> significant issues will slow down.
> > >>>>
> > >>>> Version ain't nothing but a number, but if it has any meaning it's
> the
> > >>>> semantic versioning meaning. 1.0 imposes extra handicaps around
> > >>>> striving to maintain backwards-compatibility. That may end up bein=
g
> > >>>> bent to fit in important changes that are going to be required in
> this
> > >>>> continuing period of change. Hadoop does this all the time
> > >>>> unfortunately and gets away with it, I suppose -- minor version
> > >>>> releases are really major. (On the other extreme, HBase is at 0.98
> and
> > >>>> quite production-ready.)
> > >>>>
> > >>>> Just consider this a second vote for focus on fixes and 1.0.x rath=
er
> > >>>> than new features and 1.x. I think there are a few steps that coul=
d
> > >>>> streamline triage of this flood of contributions, and make all of
> this
> > >>>> easier, but that's for another thread.
> > >>>>
> > >>>>
> > >>>> On Fri, May 16, 2014 at 8:50 PM, Mark Hamstra <
> > mark@clearstorydata.com
> > >>>
> > >>> wrote:
> > >>>>> +1, but just barely.  We've got quite a number of outstanding bug=
s
> > >>>>> identified, and many of them have fixes in progress.  I'd hate to
> see
> > >>> those
> > >>>>> efforts get lost in a post-1.0.0 flood of new features targeted a=
t
> > >>> 1.1.0 --
> > >>>>> in other words, I'd like to see 1.0.1 retain a high priority
> relative
> > >>> to
> > >>>>> 1.1.0.
> > >>>>>
> > >>>>> Looking through the unresolved JIRAs, it doesn't look like any of
> the
> > >>>>> identified bugs are show-stoppers or strictly regressions
> (although I
> > >>> will
> > >>>>> note that one that I have in progress, SPARK-1749, is a bug that =
we
> > >>>>> introduced with recent work -- it's not strictly a regression
> because
> > >>> we
> > >>>>> had equally bad but different behavior when the DAGScheduler
> > >> exceptions
> > >>>>> weren't previously being handled at all vs. being slightly
> > >> mis-handled
> > >>>>> now), so I'm not currently seeing a reason not to release.
> > >>>
> > >>
> >
> >
>

--f46d04428d2a899c8504f9a08562--

From dev-return-7659-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 00:25:25 2014
Return-Path: <dev-return-7659-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CD79A11E59
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 00:25:25 +0000 (UTC)
Received: (qmail 82853 invoked by uid 500); 18 May 2014 00:00:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55943 invoked by uid 500); 17 May 2014 23:35:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45708 invoked by uid 99); 17 May 2014 23:12:20 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 23:12:20 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.47 as permitted sender)
Received: from [209.85.220.47] (HELO mail-pa0-f47.google.com) (209.85.220.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 17 May 2014 23:12:17 +0000
Received: by mail-pa0-f47.google.com with SMTP id lf10so4097579pab.20
        for <dev@spark.apache.org>; Sat, 17 May 2014 16:11:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=TmwVvsOR/3l8ILlYwTVcrXjYz+KVUJ+oLyNdOg/9tH8=;
        b=UHoTKeAjowIWdbT6GoFFkY8LSje1TLoTl55Fb4paogoj8NeWptelMcnBtCwyvIXLRV
         t2EvHmcMDCQuJlgHOWucv1oddom8ErKqj+Yw5E77bz7cz/cKmKSPaTr8L2x1hBE2HqeJ
         GEMUuuNHEhblUMVNo++O4npQY4oUSH1qUomh0t7Hfv2mYX0Z9V/zTnlH6pNJ7t2q+iPk
         H83UkYTfV4wL5vj2Ut5/6nyAXk4lw1oOx+jQ22pMqOewg1sg8mesL9N7SnGLM+OLZGwL
         4Sh+emSZDdYWDFu1gBCRbRNUGwa0LlYnN3fwc6XrY7EHDEJxqsnispSTqPFuRJLW1vas
         oBow==
X-Received: by 10.67.14.231 with SMTP id fj7mr31539167pad.115.1400368313202;
        Sat, 17 May 2014 16:11:53 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id tg9sm21849446pbc.29.2014.05.17.16.11.49
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sat, 17 May 2014 16:11:50 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAJiQeY+Sy4EUKhM8ztsSfxaNoOG_73b=s7W_u5YRBPk-JegUDA@mail.gmail.com>
Date: Sat, 17 May 2014 16:11:47 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <378AC499-9AC0-482E-85C6-2569CCE67615@gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com> <CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com> <CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com> <CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com> <CA+-p3AEL6ct-bJwkBSXm+wTLBHEbBoGnnGOJGAjWHvQogDs-4g@mail.gmail.com> <CALRHqP-jLhJM3_5qCTWzrbU+GpmVsd2DFtvMXsH4-PGuT6X8ww@mail.gmail.com> <75094F23-AACD-4427-AE5B-A25AC02189A8@gmail.com> <CAJiQeY+Sy4EUKhM8ztsSfxaNoOG_73b=s7W_u5YRBPk-JegUDA@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

Yup, this is a good point, the interface includes stuff like launch =
scripts and environment variables. However I do think that the current =
features of spark-submit can all be supported in future releases. We=92ll =
definitely have a very strict standard for modifying these later on.

Matei

On May 17, 2014, at 2:05 PM, Mridul Muralidharan <mridul@gmail.com> =
wrote:

> I would make the case for interface stability not just api stability.
> Particularly given that we have significantly changed some of our
> interfaces, I want to ensure developers/users are not seeing red =
flags.
>=20
> Bugs and code stability can be addressed in minor releases if found, =
but
> behavioral change and/or interface changes would be a much more =
invasive
> issue for our users.
>=20
> Regards
> Mridul
> On 18-May-2014 2:19 am, "Matei Zaharia" <matei.zaharia@gmail.com> =
wrote:
>=20
>> As others have said, the 1.0 milestone is about API stability, not =
about
>> saying =93we=92ve eliminated all bugs=94. The sooner you declare 1.0, =
the sooner
>> users can confidently build on Spark, knowing that the application =
they
>> build today will still run on Spark 1.9.9 three years from now. This =
is
>> something that I=92ve seen done badly (and experienced the effects =
thereof)
>> in other big data projects, such as MapReduce and even YARN. The =
result is
>> that you annoy users, you end up with a fragmented userbase where =
everyone
>> is building against a different version, and you drastically slow =
down
>> development.
>>=20
>> With a project as fast-growing as fast-growing as Spark in =
particular,
>> there will be new bugs discovered and reported continuously, =
especially in
>> the non-core components. Look at the graph of # of contributors in =
time to
>> Spark: https://www.ohloh.net/p/apache-spark (bottom-most graph; =
=93commits=94
>> changed when we started merging each patch as a single commit). This =
is not
>> slowing down, and we need to have the culture now that we treat API
>> stability and release numbers at the level expected for a 1.0 project
>> instead of having people come in and randomly change the API.
>>=20
>> I=92ll also note that the issues marked =93blocker=94 were marked so =
by their
>> reporters, since the reporter can set the priority. I don=92t =
consider stuff
>> like parallelize() not partitioning ranges in the same way as other
>> collections a blocker =97 it=92s a bug, it would be good to fix it, =
but it only
>> affects a small number of use cases. Of course if we find a real =
blocker
>> (in particular a regression from a previous version, or a feature =
that=92s
>> just completely broken), we will delay the release for that, but at =
some
>> point you have to say =93okay, this fix will go into the next =
maintenance
>> release=94. Maybe we need to write a clear policy for what the issue
>> priorities mean.
>>=20
>> Finally, I believe it=92s much better to have a culture where you can =
make
>> releases on a regular schedule, and have the option to make a =
maintenance
>> release in 3-4 days if you find new bugs, than one where you pile up =
stuff
>> into each release. This is what much large project than us, like =
Linux, do,
>> and it=92s the only way to avoid indefinite stalling with a large =
contributor
>> base. In the worst case, if you find a new bug that warrants =
immediate
>> release, it goes into 1.0.1 a week after 1.0.0 (we can vote on 1.0.1 =
in
>> three days with just your bug fix in it). And if you find an API that =
you=92d
>> like to improve, just add a new one and maybe deprecate the old one =97=
 at
>> some point we have to respect our users and let them know that code =
they
>> write today will still run tomorrow.
>>=20
>> Matei
>>=20
>> On May 17, 2014, at 10:32 AM, Kan Zhang <kzhang@apache.org> wrote:
>>=20
>>> +1 on the running commentary here, non-binding of course :-)
>>>=20
>>>=20
>>> On Sat, May 17, 2014 at 8:44 AM, Andrew Ash <andrew@andrewash.com>
>> wrote:
>>>=20
>>>> +1 on the next release feeling more like a 0.10 than a 1.0
>>>> On May 17, 2014 4:38 AM, "Mridul Muralidharan" <mridul@gmail.com>
>> wrote:
>>>>=20
>>>>> I had echoed similar sentiments a while back when there was a
>> discussion
>>>>> around 0.10 vs 1.0 ... I would have preferred 0.10 to stabilize =
the api
>>>>> changes, add missing functionality, go through a hardening release
>> before
>>>>> 1.0
>>>>>=20
>>>>> But the community preferred a 1.0 :-)
>>>>>=20
>>>>> Regards,
>>>>> Mridul
>>>>>=20
>>>>> On 17-May-2014 3:19 pm, "Sean Owen" <sowen@cloudera.com> wrote:
>>>>>>=20
>>>>>> On this note, non-binding commentary:
>>>>>>=20
>>>>>> Releases happen in local minima of change, usually created by
>>>>>> internally enforced code freeze. Spark is incredibly busy now due =
to
>>>>>> external factors -- recently a TLP, recently discovered by a =
large new
>>>>>> audience, ease of contribution enabled by Github. It's getting =
like
>>>>>> the first year of mainstream battle-testing in a month. It's been =
very
>>>>>> hard to freeze anything! I see a number of non-trivial issues =
being
>>>>>> reported, and I don't think it has been possible to triage all of
>>>>>> them, even.
>>>>>>=20
>>>>>> Given the high rate of change, my instinct would have been to =
release
>>>>>> 0.10.0 now. But won't it always be very busy? I do think the rate =
of
>>>>>> significant issues will slow down.
>>>>>>=20
>>>>>> Version ain't nothing but a number, but if it has any meaning =
it's the
>>>>>> semantic versioning meaning. 1.0 imposes extra handicaps around
>>>>>> striving to maintain backwards-compatibility. That may end up =
being
>>>>>> bent to fit in important changes that are going to be required in =
this
>>>>>> continuing period of change. Hadoop does this all the time
>>>>>> unfortunately and gets away with it, I suppose -- minor version
>>>>>> releases are really major. (On the other extreme, HBase is at =
0.98 and
>>>>>> quite production-ready.)
>>>>>>=20
>>>>>> Just consider this a second vote for focus on fixes and 1.0.x =
rather
>>>>>> than new features and 1.x. I think there are a few steps that =
could
>>>>>> streamline triage of this flood of contributions, and make all of =
this
>>>>>> easier, but that's for another thread.
>>>>>>=20
>>>>>>=20
>>>>>> On Fri, May 16, 2014 at 8:50 PM, Mark Hamstra <
>> mark@clearstorydata.com
>>>>>=20
>>>>> wrote:
>>>>>>> +1, but just barely.  We've got quite a number of outstanding =
bugs
>>>>>>> identified, and many of them have fixes in progress.  I'd hate =
to see
>>>>> those
>>>>>>> efforts get lost in a post-1.0.0 flood of new features targeted =
at
>>>>> 1.1.0 --
>>>>>>> in other words, I'd like to see 1.0.1 retain a high priority =
relative
>>>>> to
>>>>>>> 1.1.0.
>>>>>>>=20
>>>>>>> Looking through the unresolved JIRAs, it doesn't look like any =
of the
>>>>>>> identified bugs are show-stoppers or strictly regressions =
(although I
>>>>> will
>>>>>>> note that one that I have in progress, SPARK-1749, is a bug that =
we
>>>>>>> introduced with recent work -- it's not strictly a regression =
because
>>>>> we
>>>>>>> had equally bad but different behavior when the DAGScheduler
>>>> exceptions
>>>>>>> weren't previously being handled at all vs. being slightly
>>>> mis-handled
>>>>>>> now), so I'm not currently seeing a reason not to release.
>>>>>=20
>>>>=20
>>=20
>>=20


From dev-return-7660-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 02:05:24 2014
Return-Path: <dev-return-7660-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 442DE11F70
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 02:05:24 +0000 (UTC)
Received: (qmail 46682 invoked by uid 500); 18 May 2014 01:40:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46165 invoked by uid 500); 18 May 2014 01:40:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43581 invoked by uid 99); 18 May 2014 01:27:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 01:27:56 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of andykonwinski@gmail.com designates 209.85.215.41 as permitted sender)
Received: from [209.85.215.41] (HELO mail-la0-f41.google.com) (209.85.215.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 01:27:51 +0000
Received: by mail-la0-f41.google.com with SMTP id e16so3088144lan.0
        for <dev@spark.apache.org>; Sat, 17 May 2014 18:27:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=BBnwKB9hzLcpDZeHHK+NHNeYuXvtnNHYkjUCBKppJs8=;
        b=IIA/DsMAgmmmoemUnYdPpqHZB6Nnd2edbvuq1dC+Skx4nLivZ0tPRfbjVEIuepBkU8
         OqzbJXF4whm3jsBacPmTN+ZHaJNLOU8Alnhsu3Ir5ET2cNgwob8GMUAVI3gEPXqaGXT9
         tTKBTnayr/HdTah347D1KKv7oc7RjUpPfxXDZ4rYsyjderSMZQDQ6d/6lGmGh5Im/DC7
         r8ItukU2j6Mwz36P9jqj19zpKD39eFiFlbh+h53AS3K2hE5mw74Idoq3ouWnOa3J4Fnu
         WS3Lc28mw+h1Ht3mxLLYP3RJSwjs2fTKwHL+k0zwagUAhDKpNiiM8sWw2CjnJtx2ljy6
         4kKQ==
MIME-Version: 1.0
X-Received: by 10.152.28.225 with SMTP id e1mr10417282lah.1.1400376449439;
 Sat, 17 May 2014 18:27:29 -0700 (PDT)
Received: by 10.112.147.4 with HTTP; Sat, 17 May 2014 18:27:29 -0700 (PDT)
Received: by 10.112.147.4 with HTTP; Sat, 17 May 2014 18:27:29 -0700 (PDT)
In-Reply-To: <CABDsqqb64pL9ENMv0YVoo2GRKWQQe=jCb5TLZV4yBeRwg2ZyTA@mail.gmail.com>
References: <CABDsqqb64pL9ENMv0YVoo2GRKWQQe=jCb5TLZV4yBeRwg2ZyTA@mail.gmail.com>
Date: Sat, 17 May 2014 18:27:29 -0700
Message-ID: <CALEZFQyMp9XgjYeToh7YStVzu8Q-3uSqWaxad6pLw=mEg+7r3Q@mail.gmail.com>
Subject: Re: can RDD be shared across mutil spark applications?
From: Andy Konwinski <andykonwinski@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0160b6bed21e7b04f9a28c84
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160b6bed21e7b04f9a28c84
Content-Type: text/plain; charset=UTF-8

RDDs cannot currently be shared across multiple SparkContexts without using
something like the Tachyon project (which is a separate project/codebase).

Andy
On May 16, 2014 2:14 PM, "qingyang li" <liqingyang1985@gmail.com> wrote:

>
>

--089e0160b6bed21e7b04f9a28c84--

From dev-return-7661-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 04:43:35 2014
Return-Path: <dev-return-7661-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A5FAC1117B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 04:43:35 +0000 (UTC)
Received: (qmail 66688 invoked by uid 500); 18 May 2014 04:35:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48133 invoked by uid 500); 18 May 2014 04:10:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46073 invoked by uid 99); 18 May 2014 04:02:34 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 04:02:34 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ctn@adatao.com designates 209.85.213.180 as permitted sender)
Received: from [209.85.213.180] (HELO mail-ig0-f180.google.com) (209.85.213.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 04:02:29 +0000
Received: by mail-ig0-f180.google.com with SMTP id c1so2262410igq.13
        for <dev@spark.apache.org>; Sat, 17 May 2014 21:02:03 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=adatao.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=/Nrd+tBrcoMIPIDckjdV69es6nh+Xl6PYUCdGKAIw90=;
        b=WqTxo4Qo1UurHEF1Z+n5IlX+VuyAuAKQnGTK26MxE3wHpkEzXuvpY+o6ErZyqo2TRd
         MuJe9yy18qUpx5GmNycVH2UHU/W1NGoRljZtQfxP7AriAW4Ifby4gQQ4x3QEV9texJsL
         xnnEKqGJFSyQ7hR0m3OArGEqO5dIyyH/OOmPI=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=/Nrd+tBrcoMIPIDckjdV69es6nh+Xl6PYUCdGKAIw90=;
        b=Rr9PSZ96htYUzQBF5MeE1yKsmPiMV0OlAtBfLNPZRUOgV7kWiyzHAE0kJSouz9Z4Wt
         5m84GYB6DaYxJBkFCLeEIcY87Xvv1diwP0y5Seu/FtUhyf7mleAQG+BaxUaQL4N3cu3p
         eoI0kzmPajYsO4P1B0xo61FniuxJ6fU1lyl/3+lsCzL2BRleU2DY1/NX9GAYXoHJ+ftI
         5DgA37NiuAh7wtES7KYpA/75aIKr6UcO1NNsK4+pDoojh309K+JJq/z3u5Mz83L46EDI
         RIl2USta96+AcfIOnAZ8jSesYisihUDbPEwakaofti42RxNKpYbH5EQfINr72Sm/cecS
         AiSA==
X-Gm-Message-State: ALoCoQmHzOhd051APpf+de6MK06S5M7KBYtuF+7lMM4DaoGrfOG5sc5ZCZwgb5aDySyhRkgOf4Ub
MIME-Version: 1.0
X-Received: by 10.50.79.227 with SMTP id m3mr7537502igx.47.1400385723703; Sat,
 17 May 2014 21:02:03 -0700 (PDT)
Received: by 10.64.17.40 with HTTP; Sat, 17 May 2014 21:02:03 -0700 (PDT)
X-Originating-IP: [175.159.101.36]
Received: by 10.64.17.40 with HTTP; Sat, 17 May 2014 21:02:03 -0700 (PDT)
In-Reply-To: <CALEZFQyMp9XgjYeToh7YStVzu8Q-3uSqWaxad6pLw=mEg+7r3Q@mail.gmail.com>
References: <CABDsqqb64pL9ENMv0YVoo2GRKWQQe=jCb5TLZV4yBeRwg2ZyTA@mail.gmail.com>
	<CALEZFQyMp9XgjYeToh7YStVzu8Q-3uSqWaxad6pLw=mEg+7r3Q@mail.gmail.com>
Date: Sun, 18 May 2014 12:02:03 +0800
Message-ID: <CAGh_TuNEmH3z0-uZKa6Xmt9FO2ZjpHx+mTXoFx4dknc2=p=9wQ@mail.gmail.com>
Subject: Re: can RDD be shared across mutil spark applications?
From: Christopher Nguyen <ctn@adatao.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01175f5d9c399c04f9a4b5f4
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01175f5d9c399c04f9a4b5f4
Content-Type: text/plain; charset=UTF-8

Qing Yang, Andy is correct in answering your direct question.

At the same time, depending on your context, you may be able to apply a
pattern where you turn the single Spark application into a service, and
multiple clients if that service can indeed share access to the same RDDs.

Several groups have built apps based on this pattern, and we will also show
something with this behavior at the upcoming Spark Summit (multiple users
collaborating on named DDFs with the same underlying RDDs).

Sent while mobile. Pls excuse typos etc.
On May 18, 2014 9:40 AM, "Andy Konwinski" <andykonwinski@gmail.com> wrote:

> RDDs cannot currently be shared across multiple SparkContexts without using
> something like the Tachyon project (which is a separate project/codebase).
>
> Andy
> On May 16, 2014 2:14 PM, "qingyang li" <liqingyang1985@gmail.com> wrote:
>
> >
> >
>

--089e01175f5d9c399c04f9a4b5f4--

From dev-return-7662-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 06:40:36 2014
Return-Path: <dev-return-7662-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C9CBA112CA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 06:40:36 +0000 (UTC)
Received: (qmail 40240 invoked by uid 500); 18 May 2014 06:15:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14789 invoked by uid 500); 18 May 2014 05:50:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11952 invoked by uid 99); 18 May 2014 05:29:18 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 05:29:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of liquanpei@gmail.com designates 74.125.82.169 as permitted sender)
Received: from [74.125.82.169] (HELO mail-we0-f169.google.com) (74.125.82.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 05:29:14 +0000
Received: by mail-we0-f169.google.com with SMTP id u56so4250501wes.0
        for <dev@spark.apache.org>; Sat, 17 May 2014 22:28:51 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=p6u/HCnaFPnEcUXGnt2x7sDfb24oyFq9liMl1M5exM8=;
        b=X/9VSd98vVtIljs1E7D1gSWkR+shVhArhEBGpwBgKQQJK2i9kLv9jbPXEOs5ra6uMG
         azxtDWgAyYBbM+NYHgXxkU2soCcK7YzAYEGU7ToT86UFCP2et7QoEB8/o8kkT8ETNKIX
         N2bzwzQGsHKcoGLrGavkcNf9UX4DlQeDyP+1Bqdbm/FdT1/B/s0Qu39aP0i4We08+xzi
         m5kfVpKOfK877AVvA2+7N1+pX2Z/DozXBpp03JejUuXKjXwkUmNnNDGgue0TVl1GAVQM
         7V6AIgU0BtLxnYifDmTz3PMCl9ixlKuuNy/zvBeeQ0JHCONMuswxEMbB4LTzzFk6sy04
         THMw==
MIME-Version: 1.0
X-Received: by 10.180.19.167 with SMTP id g7mr6179177wie.46.1400390931556;
 Sat, 17 May 2014 22:28:51 -0700 (PDT)
Received: by 10.180.101.167 with HTTP; Sat, 17 May 2014 22:28:51 -0700 (PDT)
Date: Sat, 17 May 2014 22:28:51 -0700
Message-ID: <CAJmC80-0P6wfo5aQ7=-gfyF6h9J1AgyD29_qbzOoeNEt2d2RiA@mail.gmail.com>
Subject: Matrix Multiplication of two RDD[Array[Double]]'s
From: Liquan Pei <liquanpei@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=bcaec53d5dff05b01f04f9a5ec1d
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec53d5dff05b01f04f9a5ec1d
Content-Type: text/plain; charset=UTF-8

Hi

I am currently implementing an algorithm involving matrix multiplication.
Basically, I have matrices represented as RDD[Array[Double]]. For example,
If I have A:RDD[Array[Double]] and B:RDD[Array[Double]] and what would be
the most efficient way to get C = A * B

Both A and B are large, so it would not be possible to save either of them
in memory.

Thanks a lot for your help!

Liquan

--bcaec53d5dff05b01f04f9a5ec1d--

From dev-return-7664-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 07:30:39 2014
Return-Path: <dev-return-7664-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B0110113AE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 07:30:39 +0000 (UTC)
Received: (qmail 89399 invoked by uid 500); 18 May 2014 07:05:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87413 invoked by uid 500); 18 May 2014 07:05:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78334 invoked by uid 99); 18 May 2014 06:41:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 06:41:36 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.192.41 as permitted sender)
Received: from [209.85.192.41] (HELO mail-qg0-f41.google.com) (209.85.192.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 06:41:32 +0000
Received: by mail-qg0-f41.google.com with SMTP id j5so6946162qga.0
        for <dev@spark.apache.org>; Sat, 17 May 2014 23:41:11 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=RpyNbsptGlWEwcEM4FaOQDr8lJDyNCUvGlxCVs79M10=;
        b=gPXuD1CSk7omrttOqEkL0o9Aly0K+UZwqnzcBczaVmH9ROtL6TZu5JgMbxaqsJXM/J
         GVzQ9/Lmf23xwQn5HQokIEvxICQVIWW2BGtBT7nvReTZjpGcOsdhyRZSR7W9Tq0R9S3J
         ryw3cs1G01ZGkyFIKyyI9ZYQ4vG2BOsVlI3FBFlQt2clgiwjpwAIrSdEJ2xk++xqCPR5
         5F1DYOV1w/aDQKbO58EhIjHDs+UStbSCfTTA2gympypGrXFnmbZcPQUugOXoqOQpqfHO
         Ob47CbB12aJ+B8cffXU6DsczErShjQQv4pw9hLrZlMIPx6Fc3VeE/3XYuBFdW/w1zAH6
         hf7A==
MIME-Version: 1.0
X-Received: by 10.140.82.7 with SMTP id g7mr37211618qgd.74.1400395271549; Sat,
 17 May 2014 23:41:11 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Sat, 17 May 2014 23:41:11 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Sat, 17 May 2014 23:41:11 -0700 (PDT)
In-Reply-To: <CAAsvFPm9OeOd7v2wsevLpKdqaCjfQuAA21r8itVu4-c2vft2OA@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
	<CA+-p3AEL6ct-bJwkBSXm+wTLBHEbBoGnnGOJGAjWHvQogDs-4g@mail.gmail.com>
	<CALRHqP-jLhJM3_5qCTWzrbU+GpmVsd2DFtvMXsH4-PGuT6X8ww@mail.gmail.com>
	<75094F23-AACD-4427-AE5B-A25AC02189A8@gmail.com>
	<CAJiQeY+Sy4EUKhM8ztsSfxaNoOG_73b=s7W_u5YRBPk-JegUDA@mail.gmail.com>
	<CAAsvFPm9OeOd7v2wsevLpKdqaCjfQuAA21r8itVu4-c2vft2OA@mail.gmail.com>
Date: Sun, 18 May 2014 12:11:11 +0530
Message-ID: <CAJiQeYKWeKoZuY9MjQTxzHn5d5XwvHwo0e8sEuEWL6CFW4p6CA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c11586b4bb9c04f9a6eee5
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c11586b4bb9c04f9a6eee5
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

On 18-May-2014 5:05 am, "Mark Hamstra" <mark@clearstorydata.com> wrote:
>
> I don't understand.  We never said that interfaces wouldn't change from
0.9

Agreed.

> to 1.0.  What we are committing to is stability going forward from the
> 1.0.0 baseline.  Nobody is disputing that backward-incompatible behavior
or
> interface changes would be an issue post-1.0.0.  The question is whether

The point is, how confident are we that these are the right set of
interface definitions.
We think it is, but we could also have gone through a 0.10 to vet the
proposed 1.0 changes to stabilize them.

To give examples for which we don't have solutions currently (which we are
facing internally here btw, so not academic exercise) :

- Current spark shuffle model breaks very badly as number of partitions
increases (input and output).

- As number of nodes increase, the overhead per node keeps going up. Spark
currently is more geared towards large memory machines; when the RAM per
node is modest (8 to 16 gig) but large number of them are available, it
does not do too well.

- Current block abstraction breaks as data per block goes beyond 2 gig.

- Cogroup/join when value per key or number of keys (or both) is high
breaks currently.

- Shuffle consolidation is so badly broken it is not funny.

- Currently there is no way of effectively leveraging accelerator
cards/coprocessors/gpus from spark - to do so, I suspect we will need to
redefine OFF_HEAP.

- Effectively leveraging ssd is still an open question IMO when you have
mix of both available.

We have resolved some of these and looking at the rest. These are not
unique to our internal usage profile, I have seen most of these asked
elsewhere too.

Thankfully some of the 1.0 changes actually are geared towards helping to
alleviate some of the above (Iterable change for ex), most of the rest are
internal impl detail of spark core which helps a lot - but there are cases
where this is not so.

Unfortunately I don't know yet if the unresolved/uninvestigated issues will
require more changes or not.

Given this I am very skeptical of expecting current spark interfaces to be
sufficient for next 1 year (forget 3)

I understand this is an argument which can be made to never release 1.0 :-)
Which is why I was ok with a 1.0 instead of 0.10 release in spite of my
preference.

This is a good problem to have IMO ... People are using spark extensively
and in circumstances that we did not envision : necessitating changes even
to spark core.

But the claim that 1.0 interfaces are stable is not something I buy - they
are not, we will need to break them soon and cost of maintaining backward
compatibility will be high.

We just need to make an informed decision to live with that cost, not hand
wave it away.

Regards
Mridul

> there is anything apparent now that is expected to require such disruptiv=
e
> changes if we were to commit to the current release candidate as our
> guaranteed 1.0.0 baseline.
>
>
> On Sat, May 17, 2014 at 2:05 PM, Mridul Muralidharan <mridul@gmail.com
>wrote:
>
> > I would make the case for interface stability not just api stability.
> > Particularly given that we have significantly changed some of our
> > interfaces, I want to ensure developers/users are not seeing red flags.
> >
> > Bugs and code stability can be addressed in minor releases if found, bu=
t
> > behavioral change and/or interface changes would be a much more invasiv=
e
> > issue for our users.
> >
> > Regards
> > Mridul
> > On 18-May-2014 2:19 am, "Matei Zaharia" <matei.zaharia@gmail.com> wrote=
:
> >
> > > As others have said, the 1.0 milestone is about API stability, not
about
> > > saying =E2=80=9Cwe=E2=80=99ve eliminated all bugs=E2=80=9D. The soone=
r you declare 1.0, the
> > sooner
> > > users can confidently build on Spark, knowing that the application
they
> > > build today will still run on Spark 1.9.9 three years from now. This
is
> > > something that I=E2=80=99ve seen done badly (and experienced the effe=
cts
thereof)
> > > in other big data projects, such as MapReduce and even YARN. The
result
> > is
> > > that you annoy users, you end up with a fragmented userbase where
> > everyone
> > > is building against a different version, and you drastically slow dow=
n
> > > development.
> > >
> > > With a project as fast-growing as fast-growing as Spark in particular=
,
> > > there will be new bugs discovered and reported continuously,
especially
> > in
> > > the non-core components. Look at the graph of # of contributors in
time
> > to
> > > Spark: https://www.ohloh.net/p/apache-spark (bottom-most graph;
> > =E2=80=9Ccommits=E2=80=9D
> > > changed when we started merging each patch as a single commit). This
is
> > not
> > > slowing down, and we need to have the culture now that we treat API
> > > stability and release numbers at the level expected for a 1.0 project
> > > instead of having people come in and randomly change the API.
> > >
> > > I=E2=80=99ll also note that the issues marked =E2=80=9Cblocker=E2=80=
=9D were marked so by
their
> > > reporters, since the reporter can set the priority. I don=E2=80=99t c=
onsider
> > stuff
> > > like parallelize() not partitioning ranges in the same way as other
> > > collections a blocker =E2=80=94 it=E2=80=99s a bug, it would be good =
to fix it, but it
> > only
> > > affects a small number of use cases. Of course if we find a real
blocker
> > > (in particular a regression from a previous version, or a feature
that=E2=80=99s
> > > just completely broken), we will delay the release for that, but at
some
> > > point you have to say =E2=80=9Cokay, this fix will go into the next
maintenance
> > > release=E2=80=9D. Maybe we need to write a clear policy for what the =
issue
> > > priorities mean.
> > >
> > > Finally, I believe it=E2=80=99s much better to have a culture where y=
ou can
make
> > > releases on a regular schedule, and have the option to make a
maintenance
> > > release in 3-4 days if you find new bugs, than one where you pile up
> > stuff
> > > into each release. This is what much large project than us, like
Linux,
> > do,
> > > and it=E2=80=99s the only way to avoid indefinite stalling with a lar=
ge
> > contributor
> > > base. In the worst case, if you find a new bug that warrants immediat=
e
> > > release, it goes into 1.0.1 a week after 1.0.0 (we can vote on 1.0.1
in
> > > three days with just your bug fix in it). And if you find an API that
> > you=E2=80=99d
> > > like to improve, just add a new one and maybe deprecate the old one =
=E2=80=94
at
> > > some point we have to respect our users and let them know that code
they
> > > write today will still run tomorrow.
> > >
> > > Matei
> > >
> > > On May 17, 2014, at 10:32 AM, Kan Zhang <kzhang@apache.org> wrote:
> > >
> > > > +1 on the running commentary here, non-binding of course :-)
> > > >
> > > >
> > > > On Sat, May 17, 2014 at 8:44 AM, Andrew Ash <andrew@andrewash.com>
> > > wrote:
> > > >
> > > >> +1 on the next release feeling more like a 0.10 than a 1.0
> > > >> On May 17, 2014 4:38 AM, "Mridul Muralidharan" <mridul@gmail.com>
> > > wrote:
> > > >>
> > > >>> I had echoed similar sentiments a while back when there was a
> > > discussion
> > > >>> around 0.10 vs 1.0 ... I would have preferred 0.10 to stabilize
the
> > api
> > > >>> changes, add missing functionality, go through a hardening releas=
e
> > > before
> > > >>> 1.0
> > > >>>
> > > >>> But the community preferred a 1.0 :-)
> > > >>>
> > > >>> Regards,
> > > >>> Mridul
> > > >>>
> > > >>> On 17-May-2014 3:19 pm, "Sean Owen" <sowen@cloudera.com> wrote:
> > > >>>>
> > > >>>> On this note, non-binding commentary:
> > > >>>>
> > > >>>> Releases happen in local minima of change, usually created by
> > > >>>> internally enforced code freeze. Spark is incredibly busy now
due to
> > > >>>> external factors -- recently a TLP, recently discovered by a
large
> > new
> > > >>>> audience, ease of contribution enabled by Github. It's getting
like
> > > >>>> the first year of mainstream battle-testing in a month. It's bee=
n
> > very
> > > >>>> hard to freeze anything! I see a number of non-trivial issues
being
> > > >>>> reported, and I don't think it has been possible to triage all o=
f
> > > >>>> them, even.
> > > >>>>
> > > >>>> Given the high rate of change, my instinct would have been to
> > release
> > > >>>> 0.10.0 now. But won't it always be very busy? I do think the
rate of
> > > >>>> significant issues will slow down.
> > > >>>>
> > > >>>> Version ain't nothing but a number, but if it has any meaning
it's
> > the
> > > >>>> semantic versioning meaning. 1.0 imposes extra handicaps around
> > > >>>> striving to maintain backwards-compatibility. That may end up
being
> > > >>>> bent to fit in important changes that are going to be required i=
n
> > this
> > > >>>> continuing period of change. Hadoop does this all the time
> > > >>>> unfortunately and gets away with it, I suppose -- minor version
> > > >>>> releases are really major. (On the other extreme, HBase is at
0.98
> > and
> > > >>>> quite production-ready.)
> > > >>>>
> > > >>>> Just consider this a second vote for focus on fixes and 1.0.x
rather
> > > >>>> than new features and 1.x. I think there are a few steps that
could
> > > >>>> streamline triage of this flood of contributions, and make all o=
f
> > this
> > > >>>> easier, but that's for another thread.
> > > >>>>
> > > >>>>
> > > >>>> On Fri, May 16, 2014 at 8:50 PM, Mark Hamstra <
> > > mark@clearstorydata.com
> > > >>>
> > > >>> wrote:
> > > >>>>> +1, but just barely.  We've got quite a number of outstanding
bugs
> > > >>>>> identified, and many of them have fixes in progress.  I'd hate
to
> > see
> > > >>> those
> > > >>>>> efforts get lost in a post-1.0.0 flood of new features targeted
at
> > > >>> 1.1.0 --
> > > >>>>> in other words, I'd like to see 1.0.1 retain a high priority
> > relative
> > > >>> to
> > > >>>>> 1.1.0.
> > > >>>>>
> > > >>>>> Looking through the unresolved JIRAs, it doesn't look like any
of
> > the
> > > >>>>> identified bugs are show-stoppers or strictly regressions
> > (although I
> > > >>> will
> > > >>>>> note that one that I have in progress, SPARK-1749, is a bug
that we
> > > >>>>> introduced with recent work -- it's not strictly a regression
> > because
> > > >>> we
> > > >>>>> had equally bad but different behavior when the DAGScheduler
> > > >> exceptions
> > > >>>>> weren't previously being handled at all vs. being slightly
> > > >> mis-handled
> > > >>>>> now), so I'm not currently seeing a reason not to release.
> > > >>>
> > > >>
> > >
> > >
> >

--001a11c11586b4bb9c04f9a6eee5--

From dev-return-7663-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 07:30:40 2014
Return-Path: <dev-return-7663-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 03DB4113B8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 07:30:40 +0000 (UTC)
Received: (qmail 90538 invoked by uid 500); 18 May 2014 07:05:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63286 invoked by uid 500); 18 May 2014 06:40:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57943 invoked by uid 99); 18 May 2014 06:26:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 06:26:35 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.212.171 as permitted sender)
Received: from [209.85.212.171] (HELO mail-wi0-f171.google.com) (209.85.212.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 06:26:33 +0000
Received: by mail-wi0-f171.google.com with SMTP id cc10so61640wib.10
        for <dev@spark.apache.org>; Sat, 17 May 2014 23:26:10 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=jIvlEF6sELazOYhZ6Slmd1X0qjSknHAcAvSjZW6I/pk=;
        b=VS753QxwcRpWea+1RxvSzSUYgbxnr5FQB1m3D7wiQw4kzT2Z4m7+KSy+DNopkO90zz
         DEcKC/VoVqIA394Eh81nCLQKfpSsVdI5aeA9XR5DTkXKSHFWMtSKJM8I/ZmXkff47nEV
         admnoNZ8dPvSGFwIm1/rHsK1xMyNUqb9Cq1HmBar5vbf7bJEdCJ/VExmSmPRZs4SYYTX
         snDnQQfFDyMrQn/p+VFCypDgXXahpZ05KCosSl4fXDuwAE/H5ZAfGKQTa85ihQEEhNbW
         SKT80cG6wqX4prmaRS9qo6ho6eTxOMjqGb4nhGqhzxL7ft9urBS/ibcqVDUpHaRghpy+
         MLxw==
MIME-Version: 1.0
X-Received: by 10.180.212.77 with SMTP id ni13mr6266871wic.5.1400394370219;
 Sat, 17 May 2014 23:26:10 -0700 (PDT)
Received: by 10.194.187.20 with HTTP; Sat, 17 May 2014 23:26:10 -0700 (PDT)
In-Reply-To: <CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
Date: Sat, 17 May 2014 23:26:10 -0700
Message-ID: <CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I can re-produce the error with Spark 1.0-RC and YARN (CDH-5). The
reflection approach mentioned by DB didn't work either. I checked the
distributed cache on a worker node and found the jar there. It is also
in the Environment tab of the WebUI. The workaround is making an
assembly jar.

DB, could you create a JIRA and describe what you have found so far? Thanks!

Best,
Xiangrui

On Sat, May 17, 2014 at 1:29 AM, Mridul Muralidharan <mridul@gmail.com> wrote:
> Can you try moving your mapPartitions to another class/object which is
> referenced only after sc.addJar ?
>
> I would suspect CNFEx is coming while loading the class containing
> mapPartitions before addJars is executed.
>
> In general though, dynamic loading of classes means you use reflection to
> instantiate it since expectation is you don't know which implementation
> provides the interface ... If you statically know it apriori, you bundle it
> in your classpath.
>
> Regards
> Mridul
> On 17-May-2014 7:28 am, "DB Tsai" <dbtsai@stanford.edu> wrote:
>
>> Finally find a way out of the ClassLoader maze! It took me some times to
>> understand how it works; I think it worths to document it in a separated
>> thread.
>>
>> We're trying to add external utility.jar which contains CSVRecordParser,
>> and we added the jar to executors through sc.addJar APIs.
>>
>> If the instance of CSVRecordParser is created without reflection, it
>> raises *ClassNotFound
>> Exception*.
>>
>> data.mapPartitions(lines => {
>>     val csvParser = new CSVRecordParser((delimiter.charAt(0))
>>     lines.foreach(line => {
>>       val lineElems = csvParser.parseLine(line)
>>     })
>>     ...
>>     ...
>>  )
>>
>>
>> If the instance of CSVRecordParser is created through reflection, it works.
>>
>> data.mapPartitions(lines => {
>>     val loader = Thread.currentThread.getContextClassLoader
>>     val CSVRecordParser =
>>         loader.loadClass("com.alpine.hadoop.ext.CSVRecordParser")
>>
>>     val csvParser = CSVRecordParser.getConstructor(Character.TYPE)
>>         .newInstance(delimiter.charAt(0).asInstanceOf[Character])
>>
>>     val parseLine = CSVRecordParser
>>         .getDeclaredMethod("parseLine", classOf[String])
>>
>>     lines.foreach(line => {
>>        val lineElems = parseLine.invoke(csvParser,
>> line).asInstanceOf[Array[String]]
>>     })
>>     ...
>>     ...
>>  )
>>
>>
>> This is identical to this question,
>>
>> http://stackoverflow.com/questions/7452411/thread-currentthread-setcontextclassloader-without-using-reflection
>>
>> It's not intuitive for users to load external classes through reflection,
>> but couple available solutions including 1) messing around
>> systemClassLoader by calling systemClassLoader.addURI through reflection or
>> 2) forking another JVM to add jars into classpath before bootstrap loader
>> are very tricky.
>>
>> Any thought on fixing it properly?
>>
>> @Xiangrui,
>> netlib-java jniloader is loaded from netlib-java through reflection, so
>> this problem will not be seen.
>>
>> Sincerely,
>>
>> DB Tsai
>> -------------------------------------------------------
>> My Blog: https://www.dbtsai.com
>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>

From dev-return-7665-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 10:00:41 2014
Return-Path: <dev-return-7665-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8A47B11594
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 10:00:41 +0000 (UTC)
Received: (qmail 20542 invoked by uid 500); 18 May 2014 09:35:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20392 invoked by uid 500); 18 May 2014 09:35:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17276 invoked by uid 99); 18 May 2014 09:14:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 09:14:26 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.192.46 as permitted sender)
Received: from [209.85.192.46] (HELO mail-qg0-f46.google.com) (209.85.192.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 09:14:23 +0000
Received: by mail-qg0-f46.google.com with SMTP id q108so6819676qgd.19
        for <dev@spark.apache.org>; Sun, 18 May 2014 02:13:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type:content-transfer-encoding;
        bh=WelV/pWtVXXfZbF52kdNles0Zom3KOHG85QgxN7nj6s=;
        b=bH+K+NNTbqUjEZM0lRIrxR9KTBp1S6hTxezsla9mg+WL61B7APdSrI+4GtzmCJUNNn
         hrAp78uDyP4W640ROlB55BQm2vvclT7eTzoWeuqYD08baLqHS8EfqL/6/6onCWGZKTTs
         Ziqh2Phsu55EHEOORHsCr61odJ3Ei4eKLLbRBC7Y4pgTByX2Yjp1FHzesszLP8PaKetl
         fgp9aIMVFOGNZDzW1IsBejeRtgDCRwURvoo7Nce7t+8HXYoMLkhgV1XI/kQ1xzzIrEmg
         W1UX98qV33PwvL5SlTzZIV1zH69pHK1Y0ZX9ACcxVyulP8Bq278YSkB7YWHtGJP07c8G
         rPdw==
MIME-Version: 1.0
X-Received: by 10.224.79.143 with SMTP id p15mr37897435qak.57.1400404439829;
 Sun, 18 May 2014 02:13:59 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Sun, 18 May 2014 02:13:59 -0700 (PDT)
In-Reply-To: <CAJiQeYKWeKoZuY9MjQTxzHn5d5XwvHwo0e8sEuEWL6CFW4p6CA@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
	<CA+-p3AEL6ct-bJwkBSXm+wTLBHEbBoGnnGOJGAjWHvQogDs-4g@mail.gmail.com>
	<CALRHqP-jLhJM3_5qCTWzrbU+GpmVsd2DFtvMXsH4-PGuT6X8ww@mail.gmail.com>
	<75094F23-AACD-4427-AE5B-A25AC02189A8@gmail.com>
	<CAJiQeY+Sy4EUKhM8ztsSfxaNoOG_73b=s7W_u5YRBPk-JegUDA@mail.gmail.com>
	<CAAsvFPm9OeOd7v2wsevLpKdqaCjfQuAA21r8itVu4-c2vft2OA@mail.gmail.com>
	<CAJiQeYKWeKoZuY9MjQTxzHn5d5XwvHwo0e8sEuEWL6CFW4p6CA@mail.gmail.com>
Date: Sun, 18 May 2014 14:43:59 +0530
Message-ID: <CAJiQeYKAbeAp7ZQK7bbxGF3YhfqeOOnt5-p7QO8n3g97f=YCKg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

So I think I need to clarify a few things here - particularly since
this mail went to the wrong mailing list and a much wider audience
than I intended it for :-)


Most of the issues I mentioned are internal implementation detail of
spark core : which means, we can enhance them in future without
disruption to our userbase (ability to support large number of
input/output partitions. Note: this is of order of 100k input and
output partitions with uniform spread of keys - very rarely seen
outside of some crazy jobs).

Some of the issues I mentioned would reqiure DeveloperApi changes -
which are not user exposed : they would impact developer use of these
api's - which are mostly internally provided by spark. (Like fixing
blocks > 2G would require change to Serializer api)

A smaller faction might require interface changes - note, I am
referring specifically to configuration changes (removing/deprecating
some) and possibly newer options to submit/env, etc - I dont envision
any programming api change itself.
The only api change we did was from Seq -> Iterable - which is
actually to address some of the issues I mentioned (join/cogroup).

Remaining are bugs which need to be addressed or the feature
removed/enhanced like shuffle consolidation.

There might be semantic extension of some things like OFF_HEAP storage
level to address other computation models - but that would not have an
impact on end user - since other options would be pluggable with
default set to Tachyon so that there is no user expectation change.


So will the interface possibly change ? Sure though we will try to
keep it backwardly compatible (as we did with 1.0).
Will the api change - other than backward compatible enhancements, probably=
 not.


Regards,
Mridul


On Sun, May 18, 2014 at 12:11 PM, Mridul Muralidharan <mridul@gmail.com> wr=
ote:
>
> On 18-May-2014 5:05 am, "Mark Hamstra" <mark@clearstorydata.com> wrote:
>>
>> I don't understand.  We never said that interfaces wouldn't change from
>> 0.9
>
> Agreed.
>
>> to 1.0.  What we are committing to is stability going forward from the
>> 1.0.0 baseline.  Nobody is disputing that backward-incompatible behavior
>> or
>> interface changes would be an issue post-1.0.0.  The question is whether
>
> The point is, how confident are we that these are the right set of interf=
ace
> definitions.
> We think it is, but we could also have gone through a 0.10 to vet the
> proposed 1.0 changes to stabilize them.
>
> To give examples for which we don't have solutions currently (which we ar=
e
> facing internally here btw, so not academic exercise) :
>
> - Current spark shuffle model breaks very badly as number of partitions
> increases (input and output).
>
> - As number of nodes increase, the overhead per node keeps going up. Spar=
k
> currently is more geared towards large memory machines; when the RAM per
> node is modest (8 to 16 gig) but large number of them are available, it d=
oes
> not do too well.
>
> - Current block abstraction breaks as data per block goes beyond 2 gig.
>
> - Cogroup/join when value per key or number of keys (or both) is high bre=
aks
> currently.
>
> - Shuffle consolidation is so badly broken it is not funny.
>
> - Currently there is no way of effectively leveraging accelerator
> cards/coprocessors/gpus from spark - to do so, I suspect we will need to
> redefine OFF_HEAP.
>
> - Effectively leveraging ssd is still an open question IMO when you have =
mix
> of both available.
>
> We have resolved some of these and looking at the rest. These are not uni=
que
> to our internal usage profile, I have seen most of these asked elsewhere
> too.
>
> Thankfully some of the 1.0 changes actually are geared towards helping to
> alleviate some of the above (Iterable change for ex), most of the rest ar=
e
> internal impl detail of spark core which helps a lot - but there are case=
s
> where this is not so.
>
> Unfortunately I don't know yet if the unresolved/uninvestigated issues wi=
ll
> require more changes or not.
>
> Given this I am very skeptical of expecting current spark interfaces to b=
e
> sufficient for next 1 year (forget 3)
>
> I understand this is an argument which can be made to never release 1.0 :=
-)
> Which is why I was ok with a 1.0 instead of 0.10 release in spite of my
> preference.
>
> This is a good problem to have IMO ... People are using spark extensively
> and in circumstances that we did not envision : necessitating changes eve=
n
> to spark core.
>
> But the claim that 1.0 interfaces are stable is not something I buy - the=
y
> are not, we will need to break them soon and cost of maintaining backward
> compatibility will be high.
>
> We just need to make an informed decision to live with that cost, not han=
d
> wave it away.
>
> Regards
> Mridul
>
>> there is anything apparent now that is expected to require such disrupti=
ve
>> changes if we were to commit to the current release candidate as our
>> guaranteed 1.0.0 baseline.
>>
>>
>> On Sat, May 17, 2014 at 2:05 PM, Mridul Muralidharan
>> <mridul@gmail.com>wrote:
>>
>> > I would make the case for interface stability not just api stability.
>> > Particularly given that we have significantly changed some of our
>> > interfaces, I want to ensure developers/users are not seeing red flags=
.
>> >
>> > Bugs and code stability can be addressed in minor releases if found, b=
ut
>> > behavioral change and/or interface changes would be a much more invasi=
ve
>> > issue for our users.
>> >
>> > Regards
>> > Mridul
>> > On 18-May-2014 2:19 am, "Matei Zaharia" <matei.zaharia@gmail.com> wrot=
e:
>> >
>> > > As others have said, the 1.0 milestone is about API stability, not
>> > > about
>> > > saying =E2=80=9Cwe=E2=80=99ve eliminated all bugs=E2=80=9D. The soon=
er you declare 1.0, the
>> > sooner
>> > > users can confidently build on Spark, knowing that the application
>> > > they
>> > > build today will still run on Spark 1.9.9 three years from now. This
>> > > is
>> > > something that I=E2=80=99ve seen done badly (and experienced the eff=
ects
>> > > thereof)
>> > > in other big data projects, such as MapReduce and even YARN. The
>> > > result
>> > is
>> > > that you annoy users, you end up with a fragmented userbase where
>> > everyone
>> > > is building against a different version, and you drastically slow do=
wn
>> > > development.
>> > >
>> > > With a project as fast-growing as fast-growing as Spark in particula=
r,
>> > > there will be new bugs discovered and reported continuously,
>> > > especially
>> > in
>> > > the non-core components. Look at the graph of # of contributors in
>> > > time
>> > to
>> > > Spark: https://www.ohloh.net/p/apache-spark (bottom-most graph;
>> > =E2=80=9Ccommits=E2=80=9D
>> > > changed when we started merging each patch as a single commit). This
>> > > is
>> > not
>> > > slowing down, and we need to have the culture now that we treat API
>> > > stability and release numbers at the level expected for a 1.0 projec=
t
>> > > instead of having people come in and randomly change the API.
>> > >
>> > > I=E2=80=99ll also note that the issues marked =E2=80=9Cblocker=E2=80=
=9D were marked so by
>> > > their
>> > > reporters, since the reporter can set the priority. I don=E2=80=99t =
consider
>> > stuff
>> > > like parallelize() not partitioning ranges in the same way as other
>> > > collections a blocker =E2=80=94 it=E2=80=99s a bug, it would be good=
 to fix it, but it
>> > only
>> > > affects a small number of use cases. Of course if we find a real
>> > > blocker
>> > > (in particular a regression from a previous version, or a feature
>> > > that=E2=80=99s
>> > > just completely broken), we will delay the release for that, but at
>> > > some
>> > > point you have to say =E2=80=9Cokay, this fix will go into the next
>> > > maintenance
>> > > release=E2=80=9D. Maybe we need to write a clear policy for what the=
 issue
>> > > priorities mean.
>> > >
>> > > Finally, I believe it=E2=80=99s much better to have a culture where =
you can
>> > > make
>> > > releases on a regular schedule, and have the option to make a
>> > > maintenance
>> > > release in 3-4 days if you find new bugs, than one where you pile up
>> > stuff
>> > > into each release. This is what much large project than us, like
>> > > Linux,
>> > do,
>> > > and it=E2=80=99s the only way to avoid indefinite stalling with a la=
rge
>> > contributor
>> > > base. In the worst case, if you find a new bug that warrants immedia=
te
>> > > release, it goes into 1.0.1 a week after 1.0.0 (we can vote on 1.0.1
>> > > in
>> > > three days with just your bug fix in it). And if you find an API tha=
t
>> > you=E2=80=99d
>> > > like to improve, just add a new one and maybe deprecate the old one =
=E2=80=94
>> > > at
>> > > some point we have to respect our users and let them know that code
>> > > they
>> > > write today will still run tomorrow.
>> > >
>> > > Matei
>> > >
>> > > On May 17, 2014, at 10:32 AM, Kan Zhang <kzhang@apache.org> wrote:
>> > >
>> > > > +1 on the running commentary here, non-binding of course :-)
>> > > >
>> > > >
>> > > > On Sat, May 17, 2014 at 8:44 AM, Andrew Ash <andrew@andrewash.com>
>> > > wrote:
>> > > >
>> > > >> +1 on the next release feeling more like a 0.10 than a 1.0
>> > > >> On May 17, 2014 4:38 AM, "Mridul Muralidharan" <mridul@gmail.com>
>> > > wrote:
>> > > >>
>> > > >>> I had echoed similar sentiments a while back when there was a
>> > > discussion
>> > > >>> around 0.10 vs 1.0 ... I would have preferred 0.10 to stabilize
>> > > >>> the
>> > api
>> > > >>> changes, add missing functionality, go through a hardening relea=
se
>> > > before
>> > > >>> 1.0
>> > > >>>
>> > > >>> But the community preferred a 1.0 :-)
>> > > >>>
>> > > >>> Regards,
>> > > >>> Mridul
>> > > >>>
>> > > >>> On 17-May-2014 3:19 pm, "Sean Owen" <sowen@cloudera.com> wrote:
>> > > >>>>
>> > > >>>> On this note, non-binding commentary:
>> > > >>>>
>> > > >>>> Releases happen in local minima of change, usually created by
>> > > >>>> internally enforced code freeze. Spark is incredibly busy now d=
ue
>> > > >>>> to
>> > > >>>> external factors -- recently a TLP, recently discovered by a
>> > > >>>> large
>> > new
>> > > >>>> audience, ease of contribution enabled by Github. It's getting
>> > > >>>> like
>> > > >>>> the first year of mainstream battle-testing in a month. It's be=
en
>> > very
>> > > >>>> hard to freeze anything! I see a number of non-trivial issues
>> > > >>>> being
>> > > >>>> reported, and I don't think it has been possible to triage all =
of
>> > > >>>> them, even.
>> > > >>>>
>> > > >>>> Given the high rate of change, my instinct would have been to
>> > release
>> > > >>>> 0.10.0 now. But won't it always be very busy? I do think the ra=
te
>> > > >>>> of
>> > > >>>> significant issues will slow down.
>> > > >>>>
>> > > >>>> Version ain't nothing but a number, but if it has any meaning
>> > > >>>> it's
>> > the
>> > > >>>> semantic versioning meaning. 1.0 imposes extra handicaps around
>> > > >>>> striving to maintain backwards-compatibility. That may end up
>> > > >>>> being
>> > > >>>> bent to fit in important changes that are going to be required =
in
>> > this
>> > > >>>> continuing period of change. Hadoop does this all the time
>> > > >>>> unfortunately and gets away with it, I suppose -- minor version
>> > > >>>> releases are really major. (On the other extreme, HBase is at
>> > > >>>> 0.98
>> > and
>> > > >>>> quite production-ready.)
>> > > >>>>
>> > > >>>> Just consider this a second vote for focus on fixes and 1.0.x
>> > > >>>> rather
>> > > >>>> than new features and 1.x. I think there are a few steps that
>> > > >>>> could
>> > > >>>> streamline triage of this flood of contributions, and make all =
of
>> > this
>> > > >>>> easier, but that's for another thread.
>> > > >>>>
>> > > >>>>
>> > > >>>> On Fri, May 16, 2014 at 8:50 PM, Mark Hamstra <
>> > > mark@clearstorydata.com
>> > > >>>
>> > > >>> wrote:
>> > > >>>>> +1, but just barely.  We've got quite a number of outstanding
>> > > >>>>> bugs
>> > > >>>>> identified, and many of them have fixes in progress.  I'd hate
>> > > >>>>> to
>> > see
>> > > >>> those
>> > > >>>>> efforts get lost in a post-1.0.0 flood of new features targete=
d
>> > > >>>>> at
>> > > >>> 1.1.0 --
>> > > >>>>> in other words, I'd like to see 1.0.1 retain a high priority
>> > relative
>> > > >>> to
>> > > >>>>> 1.1.0.
>> > > >>>>>
>> > > >>>>> Looking through the unresolved JIRAs, it doesn't look like any
>> > > >>>>> of
>> > the
>> > > >>>>> identified bugs are show-stoppers or strictly regressions
>> > (although I
>> > > >>> will
>> > > >>>>> note that one that I have in progress, SPARK-1749, is a bug th=
at
>> > > >>>>> we
>> > > >>>>> introduced with recent work -- it's not strictly a regression
>> > because
>> > > >>> we
>> > > >>>>> had equally bad but different behavior when the DAGScheduler
>> > > >> exceptions
>> > > >>>>> weren't previously being handled at all vs. being slightly
>> > > >> mis-handled
>> > > >>>>> now), so I'm not currently seeing a reason not to release.
>> > > >>>
>> > > >>
>> > >
>> > >
>> >

From dev-return-7666-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 17:39:45 2014
Return-Path: <dev-return-7666-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6172A11B84
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 17:39:45 +0000 (UTC)
Received: (qmail 77131 invoked by uid 500); 18 May 2014 17:30:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27841 invoked by uid 500); 18 May 2014 17:05:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27421 invoked by uid 99); 18 May 2014 16:59:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 16:59:09 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.212.177 as permitted sender)
Received: from [209.85.212.177] (HELO mail-wi0-f177.google.com) (209.85.212.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 16:59:07 +0000
Received: by mail-wi0-f177.google.com with SMTP id f8so3063568wiw.10
        for <dev@spark.apache.org>; Sun, 18 May 2014 09:58:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=LuJDC+1tBTCzIUZsM0u6CNMYBfQH2CR7Wts6f2PHOdg=;
        b=Qf/najzVy7tuocro4AHY5xVWtSyWAQPwcEmBXEefDRWLQ32XSrzBxuA4s/XNa1FoWX
         +GVi1dtIpF77jZvEwCRRvIDH4Sr+tC59UzRmzlymfP22scSuoenqxFZ2anEs1eHHv9P5
         hEvcAOB/gwTd8jCrlZUnFmZ1ErSDB5Zue2n7bCZHtSYNHirc3wDVn1u8Env1C7qlfM/E
         QEZytEKL/JULqkKZdVHCyb5Gys9PuoH1tbWZcMUB4KCMl+1df6QpBJJrKI55DoXv4Fnt
         mqU8GAu6XYdWE+5GtkaeU/1H1B/12ct7zGHTD4q9HGngQVOG7I7d/C1JBActj0RDXlcI
         O2/g==
MIME-Version: 1.0
X-Received: by 10.180.77.225 with SMTP id v1mr8427027wiw.5.1400432324132; Sun,
 18 May 2014 09:58:44 -0700 (PDT)
Received: by 10.194.187.20 with HTTP; Sun, 18 May 2014 09:58:44 -0700 (PDT)
In-Reply-To: <CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
Date: Sun, 18 May 2014 09:58:44 -0700
Message-ID: <CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I created a JIRA: https://issues.apache.org/jira/browse/SPARK-1870

DB, could you add more info to that JIRA? Thanks!

-Xiangrui

On Sun, May 18, 2014 at 9:46 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
> Btw, I tried
>
> rdd.map { i =>
>   System.getProperty("java.class.path")
> }.collect()
>
> but didn't see the jars added via "--jars" on the executor classpath.
>
> -Xiangrui
>
> On Sat, May 17, 2014 at 11:26 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>> I can re-produce the error with Spark 1.0-RC and YARN (CDH-5). The
>> reflection approach mentioned by DB didn't work either. I checked the
>> distributed cache on a worker node and found the jar there. It is also
>> in the Environment tab of the WebUI. The workaround is making an
>> assembly jar.
>>
>> DB, could you create a JIRA and describe what you have found so far? Thanks!
>>
>> Best,
>> Xiangrui
>>
>> On Sat, May 17, 2014 at 1:29 AM, Mridul Muralidharan <mridul@gmail.com> wrote:
>>> Can you try moving your mapPartitions to another class/object which is
>>> referenced only after sc.addJar ?
>>>
>>> I would suspect CNFEx is coming while loading the class containing
>>> mapPartitions before addJars is executed.
>>>
>>> In general though, dynamic loading of classes means you use reflection to
>>> instantiate it since expectation is you don't know which implementation
>>> provides the interface ... If you statically know it apriori, you bundle it
>>> in your classpath.
>>>
>>> Regards
>>> Mridul
>>> On 17-May-2014 7:28 am, "DB Tsai" <dbtsai@stanford.edu> wrote:
>>>
>>>> Finally find a way out of the ClassLoader maze! It took me some times to
>>>> understand how it works; I think it worths to document it in a separated
>>>> thread.
>>>>
>>>> We're trying to add external utility.jar which contains CSVRecordParser,
>>>> and we added the jar to executors through sc.addJar APIs.
>>>>
>>>> If the instance of CSVRecordParser is created without reflection, it
>>>> raises *ClassNotFound
>>>> Exception*.
>>>>
>>>> data.mapPartitions(lines => {
>>>>     val csvParser = new CSVRecordParser((delimiter.charAt(0))
>>>>     lines.foreach(line => {
>>>>       val lineElems = csvParser.parseLine(line)
>>>>     })
>>>>     ...
>>>>     ...
>>>>  )
>>>>
>>>>
>>>> If the instance of CSVRecordParser is created through reflection, it works.
>>>>
>>>> data.mapPartitions(lines => {
>>>>     val loader = Thread.currentThread.getContextClassLoader
>>>>     val CSVRecordParser =
>>>>         loader.loadClass("com.alpine.hadoop.ext.CSVRecordParser")
>>>>
>>>>     val csvParser = CSVRecordParser.getConstructor(Character.TYPE)
>>>>         .newInstance(delimiter.charAt(0).asInstanceOf[Character])
>>>>
>>>>     val parseLine = CSVRecordParser
>>>>         .getDeclaredMethod("parseLine", classOf[String])
>>>>
>>>>     lines.foreach(line => {
>>>>        val lineElems = parseLine.invoke(csvParser,
>>>> line).asInstanceOf[Array[String]]
>>>>     })
>>>>     ...
>>>>     ...
>>>>  )
>>>>
>>>>
>>>> This is identical to this question,
>>>>
>>>> http://stackoverflow.com/questions/7452411/thread-currentthread-setcontextclassloader-without-using-reflection
>>>>
>>>> It's not intuitive for users to load external classes through reflection,
>>>> but couple available solutions including 1) messing around
>>>> systemClassLoader by calling systemClassLoader.addURI through reflection or
>>>> 2) forking another JVM to add jars into classpath before bootstrap loader
>>>> are very tricky.
>>>>
>>>> Any thought on fixing it properly?
>>>>
>>>> @Xiangrui,
>>>> netlib-java jniloader is loaded from netlib-java through reflection, so
>>>> this problem will not be seen.
>>>>
>>>> Sincerely,
>>>>
>>>> DB Tsai
>>>> -------------------------------------------------------
>>>> My Blog: https://www.dbtsai.com
>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>

From dev-return-7667-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 17:55:56 2014
Return-Path: <dev-return-7667-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4EEE411BD8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 17:55:56 +0000 (UTC)
Received: (qmail 79286 invoked by uid 500); 18 May 2014 17:30:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28811 invoked by uid 500); 18 May 2014 17:05:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7277 invoked by uid 99); 18 May 2014 16:46:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 16:46:54 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 74.125.82.175 as permitted sender)
Received: from [74.125.82.175] (HELO mail-we0-f175.google.com) (74.125.82.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 16:46:50 +0000
Received: by mail-we0-f175.google.com with SMTP id t61so4439118wes.6
        for <dev@spark.apache.org>; Sun, 18 May 2014 09:46:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=btyIMwToToKGs1l5ijFY3Na6uI3XRJNZEY6wIXdKpFk=;
        b=YcKFF8RiOwK29F+bI7x5SK67ILWFLylvhp2pGNR9vTT6jkvlXL5gpS6eE06v6shf+r
         hIuiiBz/L5O5804O/KpuFr+0KzfnQ6/4L5fZ6iM6I/2n3QDfJaRl8DHe7wahpkbEKX6w
         NDND/GoX1OvqrsejYrgZoPjpE4UbFKHjYPfxMW8rXcFOXgbFO7VUIvV4YRY5bEpGTp2J
         YiXsBeXZVZkp3K3v+/1wZZUFvsK2tfiIxY7u11mca0qjEVTAgnS+lq4uzJEkkpyKAbFa
         nus+lgCn/8D2fX2yoH/n2vBk/SaeNAOo6DyHw4yiuGj8GwTHzVffj5SM/gY2CcrfIQpd
         0/fw==
MIME-Version: 1.0
X-Received: by 10.194.94.71 with SMTP id da7mr3820467wjb.56.1400431589565;
 Sun, 18 May 2014 09:46:29 -0700 (PDT)
Received: by 10.194.187.20 with HTTP; Sun, 18 May 2014 09:46:29 -0700 (PDT)
In-Reply-To: <CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
Date: Sun, 18 May 2014 09:46:29 -0700
Message-ID: <CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Btw, I tried

rdd.map { i =>
  System.getProperty("java.class.path")
}.collect()

but didn't see the jars added via "--jars" on the executor classpath.

-Xiangrui

On Sat, May 17, 2014 at 11:26 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
> I can re-produce the error with Spark 1.0-RC and YARN (CDH-5). The
> reflection approach mentioned by DB didn't work either. I checked the
> distributed cache on a worker node and found the jar there. It is also
> in the Environment tab of the WebUI. The workaround is making an
> assembly jar.
>
> DB, could you create a JIRA and describe what you have found so far? Thanks!
>
> Best,
> Xiangrui
>
> On Sat, May 17, 2014 at 1:29 AM, Mridul Muralidharan <mridul@gmail.com> wrote:
>> Can you try moving your mapPartitions to another class/object which is
>> referenced only after sc.addJar ?
>>
>> I would suspect CNFEx is coming while loading the class containing
>> mapPartitions before addJars is executed.
>>
>> In general though, dynamic loading of classes means you use reflection to
>> instantiate it since expectation is you don't know which implementation
>> provides the interface ... If you statically know it apriori, you bundle it
>> in your classpath.
>>
>> Regards
>> Mridul
>> On 17-May-2014 7:28 am, "DB Tsai" <dbtsai@stanford.edu> wrote:
>>
>>> Finally find a way out of the ClassLoader maze! It took me some times to
>>> understand how it works; I think it worths to document it in a separated
>>> thread.
>>>
>>> We're trying to add external utility.jar which contains CSVRecordParser,
>>> and we added the jar to executors through sc.addJar APIs.
>>>
>>> If the instance of CSVRecordParser is created without reflection, it
>>> raises *ClassNotFound
>>> Exception*.
>>>
>>> data.mapPartitions(lines => {
>>>     val csvParser = new CSVRecordParser((delimiter.charAt(0))
>>>     lines.foreach(line => {
>>>       val lineElems = csvParser.parseLine(line)
>>>     })
>>>     ...
>>>     ...
>>>  )
>>>
>>>
>>> If the instance of CSVRecordParser is created through reflection, it works.
>>>
>>> data.mapPartitions(lines => {
>>>     val loader = Thread.currentThread.getContextClassLoader
>>>     val CSVRecordParser =
>>>         loader.loadClass("com.alpine.hadoop.ext.CSVRecordParser")
>>>
>>>     val csvParser = CSVRecordParser.getConstructor(Character.TYPE)
>>>         .newInstance(delimiter.charAt(0).asInstanceOf[Character])
>>>
>>>     val parseLine = CSVRecordParser
>>>         .getDeclaredMethod("parseLine", classOf[String])
>>>
>>>     lines.foreach(line => {
>>>        val lineElems = parseLine.invoke(csvParser,
>>> line).asInstanceOf[Array[String]]
>>>     })
>>>     ...
>>>     ...
>>>  )
>>>
>>>
>>> This is identical to this question,
>>>
>>> http://stackoverflow.com/questions/7452411/thread-currentthread-setcontextclassloader-without-using-reflection
>>>
>>> It's not intuitive for users to load external classes through reflection,
>>> but couple available solutions including 1) messing around
>>> systemClassLoader by calling systemClassLoader.addURI through reflection or
>>> 2) forking another JVM to add jars into classpath before bootstrap loader
>>> are very tricky.
>>>
>>> Any thought on fixing it properly?
>>>
>>> @Xiangrui,
>>> netlib-java jniloader is loaded from netlib-java through reflection, so
>>> this problem will not be seen.
>>>
>>> Sincerely,
>>>
>>> DB Tsai
>>> -------------------------------------------------------
>>> My Blog: https://www.dbtsai.com
>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>

From dev-return-7668-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 18:46:04 2014
Return-Path: <dev-return-7668-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9E74011CCD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 18:46:04 +0000 (UTC)
Received: (qmail 56094 invoked by uid 500); 18 May 2014 18:20:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25520 invoked by uid 500); 18 May 2014 17:55:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1503 invoked by uid 99); 18 May 2014 17:34:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 17:34:59 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [85.128.177.112] (HELO alu112.rev.netart.pl) (85.128.177.112)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 17:34:53 +0000
Received: from mail-yk0-f170.google.com (unknown [209.85.160.170])
	by laskowski.nazwa.pl (Postfix) with ESMTP id B282BF26D21
	for <dev@spark.apache.org>; Sun, 18 May 2014 19:34:31 +0200 (CEST)
Received: by mail-yk0-f170.google.com with SMTP id 10so3814390ykt.29
        for <dev@spark.apache.org>; Sun, 18 May 2014 10:34:30 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.236.226.101 with SMTP id a95mr45868598yhq.88.1400434470030;
 Sun, 18 May 2014 10:34:30 -0700 (PDT)
Received: by 10.170.56.7 with HTTP; Sun, 18 May 2014 10:34:29 -0700 (PDT)
Received: by 10.170.56.7 with HTTP; Sun, 18 May 2014 10:34:29 -0700 (PDT)
In-Reply-To: <6EF34F88-F79D-49B0-92FD-C421FBB0B6ED@gmail.com>
References: <JIRA.12714626.1400189782796@arcas>
	<JIRA.12714626.1400189782796.363092.1400238099511@arcas>
	<CAJiQeY+zsu3cfX1zyA=qK1KO8hirddmjGSB7HohTTA_a9jk3wQ@mail.gmail.com>
	<CAJgQjQ_1qrz4=05E=Wwj+O4UqKHwFKWQQiZosKbPjvwocMz9mw@mail.gmail.com>
	<CAJiQeY+eGxSzM-6C_poKCY53Gkixg85rdXiXXx+qi0z0bvCrrQ@mail.gmail.com>
	<6EF34F88-F79D-49B0-92FD-C421FBB0B6ED@gmail.com>
Date: Sun, 18 May 2014 19:34:29 +0200
Message-ID: <CACo38_QBRwZkxRwqbja_CTdPYy9ysY2b=ewntcq_5A-LVs9O0w@mail.gmail.com>
Subject: Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing
From: Jacek Laskowski <jacek@japila.pl>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1130ceac1e149104f9b00f6d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1130ceac1e149104f9b00f6d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi,

I'm curious if it's a common approach to have discussions in JIRA not here.
I don't think it's the ASF way.

Pozdrawiam,
Jacek Laskowski
http://blog.japila.pl
17 maj 2014 23:55 "Matei Zaharia" <matei.zaharia@gmail.com> napisa=C5=82(a)=
:

> We do actually have replicated StorageLevels in Spark. You can use
> MEMORY_AND_DISK_2 or construct your own StorageLevel with your own custom
> replication factor.
>
> BTW you guys should probably have this discussion on the JIRA rather than
> the dev list; I think the replies somehow ended up on the dev list.
>
> Matei
>
> On May 17, 2014, at 1:36 AM, Mridul Muralidharan <mridul@gmail.com> wrote=
:
>
> > We don't have 3x replication in spark :-)
> > And if we use replicated storagelevel, while decreasing odds of failure=
,
> it
> > does not eliminate it (since we are not doing a great job with
> replication
> > anyway from fault tolerance point of view).
> > Also it does take a nontrivial performance hit with replicated levels.
> >
> > Regards,
> > Mridul
> > On 17-May-2014 8:16 am, "Xiangrui Meng" <mengxr@gmail.com> wrote:
> >
> >> With 3x replication, we should be able to achieve fault tolerance.
> >> This checkPointed RDD can be cleared if we have another in-memory
> >> checkPointed RDD down the line. It can avoid hitting disk if we have
> >> enough memory to use. We need to investigate more to find a good
> >> solution. -Xiangrui
> >>
> >> On Fri, May 16, 2014 at 4:00 PM, Mridul Muralidharan <mridul@gmail.com=
>
> >> wrote:
> >>> Effectively this is persist without fault tolerance.
> >>> Failure of any node means complete lack of fault tolerance.
> >>> I would be very skeptical of truncating lineage if it is not reliable=
.
> >>> On 17-May-2014 3:49 am, "Xiangrui Meng (JIRA)" <jira@apache.org>
> wrote:
> >>>
> >>>> Xiangrui Meng created SPARK-1855:
> >>>> ------------------------------------
> >>>>
> >>>>             Summary: Provide memory-and-local-disk RDD checkpointing
> >>>>                 Key: SPARK-1855
> >>>>                 URL: https://issues.apache.org/jira/browse/SPARK-185=
5
> >>>>             Project: Spark
> >>>>          Issue Type: New Feature
> >>>>          Components: MLlib, Spark Core
> >>>>    Affects Versions: 1.0.0
> >>>>            Reporter: Xiangrui Meng
> >>>>
> >>>>
> >>>> Checkpointing is used to cut long lineage while maintaining fault
> >>>> tolerance. The current implementation is HDFS-based. Using the
> BlockRDD
> >> we
> >>>> can create in-memory-and-local-disk (with replication) checkpoints
> that
> >> are
> >>>> not as reliable as HDFS-based solution but faster.
> >>>>
> >>>> It can help applications that require many iterations.
> >>>>
> >>>>
> >>>>
> >>>> --
> >>>> This message was sent by Atlassian JIRA
> >>>> (v6.2#6252)
> >>>>
> >>
>
>

--001a1130ceac1e149104f9b00f6d--

From dev-return-7669-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 19:36:05 2014
Return-Path: <dev-return-7669-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0168D11E0B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 19:36:05 +0000 (UTC)
Received: (qmail 33717 invoked by uid 500); 18 May 2014 19:11:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97429 invoked by uid 500); 18 May 2014 18:46:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81607 invoked by uid 99); 18 May 2014 18:29:37 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 18:29:37 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.172] (HELO mail-vc0-f172.google.com) (209.85.220.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 18:29:33 +0000
Received: by mail-vc0-f172.google.com with SMTP id hr9so8523757vcb.31
        for <dev@spark.apache.org>; Sun, 18 May 2014 11:29:09 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=+z4QoWpAgxVijJtULRAiwidSO9KT4QhW28XDjxBywxU=;
        b=XGsIBpR0BRSUTVhaNVx+iV7R4/Jnt9UuiCjghvK5CiLSVod7c+cETBVSuPiGxew0cl
         osbDLFWkqmNwCSYNdTW3DhhMBeUbfkF7oJmovXCUkDT7nnWwT3zSG6BOBnhALs8nOQeg
         h8+WDKp3QhzTzxWLNIU4YXME2sMHLS8txS1pBAlsFpgfChSxuzs12qfrX9IwVp0tE435
         X7BdLttlEtFYh7aRvXASLPjUeRzusyaWB0yCCv8RsGjUbdqh4yZDoOrrfkcUvoTf3WQj
         c/0EwOQVAUYBliPuid7Lo5xk3xg7Wwub1xtYkPHAnpP4cRoLUV76iK0P/Ncdez0fxfH3
         cJSA==
X-Gm-Message-State: ALoCoQmm+M7lyLEm1vsdAEICQFL+Rfv7kFID7OKPw3M3t6lbYNOjHlqFSnSX/A7xjLH1y0sGbKoF
X-Received: by 10.52.110.105 with SMTP id hz9mr8335788vdb.9.1400437749197;
        Sun, 18 May 2014 11:29:09 -0700 (PDT)
Received: from mail-vc0-f180.google.com (mail-vc0-f180.google.com [209.85.220.180])
        by mx.google.com with ESMTPSA id om10sm16905187vec.9.2014.05.18.11.29.08
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 18 May 2014 11:29:08 -0700 (PDT)
Received: by mail-vc0-f180.google.com with SMTP id hy4so8603664vcb.11
        for <dev@spark.apache.org>; Sun, 18 May 2014 11:29:07 -0700 (PDT)
X-Received: by 10.220.195.196 with SMTP id ed4mr1334491vcb.42.1400437747690;
 Sun, 18 May 2014 11:29:07 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Sun, 18 May 2014 11:28:47 -0700 (PDT)
In-Reply-To: <CACo38_QBRwZkxRwqbja_CTdPYy9ysY2b=ewntcq_5A-LVs9O0w@mail.gmail.com>
References: <JIRA.12714626.1400189782796@arcas> <JIRA.12714626.1400189782796.363092.1400238099511@arcas>
 <CAJiQeY+zsu3cfX1zyA=qK1KO8hirddmjGSB7HohTTA_a9jk3wQ@mail.gmail.com>
 <CAJgQjQ_1qrz4=05E=Wwj+O4UqKHwFKWQQiZosKbPjvwocMz9mw@mail.gmail.com>
 <CAJiQeY+eGxSzM-6C_poKCY53Gkixg85rdXiXXx+qi0z0bvCrrQ@mail.gmail.com>
 <6EF34F88-F79D-49B0-92FD-C421FBB0B6ED@gmail.com> <CACo38_QBRwZkxRwqbja_CTdPYy9ysY2b=ewntcq_5A-LVs9O0w@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Sun, 18 May 2014 11:28:47 -0700
Message-ID: <CA+-p3AFtOm_f6QopygGkA0jGAOvec1FXOEwrK+d238cTrHKW2w@mail.gmail.com>
Subject: Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c1c3247b35b304f9b0d2fd
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1c3247b35b304f9b0d2fd
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

The nice thing about putting discussion on the Jira is that everything
about the bug is in one place.  So people looking to understand the
discussion a few years from now only have to look on the jira ticket rather
than also search the mailing list archives and hope commenters all put the
string "SPARK-1855" into the messages.


On Sun, May 18, 2014 at 10:34 AM, Jacek Laskowski <jacek@japila.pl> wrote:

> Hi,
>
> I'm curious if it's a common approach to have discussions in JIRA not her=
e.
> I don't think it's the ASF way.
>
> Pozdrawiam,
> Jacek Laskowski
> http://blog.japila.pl
> 17 maj 2014 23:55 "Matei Zaharia" <matei.zaharia@gmail.com> napisa=C5=82(=
a):
>
> > We do actually have replicated StorageLevels in Spark. You can use
> > MEMORY_AND_DISK_2 or construct your own StorageLevel with your own cust=
om
> > replication factor.
> >
> > BTW you guys should probably have this discussion on the JIRA rather th=
an
> > the dev list; I think the replies somehow ended up on the dev list.
> >
> > Matei
> >
> > On May 17, 2014, at 1:36 AM, Mridul Muralidharan <mridul@gmail.com>
> wrote:
> >
> > > We don't have 3x replication in spark :-)
> > > And if we use replicated storagelevel, while decreasing odds of
> failure,
> > it
> > > does not eliminate it (since we are not doing a great job with
> > replication
> > > anyway from fault tolerance point of view).
> > > Also it does take a nontrivial performance hit with replicated levels=
.
> > >
> > > Regards,
> > > Mridul
> > > On 17-May-2014 8:16 am, "Xiangrui Meng" <mengxr@gmail.com> wrote:
> > >
> > >> With 3x replication, we should be able to achieve fault tolerance.
> > >> This checkPointed RDD can be cleared if we have another in-memory
> > >> checkPointed RDD down the line. It can avoid hitting disk if we have
> > >> enough memory to use. We need to investigate more to find a good
> > >> solution. -Xiangrui
> > >>
> > >> On Fri, May 16, 2014 at 4:00 PM, Mridul Muralidharan <
> mridul@gmail.com>
> > >> wrote:
> > >>> Effectively this is persist without fault tolerance.
> > >>> Failure of any node means complete lack of fault tolerance.
> > >>> I would be very skeptical of truncating lineage if it is not
> reliable.
> > >>> On 17-May-2014 3:49 am, "Xiangrui Meng (JIRA)" <jira@apache.org>
> > wrote:
> > >>>
> > >>>> Xiangrui Meng created SPARK-1855:
> > >>>> ------------------------------------
> > >>>>
> > >>>>             Summary: Provide memory-and-local-disk RDD checkpointi=
ng
> > >>>>                 Key: SPARK-1855
> > >>>>                 URL:
> https://issues.apache.org/jira/browse/SPARK-1855
> > >>>>             Project: Spark
> > >>>>          Issue Type: New Feature
> > >>>>          Components: MLlib, Spark Core
> > >>>>    Affects Versions: 1.0.0
> > >>>>            Reporter: Xiangrui Meng
> > >>>>
> > >>>>
> > >>>> Checkpointing is used to cut long lineage while maintaining fault
> > >>>> tolerance. The current implementation is HDFS-based. Using the
> > BlockRDD
> > >> we
> > >>>> can create in-memory-and-local-disk (with replication) checkpoints
> > that
> > >> are
> > >>>> not as reliable as HDFS-based solution but faster.
> > >>>>
> > >>>> It can help applications that require many iterations.
> > >>>>
> > >>>>
> > >>>>
> > >>>> --
> > >>>> This message was sent by Atlassian JIRA
> > >>>> (v6.2#6252)
> > >>>>
> > >>
> >
> >
>

--001a11c1c3247b35b304f9b0d2fd--

From dev-return-7671-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 19:44:06 2014
Return-Path: <dev-return-7671-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0AE9111E39
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 19:44:05 +0000 (UTC)
Received: (qmail 61407 invoked by uid 500); 18 May 2014 19:35:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34047 invoked by uid 500); 18 May 2014 19:11:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21550 invoked by uid 99); 18 May 2014 18:55:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 18:55:04 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.43 as permitted sender)
Received: from [209.85.219.43] (HELO mail-oa0-f43.google.com) (209.85.219.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 18:55:01 +0000
Received: by mail-oa0-f43.google.com with SMTP id l6so5277695oag.2
        for <dev@spark.apache.org>; Sun, 18 May 2014 11:54:38 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=sWZh6JQQ1rA8fwR0uqNGW66G9+T5mhIi+NtgyNfgYJQ=;
        b=w7TY0A+ILmsQpYtQ+cuD/8HpyBz4Pon6j92sEpk8sJ2XTUhdP0/peR2dHmagMhZxGD
         PqlqgaPnLj/6etVbJQFXLYFzpySaCny1vAnYMossN41xb1xQCW5IUWgVPPWj3GYgUClw
         J1EMsa+v6dKEtw/hfA5NeSlDoG37G3D7Qgv0HxKz3mlDaiO6/yApvr/3Vpm0fgf7NRag
         GoeGNNl30Bo3Gk48Q8K22AtuY3iZ1oh52suHQHx5ovjUmOFkkG+f9fJd6O1PxRBTthBK
         EKt4jRQQRFXKit8Lyh5JMZtZVpjsWkq7YhIPlySaKABZBa0PF6i3SzaJe+X4l2J9KIT7
         +VcQ==
MIME-Version: 1.0
X-Received: by 10.60.179.138 with SMTP id dg10mr30929324oec.13.1400439278060;
 Sun, 18 May 2014 11:54:38 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Sun, 18 May 2014 11:54:37 -0700 (PDT)
In-Reply-To: <CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
Date: Sun, 18 May 2014 11:54:37 -0700
Message-ID: <CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

@xiangrui - we don't expect these to be present on the system
classpath, because they get dynamically added by Spark (e.g. your
application can call sc.addJar well after the JVM's have started).

@db - I'm pretty surprised to see that behavior. It's definitely not
intended that users need reflection to instantiate their classes -
something odd is going on in your case. If you could create an
isolated example and post it to the JIRA, that would be great.

On Sun, May 18, 2014 at 9:58 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
> I created a JIRA: https://issues.apache.org/jira/browse/SPARK-1870
>
> DB, could you add more info to that JIRA? Thanks!
>
> -Xiangrui
>
> On Sun, May 18, 2014 at 9:46 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>> Btw, I tried
>>
>> rdd.map { i =>
>>   System.getProperty("java.class.path")
>> }.collect()
>>
>> but didn't see the jars added via "--jars" on the executor classpath.
>>
>> -Xiangrui
>>
>> On Sat, May 17, 2014 at 11:26 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>> I can re-produce the error with Spark 1.0-RC and YARN (CDH-5). The
>>> reflection approach mentioned by DB didn't work either. I checked the
>>> distributed cache on a worker node and found the jar there. It is also
>>> in the Environment tab of the WebUI. The workaround is making an
>>> assembly jar.
>>>
>>> DB, could you create a JIRA and describe what you have found so far? Thanks!
>>>
>>> Best,
>>> Xiangrui
>>>
>>> On Sat, May 17, 2014 at 1:29 AM, Mridul Muralidharan <mridul@gmail.com> wrote:
>>>> Can you try moving your mapPartitions to another class/object which is
>>>> referenced only after sc.addJar ?
>>>>
>>>> I would suspect CNFEx is coming while loading the class containing
>>>> mapPartitions before addJars is executed.
>>>>
>>>> In general though, dynamic loading of classes means you use reflection to
>>>> instantiate it since expectation is you don't know which implementation
>>>> provides the interface ... If you statically know it apriori, you bundle it
>>>> in your classpath.
>>>>
>>>> Regards
>>>> Mridul
>>>> On 17-May-2014 7:28 am, "DB Tsai" <dbtsai@stanford.edu> wrote:
>>>>
>>>>> Finally find a way out of the ClassLoader maze! It took me some times to
>>>>> understand how it works; I think it worths to document it in a separated
>>>>> thread.
>>>>>
>>>>> We're trying to add external utility.jar which contains CSVRecordParser,
>>>>> and we added the jar to executors through sc.addJar APIs.
>>>>>
>>>>> If the instance of CSVRecordParser is created without reflection, it
>>>>> raises *ClassNotFound
>>>>> Exception*.
>>>>>
>>>>> data.mapPartitions(lines => {
>>>>>     val csvParser = new CSVRecordParser((delimiter.charAt(0))
>>>>>     lines.foreach(line => {
>>>>>       val lineElems = csvParser.parseLine(line)
>>>>>     })
>>>>>     ...
>>>>>     ...
>>>>>  )
>>>>>
>>>>>
>>>>> If the instance of CSVRecordParser is created through reflection, it works.
>>>>>
>>>>> data.mapPartitions(lines => {
>>>>>     val loader = Thread.currentThread.getContextClassLoader
>>>>>     val CSVRecordParser =
>>>>>         loader.loadClass("com.alpine.hadoop.ext.CSVRecordParser")
>>>>>
>>>>>     val csvParser = CSVRecordParser.getConstructor(Character.TYPE)
>>>>>         .newInstance(delimiter.charAt(0).asInstanceOf[Character])
>>>>>
>>>>>     val parseLine = CSVRecordParser
>>>>>         .getDeclaredMethod("parseLine", classOf[String])
>>>>>
>>>>>     lines.foreach(line => {
>>>>>        val lineElems = parseLine.invoke(csvParser,
>>>>> line).asInstanceOf[Array[String]]
>>>>>     })
>>>>>     ...
>>>>>     ...
>>>>>  )
>>>>>
>>>>>
>>>>> This is identical to this question,
>>>>>
>>>>> http://stackoverflow.com/questions/7452411/thread-currentthread-setcontextclassloader-without-using-reflection
>>>>>
>>>>> It's not intuitive for users to load external classes through reflection,
>>>>> but couple available solutions including 1) messing around
>>>>> systemClassLoader by calling systemClassLoader.addURI through reflection or
>>>>> 2) forking another JVM to add jars into classpath before bootstrap loader
>>>>> are very tricky.
>>>>>
>>>>> Any thought on fixing it properly?
>>>>>
>>>>> @Xiangrui,
>>>>> netlib-java jniloader is loaded from netlib-java through reflection, so
>>>>> this problem will not be seen.
>>>>>
>>>>> Sincerely,
>>>>>
>>>>> DB Tsai
>>>>> -------------------------------------------------------
>>>>> My Blog: https://www.dbtsai.com
>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>

From dev-return-7670-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 20:06:51 2014
Return-Path: <dev-return-7670-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8DE6311E91
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 20:06:51 +0000 (UTC)
Received: (qmail 33152 invoked by uid 500); 18 May 2014 19:11:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31325 invoked by uid 500); 18 May 2014 19:10:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27898 invoked by uid 99); 18 May 2014 18:59:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 18:59:18 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.172 as permitted sender)
Received: from [209.85.214.172] (HELO mail-ob0-f172.google.com) (209.85.214.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 18:59:14 +0000
Received: by mail-ob0-f172.google.com with SMTP id wp18so5178777obc.17
        for <dev@spark.apache.org>; Sun, 18 May 2014 11:58:54 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=pEAAC1c98DeiHPBGURuJ+3BnARSIaFN68pnNX1XxC8Q=;
        b=UwW6Y8rpw9PdZ9NoeGjE/jf1Ma6IccbZTO5Ps7YpPeGKs5cP7ItsUSUSMhCzJlaD7I
         FlreTzOGqlRVlQV999+wkFjIbdETsPBqaVRtGUHEC6/TcvIcns088U91gI1stuHPsazL
         JdocBNiTAmu41Mvkz7gUMOq68nRfexgG4C1u9RjzoWViXeR7X65I3d3PhbvFiUqVWnu8
         tO0NOtcGugjTZQ5heO4WfMk8Lkoym+aYwnNscGLKPLqRMBjqIABXyYHLiqU/H6b2k/LZ
         XR3qW8ir0AYrjIL46QHKQ0WMPnapCHId0ZwgPUeLHyd5pbDLdXomXXJ5bE8wtxmTjL8V
         WGxA==
MIME-Version: 1.0
X-Received: by 10.60.54.38 with SMTP id g6mr3317949oep.79.1400439534225; Sun,
 18 May 2014 11:58:54 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Sun, 18 May 2014 11:58:54 -0700 (PDT)
In-Reply-To: <CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
Date: Sun, 18 May 2014 11:58:54 -0700
Message-ID: <CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

@db - it's possible that you aren't including the jar in the classpath
of your driver program (I think this is what mridul was suggesting).
It would be helpful to see the stack trace of the CNFE.

- Patrick

On Sun, May 18, 2014 at 11:54 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> @xiangrui - we don't expect these to be present on the system
> classpath, because they get dynamically added by Spark (e.g. your
> application can call sc.addJar well after the JVM's have started).
>
> @db - I'm pretty surprised to see that behavior. It's definitely not
> intended that users need reflection to instantiate their classes -
> something odd is going on in your case. If you could create an
> isolated example and post it to the JIRA, that would be great.
>
> On Sun, May 18, 2014 at 9:58 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>> I created a JIRA: https://issues.apache.org/jira/browse/SPARK-1870
>>
>> DB, could you add more info to that JIRA? Thanks!
>>
>> -Xiangrui
>>
>> On Sun, May 18, 2014 at 9:46 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>> Btw, I tried
>>>
>>> rdd.map { i =>
>>>   System.getProperty("java.class.path")
>>> }.collect()
>>>
>>> but didn't see the jars added via "--jars" on the executor classpath.
>>>
>>> -Xiangrui
>>>
>>> On Sat, May 17, 2014 at 11:26 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>>> I can re-produce the error with Spark 1.0-RC and YARN (CDH-5). The
>>>> reflection approach mentioned by DB didn't work either. I checked the
>>>> distributed cache on a worker node and found the jar there. It is also
>>>> in the Environment tab of the WebUI. The workaround is making an
>>>> assembly jar.
>>>>
>>>> DB, could you create a JIRA and describe what you have found so far? Thanks!
>>>>
>>>> Best,
>>>> Xiangrui
>>>>
>>>> On Sat, May 17, 2014 at 1:29 AM, Mridul Muralidharan <mridul@gmail.com> wrote:
>>>>> Can you try moving your mapPartitions to another class/object which is
>>>>> referenced only after sc.addJar ?
>>>>>
>>>>> I would suspect CNFEx is coming while loading the class containing
>>>>> mapPartitions before addJars is executed.
>>>>>
>>>>> In general though, dynamic loading of classes means you use reflection to
>>>>> instantiate it since expectation is you don't know which implementation
>>>>> provides the interface ... If you statically know it apriori, you bundle it
>>>>> in your classpath.
>>>>>
>>>>> Regards
>>>>> Mridul
>>>>> On 17-May-2014 7:28 am, "DB Tsai" <dbtsai@stanford.edu> wrote:
>>>>>
>>>>>> Finally find a way out of the ClassLoader maze! It took me some times to
>>>>>> understand how it works; I think it worths to document it in a separated
>>>>>> thread.
>>>>>>
>>>>>> We're trying to add external utility.jar which contains CSVRecordParser,
>>>>>> and we added the jar to executors through sc.addJar APIs.
>>>>>>
>>>>>> If the instance of CSVRecordParser is created without reflection, it
>>>>>> raises *ClassNotFound
>>>>>> Exception*.
>>>>>>
>>>>>> data.mapPartitions(lines => {
>>>>>>     val csvParser = new CSVRecordParser((delimiter.charAt(0))
>>>>>>     lines.foreach(line => {
>>>>>>       val lineElems = csvParser.parseLine(line)
>>>>>>     })
>>>>>>     ...
>>>>>>     ...
>>>>>>  )
>>>>>>
>>>>>>
>>>>>> If the instance of CSVRecordParser is created through reflection, it works.
>>>>>>
>>>>>> data.mapPartitions(lines => {
>>>>>>     val loader = Thread.currentThread.getContextClassLoader
>>>>>>     val CSVRecordParser =
>>>>>>         loader.loadClass("com.alpine.hadoop.ext.CSVRecordParser")
>>>>>>
>>>>>>     val csvParser = CSVRecordParser.getConstructor(Character.TYPE)
>>>>>>         .newInstance(delimiter.charAt(0).asInstanceOf[Character])
>>>>>>
>>>>>>     val parseLine = CSVRecordParser
>>>>>>         .getDeclaredMethod("parseLine", classOf[String])
>>>>>>
>>>>>>     lines.foreach(line => {
>>>>>>        val lineElems = parseLine.invoke(csvParser,
>>>>>> line).asInstanceOf[Array[String]]
>>>>>>     })
>>>>>>     ...
>>>>>>     ...
>>>>>>  )
>>>>>>
>>>>>>
>>>>>> This is identical to this question,
>>>>>>
>>>>>> http://stackoverflow.com/questions/7452411/thread-currentthread-setcontextclassloader-without-using-reflection
>>>>>>
>>>>>> It's not intuitive for users to load external classes through reflection,
>>>>>> but couple available solutions including 1) messing around
>>>>>> systemClassLoader by calling systemClassLoader.addURI through reflection or
>>>>>> 2) forking another JVM to add jars into classpath before bootstrap loader
>>>>>> are very tricky.
>>>>>>
>>>>>> Any thought on fixing it properly?
>>>>>>
>>>>>> @Xiangrui,
>>>>>> netlib-java jniloader is loaded from netlib-java through reflection, so
>>>>>> this problem will not be seen.
>>>>>>
>>>>>> Sincerely,
>>>>>>
>>>>>> DB Tsai
>>>>>> -------------------------------------------------------
>>>>>> My Blog: https://www.dbtsai.com
>>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>

From dev-return-7673-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 20:26:05 2014
Return-Path: <dev-return-7673-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4F45911F10
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 20:26:05 +0000 (UTC)
Received: (qmail 99205 invoked by uid 500); 18 May 2014 20:01:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65965 invoked by uid 500); 18 May 2014 19:36:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60866 invoked by uid 99); 18 May 2014 19:32:08 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 19:32:08 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.169] (HELO mail-vc0-f169.google.com) (209.85.220.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 19:32:03 +0000
Received: by mail-vc0-f169.google.com with SMTP id ij19so8782380vcb.28
        for <dev@spark.apache.org>; Sun, 18 May 2014 12:31:42 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=QGFZyYsGRqkzio4DAMhgkyViTs0uT+DU3UcsDPWHnok=;
        b=RCpcEt7hml5aFgyvhP66m+k12pGobfJzh41nPFNnral4G5QXC5OlJHlQWd6Oyk70zD
         UFanz+xswQZ1pYhEVMPXqPPkuaH1G9iVtn5S7/Di0BbYrxx3TEjnT89D6xjfdX5xHm5m
         HLpWMZjRXgR1WA0t0YUseN0O+hETgsvO7zrNs31M5VBzW1Uo0bXF2/jU5u0D2YZYLYOC
         D3vrfbMMGoVCOjrR76ODBDar7t1YbPAfPRkAnXnzamQEjsNaIleGa5A2jXg0bOCelclv
         lIDfvKUJoGAScdaWKrnw1NEHqa8zFCnv2EX4ezwh7DyKpdG8GE88PUnKH9xLmrzG6qGx
         FFug==
X-Gm-Message-State: ALoCoQnoTmnqkig0jorQG5RkJiFNpfHxencqx8ydAvfvjpD6r3Rek9du3skkD7lIU89OTKIBaiSU
X-Received: by 10.58.211.229 with SMTP id nf5mr26823030vec.19.1400441502527;
        Sun, 18 May 2014 12:31:42 -0700 (PDT)
Received: from mail-vc0-f175.google.com (mail-vc0-f175.google.com [209.85.220.175])
        by mx.google.com with ESMTPSA id c7sm17048931vei.16.2014.05.18.12.31.41
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 18 May 2014 12:31:41 -0700 (PDT)
Received: by mail-vc0-f175.google.com with SMTP id hu19so8448030vcb.6
        for <dev@spark.apache.org>; Sun, 18 May 2014 12:31:40 -0700 (PDT)
X-Received: by 10.58.188.115 with SMTP id fz19mr1827249vec.40.1400441500823;
 Sun, 18 May 2014 12:31:40 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Sun, 18 May 2014 12:31:20 -0700 (PDT)
In-Reply-To: <CAJmC80-0P6wfo5aQ7=-gfyF6h9J1AgyD29_qbzOoeNEt2d2RiA@mail.gmail.com>
References: <CAJmC80-0P6wfo5aQ7=-gfyF6h9J1AgyD29_qbzOoeNEt2d2RiA@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Sun, 18 May 2014 12:31:20 -0700
Message-ID: <CA+-p3AEqkN4twn1OrFCaF8=ZeFqwgcMWx_HeVYn3FVnvgn25TA@mail.gmail.com>
Subject: Re: Matrix Multiplication of two RDD[Array[Double]]'s
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e012945fa2f7aa204f9b1b23f
X-Virus-Checked: Checked by ClamAV on apache.org

--089e012945fa2f7aa204f9b1b23f
Content-Type: text/plain; charset=UTF-8

Hi Liquan,

There is some working being done on implementing linear algebra algorithms
on Spark for use in higher-level machine learning algorithms.  That work is
happening in the MLlib project, which has a
org.apache.spark.mllib.linalgpackage you may find useful.

See
https://github.com/apache/spark/tree/master/mllib/src/main/scala/org/apache/spark/mllib/linalg

>From my quick look (never read this code before and not familiar with
MLlib) both the IndexedRowMatrix and RowMatrix implement a multiply
operation:

aash@aash-mbp~/git/spark/mllib/src/main/scala/org/apache/spark/mllib/linalg$
git grep
'def multiply'
distributed/IndexedRowMatrix.scala:  def multiply(B: Matrix):
IndexedRowMatrix = {
distributed/RowMatrix.scala:  def multiply(B: Matrix): RowMatrix = {
aash@aash-mbp~/git/spark/mllib/src/main/scala/org/apache/spark/mllib/linalg$

Can you look into using that code and let us know if it meets your needs?

Thanks!
Andrew


On Sat, May 17, 2014 at 10:28 PM, Liquan Pei <liquanpei@gmail.com> wrote:

> Hi
>
> I am currently implementing an algorithm involving matrix multiplication.
> Basically, I have matrices represented as RDD[Array[Double]]. For example,
> If I have A:RDD[Array[Double]] and B:RDD[Array[Double]] and what would be
> the most efficient way to get C = A * B
>
> Both A and B are large, so it would not be possible to save either of them
> in memory.
>
> Thanks a lot for your help!
>
> Liquan
>

--089e012945fa2f7aa204f9b1b23f--

From dev-return-7672-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 20:26:05 2014
Return-Path: <dev-return-7672-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DF9F111F1E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 20:26:05 +0000 (UTC)
Received: (qmail 95368 invoked by uid 500); 18 May 2014 20:01:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62606 invoked by uid 500); 18 May 2014 19:36:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60958 invoked by uid 99); 18 May 2014 19:33:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 19:33:50 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.48 as permitted sender)
Received: from [209.85.160.48] (HELO mail-pb0-f48.google.com) (209.85.160.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 19:33:47 +0000
Received: by mail-pb0-f48.google.com with SMTP id rr13so4821750pbb.7
        for <dev@spark.apache.org>; Sun, 18 May 2014 12:33:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=LVjLxu8up3bEE98oZ8P4tHgBadwL8lpWsEj35ysyZuI=;
        b=CY2550/XmW4OOS8TY/0JlvBie8rHl5SehlvXD0e3b7CQViym0luVnE17ikk6/AKyET
         zgnMw177cluAwFuMkfFko1fQPqvICHqpHyYXOC7CWCezyt3oxToI6D7pDTq5HMcjZbUP
         ZXzTrecGxLZhr8hvqsEjRAEllbIB3eiKHJfV0cscT9OytAGITXc3A+kbI7DHnqehWrDx
         5I5TcXKjcGHkuv3ZLQSqrrRHImNN1/0JvVwjxynuNaAFQCC5y3c7HA33jxCEHMlIrtK1
         +a7X2nVWvwsAEqKgfT8HZlxi8pTg4gRgNVzL7UJcHs350z5g2DenqKvtCVBGO0aJPmk0
         WetQ==
X-Received: by 10.68.229.68 with SMTP id so4mr36808905pbc.110.1400441603083;
        Sun, 18 May 2014 12:33:23 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id as12sm65059386pac.43.2014.05.18.12.33.19
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 18 May 2014 12:33:20 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc9)
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CACBYxKLR3xE4FSqZQwrjmP+bhhqy_mAat0hHSu+1pOBEVTttrA@mail.gmail.com>
Date: Sun, 18 May 2014 12:33:18 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <9849C2AA-3131-4E31-85DF-7F6D61632545@gmail.com>
References: <CABPQxstaxcbGSQTqUxSSUQfRpjQs=BqRkrEVvzOXbXZo8kzShg@mail.gmail.com> <CABPQxstbwN64rQvx6kUoESkO4GJOohYSAO+s_F85wFWjb6T1zw@mail.gmail.com> <CAAsvFPn2RwpFMz7ajy=t8GSjiKSPqARgDtPqXk007Cd7_xvA-Q@mail.gmail.com> <CAMJOb8nTR38Uf50xVxv5O+OrBkO0ELDFYiHrziBVm9qknAgpYA@mail.gmail.com> <CACBYxKLR3xE4FSqZQwrjmP+bhhqy_mAat0hHSu+1pOBEVTttrA@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

I took the always fun task of testing it on Windows, and unfortunately, =
I found some small problems with the prebuilt packages due to recent =
changes to the launch scripts: bin/spark-class2.cmd looks in ./jars =
instead of ./lib for the assembly JAR, and bin/run-example2.cmd doesn=92t =
quite match the master-setting behavior of the Unix based one. I=92ll =
send a pull request to fix them soon.

Matei


On May 17, 2014, at 11:32 AM, Sandy Ryza <sandy.ryza@cloudera.com> =
wrote:

> +1
>=20
> Reran my tests from rc5:
>=20
> * Built the release from source.
> * Compiled Java and Scala apps that interact with HDFS against it.
> * Ran them in local mode.
> * Ran them against a pseudo-distributed YARN cluster in both =
yarn-client
> mode and yarn-cluster mode.
>=20
>=20
> On Sat, May 17, 2014 at 10:08 AM, Andrew Or <andrew@databricks.com> =
wrote:
>=20
>> +1
>>=20
>>=20
>> 2014-05-17 8:53 GMT-07:00 Mark Hamstra <mark@clearstorydata.com>:
>>=20
>>> +1
>>>=20
>>>=20
>>> On Sat, May 17, 2014 at 12:58 AM, Patrick Wendell =
<pwendell@gmail.com
>>>> wrote:
>>>=20
>>>> I'll start the voting with a +1.
>>>>=20
>>>> On Sat, May 17, 2014 at 12:58 AM, Patrick Wendell =
<pwendell@gmail.com>
>>>> wrote:
>>>>> Please vote on releasing the following candidate as Apache Spark
>>> version
>>>> 1.0.0!
>>>>> This has one bug fix and one minor feature on top of rc8:
>>>>> SPARK-1864: https://github.com/apache/spark/pull/808
>>>>> SPARK-1808: https://github.com/apache/spark/pull/799
>>>>>=20
>>>>> The tag to be voted on is v1.0.0-rc9 (commit 920f947):
>>>>>=20
>>>>=20
>>>=20
>> =
https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D920f9=
47eb5a22a679c0c3186cf69ee75f6041c75
>>>>>=20
>>>>> The release files, including signatures, digests, etc. can be =
found
>> at:
>>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc9/
>>>>>=20
>>>>> Release artifacts are signed with the following key:
>>>>> https://people.apache.org/keys/committer/pwendell.asc
>>>>>=20
>>>>> The staging repository for this release can be found at:
>>>>>=20
>>> =
https://repository.apache.org/content/repositories/orgapachespark-1017/
>>>>>=20
>>>>> The documentation corresponding to this release can be found at:
>>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc9-docs/
>>>>>=20
>>>>> Please vote on releasing this package as Apache Spark 1.0.0!
>>>>>=20
>>>>> The vote is open until Tuesday, May 20, at 08:56 UTC and passes if
>>>>> amajority of at least 3 +1 PMC votes are cast.
>>>>>=20
>>>>> [ ] +1 Release this package as Apache Spark 1.0.0
>>>>> [ ] -1 Do not release this package because ...
>>>>>=20
>>>>> To learn more about Apache Spark, please see
>>>>> http://spark.apache.org/
>>>>>=20
>>>>> =3D=3D API Changes =3D=3D
>>>>> We welcome users to compile Spark applications against 1.0. There =
are
>>>>> a few API changes in this release. Here are links to the =
associated
>>>>> upgrade guides - user facing changes have been kept as small as
>>>>> possible.
>>>>>=20
>>>>> changes to ML vector specification:
>>>>>=20
>>>>=20
>>>=20
>> =
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#f=
rom-09-to-10
>>>>>=20
>>>>> changes to the Java API:
>>>>>=20
>>>>=20
>>>=20
>> =
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programming-g=
uide.html#upgrading-from-pre-10-versions-of-spark
>>>>>=20
>>>>> changes to the streaming API:
>>>>>=20
>>>>=20
>>>=20
>> =
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-programm=
ing-guide.html#migration-guide-from-091-or-below-to-1x
>>>>>=20
>>>>> changes to the GraphX API:
>>>>>=20
>>>>=20
>>>=20
>> =
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming=
-guide.html#upgrade-guide-from-spark-091
>>>>>=20
>>>>> coGroup and related functions now return Iterable[T] instead of
>> Seq[T]
>>>>> =3D=3D> Call toSeq on the result to restore the old behavior
>>>>>=20
>>>>> SparkContext.jarOfClass returns Option[String] instead of =
Seq[String]
>>>>> =3D=3D> Call toSeq on the result to restore old behavior
>>>>=20
>>>=20
>>=20


From dev-return-7674-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 21:29:05 2014
Return-Path: <dev-return-7674-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EA2E111045
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 21:29:05 +0000 (UTC)
Received: (qmail 69874 invoked by uid 500); 18 May 2014 20:49:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36686 invoked by uid 500); 18 May 2014 20:26:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32759 invoked by uid 99); 18 May 2014 20:19:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 20:19:59 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 74.125.82.182 as permitted sender)
Received: from [74.125.82.182] (HELO mail-we0-f182.google.com) (74.125.82.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 20:19:56 +0000
Received: by mail-we0-f182.google.com with SMTP id t60so4718387wes.27
        for <dev@spark.apache.org>; Sun, 18 May 2014 13:19:35 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=dMZpNvlNb5qfsjVlPvQpWzEN+nia+xhSRT0hLClZ020=;
        b=v9JskHCmnsTUpQe8k5iRW6bsRSsPlOWYOmEKzZsMdU0iNK0hdu+zKo4rM5IsybPuus
         syYOBLVDmTJh7z1IlY1ifeUAVGfED0rMagitBiSxED6aQQ2nK50XXekjCGyVu/GO7ye4
         D49OBdpAtoVk/xkeirQx6bzq8GiiZuVaqv2qIlgDe+HC4RiNrQ8s+KcUsE1p5GOFVaL/
         2cTJdVoxiblJ+h6HvO93Gw9CRx5gCDXOf372VLpWNjKpqhO1WZElCKFWm+8lA9tV/DzF
         qL/27XEfoYBlBNzWMBS4sTq67GpbAWH2uqsdafMjYqpQ1iYljaelekHgPUY9ijTNJLRy
         6kMg==
MIME-Version: 1.0
X-Received: by 10.180.77.225 with SMTP id v1mr9090851wiw.5.1400444375073; Sun,
 18 May 2014 13:19:35 -0700 (PDT)
Received: by 10.194.187.20 with HTTP; Sun, 18 May 2014 13:19:35 -0700 (PDT)
In-Reply-To: <CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
Date: Sun, 18 May 2014 13:19:35 -0700
Message-ID: <CAJgQjQ-EmoPcWa-eJQQQUPZ9Pnf3sQQj+PNaVc2oUB3ctqQ5WA@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Patrick,

If spark-submit works correctly, user only needs to specify runtime
jars via `--jars` instead of using `sc.addJar`. Is it correct? I
checked SparkSubmit and yarn.Client but didn't find any code to handle
`args.jars` for YARN mode. So I don't know where in the code the jars
in the distributed cache are added to runtime classpath on executors.

Best,
Xiangrui

On Sun, May 18, 2014 at 11:58 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> @db - it's possible that you aren't including the jar in the classpath
> of your driver program (I think this is what mridul was suggesting).
> It would be helpful to see the stack trace of the CNFE.
>
> - Patrick
>
> On Sun, May 18, 2014 at 11:54 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>> @xiangrui - we don't expect these to be present on the system
>> classpath, because they get dynamically added by Spark (e.g. your
>> application can call sc.addJar well after the JVM's have started).
>>
>> @db - I'm pretty surprised to see that behavior. It's definitely not
>> intended that users need reflection to instantiate their classes -
>> something odd is going on in your case. If you could create an
>> isolated example and post it to the JIRA, that would be great.
>>
>> On Sun, May 18, 2014 at 9:58 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>> I created a JIRA: https://issues.apache.org/jira/browse/SPARK-1870
>>>
>>> DB, could you add more info to that JIRA? Thanks!
>>>
>>> -Xiangrui
>>>
>>> On Sun, May 18, 2014 at 9:46 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>>> Btw, I tried
>>>>
>>>> rdd.map { i =>
>>>>   System.getProperty("java.class.path")
>>>> }.collect()
>>>>
>>>> but didn't see the jars added via "--jars" on the executor classpath.
>>>>
>>>> -Xiangrui
>>>>
>>>> On Sat, May 17, 2014 at 11:26 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>>>> I can re-produce the error with Spark 1.0-RC and YARN (CDH-5). The
>>>>> reflection approach mentioned by DB didn't work either. I checked the
>>>>> distributed cache on a worker node and found the jar there. It is also
>>>>> in the Environment tab of the WebUI. The workaround is making an
>>>>> assembly jar.
>>>>>
>>>>> DB, could you create a JIRA and describe what you have found so far? Thanks!
>>>>>
>>>>> Best,
>>>>> Xiangrui
>>>>>
>>>>> On Sat, May 17, 2014 at 1:29 AM, Mridul Muralidharan <mridul@gmail.com> wrote:
>>>>>> Can you try moving your mapPartitions to another class/object which is
>>>>>> referenced only after sc.addJar ?
>>>>>>
>>>>>> I would suspect CNFEx is coming while loading the class containing
>>>>>> mapPartitions before addJars is executed.
>>>>>>
>>>>>> In general though, dynamic loading of classes means you use reflection to
>>>>>> instantiate it since expectation is you don't know which implementation
>>>>>> provides the interface ... If you statically know it apriori, you bundle it
>>>>>> in your classpath.
>>>>>>
>>>>>> Regards
>>>>>> Mridul
>>>>>> On 17-May-2014 7:28 am, "DB Tsai" <dbtsai@stanford.edu> wrote:
>>>>>>
>>>>>>> Finally find a way out of the ClassLoader maze! It took me some times to
>>>>>>> understand how it works; I think it worths to document it in a separated
>>>>>>> thread.
>>>>>>>
>>>>>>> We're trying to add external utility.jar which contains CSVRecordParser,
>>>>>>> and we added the jar to executors through sc.addJar APIs.
>>>>>>>
>>>>>>> If the instance of CSVRecordParser is created without reflection, it
>>>>>>> raises *ClassNotFound
>>>>>>> Exception*.
>>>>>>>
>>>>>>> data.mapPartitions(lines => {
>>>>>>>     val csvParser = new CSVRecordParser((delimiter.charAt(0))
>>>>>>>     lines.foreach(line => {
>>>>>>>       val lineElems = csvParser.parseLine(line)
>>>>>>>     })
>>>>>>>     ...
>>>>>>>     ...
>>>>>>>  )
>>>>>>>
>>>>>>>
>>>>>>> If the instance of CSVRecordParser is created through reflection, it works.
>>>>>>>
>>>>>>> data.mapPartitions(lines => {
>>>>>>>     val loader = Thread.currentThread.getContextClassLoader
>>>>>>>     val CSVRecordParser =
>>>>>>>         loader.loadClass("com.alpine.hadoop.ext.CSVRecordParser")
>>>>>>>
>>>>>>>     val csvParser = CSVRecordParser.getConstructor(Character.TYPE)
>>>>>>>         .newInstance(delimiter.charAt(0).asInstanceOf[Character])
>>>>>>>
>>>>>>>     val parseLine = CSVRecordParser
>>>>>>>         .getDeclaredMethod("parseLine", classOf[String])
>>>>>>>
>>>>>>>     lines.foreach(line => {
>>>>>>>        val lineElems = parseLine.invoke(csvParser,
>>>>>>> line).asInstanceOf[Array[String]]
>>>>>>>     })
>>>>>>>     ...
>>>>>>>     ...
>>>>>>>  )
>>>>>>>
>>>>>>>
>>>>>>> This is identical to this question,
>>>>>>>
>>>>>>> http://stackoverflow.com/questions/7452411/thread-currentthread-setcontextclassloader-without-using-reflection
>>>>>>>
>>>>>>> It's not intuitive for users to load external classes through reflection,
>>>>>>> but couple available solutions including 1) messing around
>>>>>>> systemClassLoader by calling systemClassLoader.addURI through reflection or
>>>>>>> 2) forking another JVM to add jars into classpath before bootstrap loader
>>>>>>> are very tricky.
>>>>>>>
>>>>>>> Any thought on fixing it properly?
>>>>>>>
>>>>>>> @Xiangrui,
>>>>>>> netlib-java jniloader is loaded from netlib-java through reflection, so
>>>>>>> this problem will not be seen.
>>>>>>>
>>>>>>> Sincerely,
>>>>>>>
>>>>>>> DB Tsai
>>>>>>> -------------------------------------------------------
>>>>>>> My Blog: https://www.dbtsai.com
>>>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>>>>>

From dev-return-7676-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 22:54:37 2014
Return-Path: <dev-return-7676-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1BA7011307
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 22:54:37 +0000 (UTC)
Received: (qmail 17024 invoked by uid 500); 18 May 2014 21:39:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16665 invoked by uid 500); 18 May 2014 21:38:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16375 invoked by uid 99); 18 May 2014 21:37:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 21:37:40 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.49 as permitted sender)
Received: from [209.85.160.49] (HELO mail-pb0-f49.google.com) (209.85.160.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 21:37:05 +0000
Received: by mail-pb0-f49.google.com with SMTP id jt11so4875684pbb.8
        for <dev@spark.apache.org>; Sun, 18 May 2014 14:36:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=r3OQSHgJELSmZdLIYzURtnUSeYcrWphIoAx53ldFDe0=;
        b=WqrBbO2mcpA7H7C4Xg2DhzomLj25McYJFJpydqLnU6s2vhIf6SL2VRzk0oLdRJjv2e
         TT0tot/Z3JKCMT4RV4Kv6La9TIGRgdZjdzg7lfSLjzJXsDtM1m94anEt9cVS1/scefbI
         4421NWuek++G3zrWE9LbvE9TDorhy6hxKGNHZ84MOysmI6HNaQgPlT+ZUEYol0jAtBgB
         +mIqNpFSn+TshJjqoA6eFlxd35ZCN+y3i6S2nefmlduP2jy7HTQg3EElZKeROrJc+HZR
         3k7ggxb54/LYYwCHDlMFLCS0ojFXS1H2TAaXf5BVoyXXxwnKfslkZeHPN4+/PeWKKKkq
         y8UA==
X-Received: by 10.68.181.165 with SMTP id dx5mr38337213pbc.38.1400449002014;
        Sun, 18 May 2014 14:36:42 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id io8sm26279612pbc.96.2014.05.18.14.36.39
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 18 May 2014 14:36:40 -0700 (PDT)
Content-Type: text/plain; charset=utf-8
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CA+-p3AFtOm_f6QopygGkA0jGAOvec1FXOEwrK+d238cTrHKW2w@mail.gmail.com>
Date: Sun, 18 May 2014 14:36:38 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <6FC3E0D4-61E8-4BD8-B9C8-37EE8639E7B5@gmail.com>
References: <JIRA.12714626.1400189782796@arcas> <JIRA.12714626.1400189782796.363092.1400238099511@arcas> <CAJiQeY+zsu3cfX1zyA=qK1KO8hirddmjGSB7HohTTA_a9jk3wQ@mail.gmail.com> <CAJgQjQ_1qrz4=05E=Wwj+O4UqKHwFKWQQiZosKbPjvwocMz9mw@mail.gmail.com> <CAJiQeY+eGxSzM-6C_poKCY53Gkixg85rdXiXXx+qi0z0bvCrrQ@mail.gmail.com> <6EF34F88-F79D-49B0-92FD-C421FBB0B6ED@gmail.com> <CACo38_QBRwZkxRwqbja_CTdPYy9ysY2b=ewntcq_5A-LVs9O0w@mail.gmail.com> <CA+-p3AFtOm_f6QopygGkA0jGAOvec1FXOEwrK+d238cTrHKW2w@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

JIRAs comments are mirrored to the issues@spark.apache.org list, so =
people who want to get them by email can do so. In theory one should =
also be able to reply to one of those emails and have the message show =
up in JIRA, but I don=E2=80=99t think ours is configured that way. I=E2=80=
=99m not sure why it wouldn=E2=80=99t be the =E2=80=9CASF way=E2=80=9D =
when the JIRA instance is hosted by the ASF and mirrored on ASF lists.

Matei

On May 18, 2014, at 11:28 AM, Andrew Ash <andrew@andrewash.com> wrote:

> The nice thing about putting discussion on the Jira is that everything
> about the bug is in one place.  So people looking to understand the
> discussion a few years from now only have to look on the jira ticket =
rather
> than also search the mailing list archives and hope commenters all put =
the
> string "SPARK-1855" into the messages.
>=20
>=20
> On Sun, May 18, 2014 at 10:34 AM, Jacek Laskowski <jacek@japila.pl> =
wrote:
>=20
>> Hi,
>>=20
>> I'm curious if it's a common approach to have discussions in JIRA not =
here.
>> I don't think it's the ASF way.
>>=20
>> Pozdrawiam,
>> Jacek Laskowski
>> http://blog.japila.pl
>> 17 maj 2014 23:55 "Matei Zaharia" <matei.zaharia@gmail.com> =
napisa=C5=82(a):
>>=20
>>> We do actually have replicated StorageLevels in Spark. You can use
>>> MEMORY_AND_DISK_2 or construct your own StorageLevel with your own =
custom
>>> replication factor.
>>>=20
>>> BTW you guys should probably have this discussion on the JIRA rather =
than
>>> the dev list; I think the replies somehow ended up on the dev list.
>>>=20
>>> Matei
>>>=20
>>> On May 17, 2014, at 1:36 AM, Mridul Muralidharan <mridul@gmail.com>
>> wrote:
>>>=20
>>>> We don't have 3x replication in spark :-)
>>>> And if we use replicated storagelevel, while decreasing odds of
>> failure,
>>> it
>>>> does not eliminate it (since we are not doing a great job with
>>> replication
>>>> anyway from fault tolerance point of view).
>>>> Also it does take a nontrivial performance hit with replicated =
levels.
>>>>=20
>>>> Regards,
>>>> Mridul
>>>> On 17-May-2014 8:16 am, "Xiangrui Meng" <mengxr@gmail.com> wrote:
>>>>=20
>>>>> With 3x replication, we should be able to achieve fault tolerance.
>>>>> This checkPointed RDD can be cleared if we have another in-memory
>>>>> checkPointed RDD down the line. It can avoid hitting disk if we =
have
>>>>> enough memory to use. We need to investigate more to find a good
>>>>> solution. -Xiangrui
>>>>>=20
>>>>> On Fri, May 16, 2014 at 4:00 PM, Mridul Muralidharan <
>> mridul@gmail.com>
>>>>> wrote:
>>>>>> Effectively this is persist without fault tolerance.
>>>>>> Failure of any node means complete lack of fault tolerance.
>>>>>> I would be very skeptical of truncating lineage if it is not
>> reliable.
>>>>>> On 17-May-2014 3:49 am, "Xiangrui Meng (JIRA)" <jira@apache.org>
>>> wrote:
>>>>>>=20
>>>>>>> Xiangrui Meng created SPARK-1855:
>>>>>>> ------------------------------------
>>>>>>>=20
>>>>>>>            Summary: Provide memory-and-local-disk RDD =
checkpointing
>>>>>>>                Key: SPARK-1855
>>>>>>>                URL:
>> https://issues.apache.org/jira/browse/SPARK-1855
>>>>>>>            Project: Spark
>>>>>>>         Issue Type: New Feature
>>>>>>>         Components: MLlib, Spark Core
>>>>>>>   Affects Versions: 1.0.0
>>>>>>>           Reporter: Xiangrui Meng
>>>>>>>=20
>>>>>>>=20
>>>>>>> Checkpointing is used to cut long lineage while maintaining =
fault
>>>>>>> tolerance. The current implementation is HDFS-based. Using the
>>> BlockRDD
>>>>> we
>>>>>>> can create in-memory-and-local-disk (with replication) =
checkpoints
>>> that
>>>>> are
>>>>>>> not as reliable as HDFS-based solution but faster.
>>>>>>>=20
>>>>>>> It can help applications that require many iterations.
>>>>>>>=20
>>>>>>>=20
>>>>>>>=20
>>>>>>> --
>>>>>>> This message was sent by Atlassian JIRA
>>>>>>> (v6.2#6252)
>>>>>>>=20
>>>>>=20
>>>=20
>>>=20
>>=20


From dev-return-7675-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 22:58:43 2014
Return-Path: <dev-return-7675-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0026111346
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 22:58:42 +0000 (UTC)
Received: (qmail 99172 invoked by uid 500); 18 May 2014 21:19:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99140 invoked by uid 500); 18 May 2014 21:19:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99071 invoked by uid 99); 18 May 2014 21:19:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 21:19:45 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [85.128.177.112] (HELO alu112.rev.netart.pl) (85.128.177.112)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 21:19:42 +0000
Received: from mail-yh0-f41.google.com (unknown [209.85.213.41])
	by laskowski.nazwa.pl (Postfix) with ESMTP id 2D79230CFCF2
	for <dev@spark.apache.org>; Sun, 18 May 2014 23:19:19 +0200 (CEST)
Received: by mail-yh0-f41.google.com with SMTP id f73so6204806yha.0
        for <dev@spark.apache.org>; Sun, 18 May 2014 14:19:17 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.236.30.105 with SMTP id j69mr23791542yha.19.1400447957953;
 Sun, 18 May 2014 14:19:17 -0700 (PDT)
Received: by 10.170.56.7 with HTTP; Sun, 18 May 2014 14:19:17 -0700 (PDT)
In-Reply-To: <CA+-p3AFtOm_f6QopygGkA0jGAOvec1FXOEwrK+d238cTrHKW2w@mail.gmail.com>
References: <JIRA.12714626.1400189782796@arcas>
	<JIRA.12714626.1400189782796.363092.1400238099511@arcas>
	<CAJiQeY+zsu3cfX1zyA=qK1KO8hirddmjGSB7HohTTA_a9jk3wQ@mail.gmail.com>
	<CAJgQjQ_1qrz4=05E=Wwj+O4UqKHwFKWQQiZosKbPjvwocMz9mw@mail.gmail.com>
	<CAJiQeY+eGxSzM-6C_poKCY53Gkixg85rdXiXXx+qi0z0bvCrrQ@mail.gmail.com>
	<6EF34F88-F79D-49B0-92FD-C421FBB0B6ED@gmail.com>
	<CACo38_QBRwZkxRwqbja_CTdPYy9ysY2b=ewntcq_5A-LVs9O0w@mail.gmail.com>
	<CA+-p3AFtOm_f6QopygGkA0jGAOvec1FXOEwrK+d238cTrHKW2w@mail.gmail.com>
Date: Sun, 18 May 2014 23:19:17 +0200
Message-ID: <CACo38_QrretkNzR6NoXeBw9E8jnaETvGGCiUt9kfA+O=mSxicw@mail.gmail.com>
Subject: Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing
From: Jacek Laskowski <jacek@japila.pl>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Sun, May 18, 2014 at 8:28 PM, Andrew Ash <andrew@andrewash.com> wrote:
> The nice thing about putting discussion on the Jira is that everything
> about the bug is in one place.  So people looking to understand the
> discussion a few years from now only have to look on the jira ticket rather
> than also search the mailing list archives and hope commenters all put the
> string "SPARK-1855" into the messages.

My understanding is that JIRA is not for discussions. In a sense it
could be used for a few opinions, but have never seen it elsewhere and
am curious if it's an approach for the project (that I might accept
ultimately, but that would require some adoption time).

What wrong with linking a discussion thread to a JIRA issue?

Jacek

-- 
Jacek Laskowski | http://blog.japila.pl
"Never discourage anyone who continually makes progress, no matter how
slow." Plato

From dev-return-7677-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 23:01:13 2014
Return-Path: <dev-return-7677-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 69321113A5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 23:01:13 +0000 (UTC)
Received: (qmail 76017 invoked by uid 500); 18 May 2014 23:01:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75954 invoked by uid 500); 18 May 2014 23:01:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75946 invoked by uid 99); 18 May 2014 23:01:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 23:01:13 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.175 as permitted sender)
Received: from [209.85.216.175] (HELO mail-qc0-f175.google.com) (209.85.216.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 23:01:09 +0000
Received: by mail-qc0-f175.google.com with SMTP id w7so7847582qcr.34
        for <dev@spark.apache.org>; Sun, 18 May 2014 16:00:48 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=ATdEPhfsQsVOeoTYIK3pA8cTrVzpnFs5E7xM2MH9jSo=;
        b=k3s1cPIk0S7weWHDrGWKlaHjaQdhtrH63gTgEOK4woSOtJYxf1i2b6sdX1ooc7C5Jn
         a5nr2Tfbw1QdGZ8SoY8dZ8ps0/Dh5Gw1hMww7E4tZeCqWGs6YxPEM7AWy1jWtCiWf0hd
         Hh/6rtR2/pbdbdgBsRxx7TyWO9Id7ovLavHCLLHBL4YNL7GY4cHKwdi6jEGeZphYq/8F
         X+TGXtk9Zy310+2b6Sw3vBSTJCiiC8Xp2ZIt0voAPIl9uQygyRCBqQmm/wTplrk6HlQE
         ucpnhYSEr6MZUVa56XY9O0LMBTbWonQD3Vcrf+8Ph7T36KyjKnarr1s89d0vxy31G1wM
         bbYQ==
X-Gm-Message-State: ALoCoQnLCIG69hplFhaoshIwmt13k9VAbVGdvCiuQx/jrA+3tHtKn6f7sUTEiSdVN8mOprooiHui
MIME-Version: 1.0
X-Received: by 10.224.53.194 with SMTP id n2mr41501981qag.48.1400454048427;
 Sun, 18 May 2014 16:00:48 -0700 (PDT)
Received: by 10.140.106.202 with HTTP; Sun, 18 May 2014 16:00:48 -0700 (PDT)
In-Reply-To: <CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
Date: Sun, 18 May 2014 16:00:48 -0700
Message-ID: <CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1132f74414f55204f9b49ea6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1132f74414f55204f9b49ea6
Content-Type: text/plain; charset=UTF-8

I spoke with DB offline about this a little while ago and he confirmed that
he was able to access the jar from the driver.

The issue appears to be a general Java issue: you can't directly
instantiate a class from a dynamically loaded jar.

I reproduced it locally outside of Spark with:
---
    URLClassLoader urlClassLoader = new URLClassLoader(new URL[] { new
File("myotherjar.jar").toURI().toURL() }, null);
    Thread.currentThread().setContextClassLoader(urlClassLoader);
    MyClassFromMyOtherJar obj = new MyClassFromMyOtherJar();
---

I was able to load the class with reflection.



On Sun, May 18, 2014 at 11:58 AM, Patrick Wendell <pwendell@gmail.com>wrote:

> @db - it's possible that you aren't including the jar in the classpath
> of your driver program (I think this is what mridul was suggesting).
> It would be helpful to see the stack trace of the CNFE.
>
> - Patrick
>
> On Sun, May 18, 2014 at 11:54 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > @xiangrui - we don't expect these to be present on the system
> > classpath, because they get dynamically added by Spark (e.g. your
> > application can call sc.addJar well after the JVM's have started).
> >
> > @db - I'm pretty surprised to see that behavior. It's definitely not
> > intended that users need reflection to instantiate their classes -
> > something odd is going on in your case. If you could create an
> > isolated example and post it to the JIRA, that would be great.
> >
> > On Sun, May 18, 2014 at 9:58 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
> >> I created a JIRA: https://issues.apache.org/jira/browse/SPARK-1870
> >>
> >> DB, could you add more info to that JIRA? Thanks!
> >>
> >> -Xiangrui
> >>
> >> On Sun, May 18, 2014 at 9:46 AM, Xiangrui Meng <mengxr@gmail.com>
> wrote:
> >>> Btw, I tried
> >>>
> >>> rdd.map { i =>
> >>>   System.getProperty("java.class.path")
> >>> }.collect()
> >>>
> >>> but didn't see the jars added via "--jars" on the executor classpath.
> >>>
> >>> -Xiangrui
> >>>
> >>> On Sat, May 17, 2014 at 11:26 PM, Xiangrui Meng <mengxr@gmail.com>
> wrote:
> >>>> I can re-produce the error with Spark 1.0-RC and YARN (CDH-5). The
> >>>> reflection approach mentioned by DB didn't work either. I checked the
> >>>> distributed cache on a worker node and found the jar there. It is also
> >>>> in the Environment tab of the WebUI. The workaround is making an
> >>>> assembly jar.
> >>>>
> >>>> DB, could you create a JIRA and describe what you have found so far?
> Thanks!
> >>>>
> >>>> Best,
> >>>> Xiangrui
> >>>>
> >>>> On Sat, May 17, 2014 at 1:29 AM, Mridul Muralidharan <
> mridul@gmail.com> wrote:
> >>>>> Can you try moving your mapPartitions to another class/object which
> is
> >>>>> referenced only after sc.addJar ?
> >>>>>
> >>>>> I would suspect CNFEx is coming while loading the class containing
> >>>>> mapPartitions before addJars is executed.
> >>>>>
> >>>>> In general though, dynamic loading of classes means you use
> reflection to
> >>>>> instantiate it since expectation is you don't know which
> implementation
> >>>>> provides the interface ... If you statically know it apriori, you
> bundle it
> >>>>> in your classpath.
> >>>>>
> >>>>> Regards
> >>>>> Mridul
> >>>>> On 17-May-2014 7:28 am, "DB Tsai" <dbtsai@stanford.edu> wrote:
> >>>>>
> >>>>>> Finally find a way out of the ClassLoader maze! It took me some
> times to
> >>>>>> understand how it works; I think it worths to document it in a
> separated
> >>>>>> thread.
> >>>>>>
> >>>>>> We're trying to add external utility.jar which contains
> CSVRecordParser,
> >>>>>> and we added the jar to executors through sc.addJar APIs.
> >>>>>>
> >>>>>> If the instance of CSVRecordParser is created without reflection, it
> >>>>>> raises *ClassNotFound
> >>>>>> Exception*.
> >>>>>>
> >>>>>> data.mapPartitions(lines => {
> >>>>>>     val csvParser = new CSVRecordParser((delimiter.charAt(0))
> >>>>>>     lines.foreach(line => {
> >>>>>>       val lineElems = csvParser.parseLine(line)
> >>>>>>     })
> >>>>>>     ...
> >>>>>>     ...
> >>>>>>  )
> >>>>>>
> >>>>>>
> >>>>>> If the instance of CSVRecordParser is created through reflection,
> it works.
> >>>>>>
> >>>>>> data.mapPartitions(lines => {
> >>>>>>     val loader = Thread.currentThread.getContextClassLoader
> >>>>>>     val CSVRecordParser =
> >>>>>>         loader.loadClass("com.alpine.hadoop.ext.CSVRecordParser")
> >>>>>>
> >>>>>>     val csvParser = CSVRecordParser.getConstructor(Character.TYPE)
> >>>>>>         .newInstance(delimiter.charAt(0).asInstanceOf[Character])
> >>>>>>
> >>>>>>     val parseLine = CSVRecordParser
> >>>>>>         .getDeclaredMethod("parseLine", classOf[String])
> >>>>>>
> >>>>>>     lines.foreach(line => {
> >>>>>>        val lineElems = parseLine.invoke(csvParser,
> >>>>>> line).asInstanceOf[Array[String]]
> >>>>>>     })
> >>>>>>     ...
> >>>>>>     ...
> >>>>>>  )
> >>>>>>
> >>>>>>
> >>>>>> This is identical to this question,
> >>>>>>
> >>>>>>
> http://stackoverflow.com/questions/7452411/thread-currentthread-setcontextclassloader-without-using-reflection
> >>>>>>
> >>>>>> It's not intuitive for users to load external classes through
> reflection,
> >>>>>> but couple available solutions including 1) messing around
> >>>>>> systemClassLoader by calling systemClassLoader.addURI through
> reflection or
> >>>>>> 2) forking another JVM to add jars into classpath before bootstrap
> loader
> >>>>>> are very tricky.
> >>>>>>
> >>>>>> Any thought on fixing it properly?
> >>>>>>
> >>>>>> @Xiangrui,
> >>>>>> netlib-java jniloader is loaded from netlib-java through
> reflection, so
> >>>>>> this problem will not be seen.
> >>>>>>
> >>>>>> Sincerely,
> >>>>>>
> >>>>>> DB Tsai
> >>>>>> -------------------------------------------------------
> >>>>>> My Blog: https://www.dbtsai.com
> >>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
> >>>>>>
>

--001a1132f74414f55204f9b49ea6--

From dev-return-7678-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 23:01:50 2014
Return-Path: <dev-return-7678-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2C483113B9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 23:01:50 +0000 (UTC)
Received: (qmail 84012 invoked by uid 500); 18 May 2014 23:01:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83951 invoked by uid 500); 18 May 2014 23:01:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83943 invoked by uid 99); 18 May 2014 23:01:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 23:01:49 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.53 as permitted sender)
Received: from [209.85.160.53] (HELO mail-pb0-f53.google.com) (209.85.160.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 23:01:46 +0000
Received: by mail-pb0-f53.google.com with SMTP id md12so4930680pbc.26
        for <dev@spark.apache.org>; Sun, 18 May 2014 16:01:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=A6KnlJLng85Gb/kVNLA/TWm0Mg9JnRAwtr/3pF1swis=;
        b=ABd0Sqq3/vbZCnqTpG+36pq8ZYzNGA0lA9+KAV98lLYcwE7Oc3Vjr+t7IfE0/YNK33
         HUy6RI3txpZSbFdVCD1phvKL64nVXv90B8RXWcOe2WBc+300lGLj7b2j0ob9gyW2H9qW
         eO67bH8Q7Fb6aTpBrrSJvWKHwjjQvUyWh5WL+MXmnIZuUeWRH4FsbWOLLs18Lkpoie5l
         fVYRfb45rARMwQWk5z9RfAkxwn2SFcbFAvRlfU34JmYGvaKMCWeQb0PVhCu5LyMHUNIF
         USzZ91Byj7O8jWYBJ4eHOVBDyryizzDBSvYN3Rtx0d4wyMvYKtMSZTxlpX7aqP2x9CJp
         EJvQ==
X-Received: by 10.68.229.68 with SMTP id so4mr37648052pbc.110.1400454082667;
        Sun, 18 May 2014 16:01:22 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id cz3sm26518289pbc.9.2014.05.18.16.01.18
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 18 May 2014 16:01:19 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CACo38_QrretkNzR6NoXeBw9E8jnaETvGGCiUt9kfA+O=mSxicw@mail.gmail.com>
Date: Sun, 18 May 2014 16:01:17 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <6AFDEA6E-E06C-4569-B92F-0A0CA8FACA3A@gmail.com>
References: <JIRA.12714626.1400189782796@arcas> <JIRA.12714626.1400189782796.363092.1400238099511@arcas> <CAJiQeY+zsu3cfX1zyA=qK1KO8hirddmjGSB7HohTTA_a9jk3wQ@mail.gmail.com> <CAJgQjQ_1qrz4=05E=Wwj+O4UqKHwFKWQQiZosKbPjvwocMz9mw@mail.gmail.com> <CAJiQeY+eGxSzM-6C_poKCY53Gkixg85rdXiXXx+qi0z0bvCrrQ@mail.gmail.com> <6EF34F88-F79D-49B0-92FD-C421FBB0B6ED@gmail.com> <CACo38_QBRwZkxRwqbja_CTdPYy9ysY2b=ewntcq_5A-LVs9O0w@mail.gmail.com> <CA+-p3AFtOm_f6QopygGkA0jGAOvec1FXOEwrK+d238cTrHKW2w@mail.gmail.com> <CACo38_QrretkNzR6NoXeBw9E8jnaETvGGCiUt9kfA+O=mSxicw@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

Ah, maybe it=92s just different in other Apache projects. All the ones =
I=92ve participated in have had their design discussions on JIRA. For =
example take a look at https://issues.apache.org/jira/browse/HDFS-4949. =
(Most design discussions in Hadoop are also on JIRA).

Hosting it this way is more convenient because most users come in =
looking at the issue tracker, not at mailing list archives (if only =
because the issue tracker is much more searchable for issues).

Matei

On May 18, 2014, at 2:19 PM, Jacek Laskowski <jacek@japila.pl> wrote:

> On Sun, May 18, 2014 at 8:28 PM, Andrew Ash <andrew@andrewash.com> =
wrote:
>> The nice thing about putting discussion on the Jira is that =
everything
>> about the bug is in one place.  So people looking to understand the
>> discussion a few years from now only have to look on the jira ticket =
rather
>> than also search the mailing list archives and hope commenters all =
put the
>> string "SPARK-1855" into the messages.
>=20
> My understanding is that JIRA is not for discussions. In a sense it
> could be used for a few opinions, but have never seen it elsewhere and
> am curious if it's an approach for the project (that I might accept
> ultimately, but that would require some adoption time).
>=20
> What wrong with linking a discussion thread to a JIRA issue?
>=20
> Jacek
>=20
> --=20
> Jacek Laskowski | http://blog.japila.pl
> "Never discourage anyone who continually makes progress, no matter how
> slow." Plato


From dev-return-7679-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 23:05:44 2014
Return-Path: <dev-return-7679-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 99BA71142F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 23:05:44 +0000 (UTC)
Received: (qmail 98314 invoked by uid 500); 18 May 2014 23:05:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98257 invoked by uid 500); 18 May 2014 23:05:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98249 invoked by uid 99); 18 May 2014 23:05:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 23:05:44 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 74.125.82.44 as permitted sender)
Received: from [74.125.82.44] (HELO mail-wg0-f44.google.com) (74.125.82.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 23:05:40 +0000
Received: by mail-wg0-f44.google.com with SMTP id a1so6983775wgh.15
        for <dev@spark.apache.org>; Sun, 18 May 2014 16:05:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=r7yjhBDpeG+FPP/Pl7/NNzlmQleCTiuUbHqLZAf+Scw=;
        b=TLPHFE4zZqhl9IdkDQnxNBkJNHelXmEfFjU+FChAbAH5NP0Yv37sBEw8ICHSeK8DWS
         bhW3ME6xlPQzZiBtPiuYCc7BRsZ6tnjoEkM+9Mu/26nM3qBmz+dIgmLgGovUxwcOBoXg
         V+1IdnUoPin/JxLYqOJSkSC8jQ3uRueU1fTEmd89+HFTReTRzi6tfz0EL9CBge/fEnt0
         lHdoOfXHFeR6oSyQcjB1/wCik0O7s63NPZjPUATsssLK33mE1V2Odkyi7wZ4A4fRHWXT
         lbPS0LK0L4UYu7wKb++z6vhinJCnK/6tPIKJPTiKIRZn4xRuo2xEWFyR70YMGOwrPWPV
         c/jg==
MIME-Version: 1.0
X-Received: by 10.194.7.232 with SMTP id m8mr26388727wja.47.1400454319496;
 Sun, 18 May 2014 16:05:19 -0700 (PDT)
Received: by 10.194.187.20 with HTTP; Sun, 18 May 2014 16:05:19 -0700 (PDT)
In-Reply-To: <CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
	<CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
Date: Sun, 18 May 2014 16:05:19 -0700
Message-ID: <CAJgQjQ_mdw+9+N8DoQDaZf5DgT_FCZrBqEEPPLbethwGFc5TcA@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Sandy,

It is hard to imagine that a user needs to create an object in that
way. Since the jars are already in distributed cache before the
executor starts, is there any reason we cannot add the locally cached
jars to classpath directly?

Best,
Xiangrui

On Sun, May 18, 2014 at 4:00 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
> I spoke with DB offline about this a little while ago and he confirmed that
> he was able to access the jar from the driver.
>
> The issue appears to be a general Java issue: you can't directly
> instantiate a class from a dynamically loaded jar.
>
> I reproduced it locally outside of Spark with:
> ---
>     URLClassLoader urlClassLoader = new URLClassLoader(new URL[] { new
> File("myotherjar.jar").toURI().toURL() }, null);
>     Thread.currentThread().setContextClassLoader(urlClassLoader);
>     MyClassFromMyOtherJar obj = new MyClassFromMyOtherJar();
> ---
>
> I was able to load the class with reflection.
>
>
>
> On Sun, May 18, 2014 at 11:58 AM, Patrick Wendell <pwendell@gmail.com>wrote:
>
>> @db - it's possible that you aren't including the jar in the classpath
>> of your driver program (I think this is what mridul was suggesting).
>> It would be helpful to see the stack trace of the CNFE.
>>
>> - Patrick
>>
>> On Sun, May 18, 2014 at 11:54 AM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>> > @xiangrui - we don't expect these to be present on the system
>> > classpath, because they get dynamically added by Spark (e.g. your
>> > application can call sc.addJar well after the JVM's have started).
>> >
>> > @db - I'm pretty surprised to see that behavior. It's definitely not
>> > intended that users need reflection to instantiate their classes -
>> > something odd is going on in your case. If you could create an
>> > isolated example and post it to the JIRA, that would be great.
>> >
>> > On Sun, May 18, 2014 at 9:58 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>> >> I created a JIRA: https://issues.apache.org/jira/browse/SPARK-1870
>> >>
>> >> DB, could you add more info to that JIRA? Thanks!
>> >>
>> >> -Xiangrui
>> >>
>> >> On Sun, May 18, 2014 at 9:46 AM, Xiangrui Meng <mengxr@gmail.com>
>> wrote:
>> >>> Btw, I tried
>> >>>
>> >>> rdd.map { i =>
>> >>>   System.getProperty("java.class.path")
>> >>> }.collect()
>> >>>
>> >>> but didn't see the jars added via "--jars" on the executor classpath.
>> >>>
>> >>> -Xiangrui
>> >>>
>> >>> On Sat, May 17, 2014 at 11:26 PM, Xiangrui Meng <mengxr@gmail.com>
>> wrote:
>> >>>> I can re-produce the error with Spark 1.0-RC and YARN (CDH-5). The
>> >>>> reflection approach mentioned by DB didn't work either. I checked the
>> >>>> distributed cache on a worker node and found the jar there. It is also
>> >>>> in the Environment tab of the WebUI. The workaround is making an
>> >>>> assembly jar.
>> >>>>
>> >>>> DB, could you create a JIRA and describe what you have found so far?
>> Thanks!
>> >>>>
>> >>>> Best,
>> >>>> Xiangrui
>> >>>>
>> >>>> On Sat, May 17, 2014 at 1:29 AM, Mridul Muralidharan <
>> mridul@gmail.com> wrote:
>> >>>>> Can you try moving your mapPartitions to another class/object which
>> is
>> >>>>> referenced only after sc.addJar ?
>> >>>>>
>> >>>>> I would suspect CNFEx is coming while loading the class containing
>> >>>>> mapPartitions before addJars is executed.
>> >>>>>
>> >>>>> In general though, dynamic loading of classes means you use
>> reflection to
>> >>>>> instantiate it since expectation is you don't know which
>> implementation
>> >>>>> provides the interface ... If you statically know it apriori, you
>> bundle it
>> >>>>> in your classpath.
>> >>>>>
>> >>>>> Regards
>> >>>>> Mridul
>> >>>>> On 17-May-2014 7:28 am, "DB Tsai" <dbtsai@stanford.edu> wrote:
>> >>>>>
>> >>>>>> Finally find a way out of the ClassLoader maze! It took me some
>> times to
>> >>>>>> understand how it works; I think it worths to document it in a
>> separated
>> >>>>>> thread.
>> >>>>>>
>> >>>>>> We're trying to add external utility.jar which contains
>> CSVRecordParser,
>> >>>>>> and we added the jar to executors through sc.addJar APIs.
>> >>>>>>
>> >>>>>> If the instance of CSVRecordParser is created without reflection, it
>> >>>>>> raises *ClassNotFound
>> >>>>>> Exception*.
>> >>>>>>
>> >>>>>> data.mapPartitions(lines => {
>> >>>>>>     val csvParser = new CSVRecordParser((delimiter.charAt(0))
>> >>>>>>     lines.foreach(line => {
>> >>>>>>       val lineElems = csvParser.parseLine(line)
>> >>>>>>     })
>> >>>>>>     ...
>> >>>>>>     ...
>> >>>>>>  )
>> >>>>>>
>> >>>>>>
>> >>>>>> If the instance of CSVRecordParser is created through reflection,
>> it works.
>> >>>>>>
>> >>>>>> data.mapPartitions(lines => {
>> >>>>>>     val loader = Thread.currentThread.getContextClassLoader
>> >>>>>>     val CSVRecordParser =
>> >>>>>>         loader.loadClass("com.alpine.hadoop.ext.CSVRecordParser")
>> >>>>>>
>> >>>>>>     val csvParser = CSVRecordParser.getConstructor(Character.TYPE)
>> >>>>>>         .newInstance(delimiter.charAt(0).asInstanceOf[Character])
>> >>>>>>
>> >>>>>>     val parseLine = CSVRecordParser
>> >>>>>>         .getDeclaredMethod("parseLine", classOf[String])
>> >>>>>>
>> >>>>>>     lines.foreach(line => {
>> >>>>>>        val lineElems = parseLine.invoke(csvParser,
>> >>>>>> line).asInstanceOf[Array[String]]
>> >>>>>>     })
>> >>>>>>     ...
>> >>>>>>     ...
>> >>>>>>  )
>> >>>>>>
>> >>>>>>
>> >>>>>> This is identical to this question,
>> >>>>>>
>> >>>>>>
>> http://stackoverflow.com/questions/7452411/thread-currentthread-setcontextclassloader-without-using-reflection
>> >>>>>>
>> >>>>>> It's not intuitive for users to load external classes through
>> reflection,
>> >>>>>> but couple available solutions including 1) messing around
>> >>>>>> systemClassLoader by calling systemClassLoader.addURI through
>> reflection or
>> >>>>>> 2) forking another JVM to add jars into classpath before bootstrap
>> loader
>> >>>>>> are very tricky.
>> >>>>>>
>> >>>>>> Any thought on fixing it properly?
>> >>>>>>
>> >>>>>> @Xiangrui,
>> >>>>>> netlib-java jniloader is loaded from netlib-java through
>> reflection, so
>> >>>>>> this problem will not be seen.
>> >>>>>>
>> >>>>>> Sincerely,
>> >>>>>>
>> >>>>>> DB Tsai
>> >>>>>> -------------------------------------------------------
>> >>>>>> My Blog: https://www.dbtsai.com
>> >>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>> >>>>>>
>>

From dev-return-7680-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 23:06:03 2014
Return-Path: <dev-return-7680-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 60DED11437
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 23:06:03 +0000 (UTC)
Received: (qmail 99490 invoked by uid 500); 18 May 2014 23:06:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99430 invoked by uid 500); 18 May 2014 23:06:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99422 invoked by uid 99); 18 May 2014 23:06:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 23:06:03 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.54 as permitted sender)
Received: from [209.85.220.54] (HELO mail-pa0-f54.google.com) (209.85.220.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 23:05:58 +0000
Received: by mail-pa0-f54.google.com with SMTP id bj1so4917902pad.13
        for <dev@spark.apache.org>; Sun, 18 May 2014 16:05:38 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=7IPtPQSx9XfgRTVT1FnjgvWmTI8JUq37Rkj6jB6/6Ic=;
        b=PxApNwHqruO6v2RGHhPjIODjIu+zc1a/GH6IBR8ryr9hb3owZyl3jCmRcr85mAn+0y
         vb9orwn6BcV2m/AzoK3uPKUmNQdFQnv8kv38OWwgo9GA29BdSsPfmnIka/DtSJ+CP9ih
         GqRaB/NAjeAzaNDnxnz9vPYSBQ0qcUclwOqSEeZ2Hnq+cWTxuzs82/ux9S0FpAa32m9m
         rxhrPg98C8OBtxW0DqjD/WLR1gB6+kPvcFdh4cZsIgQ45OeR/iRWlcu68AHqT0V01t7i
         3Qt1B0YbgNwq+wye65Cao46+LplxpTecwRXj3mSS6eQdq7j49vr32xr2LS/rotG99sA9
         8WBw==
X-Received: by 10.68.163.100 with SMTP id yh4mr38684474pbb.122.1400454337976;
        Sun, 18 May 2014 16:05:37 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id vf9sm26491484pbc.94.2014.05.18.16.05.35
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 18 May 2014 16:05:35 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <6AFDEA6E-E06C-4569-B92F-0A0CA8FACA3A@gmail.com>
Date: Sun, 18 May 2014 16:05:33 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <33CE569B-E0B5-4167-88C9-928F7A141335@gmail.com>
References: <JIRA.12714626.1400189782796@arcas> <JIRA.12714626.1400189782796.363092.1400238099511@arcas> <CAJiQeY+zsu3cfX1zyA=qK1KO8hirddmjGSB7HohTTA_a9jk3wQ@mail.gmail.com> <CAJgQjQ_1qrz4=05E=Wwj+O4UqKHwFKWQQiZosKbPjvwocMz9mw@mail.gmail.com> <CAJiQeY+eGxSzM-6C_poKCY53Gkixg85rdXiXXx+qi0z0bvCrrQ@mail.gmail.com> <6EF34F88-F79D-49B0-92FD-C421FBB0B6ED@gmail.com> <CACo38_QBRwZkxRwqbja_CTdPYy9ysY2b=ewntcq_5A-LVs9O0w@mail.gmail.com> <CA+-p3AFtOm_f6QopygGkA0jGAOvec1FXOEwrK+d238cTrHKW2w@mail.gmail.com> <CACo38_QrretkNzR6NoXeBw9E8jnaETvGGCiUt9kfA+O=mSxicw@mail.gmail.com> <6AFDEA6E-E06C-4569-B92F-0A0CA8FACA3A@gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

BTW in Spark the consensus so far was that we=92d use the dev@ list for =
high-level discussions (e.g. change in the development process, major =
features, proposals of new components, release votes) and keep =
lower-level issue tracking in JIRA. This is just how the project =
operated before so it was the easiest way for people to continue.

Matei

On May 18, 2014, at 4:01 PM, Matei Zaharia <matei.zaharia@gmail.com> =
wrote:

> Ah, maybe it=92s just different in other Apache projects. All the ones =
I=92ve participated in have had their design discussions on JIRA. For =
example take a look at https://issues.apache.org/jira/browse/HDFS-4949. =
(Most design discussions in Hadoop are also on JIRA).
>=20
> Hosting it this way is more convenient because most users come in =
looking at the issue tracker, not at mailing list archives (if only =
because the issue tracker is much more searchable for issues).
>=20
> Matei
>=20
> On May 18, 2014, at 2:19 PM, Jacek Laskowski <jacek@japila.pl> wrote:
>=20
>> On Sun, May 18, 2014 at 8:28 PM, Andrew Ash <andrew@andrewash.com> =
wrote:
>>> The nice thing about putting discussion on the Jira is that =
everything
>>> about the bug is in one place.  So people looking to understand the
>>> discussion a few years from now only have to look on the jira ticket =
rather
>>> than also search the mailing list archives and hope commenters all =
put the
>>> string "SPARK-1855" into the messages.
>>=20
>> My understanding is that JIRA is not for discussions. In a sense it
>> could be used for a few opinions, but have never seen it elsewhere =
and
>> am curious if it's an approach for the project (that I might accept
>> ultimately, but that would require some adoption time).
>>=20
>> What wrong with linking a discussion thread to a JIRA issue?
>>=20
>> Jacek
>>=20
>> --=20
>> Jacek Laskowski | http://blog.japila.pl
>> "Never discourage anyone who continually makes progress, no matter =
how
>> slow." Plato
>=20


From dev-return-7681-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 18 23:49:56 2014
Return-Path: <dev-return-7681-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 46D8411832
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 18 May 2014 23:49:56 +0000 (UTC)
Received: (qmail 36492 invoked by uid 500); 18 May 2014 23:49:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36430 invoked by uid 500); 18 May 2014 23:49:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36420 invoked by uid 99); 18 May 2014 23:49:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 23:49:55 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.180 as permitted sender)
Received: from [209.85.216.180] (HELO mail-qc0-f180.google.com) (209.85.216.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 18 May 2014 23:49:51 +0000
Received: by mail-qc0-f180.google.com with SMTP id i17so7719354qcy.25
        for <dev@spark.apache.org>; Sun, 18 May 2014 16:49:31 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=KvQgJ2t9d2McX2HGp+qvnov7sWr+i31obA76dU86zls=;
        b=Dx1CG7++EkQ5T+G4XP3kLrj0npPbNQ5ohAfOB3WLukxH+FVdmsE49WEWdlOd4BPXC4
         2anGHbslbqEHZRyainI3JJyVHCR81Bklh2StUtByUy3gAOrbq3kC2vEUzdePh0Y/I4Td
         MZfiLFSXdjtK9vxVD7CMBYZHDAuMWe+rTaaFBBJrC2VYFhdELl73pXuN6NSTnlDn73rs
         DKnaSiQSWjfqQl+n/++GD+sSc7IGW7Rm4ti+ynlKcvDvkQ42NJHKGE1d9+ce02GrAJGO
         Aby0/iB+6DJ3Xh/yl1A9O8WUT5AZBboTB9WOLznDK37DO2+f7EbqtR429uUP6qm0Cqg9
         G+pw==
X-Gm-Message-State: ALoCoQmaJHTheCjhqXHt9pmJDFKt6uVnDLOOL92fC3bXblrJIhzOGaWvSgFh2z2FoZO/FI5g+qG5
MIME-Version: 1.0
X-Received: by 10.224.129.66 with SMTP id n2mr42947716qas.55.1400456971074;
 Sun, 18 May 2014 16:49:31 -0700 (PDT)
Received: by 10.140.106.202 with HTTP; Sun, 18 May 2014 16:49:31 -0700 (PDT)
In-Reply-To: <CAJgQjQ_mdw+9+N8DoQDaZf5DgT_FCZrBqEEPPLbethwGFc5TcA@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
	<CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
	<CAJgQjQ_mdw+9+N8DoQDaZf5DgT_FCZrBqEEPPLbethwGFc5TcA@mail.gmail.com>
Date: Sun, 18 May 2014 16:49:31 -0700
Message-ID: <CACBYxK+9t1_wJtUz=pFsyWdbdpJEnnJ4tHeTFkWEeHTDjMbL3g@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1db9848d04e04f9b54c44
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1db9848d04e04f9b54c44
Content-Type: text/plain; charset=UTF-8

Hey Xiangrui,

If the jars are placed in the distributed cache and loaded statically, as
the primary app jar is in YARN, then it shouldn't be an issue.  Other jars,
however, including additional jars that are sc.addJar'd and jars specified
with the spark-submit --jars argument, are loaded dynamically by executors
with a URLClassLoader.  These jars aren't next to the executors when they
start - the executors fetch them from the driver's HTTP server.


On Sun, May 18, 2014 at 4:05 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Hi Sandy,
>
> It is hard to imagine that a user needs to create an object in that
> way. Since the jars are already in distributed cache before the
> executor starts, is there any reason we cannot add the locally cached
> jars to classpath directly?
>
> Best,
> Xiangrui
>
> On Sun, May 18, 2014 at 4:00 PM, Sandy Ryza <sandy.ryza@cloudera.com>
> wrote:
> > I spoke with DB offline about this a little while ago and he confirmed
> that
> > he was able to access the jar from the driver.
> >
> > The issue appears to be a general Java issue: you can't directly
> > instantiate a class from a dynamically loaded jar.
> >
> > I reproduced it locally outside of Spark with:
> > ---
> >     URLClassLoader urlClassLoader = new URLClassLoader(new URL[] { new
> > File("myotherjar.jar").toURI().toURL() }, null);
> >     Thread.currentThread().setContextClassLoader(urlClassLoader);
> >     MyClassFromMyOtherJar obj = new MyClassFromMyOtherJar();
> > ---
> >
> > I was able to load the class with reflection.
> >
> >
> >
> > On Sun, May 18, 2014 at 11:58 AM, Patrick Wendell <pwendell@gmail.com
> >wrote:
> >
> >> @db - it's possible that you aren't including the jar in the classpath
> >> of your driver program (I think this is what mridul was suggesting).
> >> It would be helpful to see the stack trace of the CNFE.
> >>
> >> - Patrick
> >>
> >> On Sun, May 18, 2014 at 11:54 AM, Patrick Wendell <pwendell@gmail.com>
> >> wrote:
> >> > @xiangrui - we don't expect these to be present on the system
> >> > classpath, because they get dynamically added by Spark (e.g. your
> >> > application can call sc.addJar well after the JVM's have started).
> >> >
> >> > @db - I'm pretty surprised to see that behavior. It's definitely not
> >> > intended that users need reflection to instantiate their classes -
> >> > something odd is going on in your case. If you could create an
> >> > isolated example and post it to the JIRA, that would be great.
> >> >
> >> > On Sun, May 18, 2014 at 9:58 AM, Xiangrui Meng <mengxr@gmail.com>
> wrote:
> >> >> I created a JIRA: https://issues.apache.org/jira/browse/SPARK-1870
> >> >>
> >> >> DB, could you add more info to that JIRA? Thanks!
> >> >>
> >> >> -Xiangrui
> >> >>
> >> >> On Sun, May 18, 2014 at 9:46 AM, Xiangrui Meng <mengxr@gmail.com>
> >> wrote:
> >> >>> Btw, I tried
> >> >>>
> >> >>> rdd.map { i =>
> >> >>>   System.getProperty("java.class.path")
> >> >>> }.collect()
> >> >>>
> >> >>> but didn't see the jars added via "--jars" on the executor
> classpath.
> >> >>>
> >> >>> -Xiangrui
> >> >>>
> >> >>> On Sat, May 17, 2014 at 11:26 PM, Xiangrui Meng <mengxr@gmail.com>
> >> wrote:
> >> >>>> I can re-produce the error with Spark 1.0-RC and YARN (CDH-5). The
> >> >>>> reflection approach mentioned by DB didn't work either. I checked
> the
> >> >>>> distributed cache on a worker node and found the jar there. It is
> also
> >> >>>> in the Environment tab of the WebUI. The workaround is making an
> >> >>>> assembly jar.
> >> >>>>
> >> >>>> DB, could you create a JIRA and describe what you have found so
> far?
> >> Thanks!
> >> >>>>
> >> >>>> Best,
> >> >>>> Xiangrui
> >> >>>>
> >> >>>> On Sat, May 17, 2014 at 1:29 AM, Mridul Muralidharan <
> >> mridul@gmail.com> wrote:
> >> >>>>> Can you try moving your mapPartitions to another class/object
> which
> >> is
> >> >>>>> referenced only after sc.addJar ?
> >> >>>>>
> >> >>>>> I would suspect CNFEx is coming while loading the class containing
> >> >>>>> mapPartitions before addJars is executed.
> >> >>>>>
> >> >>>>> In general though, dynamic loading of classes means you use
> >> reflection to
> >> >>>>> instantiate it since expectation is you don't know which
> >> implementation
> >> >>>>> provides the interface ... If you statically know it apriori, you
> >> bundle it
> >> >>>>> in your classpath.
> >> >>>>>
> >> >>>>> Regards
> >> >>>>> Mridul
> >> >>>>> On 17-May-2014 7:28 am, "DB Tsai" <dbtsai@stanford.edu> wrote:
> >> >>>>>
> >> >>>>>> Finally find a way out of the ClassLoader maze! It took me some
> >> times to
> >> >>>>>> understand how it works; I think it worths to document it in a
> >> separated
> >> >>>>>> thread.
> >> >>>>>>
> >> >>>>>> We're trying to add external utility.jar which contains
> >> CSVRecordParser,
> >> >>>>>> and we added the jar to executors through sc.addJar APIs.
> >> >>>>>>
> >> >>>>>> If the instance of CSVRecordParser is created without
> reflection, it
> >> >>>>>> raises *ClassNotFound
> >> >>>>>> Exception*.
> >> >>>>>>
> >> >>>>>> data.mapPartitions(lines => {
> >> >>>>>>     val csvParser = new CSVRecordParser((delimiter.charAt(0))
> >> >>>>>>     lines.foreach(line => {
> >> >>>>>>       val lineElems = csvParser.parseLine(line)
> >> >>>>>>     })
> >> >>>>>>     ...
> >> >>>>>>     ...
> >> >>>>>>  )
> >> >>>>>>
> >> >>>>>>
> >> >>>>>> If the instance of CSVRecordParser is created through reflection,
> >> it works.
> >> >>>>>>
> >> >>>>>> data.mapPartitions(lines => {
> >> >>>>>>     val loader = Thread.currentThread.getContextClassLoader
> >> >>>>>>     val CSVRecordParser =
> >> >>>>>>         loader.loadClass("com.alpine.hadoop.ext.CSVRecordParser")
> >> >>>>>>
> >> >>>>>>     val csvParser =
> CSVRecordParser.getConstructor(Character.TYPE)
> >> >>>>>>         .newInstance(delimiter.charAt(0).asInstanceOf[Character])
> >> >>>>>>
> >> >>>>>>     val parseLine = CSVRecordParser
> >> >>>>>>         .getDeclaredMethod("parseLine", classOf[String])
> >> >>>>>>
> >> >>>>>>     lines.foreach(line => {
> >> >>>>>>        val lineElems = parseLine.invoke(csvParser,
> >> >>>>>> line).asInstanceOf[Array[String]]
> >> >>>>>>     })
> >> >>>>>>     ...
> >> >>>>>>     ...
> >> >>>>>>  )
> >> >>>>>>
> >> >>>>>>
> >> >>>>>> This is identical to this question,
> >> >>>>>>
> >> >>>>>>
> >>
> http://stackoverflow.com/questions/7452411/thread-currentthread-setcontextclassloader-without-using-reflection
> >> >>>>>>
> >> >>>>>> It's not intuitive for users to load external classes through
> >> reflection,
> >> >>>>>> but couple available solutions including 1) messing around
> >> >>>>>> systemClassLoader by calling systemClassLoader.addURI through
> >> reflection or
> >> >>>>>> 2) forking another JVM to add jars into classpath before
> bootstrap
> >> loader
> >> >>>>>> are very tricky.
> >> >>>>>>
> >> >>>>>> Any thought on fixing it properly?
> >> >>>>>>
> >> >>>>>> @Xiangrui,
> >> >>>>>> netlib-java jniloader is loaded from netlib-java through
> >> reflection, so
> >> >>>>>> this problem will not be seen.
> >> >>>>>>
> >> >>>>>> Sincerely,
> >> >>>>>>
> >> >>>>>> DB Tsai
> >> >>>>>> -------------------------------------------------------
> >> >>>>>> My Blog: https://www.dbtsai.com
> >> >>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
> >> >>>>>>
> >>
>

--001a11c1db9848d04e04f9b54c44--

From dev-return-7682-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 00:01:24 2014
Return-Path: <dev-return-7682-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2F1F911887
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 00:01:24 +0000 (UTC)
Received: (qmail 53568 invoked by uid 500); 19 May 2014 00:01:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53512 invoked by uid 500); 19 May 2014 00:01:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53504 invoked by uid 99); 19 May 2014 00:01:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 00:01:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.81 as permitted sender)
Received: from [171.67.219.81] (HELO smtp.stanford.edu) (171.67.219.81)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 00:01:19 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 5FC9720C5F
	for <dev@spark.apache.org>; Sun, 18 May 2014 17:00:59 -0700 (PDT)
Received: from mail-qg0-f42.google.com (mail-qg0-f42.google.com [209.85.192.42])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id B882D20BFF
	for <dev@spark.apache.org>; Sun, 18 May 2014 17:00:58 -0700 (PDT)
Received: by mail-qg0-f42.google.com with SMTP id q107so7795930qgd.1
        for <dev@spark.apache.org>; Sun, 18 May 2014 17:00:57 -0700 (PDT)
X-Gm-Message-State: ALoCoQlYAH5kuXCDbd/+lxQ/PdYgglG/uOMwq4cxRdy6TWZAVfjI9YZml6gLhMMe6XJmgj8pzfjn
MIME-Version: 1.0
X-Received: by 10.224.95.73 with SMTP id c9mr43429632qan.68.1400457657947;
 Sun, 18 May 2014 17:00:57 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Sun, 18 May 2014 17:00:57 -0700 (PDT)
In-Reply-To: <CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
Date: Sun, 18 May 2014 17:00:57 -0700
Message-ID: <CAEYYnxbHetbvR1POvyE=1rB_0WGW2SR6wV34wwAk_Uew6O32Ug@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c347aa39ac3a04f9b5757b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c347aa39ac3a04f9b5757b
Content-Type: text/plain; charset=UTF-8

The reflection actually works. But you need to get the loader by `val
loader = Thread.currentThread.getContextClassLoader` which is set by Spark
executor. Our team verified this, and uses it as workaround.



Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Sun, May 18, 2014 at 9:46 AM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Btw, I tried
>
> rdd.map { i =>
>   System.getProperty("java.class.path")
> }.collect()
>
> but didn't see the jars added via "--jars" on the executor classpath.
>
> -Xiangrui
>
> On Sat, May 17, 2014 at 11:26 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
> > I can re-produce the error with Spark 1.0-RC and YARN (CDH-5). The
> > reflection approach mentioned by DB didn't work either. I checked the
> > distributed cache on a worker node and found the jar there. It is also
> > in the Environment tab of the WebUI. The workaround is making an
> > assembly jar.
> >
> > DB, could you create a JIRA and describe what you have found so far?
> Thanks!
> >
> > Best,
> > Xiangrui
> >
> > On Sat, May 17, 2014 at 1:29 AM, Mridul Muralidharan <mridul@gmail.com>
> wrote:
> >> Can you try moving your mapPartitions to another class/object which is
> >> referenced only after sc.addJar ?
> >>
> >> I would suspect CNFEx is coming while loading the class containing
> >> mapPartitions before addJars is executed.
> >>
> >> In general though, dynamic loading of classes means you use reflection
> to
> >> instantiate it since expectation is you don't know which implementation
> >> provides the interface ... If you statically know it apriori, you
> bundle it
> >> in your classpath.
> >>
> >> Regards
> >> Mridul
> >> On 17-May-2014 7:28 am, "DB Tsai" <dbtsai@stanford.edu> wrote:
> >>
> >>> Finally find a way out of the ClassLoader maze! It took me some times
> to
> >>> understand how it works; I think it worths to document it in a
> separated
> >>> thread.
> >>>
> >>> We're trying to add external utility.jar which contains
> CSVRecordParser,
> >>> and we added the jar to executors through sc.addJar APIs.
> >>>
> >>> If the instance of CSVRecordParser is created without reflection, it
> >>> raises *ClassNotFound
> >>> Exception*.
> >>>
> >>> data.mapPartitions(lines => {
> >>>     val csvParser = new CSVRecordParser((delimiter.charAt(0))
> >>>     lines.foreach(line => {
> >>>       val lineElems = csvParser.parseLine(line)
> >>>     })
> >>>     ...
> >>>     ...
> >>>  )
> >>>
> >>>
> >>> If the instance of CSVRecordParser is created through reflection, it
> works.
> >>>
> >>> data.mapPartitions(lines => {
> >>>     val loader = Thread.currentThread.getContextClassLoader
> >>>     val CSVRecordParser =
> >>>         loader.loadClass("com.alpine.hadoop.ext.CSVRecordParser")
> >>>
> >>>     val csvParser = CSVRecordParser.getConstructor(Character.TYPE)
> >>>         .newInstance(delimiter.charAt(0).asInstanceOf[Character])
> >>>
> >>>     val parseLine = CSVRecordParser
> >>>         .getDeclaredMethod("parseLine", classOf[String])
> >>>
> >>>     lines.foreach(line => {
> >>>        val lineElems = parseLine.invoke(csvParser,
> >>> line).asInstanceOf[Array[String]]
> >>>     })
> >>>     ...
> >>>     ...
> >>>  )
> >>>
> >>>
> >>> This is identical to this question,
> >>>
> >>>
> http://stackoverflow.com/questions/7452411/thread-currentthread-setcontextclassloader-without-using-reflection
> >>>
> >>> It's not intuitive for users to load external classes through
> reflection,
> >>> but couple available solutions including 1) messing around
> >>> systemClassLoader by calling systemClassLoader.addURI through
> reflection or
> >>> 2) forking another JVM to add jars into classpath before bootstrap
> loader
> >>> are very tricky.
> >>>
> >>> Any thought on fixing it properly?
> >>>
> >>> @Xiangrui,
> >>> netlib-java jniloader is loaded from netlib-java through reflection, so
> >>> this problem will not be seen.
> >>>
> >>> Sincerely,
> >>>
> >>> DB Tsai
> >>> -------------------------------------------------------
> >>> My Blog: https://www.dbtsai.com
> >>> LinkedIn: https://www.linkedin.com/in/dbtsai
> >>>
>

--001a11c347aa39ac3a04f9b5757b--

From dev-return-7683-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 00:03:56 2014
Return-Path: <dev-return-7683-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 998491189F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 00:03:56 +0000 (UTC)
Received: (qmail 61976 invoked by uid 500); 19 May 2014 00:03:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61914 invoked by uid 500); 19 May 2014 00:03:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61906 invoked by uid 99); 19 May 2014 00:03:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 00:03:56 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 00:03:52 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id A0250101055
	for <dev@spark.apache.org>; Sun, 18 May 2014 17:03:31 -0700 (PDT)
Received: from mail-qg0-f44.google.com (mail-qg0-f44.google.com [209.85.192.44])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id E9E6F10104E
	for <dev@spark.apache.org>; Sun, 18 May 2014 17:03:30 -0700 (PDT)
Received: by mail-qg0-f44.google.com with SMTP id i50so7799822qgf.31
        for <dev@spark.apache.org>; Sun, 18 May 2014 17:03:30 -0700 (PDT)
X-Gm-Message-State: ALoCoQl3kN/JLXEWmNQvACwKbj2xzIexpy0mpi+3GY8NZMQEf2IMGMML7BcoT+MHdBoPURjlP2D4
MIME-Version: 1.0
X-Received: by 10.224.19.131 with SMTP id a3mr42564751qab.3.1400457810144;
 Sun, 18 May 2014 17:03:30 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Sun, 18 May 2014 17:03:30 -0700 (PDT)
In-Reply-To: <CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
Date: Sun, 18 May 2014 17:03:30 -0700
Message-ID: <CAEYYnxbnqHNHnMZTfQamszU2=0OJZL1KDHY9y5G2pUJLcmS_LA@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1132e9664c02d704f9b57efd
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1132e9664c02d704f9b57efd
Content-Type: text/plain; charset=UTF-8

The jars are included in my driver, and I can successfully use them in the
driver. I'm working on a patch, and it's almost working. Will submit a PR
soon.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Sun, May 18, 2014 at 11:58 AM, Patrick Wendell <pwendell@gmail.com>wrote:

> @db - it's possible that you aren't including the jar in the classpath
> of your driver program (I think this is what mridul was suggesting).
> It would be helpful to see the stack trace of the CNFE.
>
> - Patrick
>
> On Sun, May 18, 2014 at 11:54 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > @xiangrui - we don't expect these to be present on the system
> > classpath, because they get dynamically added by Spark (e.g. your
> > application can call sc.addJar well after the JVM's have started).
> >
> > @db - I'm pretty surprised to see that behavior. It's definitely not
> > intended that users need reflection to instantiate their classes -
> > something odd is going on in your case. If you could create an
> > isolated example and post it to the JIRA, that would be great.
> >
> > On Sun, May 18, 2014 at 9:58 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
> >> I created a JIRA: https://issues.apache.org/jira/browse/SPARK-1870
> >>
> >> DB, could you add more info to that JIRA? Thanks!
> >>
> >> -Xiangrui
> >>
> >> On Sun, May 18, 2014 at 9:46 AM, Xiangrui Meng <mengxr@gmail.com>
> wrote:
> >>> Btw, I tried
> >>>
> >>> rdd.map { i =>
> >>>   System.getProperty("java.class.path")
> >>> }.collect()
> >>>
> >>> but didn't see the jars added via "--jars" on the executor classpath.
> >>>
> >>> -Xiangrui
> >>>
> >>> On Sat, May 17, 2014 at 11:26 PM, Xiangrui Meng <mengxr@gmail.com>
> wrote:
> >>>> I can re-produce the error with Spark 1.0-RC and YARN (CDH-5). The
> >>>> reflection approach mentioned by DB didn't work either. I checked the
> >>>> distributed cache on a worker node and found the jar there. It is also
> >>>> in the Environment tab of the WebUI. The workaround is making an
> >>>> assembly jar.
> >>>>
> >>>> DB, could you create a JIRA and describe what you have found so far?
> Thanks!
> >>>>
> >>>> Best,
> >>>> Xiangrui
> >>>>
> >>>> On Sat, May 17, 2014 at 1:29 AM, Mridul Muralidharan <
> mridul@gmail.com> wrote:
> >>>>> Can you try moving your mapPartitions to another class/object which
> is
> >>>>> referenced only after sc.addJar ?
> >>>>>
> >>>>> I would suspect CNFEx is coming while loading the class containing
> >>>>> mapPartitions before addJars is executed.
> >>>>>
> >>>>> In general though, dynamic loading of classes means you use
> reflection to
> >>>>> instantiate it since expectation is you don't know which
> implementation
> >>>>> provides the interface ... If you statically know it apriori, you
> bundle it
> >>>>> in your classpath.
> >>>>>
> >>>>> Regards
> >>>>> Mridul
> >>>>> On 17-May-2014 7:28 am, "DB Tsai" <dbtsai@stanford.edu> wrote:
> >>>>>
> >>>>>> Finally find a way out of the ClassLoader maze! It took me some
> times to
> >>>>>> understand how it works; I think it worths to document it in a
> separated
> >>>>>> thread.
> >>>>>>
> >>>>>> We're trying to add external utility.jar which contains
> CSVRecordParser,
> >>>>>> and we added the jar to executors through sc.addJar APIs.
> >>>>>>
> >>>>>> If the instance of CSVRecordParser is created without reflection, it
> >>>>>> raises *ClassNotFound
> >>>>>> Exception*.
> >>>>>>
> >>>>>> data.mapPartitions(lines => {
> >>>>>>     val csvParser = new CSVRecordParser((delimiter.charAt(0))
> >>>>>>     lines.foreach(line => {
> >>>>>>       val lineElems = csvParser.parseLine(line)
> >>>>>>     })
> >>>>>>     ...
> >>>>>>     ...
> >>>>>>  )
> >>>>>>
> >>>>>>
> >>>>>> If the instance of CSVRecordParser is created through reflection,
> it works.
> >>>>>>
> >>>>>> data.mapPartitions(lines => {
> >>>>>>     val loader = Thread.currentThread.getContextClassLoader
> >>>>>>     val CSVRecordParser =
> >>>>>>         loader.loadClass("com.alpine.hadoop.ext.CSVRecordParser")
> >>>>>>
> >>>>>>     val csvParser = CSVRecordParser.getConstructor(Character.TYPE)
> >>>>>>         .newInstance(delimiter.charAt(0).asInstanceOf[Character])
> >>>>>>
> >>>>>>     val parseLine = CSVRecordParser
> >>>>>>         .getDeclaredMethod("parseLine", classOf[String])
> >>>>>>
> >>>>>>     lines.foreach(line => {
> >>>>>>        val lineElems = parseLine.invoke(csvParser,
> >>>>>> line).asInstanceOf[Array[String]]
> >>>>>>     })
> >>>>>>     ...
> >>>>>>     ...
> >>>>>>  )
> >>>>>>
> >>>>>>
> >>>>>> This is identical to this question,
> >>>>>>
> >>>>>>
> http://stackoverflow.com/questions/7452411/thread-currentthread-setcontextclassloader-without-using-reflection
> >>>>>>
> >>>>>> It's not intuitive for users to load external classes through
> reflection,
> >>>>>> but couple available solutions including 1) messing around
> >>>>>> systemClassLoader by calling systemClassLoader.addURI through
> reflection or
> >>>>>> 2) forking another JVM to add jars into classpath before bootstrap
> loader
> >>>>>> are very tricky.
> >>>>>>
> >>>>>> Any thought on fixing it properly?
> >>>>>>
> >>>>>> @Xiangrui,
> >>>>>> netlib-java jniloader is loaded from netlib-java through
> reflection, so
> >>>>>> this problem will not be seen.
> >>>>>>
> >>>>>> Sincerely,
> >>>>>>
> >>>>>> DB Tsai
> >>>>>> -------------------------------------------------------
> >>>>>> My Blog: https://www.dbtsai.com
> >>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
> >>>>>>
>

--001a1132e9664c02d704f9b57efd--

From dev-return-7684-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 00:09:21 2014
Return-Path: <dev-return-7684-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6D3C2118BD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 00:09:21 +0000 (UTC)
Received: (qmail 66691 invoked by uid 500); 19 May 2014 00:09:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66632 invoked by uid 500); 19 May 2014 00:09:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66624 invoked by uid 99); 19 May 2014 00:09:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 00:09:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.81 as permitted sender)
Received: from [171.67.219.81] (HELO smtp.stanford.edu) (171.67.219.81)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 00:09:17 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id C10BE20D2C
	for <dev@spark.apache.org>; Sun, 18 May 2014 17:08:56 -0700 (PDT)
Received: from mail-qc0-f170.google.com (mail-qc0-f170.google.com [209.85.216.170])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 0620B20BC2
	for <dev@spark.apache.org>; Sun, 18 May 2014 17:08:55 -0700 (PDT)
Received: by mail-qc0-f170.google.com with SMTP id i8so8005540qcq.1
        for <dev@spark.apache.org>; Sun, 18 May 2014 17:08:55 -0700 (PDT)
X-Gm-Message-State: ALoCoQnLspV3stEeJ5qj7A6hD8hH2ZKczPKro3n48ir/Kek8cexhlpDcVePk2JB/CvSQ4ZmzHW3n
MIME-Version: 1.0
X-Received: by 10.140.88.241 with SMTP id t104mr42854329qgd.29.1400458135207;
 Sun, 18 May 2014 17:08:55 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Sun, 18 May 2014 17:08:55 -0700 (PDT)
In-Reply-To: <CACBYxK+9t1_wJtUz=pFsyWdbdpJEnnJ4tHeTFkWEeHTDjMbL3g@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
	<CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
	<CAJgQjQ_mdw+9+N8DoQDaZf5DgT_FCZrBqEEPPLbethwGFc5TcA@mail.gmail.com>
	<CACBYxK+9t1_wJtUz=pFsyWdbdpJEnnJ4tHeTFkWEeHTDjMbL3g@mail.gmail.com>
Date: Sun, 18 May 2014 17:08:55 -0700
Message-ID: <CAEYYnxYv8=gJKgx2wqe=2ZoKXb_5e=eioGa_hX5U1Jw4h1T4nQ@mail.gmail.com>
Subject: Fwd: Calling external classes added by sc.addJar needs to be through reflection
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c135ecac190804f9b591d7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c135ecac190804f9b591d7
Content-Type: text/plain; charset=UTF-8

Since the additional jars added by sc.addJars are through http server, even
it works, we still want to have a better way due to scalability (imagine
that thousands of workers downloading jars from driver).

If we ignore the fundamental scalability issue, this can be fixed by using
the customClassloader to create a wrapped class, and in this wrapped class,
the classloader is inherited from the customClassloader so that users don't
need to do reflection in the wrapped class. I'm working on this now.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


---------- Forwarded message ----------
From: Sandy Ryza <sandy.ryza@cloudera.com>
Date: Sun, May 18, 2014 at 4:49 PM
Subject: Re: Calling external classes added by sc.addJar needs to be
through reflection
To: "dev@spark.apache.org" <dev@spark.apache.org>


Hey Xiangrui,

If the jars are placed in the distributed cache and loaded statically, as
the primary app jar is in YARN, then it shouldn't be an issue.  Other jars,
however, including additional jars that are sc.addJar'd and jars specified
with the spark-submit --jars argument, are loaded dynamically by executors
with a URLClassLoader.  These jars aren't next to the executors when they
start - the executors fetch them from the driver's HTTP server.


On Sun, May 18, 2014 at 4:05 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Hi Sandy,
>
> It is hard to imagine that a user needs to create an object in that
> way. Since the jars are already in distributed cache before the
> executor starts, is there any reason we cannot add the locally cached
> jars to classpath directly?
>
> Best,
> Xiangrui
>
> On Sun, May 18, 2014 at 4:00 PM, Sandy Ryza <sandy.ryza@cloudera.com>
> wrote:
> > I spoke with DB offline about this a little while ago and he confirmed
> that
> > he was able to access the jar from the driver.
> >
> > The issue appears to be a general Java issue: you can't directly
> > instantiate a class from a dynamically loaded jar.
> >
> > I reproduced it locally outside of Spark with:
> > ---
> >     URLClassLoader urlClassLoader = new URLClassLoader(new URL[] { new
> > File("myotherjar.jar").toURI().toURL() }, null);
> >     Thread.currentThread().setContextClassLoader(urlClassLoader);
> >     MyClassFromMyOtherJar obj = new MyClassFromMyOtherJar();
> > ---
> >
> > I was able to load the class with reflection.
> >
> >
> >
> > On Sun, May 18, 2014 at 11:58 AM, Patrick Wendell <pwendell@gmail.com
> >wrote:
> >
> >> @db - it's possible that you aren't including the jar in the classpath
> >> of your driver program (I think this is what mridul was suggesting).
> >> It would be helpful to see the stack trace of the CNFE.
> >>
> >> - Patrick
> >>
> >> On Sun, May 18, 2014 at 11:54 AM, Patrick Wendell <pwendell@gmail.com>
> >> wrote:
> >> > @xiangrui - we don't expect these to be present on the system
> >> > classpath, because they get dynamically added by Spark (e.g. your
> >> > application can call sc.addJar well after the JVM's have started).
> >> >
> >> > @db - I'm pretty surprised to see that behavior. It's definitely not
> >> > intended that users need reflection to instantiate their classes -
> >> > something odd is going on in your case. If you could create an
> >> > isolated example and post it to the JIRA, that would be great.
> >> >
> >> > On Sun, May 18, 2014 at 9:58 AM, Xiangrui Meng <mengxr@gmail.com>
> wrote:
> >> >> I created a JIRA: https://issues.apache.org/jira/browse/SPARK-1870
> >> >>
> >> >> DB, could you add more info to that JIRA? Thanks!
> >> >>
> >> >> -Xiangrui
> >> >>
> >> >> On Sun, May 18, 2014 at 9:46 AM, Xiangrui Meng <mengxr@gmail.com>
> >> wrote:
> >> >>> Btw, I tried
> >> >>>
> >> >>> rdd.map { i =>
> >> >>>   System.getProperty("java.class.path")
> >> >>> }.collect()
> >> >>>
> >> >>> but didn't see the jars added via "--jars" on the executor
> classpath.
> >> >>>
> >> >>> -Xiangrui
> >> >>>
> >> >>> On Sat, May 17, 2014 at 11:26 PM, Xiangrui Meng <mengxr@gmail.com>
> >> wrote:
> >> >>>> I can re-produce the error with Spark 1.0-RC and YARN (CDH-5). The
> >> >>>> reflection approach mentioned by DB didn't work either. I checked
> the
> >> >>>> distributed cache on a worker node and found the jar there. It is
> also
> >> >>>> in the Environment tab of the WebUI. The workaround is making an
> >> >>>> assembly jar.
> >> >>>>
> >> >>>> DB, could you create a JIRA and describe what you have found so
> far?
> >> Thanks!
> >> >>>>
> >> >>>> Best,
> >> >>>> Xiangrui
> >> >>>>
> >> >>>> On Sat, May 17, 2014 at 1:29 AM, Mridul Muralidharan <
> >> mridul@gmail.com> wrote:
> >> >>>>> Can you try moving your mapPartitions to another class/object
> which
> >> is
> >> >>>>> referenced only after sc.addJar ?
> >> >>>>>
> >> >>>>> I would suspect CNFEx is coming while loading the class
containing
> >> >>>>> mapPartitions before addJars is executed.
> >> >>>>>
> >> >>>>> In general though, dynamic loading of classes means you use
> >> reflection to
> >> >>>>> instantiate it since expectation is you don't know which
> >> implementation
> >> >>>>> provides the interface ... If you statically know it apriori, you
> >> bundle it
> >> >>>>> in your classpath.
> >> >>>>>
> >> >>>>> Regards
> >> >>>>> Mridul
> >> >>>>> On 17-May-2014 7:28 am, "DB Tsai" <dbtsai@stanford.edu> wrote:
> >> >>>>>
> >> >>>>>> Finally find a way out of the ClassLoader maze! It took me some
> >> times to
> >> >>>>>> understand how it works; I think it worths to document it in a
> >> separated
> >> >>>>>> thread.
> >> >>>>>>
> >> >>>>>> We're trying to add external utility.jar which contains
> >> CSVRecordParser,
> >> >>>>>> and we added the jar to executors through sc.addJar APIs.
> >> >>>>>>
> >> >>>>>> If the instance of CSVRecordParser is created without
> reflection, it
> >> >>>>>> raises *ClassNotFound
> >> >>>>>> Exception*.
> >> >>>>>>
> >> >>>>>> data.mapPartitions(lines => {
> >> >>>>>>     val csvParser = new CSVRecordParser((delimiter.charAt(0))
> >> >>>>>>     lines.foreach(line => {
> >> >>>>>>       val lineElems = csvParser.parseLine(line)
> >> >>>>>>     })
> >> >>>>>>     ...
> >> >>>>>>     ...
> >> >>>>>>  )
> >> >>>>>>
> >> >>>>>>
> >> >>>>>> If the instance of CSVRecordParser is created through
reflection,
> >> it works.
> >> >>>>>>
> >> >>>>>> data.mapPartitions(lines => {
> >> >>>>>>     val loader = Thread.currentThread.getContextClassLoader
> >> >>>>>>     val CSVRecordParser =
> >> >>>>>>
loader.loadClass("com.alpine.hadoop.ext.CSVRecordParser")
> >> >>>>>>
> >> >>>>>>     val csvParser =
> CSVRecordParser.getConstructor(Character.TYPE)
> >> >>>>>>
.newInstance(delimiter.charAt(0).asInstanceOf[Character])
> >> >>>>>>
> >> >>>>>>     val parseLine = CSVRecordParser
> >> >>>>>>         .getDeclaredMethod("parseLine", classOf[String])
> >> >>>>>>
> >> >>>>>>     lines.foreach(line => {
> >> >>>>>>        val lineElems = parseLine.invoke(csvParser,
> >> >>>>>> line).asInstanceOf[Array[String]]
> >> >>>>>>     })
> >> >>>>>>     ...
> >> >>>>>>     ...
> >> >>>>>>  )
> >> >>>>>>
> >> >>>>>>
> >> >>>>>> This is identical to this question,
> >> >>>>>>
> >> >>>>>>
> >>
>
http://stackoverflow.com/questions/7452411/thread-currentthread-setcontextclassloader-without-using-reflection
> >> >>>>>>
> >> >>>>>> It's not intuitive for users to load external classes through
> >> reflection,
> >> >>>>>> but couple available solutions including 1) messing around
> >> >>>>>> systemClassLoader by calling systemClassLoader.addURI through
> >> reflection or
> >> >>>>>> 2) forking another JVM to add jars into classpath before
> bootstrap
> >> loader
> >> >>>>>> are very tricky.
> >> >>>>>>
> >> >>>>>> Any thought on fixing it properly?
> >> >>>>>>
> >> >>>>>> @Xiangrui,
> >> >>>>>> netlib-java jniloader is loaded from netlib-java through
> >> reflection, so
> >> >>>>>> this problem will not be seen.
> >> >>>>>>
> >> >>>>>> Sincerely,
> >> >>>>>>
> >> >>>>>> DB Tsai
> >> >>>>>> -------------------------------------------------------
> >> >>>>>> My Blog: https://www.dbtsai.com
> >> >>>>>> LinkedIn: https://www.linkedin.com/in/dbtsai
> >> >>>>>>
> >>
>

--001a11c135ecac190804f9b591d7--

From dev-return-7685-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 00:28:53 2014
Return-Path: <dev-return-7685-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0E8761191D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 00:28:53 +0000 (UTC)
Received: (qmail 98566 invoked by uid 500); 19 May 2014 00:28:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98508 invoked by uid 500); 19 May 2014 00:28:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98500 invoked by uid 99); 19 May 2014 00:28:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 00:28:52 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.47 as permitted sender)
Received: from [209.85.160.47] (HELO mail-pb0-f47.google.com) (209.85.160.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 00:28:48 +0000
Received: by mail-pb0-f47.google.com with SMTP id rp16so5047429pbb.20
        for <dev@spark.apache.org>; Sun, 18 May 2014 17:28:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=aF84+bIa+eKUKXu6zJSRiHgnWL7pHZMoRSRL6g074AM=;
        b=WOtao6Cd4BL1Fib7RAU/oTdVy9BFBw1T4xlM7occQt4QbTIiw2m30JCE9YzZEPhpga
         0n0LVRo3ug1urdF3ZYncg/6SkU8erc6J2VYsPSJCyGW69BsY7DLlLO/a/xoVgdxxodOU
         Urglu1EvAJekdCRYdtB45RuUOwzTjw35jjL6UjHk7QSC4/OyDVH6shQFeIOYj0jFeqzg
         DsdjRGeju/mUtSFXRDsRJP9Cjj4vV4Wcsk2C8prC2xKWTUWSFxpOFkdgEafJex3yJgH6
         /S2NoeZQ/KuqKuzRf3/rMR45k2/uqrmtUfKgfX4nq5glVOdaQaf/PLD3+CagBj0UccLO
         K5Vg==
X-Received: by 10.66.183.11 with SMTP id ei11mr38548677pac.116.1400459307899;
        Sun, 18 May 2014 17:28:27 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id pl5sm26698226pbc.90.2014.05.18.17.28.25
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 18 May 2014 17:28:25 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc9)
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <9849C2AA-3131-4E31-85DF-7F6D61632545@gmail.com>
Date: Sun, 18 May 2014 17:28:23 -0700
Cc: Tom Graves <tgraves_cs@yahoo.com>
Content-Transfer-Encoding: quoted-printable
Message-Id: <4D62B3E0-B6E7-4916-953D-A196FC6BC24F@gmail.com>
References: <CABPQxstaxcbGSQTqUxSSUQfRpjQs=BqRkrEVvzOXbXZo8kzShg@mail.gmail.com> <CABPQxstbwN64rQvx6kUoESkO4GJOohYSAO+s_F85wFWjb6T1zw@mail.gmail.com> <CAAsvFPn2RwpFMz7ajy=t8GSjiKSPqARgDtPqXk007Cd7_xvA-Q@mail.gmail.com> <CAMJOb8nTR38Uf50xVxv5O+OrBkO0ELDFYiHrziBVm9qknAgpYA@mail.gmail.com> <CACBYxKLR3xE4FSqZQwrjmP+bhhqy_mAat0hHSu+1pOBEVTttrA@mail.gmail.com> <9849C2AA-3131-4E31-85DF-7F6D61632545@gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

Alright, I=92ve opened https://github.com/apache/spark/pull/819 with the =
Windows fixes. I also found one other likely bug, =
https://issues.apache.org/jira/browse/SPARK-1875, in the binary packages =
for Hadoop1 built in this RC. I think this is due to Hadoop 1=92s =
security code depending on a different version of org.apache.commons =
than Hadoop 2, but it needs investigation. Tom, any thoughts on this?

Matei

On May 18, 2014, at 12:33 PM, Matei Zaharia <matei.zaharia@gmail.com> =
wrote:

> I took the always fun task of testing it on Windows, and =
unfortunately, I found some small problems with the prebuilt packages =
due to recent changes to the launch scripts: bin/spark-class2.cmd looks =
in ./jars instead of ./lib for the assembly JAR, and =
bin/run-example2.cmd doesn=92t quite match the master-setting behavior =
of the Unix based one. I=92ll send a pull request to fix them soon.
>=20
> Matei
>=20
>=20
> On May 17, 2014, at 11:32 AM, Sandy Ryza <sandy.ryza@cloudera.com> =
wrote:
>=20
>> +1
>>=20
>> Reran my tests from rc5:
>>=20
>> * Built the release from source.
>> * Compiled Java and Scala apps that interact with HDFS against it.
>> * Ran them in local mode.
>> * Ran them against a pseudo-distributed YARN cluster in both =
yarn-client
>> mode and yarn-cluster mode.
>>=20
>>=20
>> On Sat, May 17, 2014 at 10:08 AM, Andrew Or <andrew@databricks.com> =
wrote:
>>=20
>>> +1
>>>=20
>>>=20
>>> 2014-05-17 8:53 GMT-07:00 Mark Hamstra <mark@clearstorydata.com>:
>>>=20
>>>> +1
>>>>=20
>>>>=20
>>>> On Sat, May 17, 2014 at 12:58 AM, Patrick Wendell =
<pwendell@gmail.com
>>>>> wrote:
>>>>=20
>>>>> I'll start the voting with a +1.
>>>>>=20
>>>>> On Sat, May 17, 2014 at 12:58 AM, Patrick Wendell =
<pwendell@gmail.com>
>>>>> wrote:
>>>>>> Please vote on releasing the following candidate as Apache Spark
>>>> version
>>>>> 1.0.0!
>>>>>> This has one bug fix and one minor feature on top of rc8:
>>>>>> SPARK-1864: https://github.com/apache/spark/pull/808
>>>>>> SPARK-1808: https://github.com/apache/spark/pull/799
>>>>>>=20
>>>>>> The tag to be voted on is v1.0.0-rc9 (commit 920f947):
>>>>>>=20
>>>>>=20
>>>>=20
>>> =
https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D920f9=
47eb5a22a679c0c3186cf69ee75f6041c75
>>>>>>=20
>>>>>> The release files, including signatures, digests, etc. can be =
found
>>> at:
>>>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc9/
>>>>>>=20
>>>>>> Release artifacts are signed with the following key:
>>>>>> https://people.apache.org/keys/committer/pwendell.asc
>>>>>>=20
>>>>>> The staging repository for this release can be found at:
>>>>>>=20
>>>> =
https://repository.apache.org/content/repositories/orgapachespark-1017/
>>>>>>=20
>>>>>> The documentation corresponding to this release can be found at:
>>>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc9-docs/
>>>>>>=20
>>>>>> Please vote on releasing this package as Apache Spark 1.0.0!
>>>>>>=20
>>>>>> The vote is open until Tuesday, May 20, at 08:56 UTC and passes =
if
>>>>>> amajority of at least 3 +1 PMC votes are cast.
>>>>>>=20
>>>>>> [ ] +1 Release this package as Apache Spark 1.0.0
>>>>>> [ ] -1 Do not release this package because ...
>>>>>>=20
>>>>>> To learn more about Apache Spark, please see
>>>>>> http://spark.apache.org/
>>>>>>=20
>>>>>> =3D=3D API Changes =3D=3D
>>>>>> We welcome users to compile Spark applications against 1.0. There =
are
>>>>>> a few API changes in this release. Here are links to the =
associated
>>>>>> upgrade guides - user facing changes have been kept as small as
>>>>>> possible.
>>>>>>=20
>>>>>> changes to ML vector specification:
>>>>>>=20
>>>>>=20
>>>>=20
>>> =
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#f=
rom-09-to-10
>>>>>>=20
>>>>>> changes to the Java API:
>>>>>>=20
>>>>>=20
>>>>=20
>>> =
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programming-g=
uide.html#upgrading-from-pre-10-versions-of-spark
>>>>>>=20
>>>>>> changes to the streaming API:
>>>>>>=20
>>>>>=20
>>>>=20
>>> =
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-programm=
ing-guide.html#migration-guide-from-091-or-below-to-1x
>>>>>>=20
>>>>>> changes to the GraphX API:
>>>>>>=20
>>>>>=20
>>>>=20
>>> =
http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming=
-guide.html#upgrade-guide-from-spark-091
>>>>>>=20
>>>>>> coGroup and related functions now return Iterable[T] instead of
>>> Seq[T]
>>>>>> =3D=3D> Call toSeq on the result to restore the old behavior
>>>>>>=20
>>>>>> SparkContext.jarOfClass returns Option[String] instead of =
Seq[String]
>>>>>> =3D=3D> Call toSeq on the result to restore old behavior
>>>>>=20
>>>>=20
>>>=20
>=20


From dev-return-7686-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 01:14:03 2014
Return-Path: <dev-return-7686-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9FB84119B9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 01:14:03 +0000 (UTC)
Received: (qmail 27309 invoked by uid 500); 19 May 2014 01:14:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27247 invoked by uid 500); 19 May 2014 01:14:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27239 invoked by uid 99); 19 May 2014 01:14:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 01:14:03 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of liqingyang1985@gmail.com designates 74.125.82.42 as permitted sender)
Received: from [74.125.82.42] (HELO mail-wg0-f42.google.com) (74.125.82.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 01:14:00 +0000
Received: by mail-wg0-f42.google.com with SMTP id y10so7057492wgg.13
        for <dev@spark.apache.org>; Sun, 18 May 2014 18:13:37 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=e4CCeqdhyQ9JLhvFcNmQ+2uXKBV8oUe7dVaEXGbnS8k=;
        b=RAz3UhoMZqi3XiOq5Ccwf8XIU2rITk4UJw/jiWWy1+YrplmsHfOmRduxwPD2kUUiFp
         ZxFZzQNjr8CklfeIgaD684S1R6IPJXS/fyq2fO4ZEfo3jM95diUPGeoS7tJFkpgj7srn
         aE0GuN5hLLWUGm0e+Hsyp4eA5QMr3rQ5v+f+xhiO0x8bJtyDRwguWMxIgeEyAxF+2kyi
         z8B0BQZ6Ra5TNyQNoK4XWSxhcunpa99ao+W7DhtJKgczYwltMkMPoVLCKKrtIPICWexU
         57FQoZv8DGLcIisxaa8tsJn9Ph4SbE2U/h5OPzt1hNUEpjVq+KPjzwVEaL+PUIkSIQJT
         0H+w==
MIME-Version: 1.0
X-Received: by 10.194.62.176 with SMTP id z16mr391364wjr.76.1400462017091;
 Sun, 18 May 2014 18:13:37 -0700 (PDT)
Received: by 10.194.61.39 with HTTP; Sun, 18 May 2014 18:13:37 -0700 (PDT)
In-Reply-To: <CAGh_TuNEmH3z0-uZKa6Xmt9FO2ZjpHx+mTXoFx4dknc2=p=9wQ@mail.gmail.com>
References: <CABDsqqb64pL9ENMv0YVoo2GRKWQQe=jCb5TLZV4yBeRwg2ZyTA@mail.gmail.com>
	<CALEZFQyMp9XgjYeToh7YStVzu8Q-3uSqWaxad6pLw=mEg+7r3Q@mail.gmail.com>
	<CAGh_TuNEmH3z0-uZKa6Xmt9FO2ZjpHx+mTXoFx4dknc2=p=9wQ@mail.gmail.com>
Date: Mon, 19 May 2014 09:13:37 +0800
Message-ID: <CABDsqqZfOHy=w0bVQaH2hna9M1-DPWEmFgK4npKAeTkuq=bqgQ@mail.gmail.com>
Subject: Re: can RDD be shared across mutil spark applications?
From: qingyang li <liqingyang1985@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7ba979c20cdbce04f9b679db
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7ba979c20cdbce04f9b679db
Content-Type: text/plain; charset=UTF-8

thanks for sharing,  I am using tachyon to store RDD now.


2014-05-18 12:02 GMT+08:00 Christopher Nguyen <ctn@adatao.com>:

> Qing Yang, Andy is correct in answering your direct question.
>
> At the same time, depending on your context, you may be able to apply a
> pattern where you turn the single Spark application into a service, and
> multiple clients if that service can indeed share access to the same RDDs.
>
> Several groups have built apps based on this pattern, and we will also show
> something with this behavior at the upcoming Spark Summit (multiple users
> collaborating on named DDFs with the same underlying RDDs).
>
> Sent while mobile. Pls excuse typos etc.
> On May 18, 2014 9:40 AM, "Andy Konwinski" <andykonwinski@gmail.com> wrote:
>
> > RDDs cannot currently be shared across multiple SparkContexts without
> using
> > something like the Tachyon project (which is a separate
> project/codebase).
> >
> > Andy
> > On May 16, 2014 2:14 PM, "qingyang li" <liqingyang1985@gmail.com> wrote:
> >
> > >
> > >
> >
>

--047d7ba979c20cdbce04f9b679db--

From dev-return-7687-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 02:00:38 2014
Return-Path: <dev-return-7687-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 16A1E11AC3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 02:00:38 +0000 (UTC)
Received: (qmail 62018 invoked by uid 500); 19 May 2014 02:00:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61961 invoked by uid 500); 19 May 2014 02:00:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61953 invoked by uid 99); 19 May 2014 02:00:37 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 02:00:37 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tgraves_cs@yahoo.com designates 216.109.115.174 as permitted sender)
Received: from [216.109.115.174] (HELO nm48-vm7.bullet.mail.bf1.yahoo.com) (216.109.115.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 02:00:33 +0000
Received: from [66.196.81.172] by nm48.bullet.mail.bf1.yahoo.com with NNFMP; 19 May 2014 02:00:09 -0000
Received: from [98.139.212.247] by tm18.bullet.mail.bf1.yahoo.com with NNFMP; 19 May 2014 02:00:09 -0000
Received: from [127.0.0.1] by omp1056.mail.bf1.yahoo.com with NNFMP; 19 May 2014 02:00:09 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 388072.74954.bm@omp1056.mail.bf1.yahoo.com
Received: (qmail 60092 invoked by uid 60001); 19 May 2014 02:00:09 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.com; s=s1024; t=1400464809; bh=nGPu7R7FRxbCzWQ/NvxrhD96oCGXPu49eATIfexsJuA=; h=References:Message-ID:Date:From:Reply-To:Subject:To:In-Reply-To:MIME-Version:Content-Type; b=bfRQ9FxtV8TX8jFtpwd+C1AuvTrE1nTgR++0767hx2mWnlU7aALwRa4nMklOnF7sDVr7fqjdzzRKhrHqIIkUA0YDfO4wJxJQnPSxy86v3c0FBLEAexH2kIFDcC3gj3X2m1WROF98UhDOt9w5x7VCVLVyPXjzcCJxnIBoQ+j14kk=
X-YMail-OSG: 68Tiy.wVM1kXnNZ66AyVuKJ5dMdvx8ru55ZX17l3Uk9v7N2
 P_1VflTRb46DeGAffaBXS0kIQ8PShQjRS0mzbINJ6lp_lm9EtJOyzBKgaOBp
 oc7CAFeCPWwCfxOJM7wUI9v_Ig9vNdV_00m2LmKmfmcZbiZo1wi6en02j8jI
 _kkQLT9JonPYkjlhQA9ECZ0YhuVwh9WAb1IvA0jOkkMEIMBWDL6PSIAYJU59
 dcl1Y7FHkiY09lXiSVjOqPartnAtnmS4f_aeER_QzYP9mIYjh9_AQvAOBd5L
 r1neWGcVihdDMznecbBReLKPLbCscAxz8YjGR5Ldvb2F3QhwQOjTn.M2ksI7
 GjKGBszGdVlXprJMI61gyvUUQCQqrW9hFb5x9DvkDWiILYyiA7JjUN5zGxru
 wP9wkYl5kR3zL5k9JPPxNfiKDEPL2drozu2NmXufsQ99DMPbVDQ.MMDab8vC
 VOXoAEBQWDkTYtI1TLYncYDARe0IngXud_G51XKPMjzG.ZFdX4kmuMKj4vH4
 1UbvF8iSq836teanixAx32byduVkTpIBUPjo.QQ25Ptb2uXRCEjCP6xpFxcT
 BI0RU_YvSr4e5FQfMAre8oRAoCtZJJXpimA682enMosvj.m.eJ_kz_vANgty
 LADGyCzobzvXQw1_T4PYC7cy495Em8wQbn1a_G6cnHPW3kD.bIgkv_2sYQg3
 jsZVh
Received: from [70.194.101.202] by web140103.mail.bf1.yahoo.com via HTTP; Sun, 18 May 2014 19:00:09 PDT
X-Rocket-MIMEInfo: 002.001,bm8gaWRlYXMgb2ZmIGhhbmQsIEknbGwgdGFrZSBhIGxvb2sgdG9tb3Jyb3cuCgpUb20KCgpPbiBTdW5kYXksIE1heSAxOCwgMjAxNCA3OjI4IFBNLCBNYXRlaSBaYWhhcmlhIDxtYXRlaS56YWhhcmlhQGdtYWlsLmNvbT4gd3JvdGU6CiAKCgpBbHJpZ2h0LCBJ4oCZdmUgb3BlbmVkIGh0dHBzOi8vZ2l0aHViLmNvbS9hcGFjaGUvc3BhcmsvcHVsbC84MTkgd2l0aCB0aGUgV2luZG93cyBmaXhlcy4gSSBhbHNvIGZvdW5kIG9uZSBvdGhlciBsaWtlbHkgYnVnLCBodHRwczovL2lzc3Vlcy5hcGFjaGUub3JnL2ppcmEBMAEBAQE-
X-Mailer: YahooMailWebService/0.8.188.663
References: <CABPQxstaxcbGSQTqUxSSUQfRpjQs=BqRkrEVvzOXbXZo8kzShg@mail.gmail.com> <CABPQxstbwN64rQvx6kUoESkO4GJOohYSAO+s_F85wFWjb6T1zw@mail.gmail.com> <CAAsvFPn2RwpFMz7ajy=t8GSjiKSPqARgDtPqXk007Cd7_xvA-Q@mail.gmail.com> <CAMJOb8nTR38Uf50xVxv5O+OrBkO0ELDFYiHrziBVm9qknAgpYA@mail.gmail.com> <CACBYxKLR3xE4FSqZQwrjmP+bhhqy_mAat0hHSu+1pOBEVTttrA@mail.gmail.com> <9849C2AA-3131-4E31-85DF-7F6D61632545@gmail.com> <4D62B3E0-B6E7-4916-953D-A196FC6BC24F@gmail.com>
Message-ID: <1400464809.76189.YahooMailNeo@web140103.mail.bf1.yahoo.com>
Date: Sun, 18 May 2014 19:00:09 -0700 (PDT)
From: Tom Graves <tgraves_cs@yahoo.com>
Reply-To: Tom Graves <tgraves_cs@yahoo.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc9)
To: "dev@spark.apache.org" <dev@spark.apache.org>
In-Reply-To: <4D62B3E0-B6E7-4916-953D-A196FC6BC24F@gmail.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="-916770207-507177574-1400464809=:76189"
X-Virus-Checked: Checked by ClamAV on apache.org

---916770207-507177574-1400464809=:76189
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

no ideas off hand, I'll take a look tomorrow.=0A=0ATom=0A=0A=0AOn Sunday, M=
ay 18, 2014 7:28 PM, Matei Zaharia <matei.zaharia@gmail.com> wrote:=0A =0A=
=0A=0AAlright, I=E2=80=99ve opened https://github.com/apache/spark/pull/819=
 with the Windows fixes. I also found one other likely bug, https://issues.=
apache.org/jira/browse/SPARK-1875, in the binary packages for Hadoop1 built=
 in this RC. I think this is due to Hadoop 1=E2=80=99s security code depend=
ing on a different version of org.apache.commons than Hadoop 2, but it need=
s investigation. Tom, any thoughts on this?=0A=0AMatei=0A=0A=0AOn May 18, 2=
014, at 12:33 PM, Matei Zaharia <matei.zaharia@gmail.com> wrote:=0A=0A> I t=
ook the always fun task of testing it on Windows, and unfortunately, I foun=
d some small problems with the prebuilt packages due to recent changes to t=
he launch scripts: bin/spark-class2.cmd looks in ./jars instead of ./lib fo=
r the assembly JAR, and bin/run-example2.cmd doesn=E2=80=99t quite match th=
e master-setting behavior of the Unix based one. I=E2=80=99ll send a pull r=
equest to fix them soon.=0A> =0A> Matei=0A> =0A> =0A> On May 17, 2014, at 1=
1:32 AM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:=0A> =0A>> +1=0A>> =0A>=
> Reran my tests from rc5:=0A>> =0A>> * Built the release from source.=0A>>=
 * Compiled Java and Scala apps that interact with HDFS against it.=0A>> * =
Ran them in local mode.=0A>> * Ran them against a pseudo-distributed YARN c=
luster in both yarn-client=0A>> mode and yarn-cluster mode.=0A>> =0A>> =0A>=
> On Sat, May 17, 2014 at 10:08 AM, Andrew Or <andrew@databricks.com> wrote=
:=0A>> =0A>>> +1=0A>>> =0A>>> =0A>>> 2014-05-17 8:53 GMT-07:00 Mark Hamstra=
 <mark@clearstorydata.com>:=0A>>> =0A>>>> +1=0A>>>> =0A>>>> =0A>>>> On Sat,=
 May 17, 2014 at 12:58 AM, Patrick Wendell <pwendell@gmail.com=0A>>>>> wrot=
e:=0A>>>> =0A>>>>> I'll start the voting with a +1.=0A>>>>> =0A>>>>> On Sat=
, May 17, 2014 at 12:58 AM, Patrick Wendell <pwendell@gmail.com>=0A>>>>> wr=
ote:=0A>>>>>> Please vote on releasing the following candidate as Apache Sp=
ark=0A>>>> version=0A>>>>> 1.0.0!=0A>>>>>> This has one bug fix and one min=
or feature on top of rc8:=0A>>>>>> SPARK-1864: https://github.com/apache/sp=
ark/pull/808=0A>>>>>> SPARK-1808: https://github.com/apache/spark/pull/799=
=0A>>>>>> =0A>>>>>> The tag to be voted on is v1.0.0-rc9 (commit 920f947):=
=0A>>>>>> =0A>>>>> =0A>>>> =0A>>> https://git-wip-us.apache.org/repos/asf?p=
=3Dspark.git;a=3Dcommit;h=3D920f947eb5a22a679c0c3186cf69ee75f6041c75=0A>>>>=
>> =0A>>>>>> The release files, including signatures, digests, etc. can be =
found=0A>>> at:=0A>>>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc9=
/=0A>>>>>> =0A>>>>>> Release artifacts are signed with the following key:=
=0A>>>>>> https://people.apache.org/keys/committer/pwendell.asc=0A>>>>>> =
=0A>>>>>> The staging repository for this release can be found at:=0A>>>>>>=
 =0A>>>> https://repository.apache.org/content/repositories/orgapachespark-=
1017/=0A>>>>>> =0A>>>>>> The documentation corresponding to this release ca=
n be found at:=0A>>>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc9-=
docs/=0A>>>>>> =0A>>>>>> Please vote on releasing this package as Apache Sp=
ark 1.0.0!=0A>>>>>> =0A>>>>>> The vote is open until Tuesday, May 20, at 08=
:56 UTC and passes if=0A>>>>>> amajority of at least 3 +1 PMC votes are cas=
t.=0A>>>>>> =0A>>>>>> [ ] +1 Release this package as Apache Spark 1.0.0=0A>=
>>>>> [ ] -1 Do not release this package because ...=0A>>>>>> =0A>>>>>> To =
learn more about Apache Spark, please see=0A>>>>>> http://spark.apache.org/=
=0A>>>>>> =0A>>>>>> =3D=3D API Changes =3D=3D=0A>>>>>> We welcome users to =
compile Spark applications against 1.0. There are=0A>>>>>> a few API change=
s in this release. Here are links to the associated=0A>>>>>> upgrade guides=
 - user facing changes have been kept as small as=0A>>>>>> possible.=0A>>>>=
>> =0A>>>>>> changes to ML vector specification:=0A>>>>>> =0A>>>>> =0A>>>> =
=0A>>> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.=
html#from-09-to-10=0A>>>>>> =0A>>>>>> changes to the Java API:=0A>>>>>> =0A=
>>>>> =0A>>>> =0A>>> http://people.apache.org/~pwendell/spark-1.0.0-rc8-doc=
s/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark=0A>>>=
>>> =0A>>>>>> changes to the streaming API:=0A>>>>>> =0A>>>>> =0A>>>> =0A>>=
> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-program=
ming-guide.html#migration-guide-from-091-or-below-to-1x=0A>>>>>> =0A>>>>>> =
changes to the GraphX API:=0A>>>>>> =0A>>>>> =0A>>>> =0A>>> http://people.a=
pache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming-guide.html#upgr=
ade-guide-from-spark-091=0A>>>>>> =0A>>>>>> coGroup and related functions n=
ow return Iterable[T] instead of=0A>>> Seq[T]=0A>>>>>> =3D=3D> Call toSeq o=
n the result to restore the old behavior=0A>>>>>> =0A>>>>>> SparkContext.ja=
rOfClass returns Option[String] instead of Seq[String]=0A>>>>>> =3D=3D> Cal=
l toSeq on the result to restore old behavior=0A>>>>> =0A>>>> =0A>>> =0A> 
---916770207-507177574-1400464809=:76189--

From dev-return-7688-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 02:08:53 2014
Return-Path: <dev-return-7688-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0493311ADE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 02:08:53 +0000 (UTC)
Received: (qmail 67107 invoked by uid 500); 19 May 2014 02:08:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67046 invoked by uid 500); 19 May 2014 02:08:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67038 invoked by uid 99); 19 May 2014 02:08:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 02:08:52 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.44 as permitted sender)
Received: from [209.85.219.44] (HELO mail-oa0-f44.google.com) (209.85.219.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 02:08:49 +0000
Received: by mail-oa0-f44.google.com with SMTP id o6so5500038oag.17
        for <dev@spark.apache.org>; Sun, 18 May 2014 19:08:28 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=eeiWxLCafB3l153ahbYR2YKtt7ba6DwEHJP7tnSRnkU=;
        b=Vlzko/IKwKgzDx8aFNJw2ACEy8P0GY+r3MUrHie+USozJVd3cgkr4VI024JnsPlvXX
         nqKiiGU2NYkEY868anpTTpopz2ExfOwpTXK1oZ3j8Gpl5IDigAxKUApySzg4P+fhmsCp
         b0xJH6xUHm0fggkiBxrnO4XYL/AtLY6YTQmzDLu6q0+fXAJVsvRWjDY7qFjLoG5/BfbH
         jJzK+J3ueGQdoyZUVyE5+iWLl0dYZMCVrF0ICx3YkcfAyRGgGiOkUfOL5KtrRuGtRJXF
         H6JBxV39pD02F6Xa385xJePaJTY/Qh350pq2G1uT9ATeBcRL1pAE0VsZndHqq3riKvVM
         I6Qw==
MIME-Version: 1.0
X-Received: by 10.182.105.1 with SMTP id gi1mr32986006obb.9.1400465308147;
 Sun, 18 May 2014 19:08:28 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Sun, 18 May 2014 19:08:28 -0700 (PDT)
In-Reply-To: <4D62B3E0-B6E7-4916-953D-A196FC6BC24F@gmail.com>
References: <CABPQxstaxcbGSQTqUxSSUQfRpjQs=BqRkrEVvzOXbXZo8kzShg@mail.gmail.com>
	<CABPQxstbwN64rQvx6kUoESkO4GJOohYSAO+s_F85wFWjb6T1zw@mail.gmail.com>
	<CAAsvFPn2RwpFMz7ajy=t8GSjiKSPqARgDtPqXk007Cd7_xvA-Q@mail.gmail.com>
	<CAMJOb8nTR38Uf50xVxv5O+OrBkO0ELDFYiHrziBVm9qknAgpYA@mail.gmail.com>
	<CACBYxKLR3xE4FSqZQwrjmP+bhhqy_mAat0hHSu+1pOBEVTttrA@mail.gmail.com>
	<9849C2AA-3131-4E31-85DF-7F6D61632545@gmail.com>
	<4D62B3E0-B6E7-4916-953D-A196FC6BC24F@gmail.com>
Date: Sun, 18 May 2014 19:08:28 -0700
Message-ID: <CABPQxssnZ=aq6djG-YL=CtajhRhnT945qGcNhAq6RZEdDDU0tA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc9)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: Tom Graves <tgraves_cs@yahoo.com>
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Matei - the issue you found is not related to security. This patch
a few days ago broke builds for Hadoop 1 with YARN support enabled.
The patch directly altered the way we deal with commons-lang
dependency, which is what is at the base of this stack trace.

https://github.com/apache/spark/pull/754

- Patrick

On Sun, May 18, 2014 at 5:28 PM, Matei Zaharia <matei.zaharia@gmail.com> wr=
ote:
> Alright, I've opened https://github.com/apache/spark/pull/819 with the Wi=
ndows fixes. I also found one other likely bug, https://issues.apache.org/j=
ira/browse/SPARK-1875, in the binary packages for Hadoop1 built in this RC.=
 I think this is due to Hadoop 1's security code depending on a different v=
ersion of org.apache.commons than Hadoop 2, but it needs investigation. Tom=
, any thoughts on this?
>
> Matei
>
> On May 18, 2014, at 12:33 PM, Matei Zaharia <matei.zaharia@gmail.com> wro=
te:
>
>> I took the always fun task of testing it on Windows, and unfortunately, =
I found some small problems with the prebuilt packages due to recent change=
s to the launch scripts: bin/spark-class2.cmd looks in ./jars instead of ./=
lib for the assembly JAR, and bin/run-example2.cmd doesn't quite match the =
master-setting behavior of the Unix based one. I'll send a pull request to =
fix them soon.
>>
>> Matei
>>
>>
>> On May 17, 2014, at 11:32 AM, Sandy Ryza <sandy.ryza@cloudera.com> wrote=
:
>>
>>> +1
>>>
>>> Reran my tests from rc5:
>>>
>>> * Built the release from source.
>>> * Compiled Java and Scala apps that interact with HDFS against it.
>>> * Ran them in local mode.
>>> * Ran them against a pseudo-distributed YARN cluster in both yarn-clien=
t
>>> mode and yarn-cluster mode.
>>>
>>>
>>> On Sat, May 17, 2014 at 10:08 AM, Andrew Or <andrew@databricks.com> wro=
te:
>>>
>>>> +1
>>>>
>>>>
>>>> 2014-05-17 8:53 GMT-07:00 Mark Hamstra <mark@clearstorydata.com>:
>>>>
>>>>> +1
>>>>>
>>>>>
>>>>> On Sat, May 17, 2014 at 12:58 AM, Patrick Wendell <pwendell@gmail.com
>>>>>> wrote:
>>>>>
>>>>>> I'll start the voting with a +1.
>>>>>>
>>>>>> On Sat, May 17, 2014 at 12:58 AM, Patrick Wendell <pwendell@gmail.co=
m>
>>>>>> wrote:
>>>>>>> Please vote on releasing the following candidate as Apache Spark
>>>>> version
>>>>>> 1.0.0!
>>>>>>> This has one bug fix and one minor feature on top of rc8:
>>>>>>> SPARK-1864: https://github.com/apache/spark/pull/808
>>>>>>> SPARK-1808: https://github.com/apache/spark/pull/799
>>>>>>>
>>>>>>> The tag to be voted on is v1.0.0-rc9 (commit 920f947):
>>>>>>>
>>>>>>
>>>>>
>>>> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D9=
20f947eb5a22a679c0c3186cf69ee75f6041c75
>>>>>>>
>>>>>>> The release files, including signatures, digests, etc. can be found
>>>> at:
>>>>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc9/
>>>>>>>
>>>>>>> Release artifacts are signed with the following key:
>>>>>>> https://people.apache.org/keys/committer/pwendell.asc
>>>>>>>
>>>>>>> The staging repository for this release can be found at:
>>>>>>>
>>>>> https://repository.apache.org/content/repositories/orgapachespark-101=
7/
>>>>>>>
>>>>>>> The documentation corresponding to this release can be found at:
>>>>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc9-docs/
>>>>>>>
>>>>>>> Please vote on releasing this package as Apache Spark 1.0.0!
>>>>>>>
>>>>>>> The vote is open until Tuesday, May 20, at 08:56 UTC and passes if
>>>>>>> amajority of at least 3 +1 PMC votes are cast.
>>>>>>>
>>>>>>> [ ] +1 Release this package as Apache Spark 1.0.0
>>>>>>> [ ] -1 Do not release this package because ...
>>>>>>>
>>>>>>> To learn more about Apache Spark, please see
>>>>>>> http://spark.apache.org/
>>>>>>>
>>>>>>> =3D=3D API Changes =3D=3D
>>>>>>> We welcome users to compile Spark applications against 1.0. There a=
re
>>>>>>> a few API changes in this release. Here are links to the associated
>>>>>>> upgrade guides - user facing changes have been kept as small as
>>>>>>> possible.
>>>>>>>
>>>>>>> changes to ML vector specification:
>>>>>>>
>>>>>>
>>>>>
>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.ht=
ml#from-09-to-10
>>>>>>>
>>>>>>> changes to the Java API:
>>>>>>>
>>>>>>
>>>>>
>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programmi=
ng-guide.html#upgrading-from-pre-10-versions-of-spark
>>>>>>>
>>>>>>> changes to the streaming API:
>>>>>>>
>>>>>>
>>>>>
>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-prog=
ramming-guide.html#migration-guide-from-091-or-below-to-1x
>>>>>>>
>>>>>>> changes to the GraphX API:
>>>>>>>
>>>>>>
>>>>>
>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-program=
ming-guide.html#upgrade-guide-from-spark-091
>>>>>>>
>>>>>>> coGroup and related functions now return Iterable[T] instead of
>>>> Seq[T]
>>>>>>> =3D=3D> Call toSeq on the result to restore the old behavior
>>>>>>>
>>>>>>> SparkContext.jarOfClass returns Option[String] instead of Seq[Strin=
g]
>>>>>>> =3D=3D> Call toSeq on the result to restore old behavior
>>>>>>
>>>>>
>>>>
>>
>

From dev-return-7689-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 03:08:30 2014
Return-Path: <dev-return-7689-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 212EC11BCD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 03:08:30 +0000 (UTC)
Received: (qmail 99537 invoked by uid 500); 19 May 2014 03:08:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99474 invoked by uid 500); 19 May 2014 03:08:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99466 invoked by uid 99); 19 May 2014 03:08:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 03:08:29 +0000
X-ASF-Spam-Status: No, hits=4.3 required=10.0
	tests=FREEMAIL_REPLY,FROM_EXCESS_BASE64,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_SOFTFAIL
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of witgo@qq.com does not designate 103.7.29.139 as permitted sender)
Received: from [103.7.29.139] (HELO smtpbg64.qq.com) (103.7.29.139)
    by apache.org (qpsmtpd/0.29) with SMTP; Mon, 19 May 2014 03:08:26 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=qq.com; s=s201307;
	t=1400468876; bh=P5ZJvrRCtEymeZFkkvkEtde053fwbas98kvcn/fMlQk=;
	h=X-QQ-FEAT:X-QQ-SSF:X-HAS-ATTACH:X-QQ-BUSINESS-ORIGIN:
	 X-Originating-IP:In-Reply-To:References:X-QQ-STYLE:X-QQ-mid:From:To:Subject:Mime-Version:Content-Type:Content-Transfer-Encoding:Date:
	 X-Priority:Message-ID:X-QQ-MIME:X-Mailer:X-QQ-Mailer:
	 X-QQ-ReplyHash:X-QQ-SENDSIZE;
	b=GP4qEQfu7yDodHfAfWl2fMGNVN0+jG9FnIcWoGoQUIhfk3iHUq5kIMYmS7CP9bfya
	 S1S85KyijixKoIkztV9yCI7v82gK0NiMTRqQSU6lJdcr3XB8dqPSM9tQRNRMssDwNc
	 RplD1mb1fojgLrJTcYVLZ1cpgkpsXnszFBPtfYPE=
X-QQ-FEAT: R7i4HJoRCHVtMds8V68HZJFHn3FtA8NphLrnmfM+teyqEFA+T2Wi0SjVYogxU
	8ZQtcE7JcKmJDpXu9bWhreveEDxrbYinczH7WTmpfs2Tu1MtKeQuqtJmRJKt3k+iyrI1BK3
	Ct78HEDjQ5HxNJcWul8KaXPu+SIDERzHQhnGfjc=
X-QQ-SSF: 000000000000001000000000000000M
X-HAS-ATTACH: no
X-QQ-BUSINESS-ORIGIN: 2
X-Originating-IP: 219.142.170.212
In-Reply-To: <CABPQxssnZ=aq6djG-YL=CtajhRhnT945qGcNhAq6RZEdDDU0tA@mail.gmail.com>
References: <CABPQxstaxcbGSQTqUxSSUQfRpjQs=BqRkrEVvzOXbXZo8kzShg@mail.gmail.com>
	<CABPQxstbwN64rQvx6kUoESkO4GJOohYSAO+s_F85wFWjb6T1zw@mail.gmail.com>
	<CAAsvFPn2RwpFMz7ajy=t8GSjiKSPqARgDtPqXk007Cd7_xvA-Q@mail.gmail.com>
	<CAMJOb8nTR38Uf50xVxv5O+OrBkO0ELDFYiHrziBVm9qknAgpYA@mail.gmail.com>
	<CACBYxKLR3xE4FSqZQwrjmP+bhhqy_mAat0hHSu+1pOBEVTttrA@mail.gmail.com>
	<9849C2AA-3131-4E31-85DF-7F6D61632545@gmail.com>
	<4D62B3E0-B6E7-4916-953D-A196FC6BC24F@gmail.com>
	<CABPQxssnZ=aq6djG-YL=CtajhRhnT945qGcNhAq6RZEdDDU0tA@mail.gmail.com>
X-QQ-STYLE: 
X-QQ-mid: webmail421t1400468875t160686
From: "=?ISO-8859-1?B?d2l0Z28=?=" <witgo@qq.com>
To: "=?ISO-8859-1?B?UGF0cmljayBXZW5kZWxs?=" <dev@spark.apache.org>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc9)
Mime-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_5379758B_08C10810_2427A648"
Content-Transfer-Encoding: 8Bit
Date: Mon, 19 May 2014 11:07:55 +0800
X-Priority: 3
Message-ID: <tencent_60271E3C31F67D7B32E81699@qq.com>
X-QQ-MIME: TCMime 1.0 by Tencent
X-Mailer: QQMail 2.x
X-QQ-Mailer: QQMail 2.x
X-QQ-ReplyHash: 1693816135
X-QQ-SENDSIZE: 520
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_5379758B_08C10810_2427A648
Content-Type: text/plain;
	charset="ISO-8859-1"
Content-Transfer-Encoding: base64
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------=_NextPart_5379758B_08C10810_2427A648--


From dev-return-7690-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 04:16:22 2014
Return-Path: <dev-return-7690-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D83EB11C9B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 04:16:22 +0000 (UTC)
Received: (qmail 74050 invoked by uid 500); 19 May 2014 04:16:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73985 invoked by uid 500); 19 May 2014 04:16:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73977 invoked by uid 99); 19 May 2014 04:16:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 04:16:22 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.192.42 as permitted sender)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 04:16:17 +0000
Received: by mail-qg0-f42.google.com with SMTP id q107so8055587qgd.1
        for <dev@spark.apache.org>; Sun, 18 May 2014 21:15:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=JW/OB3NaqoWxcVTTudIBAov7cXleduO76o0LBvOG2WE=;
        b=wlljcnOgrNv0DkI3pMLQM6edCBYg74RFxuWRjDlEbkBUyVJoVsJ50bnXQCfKsdaKtQ
         JjCmlwsXK0teKEehYMmiY2zXSQuCmdaSOiGcgg6yBD8pzgY/Ueq1m3NofiNCigQP5e43
         rl4VzZ8H4NNxnrhPA4ic3/j2rAk0TnCJZUeGEW+sGxn1zmHXO+3q3jL3uSQycHsLIVQo
         +ys5HCIkHQzH1Ww6THdmFsun+s4hvHVtDhjCCF5DCal9B5k8xqetUnzQxNSdYu4QU7xT
         CjIeGJnjSynN9wyOYCm3KVOyxY9UyXIjlhrA26aDkxiEvGKiQe4Wk44vYQ9T+x4+Wieq
         yg7Q==
MIME-Version: 1.0
X-Received: by 10.224.60.137 with SMTP id p9mr42876273qah.92.1400472956319;
 Sun, 18 May 2014 21:15:56 -0700 (PDT)
Received: by 10.140.106.34 with HTTP; Sun, 18 May 2014 21:15:56 -0700 (PDT)
In-Reply-To: <6EF34F88-F79D-49B0-92FD-C421FBB0B6ED@gmail.com>
References: <JIRA.12714626.1400189782796@arcas>
	<JIRA.12714626.1400189782796.363092.1400238099511@arcas>
	<CAJiQeY+zsu3cfX1zyA=qK1KO8hirddmjGSB7HohTTA_a9jk3wQ@mail.gmail.com>
	<CAJgQjQ_1qrz4=05E=Wwj+O4UqKHwFKWQQiZosKbPjvwocMz9mw@mail.gmail.com>
	<CAJiQeY+eGxSzM-6C_poKCY53Gkixg85rdXiXXx+qi0z0bvCrrQ@mail.gmail.com>
	<6EF34F88-F79D-49B0-92FD-C421FBB0B6ED@gmail.com>
Date: Mon, 19 May 2014 09:45:56 +0530
Message-ID: <CAJiQeYL5-F+C-uoq1YjfcXHK2-jCcPt2w7WsLGySXEEgUDvbVg@mail.gmail.com>
Subject: Re: [jira] [Created] (SPARK-1855) Provide memory-and-local-disk RDD checkpointing
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

My bad ... I was replying via mobile, and I did not realize responses
to JIRA mails were not mirrored to JIRA - unlike PR responses !


Regards,
Mridul

On Sun, May 18, 2014 at 2:50 AM, Matei Zaharia <matei.zaharia@gmail.com> wrote:
> We do actually have replicated StorageLevels in Spark. You can use MEMORY_AND_DISK_2 or construct your own StorageLevel with your own custom replication factor.
>
> BTW you guys should probably have this discussion on the JIRA rather than the dev list; I think the replies somehow ended up on the dev list.
>
> Matei
>
> On May 17, 2014, at 1:36 AM, Mridul Muralidharan <mridul@gmail.com> wrote:
>
>> We don't have 3x replication in spark :-)
>> And if we use replicated storagelevel, while decreasing odds of failure, it
>> does not eliminate it (since we are not doing a great job with replication
>> anyway from fault tolerance point of view).
>> Also it does take a nontrivial performance hit with replicated levels.
>>
>> Regards,
>> Mridul
>> On 17-May-2014 8:16 am, "Xiangrui Meng" <mengxr@gmail.com> wrote:
>>
>>> With 3x replication, we should be able to achieve fault tolerance.
>>> This checkPointed RDD can be cleared if we have another in-memory
>>> checkPointed RDD down the line. It can avoid hitting disk if we have
>>> enough memory to use. We need to investigate more to find a good
>>> solution. -Xiangrui
>>>
>>> On Fri, May 16, 2014 at 4:00 PM, Mridul Muralidharan <mridul@gmail.com>
>>> wrote:
>>>> Effectively this is persist without fault tolerance.
>>>> Failure of any node means complete lack of fault tolerance.
>>>> I would be very skeptical of truncating lineage if it is not reliable.
>>>> On 17-May-2014 3:49 am, "Xiangrui Meng (JIRA)" <jira@apache.org> wrote:
>>>>
>>>>> Xiangrui Meng created SPARK-1855:
>>>>> ------------------------------------
>>>>>
>>>>>             Summary: Provide memory-and-local-disk RDD checkpointing
>>>>>                 Key: SPARK-1855
>>>>>                 URL: https://issues.apache.org/jira/browse/SPARK-1855
>>>>>             Project: Spark
>>>>>          Issue Type: New Feature
>>>>>          Components: MLlib, Spark Core
>>>>>    Affects Versions: 1.0.0
>>>>>            Reporter: Xiangrui Meng
>>>>>
>>>>>
>>>>> Checkpointing is used to cut long lineage while maintaining fault
>>>>> tolerance. The current implementation is HDFS-based. Using the BlockRDD
>>> we
>>>>> can create in-memory-and-local-disk (with replication) checkpoints that
>>> are
>>>>> not as reliable as HDFS-based solution but faster.
>>>>>
>>>>> It can help applications that require many iterations.
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> This message was sent by Atlassian JIRA
>>>>> (v6.2#6252)
>>>>>
>>>
>

From dev-return-7691-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 06:58:06 2014
Return-Path: <dev-return-7691-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5087911FC3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 06:58:06 +0000 (UTC)
Received: (qmail 29749 invoked by uid 500); 19 May 2014 06:58:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 29690 invoked by uid 500); 19 May 2014 06:58:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 29682 invoked by uid 99); 19 May 2014 06:58:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 06:58:06 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.220.177 as permitted sender)
Received: from [209.85.220.177] (HELO mail-vc0-f177.google.com) (209.85.220.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 06:58:01 +0000
Received: by mail-vc0-f177.google.com with SMTP id if17so8976719vcb.22
        for <dev@spark.apache.org>; Sun, 18 May 2014 23:57:41 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=AZWtDHVidqLT8j10UUnoXB72m0CQWs/qn8z1SVHN38Q=;
        b=hHuAU19KRJYdoC9C2snQVDyVcrJ860+pGDHPezI/VAQu1MtEzZXd+dCpgdR5emslUs
         NO6m4MffvGeFkUrSto04bCWrDxunQv2Ct0HU6wmNVBOLjAgHnDcYQZ1dTuV0AVGCPKI1
         v8MM8CmQ/M+rwUaGKDwMnGJKxJYbNWqJQrd5Z1caUiNNDeWS573bQJTm2SNoSdKXDJUw
         OOjiVQslEUFGCmtv85+pCLzdQt6f8HtnHCWoNB0IsYczKT+h4msiyc8/7Xs7dM53jYuw
         +/GqSQmUo4QQkUruSRj9zgu3PVH/qy6HDgtFEEAxyzfKm87FE4c0M/+Pgo72EVEZykHf
         gvXg==
X-Gm-Message-State: ALoCoQnQ9BBSj8+UIR2UmarBU2687E8CQfJDGI7ICnUBOsqnkbReoGx90/yHDHzFfsX19ZicPRzL
X-Received: by 10.58.116.1 with SMTP id js1mr16854892veb.29.1400482660922;
 Sun, 18 May 2014 23:57:40 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.111.69 with HTTP; Sun, 18 May 2014 23:57:20 -0700 (PDT)
In-Reply-To: <CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
 <CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
 <CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
 <CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
 <CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
 <CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
 <CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com> <CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 19 May 2014 07:57:20 +0100
Message-ID: <CAMAsSdKxE-biW8fRqkRn6NmqsszFgQ=f1w_Z8yt8cSOj3qBB2Q@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I might be stating the obvious for everyone, but the issue here is not
reflection or the source of the JAR, but the ClassLoader. The basic
rules are this.

"new Foo" will use the ClassLoader that defines Foo. This is usually
the ClassLoader that loaded whatever it is that first referenced Foo
and caused it to be loaded -- usually the ClassLoader holding your
other app classes.

ClassLoaders can have a parent-child relationship. ClassLoaders always
look in their parent before themselves.

(Careful then -- in contexts like Hadoop or Tomcat where your app is
loaded in a child ClassLoader, and you reference a class that Hadoop
or Tomcat also has (like a lib class) you will get the container's
version!)

When you load an external JAR it has a separate ClassLoader which does
not necessarily bear any relation to the one containing your app
classes, so yeah it is not generally going to make "new Foo" work.

Reflection lets you pick the ClassLoader, yes.

I would not call setContextClassLoader.

On Mon, May 19, 2014 at 12:00 AM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
> I spoke with DB offline about this a little while ago and he confirmed that
> he was able to access the jar from the driver.
>
> The issue appears to be a general Java issue: you can't directly
> instantiate a class from a dynamically loaded jar.
>
> I reproduced it locally outside of Spark with:
> ---
>     URLClassLoader urlClassLoader = new URLClassLoader(new URL[] { new
> File("myotherjar.jar").toURI().toURL() }, null);
>     Thread.currentThread().setContextClassLoader(urlClassLoader);
>     MyClassFromMyOtherJar obj = new MyClassFromMyOtherJar();
> ---
>
> I was able to load the class with reflection.

From dev-return-7692-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 07:08:13 2014
Return-Path: <dev-return-7692-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F22CC11022
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 07:08:12 +0000 (UTC)
Received: (qmail 49204 invoked by uid 500); 19 May 2014 07:08:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49139 invoked by uid 500); 19 May 2014 07:08:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49131 invoked by uid 99); 19 May 2014 07:08:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 07:08:12 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.177] (HELO mail-vc0-f177.google.com) (209.85.220.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 07:08:09 +0000
Received: by mail-vc0-f177.google.com with SMTP id if17so8974856vcb.36
        for <dev@spark.apache.org>; Mon, 19 May 2014 00:07:46 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=juX7LUke80Hg5jMDAjDpVlKeLq5h0ll0+wWX+1x5kOU=;
        b=QN9rynxmfEmhSBqVcQ9IIc4sWERh5YQNGuSen02YiBQQ4Wl8g6+n/E0c/ByMS1upXx
         EiBqGkNu1hBwN54gKrWRQtDE/YxIn1jrvm/0sWWMg+vJIngAk8nUXc+cJjxEgITzLqAm
         A4Yf3j53xd07Lb0Vdyp6nDDh8xoTx3U7njH8G8lrpJxOO6VSED/TxEhRXxDFJeaZ4+EB
         8X0hLAdzuMLbcPmMfFfCKTKz00aJC9y8WfI4Zm3G8KTrfVJj1iPAVrg+bb1NIQ5X6bpR
         BvNkgpzloyGZw49PkAmXj33jD+d/u1H1TjLlXRmE0gmy2vk5vfYzKIy2HUa20g5e0CLN
         hHOw==
X-Gm-Message-State: ALoCoQkX2QklHaJwnZWFtX/kb8jVB67i0J7wCL8fqTtLSIokikNYVrB/kLMaKzYVLDDWxu0CfSSe
X-Received: by 10.53.5.67 with SMTP id ck3mr22434vdd.45.1400483265990;
        Mon, 19 May 2014 00:07:45 -0700 (PDT)
Received: from mail-ve0-f170.google.com (mail-ve0-f170.google.com [209.85.128.170])
        by mx.google.com with ESMTPSA id gi3sm18723806vec.14.2014.05.19.00.07.44
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 19 May 2014 00:07:44 -0700 (PDT)
Received: by mail-ve0-f170.google.com with SMTP id db11so5970529veb.15
        for <dev@spark.apache.org>; Mon, 19 May 2014 00:07:44 -0700 (PDT)
X-Received: by 10.52.175.69 with SMTP id by5mr10885066vdc.16.1400483264646;
 Mon, 19 May 2014 00:07:44 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Mon, 19 May 2014 00:07:24 -0700 (PDT)
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 19 May 2014 00:07:24 -0700
Message-ID: <CA+-p3AHJOsDSSyYBBNbuocbWTA88VQk5XTCG-DmW9ohdeCoyGA@mail.gmail.com>
Subject: TorrentBroadcast aka Cornet?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=bcaec5171e1f80d86c04f9bb6b66
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec5171e1f80d86c04f9bb6b66
Content-Type: text/plain; charset=UTF-8

Hi Spark devs,

Is the algorithm for
TorrentBroadcast<https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala>the
same as Cornet from the below paper?

http://www.mosharaf.com/wp-content/uploads/orchestra-sigcomm11.pdf

If so it would be nice to include a link to the paper in the Javadoc for
the class.

Thanks!
Andrew

--bcaec5171e1f80d86c04f9bb6b66--

From dev-return-7693-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 07:17:44 2014
Return-Path: <dev-return-7693-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1134C11083
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 07:17:44 +0000 (UTC)
Received: (qmail 72193 invoked by uid 500); 19 May 2014 07:17:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72132 invoked by uid 500); 19 May 2014 07:17:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72124 invoked by uid 99); 19 May 2014 07:17:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 07:17:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.81 as permitted sender)
Received: from [171.67.219.81] (HELO smtp.stanford.edu) (171.67.219.81)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 07:17:39 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 1E3C92126A
	for <dev@spark.apache.org>; Mon, 19 May 2014 00:17:19 -0700 (PDT)
Received: from mail-qc0-f175.google.com (mail-qc0-f175.google.com [209.85.216.175])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 8A7522126D
	for <dev@spark.apache.org>; Mon, 19 May 2014 00:17:18 -0700 (PDT)
Received: by mail-qc0-f175.google.com with SMTP id w7so8217838qcr.6
        for <dev@spark.apache.org>; Mon, 19 May 2014 00:17:17 -0700 (PDT)
X-Gm-Message-State: ALoCoQlrY8jeGvfG/1HyLQUZU2tbQqs6IgKfZGwQAr/51IdjsSmL4aFvecfo9UAJRbuQXDgDRl0L
MIME-Version: 1.0
X-Received: by 10.140.105.119 with SMTP id b110mr44439798qgf.28.1400483837756;
 Mon, 19 May 2014 00:17:17 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Mon, 19 May 2014 00:17:17 -0700 (PDT)
In-Reply-To: <CAMAsSdKxE-biW8fRqkRn6NmqsszFgQ=f1w_Z8yt8cSOj3qBB2Q@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
	<CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
	<CAMAsSdKxE-biW8fRqkRn6NmqsszFgQ=f1w_Z8yt8cSOj3qBB2Q@mail.gmail.com>
Date: Mon, 19 May 2014 00:17:17 -0700
Message-ID: <CAEYYnxY9UZDuhBoHAP5kD3dCXxd0TwR761tt-kswjTEddQCCKg@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11394402a9db6a04f9bb8db6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11394402a9db6a04f9bb8db6
Content-Type: text/plain; charset=UTF-8

Hi Sean,

It's true that the issue here is classloader, and due to the classloader
delegation model, users have to use reflection in the executors to pick up
the classloader in order to use those classes added by sc.addJars APIs.
However, it's very inconvenience for users, and not documented in spark.

I'm working on a patch to solve it by calling the protected method addURL
in URLClassLoader to update the current default classloader, so no
customClassLoader anymore. I wonder if this is an good way to go.

  private def addURL(url: URL, loader: URLClassLoader){
    try {
      val method: Method =
classOf[URLClassLoader].getDeclaredMethod("addURL", classOf[URL])
      method.setAccessible(true)
      method.invoke(loader, url)
    }
    catch {
      case t: Throwable => {
        throw new IOException("Error, could not add URL to system
classloader")
      }
    }
  }



Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Sun, May 18, 2014 at 11:57 PM, Sean Owen <sowen@cloudera.com> wrote:

> I might be stating the obvious for everyone, but the issue here is not
> reflection or the source of the JAR, but the ClassLoader. The basic
> rules are this.
>
> "new Foo" will use the ClassLoader that defines Foo. This is usually
> the ClassLoader that loaded whatever it is that first referenced Foo
> and caused it to be loaded -- usually the ClassLoader holding your
> other app classes.
>
> ClassLoaders can have a parent-child relationship. ClassLoaders always
> look in their parent before themselves.
>
> (Careful then -- in contexts like Hadoop or Tomcat where your app is
> loaded in a child ClassLoader, and you reference a class that Hadoop
> or Tomcat also has (like a lib class) you will get the container's
> version!)
>
> When you load an external JAR it has a separate ClassLoader which does
> not necessarily bear any relation to the one containing your app
> classes, so yeah it is not generally going to make "new Foo" work.
>
> Reflection lets you pick the ClassLoader, yes.
>
> I would not call setContextClassLoader.
>
> On Mon, May 19, 2014 at 12:00 AM, Sandy Ryza <sandy.ryza@cloudera.com>
> wrote:
> > I spoke with DB offline about this a little while ago and he confirmed
> that
> > he was able to access the jar from the driver.
> >
> > The issue appears to be a general Java issue: you can't directly
> > instantiate a class from a dynamically loaded jar.
> >
> > I reproduced it locally outside of Spark with:
> > ---
> >     URLClassLoader urlClassLoader = new URLClassLoader(new URL[] { new
> > File("myotherjar.jar").toURI().toURL() }, null);
> >     Thread.currentThread().setContextClassLoader(urlClassLoader);
> >     MyClassFromMyOtherJar obj = new MyClassFromMyOtherJar();
> > ---
> >
> > I was able to load the class with reflection.
>

--001a11394402a9db6a04f9bb8db6--

From dev-return-7694-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 07:27:12 2014
Return-Path: <dev-return-7694-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 69AAF110E7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 07:27:12 +0000 (UTC)
Received: (qmail 9905 invoked by uid 500); 19 May 2014 07:27:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9840 invoked by uid 500); 19 May 2014 07:27:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9832 invoked by uid 99); 19 May 2014 07:27:12 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 07:27:12 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.170] (HELO mail-vc0-f170.google.com) (209.85.220.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 07:27:08 +0000
Received: by mail-vc0-f170.google.com with SMTP id lf12so9171485vcb.29
        for <dev@spark.apache.org>; Mon, 19 May 2014 00:26:47 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=ocP9AAkBm5jGVZHy2hpkBPcpZyuFo3ZCJMEkpvwm5uw=;
        b=Ry8u8mhHxvgX6EVREK+ANxgBewzxdzZv58Mxe4AiNMUF6Os56+/MvTcdPIrdba4VhW
         5f9OOU5CWsdTUZMffCLdeR3INhd5DL5BjMYgfz5Q9za6T7ww/1hHaq7xYsSD6EvyRprR
         cwZXwiOKqqFUJAmFYwsbO6FUSWWKjn+QhWkGt3L1QDgmmYhScrRdcwvHoGZ1GRTPW/cf
         Uc36FEy1wKhM0L15jzxn/c5W/mZI8YigmdBZwcUbGqf4zlHoDOxh7EhZncqOgTLUAYlr
         dOzKQPHj4ukmjqIjiwQ7e8HFFykZHJ6iyOoZUDyW2jDjbo05t52Ns2hw9VLgFw11As3d
         aKOw==
X-Gm-Message-State: ALoCoQnVBrz02dF/xCYHRLcbhfKOAeXRBQSVxaPxhJCSshHQBB6PEoDEzmB9AGgSiclmpBo5QxYe
X-Received: by 10.221.64.80 with SMTP id xh16mr5553992vcb.35.1400484407130;
        Mon, 19 May 2014 00:26:47 -0700 (PDT)
Received: from mail-ve0-f178.google.com (mail-ve0-f178.google.com [209.85.128.178])
        by mx.google.com with ESMTPSA id j8sm18770259vet.7.2014.05.19.00.26.46
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 19 May 2014 00:26:46 -0700 (PDT)
Received: by mail-ve0-f178.google.com with SMTP id sa20so6129972veb.9
        for <dev@spark.apache.org>; Mon, 19 May 2014 00:26:45 -0700 (PDT)
X-Received: by 10.58.134.101 with SMTP id pj5mr108088veb.38.1400484405693;
 Mon, 19 May 2014 00:26:45 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Mon, 19 May 2014 00:26:25 -0700 (PDT)
In-Reply-To: <CAEYYnxY9UZDuhBoHAP5kD3dCXxd0TwR761tt-kswjTEddQCCKg@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
 <CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
 <CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
 <CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
 <CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
 <CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
 <CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
 <CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
 <CAMAsSdKxE-biW8fRqkRn6NmqsszFgQ=f1w_Z8yt8cSOj3qBB2Q@mail.gmail.com> <CAEYYnxY9UZDuhBoHAP5kD3dCXxd0TwR761tt-kswjTEddQCCKg@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 19 May 2014 00:26:25 -0700
Message-ID: <CA+-p3AGT8E5caKU+2ExxjbpV-UqaaTot+Ni3DXEdOf37tADLgw@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01183daa83d45904f9bbaf2b
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01183daa83d45904f9bbaf2b
Content-Type: text/plain; charset=UTF-8

Sounds like the problem is that classloaders always look in their parents
before themselves, and Spark users want executors to pick up classes from
their custom code before the ones in Spark plus its dependencies.

Would a custom classloader that delegates to the parent after first
checking itself fix this up?


On Mon, May 19, 2014 at 12:17 AM, DB Tsai <dbtsai@stanford.edu> wrote:

> Hi Sean,
>
> It's true that the issue here is classloader, and due to the classloader
> delegation model, users have to use reflection in the executors to pick up
> the classloader in order to use those classes added by sc.addJars APIs.
> However, it's very inconvenience for users, and not documented in spark.
>
> I'm working on a patch to solve it by calling the protected method addURL
> in URLClassLoader to update the current default classloader, so no
> customClassLoader anymore. I wonder if this is an good way to go.
>
>   private def addURL(url: URL, loader: URLClassLoader){
>     try {
>       val method: Method =
> classOf[URLClassLoader].getDeclaredMethod("addURL", classOf[URL])
>       method.setAccessible(true)
>       method.invoke(loader, url)
>     }
>     catch {
>       case t: Throwable => {
>         throw new IOException("Error, could not add URL to system
> classloader")
>       }
>     }
>   }
>
>
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Sun, May 18, 2014 at 11:57 PM, Sean Owen <sowen@cloudera.com> wrote:
>
> > I might be stating the obvious for everyone, but the issue here is not
> > reflection or the source of the JAR, but the ClassLoader. The basic
> > rules are this.
> >
> > "new Foo" will use the ClassLoader that defines Foo. This is usually
> > the ClassLoader that loaded whatever it is that first referenced Foo
> > and caused it to be loaded -- usually the ClassLoader holding your
> > other app classes.
> >
> > ClassLoaders can have a parent-child relationship. ClassLoaders always
> > look in their parent before themselves.
> >
> > (Careful then -- in contexts like Hadoop or Tomcat where your app is
> > loaded in a child ClassLoader, and you reference a class that Hadoop
> > or Tomcat also has (like a lib class) you will get the container's
> > version!)
> >
> > When you load an external JAR it has a separate ClassLoader which does
> > not necessarily bear any relation to the one containing your app
> > classes, so yeah it is not generally going to make "new Foo" work.
> >
> > Reflection lets you pick the ClassLoader, yes.
> >
> > I would not call setContextClassLoader.
> >
> > On Mon, May 19, 2014 at 12:00 AM, Sandy Ryza <sandy.ryza@cloudera.com>
> > wrote:
> > > I spoke with DB offline about this a little while ago and he confirmed
> > that
> > > he was able to access the jar from the driver.
> > >
> > > The issue appears to be a general Java issue: you can't directly
> > > instantiate a class from a dynamically loaded jar.
> > >
> > > I reproduced it locally outside of Spark with:
> > > ---
> > >     URLClassLoader urlClassLoader = new URLClassLoader(new URL[] { new
> > > File("myotherjar.jar").toURI().toURL() }, null);
> > >     Thread.currentThread().setContextClassLoader(urlClassLoader);
> > >     MyClassFromMyOtherJar obj = new MyClassFromMyOtherJar();
> > > ---
> > >
> > > I was able to load the class with reflection.
> >
>

--089e01183daa83d45904f9bbaf2b--

From dev-return-7695-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 07:30:40 2014
Return-Path: <dev-return-7695-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6A473110F2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 07:30:40 +0000 (UTC)
Received: (qmail 14739 invoked by uid 500); 19 May 2014 07:30:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14685 invoked by uid 500); 19 May 2014 07:30:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14677 invoked by uid 99); 19 May 2014 07:30:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 07:30:40 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.220.172 as permitted sender)
Received: from [209.85.220.172] (HELO mail-vc0-f172.google.com) (209.85.220.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 07:30:36 +0000
Received: by mail-vc0-f172.google.com with SMTP id hr9so9129723vcb.17
        for <dev@spark.apache.org>; Mon, 19 May 2014 00:30:15 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=SJ86iylvcpp/KHD4y+2ib+n7sKqtFYfLgtcHWsqBzgc=;
        b=A/8fb/Lang+CbzN3KbvWZuicxbdn85FMjq+OI2CyhTzM2q8DthPtjlyfArsRKD73Kg
         yEsmTB7FXRyIHT9e/HSDRkE1xh4PgrWm4inaUZ26QRJtj+q8dvbGGszGagPL1TfZQpQ5
         4fwyOBIS5SzFozNVrQTUtZWFTmNSAibWB12RWMxkavOZ3T66lprQVOJpAM/spiqDLTM9
         ejVf4jgRt81xUE21yA+riI0N1+kHh6jPxNirJr3lSpQ/djp8QKy7igoEhYpA7XQ+L8++
         xjuChPzSbbyOZBsjmxy2kpYuuoaB+sFakvDD6K1N80RrcFiB9hib6rrXxWPbihSvSwO1
         QqAg==
X-Gm-Message-State: ALoCoQmH6uUCfXyuiHiQeLPo16D5TxrYtolwMNSBW6eKgKl7L+UNIPemjwpozw3afiTQxkquNZFy
X-Received: by 10.52.37.48 with SMTP id v16mr11069818vdj.4.1400484615761; Mon,
 19 May 2014 00:30:15 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.111.69 with HTTP; Mon, 19 May 2014 00:29:55 -0700 (PDT)
In-Reply-To: <CA+-p3AGT8E5caKU+2ExxjbpV-UqaaTot+Ni3DXEdOf37tADLgw@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
 <CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
 <CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
 <CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
 <CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
 <CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
 <CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
 <CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
 <CAMAsSdKxE-biW8fRqkRn6NmqsszFgQ=f1w_Z8yt8cSOj3qBB2Q@mail.gmail.com>
 <CAEYYnxY9UZDuhBoHAP5kD3dCXxd0TwR761tt-kswjTEddQCCKg@mail.gmail.com> <CA+-p3AGT8E5caKU+2ExxjbpV-UqaaTot+Ni3DXEdOf37tADLgw@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 19 May 2014 08:29:55 +0100
Message-ID: <CAMAsSdJVckDE5=3S8wZj9P1-1-D6qBumGnofk9G7A7cksh1RVA@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I don't think a customer classloader is necessary.

Well, it occurs to me that this is no new problem. Hadoop, Tomcat, etc
all run custom user code that creates new user objects without
reflection. I should go see how that's done. Maybe it's totally valid
to set the thread's context classloader for just this purpose, and I
am not thinking clearly.

On Mon, May 19, 2014 at 8:26 AM, Andrew Ash <andrew@andrewash.com> wrote:
> Sounds like the problem is that classloaders always look in their parents
> before themselves, and Spark users want executors to pick up classes from
> their custom code before the ones in Spark plus its dependencies.
>
> Would a custom classloader that delegates to the parent after first
> checking itself fix this up?
>
>
> On Mon, May 19, 2014 at 12:17 AM, DB Tsai <dbtsai@stanford.edu> wrote:
>
>> Hi Sean,
>>
>> It's true that the issue here is classloader, and due to the classloader
>> delegation model, users have to use reflection in the executors to pick up
>> the classloader in order to use those classes added by sc.addJars APIs.
>> However, it's very inconvenience for users, and not documented in spark.
>>
>> I'm working on a patch to solve it by calling the protected method addURL
>> in URLClassLoader to update the current default classloader, so no
>> customClassLoader anymore. I wonder if this is an good way to go.
>>
>>   private def addURL(url: URL, loader: URLClassLoader){
>>     try {
>>       val method: Method =
>> classOf[URLClassLoader].getDeclaredMethod("addURL", classOf[URL])
>>       method.setAccessible(true)
>>       method.invoke(loader, url)
>>     }
>>     catch {
>>       case t: Throwable => {
>>         throw new IOException("Error, could not add URL to system
>> classloader")
>>       }
>>     }
>>   }
>>
>>
>>
>> Sincerely,
>>
>> DB Tsai
>> -------------------------------------------------------
>> My Blog: https://www.dbtsai.com
>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>
>>
>> On Sun, May 18, 2014 at 11:57 PM, Sean Owen <sowen@cloudera.com> wrote:
>>
>> > I might be stating the obvious for everyone, but the issue here is not
>> > reflection or the source of the JAR, but the ClassLoader. The basic
>> > rules are this.
>> >
>> > "new Foo" will use the ClassLoader that defines Foo. This is usually
>> > the ClassLoader that loaded whatever it is that first referenced Foo
>> > and caused it to be loaded -- usually the ClassLoader holding your
>> > other app classes.
>> >
>> > ClassLoaders can have a parent-child relationship. ClassLoaders always
>> > look in their parent before themselves.
>> >
>> > (Careful then -- in contexts like Hadoop or Tomcat where your app is
>> > loaded in a child ClassLoader, and you reference a class that Hadoop
>> > or Tomcat also has (like a lib class) you will get the container's
>> > version!)
>> >
>> > When you load an external JAR it has a separate ClassLoader which does
>> > not necessarily bear any relation to the one containing your app
>> > classes, so yeah it is not generally going to make "new Foo" work.
>> >
>> > Reflection lets you pick the ClassLoader, yes.
>> >
>> > I would not call setContextClassLoader.
>> >
>> > On Mon, May 19, 2014 at 12:00 AM, Sandy Ryza <sandy.ryza@cloudera.com>
>> > wrote:
>> > > I spoke with DB offline about this a little while ago and he confirmed
>> > that
>> > > he was able to access the jar from the driver.
>> > >
>> > > The issue appears to be a general Java issue: you can't directly
>> > > instantiate a class from a dynamically loaded jar.
>> > >
>> > > I reproduced it locally outside of Spark with:
>> > > ---
>> > >     URLClassLoader urlClassLoader = new URLClassLoader(new URL[] { new
>> > > File("myotherjar.jar").toURI().toURL() }, null);
>> > >     Thread.currentThread().setContextClassLoader(urlClassLoader);
>> > >     MyClassFromMyOtherJar obj = new MyClassFromMyOtherJar();
>> > > ---
>> > >
>> > > I was able to load the class with reflection.
>> >
>>

From dev-return-7696-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 07:39:20 2014
Return-Path: <dev-return-7696-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D797011134
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 07:39:20 +0000 (UTC)
Received: (qmail 33161 invoked by uid 500); 19 May 2014 07:39:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33098 invoked by uid 500); 19 May 2014 07:39:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33090 invoked by uid 99); 19 May 2014 07:39:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 07:39:20 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.46 as permitted sender)
Received: from [209.85.220.46] (HELO mail-pa0-f46.google.com) (209.85.220.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 07:39:17 +0000
Received: by mail-pa0-f46.google.com with SMTP id kq14so5408153pab.19
        for <dev@spark.apache.org>; Mon, 19 May 2014 00:38:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=VjbM8Gx508396JrYtLrufWfhQJ27Q1TaAw6Rjkh4BU0=;
        b=seQBVEm2Wq8a3ZFfA2Dwdpas/SgQmQItXGl/435GcWnuCLBEpybKRKnR3kGDxmXFGC
         qyLNnddzVe2gHzInCUkhQpGKzj0HZZkHd7bP4sos3UaT9kzDAt4KIptswyQYQv6uyTrK
         JoAP6eZO/mV8xdUU0eFS15OFS0rZ0N4J0sxPaKZ1YZXJGaO/RqAcAcuiM2fh5t9u3ehL
         3HV7A6YFavgkiz00S219MRWUxD4xovpJL6cJjAFbz/SIRUIoa21JGlJfZUEM/lgYmJRv
         UaKCsKw4vxwJg6AAeBaIc+9rZWwLc14jbt4FzeeB+AVhOUThvAjX+jG4v/t1Ij1TZrzA
         /eDg==
X-Received: by 10.68.178.194 with SMTP id da2mr40285112pbc.151.1400485133627;
        Mon, 19 May 2014 00:38:53 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id gu1sm28467495pbd.0.2014.05.19.00.38.52
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 19 May 2014 00:38:53 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: TorrentBroadcast aka Cornet?
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CA+-p3AHJOsDSSyYBBNbuocbWTA88VQk5XTCG-DmW9ohdeCoyGA@mail.gmail.com>
Date: Mon, 19 May 2014 00:38:49 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <5F94CBFB-5870-4C11-8576-88BBAAD353BD@gmail.com>
References: <CA+-p3AHJOsDSSyYBBNbuocbWTA88VQk5XTCG-DmW9ohdeCoyGA@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

TorrentBroadcast is actually slightly simpler, but it=92s based on that. =
It has similar performance. I=92d like to make it the default in a =
future version, we just haven=92t had a ton of testing with it yet (kind =
of an oversight in this release unfortunately).

Matei

On May 19, 2014, at 12:07 AM, Andrew Ash <andrew@andrewash.com> wrote:

> Hi Spark devs,
>=20
> Is the algorithm for
> =
TorrentBroadcast<https://github.com/apache/spark/blob/master/core/src/main=
/scala/org/apache/spark/broadcast/TorrentBroadcast.scala>the
> same as Cornet from the below paper?
>=20
> http://www.mosharaf.com/wp-content/uploads/orchestra-sigcomm11.pdf
>=20
> If so it would be nice to include a link to the paper in the Javadoc =
for
> the class.
>=20
> Thanks!
> Andrew


From dev-return-7697-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 07:58:39 2014
Return-Path: <dev-return-7697-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 51F4C111C0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 07:58:39 +0000 (UTC)
Received: (qmail 71084 invoked by uid 500); 19 May 2014 07:58:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71021 invoked by uid 500); 19 May 2014 07:58:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71013 invoked by uid 99); 19 May 2014 07:58:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 07:58:39 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of gilv@il.ibm.com designates 195.75.94.112 as permitted sender)
Received: from [195.75.94.112] (HELO e06smtp16.uk.ibm.com) (195.75.94.112)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 07:58:32 +0000
Received: from /spool/local
	by e06smtp16.uk.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <gilv@il.ibm.com>;
	Mon, 19 May 2014 08:58:10 +0100
Received: from d06dlp03.portsmouth.uk.ibm.com (9.149.20.15)
	by e06smtp16.uk.ibm.com (192.168.101.146) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Mon, 19 May 2014 08:58:09 +0100
Received: from b06cxnps4075.portsmouth.uk.ibm.com (d06relay12.portsmouth.uk.ibm.com [9.149.109.197])
	by d06dlp03.portsmouth.uk.ibm.com (Postfix) with ESMTP id 6A9DC1B08041
	for <dev@spark.apache.org>; Mon, 19 May 2014 08:58:25 +0100 (BST)
Received: from d06av09.portsmouth.uk.ibm.com (d06av09.portsmouth.uk.ibm.com [9.149.37.250])
	by b06cxnps4075.portsmouth.uk.ibm.com (8.13.8/8.13.8/NCO v10.0) with ESMTP id s4J7w8hP63045742
	for <dev@spark.apache.org>; Mon, 19 May 2014 07:58:08 GMT
Received: from d06av09.portsmouth.uk.ibm.com (localhost [127.0.0.1])
	by d06av09.portsmouth.uk.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id s4J7w8LI006723
	for <dev@spark.apache.org>; Mon, 19 May 2014 01:58:08 -0600
Received: from d06ml319.portsmouth.uk.ibm.com (d06ml319.portsmouth.uk.ibm.com [9.149.76.146])
	by d06av09.portsmouth.uk.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id s4J7w8Wd006717
	for <dev@spark.apache.org>; Mon, 19 May 2014 01:58:08 -0600
To: dev@spark.apache.org
MIME-Version: 1.0
Subject: queston about Spark repositories in GitHub
X-KeepSent: C5D3C0FB:3AF0E5D4-C2257CDD:002A910B;
 type=4; name=$KeepSent
X-Mailer: Lotus Notes Release 8.5.3FP4 SHF39 May 13, 2013
From: Gil Vernik <GILV@il.ibm.com>
Message-ID: <OFC5D3C0FB.3AF0E5D4-ONC2257CDD.002A910B-C2257CDD.002BC64E@il.ibm.com>
Date: Mon, 19 May 2014 10:58:07 +0300
X-MIMETrack: Serialize by Router on D06ML319/06/M/IBM(Release 8.5.3FP5IF1HF3|November 07, 2013) at
 19/05/2014 10:58:08,
	Serialize complete at 19/05/2014 10:58:08
Content-Type: multipart/alternative; boundary="=_alternative 002BC5A5C2257CDD_="
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 14051907-3548-0000-0000-0000091CD07A
X-Virus-Checked: Checked by ClamAV on apache.org

--=_alternative 002BC5A5C2257CDD_=
Content-Type: text/plain; charset="US-ASCII"

Hello,

I am new to the Spark community, so I apologize if I ask something 
obvious.
I follow the document about contribution to Spark where it's written that 
I need to fork the https://github.com/apache/spark repository.

I got a little bit confused since  the repository 
https://github.com/apache/spark  contains various branches: brach-0.5 - 
branch-1.0,  master, scala-2.9, streaming

What is the branch with the latest most updated code for the next planned 
release? I guess it's Spark 1.0, so should I work with  branch-1.0 or 
master?
Another question, what is Scala-2.9 and Streaming branches? 

Thanking you in advance,
Gil Vernik.





--=_alternative 002BC5A5C2257CDD_=--


From dev-return-7698-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 08:00:51 2014
Return-Path: <dev-return-7698-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1AB78111D8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 08:00:51 +0000 (UTC)
Received: (qmail 74245 invoked by uid 500); 19 May 2014 08:00:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74181 invoked by uid 500); 19 May 2014 08:00:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74173 invoked by uid 99); 19 May 2014 08:00:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 08:00:49 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.50 as permitted sender)
Received: from [209.85.160.50] (HELO mail-pb0-f50.google.com) (209.85.160.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 08:00:46 +0000
Received: by mail-pb0-f50.google.com with SMTP id ma3so5555665pbc.9
        for <dev@spark.apache.org>; Mon, 19 May 2014 01:00:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=IlEEb2DkKebrrgEaVj6nCOG92GsiUuiPoWsqZO0kw6M=;
        b=N5jsm3VtEDAZbIv4oq042wIwYeJvAyT6tdJ4EJvRIxQX+dPkWHDnHtcwzdDEGI0Nx9
         SmTdi64Wu6947QgZcHxY9YJQXMJhc+hGk6e4zJO5iKgSImRvdfpa2kBwCcXEQvajWBcr
         zCrxr8FSkJgoQ7bjNxcLv1UhTBholEFhztGj3PXyCg+XqmWMpLh2xGMD7KOD1BFRU/D3
         kAgidTIIWwh6mMN2BLALOblCW52wFA383GNtx4aKzqMEQQZguHC5VullFBIg7WyCo48Z
         2Y5A+w7qpS9zRQtyJ5UHl61bbHSGbdshCJV9o34kd/ukHUgdYGF9IepChvEcCPn8Vclh
         rNMA==
X-Received: by 10.68.189.137 with SMTP id gi9mr41072138pbc.79.1400486422065;
        Mon, 19 May 2014 01:00:22 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id bw2sm72189248pad.46.2014.05.19.01.00.20
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 19 May 2014 01:00:21 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: queston about Spark repositories in GitHub
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <OFC5D3C0FB.3AF0E5D4-ONC2257CDD.002A910B-C2257CDD.002BC64E@il.ibm.com>
Date: Mon, 19 May 2014 01:00:18 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <B99327FE-708C-455E-B383-17FBC3D624F1@gmail.com>
References: <OFC5D3C0FB.3AF0E5D4-ONC2257CDD.002A910B-C2257CDD.002BC64E@il.ibm.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

=93master=94 is where development happens, while branch-1.0, branch-0.9, =
etc are for maintenance releases in those versions. Most likely if you =
want to contribute you should use master. Some of the other named =
branches were for big features in the past, but none are actively used =
now.

Matei

On May 19, 2014, at 12:58 AM, Gil Vernik <GILV@il.ibm.com> wrote:

> Hello,
>=20
> I am new to the Spark community, so I apologize if I ask something=20
> obvious.
> I follow the document about contribution to Spark where it's written =
that=20
> I need to fork the https://github.com/apache/spark repository.
>=20
> I got a little bit confused since  the repository=20
> https://github.com/apache/spark  contains various branches: brach-0.5 =
-=20
> branch-1.0,  master, scala-2.9, streaming
>=20
> What is the branch with the latest most updated code for the next =
planned=20
> release? I guess it's Spark 1.0, so should I work with  branch-1.0 or=20=

> master?
> Another question, what is Scala-2.9 and Streaming branches?=20
>=20
> Thanking you in advance,
> Gil Vernik.
>=20
>=20
>=20
>=20


From dev-return-7699-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 20:10:35 2014
Return-Path: <dev-return-7699-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 56C301178D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 20:10:35 +0000 (UTC)
Received: (qmail 68155 invoked by uid 500); 19 May 2014 20:10:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68097 invoked by uid 500); 19 May 2014 20:10:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68088 invoked by uid 99); 19 May 2014 20:10:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 20:10:35 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of glenn.strycker@gmail.com does not designate 216.139.250.139 as permitted sender)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 20:10:30 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <glenn.strycker@gmail.com>)
	id 1WmTsX-0000z3-1m
	for dev@spark.incubator.apache.org; Mon, 19 May 2014 13:09:45 -0700
Date: Mon, 19 May 2014 13:09:30 -0700 (PDT)
From: GlennStrycker <glenn.strycker@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1400530169998-6693.post@n3.nabble.com>
Subject: BUG:  graph.triplets does not return proper values
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

graph.triplets does not work -- it returns incorrect results

I have a graph with the following edges:

orig_graph.edges.collect
=  Array(Edge(1,4,1), Edge(1,5,1), Edge(1,7,1), Edge(2,5,1), Edge(2,6,1),
Edge(3,5,1), Edge(3,6,1), Edge(3,7,1), Edge(4,1,1), Edge(5,1,1),
Edge(5,2,1), Edge(5,3,1), Edge(6,2,1), Edge(6,3,1), Edge(7,1,1),
Edge(7,3,1))

When I run triplets.collect, I only get the last edge repeated 16 times:

orig_graph.triplets.collect
= Array(((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1),
((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1),
((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1),
((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1))

I've also tried writing various map steps first before calling the triplet
function, but I get the same results as above.

Similarly, the example on the graphx programming guide page
(http://spark.apache.org/docs/0.9.0/graphx-programming-guide.html) is
incorrect.

val facts: RDD[String] =
  graph.triplets.map(triplet =>
    triplet.srcAttr._1 + " is the " + triplet.attr + " of " +
triplet.dstAttr._1)

does not work, but

val facts: RDD[String] =
  graph.triplets.map(triplet =>
    triplet.srcAttr + " is the " + triplet.attr + " of " + triplet.dstAttr)

does work, although the results are meaningless.  For my graph example, I
get the following line repeated 16 times:

1 is the 1 of 1



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/BUG-graph-triplets-does-not-return-proper-values-tp6693.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7700-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 20:30:23 2014
Return-Path: <dev-return-7700-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EB64411F30
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 20:30:23 +0000 (UTC)
Received: (qmail 43325 invoked by uid 500); 19 May 2014 20:30:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43253 invoked by uid 500); 19 May 2014 20:30:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43245 invoked by uid 99); 19 May 2014 20:30:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 20:30:23 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.180] (HELO mail-qc0-f180.google.com) (209.85.216.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 20:30:18 +0000
Received: by mail-qc0-f180.google.com with SMTP id i17so10041530qcy.11
        for <dev@spark.apache.org>; Mon, 19 May 2014 13:29:58 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=DvHS0xleV6q8bqB/dG9erTLXBIXE6cuMGRtl+00S/lg=;
        b=gRIRWxrRdbgY3ZlZkDBCl7tjdf0Nybn2HnlRdfn4xnS808aG3zoek52J5ZrTFu7zEt
         rGJUhBVWCPKR3wx+wffBRuWxQQoxjIMd7hV2WQxyJiKVVsjCgBNAT8FXUchBt7qolUzK
         5h+/1ICmtlsRwWs5PSLQOr7oiYByZFpkH/3sc8OhCOXX7gU8HCSxnEXx2JZuqhOWiwYS
         qaJ23HGXgukgWF73JVXDxZtkobULB4ogYBCe34EgjBGe/88UoUTu2CsiBYb/HGmiiv80
         Rxh4MkoKDBaBxBJOpP8QW0qM291U6stx3OW7/eTcSeHDYNs6Hhr26ld3bz34MuJmU9OM
         wCLA==
X-Gm-Message-State: ALoCoQnlexslwknBc9m9h/JIVErD0GfA8XNgrKNHPYk4xeQl9+awVx2V5YKbIAmNXRkb4oKabrbR
X-Received: by 10.140.26.179 with SMTP id 48mr39055713qgv.51.1400531397898;
 Mon, 19 May 2014 13:29:57 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.126.1 with HTTP; Mon, 19 May 2014 13:29:37 -0700 (PDT)
In-Reply-To: <1400530169998-6693.post@n3.nabble.com>
References: <1400530169998-6693.post@n3.nabble.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 19 May 2014 13:29:37 -0700
Message-ID: <CAPh_B=bwgbWJxNV0YEGt-Dr3yWTY0QDd6DZX9tcfZDQTVdermg@mail.gmail.com>
Subject: Re: BUG: graph.triplets does not return proper values
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c035f47808d504f9c6a036
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c035f47808d504f9c6a036
Content-Type: text/plain; charset=UTF-8

This was an optimization that reuses a triplet object in GraphX, and when
you do a collect directly on triplets, the same object is returned.

It has been fixed in Spark 1.0 here:
https://issues.apache.org/jira/browse/SPARK-1188

To work around in older version of Spark, you can add a copy step to it,
e.g.

graph.triplets.map(_.copy()).collect()



On Mon, May 19, 2014 at 1:09 PM, GlennStrycker <glenn.strycker@gmail.com>wrote:

> graph.triplets does not work -- it returns incorrect results
>
> I have a graph with the following edges:
>
> orig_graph.edges.collect
> =  Array(Edge(1,4,1), Edge(1,5,1), Edge(1,7,1), Edge(2,5,1), Edge(2,6,1),
> Edge(3,5,1), Edge(3,6,1), Edge(3,7,1), Edge(4,1,1), Edge(5,1,1),
> Edge(5,2,1), Edge(5,3,1), Edge(6,2,1), Edge(6,3,1), Edge(7,1,1),
> Edge(7,3,1))
>
> When I run triplets.collect, I only get the last edge repeated 16 times:
>
> orig_graph.triplets.collect
> = Array(((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1),
> ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1),
> ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1),
> ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1), ((7,1),(3,1),1))
>
> I've also tried writing various map steps first before calling the triplet
> function, but I get the same results as above.
>
> Similarly, the example on the graphx programming guide page
> (http://spark.apache.org/docs/0.9.0/graphx-programming-guide.html) is
> incorrect.
>
> val facts: RDD[String] =
>   graph.triplets.map(triplet =>
>     triplet.srcAttr._1 + " is the " + triplet.attr + " of " +
> triplet.dstAttr._1)
>
> does not work, but
>
> val facts: RDD[String] =
>   graph.triplets.map(triplet =>
>     triplet.srcAttr + " is the " + triplet.attr + " of " + triplet.dstAttr)
>
> does work, although the results are meaningless.  For my graph example, I
> get the following line repeated 16 times:
>
> 1 is the 1 of 1
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/BUG-graph-triplets-does-not-return-proper-values-tp6693.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c035f47808d504f9c6a036--

From dev-return-7701-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 22:17:08 2014
Return-Path: <dev-return-7701-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7B6F8113E0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 22:17:08 +0000 (UTC)
Received: (qmail 85827 invoked by uid 500); 19 May 2014 22:17:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85765 invoked by uid 500); 19 May 2014 22:17:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85757 invoked by uid 99); 19 May 2014 22:17:08 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 22:17:08 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of glenn.strycker@gmail.com does not designate 216.139.250.139 as permitted sender)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 22:17:03 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <glenn.strycker@gmail.com>)
	id 1WmVqz-0007no-Qn
	for dev@spark.incubator.apache.org; Mon, 19 May 2014 15:16:17 -0700
Date: Mon, 19 May 2014 15:16:02 -0700 (PDT)
From: GlennStrycker <glenn.strycker@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1400537762804-6695.post@n3.nabble.com>
In-Reply-To: <CAPh_B=bwgbWJxNV0YEGt-Dr3yWTY0QDd6DZX9tcfZDQTVdermg@mail.gmail.com>
References: <1400530169998-6693.post@n3.nabble.com> <CAPh_B=bwgbWJxNV0YEGt-Dr3yWTY0QDd6DZX9tcfZDQTVdermg@mail.gmail.com>
Subject: Re: BUG: graph.triplets does not return proper values
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks, rxin, this worked!

I am having a similar problem with .reduce... do I need to insert .copy()
functions in that statement as well?

This part works:
orig_graph.edges.map(_.copy()).flatMap(edge => Seq(edge) ).map(edge =>
(Edge(edge.copy().srcId, edge.copy().dstId, edge.copy().attr), 1)).collect

=Array((Edge(1,4,1),1), (Edge(1,5,1),1), (Edge(1,7,1),1), (Edge(2,5,1),1),
(Edge(2,6,1),1), (Edge(3,5,1),1), (Edge(3,6,1),1), (Edge(3,7,1),1),
(Edge(4,1,1),1), (Edge(5,1,1),1), (Edge(5,2,1),1), (Edge(5,3,1),1),
(Edge(6,2,1),1), (Edge(6,3,1),1), (Edge(7,1,1),1), (Edge(7,3,1),1))

But when I try adding on a reduce statement, I only get one element, not 16:
orig_graph.edges.map(_.copy()).flatMap(edge => Seq(edge) ).map(edge =>
(Edge(edge.copy().srcId, edge.copy().dstId, edge.copy().attr), 1)).reduce(
(A,B) => { if (A._1.dstId == B._1.srcId) (Edge(A._1.srcId, B._1.dstId, 2),
1) else if (A._1.srcId == B._1.dstId) (Edge(B._1.srcId, A._1.dstId, 2), 1)
else (Edge(0, 0, 0), 0) } )

=(Edge(0,0,0),0)



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/BUG-graph-triplets-does-not-return-proper-values-tp6693p6695.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7702-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 22:24:37 2014
Return-Path: <dev-return-7702-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6BE7911448
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 22:24:37 +0000 (UTC)
Received: (qmail 7143 invoked by uid 500); 19 May 2014 22:24:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7084 invoked by uid 500); 19 May 2014 22:24:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7068 invoked by uid 99); 19 May 2014 22:24:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 22:24:37 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.174] (HELO mail-qc0-f174.google.com) (209.85.216.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 22:24:33 +0000
Received: by mail-qc0-f174.google.com with SMTP id x13so9943916qcv.5
        for <dev@spark.incubator.apache.org>; Mon, 19 May 2014 15:24:12 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=uY3sz16vq2lBcv+nCqHPj9i4QGdYCVkxT1FkL0DFHVc=;
        b=FcYs6XWCTjJEMIHgOpeTm+6mdWFicwSYSLUqkOwELihsFA1Z+k/3nK9J14tHDLUr+k
         Vfu82FslrCjYHkBljkr1xrxknCYeZWnbiJTWo05+7AU7NfHDpQC8W7SdfdHDxRNQR8Er
         e79qGoryydA4fMUqhgpcthW9RiG39YgdVwxtwZEgcA4wUlznjGGoee9fK/qu61Z0BCtO
         pmlxOTxGq5mlU/HtUXPlC07eSp9NTPW8aYv1cloUJFlzW+KfVcCcjvFYWPR8x6+dPlaD
         J4Q+1S1S4mlMoUOXOg0DGbIMEE6oLTNgkEewEklGRYO5g4YfjDwoayBOc4bpcjnHkqZX
         AqSQ==
X-Gm-Message-State: ALoCoQnOaQAL5rX2vKb6eLt96GXaZGLLOg4hDfIQpUiBHLW6YsWi/Z6yvuHdryqALUMBBOiE8ENM
X-Received: by 10.224.35.209 with SMTP id q17mr52499999qad.9.1400538252267;
 Mon, 19 May 2014 15:24:12 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.126.1 with HTTP; Mon, 19 May 2014 15:23:52 -0700 (PDT)
In-Reply-To: <1400537762804-6695.post@n3.nabble.com>
References: <1400530169998-6693.post@n3.nabble.com> <CAPh_B=bwgbWJxNV0YEGt-Dr3yWTY0QDd6DZX9tcfZDQTVdermg@mail.gmail.com>
 <1400537762804-6695.post@n3.nabble.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 19 May 2014 15:23:52 -0700
Message-ID: <CAPh_B=Yi=a-AvwYp1+jUXcwTids_KgCY3Hp+5JPQJfSb2W3O1A@mail.gmail.com>
Subject: Re: BUG: graph.triplets does not return proper values
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e013a0040059bfb04f9c839d8
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013a0040059bfb04f9c839d8
Content-Type: text/plain; charset=UTF-8

Yea unfortunately you need that as well. When 1.0 is released, you wouldn't
need to do that anymore.

BTW - you can also just check out the source code from github to build 1.0.
The current branch-1.0 branch is very already at release candidate status -
so it should be almost identical to the actual 1.0 release.

https://github.com/apache/spark/tree/branch-1.0


On Mon, May 19, 2014 at 3:16 PM, GlennStrycker <glenn.strycker@gmail.com>wrote:

> Thanks, rxin, this worked!
>
> I am having a similar problem with .reduce... do I need to insert .copy()
> functions in that statement as well?
>
> This part works:
> orig_graph.edges.map(_.copy()).flatMap(edge => Seq(edge) ).map(edge =>
> (Edge(edge.copy().srcId, edge.copy().dstId, edge.copy().attr), 1)).collect
>
> =Array((Edge(1,4,1),1), (Edge(1,5,1),1), (Edge(1,7,1),1), (Edge(2,5,1),1),
> (Edge(2,6,1),1), (Edge(3,5,1),1), (Edge(3,6,1),1), (Edge(3,7,1),1),
> (Edge(4,1,1),1), (Edge(5,1,1),1), (Edge(5,2,1),1), (Edge(5,3,1),1),
> (Edge(6,2,1),1), (Edge(6,3,1),1), (Edge(7,1,1),1), (Edge(7,3,1),1))
>
> But when I try adding on a reduce statement, I only get one element, not
> 16:
> orig_graph.edges.map(_.copy()).flatMap(edge => Seq(edge) ).map(edge =>
> (Edge(edge.copy().srcId, edge.copy().dstId, edge.copy().attr), 1)).reduce(
> (A,B) => { if (A._1.dstId == B._1.srcId) (Edge(A._1.srcId, B._1.dstId, 2),
> 1) else if (A._1.srcId == B._1.dstId) (Edge(B._1.srcId, A._1.dstId, 2), 1)
> else (Edge(0, 0, 0), 0) } )
>
> =(Edge(0,0,0),0)
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/BUG-graph-triplets-does-not-return-proper-values-tp6693p6695.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--089e013a0040059bfb04f9c839d8--

From dev-return-7703-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 22:24:37 2014
Return-Path: <dev-return-7703-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EB49711449
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 22:24:37 +0000 (UTC)
Received: (qmail 7298 invoked by uid 500); 19 May 2014 22:24:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7087 invoked by uid 500); 19 May 2014 22:24:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7072 invoked by uid 99); 19 May 2014 22:24:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 22:24:37 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.169] (HELO mail-qc0-f169.google.com) (209.85.216.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 22:24:33 +0000
Received: by mail-qc0-f169.google.com with SMTP id e16so10308298qcx.0
        for <dev@spark.apache.org>; Mon, 19 May 2014 15:24:12 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=uY3sz16vq2lBcv+nCqHPj9i4QGdYCVkxT1FkL0DFHVc=;
        b=ZvYDl8rhcFM0pMU9Hzireakud6IbS6kdfcDcZo5NkbaFxqFDPhiOo5rO6golGIA0Mu
         jrl9BdgvlmK7FMiBLgfgMobScfVzY+4yVWTUcrUV3OsPyM5dFfEF4oFaf4z9WLtz8rJE
         dq5dx6AW102fnkpHrKFLanIZPyUnic0UZFABnr3lvboXGHIFdOvWfUNFbuW3bpPhojYR
         05nCNoOPBjop8T0Dxk/qcIQ8x3YoZeTmuN1xLGJBS8DXP7z4KNEQALSUjSyer6vfFY5j
         jlw7vzc2ldNizcqJ5uIo1axXtynTAtJO6jYIKncidzUWF0vX7Es9yG/A2hJOyvWP6/ED
         Jt7g==
X-Gm-Message-State: ALoCoQkDK6eWGCOa9EPOxvya315AXSoS0QiD96zuAp4QzimO67OVRUEO2FupnInyj0WpW+KcqdYf
X-Received: by 10.224.35.209 with SMTP id q17mr52499999qad.9.1400538252267;
 Mon, 19 May 2014 15:24:12 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.126.1 with HTTP; Mon, 19 May 2014 15:23:52 -0700 (PDT)
In-Reply-To: <1400537762804-6695.post@n3.nabble.com>
References: <1400530169998-6693.post@n3.nabble.com> <CAPh_B=bwgbWJxNV0YEGt-Dr3yWTY0QDd6DZX9tcfZDQTVdermg@mail.gmail.com>
 <1400537762804-6695.post@n3.nabble.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 19 May 2014 15:23:52 -0700
Message-ID: <CAPh_B=Yi=a-AvwYp1+jUXcwTids_KgCY3Hp+5JPQJfSb2W3O1A@mail.gmail.com>
Subject: Re: BUG: graph.triplets does not return proper values
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e013a0040059bfb04f9c839d8
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013a0040059bfb04f9c839d8
Content-Type: text/plain; charset=UTF-8

Yea unfortunately you need that as well. When 1.0 is released, you wouldn't
need to do that anymore.

BTW - you can also just check out the source code from github to build 1.0.
The current branch-1.0 branch is very already at release candidate status -
so it should be almost identical to the actual 1.0 release.

https://github.com/apache/spark/tree/branch-1.0


On Mon, May 19, 2014 at 3:16 PM, GlennStrycker <glenn.strycker@gmail.com>wrote:

> Thanks, rxin, this worked!
>
> I am having a similar problem with .reduce... do I need to insert .copy()
> functions in that statement as well?
>
> This part works:
> orig_graph.edges.map(_.copy()).flatMap(edge => Seq(edge) ).map(edge =>
> (Edge(edge.copy().srcId, edge.copy().dstId, edge.copy().attr), 1)).collect
>
> =Array((Edge(1,4,1),1), (Edge(1,5,1),1), (Edge(1,7,1),1), (Edge(2,5,1),1),
> (Edge(2,6,1),1), (Edge(3,5,1),1), (Edge(3,6,1),1), (Edge(3,7,1),1),
> (Edge(4,1,1),1), (Edge(5,1,1),1), (Edge(5,2,1),1), (Edge(5,3,1),1),
> (Edge(6,2,1),1), (Edge(6,3,1),1), (Edge(7,1,1),1), (Edge(7,3,1),1))
>
> But when I try adding on a reduce statement, I only get one element, not
> 16:
> orig_graph.edges.map(_.copy()).flatMap(edge => Seq(edge) ).map(edge =>
> (Edge(edge.copy().srcId, edge.copy().dstId, edge.copy().attr), 1)).reduce(
> (A,B) => { if (A._1.dstId == B._1.srcId) (Edge(A._1.srcId, B._1.dstId, 2),
> 1) else if (A._1.srcId == B._1.dstId) (Edge(B._1.srcId, A._1.dstId, 2), 1)
> else (Edge(0, 0, 0), 0) } )
>
> =(Edge(0,0,0),0)
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/BUG-graph-triplets-does-not-return-proper-values-tp6693p6695.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--089e013a0040059bfb04f9c839d8--

From dev-return-7704-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 19 22:33:47 2014
Return-Path: <dev-return-7704-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 821FE1147E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 19 May 2014 22:33:47 +0000 (UTC)
Received: (qmail 18307 invoked by uid 500); 19 May 2014 22:33:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18238 invoked by uid 500); 19 May 2014 22:33:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18226 invoked by uid 99); 19 May 2014 22:33:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 22:33:47 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of glenn.strycker@gmail.com does not designate 216.139.250.139 as permitted sender)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 19 May 2014 22:33:44 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <glenn.strycker@gmail.com>)
	id 1WmW75-0008Ty-N4
	for dev@spark.incubator.apache.org; Mon, 19 May 2014 15:32:55 -0700
Date: Mon, 19 May 2014 15:32:40 -0700 (PDT)
From: GlennStrycker <glenn.strycker@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1400538760695-6697.post@n3.nabble.com>
In-Reply-To: <CAPh_B=Yi=a-AvwYp1+jUXcwTids_KgCY3Hp+5JPQJfSb2W3O1A@mail.gmail.com>
References: <1400530169998-6693.post@n3.nabble.com> <CAPh_B=bwgbWJxNV0YEGt-Dr3yWTY0QDd6DZX9tcfZDQTVdermg@mail.gmail.com> <1400537762804-6695.post@n3.nabble.com> <CAPh_B=Yi=a-AvwYp1+jUXcwTids_KgCY3Hp+5JPQJfSb2W3O1A@mail.gmail.com>
Subject: Re: BUG: graph.triplets does not return proper values
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I tried adding .copy() everywhere, but still only get one element returned,
not even an RDD object.

orig_graph.edges.map(_.copy()).flatMap(edge => Seq(edge) ).map(edge =>
(Edge(edge.copy().srcId, edge.copy().dstId, edge.copy().attr), 1)).reduce(
(A,B) => { if (A._1.copy().dstId == B._1.copy().srcId)
(Edge(A._1.copy().srcId, B._1.copy().dstId, 2), 1) else if
(A._1.copy().srcId == B._1.copy().dstId) (Edge(B._1.copy().srcId,
A._1.copy().dstId, 2), 1) else (Edge(0, 0, 3), 1) } )

= (Edge(0,0,3),1)

I'll try getting a fresh copy of the Spark 1.0 code and see if I can get it
to work.  Thanks for your help!!



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/BUG-graph-triplets-does-not-return-proper-values-tp6693p6697.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7705-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 01:49:48 2014
Return-Path: <dev-return-7705-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1D2BC11A5F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 01:49:48 +0000 (UTC)
Received: (qmail 13142 invoked by uid 500); 20 May 2014 01:49:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13082 invoked by uid 500); 20 May 2014 01:49:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13074 invoked by uid 99); 20 May 2014 01:49:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 01:49:47 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of nitinpanj@gmail.com does not designate 216.139.250.139 as permitted sender)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 01:49:44 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <nitinpanj@gmail.com>)
	id 1WmZAm-0000YU-8s
	for dev@spark.incubator.apache.org; Mon, 19 May 2014 18:48:56 -0700
Date: Mon, 19 May 2014 18:48:41 -0700 (PDT)
From: nit <nitinpanj@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1400550521233-6698.post@n3.nabble.com>
Subject: spark 1.0 standalone application
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I am not  much comfortable with sbt. I want to build a standalone application
using spark 1.0 RC9. I can build sbt assembly for my application with Spark
0.9.1, and I think in that case spark is pulled from Aka Repository?

Now if I want to use 1.0 RC9 for my application; what is the process ?
(FYI, I was able to build spark-1.0 via sbt/assembly and I can see
sbt-assembly jar; and I think I will have to copy my jar somewhere? and
update build.sbt?)

PS: I am not sure if this is the right place for this question; but since
1.0 is still RC, I felt that this may be appropriate forum.

thank! 



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/spark-1-0-standalone-application-tp6698.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7706-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 01:50:05 2014
Return-Path: <dev-return-7706-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6F24511A60
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 01:50:05 +0000 (UTC)
Received: (qmail 13917 invoked by uid 500); 20 May 2014 01:50:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13847 invoked by uid 500); 20 May 2014 01:50:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 10395 invoked by uid 99); 20 May 2014 01:46:42 -0000
X-ASF-Spam-Status: No, hits=2.2 required=5.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of liuguodong@jd.com designates 58.83.206.59 as permitted sender)
Date: Tue, 20 May 2014 09:46:04 +0800
From: liuguodong <liuguodong@jd.com>
To: user <user@spark.apache.org>
CC: dev <dev@spark.apache.org>
Reply-To: liuguodong <liuguodong@jd.com>
Subject: spark and impala , which is more fitter for MPP
X-Priority: 3
X-Has-Attach: no
X-Mailer: Foxmail 7.0.1.92[cn]
MIME-Version: 1.0
Message-ID: <20140520094557572224174@jd.com>
Content-Type: multipart/alternative;
	boundary="----=_001_NextPart233087824143_=----"
X-Originating-IP: [106.48.2.21]
X-Virus-Checked: Checked by ClamAV on apache.org

------=_001_NextPart233087824143_=----
Content-Type: text/plain; charset="gb2312"
Content-Transfer-Encoding: base64

SGksIEFMTA0KIA0KICAgICAgIE15IHF1ZXN0aW9uIGlzIHRoYXQgc3BhcmsgYW5kIGltcGFsYSAs
IHdoaWNoIGlzIG1vcmUgZml0dGVyIGZvciBNUFAgLg0KICAgICAgIFRoZSAgbW90aXZpdGlvbiBh
cyBiZWxvdyBjYXNlOg0KICAgICAgICAxLiAgdGhyZWUgYmlnIHRhYmxlIG5lZWQgbWFrZSBqb2lu
IG9wZXJhdGlvbjsgKGFib3V0IDEwMCBmaWVsZCBwZXIgdGFibGUsICBtb3JlIHRoYW4gMVRCIHBl
ciB0YWJsZSkNCiAgICAgICAgMi4gIGJlc2lkZSBhYm92ZSB0YWJsZXMsIGl0IGlzIHZlcnkgcG9z
c2libGUgdG8gdGhleSBuZWVkIG1ha2Ugam9pbiBvcGVyYXRpb24gd2l0aCBuIG90aGVyIHNtYWxs
IG9yIG1pZGRsZSB0YWJsZQ0KICAgICAgICAzLiAgZm9yIGFsbCBqb2luIG9wZXJhdGlvbiAsIHRo
ZXkgd2lsbCBiZSByYW5kb20sICBtZWFud2hpbGUgLCBpdCBuZWVkICBhIHZlcnkgcXVpY2sgcmVz
cG9uc2UuDQoNCmxvb2sgZm9yd2FyZCB5b3VyIGF1dGhvcml0YXRpdmUgaGVscCAhDQoNCkJlc3Qg
UmVnYXJkcw0KDQoNCg0KDQpsaXVndW9kb25n

------=_001_NextPart233087824143_=------

From dev-return-7707-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 01:56:46 2014
Return-Path: <dev-return-7707-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2C79711B2D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 01:56:46 +0000 (UTC)
Received: (qmail 39186 invoked by uid 500); 20 May 2014 01:56:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39121 invoked by uid 500); 20 May 2014 01:56:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39113 invoked by uid 99); 20 May 2014 01:56:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 01:56:45 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.216.176 as permitted sender)
Received: from [209.85.216.176] (HELO mail-qc0-f176.google.com) (209.85.216.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 01:56:40 +0000
Received: by mail-qc0-f176.google.com with SMTP id r5so10321226qcx.21
        for <dev@spark.incubator.apache.org>; Mon, 19 May 2014 18:56:20 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=g4Or95HQQC+6acGyx7X+dKmpcKL6TiIZ55yLhQhsAlg=;
        b=i7ACq/T/kzdkUXRSdTi4E3G7009AuKWRZsAczt61GUSfQ6QmJHCf2XRM6Aw4Y8X8pf
         0Yx6hwx1KP3nfs7ycidQt2+Y3w8aUm7aC79gHSIQ8SQ/QsFEqedHKFdi8Mgn8Z6VHh84
         VV2KeHZudFiO9AEoKw660hSe7VEmKMjpXH0fqLrE3mn93emSDaAGdmIMDAP8l7HE9Ipk
         cFDnnyoXHHE59zrHoJPpxD2Fe6/I1iPIZDsdzg1p0HLCX5baWxFKbnj+X2UwCN5mYbPy
         RYLsBqnFdgeIs+iwajGO7z2MXRKQuRxYWN48dv3L0T5UnGNdimwhdnhpzsyrPIW/AD8P
         ERHQ==
X-Received: by 10.140.83.209 with SMTP id j75mr52614648qgd.42.1400550979876;
        Mon, 19 May 2014 18:56:19 -0700 (PDT)
Received: from [192.168.2.13] ([69.157.95.72])
        by mx.google.com with ESMTPSA id 39sm11941819qgo.22.2014.05.19.18.56.19
        for <multiple recipients>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Mon, 19 May 2014 18:56:19 -0700 (PDT)
Date: Mon, 19 May 2014 22:04:19 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Cc: dev@spark.incubator.apache.org
Message-ID: <D32E235E9849451783DBF2134CC97FC9@gmail.com>
In-Reply-To: <1400550521233-6698.post@n3.nabble.com>
References: <1400550521233-6698.post@n3.nabble.com>
Subject: Re: spark 1.0 standalone application
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="537ab823_6c80ec70_179"
X-Virus-Checked: Checked by ClamAV on apache.org

--537ab823_6c80ec70_179
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

en, you have to put spark-assembly-*.jar to the lib directory of your application 

Best, 

-- 
Nan Zhu


On Monday, May 19, 2014 at 9:48 PM, nit wrote:

> I am not much comfortable with sbt. I want to build a standalone application
> using spark 1.0 RC9. I can build sbt assembly for my application with Spark
> 0.9.1, and I think in that case spark is pulled from Aka Repository?
> 
> Now if I want to use 1.0 RC9 for my application; what is the process ?
> (FYI, I was able to build spark-1.0 via sbt/assembly and I can see
> sbt-assembly jar; and I think I will have to copy my jar somewhere? and
> update build.sbt?)
> 
> PS: I am not sure if this is the right place for this question; but since
> 1.0 is still RC, I felt that this may be appropriate forum.
> 
> thank! 
> 
> 
> 
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/spark-1-0-standalone-application-tp6698.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com (http://Nabble.com).
> 
> 



--537ab823_6c80ec70_179--


From dev-return-7708-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 01:56:46 2014
Return-Path: <dev-return-7708-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B21FD11B2E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 01:56:46 +0000 (UTC)
Received: (qmail 39371 invoked by uid 500); 20 May 2014 01:56:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39253 invoked by uid 500); 20 May 2014 01:56:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39122 invoked by uid 99); 20 May 2014 01:56:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 01:56:45 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.192.44 as permitted sender)
Received: from [209.85.192.44] (HELO mail-qg0-f44.google.com) (209.85.192.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 01:56:40 +0000
Received: by mail-qg0-f44.google.com with SMTP id i50so10230070qgf.3
        for <dev@spark.apache.org>; Mon, 19 May 2014 18:56:20 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=g4Or95HQQC+6acGyx7X+dKmpcKL6TiIZ55yLhQhsAlg=;
        b=i7ACq/T/kzdkUXRSdTi4E3G7009AuKWRZsAczt61GUSfQ6QmJHCf2XRM6Aw4Y8X8pf
         0Yx6hwx1KP3nfs7ycidQt2+Y3w8aUm7aC79gHSIQ8SQ/QsFEqedHKFdi8Mgn8Z6VHh84
         VV2KeHZudFiO9AEoKw660hSe7VEmKMjpXH0fqLrE3mn93emSDaAGdmIMDAP8l7HE9Ipk
         cFDnnyoXHHE59zrHoJPpxD2Fe6/I1iPIZDsdzg1p0HLCX5baWxFKbnj+X2UwCN5mYbPy
         RYLsBqnFdgeIs+iwajGO7z2MXRKQuRxYWN48dv3L0T5UnGNdimwhdnhpzsyrPIW/AD8P
         ERHQ==
X-Received: by 10.140.83.209 with SMTP id j75mr52614648qgd.42.1400550979876;
        Mon, 19 May 2014 18:56:19 -0700 (PDT)
Received: from [192.168.2.13] ([69.157.95.72])
        by mx.google.com with ESMTPSA id 39sm11941819qgo.22.2014.05.19.18.56.19
        for <multiple recipients>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Mon, 19 May 2014 18:56:19 -0700 (PDT)
Date: Mon, 19 May 2014 22:04:19 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Cc: dev@spark.incubator.apache.org
Message-ID: <D32E235E9849451783DBF2134CC97FC9@gmail.com>
In-Reply-To: <1400550521233-6698.post@n3.nabble.com>
References: <1400550521233-6698.post@n3.nabble.com>
Subject: Re: spark 1.0 standalone application
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="537ab823_6c80ec70_179"
X-Virus-Checked: Checked by ClamAV on apache.org

--537ab823_6c80ec70_179
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

en, you have to put spark-assembly-*.jar to the lib directory of your application 

Best, 

-- 
Nan Zhu


On Monday, May 19, 2014 at 9:48 PM, nit wrote:

> I am not much comfortable with sbt. I want to build a standalone application
> using spark 1.0 RC9. I can build sbt assembly for my application with Spark
> 0.9.1, and I think in that case spark is pulled from Aka Repository?
> 
> Now if I want to use 1.0 RC9 for my application; what is the process ?
> (FYI, I was able to build spark-1.0 via sbt/assembly and I can see
> sbt-assembly jar; and I think I will have to copy my jar somewhere? and
> update build.sbt?)
> 
> PS: I am not sure if this is the right place for this question; but since
> 1.0 is still RC, I felt that this may be appropriate forum.
> 
> thank! 
> 
> 
> 
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/spark-1-0-standalone-application-tp6698.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com (http://Nabble.com).
> 
> 



--537ab823_6c80ec70_179--


From dev-return-7709-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 01:58:40 2014
Return-Path: <dev-return-7709-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 340D811B38
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 01:58:40 +0000 (UTC)
Received: (qmail 42984 invoked by uid 500); 20 May 2014 01:58:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42927 invoked by uid 500); 20 May 2014 01:58:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42919 invoked by uid 99); 20 May 2014 01:58:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 01:58:39 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.216.178 as permitted sender)
Received: from [209.85.216.178] (HELO mail-qc0-f178.google.com) (209.85.216.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 01:58:36 +0000
Received: by mail-qc0-f178.google.com with SMTP id l6so10276318qcy.23
        for <dev@spark.apache.org>; Mon, 19 May 2014 18:58:13 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=LdRuKQ14j5z7ARC17DAzBXPVVjo23mEuIRTgEXMjWF0=;
        b=dS9UPEzEavV2FCEH4pMeqDagHMwdDy4pJrCnvWRs16m4fTMAu1NSpU1El2NP9RsHJq
         n4AePgo0EI5TTNyFle7YEhh3qRHrqpjYo+jV1KrL/8K/6spcG/eT5WiY4i1NHePe+5EF
         TyEQnzWhhY46ZHKIdkvXcAHMn09qVTCDdZkZodWA+nftCUc8H75f8HqwWuRu1Opti/Z+
         eelQ+/pIPB8LWLQ7VZNFAVUkMVfqqZRTDXU0i+LET0GS9huDAb1HqigTl8mS8GB8WCxv
         MheF8yfgyh7kSq7g+v1SOiQje32cCZNCYh20iKc5D2IPWc9ZTvU/PVawF9ui2m7UlrH+
         WH2w==
X-Received: by 10.224.134.194 with SMTP id k2mr52621185qat.5.1400551092910;
        Mon, 19 May 2014 18:58:12 -0700 (PDT)
Received: from [192.168.2.13] ([69.157.95.72])
        by mx.google.com with ESMTPSA id j7sm30369273qab.27.2014.05.19.18.58.12
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Mon, 19 May 2014 18:58:12 -0700 (PDT)
Date: Mon, 19 May 2014 22:06:12 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Message-ID: <658C73382C6845FC939110E178058EF7@gmail.com>
In-Reply-To: <tencent_60271E3C31F67D7B32E81699@qq.com>
References: <CABPQxstaxcbGSQTqUxSSUQfRpjQs=BqRkrEVvzOXbXZo8kzShg@mail.gmail.com>
 <CABPQxstbwN64rQvx6kUoESkO4GJOohYSAO+s_F85wFWjb6T1zw@mail.gmail.com>
 <CAAsvFPn2RwpFMz7ajy=t8GSjiKSPqARgDtPqXk007Cd7_xvA-Q@mail.gmail.com>
 <CAMJOb8nTR38Uf50xVxv5O+OrBkO0ELDFYiHrziBVm9qknAgpYA@mail.gmail.com>
 <CACBYxKLR3xE4FSqZQwrjmP+bhhqy_mAat0hHSu+1pOBEVTttrA@mail.gmail.com>
 <9849C2AA-3131-4E31-85DF-7F6D61632545@gmail.com>
 <4D62B3E0-B6E7-4916-953D-A196FC6BC24F@gmail.com>
 <CABPQxssnZ=aq6djG-YL=CtajhRhnT945qGcNhAq6RZEdDDU0tA@mail.gmail.com>
 <tencent_60271E3C31F67D7B32E81699@qq.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc9)
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="537ab894_4f97e3e4_179"
X-Virus-Checked: Checked by ClamAV on apache.org

--537ab894_4f97e3e4_179
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

just rerun my test on rc5 

everything works

build applications with sbt and the spark-*.jar which is compiled with Hadoop 2.3

+1 

-- 
Nan Zhu


On Sunday, May 18, 2014 at 11:07 PM, witgo wrote:

> How to reproduce this bug?
> 
> 
> ------------------ Original ------------------
> From: "Patrick Wendell";<pwendell@gmail.com (mailto:pwendell@gmail.com)>;
> Date: Mon, May 19, 2014 10:08 AM
> To: "dev@spark.apache.org (mailto:dev@spark.apache.org)"<dev@spark.apache.org (mailto:dev@spark.apache.org)>; 
> Cc: "Tom Graves"<tgraves_cs@yahoo.com (mailto:tgraves_cs@yahoo.com)>; 
> Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc9)
> 
> 
> 
> Hey Matei - the issue you found is not related to security. This patch
> a few days ago broke builds for Hadoop 1 with YARN support enabled.
> The patch directly altered the way we deal with commons-lang
> dependency, which is what is at the base of this stack trace.
> 
> https://github.com/apache/spark/pull/754
> 
> - Patrick
> 
> On Sun, May 18, 2014 at 5:28 PM, Matei Zaharia <matei.zaharia@gmail.com (mailto:matei.zaharia@gmail.com)> wrote:
> > Alright, I've opened https://github.com/apache/spark/pull/819 with the Windows fixes. I also found one other likely bug, https://issues.apache.org/jira/browse/SPARK-1875, in the binary packages for Hadoop1 built in this RC. I think this is due to Hadoop 1's security code depending on a different version of org.apache.commons than Hadoop 2, but it needs investigation. Tom, any thoughts on this?
> > 
> > Matei
> > 
> > On May 18, 2014, at 12:33 PM, Matei Zaharia <matei.zaharia@gmail.com (mailto:matei.zaharia@gmail.com)> wrote:
> > 
> > > I took the always fun task of testing it on Windows, and unfortunately, I found some small problems with the prebuilt packages due to recent changes to the launch scripts: bin/spark-class2.cmd looks in ./jars instead of ./lib for the assembly JAR, and bin/run-example2.cmd doesn't quite match the master-setting behavior of the Unix based one. I'll send a pull request to fix them soon.
> > > 
> > > Matei
> > > 
> > > 
> > > On May 17, 2014, at 11:32 AM, Sandy Ryza <sandy.ryza@cloudera.com (mailto:sandy.ryza@cloudera.com)> wrote:
> > > 
> > > > +1
> > > > 
> > > > Reran my tests from rc5:
> > > > 
> > > > * Built the release from source.
> > > > * Compiled Java and Scala apps that interact with HDFS against it.
> > > > * Ran them in local mode.
> > > > * Ran them against a pseudo-distributed YARN cluster in both yarn-client
> > > > mode and yarn-cluster mode.
> > > > 
> > > > 
> > > > On Sat, May 17, 2014 at 10:08 AM, Andrew Or <andrew@databricks.com (mailto:andrew@databricks.com)> wrote:
> > > > 
> > > > > +1
> > > > > 
> > > > > 
> > > > > 2014-05-17 8:53 GMT-07:00 Mark Hamstra <mark@clearstorydata.com (mailto:mark@clearstorydata.com)>:
> > > > > 
> > > > > > +1
> > > > > > 
> > > > > > 
> > > > > > On Sat, May 17, 2014 at 12:58 AM, Patrick Wendell <pwendell@gmail.com (mailto:pwendell@gmail.com)
> > > > > > > wrote:
> > > > > > 
> > > > > > 
> > > > > > > I'll start the voting with a +1.
> > > > > > > 
> > > > > > > On Sat, May 17, 2014 at 12:58 AM, Patrick Wendell <pwendell@gmail.com (mailto:pwendell@gmail.com)>
> > > > > > > wrote:
> > > > > > > > Please vote on releasing the following candidate as Apache Spark
> > > > > > > 
> > > > > > > 
> > > > > > 
> > > > > > version
> > > > > > > 1.0.0!
> > > > > > > > This has one bug fix and one minor feature on top of rc8:
> > > > > > > > SPARK-1864: https://github.com/apache/spark/pull/808
> > > > > > > > SPARK-1808: https://github.com/apache/spark/pull/799
> > > > > > > > 
> > > > > > > > The tag to be voted on is v1.0.0-rc9 (commit 920f947):
> > > > > https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=920f947eb5a22a679c0c3186cf69ee75f6041c75
> > > > > > > > 
> > > > > > > > The release files, including signatures, digests, etc. can be found
> > > > > at:
> > > > > > > > http://people.apache.org/~pwendell/spark-1.0.0-rc9/
> > > > > > > > 
> > > > > > > > Release artifacts are signed with the following key:
> > > > > > > > https://people.apache.org/keys/committer/pwendell.asc
> > > > > > > > 
> > > > > > > > The staging repository for this release can be found at:
> > > > > > https://repository.apache.org/content/repositories/orgapachespark-1017/
> > > > > > > > 
> > > > > > > > The documentation corresponding to this release can be found at:
> > > > > > > > http://people.apache.org/~pwendell/spark-1.0.0-rc9-docs/
> > > > > > > > 
> > > > > > > > Please vote on releasing this package as Apache Spark 1.0.0!
> > > > > > > > 
> > > > > > > > The vote is open until Tuesday, May 20, at 08:56 UTC and passes if
> > > > > > > > amajority of at least 3 +1 PMC votes are cast.
> > > > > > > > 
> > > > > > > > [ ] +1 Release this package as Apache Spark 1.0.0
> > > > > > > > [ ] -1 Do not release this package because ...
> > > > > > > > 
> > > > > > > > To learn more about Apache Spark, please see
> > > > > > > > http://spark.apache.org/
> > > > > > > > 
> > > > > > > > == API Changes ==
> > > > > > > > We welcome users to compile Spark applications against 1.0. There are
> > > > > > > > a few API changes in this release. Here are links to the associated
> > > > > > > > upgrade guides - user facing changes have been kept as small as
> > > > > > > > possible.
> > > > > > > > 
> > > > > > > > changes to ML vector specification:
> > > > > http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#from-09-to-10
> > > > > > > > 
> > > > > > > > changes to the Java API:
> > > > > http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
> > > > > > > > 
> > > > > > > > changes to the streaming API:
> > > > > http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
> > > > > > > > 
> > > > > > > > changes to the GraphX API:
> > > > > http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
> > > > > > > > 
> > > > > > > > coGroup and related functions now return Iterable[T] instead of
> > > > > Seq[T]
> > > > > > > > ==> Call toSeq on the result to restore the old behavior
> > > > > > > > 
> > > > > > > > SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> > > > > > > > ==> Call toSeq on the result to restore old behavior
> > > > > > > > 
> > > > > > > 
> > > > > > 
> > > > > 
> > > > > 
> > > > 
> > > > 
> > > 
> > > 
> > 
> > 
> 
> 
> 



--537ab894_4f97e3e4_179--


From dev-return-7710-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 02:03:50 2014
Return-Path: <dev-return-7710-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0391911B71
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 02:03:50 +0000 (UTC)
Received: (qmail 54673 invoked by uid 500); 20 May 2014 02:03:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54612 invoked by uid 500); 20 May 2014 02:03:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54604 invoked by uid 99); 20 May 2014 02:03:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 02:03:46 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mark@clearstorydata.com designates 74.125.82.44 as permitted sender)
Received: from [74.125.82.44] (HELO mail-wg0-f44.google.com) (74.125.82.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 02:03:42 +0000
Received: by mail-wg0-f44.google.com with SMTP id a1so8746453wgh.27
        for <dev@spark.apache.org>; Mon, 19 May 2014 19:03:20 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=E8rjfywn16qzzyHNqpw4W1cy7J+Nyd707FsALSK2s/0=;
        b=ZBPM6ZeSbCdjbO1dj8QZuWcI/fhhCWiXJMBSGQav/t7zhMdyvlWoiwWL3ubZFLnJCd
         kncNq4aZQti7WK7LxNRl7WgGaH7uQRTqyHaNSM5j9baaXxOuMu5mS+Ipcp87eF9yidJM
         7QC5xTk9UJAzs60/EffX5Yoki5LlEboLmeeHcQSkPMHrvn4kDcPTXTBTYcWtaZhtqlkj
         4MdMNtvcipUahuWGlvzWgfn1uq5rHgEJ05s5E4wTUbMPP5RG3fcRsltOkW4uau/P9SkT
         IPHLq+XQ8iUu43WX7YyvbAxfuADUxIXr0SuR9Hp8ROEBLFVsukkZNyKUXQLGwWRfqhZv
         fxJg==
X-Gm-Message-State: ALoCoQnB/+NXOF+B75vft3Whra1enwmj8r5rKIzYc2ElYRdhrpM5E9o0Vnb0f13Qwqin3W543ru/
MIME-Version: 1.0
X-Received: by 10.180.76.179 with SMTP id l19mr732502wiw.43.1400551400513;
 Mon, 19 May 2014 19:03:20 -0700 (PDT)
Received: by 10.216.232.69 with HTTP; Mon, 19 May 2014 19:03:20 -0700 (PDT)
In-Reply-To: <D32E235E9849451783DBF2134CC97FC9@gmail.com>
References: <1400550521233-6698.post@n3.nabble.com>
	<D32E235E9849451783DBF2134CC97FC9@gmail.com>
Date: Mon, 19 May 2014 19:03:20 -0700
Message-ID: <CAAsvFPkToNizLDzO5DgTUuK2H-xGsAJg5+nmZJEjvJf2ZrJiZg@mail.gmail.com>
Subject: Re: spark 1.0 standalone application
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d0438938bb7c20a04f9cb483c
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0438938bb7c20a04f9cb483c
Content-Type: text/plain; charset=UTF-8

That's the crude way to do it.  If you run `sbt/sbt publishLocal`, then you
can resolve the artifact from your local cache in the same way that you
would resolve it if it were deployed to a remote cache.  That's just the
build step.  Actually running the application will require the necessary
jars to be accessible by the cluster nodes.


On Mon, May 19, 2014 at 7:04 PM, Nan Zhu <zhunanmcgill@gmail.com> wrote:

> en, you have to put spark-assembly-*.jar to the lib directory of your
> application
>
> Best,
>
> --
> Nan Zhu
>
>
> On Monday, May 19, 2014 at 9:48 PM, nit wrote:
>
> > I am not much comfortable with sbt. I want to build a standalone
> application
> > using spark 1.0 RC9. I can build sbt assembly for my application with
> Spark
> > 0.9.1, and I think in that case spark is pulled from Aka Repository?
> >
> > Now if I want to use 1.0 RC9 for my application; what is the process ?
> > (FYI, I was able to build spark-1.0 via sbt/assembly and I can see
> > sbt-assembly jar; and I think I will have to copy my jar somewhere? and
> > update build.sbt?)
> >
> > PS: I am not sure if this is the right place for this question; but since
> > 1.0 is still RC, I felt that this may be appropriate forum.
> >
> > thank!
> >
> >
> >
> > --
> > View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/spark-1-0-standalone-application-tp6698.html
> > Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com (http://Nabble.com).
> >
> >
>
>
>

--f46d0438938bb7c20a04f9cb483c--

From dev-return-7711-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 02:07:41 2014
Return-Path: <dev-return-7711-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7AE5211B7E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 02:07:41 +0000 (UTC)
Received: (qmail 58443 invoked by uid 500); 20 May 2014 02:07:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58379 invoked by uid 500); 20 May 2014 02:07:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58371 invoked by uid 99); 20 May 2014 02:07:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 02:07:41 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.174 as permitted sender)
Received: from [209.85.214.174] (HELO mail-ob0-f174.google.com) (209.85.214.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 02:07:37 +0000
Received: by mail-ob0-f174.google.com with SMTP id uz6so7108538obc.19
        for <dev@spark.apache.org>; Mon, 19 May 2014 19:07:16 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=gJPCmMkhbTR0/TB0ADe7JR7e2Cl6mB5bkWgafZZ8EXQ=;
        b=Y/7MDUEhpLWSRErCQMCaEU8kFNjaEZshmCo5QuX9+PUfDDeRj5Rm8fcMiY3QM7KhrG
         jJh/X2V0kY6+X7e/q7rWL+ICOYgUgDmq/Mjg4m6DwF+ZfpcxeeFWpZCasncy0uHL2Wo+
         SIzc3IhjOH9cZ4fE9fawQ+M8s+EIeNt08XgiUaD4XeHLmcyc0sQ/pfd+MOy6QauDv+Oi
         Q9KyHeSPIc1EoSYh2wqgshYieqe+OmIwp2Ymnw4pte53bSZgousaLGGOGGG1Pm0bOb3G
         Rhdb+4OKsTjGKmtQPBC4xklFRuBitHPT/txEmRPZgf4DUQ7itDJGaKQL2+l1ByQ92Kdh
         H9CA==
MIME-Version: 1.0
X-Received: by 10.60.81.227 with SMTP id d3mr34138825oey.41.1400551636695;
 Mon, 19 May 2014 19:07:16 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Mon, 19 May 2014 19:07:16 -0700 (PDT)
In-Reply-To: <CAMAsSdJVckDE5=3S8wZj9P1-1-D6qBumGnofk9G7A7cksh1RVA@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
	<CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
	<CAMAsSdKxE-biW8fRqkRn6NmqsszFgQ=f1w_Z8yt8cSOj3qBB2Q@mail.gmail.com>
	<CAEYYnxY9UZDuhBoHAP5kD3dCXxd0TwR761tt-kswjTEddQCCKg@mail.gmail.com>
	<CA+-p3AGT8E5caKU+2ExxjbpV-UqaaTot+Ni3DXEdOf37tADLgw@mail.gmail.com>
	<CAMAsSdJVckDE5=3S8wZj9P1-1-D6qBumGnofk9G7A7cksh1RVA@mail.gmail.com>
Date: Mon, 19 May 2014 19:07:16 -0700
Message-ID: <CABPQxstfSyVu6D-FSt7qa-o1co9AkhHsncTHGBrCK4rEM7fymg@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Having a user add define a custom class inside of an added jar and
instantiate it directly inside of an executor is definitely supported
in Spark and has been for a really long time (several years). This is
something we do all the time in Spark.

DB - I'd hold off on a re-architecting of this until we identify
exactly what is causing the bug you are running into.

In a nutshell, when the bytecode "new Foo()" is run on the executor,
it will ask the driver for the class over HTTP using a custom
classloader. Something in that pipeline is breaking here, possibly
related to the YARN deployment stuff.


On Mon, May 19, 2014 at 12:29 AM, Sean Owen <sowen@cloudera.com> wrote:
> I don't think a customer classloader is necessary.
>
> Well, it occurs to me that this is no new problem. Hadoop, Tomcat, etc
> all run custom user code that creates new user objects without
> reflection. I should go see how that's done. Maybe it's totally valid
> to set the thread's context classloader for just this purpose, and I
> am not thinking clearly.
>
> On Mon, May 19, 2014 at 8:26 AM, Andrew Ash <andrew@andrewash.com> wrote:
>> Sounds like the problem is that classloaders always look in their parents
>> before themselves, and Spark users want executors to pick up classes from
>> their custom code before the ones in Spark plus its dependencies.
>>
>> Would a custom classloader that delegates to the parent after first
>> checking itself fix this up?
>>
>>
>> On Mon, May 19, 2014 at 12:17 AM, DB Tsai <dbtsai@stanford.edu> wrote:
>>
>>> Hi Sean,
>>>
>>> It's true that the issue here is classloader, and due to the classloader
>>> delegation model, users have to use reflection in the executors to pick up
>>> the classloader in order to use those classes added by sc.addJars APIs.
>>> However, it's very inconvenience for users, and not documented in spark.
>>>
>>> I'm working on a patch to solve it by calling the protected method addURL
>>> in URLClassLoader to update the current default classloader, so no
>>> customClassLoader anymore. I wonder if this is an good way to go.
>>>
>>>   private def addURL(url: URL, loader: URLClassLoader){
>>>     try {
>>>       val method: Method =
>>> classOf[URLClassLoader].getDeclaredMethod("addURL", classOf[URL])
>>>       method.setAccessible(true)
>>>       method.invoke(loader, url)
>>>     }
>>>     catch {
>>>       case t: Throwable => {
>>>         throw new IOException("Error, could not add URL to system
>>> classloader")
>>>       }
>>>     }
>>>   }
>>>
>>>
>>>
>>> Sincerely,
>>>
>>> DB Tsai
>>> -------------------------------------------------------
>>> My Blog: https://www.dbtsai.com
>>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>
>>>
>>> On Sun, May 18, 2014 at 11:57 PM, Sean Owen <sowen@cloudera.com> wrote:
>>>
>>> > I might be stating the obvious for everyone, but the issue here is not
>>> > reflection or the source of the JAR, but the ClassLoader. The basic
>>> > rules are this.
>>> >
>>> > "new Foo" will use the ClassLoader that defines Foo. This is usually
>>> > the ClassLoader that loaded whatever it is that first referenced Foo
>>> > and caused it to be loaded -- usually the ClassLoader holding your
>>> > other app classes.
>>> >
>>> > ClassLoaders can have a parent-child relationship. ClassLoaders always
>>> > look in their parent before themselves.
>>> >
>>> > (Careful then -- in contexts like Hadoop or Tomcat where your app is
>>> > loaded in a child ClassLoader, and you reference a class that Hadoop
>>> > or Tomcat also has (like a lib class) you will get the container's
>>> > version!)
>>> >
>>> > When you load an external JAR it has a separate ClassLoader which does
>>> > not necessarily bear any relation to the one containing your app
>>> > classes, so yeah it is not generally going to make "new Foo" work.
>>> >
>>> > Reflection lets you pick the ClassLoader, yes.
>>> >
>>> > I would not call setContextClassLoader.
>>> >
>>> > On Mon, May 19, 2014 at 12:00 AM, Sandy Ryza <sandy.ryza@cloudera.com>
>>> > wrote:
>>> > > I spoke with DB offline about this a little while ago and he confirmed
>>> > that
>>> > > he was able to access the jar from the driver.
>>> > >
>>> > > The issue appears to be a general Java issue: you can't directly
>>> > > instantiate a class from a dynamically loaded jar.
>>> > >
>>> > > I reproduced it locally outside of Spark with:
>>> > > ---
>>> > >     URLClassLoader urlClassLoader = new URLClassLoader(new URL[] { new
>>> > > File("myotherjar.jar").toURI().toURL() }, null);
>>> > >     Thread.currentThread().setContextClassLoader(urlClassLoader);
>>> > >     MyClassFromMyOtherJar obj = new MyClassFromMyOtherJar();
>>> > > ---
>>> > >
>>> > > I was able to load the class with reflection.
>>> >
>>>

From dev-return-7712-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 02:10:43 2014
Return-Path: <dev-return-7712-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5407A11B99
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 02:10:43 +0000 (UTC)
Received: (qmail 63705 invoked by uid 500); 20 May 2014 02:10:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63647 invoked by uid 500); 20 May 2014 02:10:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63633 invoked by uid 99); 20 May 2014 02:10:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 02:10:42 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.48 as permitted sender)
Received: from [209.85.219.48] (HELO mail-oa0-f48.google.com) (209.85.219.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 02:10:40 +0000
Received: by mail-oa0-f48.google.com with SMTP id i4so7186491oah.35
        for <dev@spark.apache.org>; Mon, 19 May 2014 19:10:16 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=YjsHjVPOB0SzerIiF8G61o22PmTw/saCyaCOdmREs6o=;
        b=g72M9X7N0cXhTZoN+9Ic5XLdWDWSqPnrFc7qDYikyFTv9k79SC7JIlV0oyIM09KXHc
         EqdnCHs5oRHsQMfkhi6bfSy1Niw3xhAuZNyfnW4n/0HgBmCNQqMKL4fQBhWVjCFsnbIK
         KZm8dQzysZn4enQKIsVPDFhVCtv5daH2A3wLHNMBrs/XtR58xI945Pgrq+RNTtTDQ7Ns
         0osAKjf/Yn2Nl+eUL5tEzA57vFJAw8DWb63MC2K0hsdYWr4/n/vtxRKdgg8huriXa0ca
         EnG5Hy9PQ4b2U7O4mc+kxZVyx1dFgFSyyAy7gziL/xEHfdntifsBqX4nwGPJMmlvqZ+l
         VN5A==
MIME-Version: 1.0
X-Received: by 10.60.81.227 with SMTP id d3mr34149604oey.41.1400551816333;
 Mon, 19 May 2014 19:10:16 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Mon, 19 May 2014 19:10:16 -0700 (PDT)
In-Reply-To: <CAAsvFPkToNizLDzO5DgTUuK2H-xGsAJg5+nmZJEjvJf2ZrJiZg@mail.gmail.com>
References: <1400550521233-6698.post@n3.nabble.com>
	<D32E235E9849451783DBF2134CC97FC9@gmail.com>
	<CAAsvFPkToNizLDzO5DgTUuK2H-xGsAJg5+nmZJEjvJf2ZrJiZg@mail.gmail.com>
Date: Mon, 19 May 2014 19:10:16 -0700
Message-ID: <CABPQxstH2ysQzvW62U46yXNzQ954TUc=peprP-zVeumcHnofaw@mail.gmail.com>
Subject: Re: spark 1.0 standalone application
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Whenever we publish a release candidate, we create a temporary maven
repository that host the artifacts. We do this precisely for the case
you are running into (where a user wants to build an application
against it to test).

You can build against the release candidate by just adding that
repository in your sbt build, then linking against "spark-core"
version "1.0.0". For rc9 the repository is in the vote e-mail:

http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-0-0-rc9-td6629.html

On Mon, May 19, 2014 at 7:03 PM, Mark Hamstra <mark@clearstorydata.com> wrote:
> That's the crude way to do it.  If you run `sbt/sbt publishLocal`, then you
> can resolve the artifact from your local cache in the same way that you
> would resolve it if it were deployed to a remote cache.  That's just the
> build step.  Actually running the application will require the necessary
> jars to be accessible by the cluster nodes.
>
>
> On Mon, May 19, 2014 at 7:04 PM, Nan Zhu <zhunanmcgill@gmail.com> wrote:
>
>> en, you have to put spark-assembly-*.jar to the lib directory of your
>> application
>>
>> Best,
>>
>> --
>> Nan Zhu
>>
>>
>> On Monday, May 19, 2014 at 9:48 PM, nit wrote:
>>
>> > I am not much comfortable with sbt. I want to build a standalone
>> application
>> > using spark 1.0 RC9. I can build sbt assembly for my application with
>> Spark
>> > 0.9.1, and I think in that case spark is pulled from Aka Repository?
>> >
>> > Now if I want to use 1.0 RC9 for my application; what is the process ?
>> > (FYI, I was able to build spark-1.0 via sbt/assembly and I can see
>> > sbt-assembly jar; and I think I will have to copy my jar somewhere? and
>> > update build.sbt?)
>> >
>> > PS: I am not sure if this is the right place for this question; but since
>> > 1.0 is still RC, I felt that this may be appropriate forum.
>> >
>> > thank!
>> >
>> >
>> >
>> > --
>> > View this message in context:
>> http://apache-spark-developers-list.1001551.n3.nabble.com/spark-1-0-standalone-application-tp6698.html
>> > Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com (http://Nabble.com).
>> >
>> >
>>
>>
>>

From dev-return-7713-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 02:21:55 2014
Return-Path: <dev-return-7713-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D6BFA11BC3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 02:21:55 +0000 (UTC)
Received: (qmail 77697 invoked by uid 500); 20 May 2014 02:21:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77639 invoked by uid 500); 20 May 2014 02:21:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77631 invoked by uid 99); 20 May 2014 02:21:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 02:21:55 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of svsujeet@gmail.com designates 209.85.192.54 as permitted sender)
Received: from [209.85.192.54] (HELO mail-qg0-f54.google.com) (209.85.192.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 02:21:51 +0000
Received: by mail-qg0-f54.google.com with SMTP id q108so10246500qgd.13
        for <dev@spark.apache.org>; Mon, 19 May 2014 19:21:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=ZZep2UN+GL2TLApT4bcRqObzugfosX2BrvgxQBXdQWg=;
        b=PK2/puuei5jcFkNY82e1F5k8kbRZaYS1YHTl+LJa2YCON+a2Hs26rmngDdWI1oSt3O
         oKsTRS+g+EvVVCCvSAW56Cs/XLEXhDabvQJM2FhLv7trQ2GVKXWIzdWZ3VLhozc9Ri7S
         vWoGYPRB6eYSsbQSLRVTZLkYhkwjtdOWdwKihUK4+CLn+vCJBbytsGstwo4Z89/wiGce
         h+jYFq7nmfRAPx8XW/H6UP2EnB59cSYfuRl15rkewlSrj2+jSozY1d2xG7gPkAn4Zd3u
         2AT+nNCClB2VLdSpgLcql4G8kRlIugkbbA14uK5jiD+A1H0odq2yDSypke8ddpvRrdMW
         VKug==
MIME-Version: 1.0
X-Received: by 10.229.171.193 with SMTP id i1mr45580942qcz.15.1400552490875;
 Mon, 19 May 2014 19:21:30 -0700 (PDT)
Received: by 10.140.27.114 with HTTP; Mon, 19 May 2014 19:21:30 -0700 (PDT)
In-Reply-To: <CABPQxstH2ysQzvW62U46yXNzQ954TUc=peprP-zVeumcHnofaw@mail.gmail.com>
References: <1400550521233-6698.post@n3.nabble.com>
	<D32E235E9849451783DBF2134CC97FC9@gmail.com>
	<CAAsvFPkToNizLDzO5DgTUuK2H-xGsAJg5+nmZJEjvJf2ZrJiZg@mail.gmail.com>
	<CABPQxstH2ysQzvW62U46yXNzQ954TUc=peprP-zVeumcHnofaw@mail.gmail.com>
Date: Mon, 19 May 2014 19:21:30 -0700
Message-ID: <CANJXpKcnrxn8xtAXFpORE5AGtzi0PM8Xs3otZt0m7e4rDL1KaQ@mail.gmail.com>
Subject: Re: spark 1.0 standalone application
From: Sujeet Varakhedi <svsujeet@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c30668b5433604f9cb8953
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c30668b5433604f9cb8953
Content-Type: text/plain; charset=UTF-8

Threads like these are great candidates to be part of the "Contributors
guide". I will create a JIRA to update the guide with data past threads
like these.

Sujeet


On Mon, May 19, 2014 at 7:10 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Whenever we publish a release candidate, we create a temporary maven
> repository that host the artifacts. We do this precisely for the case
> you are running into (where a user wants to build an application
> against it to test).
>
> You can build against the release candidate by just adding that
> repository in your sbt build, then linking against "spark-core"
> version "1.0.0". For rc9 the repository is in the vote e-mail:
>
>
> http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-0-0-rc9-td6629.html
>
> On Mon, May 19, 2014 at 7:03 PM, Mark Hamstra <mark@clearstorydata.com>
> wrote:
> > That's the crude way to do it.  If you run `sbt/sbt publishLocal`, then
> you
> > can resolve the artifact from your local cache in the same way that you
> > would resolve it if it were deployed to a remote cache.  That's just the
> > build step.  Actually running the application will require the necessary
> > jars to be accessible by the cluster nodes.
> >
> >
> > On Mon, May 19, 2014 at 7:04 PM, Nan Zhu <zhunanmcgill@gmail.com> wrote:
> >
> >> en, you have to put spark-assembly-*.jar to the lib directory of your
> >> application
> >>
> >> Best,
> >>
> >> --
> >> Nan Zhu
> >>
> >>
> >> On Monday, May 19, 2014 at 9:48 PM, nit wrote:
> >>
> >> > I am not much comfortable with sbt. I want to build a standalone
> >> application
> >> > using spark 1.0 RC9. I can build sbt assembly for my application with
> >> Spark
> >> > 0.9.1, and I think in that case spark is pulled from Aka Repository?
> >> >
> >> > Now if I want to use 1.0 RC9 for my application; what is the process ?
> >> > (FYI, I was able to build spark-1.0 via sbt/assembly and I can see
> >> > sbt-assembly jar; and I think I will have to copy my jar somewhere?
> and
> >> > update build.sbt?)
> >> >
> >> > PS: I am not sure if this is the right place for this question; but
> since
> >> > 1.0 is still RC, I felt that this may be appropriate forum.
> >> >
> >> > thank!
> >> >
> >> >
> >> >
> >> > --
> >> > View this message in context:
> >>
> http://apache-spark-developers-list.1001551.n3.nabble.com/spark-1-0-standalone-application-tp6698.html
> >> > Sent from the Apache Spark Developers List mailing list archive at
> >> Nabble.com (http://Nabble.com).
> >> >
> >> >
> >>
> >>
> >>
>

--001a11c30668b5433604f9cb8953--

From dev-return-7714-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 02:33:27 2014
Return-Path: <dev-return-7714-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C421511BF9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 02:33:27 +0000 (UTC)
Received: (qmail 93192 invoked by uid 500); 20 May 2014 02:33:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93129 invoked by uid 500); 20 May 2014 02:33:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93121 invoked by uid 99); 20 May 2014 02:33:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 02:33:27 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.177] (HELO mail-qc0-f177.google.com) (209.85.216.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 02:33:22 +0000
Received: by mail-qc0-f177.google.com with SMTP id i17so10287740qcy.8
        for <dev@spark.apache.org>; Mon, 19 May 2014 19:33:01 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=FSr8fogxKgt1wZT5Vq1ZaHGwbBar9IyEnoDyqbOO948=;
        b=M98WEgD3h7Jd5S5ypwoipQMnsouNXXPmbeapdev7+L0gO+wdzuPTQNHgHt9MD/MbMt
         NCmXzyK60jQUZZRD11EUA3p2vv4rmmEuke/LxPhBuKNpz663Wepremki9+21OAyA9dDX
         KxRhHo8XOm8zLDd5rX3FidFfkGC0v9EPdKZTbYPasHhe0tUadqt1ylbAMLaIQ7VxVoxe
         t4ecASUhqY+NZqa+yaZ5AJKDmL1euSNpG1wLAPgA07ULIitacxI14sn4IJhfrY1zD1Wo
         vAJK7U57+OKzxhdqOn51YSmj+rzS/ZAz0A1pQtO5kV+Gi9LIoN+G0VAHogEPw/+KCYKO
         6y+Q==
X-Gm-Message-State: ALoCoQlT+Y7q//TUer0V84x7RoHiSbUAQ6LWVmjnUJP0zYUFP7i9T17Byg2LAkdZxP2ppoVjGPV6
X-Received: by 10.140.26.179 with SMTP id 48mr40975299qgv.51.1400553181579;
 Mon, 19 May 2014 19:33:01 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.126.1 with HTTP; Mon, 19 May 2014 19:32:41 -0700 (PDT)
In-Reply-To: <1400538760695-6697.post@n3.nabble.com>
References: <1400530169998-6693.post@n3.nabble.com> <CAPh_B=bwgbWJxNV0YEGt-Dr3yWTY0QDd6DZX9tcfZDQTVdermg@mail.gmail.com>
 <1400537762804-6695.post@n3.nabble.com> <CAPh_B=Yi=a-AvwYp1+jUXcwTids_KgCY3Hp+5JPQJfSb2W3O1A@mail.gmail.com>
 <1400538760695-6697.post@n3.nabble.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 19 May 2014 19:32:41 -0700
Message-ID: <CAPh_B=ZDpYmZggfL7CTd+zekxfwHciAfjMVg1rNspcA5ynWftg@mail.gmail.com>
Subject: Re: BUG: graph.triplets does not return proper values
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c035f4e09d0d04f9cbb262
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c035f4e09d0d04f9cbb262
Content-Type: text/plain; charset=UTF-8

reduce always return a single element - maybe you are misunderstanding what
the reduce function in collections does.


On Mon, May 19, 2014 at 3:32 PM, GlennStrycker <glenn.strycker@gmail.com>wrote:

> I tried adding .copy() everywhere, but still only get one element returned,
> not even an RDD object.
>
> orig_graph.edges.map(_.copy()).flatMap(edge => Seq(edge) ).map(edge =>
> (Edge(edge.copy().srcId, edge.copy().dstId, edge.copy().attr), 1)).reduce(
> (A,B) => { if (A._1.copy().dstId == B._1.copy().srcId)
> (Edge(A._1.copy().srcId, B._1.copy().dstId, 2), 1) else if
> (A._1.copy().srcId == B._1.copy().dstId) (Edge(B._1.copy().srcId,
> A._1.copy().dstId, 2), 1) else (Edge(0, 0, 3), 1) } )
>
> = (Edge(0,0,3),1)
>
> I'll try getting a fresh copy of the Spark 1.0 code and see if I can get it
> to work.  Thanks for your help!!
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/BUG-graph-triplets-does-not-return-proper-values-tp6693p6697.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c035f4e09d0d04f9cbb262--

From dev-return-7715-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 02:39:14 2014
Return-Path: <dev-return-7715-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3079511C14
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 02:39:14 +0000 (UTC)
Received: (qmail 757 invoked by uid 500); 20 May 2014 02:39:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 699 invoked by uid 500); 20 May 2014 02:39:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 691 invoked by uid 99); 20 May 2014 02:39:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 02:39:13 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.175 as permitted sender)
Received: from [209.85.216.175] (HELO mail-qc0-f175.google.com) (209.85.216.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 02:39:11 +0000
Received: by mail-qc0-f175.google.com with SMTP id w7so10382130qcr.6
        for <dev@spark.apache.org>; Mon, 19 May 2014 19:38:47 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=GgfZcvf2KnrWK9UXqDOpUKh8MSf/ubEi6+HlnE1YVwo=;
        b=G62MAOpivWjIrqsH7H5WJTasEJmei7hsWTChPZxwYWmBHsNUR1hKVIrg1RTBLpan9y
         CschQxAOXiJa6DJUGIn+NpK9BRK7+O4kCXOwub9Xi5ElwjZei71E2LgCVaMVARQ8H5on
         5m4e/STbTYZtG2cV40aSWR8VajV+8vScSCQX9TUCoeDvZMdGRLPkdz579hnxN8mdx34U
         ij4C/TAjvB51mTPdmeMSF8k2LEw5JoI4BV/0aqVlPXyxuHORGAGq/q7PMlf3vUkt2BVn
         tMxxdxKFUZhh1J4VC7qx7U2aJqU8VlKLi0Ui271128Y7PZRyWFFHG3WD54IAVHLMp74o
         zh3Q==
X-Gm-Message-State: ALoCoQk54rJcb2NMnbOOWmABm3xdeFf0eK1c8OtORd4z61Gb3P1FLiNwI6jt1G9SG4HkA8f4k4lA
MIME-Version: 1.0
X-Received: by 10.140.96.162 with SMTP id k31mr720990qge.38.1400553527305;
 Mon, 19 May 2014 19:38:47 -0700 (PDT)
Received: by 10.140.106.202 with HTTP; Mon, 19 May 2014 19:38:47 -0700 (PDT)
In-Reply-To: <CABPQxstfSyVu6D-FSt7qa-o1co9AkhHsncTHGBrCK4rEM7fymg@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
	<CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
	<CAMAsSdKxE-biW8fRqkRn6NmqsszFgQ=f1w_Z8yt8cSOj3qBB2Q@mail.gmail.com>
	<CAEYYnxY9UZDuhBoHAP5kD3dCXxd0TwR761tt-kswjTEddQCCKg@mail.gmail.com>
	<CA+-p3AGT8E5caKU+2ExxjbpV-UqaaTot+Ni3DXEdOf37tADLgw@mail.gmail.com>
	<CAMAsSdJVckDE5=3S8wZj9P1-1-D6qBumGnofk9G7A7cksh1RVA@mail.gmail.com>
	<CABPQxstfSyVu6D-FSt7qa-o1co9AkhHsncTHGBrCK4rEM7fymg@mail.gmail.com>
Date: Mon, 19 May 2014 19:38:47 -0700
Message-ID: <CACBYxKLb=OK2EKu=2yS_1tswYz52rBF-59+8DZL+DfG6qHF52Q@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ab75c7c197c04f9cbc70d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ab75c7c197c04f9cbc70d
Content-Type: text/plain; charset=UTF-8

It just hit me why this problem is showing up on YARN and not on standalone.

The relevant difference between YARN and standalone is that, on YARN, the
app jar is loaded by the system classloader instead of Spark's custom URL
classloader.

On YARN, the system classloader knows about [the classes in the spark jars,
the classes in the primary app jar].   The custom classloader knows about
[the classes in secondary app jars] and has the system classloader as its
parent.

A few relevant facts (mostly redundant with what Sean pointed out):
* Every class has a classloader that loaded it.
* When an object of class B is instantiated inside of class A, the
classloader used for loading B is the classloader that was used for loading
A.
* When a classloader fails to load a class, it lets its parent classloader
try.  If its parent succeeds, its parent becomes the "classloader that
loaded it".

So suppose class B is in a secondary app jar and class A is in the primary
app jar:
1. The custom classloader will try to load class A.
2. It will fail, because it only knows about the secondary jars.
3. It will delegate to its parent, the system classloader.
4. The system classloader will succeed, because it knows about the primary
app jar.
5. A's classloader will be the system classloader.
6. A tries to instantiate an instance of class B.
7. B will be loaded with A's classloader, which is the system classloader.
8. Loading B will fail, because A's classloader, which is the system
classloader, doesn't know about the secondary app jars.

In Spark standalone, A and B are both loaded by the custom classloader, so
this issue doesn't come up.

-Sandy

On Mon, May 19, 2014 at 7:07 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Having a user add define a custom class inside of an added jar and
> instantiate it directly inside of an executor is definitely supported
> in Spark and has been for a really long time (several years). This is
> something we do all the time in Spark.
>
> DB - I'd hold off on a re-architecting of this until we identify
> exactly what is causing the bug you are running into.
>
> In a nutshell, when the bytecode "new Foo()" is run on the executor,
> it will ask the driver for the class over HTTP using a custom
> classloader. Something in that pipeline is breaking here, possibly
> related to the YARN deployment stuff.
>
>
> On Mon, May 19, 2014 at 12:29 AM, Sean Owen <sowen@cloudera.com> wrote:
> > I don't think a customer classloader is necessary.
> >
> > Well, it occurs to me that this is no new problem. Hadoop, Tomcat, etc
> > all run custom user code that creates new user objects without
> > reflection. I should go see how that's done. Maybe it's totally valid
> > to set the thread's context classloader for just this purpose, and I
> > am not thinking clearly.
> >
> > On Mon, May 19, 2014 at 8:26 AM, Andrew Ash <andrew@andrewash.com>
> wrote:
> >> Sounds like the problem is that classloaders always look in their
> parents
> >> before themselves, and Spark users want executors to pick up classes
> from
> >> their custom code before the ones in Spark plus its dependencies.
> >>
> >> Would a custom classloader that delegates to the parent after first
> >> checking itself fix this up?
> >>
> >>
> >> On Mon, May 19, 2014 at 12:17 AM, DB Tsai <dbtsai@stanford.edu> wrote:
> >>
> >>> Hi Sean,
> >>>
> >>> It's true that the issue here is classloader, and due to the
> classloader
> >>> delegation model, users have to use reflection in the executors to
> pick up
> >>> the classloader in order to use those classes added by sc.addJars APIs.
> >>> However, it's very inconvenience for users, and not documented in
> spark.
> >>>
> >>> I'm working on a patch to solve it by calling the protected method
> addURL
> >>> in URLClassLoader to update the current default classloader, so no
> >>> customClassLoader anymore. I wonder if this is an good way to go.
> >>>
> >>>   private def addURL(url: URL, loader: URLClassLoader){
> >>>     try {
> >>>       val method: Method =
> >>> classOf[URLClassLoader].getDeclaredMethod("addURL", classOf[URL])
> >>>       method.setAccessible(true)
> >>>       method.invoke(loader, url)
> >>>     }
> >>>     catch {
> >>>       case t: Throwable => {
> >>>         throw new IOException("Error, could not add URL to system
> >>> classloader")
> >>>       }
> >>>     }
> >>>   }
> >>>
> >>>
> >>>
> >>> Sincerely,
> >>>
> >>> DB Tsai
> >>> -------------------------------------------------------
> >>> My Blog: https://www.dbtsai.com
> >>> LinkedIn: https://www.linkedin.com/in/dbtsai
> >>>
> >>>
> >>> On Sun, May 18, 2014 at 11:57 PM, Sean Owen <sowen@cloudera.com>
> wrote:
> >>>
> >>> > I might be stating the obvious for everyone, but the issue here is
> not
> >>> > reflection or the source of the JAR, but the ClassLoader. The basic
> >>> > rules are this.
> >>> >
> >>> > "new Foo" will use the ClassLoader that defines Foo. This is usually
> >>> > the ClassLoader that loaded whatever it is that first referenced Foo
> >>> > and caused it to be loaded -- usually the ClassLoader holding your
> >>> > other app classes.
> >>> >
> >>> > ClassLoaders can have a parent-child relationship. ClassLoaders
> always
> >>> > look in their parent before themselves.
> >>> >
> >>> > (Careful then -- in contexts like Hadoop or Tomcat where your app is
> >>> > loaded in a child ClassLoader, and you reference a class that Hadoop
> >>> > or Tomcat also has (like a lib class) you will get the container's
> >>> > version!)
> >>> >
> >>> > When you load an external JAR it has a separate ClassLoader which
> does
> >>> > not necessarily bear any relation to the one containing your app
> >>> > classes, so yeah it is not generally going to make "new Foo" work.
> >>> >
> >>> > Reflection lets you pick the ClassLoader, yes.
> >>> >
> >>> > I would not call setContextClassLoader.
> >>> >
> >>> > On Mon, May 19, 2014 at 12:00 AM, Sandy Ryza <
> sandy.ryza@cloudera.com>
> >>> > wrote:
> >>> > > I spoke with DB offline about this a little while ago and he
> confirmed
> >>> > that
> >>> > > he was able to access the jar from the driver.
> >>> > >
> >>> > > The issue appears to be a general Java issue: you can't directly
> >>> > > instantiate a class from a dynamically loaded jar.
> >>> > >
> >>> > > I reproduced it locally outside of Spark with:
> >>> > > ---
> >>> > >     URLClassLoader urlClassLoader = new URLClassLoader(new URL[] {
> new
> >>> > > File("myotherjar.jar").toURI().toURL() }, null);
> >>> > >     Thread.currentThread().setContextClassLoader(urlClassLoader);
> >>> > >     MyClassFromMyOtherJar obj = new MyClassFromMyOtherJar();
> >>> > > ---
> >>> > >
> >>> > > I was able to load the class with reflection.
> >>> >
> >>>
>

--001a113ab75c7c197c04f9cbc70d--

From dev-return-7716-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 02:43:23 2014
Return-Path: <dev-return-7716-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8DDC211C2D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 02:43:23 +0000 (UTC)
Received: (qmail 9205 invoked by uid 500); 20 May 2014 02:43:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9144 invoked by uid 500); 20 May 2014 02:43:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9136 invoked by uid 99); 20 May 2014 02:43:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 02:43:23 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 02:43:19 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 1BB781018D8
	for <dev@spark.apache.org>; Mon, 19 May 2014 19:42:55 -0700 (PDT)
Received: from mail-qc0-f175.google.com (mail-qc0-f175.google.com [209.85.216.175])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 47AB2100C5E
	for <dev@spark.apache.org>; Mon, 19 May 2014 19:42:54 -0700 (PDT)
Received: by mail-qc0-f175.google.com with SMTP id w7so10405179qcr.20
        for <dev@spark.apache.org>; Mon, 19 May 2014 19:42:53 -0700 (PDT)
X-Gm-Message-State: ALoCoQk3Pj8sjr3lEeRIGbd+/QlkzDpDE+sikB384nMd2jp3Ev3I8vkJVOKNedBGc9bEJpfFfkA4
MIME-Version: 1.0
X-Received: by 10.224.115.135 with SMTP id i7mr52493328qaq.50.1400553773471;
 Mon, 19 May 2014 19:42:53 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Mon, 19 May 2014 19:42:53 -0700 (PDT)
In-Reply-To: <CACBYxKLb=OK2EKu=2yS_1tswYz52rBF-59+8DZL+DfG6qHF52Q@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
	<CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
	<CAMAsSdKxE-biW8fRqkRn6NmqsszFgQ=f1w_Z8yt8cSOj3qBB2Q@mail.gmail.com>
	<CAEYYnxY9UZDuhBoHAP5kD3dCXxd0TwR761tt-kswjTEddQCCKg@mail.gmail.com>
	<CA+-p3AGT8E5caKU+2ExxjbpV-UqaaTot+Ni3DXEdOf37tADLgw@mail.gmail.com>
	<CAMAsSdJVckDE5=3S8wZj9P1-1-D6qBumGnofk9G7A7cksh1RVA@mail.gmail.com>
	<CABPQxstfSyVu6D-FSt7qa-o1co9AkhHsncTHGBrCK4rEM7fymg@mail.gmail.com>
	<CACBYxKLb=OK2EKu=2yS_1tswYz52rBF-59+8DZL+DfG6qHF52Q@mail.gmail.com>
Date: Mon, 19 May 2014 19:42:53 -0700
Message-ID: <CAEYYnxakT5SVD0xxWcyAXq+w-f-tY0rDA2c3YLRXK6oZ3ArF7Q@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bdcaa8c282fcf04f9cbd64e
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdcaa8c282fcf04f9cbd64e
Content-Type: text/plain; charset=UTF-8

Good summary! We fixed it in branch 0.9 since our production is still in
0.9. I'm porting it to 1.0 now, and hopefully will submit PR for 1.0
tonight.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Mon, May 19, 2014 at 7:38 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:

> It just hit me why this problem is showing up on YARN and not on
> standalone.
>
> The relevant difference between YARN and standalone is that, on YARN, the
> app jar is loaded by the system classloader instead of Spark's custom URL
> classloader.
>
> On YARN, the system classloader knows about [the classes in the spark jars,
> the classes in the primary app jar].   The custom classloader knows about
> [the classes in secondary app jars] and has the system classloader as its
> parent.
>
> A few relevant facts (mostly redundant with what Sean pointed out):
> * Every class has a classloader that loaded it.
> * When an object of class B is instantiated inside of class A, the
> classloader used for loading B is the classloader that was used for loading
> A.
> * When a classloader fails to load a class, it lets its parent classloader
> try.  If its parent succeeds, its parent becomes the "classloader that
> loaded it".
>
> So suppose class B is in a secondary app jar and class A is in the primary
> app jar:
> 1. The custom classloader will try to load class A.
> 2. It will fail, because it only knows about the secondary jars.
> 3. It will delegate to its parent, the system classloader.
> 4. The system classloader will succeed, because it knows about the primary
> app jar.
> 5. A's classloader will be the system classloader.
> 6. A tries to instantiate an instance of class B.
> 7. B will be loaded with A's classloader, which is the system classloader.
> 8. Loading B will fail, because A's classloader, which is the system
> classloader, doesn't know about the secondary app jars.
>
> In Spark standalone, A and B are both loaded by the custom classloader, so
> this issue doesn't come up.
>
> -Sandy
>
> On Mon, May 19, 2014 at 7:07 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> > Having a user add define a custom class inside of an added jar and
> > instantiate it directly inside of an executor is definitely supported
> > in Spark and has been for a really long time (several years). This is
> > something we do all the time in Spark.
> >
> > DB - I'd hold off on a re-architecting of this until we identify
> > exactly what is causing the bug you are running into.
> >
> > In a nutshell, when the bytecode "new Foo()" is run on the executor,
> > it will ask the driver for the class over HTTP using a custom
> > classloader. Something in that pipeline is breaking here, possibly
> > related to the YARN deployment stuff.
> >
> >
> > On Mon, May 19, 2014 at 12:29 AM, Sean Owen <sowen@cloudera.com> wrote:
> > > I don't think a customer classloader is necessary.
> > >
> > > Well, it occurs to me that this is no new problem. Hadoop, Tomcat, etc
> > > all run custom user code that creates new user objects without
> > > reflection. I should go see how that's done. Maybe it's totally valid
> > > to set the thread's context classloader for just this purpose, and I
> > > am not thinking clearly.
> > >
> > > On Mon, May 19, 2014 at 8:26 AM, Andrew Ash <andrew@andrewash.com>
> > wrote:
> > >> Sounds like the problem is that classloaders always look in their
> > parents
> > >> before themselves, and Spark users want executors to pick up classes
> > from
> > >> their custom code before the ones in Spark plus its dependencies.
> > >>
> > >> Would a custom classloader that delegates to the parent after first
> > >> checking itself fix this up?
> > >>
> > >>
> > >> On Mon, May 19, 2014 at 12:17 AM, DB Tsai <dbtsai@stanford.edu>
> wrote:
> > >>
> > >>> Hi Sean,
> > >>>
> > >>> It's true that the issue here is classloader, and due to the
> > classloader
> > >>> delegation model, users have to use reflection in the executors to
> > pick up
> > >>> the classloader in order to use those classes added by sc.addJars
> APIs.
> > >>> However, it's very inconvenience for users, and not documented in
> > spark.
> > >>>
> > >>> I'm working on a patch to solve it by calling the protected method
> > addURL
> > >>> in URLClassLoader to update the current default classloader, so no
> > >>> customClassLoader anymore. I wonder if this is an good way to go.
> > >>>
> > >>>   private def addURL(url: URL, loader: URLClassLoader){
> > >>>     try {
> > >>>       val method: Method =
> > >>> classOf[URLClassLoader].getDeclaredMethod("addURL", classOf[URL])
> > >>>       method.setAccessible(true)
> > >>>       method.invoke(loader, url)
> > >>>     }
> > >>>     catch {
> > >>>       case t: Throwable => {
> > >>>         throw new IOException("Error, could not add URL to system
> > >>> classloader")
> > >>>       }
> > >>>     }
> > >>>   }
> > >>>
> > >>>
> > >>>
> > >>> Sincerely,
> > >>>
> > >>> DB Tsai
> > >>> -------------------------------------------------------
> > >>> My Blog: https://www.dbtsai.com
> > >>> LinkedIn: https://www.linkedin.com/in/dbtsai
> > >>>
> > >>>
> > >>> On Sun, May 18, 2014 at 11:57 PM, Sean Owen <sowen@cloudera.com>
> > wrote:
> > >>>
> > >>> > I might be stating the obvious for everyone, but the issue here is
> > not
> > >>> > reflection or the source of the JAR, but the ClassLoader. The basic
> > >>> > rules are this.
> > >>> >
> > >>> > "new Foo" will use the ClassLoader that defines Foo. This is
> usually
> > >>> > the ClassLoader that loaded whatever it is that first referenced
> Foo
> > >>> > and caused it to be loaded -- usually the ClassLoader holding your
> > >>> > other app classes.
> > >>> >
> > >>> > ClassLoaders can have a parent-child relationship. ClassLoaders
> > always
> > >>> > look in their parent before themselves.
> > >>> >
> > >>> > (Careful then -- in contexts like Hadoop or Tomcat where your app
> is
> > >>> > loaded in a child ClassLoader, and you reference a class that
> Hadoop
> > >>> > or Tomcat also has (like a lib class) you will get the container's
> > >>> > version!)
> > >>> >
> > >>> > When you load an external JAR it has a separate ClassLoader which
> > does
> > >>> > not necessarily bear any relation to the one containing your app
> > >>> > classes, so yeah it is not generally going to make "new Foo" work.
> > >>> >
> > >>> > Reflection lets you pick the ClassLoader, yes.
> > >>> >
> > >>> > I would not call setContextClassLoader.
> > >>> >
> > >>> > On Mon, May 19, 2014 at 12:00 AM, Sandy Ryza <
> > sandy.ryza@cloudera.com>
> > >>> > wrote:
> > >>> > > I spoke with DB offline about this a little while ago and he
> > confirmed
> > >>> > that
> > >>> > > he was able to access the jar from the driver.
> > >>> > >
> > >>> > > The issue appears to be a general Java issue: you can't directly
> > >>> > > instantiate a class from a dynamically loaded jar.
> > >>> > >
> > >>> > > I reproduced it locally outside of Spark with:
> > >>> > > ---
> > >>> > >     URLClassLoader urlClassLoader = new URLClassLoader(new URL[]
> {
> > new
> > >>> > > File("myotherjar.jar").toURI().toURL() }, null);
> > >>> > >     Thread.currentThread().setContextClassLoader(urlClassLoader);
> > >>> > >     MyClassFromMyOtherJar obj = new MyClassFromMyOtherJar();
> > >>> > > ---
> > >>> > >
> > >>> > > I was able to load the class with reflection.
> > >>> >
> > >>>
> >
>

--047d7bdcaa8c282fcf04f9cbd64e--

From dev-return-7717-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 03:20:22 2014
Return-Path: <dev-return-7717-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7D7C411D0E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 03:20:22 +0000 (UTC)
Received: (qmail 60084 invoked by uid 500); 20 May 2014 03:20:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60025 invoked by uid 500); 20 May 2014 03:20:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60017 invoked by uid 99); 20 May 2014 03:20:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 03:20:22 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.176] (HELO mail-vc0-f176.google.com) (209.85.220.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 03:20:18 +0000
Received: by mail-vc0-f176.google.com with SMTP id lg15so10500010vcb.7
        for <dev@spark.apache.org>; Mon, 19 May 2014 20:19:54 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=JvDeYKlGhDOUT0AYbybc9z97IvwVzStT8fCIXmYwarQ=;
        b=NWKbsfK5paIb67aagRjV8yuWIVZy7GXrg+f5f+TjenHVhCZuT1NFFxvXJAd63CzhTf
         4leb8JbCf+tK3bRJUo9+xp0qQCl3+aPxBD1+i+6qBOY0nUn60A/kjuZ+VgYOHxLcmCjH
         U2CjpxeDS7Y/lWEVGtoR4Pmsc8I1KRpv9/j8/6Man27Yg36FgM1HR7ZnAdzQyqby20tW
         5eDAf0x3Fk0daWfipiIyYHC3IAZ3zSa/uR4Gz9QWV6UnlVG/wbCOH5QlgepuHREu3N4M
         FhaZPW0kJ0bN3PR+jfVUs9zPXD2+KY9fpm1ggkbBJqoQXY4wPHC4A6h/e+HqJGPLQLGu
         qPxg==
X-Gm-Message-State: ALoCoQn+nxMaYqLaynW+cqIuZuEvawcRzlFMj5q03qW5pGj8qLfH5tibIG6texnb96ZPt3yKF27b
X-Received: by 10.52.13.98 with SMTP id g2mr1229226vdc.46.1400555994725;
        Mon, 19 May 2014 20:19:54 -0700 (PDT)
Received: from mail-vc0-f175.google.com (mail-vc0-f175.google.com [209.85.220.175])
        by mx.google.com with ESMTPSA id wb2sm21741959veb.13.2014.05.19.20.19.52
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 19 May 2014 20:19:52 -0700 (PDT)
Received: by mail-vc0-f175.google.com with SMTP id hu19so10699326vcb.34
        for <dev@spark.apache.org>; Mon, 19 May 2014 20:19:52 -0700 (PDT)
X-Received: by 10.58.209.233 with SMTP id mp9mr22132026vec.30.1400555992448;
 Mon, 19 May 2014 20:19:52 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Mon, 19 May 2014 20:19:32 -0700 (PDT)
In-Reply-To: <5F94CBFB-5870-4C11-8576-88BBAAD353BD@gmail.com>
References: <CA+-p3AHJOsDSSyYBBNbuocbWTA88VQk5XTCG-DmW9ohdeCoyGA@mail.gmail.com>
 <5F94CBFB-5870-4C11-8576-88BBAAD353BD@gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 19 May 2014 20:19:32 -0700
Message-ID: <CA+-p3AHCTt1Ta1Z5O9yQjYxvbfhbv72vE+arfRWtuu6=wTzL+w@mail.gmail.com>
Subject: Re: TorrentBroadcast aka Cornet?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bd6c3186b08b804f9cc5a3e
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd6c3186b08b804f9cc5a3e
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Thanks for the info Matei.

Andrew


On Mon, May 19, 2014 at 12:38 AM, Matei Zaharia <matei.zaharia@gmail.com>wr=
ote:

> TorrentBroadcast is actually slightly simpler, but it=E2=80=99s based on =
that. It
> has similar performance. I=E2=80=99d like to make it the default in a fut=
ure
> version, we just haven=E2=80=99t had a ton of testing with it yet (kind o=
f an
> oversight in this release unfortunately).
>
> Matei
>
> On May 19, 2014, at 12:07 AM, Andrew Ash <andrew@andrewash.com> wrote:
>
> > Hi Spark devs,
> >
> > Is the algorithm for
> > TorrentBroadcast<
> https://github.com/apache/spark/blob/master/core/src/main/scala/org/apach=
e/spark/broadcast/TorrentBroadcast.scala
> >the
> > same as Cornet from the below paper?
> >
> > http://www.mosharaf.com/wp-content/uploads/orchestra-sigcomm11.pdf
> >
> > If so it would be nice to include a link to the paper in the Javadoc fo=
r
> > the class.
> >
> > Thanks!
> > Andrew
>
>

--047d7bd6c3186b08b804f9cc5a3e--

From dev-return-7718-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 05:01:50 2014
Return-Path: <dev-return-7718-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CAF9011F43
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 05:01:50 +0000 (UTC)
Received: (qmail 9500 invoked by uid 500); 20 May 2014 05:01:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9443 invoked by uid 500); 20 May 2014 05:01:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9433 invoked by uid 99); 20 May 2014 05:01:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 05:01:50 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of nitinpanj@gmail.com does not designate 216.139.250.139 as permitted sender)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 05:01:47 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <nitinpanj@gmail.com>)
	id 1WmcAc-0000V1-K6
	for dev@spark.incubator.apache.org; Mon, 19 May 2014 22:00:58 -0700
Date: Mon, 19 May 2014 22:00:43 -0700 (PDT)
From: nit <nitinpanj@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1400562043605-6710.post@n3.nabble.com>
In-Reply-To: <CANJXpKcnrxn8xtAXFpORE5AGtzi0PM8Xs3otZt0m7e4rDL1KaQ@mail.gmail.com>
References: <1400550521233-6698.post@n3.nabble.com> <D32E235E9849451783DBF2134CC97FC9@gmail.com> <CAAsvFPkToNizLDzO5DgTUuK2H-xGsAJg5+nmZJEjvJf2ZrJiZg@mail.gmail.com> <CABPQxstH2ysQzvW62U46yXNzQ954TUc=peprP-zVeumcHnofaw@mail.gmail.com> <CANJXpKcnrxn8xtAXFpORE5AGtzi0PM8Xs3otZt0m7e4rDL1KaQ@mail.gmail.com>
Subject: Re: spark 1.0 standalone application
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks everyone. I followed Patrick's suggestion and it worked like a charm.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/spark-1-0-standalone-application-tp6698p6710.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7719-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 05:05:26 2014
Return-Path: <dev-return-7719-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9F20111F54
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 05:05:26 +0000 (UTC)
Received: (qmail 12605 invoked by uid 500); 20 May 2014 05:05:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12543 invoked by uid 500); 20 May 2014 05:05:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12533 invoked by uid 99); 20 May 2014 05:05:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 05:05:26 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 05:05:22 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 9A7E0101CB3
	for <dev@spark.apache.org>; Mon, 19 May 2014 22:05:01 -0700 (PDT)
Received: from mail-qc0-f176.google.com (mail-qc0-f176.google.com [209.85.216.176])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id D8C7C101CA4
	for <dev@spark.apache.org>; Mon, 19 May 2014 22:04:59 -0700 (PDT)
Received: by mail-qc0-f176.google.com with SMTP id r5so10537026qcx.7
        for <dev@spark.apache.org>; Mon, 19 May 2014 22:04:59 -0700 (PDT)
X-Gm-Message-State: ALoCoQnsfNvGSeKbou5gdf+3rq3WnhVFRpiet0GQ3VzMnzRgHmwsILpbWccC5TSHiqbVD3CSuE44
MIME-Version: 1.0
X-Received: by 10.224.15.137 with SMTP id k9mr11158578qaa.104.1400562299023;
 Mon, 19 May 2014 22:04:59 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Mon, 19 May 2014 22:04:58 -0700 (PDT)
In-Reply-To: <CAEYYnxakT5SVD0xxWcyAXq+w-f-tY0rDA2c3YLRXK6oZ3ArF7Q@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
	<CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
	<CAMAsSdKxE-biW8fRqkRn6NmqsszFgQ=f1w_Z8yt8cSOj3qBB2Q@mail.gmail.com>
	<CAEYYnxY9UZDuhBoHAP5kD3dCXxd0TwR761tt-kswjTEddQCCKg@mail.gmail.com>
	<CA+-p3AGT8E5caKU+2ExxjbpV-UqaaTot+Ni3DXEdOf37tADLgw@mail.gmail.com>
	<CAMAsSdJVckDE5=3S8wZj9P1-1-D6qBumGnofk9G7A7cksh1RVA@mail.gmail.com>
	<CABPQxstfSyVu6D-FSt7qa-o1co9AkhHsncTHGBrCK4rEM7fymg@mail.gmail.com>
	<CACBYxKLb=OK2EKu=2yS_1tswYz52rBF-59+8DZL+DfG6qHF52Q@mail.gmail.com>
	<CAEYYnxakT5SVD0xxWcyAXq+w-f-tY0rDA2c3YLRXK6oZ3ArF7Q@mail.gmail.com>
Date: Mon, 19 May 2014 22:04:58 -0700
Message-ID: <CAEYYnxaYfaUOes+A4kR4w+nh0LS8fgJytM2Nj7xoBErpHiRwbA@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bdc912e51c88c04f9cdd2a5
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc912e51c88c04f9cdd2a5
Content-Type: text/plain; charset=UTF-8

In 1.0, there is a new option for users to choose which classloader has
higher priority via spark.files.userClassPathFirst, I decided to submit the
PR for 0.9 first. We use this patch in our lab and we can use those jars
added by sc.addJar without reflection.

https://github.com/apache/spark/pull/834

Can anyone comment if it's a good approach?

Thanks.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Mon, May 19, 2014 at 7:42 PM, DB Tsai <dbtsai@stanford.edu> wrote:

> Good summary! We fixed it in branch 0.9 since our production is still in
> 0.9. I'm porting it to 1.0 now, and hopefully will submit PR for 1.0
> tonight.
>
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Mon, May 19, 2014 at 7:38 PM, Sandy Ryza <sandy.ryza@cloudera.com>wrote:
>
>> It just hit me why this problem is showing up on YARN and not on
>> standalone.
>>
>> The relevant difference between YARN and standalone is that, on YARN, the
>> app jar is loaded by the system classloader instead of Spark's custom URL
>> classloader.
>>
>> On YARN, the system classloader knows about [the classes in the spark
>> jars,
>> the classes in the primary app jar].   The custom classloader knows about
>> [the classes in secondary app jars] and has the system classloader as its
>> parent.
>>
>> A few relevant facts (mostly redundant with what Sean pointed out):
>> * Every class has a classloader that loaded it.
>> * When an object of class B is instantiated inside of class A, the
>> classloader used for loading B is the classloader that was used for
>> loading
>> A.
>> * When a classloader fails to load a class, it lets its parent classloader
>> try.  If its parent succeeds, its parent becomes the "classloader that
>> loaded it".
>>
>> So suppose class B is in a secondary app jar and class A is in the primary
>> app jar:
>> 1. The custom classloader will try to load class A.
>> 2. It will fail, because it only knows about the secondary jars.
>> 3. It will delegate to its parent, the system classloader.
>> 4. The system classloader will succeed, because it knows about the primary
>> app jar.
>> 5. A's classloader will be the system classloader.
>> 6. A tries to instantiate an instance of class B.
>> 7. B will be loaded with A's classloader, which is the system classloader.
>> 8. Loading B will fail, because A's classloader, which is the system
>> classloader, doesn't know about the secondary app jars.
>>
>> In Spark standalone, A and B are both loaded by the custom classloader, so
>> this issue doesn't come up.
>>
>> -Sandy
>>
>> On Mon, May 19, 2014 at 7:07 PM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>> > Having a user add define a custom class inside of an added jar and
>> > instantiate it directly inside of an executor is definitely supported
>> > in Spark and has been for a really long time (several years). This is
>> > something we do all the time in Spark.
>> >
>> > DB - I'd hold off on a re-architecting of this until we identify
>> > exactly what is causing the bug you are running into.
>> >
>> > In a nutshell, when the bytecode "new Foo()" is run on the executor,
>> > it will ask the driver for the class over HTTP using a custom
>> > classloader. Something in that pipeline is breaking here, possibly
>> > related to the YARN deployment stuff.
>> >
>> >
>> > On Mon, May 19, 2014 at 12:29 AM, Sean Owen <sowen@cloudera.com> wrote:
>> > > I don't think a customer classloader is necessary.
>> > >
>> > > Well, it occurs to me that this is no new problem. Hadoop, Tomcat, etc
>> > > all run custom user code that creates new user objects without
>> > > reflection. I should go see how that's done. Maybe it's totally valid
>> > > to set the thread's context classloader for just this purpose, and I
>> > > am not thinking clearly.
>> > >
>> > > On Mon, May 19, 2014 at 8:26 AM, Andrew Ash <andrew@andrewash.com>
>> > wrote:
>> > >> Sounds like the problem is that classloaders always look in their
>> > parents
>> > >> before themselves, and Spark users want executors to pick up classes
>> > from
>> > >> their custom code before the ones in Spark plus its dependencies.
>> > >>
>> > >> Would a custom classloader that delegates to the parent after first
>> > >> checking itself fix this up?
>> > >>
>> > >>
>> > >> On Mon, May 19, 2014 at 12:17 AM, DB Tsai <dbtsai@stanford.edu>
>> wrote:
>> > >>
>> > >>> Hi Sean,
>> > >>>
>> > >>> It's true that the issue here is classloader, and due to the
>> > classloader
>> > >>> delegation model, users have to use reflection in the executors to
>> > pick up
>> > >>> the classloader in order to use those classes added by sc.addJars
>> APIs.
>> > >>> However, it's very inconvenience for users, and not documented in
>> > spark.
>> > >>>
>> > >>> I'm working on a patch to solve it by calling the protected method
>> > addURL
>> > >>> in URLClassLoader to update the current default classloader, so no
>> > >>> customClassLoader anymore. I wonder if this is an good way to go.
>> > >>>
>> > >>>   private def addURL(url: URL, loader: URLClassLoader){
>> > >>>     try {
>> > >>>       val method: Method =
>> > >>> classOf[URLClassLoader].getDeclaredMethod("addURL", classOf[URL])
>> > >>>       method.setAccessible(true)
>> > >>>       method.invoke(loader, url)
>> > >>>     }
>> > >>>     catch {
>> > >>>       case t: Throwable => {
>> > >>>         throw new IOException("Error, could not add URL to system
>> > >>> classloader")
>> > >>>       }
>> > >>>     }
>> > >>>   }
>> > >>>
>> > >>>
>> > >>>
>> > >>> Sincerely,
>> > >>>
>> > >>> DB Tsai
>> > >>> -------------------------------------------------------
>> > >>> My Blog: https://www.dbtsai.com
>> > >>> LinkedIn: https://www.linkedin.com/in/dbtsai
>> > >>>
>> > >>>
>> > >>> On Sun, May 18, 2014 at 11:57 PM, Sean Owen <sowen@cloudera.com>
>> > wrote:
>> > >>>
>> > >>> > I might be stating the obvious for everyone, but the issue here is
>> > not
>> > >>> > reflection or the source of the JAR, but the ClassLoader. The
>> basic
>> > >>> > rules are this.
>> > >>> >
>> > >>> > "new Foo" will use the ClassLoader that defines Foo. This is
>> usually
>> > >>> > the ClassLoader that loaded whatever it is that first referenced
>> Foo
>> > >>> > and caused it to be loaded -- usually the ClassLoader holding your
>> > >>> > other app classes.
>> > >>> >
>> > >>> > ClassLoaders can have a parent-child relationship. ClassLoaders
>> > always
>> > >>> > look in their parent before themselves.
>> > >>> >
>> > >>> > (Careful then -- in contexts like Hadoop or Tomcat where your app
>> is
>> > >>> > loaded in a child ClassLoader, and you reference a class that
>> Hadoop
>> > >>> > or Tomcat also has (like a lib class) you will get the container's
>> > >>> > version!)
>> > >>> >
>> > >>> > When you load an external JAR it has a separate ClassLoader which
>> > does
>> > >>> > not necessarily bear any relation to the one containing your app
>> > >>> > classes, so yeah it is not generally going to make "new Foo" work.
>> > >>> >
>> > >>> > Reflection lets you pick the ClassLoader, yes.
>> > >>> >
>> > >>> > I would not call setContextClassLoader.
>> > >>> >
>> > >>> > On Mon, May 19, 2014 at 12:00 AM, Sandy Ryza <
>> > sandy.ryza@cloudera.com>
>> > >>> > wrote:
>> > >>> > > I spoke with DB offline about this a little while ago and he
>> > confirmed
>> > >>> > that
>> > >>> > > he was able to access the jar from the driver.
>> > >>> > >
>> > >>> > > The issue appears to be a general Java issue: you can't directly
>> > >>> > > instantiate a class from a dynamically loaded jar.
>> > >>> > >
>> > >>> > > I reproduced it locally outside of Spark with:
>> > >>> > > ---
>> > >>> > >     URLClassLoader urlClassLoader = new URLClassLoader(new
>> URL[] {
>> > new
>> > >>> > > File("myotherjar.jar").toURI().toURL() }, null);
>> > >>> > >
>> Thread.currentThread().setContextClassLoader(urlClassLoader);
>> > >>> > >     MyClassFromMyOtherJar obj = new MyClassFromMyOtherJar();
>> > >>> > > ---
>> > >>> > >
>> > >>> > > I was able to load the class with reflection.
>> > >>> >
>> > >>>
>> >
>>
>
>

--047d7bdc912e51c88c04f9cdd2a5--

From dev-return-7720-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 05:15:14 2014
Return-Path: <dev-return-7720-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 259C811F7E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 05:15:14 +0000 (UTC)
Received: (qmail 26211 invoked by uid 500); 20 May 2014 05:15:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26152 invoked by uid 500); 20 May 2014 05:15:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26144 invoked by uid 99); 20 May 2014 05:15:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 05:15:13 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.171 as permitted sender)
Received: from [209.85.214.171] (HELO mail-ob0-f171.google.com) (209.85.214.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 05:15:10 +0000
Received: by mail-ob0-f171.google.com with SMTP id wn1so7318587obc.30
        for <dev@spark.apache.org>; Mon, 19 May 2014 22:14:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type:content-transfer-encoding;
        bh=GFJmqw+Fqq5+4mAtMHT52qRiSfxH1Frrzw5YgFCCde8=;
        b=V99+QkzbjXbEJiNctRQRho4531UsxqHYzKcesZeFhd5eqItSL7TG6YkDMpBQ0RQAdO
         lEmiAWunN4WvBuIZNFI4O+ppDZRG6UUS+gcz9G39qwh+3RtUD5USrjmBLvR9kLdpao4m
         IAQPwHi1QO0Q9fPK2lSNN91qiHeDXtwd6yOwJ/Asw6+59TiK2f6ZKWTEvP9GB8w0Rfq2
         PRUWNWb6u7NjkjYc7Jhg2U1ehe2oAcPy/6v3HSJucL+Nj0u0HnnL34eZ0T7mAp5ZDC+v
         ZxzgiJvygtvmWw6h9wUXpN+OTJl0pJfr41fNR0QTd2hR6pNRotP5U9NwYoVT6sCkYpLl
         3aQg==
MIME-Version: 1.0
X-Received: by 10.182.102.99 with SMTP id fn3mr39946494obb.57.1400562889511;
 Mon, 19 May 2014 22:14:49 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Mon, 19 May 2014 22:14:49 -0700 (PDT)
In-Reply-To: <658C73382C6845FC939110E178058EF7@gmail.com>
References: <CABPQxstaxcbGSQTqUxSSUQfRpjQs=BqRkrEVvzOXbXZo8kzShg@mail.gmail.com>
	<CABPQxstbwN64rQvx6kUoESkO4GJOohYSAO+s_F85wFWjb6T1zw@mail.gmail.com>
	<CAAsvFPn2RwpFMz7ajy=t8GSjiKSPqARgDtPqXk007Cd7_xvA-Q@mail.gmail.com>
	<CAMJOb8nTR38Uf50xVxv5O+OrBkO0ELDFYiHrziBVm9qknAgpYA@mail.gmail.com>
	<CACBYxKLR3xE4FSqZQwrjmP+bhhqy_mAat0hHSu+1pOBEVTttrA@mail.gmail.com>
	<9849C2AA-3131-4E31-85DF-7F6D61632545@gmail.com>
	<4D62B3E0-B6E7-4916-953D-A196FC6BC24F@gmail.com>
	<CABPQxssnZ=aq6djG-YL=CtajhRhnT945qGcNhAq6RZEdDDU0tA@mail.gmail.com>
	<tencent_60271E3C31F67D7B32E81699@qq.com>
	<658C73382C6845FC939110E178058EF7@gmail.com>
Date: Mon, 19 May 2014 22:14:49 -0700
Message-ID: <CABPQxssbmTBUefVSGuME-01o1z3fi_xqycB9wjQuwaBv-GTxDw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc9)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

We're cancelling this RC in favor of rc10. There were two blockers: an
issue with Windows run scripts and an issue with the packaging for
Hadoop 1 when hive support is bundled.

https://issues.apache.org/jira/browse/SPARK-1875
https://issues.apache.org/jira/browse/SPARK-1876

Thanks everyone for the testing. TD will be cutting rc10, since I'm
travelling this week (thanks TD!).

- Patrick

On Mon, May 19, 2014 at 7:06 PM, Nan Zhu <zhunanmcgill@gmail.com> wrote:
> just rerun my test on rc5
>
> everything works
>
> build applications with sbt and the spark-*.jar which is compiled with Ha=
doop 2.3
>
> +1
>
> --
> Nan Zhu
>
>
> On Sunday, May 18, 2014 at 11:07 PM, witgo wrote:
>
>> How to reproduce this bug?
>>
>>
>> ------------------ Original ------------------
>> From: "Patrick Wendell";<pwendell@gmail.com (mailto:pwendell@gmail.com)>=
;
>> Date: Mon, May 19, 2014 10:08 AM
>> To: "dev@spark.apache.org (mailto:dev@spark.apache.org)"<dev@spark.apach=
e.org (mailto:dev@spark.apache.org)>;
>> Cc: "Tom Graves"<tgraves_cs@yahoo.com (mailto:tgraves_cs@yahoo.com)>;
>> Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc9)
>>
>>
>>
>> Hey Matei - the issue you found is not related to security. This patch
>> a few days ago broke builds for Hadoop 1 with YARN support enabled.
>> The patch directly altered the way we deal with commons-lang
>> dependency, which is what is at the base of this stack trace.
>>
>> https://github.com/apache/spark/pull/754
>>
>> - Patrick
>>
>> On Sun, May 18, 2014 at 5:28 PM, Matei Zaharia <matei.zaharia@gmail.com =
(mailto:matei.zaharia@gmail.com)> wrote:
>> > Alright, I've opened https://github.com/apache/spark/pull/819 with the=
 Windows fixes. I also found one other likely bug, https://issues.apache.or=
g/jira/browse/SPARK-1875, in the binary packages for Hadoop1 built in this =
RC. I think this is due to Hadoop 1's security code depending on a differen=
t version of org.apache.commons than Hadoop 2, but it needs investigation. =
Tom, any thoughts on this?
>> >
>> > Matei
>> >
>> > On May 18, 2014, at 12:33 PM, Matei Zaharia <matei.zaharia@gmail.com (=
mailto:matei.zaharia@gmail.com)> wrote:
>> >
>> > > I took the always fun task of testing it on Windows, and unfortunate=
ly, I found some small problems with the prebuilt packages due to recent ch=
anges to the launch scripts: bin/spark-class2.cmd looks in ./jars instead o=
f ./lib for the assembly JAR, and bin/run-example2.cmd doesn't quite match =
the master-setting behavior of the Unix based one. I'll send a pull request=
 to fix them soon.
>> > >
>> > > Matei
>> > >
>> > >
>> > > On May 17, 2014, at 11:32 AM, Sandy Ryza <sandy.ryza@cloudera.com (m=
ailto:sandy.ryza@cloudera.com)> wrote:
>> > >
>> > > > +1
>> > > >
>> > > > Reran my tests from rc5:
>> > > >
>> > > > * Built the release from source.
>> > > > * Compiled Java and Scala apps that interact with HDFS against it.
>> > > > * Ran them in local mode.
>> > > > * Ran them against a pseudo-distributed YARN cluster in both yarn-=
client
>> > > > mode and yarn-cluster mode.
>> > > >
>> > > >
>> > > > On Sat, May 17, 2014 at 10:08 AM, Andrew Or <andrew@databricks.com=
 (mailto:andrew@databricks.com)> wrote:
>> > > >
>> > > > > +1
>> > > > >
>> > > > >
>> > > > > 2014-05-17 8:53 GMT-07:00 Mark Hamstra <mark@clearstorydata.com =
(mailto:mark@clearstorydata.com)>:
>> > > > >
>> > > > > > +1
>> > > > > >
>> > > > > >
>> > > > > > On Sat, May 17, 2014 at 12:58 AM, Patrick Wendell <pwendell@gm=
ail.com (mailto:pwendell@gmail.com)
>> > > > > > > wrote:
>> > > > > >
>> > > > > >
>> > > > > > > I'll start the voting with a +1.
>> > > > > > >
>> > > > > > > On Sat, May 17, 2014 at 12:58 AM, Patrick Wendell <pwendell@=
gmail.com (mailto:pwendell@gmail.com)>
>> > > > > > > wrote:
>> > > > > > > > Please vote on releasing the following candidate as Apache=
 Spark
>> > > > > > >
>> > > > > > >
>> > > > > >
>> > > > > > version
>> > > > > > > 1.0.0!
>> > > > > > > > This has one bug fix and one minor feature on top of rc8:
>> > > > > > > > SPARK-1864: https://github.com/apache/spark/pull/808
>> > > > > > > > SPARK-1808: https://github.com/apache/spark/pull/799
>> > > > > > > >
>> > > > > > > > The tag to be voted on is v1.0.0-rc9 (commit 920f947):
>> > > > > https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit=
;h=3D920f947eb5a22a679c0c3186cf69ee75f6041c75
>> > > > > > > >
>> > > > > > > > The release files, including signatures, digests, etc. can=
 be found
>> > > > > at:
>> > > > > > > > http://people.apache.org/~pwendell/spark-1.0.0-rc9/
>> > > > > > > >
>> > > > > > > > Release artifacts are signed with the following key:
>> > > > > > > > https://people.apache.org/keys/committer/pwendell.asc
>> > > > > > > >
>> > > > > > > > The staging repository for this release can be found at:
>> > > > > > https://repository.apache.org/content/repositories/orgapachesp=
ark-1017/
>> > > > > > > >
>> > > > > > > > The documentation corresponding to this release can be fou=
nd at:
>> > > > > > > > http://people.apache.org/~pwendell/spark-1.0.0-rc9-docs/
>> > > > > > > >
>> > > > > > > > Please vote on releasing this package as Apache Spark 1.0.=
0!
>> > > > > > > >
>> > > > > > > > The vote is open until Tuesday, May 20, at 08:56 UTC and p=
asses if
>> > > > > > > > amajority of at least 3 +1 PMC votes are cast.
>> > > > > > > >
>> > > > > > > > [ ] +1 Release this package as Apache Spark 1.0.0
>> > > > > > > > [ ] -1 Do not release this package because ...
>> > > > > > > >
>> > > > > > > > To learn more about Apache Spark, please see
>> > > > > > > > http://spark.apache.org/
>> > > > > > > >
>> > > > > > > > =3D=3D API Changes =3D=3D
>> > > > > > > > We welcome users to compile Spark applications against 1.0=
. There are
>> > > > > > > > a few API changes in this release. Here are links to the a=
ssociated
>> > > > > > > > upgrade guides - user facing changes have been kept as sma=
ll as
>> > > > > > > > possible.
>> > > > > > > >
>> > > > > > > > changes to ML vector specification:
>> > > > > http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-gu=
ide.html#from-09-to-10
>> > > > > > > >
>> > > > > > > > changes to the Java API:
>> > > > > http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-pro=
gramming-guide.html#upgrading-from-pre-10-versions-of-spark
>> > > > > > > >
>> > > > > > > > changes to the streaming API:
>> > > > > http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streamin=
g-programming-guide.html#migration-guide-from-091-or-below-to-1x
>> > > > > > > >
>> > > > > > > > changes to the GraphX API:
>> > > > > http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-p=
rogramming-guide.html#upgrade-guide-from-spark-091
>> > > > > > > >
>> > > > > > > > coGroup and related functions now return Iterable[T] inste=
ad of
>> > > > > Seq[T]
>> > > > > > > > =3D=3D> Call toSeq on the result to restore the old behavi=
or
>> > > > > > > >
>> > > > > > > > SparkContext.jarOfClass returns Option[String] instead of =
Seq[String]
>> > > > > > > > =3D=3D> Call toSeq on the result to restore old behavior
>> > > > > > > >
>> > > > > > >
>> > > > > >
>> > > > >
>> > > > >
>> > > >
>> > > >
>> > >
>> > >
>> >
>> >
>>
>>
>>
>
>

From dev-return-7721-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 05:16:08 2014
Return-Path: <dev-return-7721-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1C23611F83
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 05:16:08 +0000 (UTC)
Received: (qmail 30581 invoked by uid 500); 20 May 2014 05:16:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30524 invoked by uid 500); 20 May 2014 05:16:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30516 invoked by uid 99); 20 May 2014 05:16:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 05:16:07 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.192.52 as permitted sender)
Received: from [209.85.192.52] (HELO mail-qg0-f52.google.com) (209.85.192.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 05:16:03 +0000
Received: by mail-qg0-f52.google.com with SMTP id a108so10543283qge.11
        for <dev@spark.apache.org>; Mon, 19 May 2014 22:15:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=dHxQ4j+RptwtqBjGZK8dKwtVgd40ljeJWrOdvKGjsjk=;
        b=QbcKOW6SaC50yjTitEnSmyWlWsqQFSH0fR1qp03kPIdDWBd+XEzDzB2GsyHAMJJYF+
         ZyNTEyTrZROzVuvlQAh9OKKhl8nIhuNtt2AriJu17vIh0nybUVYulvVhDYZFo11f8gSF
         mBGHXpLQviZH7MZsy998jbvect9iaBBPC3T/ypXOOUsM1/YjtvPa0rZ4VTr79nmigF9X
         dKgypxJo/LZ2QLfy/6K51yqutXWRNc/O9OTaV3eZGhfnXRIbcQXcH7ZvFM+IgoVDCeUd
         vZslv+uyy1keltceIrvOZAdCBqmRnDmV1DDw7gKLC/lQVu2D2+7f7kHNsWomroQkIpRv
         kQiA==
X-Received: by 10.224.64.132 with SMTP id e4mr53803095qai.16.1400562940291;
        Mon, 19 May 2014 22:15:40 -0700 (PDT)
Received: from [192.168.2.13] (bas3-montreal42-1168084016.dsl.bell.ca. [69.159.140.48])
        by mx.google.com with ESMTPSA id q10sm31053750qah.9.2014.05.19.22.15.39
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Mon, 19 May 2014 22:15:39 -0700 (PDT)
Date: Tue, 20 May 2014 01:23:40 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Message-ID: <FC40ACDFAFFB42BBBE97A77E16F19DA2@gmail.com>
In-Reply-To: <CABPQxstH2ysQzvW62U46yXNzQ954TUc=peprP-zVeumcHnofaw@mail.gmail.com>
References: <1400550521233-6698.post@n3.nabble.com>
 <D32E235E9849451783DBF2134CC97FC9@gmail.com>
 <CAAsvFPkToNizLDzO5DgTUuK2H-xGsAJg5+nmZJEjvJf2ZrJiZg@mail.gmail.com>
 <CABPQxstH2ysQzvW62U46yXNzQ954TUc=peprP-zVeumcHnofaw@mail.gmail.com>
Subject: Re: spark 1.0 standalone application
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="537ae6dc_1816f8c4_179"
X-Virus-Checked: Checked by ClamAV on apache.org

--537ae6dc_1816f8c4_179
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

=46irst time to know there is a temporary maven repository=E2=80=A6=E2=80=
=A6.

-- =20
Nan Zhu


On Monday, May 19, 2014 at 10:10 PM, Patrick Wendell wrote:

> Whenever we publish a release candidate, we create a temporary maven
> repository that host the artifacts. We do this precisely for the case
> you are running into (where a user wants to build an application
> against it to test).
> =20
> You can build against the release candidate by just adding that
> repository in your sbt build, then linking against =22spark-core=22
> version =221.0.0=22. =46or rc9 the repository is in the vote e-mail:
> =20
> http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-=
Apache-Spark-1-0-0-rc9-td6629.html
> =20
> On Mon, May 19, 2014 at 7:03 PM, Mark Hamstra <mark=40clearstorydata.co=
m (mailto:mark=40clearstorydata.com)> wrote:
> > That's the crude way to do it. If you run =60sbt/sbt publishLocal=60,=
 then you
> > can resolve the artifact from your local cache in the same way that y=
ou
> > would resolve it if it were deployed to a remote cache. That's just t=
he
> > build step. Actually running the application will require the necessa=
ry
> > jars to be accessible by the cluster nodes.
> > =20
> > =20
> > On Mon, May 19, 2014 at 7:04 PM, Nan Zhu <zhunanmcgill=40gmail.com (m=
ailto:zhunanmcgill=40gmail.com)> wrote:
> > =20
> > > en, you have to put spark-assembly-*.jar to the lib directory of yo=
ur
> > > application
> > > =20
> > > Best,
> > > =20
> > > --
> > > Nan Zhu
> > > =20
> > > =20
> > > On Monday, May 19, 2014 at 9:48 PM, nit wrote:
> > > =20
> > > > I am not much comfortable with sbt. I want to build a standalone
> > > application
> > > > using spark 1.0 RC9. I can build sbt assembly for my application =
with
> > > =20
> > > Spark
> > > > 0.9.1, and I think in that case spark is pulled from Aka Reposito=
ry=3F
> > > > =20
> > > > Now if I want to use 1.0 RC9 for my application; what is the proc=
ess =3F
> > > > (=46YI, I was able to build spark-1.0 via sbt/assembly and I can =
see
> > > > sbt-assembly jar; and I think I will have to copy my jar somewher=
e=3F and
> > > > update build.sbt=3F)
> > > > =20
> > > > PS: I am not sure if this is the right place for this question; b=
ut since
> > > > 1.0 is still RC, I felt that this may be appropriate forum.
> > > > =20
> > > > thank=21
> > > > =20
> > > > =20
> > > > =20
> > > > --
> > > > View this message in context:
> > > > =20
> > > =20
> > > http://apache-spark-developers-list.1001551.n3.nabble.com/spark-1-0=
-standalone-application-tp6698.html
> > > > Sent from the Apache Spark Developers List mailing list archive a=
t
> > > =20
> > > Nabble.com (http://Nabble.com).
> > > > =20
> > > =20
> > > =20
> > =20
> > =20
> =20
> =20
> =20



--537ae6dc_1816f8c4_179--


From dev-return-7722-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 05:25:47 2014
Return-Path: <dev-return-7722-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ACEBE11FA6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 05:25:47 +0000 (UTC)
Received: (qmail 42080 invoked by uid 500); 20 May 2014 05:25:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42024 invoked by uid 500); 20 May 2014 05:25:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42014 invoked by uid 99); 20 May 2014 05:25:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 05:25:47 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of shivaram@berkeley.edu designates 74.125.82.182 as permitted sender)
Received: from [74.125.82.182] (HELO mail-we0-f182.google.com) (74.125.82.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 05:25:43 +0000
Received: by mail-we0-f182.google.com with SMTP id t60so6387771wes.41
        for <dev@spark.apache.org>; Mon, 19 May 2014 22:25:22 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:in-reply-to:references
         :date:message-id:subject:from:to:content-type;
        bh=ftY6MXISUos1pDsNFRcd9KpwwwmEddFIpXtcQw0iwlM=;
        b=bxH44ktFlW/lTn8/nM990Y3YHKzDy/6bJVH8AZAu9akcKTRpTQ7LLEot9ILIftOMPw
         0FnfS/5ySAqtrInhg1GMHFvRxU2M0XsBfu4u++tLxhfefKwCayhd9GRVu9Wnv9YNimVv
         W6L/gDs5hu8Shq76QJnKj+zgUY0uJVn0MjB4Q3sdFvWANQ0Wc6nW0kARh4CMxbmEcXZW
         e3vb0Cym3ZyjALtUSJxmoHoeVFimkE7ahC4LBIGKL2iUEYm/NNMoQY0OO5my4bnX+C8p
         zzeSjcWEG8gN/qv2tLJ5BwP9ZVbd22qlhm9q/uIVcjFgHCXZrfOrrZEe+t6LjAYvhPmI
         CFJQ==
X-Gm-Message-State: ALoCoQk2MdV4uneTcg/e1AeFCJxOe81k3orkgdCSOTkAA+aKg9KzxkanHQb49sapuN/auQ3kVNuk
MIME-Version: 1.0
X-Received: by 10.194.84.101 with SMTP id x5mr22793520wjy.52.1400563521653;
 Mon, 19 May 2014 22:25:21 -0700 (PDT)
Reply-To: shivaram@eecs.berkeley.edu
Received: by 10.216.92.11 with HTTP; Mon, 19 May 2014 22:25:21 -0700 (PDT)
In-Reply-To: <FC40ACDFAFFB42BBBE97A77E16F19DA2@gmail.com>
References: <1400550521233-6698.post@n3.nabble.com>
	<D32E235E9849451783DBF2134CC97FC9@gmail.com>
	<CAAsvFPkToNizLDzO5DgTUuK2H-xGsAJg5+nmZJEjvJf2ZrJiZg@mail.gmail.com>
	<CABPQxstH2ysQzvW62U46yXNzQ954TUc=peprP-zVeumcHnofaw@mail.gmail.com>
	<FC40ACDFAFFB42BBBE97A77E16F19DA2@gmail.com>
Date: Mon, 19 May 2014 22:25:21 -0700
Message-ID: <CAKx7Bf-=mGC9YMTjk+EnE9QBwF9z=G_q8f7gGTyOTajqrjDrug@mail.gmail.com>
Subject: Re: spark 1.0 standalone application
From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bb04dce31ac5804f9ce1b1e
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bb04dce31ac5804f9ce1b1e
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

On a related note there is also a staging Apache repository where the
latest rc gets pushed to
https://repository.apache.org/content/repositories/staging/org/apache/spark=
/spark-core_2.10/--

The artifact here is just named "1.0.0" (similar to the rc specific
repository that Patrick mentioned). So if you just want to build you app
against the latest staging RC you can add "
https://repository.apache.org/content/repositories/staging" to your
resolvers in SBT / Maven.

Thanks
Shivaram


On Mon, May 19, 2014 at 10:23 PM, Nan Zhu <zhunanmcgill@gmail.com> wrote:

> First time to know there is a temporary maven repository=E2=80=A6=E2=80=
=A6.
>
> --
> Nan Zhu
>
>
> On Monday, May 19, 2014 at 10:10 PM, Patrick Wendell wrote:
>
> > Whenever we publish a release candidate, we create a temporary maven
> > repository that host the artifacts. We do this precisely for the case
> > you are running into (where a user wants to build an application
> > against it to test).
> >
> > You can build against the release candidate by just adding that
> > repository in your sbt build, then linking against "spark-core"
> > version "1.0.0". For rc9 the repository is in the vote e-mail:
> >
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Ap=
ache-Spark-1-0-0-rc9-td6629.html
> >
> > On Mon, May 19, 2014 at 7:03 PM, Mark Hamstra <mark@clearstorydata.com(=
mailto:
> mark@clearstorydata.com)> wrote:
> > > That's the crude way to do it. If you run `sbt/sbt publishLocal`, the=
n
> you
> > > can resolve the artifact from your local cache in the same way that y=
ou
> > > would resolve it if it were deployed to a remote cache. That's just t=
he
> > > build step. Actually running the application will require the necessa=
ry
> > > jars to be accessible by the cluster nodes.
> > >
> > >
> > > On Mon, May 19, 2014 at 7:04 PM, Nan Zhu <zhunanmcgill@gmail.com(mail=
to:
> zhunanmcgill@gmail.com)> wrote:
> > >
> > > > en, you have to put spark-assembly-*.jar to the lib directory of yo=
ur
> > > > application
> > > >
> > > > Best,
> > > >
> > > > --
> > > > Nan Zhu
> > > >
> > > >
> > > > On Monday, May 19, 2014 at 9:48 PM, nit wrote:
> > > >
> > > > > I am not much comfortable with sbt. I want to build a standalone
> > > > application
> > > > > using spark 1.0 RC9. I can build sbt assembly for my application
> with
> > > >
> > > > Spark
> > > > > 0.9.1, and I think in that case spark is pulled from Aka
> Repository?
> > > > >
> > > > > Now if I want to use 1.0 RC9 for my application; what is the
> process ?
> > > > > (FYI, I was able to build spark-1.0 via sbt/assembly and I can se=
e
> > > > > sbt-assembly jar; and I think I will have to copy my jar
> somewhere? and
> > > > > update build.sbt?)
> > > > >
> > > > > PS: I am not sure if this is the right place for this question;
> but since
> > > > > 1.0 is still RC, I felt that this may be appropriate forum.
> > > > >
> > > > > thank!
> > > > >
> > > > >
> > > > >
> > > > > --
> > > > > View this message in context:
> > > > >
> > > >
> > > >
> http://apache-spark-developers-list.1001551.n3.nabble.com/spark-1-0-stand=
alone-application-tp6698.html
> > > > > Sent from the Apache Spark Developers List mailing list archive a=
t
> > > >
> > > > Nabble.com (http://Nabble.com).
> > > > >
> > > >
> > > >
> > >
> > >
> >
> >
> >
>
>
>

--047d7bb04dce31ac5804f9ce1b1e--

From dev-return-7723-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 13:26:40 2014
Return-Path: <dev-return-7723-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8F03F11F89
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 13:26:40 +0000 (UTC)
Received: (qmail 8394 invoked by uid 500); 20 May 2014 13:26:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8344 invoked by uid 500); 20 May 2014 13:26:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8335 invoked by uid 99); 20 May 2014 13:26:40 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 13:26:40 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 13:26:37 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <madhu@madhu.com>)
	id 1Wmk3A-0001dH-Ds
	for dev@spark.incubator.apache.org; Tue, 20 May 2014 06:25:48 -0700
Date: Tue, 20 May 2014 06:25:33 -0700 (PDT)
From: Madhu <madhu@madhu.com>
To: dev@spark.incubator.apache.org
Message-ID: <1400592333369-6715.post@n3.nabble.com>
Subject: Sorting partitions in Java
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I'm trying to sort data in each partition of an RDD.
I was able to do it successfully in Scala like this:

val sorted = rdd.mapPartitions(iter => {
  iter.toArray.sortWith((x, y) => x._2.compare(y._2) < 0).iterator
},
preservesPartitioning = true)

I used the same technique as in OrderedRDDFunctions.scala, so I assume it's
a reasonable way to do it.

This works well so far, but I can't seem to do the same thing in Java
because 'iter' in the Java APIs is an Iterator rather than an Iterable.
There may be an unattractive workaround, but I didn't pursue it.

Ideally, it would be nice to have an efficient, robust method in RDD to sort
each partition.
Does something like that exist?

Thanks!



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Sorting-partitions-in-Java-tp6715.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7724-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 13:52:13 2014
Return-Path: <dev-return-7724-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 70BE811120
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 13:52:13 +0000 (UTC)
Received: (qmail 81830 invoked by uid 500); 20 May 2014 13:52:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81366 invoked by uid 500); 20 May 2014 13:52:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81130 invoked by uid 99); 20 May 2014 13:52:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 13:52:12 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tgraves_cs@yahoo.com designates 98.139.212.180 as permitted sender)
Received: from [98.139.212.180] (HELO nm21.bullet.mail.bf1.yahoo.com) (98.139.212.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 13:52:08 +0000
Received: from [98.139.212.153] by nm21.bullet.mail.bf1.yahoo.com with NNFMP; 20 May 2014 13:51:44 -0000
Received: from [98.139.212.203] by tm10.bullet.mail.bf1.yahoo.com with NNFMP; 20 May 2014 13:51:44 -0000
Received: from [127.0.0.1] by omp1012.mail.bf1.yahoo.com with NNFMP; 20 May 2014 13:51:44 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 304857.97392.bm@omp1012.mail.bf1.yahoo.com
Received: (qmail 38345 invoked by uid 60001); 20 May 2014 13:51:44 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.com; s=s1024; t=1400593904; bh=QzCo5cT0CDEWV12iW9Cy3lpXbHURiPW4X9AoTUcg2Nw=; h=References:Message-ID:Date:From:Reply-To:Subject:To:In-Reply-To:MIME-Version:Content-Type; b=P2Zzyo+kTW4tKX7zGQVCSPpHnkCQtIQzXSz7uY6oIyITW7MMWLSvY6smgVj2vZzjESZpD+kzNkDb6vMB/kfGrk95Tn2BlYeIh6l4FuG/ai9jQZG9h3g3VwO7CKKCabdW3gx4fo+ysvb5Fr7fqyyEr0kIzujDQy3ZK15c/onIyvU=
X-YMail-OSG: kZcxlo4VM1nhwVgjpHKXqg4mSBoBQusO7wnZmsisRBxDlPW
 3HbdloXXUhTVSsB_R6YqBzF18LqDrAm.gOd3JAr_nbspqGCMOfYRAmBBsLDu
 gvISp9RD0OQdmQz.JpAFHyjgGFXbbWxo2IK6r7u0NmXkjUcJSMsNMhC9d0IY
 PoWgjT8NrFVyPemj3pE42xHCP0tvKXVUSyyRxdxuEn64nrYP9v1xrEvgoH.L
 S9SAK.svy.ns0yHiGRLH8a367FFvPsvlPZ9ZCwPxa_0qEUD8WzGbT5_7HFTt
 PFFQuKORAInTxc3YUlstRWR76u6KIHWfOXVhZhr3s3DD9.KART6VNIumPtQf
 03TmsPWqrWDmeV7dHzto.wsTzCxOBv44eb7Mw2lv2vQ2k60B0JRScvIFCqdJ
 O_bVWjQaHvec39USCLs3QsQY72zeCcLYujnQzO_K7pzSc03TyFekThIdVwkX
 K8Dh0W1.PyeXK1Wehw3WDa9Cvylps6_9HAw3wyf86BPLFrkra2haqWEpTTsG
 QIIrM8IaUUzPYNzH2glnPoq8tPNjSsFYR9v9oGHEQezCL0yy7ZeBZmP5t1nv
 57PU6UdEK29TlgRIXAMJ9_gcSbxrWV9RDl.obt3RKwZCLOLif6xW1t.xulwu
 EU34UonKL8qhbGBpVxf8Li_ks1Z.jBtW5RFbgX0jJbaT.4xEm3anRx19Rgpi
 cght1
Received: from [204.11.79.50] by web140104.mail.bf1.yahoo.com via HTTP; Tue, 20 May 2014 06:51:44 PDT
X-Rocket-MIMEInfo: 002.001,SSBhc3N1bWUgd2Ugd2lsbCBoYXZlIGFuIHJjMTAgdG8gZml4IHRoZSBpc3N1ZXMgTWF0ZWkgZm91bmQ_CgpUb20KCgpPbiBTdW5kYXksIE1heSAxOCwgMjAxNCA5OjA4IFBNLCBQYXRyaWNrIFdlbmRlbGwgPHB3ZW5kZWxsQGdtYWlsLmNvbT4gd3JvdGU6CiAKCgpIZXkgTWF0ZWkgLSB0aGUgaXNzdWUgeW91IGZvdW5kIGlzIG5vdCByZWxhdGVkIHRvIHNlY3VyaXR5LiBUaGlzIHBhdGNoCmEgZmV3IGRheXMgYWdvIGJyb2tlIGJ1aWxkcyBmb3IgSGFkb29wIDEgd2l0aCBZQVJOIHN1cHBvcnQgZW5hYmxlZC4KVGgBMAEBAQE-
X-Mailer: YahooMailWebService/0.8.188.663
References: <CABPQxstaxcbGSQTqUxSSUQfRpjQs=BqRkrEVvzOXbXZo8kzShg@mail.gmail.com>	<CABPQxstbwN64rQvx6kUoESkO4GJOohYSAO+s_F85wFWjb6T1zw@mail.gmail.com>	<CAAsvFPn2RwpFMz7ajy=t8GSjiKSPqARgDtPqXk007Cd7_xvA-Q@mail.gmail.com>	<CAMJOb8nTR38Uf50xVxv5O+OrBkO0ELDFYiHrziBVm9qknAgpYA@mail.gmail.com>	<CACBYxKLR3xE4FSqZQwrjmP+bhhqy_mAat0hHSu+1pOBEVTttrA@mail.gmail.com>	<9849C2AA-3131-4E31-85DF-7F6D61632545@gmail.com>	<4D62B3E0-B6E7-4916-953D-A196FC6BC24F@gmail.com> <CABPQxssnZ=aq6djG-YL=CtajhRhnT945qGcNhAq6RZEdDDU0tA@mail.gmail.com>
Message-ID: <1400593904.12395.YahooMailNeo@web140104.mail.bf1.yahoo.com>
Date: Tue, 20 May 2014 06:51:44 -0700 (PDT)
From: Tom Graves <tgraves_cs@yahoo.com>
Reply-To: Tom Graves <tgraves_cs@yahoo.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc9)
To: "dev@spark.apache.org" <dev@spark.apache.org>
In-Reply-To: <CABPQxssnZ=aq6djG-YL=CtajhRhnT945qGcNhAq6RZEdDDU0tA@mail.gmail.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="908171085-1636763553-1400593904=:12395"
X-Virus-Checked: Checked by ClamAV on apache.org

--908171085-1636763553-1400593904=:12395
Content-Type: text/plain; charset=us-ascii

I assume we will have an rc10 to fix the issues Matei found?

Tom


On Sunday, May 18, 2014 9:08 PM, Patrick Wendell <pwendell@gmail.com> wrote:
 


Hey Matei - the issue you found is not related to security. This patch
a few days ago broke builds for Hadoop 1 with YARN support enabled.
The patch directly altered the way we deal with commons-lang
dependency, which is what is at the base of this stack trace.

https://github.com/apache/spark/pull/754

- Patrick


On Sun, May 18, 2014 at 5:28 PM, Matei Zaharia <matei.zaharia@gmail.com> wrote:
> Alright, I've opened https://github.com/apache/spark/pull/819 with the Windows fixes. I also found one other likely bug, https://issues.apache.org/jira/browse/SPARK-1875, in the binary packages for Hadoop1 built in this RC. I think this is due to Hadoop 1's security code depending on a different version of org.apache.commons than Hadoop 2, but it needs investigation. Tom, any thoughts on this?
>
> Matei
>
> On May 18, 2014, at 12:33 PM, Matei Zaharia <matei.zaharia@gmail.com> wrote:
>
>> I took the always fun task of testing it on Windows, and unfortunately, I found some small problems with the prebuilt packages due to recent changes to the launch scripts: bin/spark-class2.cmd looks in ./jars instead of ./lib for the assembly JAR, and bin/run-example2.cmd doesn't quite match the master-setting behavior of the Unix based one. I'll send a pull request to fix them soon.
>>
>> Matei
>>
>>
>> On May 17, 2014, at 11:32 AM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
>>
>>> +1
>>>
>>> Reran my tests from rc5:
>>>
>>> * Built the release from source.
>>> * Compiled Java and Scala apps that interact with HDFS against it.
>>> * Ran them in local mode.
>>> * Ran them against a pseudo-distributed YARN cluster in both yarn-client
>>> mode and yarn-cluster mode.
>>>
>>>
>>> On Sat, May 17, 2014 at 10:08 AM, Andrew Or <andrew@databricks.com> wrote:
>>>
>>>> +1
>>>>
>>>>
>>>> 2014-05-17 8:53 GMT-07:00 Mark Hamstra <mark@clearstorydata.com>:
>>>>
>>>>> +1
>>>>>
>>>>>
>>>>> On Sat, May 17, 2014 at 12:58 AM, Patrick Wendell <pwendell@gmail.com
>>>>>> wrote:
>>>>>
>>>>>> I'll start the voting with a +1.
>>>>>>
>>>>>> On Sat, May 17, 2014 at 12:58 AM, Patrick Wendell <pwendell@gmail.com>
>>>>>> wrote:
>>>>>>> Please vote on releasing the following candidate as Apache Spark
>>>>> version
>>>>>> 1.0.0!
>>>>>>> This has one bug fix and one minor feature on top of rc8:
>>>>>>> SPARK-1864: https://github.com/apache/spark/pull/808
>>>>>>> SPARK-1808: https://github.com/apache/spark/pull/799
>>>>>>>
>>>>>>> The tag to be voted on is v1.0.0-rc9 (commit 920f947):
>>>>>>>
>>>>>>
>>>>>
>>>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=920f947eb5a22a679c0c3186cf69ee75f6041c75
>>>>>>>
>>>>>>> The release files, including signatures, digests, etc. can be found
>>>> at:
>>>>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc9/
>>>>>>>
>>>>>>> Release artifacts are signed with the following key:
>>>>>>> https://people.apache.org/keys/committer/pwendell.asc
>>>>>>>
>>>>>>> The staging repository for this release can be found at:
>>>>>>>
>>>>> https://repository.apache.org/content/repositories/orgapachespark-1017/
>>>>>>>
>>>>>>> The documentation corresponding to this release can be found at:
>>>>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc9-docs/
>>>>>>>
>>>>>>> Please vote on releasing this package as Apache Spark 1.0.0!
>>>>>>>
>>>>>>> The vote is open until Tuesday, May 20, at 08:56 UTC and passes if
>>>>>>> amajority of at least 3 +1 PMC votes are cast.
>>>>>>>
>>>>>>> [ ] +1 Release this package as Apache Spark 1.0.0
>>>>>>> [ ] -1 Do not release this package because ...
>>>>>>>
>>>>>>> To learn more about Apache Spark, please see
>>>>>>> http://spark.apache.org/
>>>>>>>
>>>>>>> == API Changes ==
>>>>>>> We welcome users to compile Spark applications against 1.0. There are
>>>>>>> a few API changes in this release. Here are links to the associated
>>>>>>> upgrade guides - user facing changes have been kept as small as
>>>>>>> possible.
>>>>>>>
>>>>>>> changes to ML vector specification:
>>>>>>>
>>>>>>
>>>>>
>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/mllib-guide.html#from-09-to-10
>>>>>>>
>>>>>>> changes to the Java API:
>>>>>>>
>>>>>>
>>>>>
>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>>>>>>>
>>>>>>> changes to the streaming API:
>>>>>>>
>>>>>>
>>>>>
>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>>>>>>>
>>>>>>> changes to the GraphX API:
>>>>>>>
>>>>>>
>>>>>
>>>> http://people.apache.org/~pwendell/spark-1.0.0-rc8-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>>>>>>>
>>>>>>> coGroup and related functions now return Iterable[T] instead of
>>>> Seq[T]
>>>>>>> ==> Call toSeq on the result to restore the old behavior
>>>>>>>
>>>>>>> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
>>>>>>> ==> Call toSeq on the result to restore old behavior
>>>>>>
>>>>>
>>>>
>>
>
--908171085-1636763553-1400593904=:12395--

From dev-return-7725-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 14:14:33 2014
Return-Path: <dev-return-7725-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 207F41129B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 14:14:33 +0000 (UTC)
Received: (qmail 51330 invoked by uid 500); 20 May 2014 14:14:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51267 invoked by uid 500); 20 May 2014 14:14:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51259 invoked by uid 99); 20 May 2014 14:14:32 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 14:14:32 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of glenn.strycker@gmail.com does not designate 216.139.250.139 as permitted sender)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 14:14:29 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <glenn.strycker@gmail.com>)
	id 1WmknV-0004KU-BV
	for dev@spark.incubator.apache.org; Tue, 20 May 2014 07:13:41 -0700
Date: Tue, 20 May 2014 07:13:26 -0700 (PDT)
From: GlennStrycker <glenn.strycker@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1400595206326-6717.post@n3.nabble.com>
In-Reply-To: <CAPh_B=ZDpYmZggfL7CTd+zekxfwHciAfjMVg1rNspcA5ynWftg@mail.gmail.com>
References: <1400530169998-6693.post@n3.nabble.com> <CAPh_B=bwgbWJxNV0YEGt-Dr3yWTY0QDd6DZX9tcfZDQTVdermg@mail.gmail.com> <1400537762804-6695.post@n3.nabble.com> <CAPh_B=Yi=a-AvwYp1+jUXcwTids_KgCY3Hp+5JPQJfSb2W3O1A@mail.gmail.com> <1400538760695-6697.post@n3.nabble.com> <CAPh_B=ZDpYmZggfL7CTd+zekxfwHciAfjMVg1rNspcA5ynWftg@mail.gmail.com>
Subject: Re: BUG: graph.triplets does not return proper values
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Oh... ha, good point.  Sorry, I'm new to mapreduce programming and forgot
about that... I'll have to adjust my reduce function to output a vector/RDD
as the element to return.  Thanks for reminding me of this!



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/BUG-graph-triplets-does-not-return-proper-values-tp6693p6717.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7726-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 14:15:35 2014
Return-Path: <dev-return-7726-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EFFC7112BD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 14:15:34 +0000 (UTC)
Received: (qmail 64994 invoked by uid 500); 20 May 2014 14:15:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64932 invoked by uid 500); 20 May 2014 14:15:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64924 invoked by uid 99); 20 May 2014 14:15:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 14:15:34 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.128.178 as permitted sender)
Received: from [209.85.128.178] (HELO mail-ve0-f178.google.com) (209.85.128.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 14:15:30 +0000
Received: by mail-ve0-f178.google.com with SMTP id sa20so678743veb.9
        for <dev@spark.apache.org>; Tue, 20 May 2014 07:15:09 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=RpXIxmNBgDOk3huaypGxC88O5dJiVNl4ZccsUWDmZqs=;
        b=b0REWtP6oJ9QpT79bjyYZFScWJBXaNFK7WrjAnLYm669til6o6ZV9xJUloQ2F/Qpal
         0+Wbe9O0Yw5t3Ej4C7hr6Ybo9s2Qy38rzmkghgpM4g4vxw0fCD7PYyAUhOMDSpD3yBox
         tU2QsA2qeW10peR5feuLC8fV2CIjBAH1fYUT4RSyvdn2LUHqTVTrb/cBvecL7MuPvm8U
         fHjMOpqGqUbwxCQLdRA2SemyEq0LG3dTP2QrfDa0ZGhSiADMviSRTBrWSY6EAgPhtwL9
         ZxWg/YARpxvnB3MZLz04ih05mEUIwXo1UoD5Yi8xH1DO4T8bb7UtW1DBLdYiB1hc8/iG
         /c6A==
X-Gm-Message-State: ALoCoQkroUjt5Fg8FYX8wgAbBOmcASNVBmIiwAByXNS1+Bwr1d4z6qQUTvfy1FEV/Kkcv4HM9Y+5
X-Received: by 10.221.59.194 with SMTP id wp2mr752140vcb.59.1400595309769;
 Tue, 20 May 2014 07:15:09 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.111.69 with HTTP; Tue, 20 May 2014 07:14:49 -0700 (PDT)
In-Reply-To: <1400592333369-6715.post@n3.nabble.com>
References: <1400592333369-6715.post@n3.nabble.com>
From: Sean Owen <sowen@cloudera.com>
Date: Tue, 20 May 2014 15:14:49 +0100
Message-ID: <CAMAsSd+CjY-NMo7vqrzB5SHAyoLUcYuiqHz7j76Y=3jryWkp5w@mail.gmail.com>
Subject: Re: Sorting partitions in Java
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

It's an Iterator in both Java and Scala. In both cases you need to
copy the stream of values into something List-like to sort it. An
Iterable would not change that (not sure the API can promise many
iterations anyway).

If you just want the equivalent of "toArray", you can use a utility
method in Commons Collections or Guava. Guava's
Lists.newArrayList(Iterator) does nicely, which you can then
Collections.sort() with a Comparator and the return its iterator()

I dug this up too, remembering a similar question:
http://mail-archives.apache.org/mod_mbox/spark-user/201312.mbox/%3C529F819F.3060901@vu.nl%3E

On Tue, May 20, 2014 at 2:25 PM, Madhu <madhu@madhu.com> wrote:
> I'm trying to sort data in each partition of an RDD.
> I was able to do it successfully in Scala like this:
>
> val sorted = rdd.mapPartitions(iter => {
>   iter.toArray.sortWith((x, y) => x._2.compare(y._2) < 0).iterator
> },
> preservesPartitioning = true)
>
> I used the same technique as in OrderedRDDFunctions.scala, so I assume it's
> a reasonable way to do it.
>
> This works well so far, but I can't seem to do the same thing in Java
> because 'iter' in the Java APIs is an Iterator rather than an Iterable.
> There may be an unattractive workaround, but I didn't pursue it.
>
> Ideally, it would be nice to have an efficient, robust method in RDD to sort
> each partition.
> Does something like that exist?
>
> Thanks!
>
>
>
> -----
> --
> Madhu
> https://www.linkedin.com/in/msiddalingaiah
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Sorting-partitions-in-Java-tp6715.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7727-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 17:11:16 2014
Return-Path: <dev-return-7727-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E961011B80
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 17:11:15 +0000 (UTC)
Received: (qmail 30373 invoked by uid 500); 20 May 2014 17:11:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30322 invoked by uid 500); 20 May 2014 17:11:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30314 invoked by uid 99); 20 May 2014 17:11:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:11:15 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:11:11 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <madhu@madhu.com>)
	id 1WmnYX-0006yv-Gp
	for dev@spark.incubator.apache.org; Tue, 20 May 2014 10:10:25 -0700
Date: Tue, 20 May 2014 10:10:10 -0700 (PDT)
From: Madhu <madhu@madhu.com>
To: dev@spark.incubator.apache.org
Message-ID: <1400605810497-6719.post@n3.nabble.com>
In-Reply-To: <CAMAsSd+CjY-NMo7vqrzB5SHAyoLUcYuiqHz7j76Y=3jryWkp5w@mail.gmail.com>
References: <1400592333369-6715.post@n3.nabble.com> <CAMAsSd+CjY-NMo7vqrzB5SHAyoLUcYuiqHz7j76Y=3jryWkp5w@mail.gmail.com>
Subject: Re: Sorting partitions in Java
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks Sean, I had seen that post you mentioned.

What you suggest looks an in-memory sort, which is fine if each partition is
small enough to fit in memory. Is it true that rdd.sortByKey(...) requires
partitions to fit in memory? I wasn't sure if there was some magic behind
the scenes that supports arbitrarily large sorts.

None of this is a show stopper, it just might require a little more code on
the part of the developer. If there's a requirement for Spark partitions to
fit in memory, developers will have to be aware of that and plan
accordingly. One nice feature of Hadoop MR is the ability to sort very large
sets without thinking about data size.

In the case that a developer repartitions an RDD such that some partitions
don't fit in memory, sorting those partitions requires more work. For these
cases, I think there is value in having a robust partition sorting method
that deals with it efficiently and reliably.

Is there another solution for sorting arbitrarily large partitions? If not,
I don't mind developing and contributing a solution.




-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Sorting-partitions-in-Java-tp6715p6719.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7729-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 17:12:35 2014
Return-Path: <dev-return-7729-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2AB4411B85
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 17:12:35 +0000 (UTC)
Received: (qmail 32345 invoked by uid 500); 20 May 2014 17:12:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32150 invoked by uid 500); 20 May 2014 17:12:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32133 invoked by uid 99); 20 May 2014 17:12:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:12:34 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.192.49 as permitted sender)
Received: from [209.85.192.49] (HELO mail-qg0-f49.google.com) (209.85.192.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:12:30 +0000
Received: by mail-qg0-f49.google.com with SMTP id a108so1216214qge.8
        for <dev@spark.incubator.apache.org>; Tue, 20 May 2014 10:12:10 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=ECBotxYpgpol26xVYTkCpLCjgRtCW71vDWFmrF7xo9w=;
        b=NW9zTPojOeIu/1UQSasjesPwJmPI5WIl+ILT2LQhIsU2mYUDOm9LSXxN7j34/vrdzi
         1AgYtQko6vGB1lDXZ7wzLnjLoNKMkVLEwOMqjEMQzKwWSoCZaBJM0x4N7tjuPo9RPVFI
         mIw3FzbIp5ngvSxPpWpWsQ0gthgJPudVWiQX43nDTyPudULbyRruu/jrEdLzyeK5CcdP
         hfB6bMSC4CMbV230CddHjs8APPV3MRGI2cZhpmEG/ZQ/zl/hGVM6mOcfj/jvCCqFPbBR
         t3Rj8BDPSr5HCX4JNvSfGkV4BqgvuzvdFaGI7DFYtrAkD0dvs/Uv1xropwxvZgJoEFJM
         mj8A==
X-Gm-Message-State: ALoCoQlR9H1iAuzh1y0zCI31M+xkvgw12kMeJrPHUbMhRAcJo0pJRLOTKUAQe18qmy7uvjvDK/jc
MIME-Version: 1.0
X-Received: by 10.224.112.74 with SMTP id v10mr59712253qap.28.1400605929968;
 Tue, 20 May 2014 10:12:09 -0700 (PDT)
Received: by 10.140.106.202 with HTTP; Tue, 20 May 2014 10:12:09 -0700 (PDT)
In-Reply-To: <1400605810497-6719.post@n3.nabble.com>
References: <1400592333369-6715.post@n3.nabble.com>
	<CAMAsSd+CjY-NMo7vqrzB5SHAyoLUcYuiqHz7j76Y=3jryWkp5w@mail.gmail.com>
	<1400605810497-6719.post@n3.nabble.com>
Date: Tue, 20 May 2014 10:12:09 -0700
Message-ID: <CACBYxK+CDV_V9UHSoVw+Jv8Eb_V2BY+0BrhvQbJhsNW+6K=2_Q@mail.gmail.com>
Subject: Re: Sorting partitions in Java
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11c3b23aed26a704f9d7fa39
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3b23aed26a704f9d7fa39
Content-Type: text/plain; charset=UTF-8

sortByKey currently requires partitions to fit in memory, but there are
plans to add external sort


On Tue, May 20, 2014 at 10:10 AM, Madhu <madhu@madhu.com> wrote:

> Thanks Sean, I had seen that post you mentioned.
>
> What you suggest looks an in-memory sort, which is fine if each partition
> is
> small enough to fit in memory. Is it true that rdd.sortByKey(...) requires
> partitions to fit in memory? I wasn't sure if there was some magic behind
> the scenes that supports arbitrarily large sorts.
>
> None of this is a show stopper, it just might require a little more code on
> the part of the developer. If there's a requirement for Spark partitions to
> fit in memory, developers will have to be aware of that and plan
> accordingly. One nice feature of Hadoop MR is the ability to sort very
> large
> sets without thinking about data size.
>
> In the case that a developer repartitions an RDD such that some partitions
> don't fit in memory, sorting those partitions requires more work. For these
> cases, I think there is value in having a robust partition sorting method
> that deals with it efficiently and reliably.
>
> Is there another solution for sorting arbitrarily large partitions? If not,
> I don't mind developing and contributing a solution.
>
>
>
>
> -----
> --
> Madhu
> https://www.linkedin.com/in/msiddalingaiah
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Sorting-partitions-in-Java-tp6715p6719.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c3b23aed26a704f9d7fa39--

From dev-return-7728-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 17:12:35 2014
Return-Path: <dev-return-7728-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A561A11B86
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 17:12:35 +0000 (UTC)
Received: (qmail 32211 invoked by uid 500); 20 May 2014 17:12:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32148 invoked by uid 500); 20 May 2014 17:12:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32132 invoked by uid 99); 20 May 2014 17:12:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:12:34 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.192.42 as permitted sender)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:12:30 +0000
Received: by mail-qg0-f42.google.com with SMTP id q107so1241315qgd.15
        for <dev@spark.apache.org>; Tue, 20 May 2014 10:12:10 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=ECBotxYpgpol26xVYTkCpLCjgRtCW71vDWFmrF7xo9w=;
        b=VuANdp77o+Z2Kot7uc7whjFPO8AqlS56UF+MdX7OKsocGFGnQya+bom/THFaKw1Rng
         oYUSDz4XHjo4AvBLDHScEWajEntAxbxVNgU7p2J2rQEyJCgCFhQ22p1aWhpAgnlWf8aQ
         WWHwnBA0q7PvUPyFBRlAQoQ2w7Q46ZNYrQ0XrNfv7p2JyI/ATOscEM5iZZtH5poBNIo7
         rOlVLEThFll5HIv7QL811om7s1T4XgsEzbgzEjrCGt05gr+Q+p2uHXtJrba5lXQr3sM3
         kupffIndzhtv1qBEgYVv4NpLOYgtbITImx0ROfGSB16BewALjEP8nCUNeSqElYkMGW2U
         4zgg==
X-Gm-Message-State: ALoCoQl8pm7amzUDYjzx6j5FFz61TlxlBQawl+BkIcnShz/JpvtcQ/GhqBnb5ZCSn3hSyQhnb0GD
MIME-Version: 1.0
X-Received: by 10.224.112.74 with SMTP id v10mr59712253qap.28.1400605929968;
 Tue, 20 May 2014 10:12:09 -0700 (PDT)
Received: by 10.140.106.202 with HTTP; Tue, 20 May 2014 10:12:09 -0700 (PDT)
In-Reply-To: <1400605810497-6719.post@n3.nabble.com>
References: <1400592333369-6715.post@n3.nabble.com>
	<CAMAsSd+CjY-NMo7vqrzB5SHAyoLUcYuiqHz7j76Y=3jryWkp5w@mail.gmail.com>
	<1400605810497-6719.post@n3.nabble.com>
Date: Tue, 20 May 2014 10:12:09 -0700
Message-ID: <CACBYxK+CDV_V9UHSoVw+Jv8Eb_V2BY+0BrhvQbJhsNW+6K=2_Q@mail.gmail.com>
Subject: Re: Sorting partitions in Java
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11c3b23aed26a704f9d7fa39
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3b23aed26a704f9d7fa39
Content-Type: text/plain; charset=UTF-8

sortByKey currently requires partitions to fit in memory, but there are
plans to add external sort


On Tue, May 20, 2014 at 10:10 AM, Madhu <madhu@madhu.com> wrote:

> Thanks Sean, I had seen that post you mentioned.
>
> What you suggest looks an in-memory sort, which is fine if each partition
> is
> small enough to fit in memory. Is it true that rdd.sortByKey(...) requires
> partitions to fit in memory? I wasn't sure if there was some magic behind
> the scenes that supports arbitrarily large sorts.
>
> None of this is a show stopper, it just might require a little more code on
> the part of the developer. If there's a requirement for Spark partitions to
> fit in memory, developers will have to be aware of that and plan
> accordingly. One nice feature of Hadoop MR is the ability to sort very
> large
> sets without thinking about data size.
>
> In the case that a developer repartitions an RDD such that some partitions
> don't fit in memory, sorting those partitions requires more work. For these
> cases, I think there is value in having a robust partition sorting method
> that deals with it efficiently and reliably.
>
> Is there another solution for sorting arbitrarily large partitions? If not,
> I don't mind developing and contributing a solution.
>
>
>
>
> -----
> --
> Madhu
> https://www.linkedin.com/in/msiddalingaiah
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Sorting-partitions-in-Java-tp6715p6719.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c3b23aed26a704f9d7fa39--

From dev-return-7731-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 17:17:03 2014
Return-Path: <dev-return-7731-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 396C311C00
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 17:17:03 +0000 (UTC)
Received: (qmail 51682 invoked by uid 500); 20 May 2014 17:17:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51585 invoked by uid 500); 20 May 2014 17:17:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51515 invoked by uid 99); 20 May 2014 17:17:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:17:02 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.175] (HELO mail-vc0-f175.google.com) (209.85.220.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:16:59 +0000
Received: by mail-vc0-f175.google.com with SMTP id hu19so1000775vcb.20
        for <dev@spark.incubator.apache.org>; Tue, 20 May 2014 10:16:35 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=ZZQ5trzsJMk0jcsSK9BLNq58WIlWEUwmCVs86fDnyH0=;
        b=Qb+Bm+6OxPYubjEu7bjZrJLqUTRw+amrK9L+ci2POFrIH9pN0cOUsaXG6Hq4+/3/Wg
         YkowT+PEHiZTNl2XXCWliXVALWACwSiRj7TyJ3b5vPaKhNETNg82VNf5ajRyJNYajHaL
         /2DQjRv834sGLUurQPN1nO22NDMLyNeiZPTT7CurbEgkYOLIKVFSEjtb16SlSNGg+Bqe
         JHeyMXALXK9wJvSAqO7z+whdCbFHLjiggyzIoVBB/rs3pIFDkmtfj4W6YJbaUTinHg1/
         L7JBT5kn9zUD4am+06CC4xpoev/AJuunRAQl9O1tBiAflZz3/3z49UwPN17x2rtnciha
         RvfA==
X-Gm-Message-State: ALoCoQkJI0GyU2ZezdBksbfAyzU7pep85WWKWJjqq07Oe/JqtaAHQWeOBAWFXtMTBVD6jEfm6xE3
X-Received: by 10.220.103.141 with SMTP id k13mr5024540vco.25.1400606195787;
        Tue, 20 May 2014 10:16:35 -0700 (PDT)
Received: from mail-vc0-f174.google.com (mail-vc0-f174.google.com [209.85.220.174])
        by mx.google.com with ESMTPSA id u3sm23769211ves.12.2014.05.20.10.16.34
        for <dev@spark.incubator.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 20 May 2014 10:16:34 -0700 (PDT)
Received: by mail-vc0-f174.google.com with SMTP id lh14so998136vcb.5
        for <dev@spark.incubator.apache.org>; Tue, 20 May 2014 10:16:34 -0700 (PDT)
X-Received: by 10.58.46.83 with SMTP id t19mr1777006vem.60.1400606194225; Tue,
 20 May 2014 10:16:34 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Tue, 20 May 2014 10:16:14 -0700 (PDT)
In-Reply-To: <CACBYxK+CDV_V9UHSoVw+Jv8Eb_V2BY+0BrhvQbJhsNW+6K=2_Q@mail.gmail.com>
References: <1400592333369-6715.post@n3.nabble.com> <CAMAsSd+CjY-NMo7vqrzB5SHAyoLUcYuiqHz7j76Y=3jryWkp5w@mail.gmail.com>
 <1400605810497-6719.post@n3.nabble.com> <CACBYxK+CDV_V9UHSoVw+Jv8Eb_V2BY+0BrhvQbJhsNW+6K=2_Q@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Tue, 20 May 2014 10:16:14 -0700
Message-ID: <CA+-p3AHZKSn1soP8Yxw8ARwGmN3t+bWDvR_WQ62hUMnrhLpXiA@mail.gmail.com>
Subject: Re: Sorting partitions in Java
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e0118456aad591504f9d80a34
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0118456aad591504f9d80a34
Content-Type: text/plain; charset=UTF-8

Sandy, is there a Jira ticket for that?


On Tue, May 20, 2014 at 10:12 AM, Sandy Ryza <sandy.ryza@cloudera.com>wrote:

> sortByKey currently requires partitions to fit in memory, but there are
> plans to add external sort
>
>
> On Tue, May 20, 2014 at 10:10 AM, Madhu <madhu@madhu.com> wrote:
>
> > Thanks Sean, I had seen that post you mentioned.
> >
> > What you suggest looks an in-memory sort, which is fine if each partition
> > is
> > small enough to fit in memory. Is it true that rdd.sortByKey(...)
> requires
> > partitions to fit in memory? I wasn't sure if there was some magic behind
> > the scenes that supports arbitrarily large sorts.
> >
> > None of this is a show stopper, it just might require a little more code
> on
> > the part of the developer. If there's a requirement for Spark partitions
> to
> > fit in memory, developers will have to be aware of that and plan
> > accordingly. One nice feature of Hadoop MR is the ability to sort very
> > large
> > sets without thinking about data size.
> >
> > In the case that a developer repartitions an RDD such that some
> partitions
> > don't fit in memory, sorting those partitions requires more work. For
> these
> > cases, I think there is value in having a robust partition sorting method
> > that deals with it efficiently and reliably.
> >
> > Is there another solution for sorting arbitrarily large partitions? If
> not,
> > I don't mind developing and contributing a solution.
> >
> >
> >
> >
> > -----
> > --
> > Madhu
> > https://www.linkedin.com/in/msiddalingaiah
> > --
> > View this message in context:
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Sorting-partitions-in-Java-tp6715p6719.html
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> >
>

--089e0118456aad591504f9d80a34--

From dev-return-7732-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 17:21:43 2014
Return-Path: <dev-return-7732-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5B6F711C59
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 17:21:43 +0000 (UTC)
Received: (qmail 73886 invoked by uid 500); 20 May 2014 17:21:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73823 invoked by uid 500); 20 May 2014 17:21:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73809 invoked by uid 99); 20 May 2014 17:21:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:21:42 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.171 as permitted sender)
Received: from [209.85.216.171] (HELO mail-qc0-f171.google.com) (209.85.216.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:21:40 +0000
Received: by mail-qc0-f171.google.com with SMTP id x13so1254966qcv.2
        for <dev@spark.apache.org>; Tue, 20 May 2014 10:21:16 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=wCck1xRsS7vf1cYFKYWDLH9DTi/2Tz+9eNVXql9BcSo=;
        b=YAYJ9OeqLE1xRgK35hg0YAaHq3eYwe8A7F/9B0jvg427S5jFLqictkeZ2NmGVmPZxn
         kw7MbQP6zW3z2EtXt+g4QpGYH7YHydLZQfNpYTQQqHm6ZdPNcFtS6M50JlrPUuekBp07
         GluYXchVXlb0wk+yt3ilRFZgCOrX+JaHvPzEZ0UlDki9Gxh6Qjew1jTmovb6Cd8X/Rfb
         dpwQomohguvlJj8NkT9MthUpZzsh/hghqBCY8wT6apZz0BJoph3ZPNn9si6dtP0XRnVD
         zXXy2IAgKHkC8gZUQTLS/u40uPFGpMLldWNGRBh2KjwVqhRDJajkQhLD7VFbVtYZWVgg
         uBUA==
X-Gm-Message-State: ALoCoQmQFCNROS4w/YKXsuyAmxP3Qu07vC3dhNN7qAYOcOgmRIeJQ8GE8QZwJ8l8LTieg85yQX5R
MIME-Version: 1.0
X-Received: by 10.224.28.65 with SMTP id l1mr6783931qac.81.1400606476580; Tue,
 20 May 2014 10:21:16 -0700 (PDT)
Received: by 10.140.106.202 with HTTP; Tue, 20 May 2014 10:21:16 -0700 (PDT)
In-Reply-To: <CA+-p3AHZKSn1soP8Yxw8ARwGmN3t+bWDvR_WQ62hUMnrhLpXiA@mail.gmail.com>
References: <1400592333369-6715.post@n3.nabble.com>
	<CAMAsSd+CjY-NMo7vqrzB5SHAyoLUcYuiqHz7j76Y=3jryWkp5w@mail.gmail.com>
	<1400605810497-6719.post@n3.nabble.com>
	<CACBYxK+CDV_V9UHSoVw+Jv8Eb_V2BY+0BrhvQbJhsNW+6K=2_Q@mail.gmail.com>
	<CA+-p3AHZKSn1soP8Yxw8ARwGmN3t+bWDvR_WQ62hUMnrhLpXiA@mail.gmail.com>
Date: Tue, 20 May 2014 10:21:16 -0700
Message-ID: <CACBYxKKTbm2td9i7FFJ+VW-YHrrjaH44KqqJuqWz3HAq10tF3w@mail.gmail.com>
Subject: Re: Sorting partitions in Java
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2cd2881e55104f9d81bbb
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2cd2881e55104f9d81bbb
Content-Type: text/plain; charset=UTF-8

There is: SPARK-545


On Tue, May 20, 2014 at 10:16 AM, Andrew Ash <andrew@andrewash.com> wrote:

> Sandy, is there a Jira ticket for that?
>
>
> On Tue, May 20, 2014 at 10:12 AM, Sandy Ryza <sandy.ryza@cloudera.com
> >wrote:
>
> > sortByKey currently requires partitions to fit in memory, but there are
> > plans to add external sort
> >
> >
> > On Tue, May 20, 2014 at 10:10 AM, Madhu <madhu@madhu.com> wrote:
> >
> > > Thanks Sean, I had seen that post you mentioned.
> > >
> > > What you suggest looks an in-memory sort, which is fine if each
> partition
> > > is
> > > small enough to fit in memory. Is it true that rdd.sortByKey(...)
> > requires
> > > partitions to fit in memory? I wasn't sure if there was some magic
> behind
> > > the scenes that supports arbitrarily large sorts.
> > >
> > > None of this is a show stopper, it just might require a little more
> code
> > on
> > > the part of the developer. If there's a requirement for Spark
> partitions
> > to
> > > fit in memory, developers will have to be aware of that and plan
> > > accordingly. One nice feature of Hadoop MR is the ability to sort very
> > > large
> > > sets without thinking about data size.
> > >
> > > In the case that a developer repartitions an RDD such that some
> > partitions
> > > don't fit in memory, sorting those partitions requires more work. For
> > these
> > > cases, I think there is value in having a robust partition sorting
> method
> > > that deals with it efficiently and reliably.
> > >
> > > Is there another solution for sorting arbitrarily large partitions? If
> > not,
> > > I don't mind developing and contributing a solution.
> > >
> > >
> > >
> > >
> > > -----
> > > --
> > > Madhu
> > > https://www.linkedin.com/in/msiddalingaiah
> > > --
> > > View this message in context:
> > >
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Sorting-partitions-in-Java-tp6715p6719.html
> > > Sent from the Apache Spark Developers List mailing list archive at
> > > Nabble.com.
> > >
> >
>

--001a11c2cd2881e55104f9d81bbb--

From dev-return-7733-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 17:21:43 2014
Return-Path: <dev-return-7733-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A9B3E11C5A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 17:21:43 +0000 (UTC)
Received: (qmail 74039 invoked by uid 500); 20 May 2014 17:21:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73827 invoked by uid 500); 20 May 2014 17:21:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73813 invoked by uid 99); 20 May 2014 17:21:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:21:42 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.192.49 as permitted sender)
Received: from [209.85.192.49] (HELO mail-qg0-f49.google.com) (209.85.192.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:21:40 +0000
Received: by mail-qg0-f49.google.com with SMTP id a108so1253144qge.22
        for <dev@spark.incubator.apache.org>; Tue, 20 May 2014 10:21:16 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=wCck1xRsS7vf1cYFKYWDLH9DTi/2Tz+9eNVXql9BcSo=;
        b=I7Qv8bK/KPmdybvZltW/Q0RFVDPugccmG+exhOakY0n4eNmVKEnINk2oZaa50LH3lP
         KfWSoJjwjZBy8HwRemplewQiqHLVCeGPCuRkuPlj/Jd2OVHNNM/T3dmWTwAe/EDFqDx5
         GUXsECKj5va/DRsm2K0aCqTn27Dx9jtPcKcqWzwM/uO7vTt4jOecoAFSG4rzoy2+vyCj
         s5YdLC8qXwclkLrwi6nLygtyVGJdFG2ZTpadfxfK2XcP9ySQeW5zTvSMmAuEFwE0RM/F
         rKyBxOJQiPhxZjukA/1Q64/PEwlMwUwlwguNEn53P2150/lWjv5Du3/edR2XbqmdjDnh
         5K8A==
X-Gm-Message-State: ALoCoQkaskgw2aEa4bCCok+A7nhAJBk9/gfMihAMcchrMhKrRAQWWmoJpgkMjD57wqiBxOKsG/79
MIME-Version: 1.0
X-Received: by 10.224.28.65 with SMTP id l1mr6783931qac.81.1400606476580; Tue,
 20 May 2014 10:21:16 -0700 (PDT)
Received: by 10.140.106.202 with HTTP; Tue, 20 May 2014 10:21:16 -0700 (PDT)
In-Reply-To: <CA+-p3AHZKSn1soP8Yxw8ARwGmN3t+bWDvR_WQ62hUMnrhLpXiA@mail.gmail.com>
References: <1400592333369-6715.post@n3.nabble.com>
	<CAMAsSd+CjY-NMo7vqrzB5SHAyoLUcYuiqHz7j76Y=3jryWkp5w@mail.gmail.com>
	<1400605810497-6719.post@n3.nabble.com>
	<CACBYxK+CDV_V9UHSoVw+Jv8Eb_V2BY+0BrhvQbJhsNW+6K=2_Q@mail.gmail.com>
	<CA+-p3AHZKSn1soP8Yxw8ARwGmN3t+bWDvR_WQ62hUMnrhLpXiA@mail.gmail.com>
Date: Tue, 20 May 2014 10:21:16 -0700
Message-ID: <CACBYxKKTbm2td9i7FFJ+VW-YHrrjaH44KqqJuqWz3HAq10tF3w@mail.gmail.com>
Subject: Re: Sorting partitions in Java
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2cd2881e55104f9d81bbb
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2cd2881e55104f9d81bbb
Content-Type: text/plain; charset=UTF-8

There is: SPARK-545


On Tue, May 20, 2014 at 10:16 AM, Andrew Ash <andrew@andrewash.com> wrote:

> Sandy, is there a Jira ticket for that?
>
>
> On Tue, May 20, 2014 at 10:12 AM, Sandy Ryza <sandy.ryza@cloudera.com
> >wrote:
>
> > sortByKey currently requires partitions to fit in memory, but there are
> > plans to add external sort
> >
> >
> > On Tue, May 20, 2014 at 10:10 AM, Madhu <madhu@madhu.com> wrote:
> >
> > > Thanks Sean, I had seen that post you mentioned.
> > >
> > > What you suggest looks an in-memory sort, which is fine if each
> partition
> > > is
> > > small enough to fit in memory. Is it true that rdd.sortByKey(...)
> > requires
> > > partitions to fit in memory? I wasn't sure if there was some magic
> behind
> > > the scenes that supports arbitrarily large sorts.
> > >
> > > None of this is a show stopper, it just might require a little more
> code
> > on
> > > the part of the developer. If there's a requirement for Spark
> partitions
> > to
> > > fit in memory, developers will have to be aware of that and plan
> > > accordingly. One nice feature of Hadoop MR is the ability to sort very
> > > large
> > > sets without thinking about data size.
> > >
> > > In the case that a developer repartitions an RDD such that some
> > partitions
> > > don't fit in memory, sorting those partitions requires more work. For
> > these
> > > cases, I think there is value in having a robust partition sorting
> method
> > > that deals with it efficiently and reliably.
> > >
> > > Is there another solution for sorting arbitrarily large partitions? If
> > not,
> > > I don't mind developing and contributing a solution.
> > >
> > >
> > >
> > >
> > > -----
> > > --
> > > Madhu
> > > https://www.linkedin.com/in/msiddalingaiah
> > > --
> > > View this message in context:
> > >
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Sorting-partitions-in-Java-tp6715p6719.html
> > > Sent from the Apache Spark Developers List mailing list archive at
> > > Nabble.com.
> > >
> >
>

--001a11c2cd2881e55104f9d81bbb--

From dev-return-7730-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 17:23:41 2014
Return-Path: <dev-return-7730-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8434111C71
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 17:23:41 +0000 (UTC)
Received: (qmail 50526 invoked by uid 500); 20 May 2014 17:17:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50464 invoked by uid 500); 20 May 2014 17:17:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50455 invoked by uid 99); 20 May 2014 17:17:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:17:00 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.174] (HELO mail-vc0-f174.google.com) (209.85.220.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:16:56 +0000
Received: by mail-vc0-f174.google.com with SMTP id lh14so1012015vcb.19
        for <dev@spark.apache.org>; Tue, 20 May 2014 10:16:35 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=ZZQ5trzsJMk0jcsSK9BLNq58WIlWEUwmCVs86fDnyH0=;
        b=jItbmcEBLT+W8JvPuhZ8xoaXPLVPKXsZLx81km9PBMpwYcEscZhSTHuXTbas/dfKPX
         oX9hBfl8koEl0OQf0FLtVjmrBZk/YfQCy57bst4LrgQoubwBqAP5Pepv6JPCJGZL2Jhn
         X+KgPrwwUwpKtMQD6fPAObXWP6Meb7dkcls2zxgFNCSkEOo/OX38ytSCAaHI7g3q/RpM
         fZ9So9tDoB41yfI09QoCap0sqtFPnBiJyjF2/IBc4P8G92bYNNDPtVbfX0phvzDDxLOz
         mL24TXrWSHP/UvqZ0qyKbVlyoGnVLP4w7v/zY3WrFzAQNRCJiHRTXWHReFgDnO1Nxg0R
         t23Q==
X-Gm-Message-State: ALoCoQmcCfIusWdvMGN4PS424HnKtQDN4dw2rmJlbsU+nyPn7DdI/VtqnEgZD1MRzm19wpu6af8+
X-Received: by 10.52.14.72 with SMTP id n8mr1422591vdc.73.1400606195828;
        Tue, 20 May 2014 10:16:35 -0700 (PDT)
Received: from mail-ve0-f172.google.com (mail-ve0-f172.google.com [209.85.128.172])
        by mx.google.com with ESMTPSA id ew15sm641327veb.4.2014.05.20.10.16.34
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 20 May 2014 10:16:34 -0700 (PDT)
Received: by mail-ve0-f172.google.com with SMTP id oz11so1004483veb.3
        for <dev@spark.apache.org>; Tue, 20 May 2014 10:16:34 -0700 (PDT)
X-Received: by 10.58.46.83 with SMTP id t19mr1777006vem.60.1400606194225; Tue,
 20 May 2014 10:16:34 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Tue, 20 May 2014 10:16:14 -0700 (PDT)
In-Reply-To: <CACBYxK+CDV_V9UHSoVw+Jv8Eb_V2BY+0BrhvQbJhsNW+6K=2_Q@mail.gmail.com>
References: <1400592333369-6715.post@n3.nabble.com> <CAMAsSd+CjY-NMo7vqrzB5SHAyoLUcYuiqHz7j76Y=3jryWkp5w@mail.gmail.com>
 <1400605810497-6719.post@n3.nabble.com> <CACBYxK+CDV_V9UHSoVw+Jv8Eb_V2BY+0BrhvQbJhsNW+6K=2_Q@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Tue, 20 May 2014 10:16:14 -0700
Message-ID: <CA+-p3AHZKSn1soP8Yxw8ARwGmN3t+bWDvR_WQ62hUMnrhLpXiA@mail.gmail.com>
Subject: Re: Sorting partitions in Java
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e0118456aad591504f9d80a34
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0118456aad591504f9d80a34
Content-Type: text/plain; charset=UTF-8

Sandy, is there a Jira ticket for that?


On Tue, May 20, 2014 at 10:12 AM, Sandy Ryza <sandy.ryza@cloudera.com>wrote:

> sortByKey currently requires partitions to fit in memory, but there are
> plans to add external sort
>
>
> On Tue, May 20, 2014 at 10:10 AM, Madhu <madhu@madhu.com> wrote:
>
> > Thanks Sean, I had seen that post you mentioned.
> >
> > What you suggest looks an in-memory sort, which is fine if each partition
> > is
> > small enough to fit in memory. Is it true that rdd.sortByKey(...)
> requires
> > partitions to fit in memory? I wasn't sure if there was some magic behind
> > the scenes that supports arbitrarily large sorts.
> >
> > None of this is a show stopper, it just might require a little more code
> on
> > the part of the developer. If there's a requirement for Spark partitions
> to
> > fit in memory, developers will have to be aware of that and plan
> > accordingly. One nice feature of Hadoop MR is the ability to sort very
> > large
> > sets without thinking about data size.
> >
> > In the case that a developer repartitions an RDD such that some
> partitions
> > don't fit in memory, sorting those partitions requires more work. For
> these
> > cases, I think there is value in having a robust partition sorting method
> > that deals with it efficiently and reliably.
> >
> > Is there another solution for sorting arbitrarily large partitions? If
> not,
> > I don't mind developing and contributing a solution.
> >
> >
> >
> >
> > -----
> > --
> > Madhu
> > https://www.linkedin.com/in/msiddalingaiah
> > --
> > View this message in context:
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Sorting-partitions-in-Java-tp6715p6719.html
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> >
>

--089e0118456aad591504f9d80a34--

From dev-return-7734-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 17:26:01 2014
Return-Path: <dev-return-7734-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A807F11C8B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 17:26:01 +0000 (UTC)
Received: (qmail 85403 invoked by uid 500); 20 May 2014 17:26:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85348 invoked by uid 500); 20 May 2014 17:26:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85340 invoked by uid 99); 20 May 2014 17:26:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:26:01 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.171] (HELO mail-vc0-f171.google.com) (209.85.220.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:25:57 +0000
Received: by mail-vc0-f171.google.com with SMTP id lc6so1023719vcb.16
        for <dev@spark.incubator.apache.org>; Tue, 20 May 2014 10:25:36 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=P43iZqfN1wFg9DjIHr4UCEz+crposNbyBZ2ZPiXA4DI=;
        b=TiAmfK+lSt0v22OjRz2nRRfp87s/J0Jfs3a5e+0fpGCT4uIlbv53z0ftgaZQ20F6KB
         FvrZdjHfzaMaUv4NLiBGeNzoQAACCXIU7NnDb6cGPzpSyX78P1jxRYDusoYTD9Z+H/Sw
         vW04t9qqpYX+GYIlTna9VmAFM1srlsV7JBc9evCXDoOyvuixFRc6r3t7kNVcd01lKUsu
         5xeesjb8O4yh6HyZbb5mfwS5+sh3/sdqyBzAn2eXEFxjnwUsdcgzMcRP/d30w9Wv0ANT
         0IqP2LtvI2A3hlDzomjGCgHPKx4LWNq7QKuINzd3aL+9atuJoMBcBs3QdbHMNzZEr1dQ
         rxgw==
X-Gm-Message-State: ALoCoQlz+HU+6VZi2L+uo7BgRrEKlDvcuAmwhPhutFXDw4dRHahHdXKzp2JIUbn/QVJ5gSiKpTCY
X-Received: by 10.221.30.14 with SMTP id sa14mr2182182vcb.44.1400606736611;
        Tue, 20 May 2014 10:25:36 -0700 (PDT)
Received: from mail-ve0-f181.google.com (mail-ve0-f181.google.com [209.85.128.181])
        by mx.google.com with ESMTPSA id 5sm34694477vdg.8.2014.05.20.10.25.35
        for <dev@spark.incubator.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 20 May 2014 10:25:35 -0700 (PDT)
Received: by mail-ve0-f181.google.com with SMTP id pa12so1002828veb.26
        for <dev@spark.incubator.apache.org>; Tue, 20 May 2014 10:25:34 -0700 (PDT)
X-Received: by 10.220.253.132 with SMTP id na4mr2541200vcb.39.1400606734824;
 Tue, 20 May 2014 10:25:34 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Tue, 20 May 2014 10:25:14 -0700 (PDT)
In-Reply-To: <CACBYxKKTbm2td9i7FFJ+VW-YHrrjaH44KqqJuqWz3HAq10tF3w@mail.gmail.com>
References: <1400592333369-6715.post@n3.nabble.com> <CAMAsSd+CjY-NMo7vqrzB5SHAyoLUcYuiqHz7j76Y=3jryWkp5w@mail.gmail.com>
 <1400605810497-6719.post@n3.nabble.com> <CACBYxK+CDV_V9UHSoVw+Jv8Eb_V2BY+0BrhvQbJhsNW+6K=2_Q@mail.gmail.com>
 <CA+-p3AHZKSn1soP8Yxw8ARwGmN3t+bWDvR_WQ62hUMnrhLpXiA@mail.gmail.com> <CACBYxKKTbm2td9i7FFJ+VW-YHrrjaH44KqqJuqWz3HAq10tF3w@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Tue, 20 May 2014 10:25:14 -0700
Message-ID: <CA+-p3AHfAR3mGJM0gmXdz8hM4zTwvy7ewTG2Qr9faeUpHP218g@mail.gmail.com>
Subject: Re: Sorting partitions in Java
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a1133ddf8e63e9c04f9d82a52
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133ddf8e63e9c04f9d82a52
Content-Type: text/plain; charset=UTF-8

Voted :)

https://issues.apache.org/jira/browse/SPARK-983


On Tue, May 20, 2014 at 10:21 AM, Sandy Ryza <sandy.ryza@cloudera.com>wrote:

> There is: SPARK-545
>
>
> On Tue, May 20, 2014 at 10:16 AM, Andrew Ash <andrew@andrewash.com> wrote:
>
> > Sandy, is there a Jira ticket for that?
> >
> >
> > On Tue, May 20, 2014 at 10:12 AM, Sandy Ryza <sandy.ryza@cloudera.com
> > >wrote:
> >
> > > sortByKey currently requires partitions to fit in memory, but there are
> > > plans to add external sort
> > >
> > >
> > > On Tue, May 20, 2014 at 10:10 AM, Madhu <madhu@madhu.com> wrote:
> > >
> > > > Thanks Sean, I had seen that post you mentioned.
> > > >
> > > > What you suggest looks an in-memory sort, which is fine if each
> > partition
> > > > is
> > > > small enough to fit in memory. Is it true that rdd.sortByKey(...)
> > > requires
> > > > partitions to fit in memory? I wasn't sure if there was some magic
> > behind
> > > > the scenes that supports arbitrarily large sorts.
> > > >
> > > > None of this is a show stopper, it just might require a little more
> > code
> > > on
> > > > the part of the developer. If there's a requirement for Spark
> > partitions
> > > to
> > > > fit in memory, developers will have to be aware of that and plan
> > > > accordingly. One nice feature of Hadoop MR is the ability to sort
> very
> > > > large
> > > > sets without thinking about data size.
> > > >
> > > > In the case that a developer repartitions an RDD such that some
> > > partitions
> > > > don't fit in memory, sorting those partitions requires more work. For
> > > these
> > > > cases, I think there is value in having a robust partition sorting
> > method
> > > > that deals with it efficiently and reliably.
> > > >
> > > > Is there another solution for sorting arbitrarily large partitions?
> If
> > > not,
> > > > I don't mind developing and contributing a solution.
> > > >
> > > >
> > > >
> > > >
> > > > -----
> > > > --
> > > > Madhu
> > > > https://www.linkedin.com/in/msiddalingaiah
> > > > --
> > > > View this message in context:
> > > >
> > >
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Sorting-partitions-in-Java-tp6715p6719.html
> > > > Sent from the Apache Spark Developers List mailing list archive at
> > > > Nabble.com.
> > > >
> > >
> >
>

--001a1133ddf8e63e9c04f9d82a52--

From dev-return-7735-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 17:26:02 2014
Return-Path: <dev-return-7735-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3140811C8D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 17:26:02 +0000 (UTC)
Received: (qmail 86001 invoked by uid 500); 20 May 2014 17:26:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85889 invoked by uid 500); 20 May 2014 17:26:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85809 invoked by uid 99); 20 May 2014 17:26:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:26:01 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.128.170] (HELO mail-ve0-f170.google.com) (209.85.128.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:25:57 +0000
Received: by mail-ve0-f170.google.com with SMTP id db11so1022275veb.29
        for <dev@spark.apache.org>; Tue, 20 May 2014 10:25:37 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=P43iZqfN1wFg9DjIHr4UCEz+crposNbyBZ2ZPiXA4DI=;
        b=AwUvS1VxWfAf0aIcJdiUiEvyGC5CQ7OA1R3vAb1ssJtnluIS1K8L2nW8Y3cXYeG+0+
         syrDZDswElGeEllI1FYo4hfDj5DW/oW0gPeu7Y6Bn2K9XEKLz7mnbyw6uRvopmlYTOTi
         6cTUq9X8XwjMcQbGxP5V8js997Z2CVOEMXe9pmultuFc6zvijCjs7G1Q2zYopRUTrHiz
         zTMpzHcd0nHtewj4hs9sDessckCZ7wqAf1c9S2nmnGesraXTSjvufvD5t2erfZoHMVFO
         5KAugfNk72otUx/pmhxMmnt11hvfUa1LGlAjg3c8gLczFUgvUkGHeWzZyY3E2OMCs831
         +OjQ==
X-Gm-Message-State: ALoCoQlC6og6TG7ra4MNIXi8cTGCwwPZhLNYbbGgnvLKUDJ6ngjIqNIlTi85nNmcWsP8+li15lW4
X-Received: by 10.58.146.5 with SMTP id sy5mr2334055veb.43.1400606736866;
        Tue, 20 May 2014 10:25:36 -0700 (PDT)
Received: from mail-vc0-f174.google.com (mail-vc0-f174.google.com [209.85.220.174])
        by mx.google.com with ESMTPSA id us17sm37690055vdb.19.2014.05.20.10.25.35
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 20 May 2014 10:25:35 -0700 (PDT)
Received: by mail-vc0-f174.google.com with SMTP id lh14so1027618vcb.19
        for <dev@spark.apache.org>; Tue, 20 May 2014 10:25:34 -0700 (PDT)
X-Received: by 10.220.253.132 with SMTP id na4mr2541200vcb.39.1400606734824;
 Tue, 20 May 2014 10:25:34 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Tue, 20 May 2014 10:25:14 -0700 (PDT)
In-Reply-To: <CACBYxKKTbm2td9i7FFJ+VW-YHrrjaH44KqqJuqWz3HAq10tF3w@mail.gmail.com>
References: <1400592333369-6715.post@n3.nabble.com> <CAMAsSd+CjY-NMo7vqrzB5SHAyoLUcYuiqHz7j76Y=3jryWkp5w@mail.gmail.com>
 <1400605810497-6719.post@n3.nabble.com> <CACBYxK+CDV_V9UHSoVw+Jv8Eb_V2BY+0BrhvQbJhsNW+6K=2_Q@mail.gmail.com>
 <CA+-p3AHZKSn1soP8Yxw8ARwGmN3t+bWDvR_WQ62hUMnrhLpXiA@mail.gmail.com> <CACBYxKKTbm2td9i7FFJ+VW-YHrrjaH44KqqJuqWz3HAq10tF3w@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Tue, 20 May 2014 10:25:14 -0700
Message-ID: <CA+-p3AHfAR3mGJM0gmXdz8hM4zTwvy7ewTG2Qr9faeUpHP218g@mail.gmail.com>
Subject: Re: Sorting partitions in Java
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a1133ddf8e63e9c04f9d82a52
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133ddf8e63e9c04f9d82a52
Content-Type: text/plain; charset=UTF-8

Voted :)

https://issues.apache.org/jira/browse/SPARK-983


On Tue, May 20, 2014 at 10:21 AM, Sandy Ryza <sandy.ryza@cloudera.com>wrote:

> There is: SPARK-545
>
>
> On Tue, May 20, 2014 at 10:16 AM, Andrew Ash <andrew@andrewash.com> wrote:
>
> > Sandy, is there a Jira ticket for that?
> >
> >
> > On Tue, May 20, 2014 at 10:12 AM, Sandy Ryza <sandy.ryza@cloudera.com
> > >wrote:
> >
> > > sortByKey currently requires partitions to fit in memory, but there are
> > > plans to add external sort
> > >
> > >
> > > On Tue, May 20, 2014 at 10:10 AM, Madhu <madhu@madhu.com> wrote:
> > >
> > > > Thanks Sean, I had seen that post you mentioned.
> > > >
> > > > What you suggest looks an in-memory sort, which is fine if each
> > partition
> > > > is
> > > > small enough to fit in memory. Is it true that rdd.sortByKey(...)
> > > requires
> > > > partitions to fit in memory? I wasn't sure if there was some magic
> > behind
> > > > the scenes that supports arbitrarily large sorts.
> > > >
> > > > None of this is a show stopper, it just might require a little more
> > code
> > > on
> > > > the part of the developer. If there's a requirement for Spark
> > partitions
> > > to
> > > > fit in memory, developers will have to be aware of that and plan
> > > > accordingly. One nice feature of Hadoop MR is the ability to sort
> very
> > > > large
> > > > sets without thinking about data size.
> > > >
> > > > In the case that a developer repartitions an RDD such that some
> > > partitions
> > > > don't fit in memory, sorting those partitions requires more work. For
> > > these
> > > > cases, I think there is value in having a robust partition sorting
> > method
> > > > that deals with it efficiently and reliably.
> > > >
> > > > Is there another solution for sorting arbitrarily large partitions?
> If
> > > not,
> > > > I don't mind developing and contributing a solution.
> > > >
> > > >
> > > >
> > > >
> > > > -----
> > > > --
> > > > Madhu
> > > > https://www.linkedin.com/in/msiddalingaiah
> > > > --
> > > > View this message in context:
> > > >
> > >
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Sorting-partitions-in-Java-tp6715p6719.html
> > > > Sent from the Apache Spark Developers List mailing list archive at
> > > > Nabble.com.
> > > >
> > >
> >
>

--001a1133ddf8e63e9c04f9d82a52--

From dev-return-7736-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 17:27:50 2014
Return-Path: <dev-return-7736-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4759111CB3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 17:27:50 +0000 (UTC)
Received: (qmail 94863 invoked by uid 500); 20 May 2014 17:27:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94804 invoked by uid 500); 20 May 2014 17:27:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94796 invoked by uid 99); 20 May 2014 17:27:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:27:49 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.220.181 as permitted sender)
Received: from [209.85.220.181] (HELO mail-vc0-f181.google.com) (209.85.220.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:27:47 +0000
Received: by mail-vc0-f181.google.com with SMTP id ld13so1013888vcb.26
        for <dev@spark.apache.org>; Tue, 20 May 2014 10:27:23 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=76HQE5rCV+iaoal2zMvK4RHwKBrnRt858oYFQH5r2i0=;
        b=GAnLxvS/rIs9RoBtS2OhH6qH5Qslls0c8IZacEKEw/XwjNoY6qOfarW98VZhmIIXbb
         Pzkghb5494W4JkACHAFZfxPZZDibKvgU6JHDgkf70cNZ2ABtfhlvVqBJs3U+MDI1Z4Z8
         am+hG/Tpi7QIvIC2OfGFofu2LB8NxQgQTgzCtCgk8fXGHvTe8rbHyyr6/xVqp0ZbN2r/
         qhsSdON9dCqZR2qeoEq4dTi76YQOurdxBvOTCgDlygOZxTycxBJPgummUWpOBJkzJaDJ
         jy348xRhp8ZgVOVZ9iiV2n9n5W3p1Twm+/0yR0NljttYSeblw1IAfhuLOz8exfwR2+u5
         7lOQ==
X-Gm-Message-State: ALoCoQl/ErCCQY6D0mPAB15MEmw5RYAMQj5jGbo+7ypEmQVg5OWFGovQr7UH2uLFCt/9IexvosJV
X-Received: by 10.220.92.193 with SMTP id s1mr5054919vcm.34.1400606843686;
 Tue, 20 May 2014 10:27:23 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.111.69 with HTTP; Tue, 20 May 2014 10:27:03 -0700 (PDT)
In-Reply-To: <1400605810497-6719.post@n3.nabble.com>
References: <1400592333369-6715.post@n3.nabble.com> <CAMAsSd+CjY-NMo7vqrzB5SHAyoLUcYuiqHz7j76Y=3jryWkp5w@mail.gmail.com>
 <1400605810497-6719.post@n3.nabble.com>
From: Sean Owen <sowen@cloudera.com>
Date: Tue, 20 May 2014 18:27:03 +0100
Message-ID: <CAMAsSdKmpboFUbat9mYh90QmevqGAF7_pN9_N0NqiefUa-6BFg@mail.gmail.com>
Subject: Re: Sorting partitions in Java
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Tue, May 20, 2014 at 6:10 PM, Madhu <madhu@madhu.com> wrote:
> What you suggest looks an in-memory sort, which is fine if each partition is
> small enough to fit in memory. Is it true that rdd.sortByKey(...) requires
> partitions to fit in memory? I wasn't sure if there was some magic behind
> the scenes that supports arbitrarily large sorts.

Yes, but so did the Scala version you posted -- I assumed that was OK
for your use case. Regardless of what Spark does, you would copy all
values into memory with toArray.

sortByKey is something fairly different. It sorts the whole RDD by
key, not values within each key. I think Sandy is talking about
something related but not quite the same.

Do you really mean you want to sort the whole RDD?

From dev-return-7737-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 17:49:04 2014
Return-Path: <dev-return-7737-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C74F711DD0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 17:49:04 +0000 (UTC)
Received: (qmail 56952 invoked by uid 500); 20 May 2014 17:49:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 56892 invoked by uid 500); 20 May 2014 17:49:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56884 invoked by uid 99); 20 May 2014 17:49:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:49:04 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 17:49:00 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <madhu@madhu.com>)
	id 1Wmo98-0000Tq-Dt
	for dev@spark.incubator.apache.org; Tue, 20 May 2014 10:48:14 -0700
Date: Tue, 20 May 2014 10:47:59 -0700 (PDT)
From: Madhu <madhu@madhu.com>
To: dev@spark.incubator.apache.org
Message-ID: <1400608079413-6725.post@n3.nabble.com>
In-Reply-To: <CAMAsSdKmpboFUbat9mYh90QmevqGAF7_pN9_N0NqiefUa-6BFg@mail.gmail.com>
References: <1400592333369-6715.post@n3.nabble.com> <CAMAsSd+CjY-NMo7vqrzB5SHAyoLUcYuiqHz7j76Y=3jryWkp5w@mail.gmail.com> <1400605810497-6719.post@n3.nabble.com> <CAMAsSdKmpboFUbat9mYh90QmevqGAF7_pN9_N0NqiefUa-6BFg@mail.gmail.com>
Subject: Re: Sorting partitions in Java
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Sean,

No, I don't want to sort the whole RDD, sortByKey seems to be good enough
for that.

Right now, I think the code I have will work for me, but I can imagine
conditions where it will run out of memory.

I'm not completely sure if  SPARK-983
<https://issues.apache.org/jira/browse/SPARK-983>    Andrew mentioned covers
the rdd.sortPartitions() use case. Can someone comment on the scope of
SPARK-983?

Thanks!



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Sorting-partitions-in-Java-tp6715p6725.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7738-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 19:17:47 2014
Return-Path: <dev-return-7738-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 41E47111DD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 19:17:47 +0000 (UTC)
Received: (qmail 59376 invoked by uid 500); 20 May 2014 19:17:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59323 invoked by uid 500); 20 May 2014 19:17:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59315 invoked by uid 99); 20 May 2014 19:17:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 19:17:46 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of glenn.strycker@gmail.com does not designate 216.139.250.139 as permitted sender)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 19:17:43 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <glenn.strycker@gmail.com>)
	id 1WmpWx-0005Uq-7L
	for dev@spark.incubator.apache.org; Tue, 20 May 2014 12:16:55 -0700
Date: Tue, 20 May 2014 12:16:40 -0700 (PDT)
From: GlennStrycker <glenn.strycker@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1400613400205-6726.post@n3.nabble.com>
In-Reply-To: <1400595206326-6717.post@n3.nabble.com>
References: <1400530169998-6693.post@n3.nabble.com> <CAPh_B=bwgbWJxNV0YEGt-Dr3yWTY0QDd6DZX9tcfZDQTVdermg@mail.gmail.com> <1400537762804-6695.post@n3.nabble.com> <CAPh_B=Yi=a-AvwYp1+jUXcwTids_KgCY3Hp+5JPQJfSb2W3O1A@mail.gmail.com> <1400538760695-6697.post@n3.nabble.com> <CAPh_B=ZDpYmZggfL7CTd+zekxfwHciAfjMVg1rNspcA5ynWftg@mail.gmail.com> <1400595206326-6717.post@n3.nabble.com>
Subject: Re: BUG: graph.triplets does not return proper values
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Wait a minute... doesn't a reduce function return 1 element PER key pair? 
For example, word-count mapreduce functions return a {word, count} element
for every unique word.  Is this supposed to be a 1-element RDD object?

The .reduce function for a MappedRDD or FlatMappedRDD both are of the form

    def reduce(f: (T, T) => T): T

So presumably if I pass the reduce function a list of values {(X,1), (X,1),
(X,1), (Y,1), (Y,1)} and the function is ( (A,B) => (A._1, A._2+B._2 ) ),
then I should get a final vector of {(X,3), (Y,2)}, correct?


I have the following object:

    scala> temp3
    res128: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.Edge[Int],
Int)] = MappedRDD[107] at map at <console>:27

and it contains the following:

    scala> temp3.collect
    . . .
    res129: Array[(org.apache.spark.graphx.Edge[Int], Int)] =
Array((Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1),
(Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1),
(Edge(4,4,1),1), (Edge(5,4,1),1), (Edge(0,0,0),1), (Edge(0,0,0),1),
(Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(7,4,1),1), (Edge(0,0,0),1),
(Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1),
(Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1),
(Edge(4,5,1),1), (Edge(5,5,1),1), (Edge(1,2,1),1), (Edge(1,3,1),1),
(Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(7,5,1),1), (Edge(0,0,0),1),
(Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1),
(Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1),
(Edge(4,7,1),1), (Edge(5,7,1),1), (Edge(0,0,0),1), (E...

but when I run the following, I only get one element in the final vector:

    scala> temp3.reduce( (A,B) => (A._1, A._2+B._2 ) )
    . . .
    res130: (org.apache.spark.graphx.Edge[Int], Int) = (Edge(0,0,0),256)

I should be additionally getting { (Edge(1,2,1),1), (Edge(1,3,1),2),
(Edge(2,3,1),2), (Edge(4,5,1),1), (Edge(5,6,1),2), (Edge(6,7,1),1),
(Edge(4,7,1),1), (Edge(5,7,1),2) }



Am I not mapping something correctly before running reduce?  I've tried both
.map and .flatMap, and put in _.copy() everywhere, e.g.

temp3.flatMap(A => Seq(A)).reduce( (A,B) => (A._1, A._2+B._2 ) )
temp3.map(_.copy()).flatMap(A => Seq(A)).reduce( (A,B) => (A._1, A._2+B._2 )
)
etc.





--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/BUG-graph-triplets-does-not-return-proper-values-tp6693p6726.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7739-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 19:42:47 2014
Return-Path: <dev-return-7739-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8601C11286
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 19:42:47 +0000 (UTC)
Received: (qmail 4334 invoked by uid 500); 20 May 2014 19:42:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4276 invoked by uid 500); 20 May 2014 19:42:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4267 invoked by uid 99); 20 May 2014 19:42:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 19:42:47 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.41] (HELO mail-qg0-f41.google.com) (209.85.192.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 19:42:42 +0000
Received: by mail-qg0-f41.google.com with SMTP id j5so1607252qga.0
        for <dev@spark.apache.org>; Tue, 20 May 2014 12:42:22 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=Dd9FEVWc5xJB+tc3pLRgTlmpX56P6Gw2pfnRfpaoh+s=;
        b=VNvsx8Fh2/TAWL6AdcQd2NmQ22yBeuc/ENQmva9TRbAZ1sY5OwYbHd4MxhipiXb2MT
         NNJsX2N3yQ7Tb5spWdYM1oGZUt5opBZAh5e+eakzwtn60m4yTpt+cTezSSaLNIzuhH02
         iDMdXZ7JdOuqRpaOp6VBY8J34y7wnPMpbircMImA2pEeVOvQlwtoKy60zDYoipvgGpi9
         5Smz9tcuThnWKYcvheAfDqPVB7rGqwfc/hjLXjXfcF34RK11tKGQn3RMt72LpBKqUydd
         EeDpSUkkQnYyo1DDrZWx4LDpzvAuN97kz2oihIxYfhutTiuVxlTQQH+E7O6wp67LQxs9
         H2iw==
X-Gm-Message-State: ALoCoQljH5VyYmEaQBYGqY5ouP03qI+SYJGm7sv4B6sT33FoR5DczL7EKKms9sm+xXHOObPD2b7E
X-Received: by 10.140.97.55 with SMTP id l52mr59886065qge.19.1400614941927;
 Tue, 20 May 2014 12:42:21 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.126.1 with HTTP; Tue, 20 May 2014 12:42:01 -0700 (PDT)
In-Reply-To: <1400613400205-6726.post@n3.nabble.com>
References: <1400530169998-6693.post@n3.nabble.com> <CAPh_B=bwgbWJxNV0YEGt-Dr3yWTY0QDd6DZX9tcfZDQTVdermg@mail.gmail.com>
 <1400537762804-6695.post@n3.nabble.com> <CAPh_B=Yi=a-AvwYp1+jUXcwTids_KgCY3Hp+5JPQJfSb2W3O1A@mail.gmail.com>
 <1400538760695-6697.post@n3.nabble.com> <CAPh_B=ZDpYmZggfL7CTd+zekxfwHciAfjMVg1rNspcA5ynWftg@mail.gmail.com>
 <1400595206326-6717.post@n3.nabble.com> <1400613400205-6726.post@n3.nabble.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 20 May 2014 12:42:01 -0700
Message-ID: <CAPh_B=bbCPagwNTmmQseX9VMAXm-QmJh2uV=At=Yo_9pT=REXw@mail.gmail.com>
Subject: Re: BUG: graph.triplets does not return proper values
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113a98ce14bfce04f9da14a8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a98ce14bfce04f9da14a8
Content-Type: text/plain; charset=UTF-8

You are probably looking for reduceByKey in that case.

"reduce" just reduces everything in the collection into a single element.


On Tue, May 20, 2014 at 12:16 PM, GlennStrycker <glenn.strycker@gmail.com>wrote:

> Wait a minute... doesn't a reduce function return 1 element PER key pair?
> For example, word-count mapreduce functions return a {word, count} element
> for every unique word.  Is this supposed to be a 1-element RDD object?
>
> The .reduce function for a MappedRDD or FlatMappedRDD both are of the form
>
>     def reduce(f: (T, T) => T): T
>
> So presumably if I pass the reduce function a list of values {(X,1), (X,1),
> (X,1), (Y,1), (Y,1)} and the function is ( (A,B) => (A._1, A._2+B._2 ) ),
> then I should get a final vector of {(X,3), (Y,2)}, correct?
>
>
> I have the following object:
>
>     scala> temp3
>     res128: org.apache.spark.rdd.RDD[(org.apache.spark.graphx.Edge[Int],
> Int)] = MappedRDD[107] at map at <console>:27
>
> and it contains the following:
>
>     scala> temp3.collect
>     . . .
>     res129: Array[(org.apache.spark.graphx.Edge[Int], Int)] =
> Array((Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1),
> (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1),
> (Edge(4,4,1),1), (Edge(5,4,1),1), (Edge(0,0,0),1), (Edge(0,0,0),1),
> (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(7,4,1),1), (Edge(0,0,0),1),
> (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1),
> (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1),
> (Edge(4,5,1),1), (Edge(5,5,1),1), (Edge(1,2,1),1), (Edge(1,3,1),1),
> (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(7,5,1),1), (Edge(0,0,0),1),
> (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1),
> (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1), (Edge(0,0,0),1),
> (Edge(4,7,1),1), (Edge(5,7,1),1), (Edge(0,0,0),1), (E...
>
> but when I run the following, I only get one element in the final vector:
>
>     scala> temp3.reduce( (A,B) => (A._1, A._2+B._2 ) )
>     . . .
>     res130: (org.apache.spark.graphx.Edge[Int], Int) = (Edge(0,0,0),256)
>
> I should be additionally getting { (Edge(1,2,1),1), (Edge(1,3,1),2),
> (Edge(2,3,1),2), (Edge(4,5,1),1), (Edge(5,6,1),2), (Edge(6,7,1),1),
> (Edge(4,7,1),1), (Edge(5,7,1),2) }
>
>
>
> Am I not mapping something correctly before running reduce?  I've tried
> both
> .map and .flatMap, and put in _.copy() everywhere, e.g.
>
> temp3.flatMap(A => Seq(A)).reduce( (A,B) => (A._1, A._2+B._2 ) )
> temp3.map(_.copy()).flatMap(A => Seq(A)).reduce( (A,B) => (A._1, A._2+B._2
> )
> )
> etc.
>
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/BUG-graph-triplets-does-not-return-proper-values-tp6693p6726.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a113a98ce14bfce04f9da14a8--

From dev-return-7740-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 20:01:37 2014
Return-Path: <dev-return-7740-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E1A9D11328
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 20:01:37 +0000 (UTC)
Received: (qmail 44404 invoked by uid 500); 20 May 2014 20:01:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44346 invoked by uid 500); 20 May 2014 20:01:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44338 invoked by uid 99); 20 May 2014 20:01:37 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 20:01:37 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of glenn.strycker@gmail.com does not designate 216.139.250.139 as permitted sender)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 20:01:34 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <glenn.strycker@gmail.com>)
	id 1WmqDN-0007Uw-T7
	for dev@spark.incubator.apache.org; Tue, 20 May 2014 13:00:45 -0700
Date: Tue, 20 May 2014 13:00:30 -0700 (PDT)
From: GlennStrycker <glenn.strycker@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1400616030883-6728.post@n3.nabble.com>
In-Reply-To: <CAPh_B=bbCPagwNTmmQseX9VMAXm-QmJh2uV=At=Yo_9pT=REXw@mail.gmail.com>
References: <1400530169998-6693.post@n3.nabble.com> <CAPh_B=bwgbWJxNV0YEGt-Dr3yWTY0QDd6DZX9tcfZDQTVdermg@mail.gmail.com> <1400537762804-6695.post@n3.nabble.com> <CAPh_B=Yi=a-AvwYp1+jUXcwTids_KgCY3Hp+5JPQJfSb2W3O1A@mail.gmail.com> <1400538760695-6697.post@n3.nabble.com> <CAPh_B=ZDpYmZggfL7CTd+zekxfwHciAfjMVg1rNspcA5ynWftg@mail.gmail.com> <1400595206326-6717.post@n3.nabble.com> <1400613400205-6726.post@n3.nabble.com> <CAPh_B=bbCPagwNTmmQseX9VMAXm-QmJh2uV=At=Yo_9pT=REXw@mail.gmail.com>
Subject: Re: BUG: graph.triplets does not return proper values
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I don't seem to have this function in my Spark installation for this object,
or the classes MappedRDD, FlatMappedRDD, EdgeRDD, VertexRDD, or Graph.

Which class should have the reduceByKey function, and how do I cast my
current RDD as this class?

Perhaps this is still due to my Spark installation being out-of-date?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/BUG-graph-triplets-does-not-return-proper-values-tp6693p6728.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7741-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 20:06:48 2014
Return-Path: <dev-return-7741-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A752411346
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 20:06:48 +0000 (UTC)
Received: (qmail 50773 invoked by uid 500); 20 May 2014 20:06:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50714 invoked by uid 500); 20 May 2014 20:06:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50706 invoked by uid 99); 20 May 2014 20:06:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 20:06:48 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mark@clearstorydata.com designates 74.125.82.49 as permitted sender)
Received: from [74.125.82.49] (HELO mail-wg0-f49.google.com) (74.125.82.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 20:06:45 +0000
Received: by mail-wg0-f49.google.com with SMTP id m15so1044380wgh.32
        for <dev@spark.apache.org>; Tue, 20 May 2014 13:06:21 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=QeJ410/HAnqBBByXzhSnQnHIBLVJa5CWxyuVYbrEv+Y=;
        b=aaaB25OP1iM7SMNXi07U/ZCT/olOGuqVWij7v95jqTHfuhicfZCikYfPW+FAGuOSCp
         uVqQ1f6Nej8EihZ24XFAGTYCCf692BNJh4bcCYYf1Uw5EgFwoVR+TG9EkDAUjIaowyzF
         0Ryv3S+4azQrX8V2FxBLxcnS71/+LssG0ksXeJnP4841wT/g+x3hn6yEt+fGei4eDsfm
         fLM/UaM9oUqy0r/cTag72IXYfGTT9a0PQq9xV0AEhWVygtvYpFB6yqENvvYPt2h8iJjJ
         aHsSjikoLsW0AgsXjYdhvI7MXq5f47Qh/31MbNBewQViXGnNLa4Kge8YKZ+/s0yBA6uY
         Q6FA==
X-Gm-Message-State: ALoCoQk0NdVdm+QryFdO8SSTphx2TAv0tZMp3JkoAB/1/BVz9lbnh9c+xi/ArXdOA4ZNEOu2If1B
MIME-Version: 1.0
X-Received: by 10.194.243.104 with SMTP id wx8mr38991144wjc.32.1400616381239;
 Tue, 20 May 2014 13:06:21 -0700 (PDT)
Received: by 10.216.161.67 with HTTP; Tue, 20 May 2014 13:06:21 -0700 (PDT)
In-Reply-To: <1400616030883-6728.post@n3.nabble.com>
References: <1400530169998-6693.post@n3.nabble.com>
	<CAPh_B=bwgbWJxNV0YEGt-Dr3yWTY0QDd6DZX9tcfZDQTVdermg@mail.gmail.com>
	<1400537762804-6695.post@n3.nabble.com>
	<CAPh_B=Yi=a-AvwYp1+jUXcwTids_KgCY3Hp+5JPQJfSb2W3O1A@mail.gmail.com>
	<1400538760695-6697.post@n3.nabble.com>
	<CAPh_B=ZDpYmZggfL7CTd+zekxfwHciAfjMVg1rNspcA5ynWftg@mail.gmail.com>
	<1400595206326-6717.post@n3.nabble.com>
	<1400613400205-6726.post@n3.nabble.com>
	<CAPh_B=bbCPagwNTmmQseX9VMAXm-QmJh2uV=At=Yo_9pT=REXw@mail.gmail.com>
	<1400616030883-6728.post@n3.nabble.com>
Date: Tue, 20 May 2014 13:06:21 -0700
Message-ID: <CAAsvFPkFnQCig2OKhL3yN91TBCVR4G_+kAExmGOg77EgcATmXA@mail.gmail.com>
Subject: Re: BUG: graph.triplets does not return proper values
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e014940f0deea8004f9da6989
X-Virus-Checked: Checked by ClamAV on apache.org

--089e014940f0deea8004f9da6989
Content-Type: text/plain; charset=UTF-8

That's all very old functionality in Spark terms, so it shouldn't have
anything to do with your installation being out-of-date.  There is also no
need to cast as long as the relevant implicit conversions are in scope:
import org.apache.spark.SparkContext._


On Tue, May 20, 2014 at 1:00 PM, GlennStrycker <glenn.strycker@gmail.com>wrote:

> I don't seem to have this function in my Spark installation for this
> object,
> or the classes MappedRDD, FlatMappedRDD, EdgeRDD, VertexRDD, or Graph.
>
> Which class should have the reduceByKey function, and how do I cast my
> current RDD as this class?
>
> Perhaps this is still due to my Spark installation being out-of-date?
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/BUG-graph-triplets-does-not-return-proper-values-tp6693p6728.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--089e014940f0deea8004f9da6989--

From dev-return-7742-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 20:08:15 2014
Return-Path: <dev-return-7742-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C4CFD1134E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 20:08:15 +0000 (UTC)
Received: (qmail 52317 invoked by uid 500); 20 May 2014 20:08:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52257 invoked by uid 500); 20 May 2014 20:08:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52249 invoked by uid 99); 20 May 2014 20:08:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 20:08:15 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.220.173 as permitted sender)
Received: from [209.85.220.173] (HELO mail-vc0-f173.google.com) (209.85.220.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 20:08:11 +0000
Received: by mail-vc0-f173.google.com with SMTP id il7so1295580vcb.18
        for <dev@spark.apache.org>; Tue, 20 May 2014 13:07:50 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=EsXHBA45H/NjtCiyXdHUb8xu0oZzkV3RDZC8RvI2jWw=;
        b=kxUOWA40+ghaS5/bQoO19VnhsowVT2eTTPiS3zHAaGN/cys33Tx6XwIcYTJvJtsCuU
         17T/+hirU0SolC4l4lIQiCA4jvVsiC7jsltee3iIqvY+Oq+Pb+46uGNYzQ4ahyn0AyPY
         Sow/cT/5FM24nYJpmXQCLKitqHZVT7mVUQndh2bBPBhyxAstx1/R+VekeWwuKW9xC7F9
         43GdWZpQtVtq8IOLJU9POutiNIt7Xo/M/KTqS8G5gLsmrsnQngBsvdO+Gd61Xfb6FrLm
         cWgNmt3ebVXrcgUX4vkCwhyCurndtiNeu0I/6Hba6JJAjDuYAygAPGj0198laMqOz8b1
         M9bA==
X-Gm-Message-State: ALoCoQm7UrENVxwAYlSf3Nq05wyXH763icVBh4HtLBpn33o4jg+cyoZqpLXu3yfg5NqqX7yp3Ssk
X-Received: by 10.58.143.13 with SMTP id sa13mr2962276veb.44.1400616470413;
 Tue, 20 May 2014 13:07:50 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.111.69 with HTTP; Tue, 20 May 2014 13:07:30 -0700 (PDT)
In-Reply-To: <1400616030883-6728.post@n3.nabble.com>
References: <1400530169998-6693.post@n3.nabble.com> <CAPh_B=bwgbWJxNV0YEGt-Dr3yWTY0QDd6DZX9tcfZDQTVdermg@mail.gmail.com>
 <1400537762804-6695.post@n3.nabble.com> <CAPh_B=Yi=a-AvwYp1+jUXcwTids_KgCY3Hp+5JPQJfSb2W3O1A@mail.gmail.com>
 <1400538760695-6697.post@n3.nabble.com> <CAPh_B=ZDpYmZggfL7CTd+zekxfwHciAfjMVg1rNspcA5ynWftg@mail.gmail.com>
 <1400595206326-6717.post@n3.nabble.com> <1400613400205-6726.post@n3.nabble.com>
 <CAPh_B=bbCPagwNTmmQseX9VMAXm-QmJh2uV=At=Yo_9pT=REXw@mail.gmail.com> <1400616030883-6728.post@n3.nabble.com>
From: Sean Owen <sowen@cloudera.com>
Date: Tue, 20 May 2014 21:07:30 +0100
Message-ID: <CAMAsSdJQnQ225SW+LKVMAahXh+UpQ+UZb00xYZSFgxoG-GSnKQ@mail.gmail.com>
Subject: Re: BUG: graph.triplets does not return proper values
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

http://spark.apache.org/docs/0.9.1/api/core/index.html#org.apache.spark.rdd.PairRDDFunctions

It becomes automagically available when your RDD contains pairs.

On Tue, May 20, 2014 at 9:00 PM, GlennStrycker <glenn.strycker@gmail.com> wrote:
> I don't seem to have this function in my Spark installation for this object,
> or the classes MappedRDD, FlatMappedRDD, EdgeRDD, VertexRDD, or Graph.
>
> Which class should have the reduceByKey function, and how do I cast my
> current RDD as this class?
>
> Perhaps this is still due to my Spark installation being out-of-date?
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/BUG-graph-triplets-does-not-return-proper-values-tp6693p6728.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7743-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 20:09:41 2014
Return-Path: <dev-return-7743-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5107E11368
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 20:09:41 +0000 (UTC)
Received: (qmail 58142 invoked by uid 500); 20 May 2014 20:09:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58080 invoked by uid 500); 20 May 2014 20:09:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58072 invoked by uid 99); 20 May 2014 20:09:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 20:09:40 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of glenn.strycker@gmail.com does not designate 216.139.250.139 as permitted sender)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 20:09:38 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <glenn.strycker@gmail.com>)
	id 1WmqLB-00084o-LF
	for dev@spark.incubator.apache.org; Tue, 20 May 2014 13:08:49 -0700
Date: Tue, 20 May 2014 13:08:34 -0700 (PDT)
From: GlennStrycker <glenn.strycker@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1400616514638-6730.post@n3.nabble.com>
In-Reply-To: <1400616030883-6728.post@n3.nabble.com>
References: <1400530169998-6693.post@n3.nabble.com> <CAPh_B=bwgbWJxNV0YEGt-Dr3yWTY0QDd6DZX9tcfZDQTVdermg@mail.gmail.com> <1400537762804-6695.post@n3.nabble.com> <CAPh_B=Yi=a-AvwYp1+jUXcwTids_KgCY3Hp+5JPQJfSb2W3O1A@mail.gmail.com> <1400538760695-6697.post@n3.nabble.com> <CAPh_B=ZDpYmZggfL7CTd+zekxfwHciAfjMVg1rNspcA5ynWftg@mail.gmail.com> <1400595206326-6717.post@n3.nabble.com> <1400613400205-6726.post@n3.nabble.com> <CAPh_B=bbCPagwNTmmQseX9VMAXm-QmJh2uV=At=Yo_9pT=REXw@mail.gmail.com> <1400616030883-6728.post@n3.nabble.com>
Subject: Re: BUG: graph.triplets does not return proper values
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

For some reason it does not appear when I hit "tab" in Spark shell, but when
I put everything together in one line, it DOES WORK!

orig_graph.edges.map(_.copy()).cartesian(orig_graph.edges.map(_.copy())).flatMap(
A => Seq(if (A._1.srcId == A._2.dstId) Edge(A._2.srcId,A._1.dstId,1) else if
(A._1.dstId == A._2.srcId) Edge(A._1.srcId,A._2.dstId,1) else Edge(0,0,0) )
).map(word => (word, 1)).reduceByKey(_ + _).collect

= Array((Edge(5,7,1),4), (Edge(5,6,1),4), (Edge(3,2,1),4), (Edge(5,5,1),3),
(Edge(1,3,1),4), (Edge(2,3,1),4), (Edge(6,5,1),4), (Edge(5,4,1),2),
(Edge(2,1,1),2), (Edge(6,7,1),2), (Edge(2,2,1),2), (Edge(7,5,1),4),
(Edge(3,1,1),4), (Edge(4,5,1),2), (Edge(0,0,0),192), (Edge(3,3,1),3),
(Edge(4,7,1),2), (Edge(1,2,1),2), (Edge(4,4,1),1), (Edge(6,6,1),2),
(Edge(7,4,1),2), (Edge(7,6,1),2), (Edge(7,7,1),2), (Edge(1,1,1),3))




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/BUG-graph-triplets-does-not-return-proper-values-tp6693p6730.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7744-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 20:14:31 2014
Return-Path: <dev-return-7744-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C7B541139E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 20:14:31 +0000 (UTC)
Received: (qmail 69896 invoked by uid 500); 20 May 2014 20:14:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69835 invoked by uid 500); 20 May 2014 20:14:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69826 invoked by uid 99); 20 May 2014 20:14:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 20:14:31 +0000
X-ASF-Spam-Status: No, hits=-0.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.128.172 as permitted sender)
Received: from [209.85.128.172] (HELO mail-ve0-f172.google.com) (209.85.128.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 20:14:26 +0000
Received: by mail-ve0-f172.google.com with SMTP id oz11so1284018veb.31
        for <dev@spark.apache.org>; Tue, 20 May 2014 13:14:06 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=+9S5Qb9iZ5IVInvZyCuhQM0MFyKWFSxSsct/okSc5vw=;
        b=uJKTO02A2tGybIBj6U5xMEC+L6FM/o1fY0gfsJOjcbfzbAPkOWx2GX+2Khj9ud50Fy
         45ce2F9fEiPXP6UFg43sjLdrMqf77f0Lme3wStjxwS4ptj5E+cy+kfofDxKnHj7WC1P1
         VKPJ0Vod5hSfZSjzrGAqCo80WeBObLKnIHX8i/NFIVhTZwxrIXyk5nJ9FUCbcSYewv65
         XnnZT43vjlRR1zzyGRUgbFGI77uaWDgaxmlHh0ieSuWDSfY/Pd8gYXun0Gu/desgQK7m
         +jmmmQtjw+i1FJp/+6KirmtDdXIiSv54hOQ7/3zJ8yRH4rz82bPYzvRr1TrXcQSsyuIe
         yvWA==
X-Received: by 10.52.38.225 with SMTP id j1mr33361vdk.83.1400616846089; Tue,
 20 May 2014 13:14:06 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.169.131 with HTTP; Tue, 20 May 2014 13:13:36 -0700 (PDT)
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Tue, 20 May 2014 13:13:36 -0700
Message-ID: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>
Subject: [VOTE] Release Apache Spark 1.0.0 (RC10)
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Please vote on releasing the following candidate as Apache Spark version 1.0.0!

This has a few bug fixes on top of rc9:
SPARK-1875: https://github.com/apache/spark/pull/824
SPARK-1876: https://github.com/apache/spark/pull/819
SPARK-1878: https://github.com/apache/spark/pull/822
SPARK-1879: https://github.com/apache/spark/pull/823

The tag to be voted on is v1.0.0-rc10 (commit d8070234):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d807023479ce10aec28ef3c1ab646ddefc2e663c

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~tdas/spark-1.0.0-rc10/

The release artifacts are signed with the following key:
https://people.apache.org/keys/committer/tdas.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1018/

The documentation corresponding to this release can be found at:
http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/

The full list of changes in this release can be found at:
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=blob;f=CHANGES.txt;h=d21f0ace6326e099360975002797eb7cba9d5273;hb=d807023479ce10aec28ef3c1ab646ddefc2e663c

Please vote on releasing this package as Apache Spark 1.0.0!

The vote is open until Friday, May 23, at 20:00 UTC and passes if
amajority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.0.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

====== API Changes ======
We welcome users to compile Spark applications against 1.0. There are
a few API changes in this release. Here are links to the associated
upgrade guides - user facing changes have been kept as small as
possible.

Changes to ML vector specification:
http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/mllib-guide.html#from-09-to-10

Changes to the Java API:
http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark

Changes to the streaming API:
http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x

Changes to the GraphX API:
http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091

Other changes:
coGroup and related functions now return Iterable[T] instead of Seq[T]
==> Call toSeq on the result to restore the old behavior

SparkContext.jarOfClass returns Option[String] instead of Seq[String]
==> Call toSeq on the result to restore old behavior

From dev-return-7745-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 20 21:07:00 2014
Return-Path: <dev-return-7745-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 03D4911570
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 May 2014 21:07:00 +0000 (UTC)
Received: (qmail 56040 invoked by uid 500); 20 May 2014 21:06:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55982 invoked by uid 500); 20 May 2014 21:06:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55973 invoked by uid 99); 20 May 2014 21:06:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 21:06:59 +0000
X-ASF-Spam-Status: No, hits=3.8 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of andykonwinski@gmail.com designates 209.85.217.177 as permitted sender)
Received: from [209.85.217.177] (HELO mail-lb0-f177.google.com) (209.85.217.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 May 2014 21:06:55 +0000
Received: by mail-lb0-f177.google.com with SMTP id s7so852778lbd.22
        for <dev@spark.apache.org>; Tue, 20 May 2014 14:06:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=En9tKGnHsCW7NOYMybL8LzjwgEAazfbi0W9BMGSpCcw=;
        b=b0QuZX8kGhuNnC64blyQdvwvia49uLURqenvZqLkIYqj/fV2M3I6J5SeKWODsq0bWh
         uMmQSnah6s8C7poxanmrJ4WieKeF25m1dQ19nB2hsrApW6X1pf/yBIxzutiE9YE1qOo1
         wzyhU+WkSnKEev4vxKnGMBoHDjIIujKkMqeAS4VQNClbN8COtmQtBIz0ptwPYlIInms/
         P7u3qn84pJU39OU8PeCZMVcOAYE8EEOpMloy53Xjn0anT9I23C35GIpjbvNknZgPZk4d
         Wy3I6nqyKTFN/sO3qkuiV/N3M2DhvDLkYGahUdD0dYRKZlzbkY3jZtANY3FyNdomTVdn
         fM6Q==
MIME-Version: 1.0
X-Received: by 10.152.19.195 with SMTP id h3mr14475738lae.47.1400619993567;
 Tue, 20 May 2014 14:06:33 -0700 (PDT)
Received: by 10.112.147.4 with HTTP; Tue, 20 May 2014 14:06:33 -0700 (PDT)
In-Reply-To: <CAAsvFP=8e079xR0JrHEQ2tM56t-Ag2Lt8JqthUum0veMcEOd7g@mail.gmail.com>
References: <1400258494709-6593.post@n3.nabble.com>
	<CAAsvFPk36+vXkZijCdZGxJHRSJcvPon3BopPNacNxXvDB=FF0Q@mail.gmail.com>
	<CAAsvFP=8e079xR0JrHEQ2tM56t-Ag2Lt8JqthUum0veMcEOd7g@mail.gmail.com>
Date: Tue, 20 May 2014 14:06:33 -0700
Message-ID: <CALEZFQwLNKsTZa9GYpcrBVSjbH45gjp+Pn0NZVANONP4Sor+aQ@mail.gmail.com>
Subject: Re: Scala examples for Spark do not work as written in documentation
From: Andy Konwinski <andykonwinski@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e013d187e2e965504f9db41ba
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013d187e2e965504f9db41ba
Content-Type: text/plain; charset=UTF-8

I fixed the bug, but I kept the parameter "i" instead of "_" since that (1)
keeps it more parallel to the python and java versions which also use
functions with a named variable and (2) doesn't require readers to know
this particular use of the "_" syntax in Scala.

Thanks for catching this Glenn.

Andy


On Fri, May 16, 2014 at 12:38 PM, Mark Hamstra <mark@clearstorydata.com>wrote:

> Sorry, looks like an extra line got inserted in there.  One more try:
>
> val count = spark.parallelize(1 to NUM_SAMPLES).map { _ =>
>   val x = Math.random()
>   val y = Math.random()
>   if (x*x + y*y < 1) 1 else 0
> }.reduce(_ + _)
>
>
>
> On Fri, May 16, 2014 at 12:36 PM, Mark Hamstra <mark@clearstorydata.com
> >wrote:
>
> > Actually, the better way to write the multi-line closure would be:
> >
> > val count = spark.parallelize(1 to NUM_SAMPLES).map { _ =>
> >
> >   val x = Math.random()
> >   val y = Math.random()
> >   if (x*x + y*y < 1) 1 else 0
> > }.reduce(_ + _)
> >
> >
> > On Fri, May 16, 2014 at 9:41 AM, GlennStrycker <glenn.strycker@gmail.com
> >wrote:
> >
> >> On the webpage http://spark.apache.org/examples.html, there is an
> example
> >> written as
> >>
> >> val count = spark.parallelize(1 to NUM_SAMPLES).map(i =>
> >>   val x = Math.random()
> >>   val y = Math.random()
> >>   if (x*x + y*y < 1) 1 else 0
> >> ).reduce(_ + _)
> >> println("Pi is roughly " + 4.0 * count / NUM_SAMPLES)
> >>
> >> This does not execute in Spark, which gives me an error:
> >> <console>:2: error: illegal start of simple expression
> >>          val x = Math.random()
> >>          ^
> >>
> >> If I rewrite the query slightly, adding in {}, it works:
> >>
> >> val count = spark.parallelize(1 to 10000).map(i =>
> >>    {
> >>    val x = Math.random()
> >>    val y = Math.random()
> >>    if (x*x + y*y < 1) 1 else 0
> >>    }
> >> ).reduce(_ + _)
> >> println("Pi is roughly " + 4.0 * count / 10000.0)
> >>
> >>
> >>
> >>
> >>
> >> --
> >> View this message in context:
> >>
> http://apache-spark-developers-list.1001551.n3.nabble.com/Scala-examples-for-Spark-do-not-work-as-written-in-documentation-tp6593.html
> >> Sent from the Apache Spark Developers List mailing list archive at
> >> Nabble.com.
> >>
> >
> >
>

--089e013d187e2e965504f9db41ba--

From dev-return-7746-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 00:26:39 2014
Return-Path: <dev-return-7746-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1C82611B34
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 00:26:39 +0000 (UTC)
Received: (qmail 51264 invoked by uid 500); 21 May 2014 00:26:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51206 invoked by uid 500); 21 May 2014 00:26:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51198 invoked by uid 99); 21 May 2014 00:26:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 00:26:38 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.212.171] (HELO mail-wi0-f171.google.com) (209.85.212.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 00:26:35 +0000
Received: by mail-wi0-f171.google.com with SMTP id cc10so4168748wib.16
        for <dev@spark.apache.org>; Tue, 20 May 2014 17:26:11 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=C3Vxb5ELwyr/U/ldz9OZj4CwScBsgDUJfYQOH8E45C8=;
        b=FIg7bz20+Z8VtdUk5CYY/K2WLOhmoDQfU5L7BHkiqiXZ4QbtDmZuwwX8EredDpb1ED
         qhbqvfhel5kKek+rA+XZ50SWyL58a2shyisBOOipxoxXE7nFUotlGdB5Nfm46LKLcbMb
         9HybrHavzi8BqLlVuBvDXZtZrbD0/gvKpI8nkcD1WcIr8gmbk88Yx/a0sHdz3X0oTAGe
         jwJr6NfNrsMO5bk9a4U09DdCrd6C2Vr9/XubJl7UjlTSiuoun6Z2sxjcTuwu2kFJhkkS
         NWr72gtGb/PiPPnrAz7YlqGgSVf75ojAr1TTF7JxPg8hTzbTNYnjbhbt0HBOxA5+uRIv
         Ea/g==
X-Gm-Message-State: ALoCoQnIPIxtQ+8y1eIld8jnkJs8NunMr8yazH+YnwqS7FAYPPXzLWObZHHaBB7yraoeZnlXPZir
MIME-Version: 1.0
X-Received: by 10.194.92.177 with SMTP id cn17mr40281516wjb.18.1400631971741;
 Tue, 20 May 2014 17:26:11 -0700 (PDT)
Received: by 10.180.88.97 with HTTP; Tue, 20 May 2014 17:26:11 -0700 (PDT)
In-Reply-To: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>
References: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>
Date: Tue, 20 May 2014 17:26:11 -0700
Message-ID: <CAMJOb8mjEt=u3qKhRGHONYD5Y9H+D0T8RYAnNOFA171tui4-rw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC10)
From: Andrew Or <andrew@databricks.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7beb91aa23125d04f9de0bba
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7beb91aa23125d04f9de0bba
Content-Type: text/plain; charset=UTF-8

+1


2014-05-20 13:13 GMT-07:00 Tathagata Das <tathagata.das1565@gmail.com>:

> Please vote on releasing the following candidate as Apache Spark version
> 1.0.0!
>
> This has a few bug fixes on top of rc9:
> SPARK-1875: https://github.com/apache/spark/pull/824
> SPARK-1876: https://github.com/apache/spark/pull/819
> SPARK-1878: https://github.com/apache/spark/pull/822
> SPARK-1879: https://github.com/apache/spark/pull/823
>
> The tag to be voted on is v1.0.0-rc10 (commit d8070234):
>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d807023479ce10aec28ef3c1ab646ddefc2e663c
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~tdas/spark-1.0.0-rc10/
>
> The release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/tdas.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1018/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>
> The full list of changes in this release can be found at:
>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=blob;f=CHANGES.txt;h=d21f0ace6326e099360975002797eb7cba9d5273;hb=d807023479ce10aec28ef3c1ab646ddefc2e663c
>
> Please vote on releasing this package as Apache Spark 1.0.0!
>
> The vote is open until Friday, May 23, at 20:00 UTC and passes if
> amajority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.0.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> ====== API Changes ======
> We welcome users to compile Spark applications against 1.0. There are
> a few API changes in this release. Here are links to the associated
> upgrade guides - user facing changes have been kept as small as
> possible.
>
> Changes to ML vector specification:
>
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/mllib-guide.html#from-09-to-10
>
> Changes to the Java API:
>
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>
> Changes to the streaming API:
>
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>
> Changes to the GraphX API:
>
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>
> Other changes:
> coGroup and related functions now return Iterable[T] instead of Seq[T]
> ==> Call toSeq on the result to restore the old behavior
>
> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> ==> Call toSeq on the result to restore old behavior
>

--047d7beb91aa23125d04f9de0bba--

From dev-return-7747-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 00:28:35 2014
Return-Path: <dev-return-7747-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D3C8B11B3C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 00:28:35 +0000 (UTC)
Received: (qmail 53124 invoked by uid 500); 21 May 2014 00:28:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53056 invoked by uid 500); 21 May 2014 00:28:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53048 invoked by uid 99); 21 May 2014 00:28:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 00:28:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.176 as permitted sender)
Received: from [209.85.216.176] (HELO mail-qc0-f176.google.com) (209.85.216.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 00:28:31 +0000
Received: by mail-qc0-f176.google.com with SMTP id r5so2023008qcx.7
        for <dev@spark.apache.org>; Tue, 20 May 2014 17:28:10 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=kVHvW2s56cOOC0OjuT4SYapApKPkquxdY1yOJUFplvE=;
        b=Nc+9NzQ8XYCGNM0G//XoOx2Yo6y9pnOZ/MoCkkeB/fSJ6dqnw/fWikMHQmo40PaK3u
         +NpHTzziq+B5KdrQNICLywGuCK2WJ+GZAh8R4loHFhOJscJjrJsceu4+U4NyupAboTdy
         /qo43ukmvZt2poffWZHgU7RFFlKkX/HfHqJuNHuem4NjsEp1KpOY0B5wYftgSGvXl9Ca
         e5vjmiEcH7ennRkXVqp+jpeIq5gQF5W9oPiEb3lbehbXB5XXgTRfHloYqnqhrUji6JqS
         qT+5j0A3S8GxSOZp4X3LLd2Dv3dDAJ7rfKx/1Pg+6hAEO5hmEb2UClnWqz3Vj+jUDqBQ
         JuDg==
X-Gm-Message-State: ALoCoQniGK4InrwTZ49kC8DEoDBfOeb4tndGjJaBTfjgKFiwn+x/MLaA+DLCiO4MAZacqZ4yYTDK
MIME-Version: 1.0
X-Received: by 10.224.49.67 with SMTP id u3mr63474251qaf.63.1400632090563;
 Tue, 20 May 2014 17:28:10 -0700 (PDT)
Received: by 10.140.106.202 with HTTP; Tue, 20 May 2014 17:28:10 -0700 (PDT)
In-Reply-To: <CAMJOb8mjEt=u3qKhRGHONYD5Y9H+D0T8RYAnNOFA171tui4-rw@mail.gmail.com>
References: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>
	<CAMJOb8mjEt=u3qKhRGHONYD5Y9H+D0T8RYAnNOFA171tui4-rw@mail.gmail.com>
Date: Tue, 20 May 2014 17:28:10 -0700
Message-ID: <CACBYxKJicFFuL9PX_=zsMxZJUGd-EpyejGvaCufmKLZ53eew1Q@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC10)
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2ef8038271f04f9de1236
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ef8038271f04f9de1236
Content-Type: text/plain; charset=UTF-8

+1


On Tue, May 20, 2014 at 5:26 PM, Andrew Or <andrew@databricks.com> wrote:

> +1
>
>
> 2014-05-20 13:13 GMT-07:00 Tathagata Das <tathagata.das1565@gmail.com>:
>
> > Please vote on releasing the following candidate as Apache Spark version
> > 1.0.0!
> >
> > This has a few bug fixes on top of rc9:
> > SPARK-1875: https://github.com/apache/spark/pull/824
> > SPARK-1876: https://github.com/apache/spark/pull/819
> > SPARK-1878: https://github.com/apache/spark/pull/822
> > SPARK-1879: https://github.com/apache/spark/pull/823
> >
> > The tag to be voted on is v1.0.0-rc10 (commit d8070234):
> >
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d807023479ce10aec28ef3c1ab646ddefc2e663c
> >
> > The release files, including signatures, digests, etc. can be found at:
> > http://people.apache.org/~tdas/spark-1.0.0-rc10/
> >
> > The release artifacts are signed with the following key:
> > https://people.apache.org/keys/committer/tdas.asc
> >
> > The staging repository for this release can be found at:
> > https://repository.apache.org/content/repositories/orgapachespark-1018/
> >
> > The documentation corresponding to this release can be found at:
> > http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
> >
> > The full list of changes in this release can be found at:
> >
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=blob;f=CHANGES.txt;h=d21f0ace6326e099360975002797eb7cba9d5273;hb=d807023479ce10aec28ef3c1ab646ddefc2e663c
> >
> > Please vote on releasing this package as Apache Spark 1.0.0!
> >
> > The vote is open until Friday, May 23, at 20:00 UTC and passes if
> > amajority of at least 3 +1 PMC votes are cast.
> >
> > [ ] +1 Release this package as Apache Spark 1.0.0
> > [ ] -1 Do not release this package because ...
> >
> > To learn more about Apache Spark, please see
> > http://spark.apache.org/
> >
> > ====== API Changes ======
> > We welcome users to compile Spark applications against 1.0. There are
> > a few API changes in this release. Here are links to the associated
> > upgrade guides - user facing changes have been kept as small as
> > possible.
> >
> > Changes to ML vector specification:
> >
> >
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/mllib-guide.html#from-09-to-10
> >
> > Changes to the Java API:
> >
> >
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
> >
> > Changes to the streaming API:
> >
> >
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
> >
> > Changes to the GraphX API:
> >
> >
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
> >
> > Other changes:
> > coGroup and related functions now return Iterable[T] instead of Seq[T]
> > ==> Call toSeq on the result to restore the old behavior
> >
> > SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> > ==> Call toSeq on the result to restore old behavior
> >
>

--001a11c2ef8038271f04f9de1236--

From dev-return-7748-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 00:28:56 2014
Return-Path: <dev-return-7748-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DFB0D11B49
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 00:28:55 +0000 (UTC)
Received: (qmail 54873 invoked by uid 500); 21 May 2014 00:28:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54810 invoked by uid 500); 21 May 2014 00:28:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54800 invoked by uid 99); 21 May 2014 00:28:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 00:28:55 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of vanzin@cloudera.com designates 209.85.192.52 as permitted sender)
Received: from [209.85.192.52] (HELO mail-qg0-f52.google.com) (209.85.192.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 00:28:51 +0000
Received: by mail-qg0-f52.google.com with SMTP id a108so2016036qge.25
        for <dev@spark.apache.org>; Tue, 20 May 2014 17:28:30 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=u010yzsL+3LYkgwMFeTpQcfh0VarZ1vGe0lQ6SLpFOI=;
        b=FFMKXkRSSKjIj4JahBSc3ousrdP0HwPRUjt/esJYd0zg2Gz3LiaN68qLdfwskOI+sK
         i2Jn0cnjWlhhcy+jsQD/DZV5WXI52Fn00/YwtcyAhKeWD76TQ/d7yhPYI7mcqvUnJ6u7
         UqnD3wGTMkcxBrj61Q81zceEBmRePsbe2JbzZhRSP/bp7ynoAV+6i4zjrMay4IYpmiLA
         jiRKaYkHR2FLWE0Ew4wC9iw6Wb1Zv1DsrHaDG/mJbPFOLS8iPIQOkQn4lpMg6ISOCgF+
         a+2p+030AYFVpZ6djq3OTs7BWITk4sOF5zjKcngnPVpIT96uLntIQQlWUAyjNBHQAyu1
         v0oQ==
X-Gm-Message-State: ALoCoQldkO1ynh+0Y5bPna6TwX5DWtjiccj2jW3SNKWGxEDo5jvnS8U3A0DabPUWJLqRs4newRYG
MIME-Version: 1.0
X-Received: by 10.140.94.39 with SMTP id f36mr61541960qge.64.1400632110619;
 Tue, 20 May 2014 17:28:30 -0700 (PDT)
Received: by 10.229.55.6 with HTTP; Tue, 20 May 2014 17:28:30 -0700 (PDT)
In-Reply-To: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>
References: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>
Date: Tue, 20 May 2014 17:28:30 -0700
Message-ID: <CAAOnQ7vqt=jhCMAz7p2z1bTy3R-du2uzEDAGLpK_q7WNuPafhg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC10)
From: Marcelo Vanzin <vanzin@cloudera.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

+1 (non-binding)

I have:
- checked signatures and checksums of the files
- built the code from the git repo using both sbt and mvn (against hadoop 2.3.0)
- ran a few simple jobs in local, yarn-client and yarn-cluster mode

Haven't explicitly tested any of the recent fixes, streaming nor sql.


On Tue, May 20, 2014 at 1:13 PM, Tathagata Das
<tathagata.das1565@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.0.0!
>
> This has a few bug fixes on top of rc9:
> SPARK-1875: https://github.com/apache/spark/pull/824
> SPARK-1876: https://github.com/apache/spark/pull/819
> SPARK-1878: https://github.com/apache/spark/pull/822
> SPARK-1879: https://github.com/apache/spark/pull/823
>
> The tag to be voted on is v1.0.0-rc10 (commit d8070234):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d807023479ce10aec28ef3c1ab646ddefc2e663c
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~tdas/spark-1.0.0-rc10/
>
> The release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/tdas.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1018/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>
> The full list of changes in this release can be found at:
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=blob;f=CHANGES.txt;h=d21f0ace6326e099360975002797eb7cba9d5273;hb=d807023479ce10aec28ef3c1ab646ddefc2e663c
>
> Please vote on releasing this package as Apache Spark 1.0.0!
>
> The vote is open until Friday, May 23, at 20:00 UTC and passes if
> amajority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.0.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> ====== API Changes ======
> We welcome users to compile Spark applications against 1.0. There are
> a few API changes in this release. Here are links to the associated
> upgrade guides - user facing changes have been kept as small as
> possible.
>
> Changes to ML vector specification:
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/mllib-guide.html#from-09-to-10
>
> Changes to the Java API:
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>
> Changes to the streaming API:
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>
> Changes to the GraphX API:
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>
> Other changes:
> coGroup and related functions now return Iterable[T] instead of Seq[T]
> ==> Call toSeq on the result to restore the old behavior
>
> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> ==> Call toSeq on the result to restore old behavior



-- 
Marcelo

From dev-return-7749-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 00:59:42 2014
Return-Path: <dev-return-7749-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BBC3E11BEF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 00:59:42 +0000 (UTC)
Received: (qmail 88557 invoked by uid 500); 21 May 2014 00:59:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88484 invoked by uid 500); 21 May 2014 00:59:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88476 invoked by uid 99); 21 May 2014 00:59:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 00:59:42 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.212.169 as permitted sender)
Received: from [209.85.212.169] (HELO mail-wi0-f169.google.com) (209.85.212.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 00:59:39 +0000
Received: by mail-wi0-f169.google.com with SMTP id hi2so7689319wib.2
        for <dev@spark.apache.org>; Tue, 20 May 2014 17:59:16 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=M2Q4z+/6KlpzHaPVmMWYlNDabpwgAIETksCXQIobjLY=;
        b=cPxSr3qLEWHwUNU+nrdN6kFUXysaW90zY8AWfU8nO6UsXG1OGtgX3Y3FjyjIsvNNJq
         gj0TpgxaXZfkjUNyaHvH1hND67sSW74/AD9Gr8wY3HusurMtrePJpsYd1O+6gDdy36ok
         9OvNmFJQeU3JkRrzh0v0VKpM9z3MkuhVDcYELGCW8CTRhllWi06AZVq+fI+YR6TMAbSS
         v1HJrZfj9pPUwV0cttCxgMkBhTERRaXArwyXOcLqc5TD6mqV0u/RamwCxhLCAXlQZ0fT
         f/jkjzOLHAmDFLVvGhDKhiUyHAko/rSZ0zRtAfXX4NXkSP0b0BnIbOJUW0Wuc0RA5FwD
         tZpA==
MIME-Version: 1.0
X-Received: by 10.194.85.225 with SMTP id k1mr32715689wjz.49.1400633956609;
 Tue, 20 May 2014 17:59:16 -0700 (PDT)
Received: by 10.194.187.20 with HTTP; Tue, 20 May 2014 17:59:16 -0700 (PDT)
In-Reply-To: <CAEYYnxaYfaUOes+A4kR4w+nh0LS8fgJytM2Nj7xoBErpHiRwbA@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
	<CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
	<CAMAsSdKxE-biW8fRqkRn6NmqsszFgQ=f1w_Z8yt8cSOj3qBB2Q@mail.gmail.com>
	<CAEYYnxY9UZDuhBoHAP5kD3dCXxd0TwR761tt-kswjTEddQCCKg@mail.gmail.com>
	<CA+-p3AGT8E5caKU+2ExxjbpV-UqaaTot+Ni3DXEdOf37tADLgw@mail.gmail.com>
	<CAMAsSdJVckDE5=3S8wZj9P1-1-D6qBumGnofk9G7A7cksh1RVA@mail.gmail.com>
	<CABPQxstfSyVu6D-FSt7qa-o1co9AkhHsncTHGBrCK4rEM7fymg@mail.gmail.com>
	<CACBYxKLb=OK2EKu=2yS_1tswYz52rBF-59+8DZL+DfG6qHF52Q@mail.gmail.com>
	<CAEYYnxakT5SVD0xxWcyAXq+w-f-tY0rDA2c3YLRXK6oZ3ArF7Q@mail.gmail.com>
	<CAEYYnxaYfaUOes+A4kR4w+nh0LS8fgJytM2Nj7xoBErpHiRwbA@mail.gmail.com>
Date: Tue, 20 May 2014 17:59:16 -0700
Message-ID: <CAJgQjQ_ifNtFFK4W99FA3cJ3BapRRfLc4FomCWxdgwTqEU+icg@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Talked with Sandy and DB offline. I think the best solution is sending
the secondary jars to the distributed cache of all containers rather
than just the master, and set the classpath to include spark jar,
primary app jar, and secondary jars before executor starts. In this
way, user only needs to specify secondary jars via --jars instead of
calling sc.addJar inside the code. It also solves the scalability
problem of serving all the jars via http.

If this solution sounds good, I can try to make a patch.

Best,
Xiangrui

On Mon, May 19, 2014 at 10:04 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> In 1.0, there is a new option for users to choose which classloader has
> higher priority via spark.files.userClassPathFirst, I decided to submit the
> PR for 0.9 first. We use this patch in our lab and we can use those jars
> added by sc.addJar without reflection.
>
> https://github.com/apache/spark/pull/834
>
> Can anyone comment if it's a good approach?
>
> Thanks.
>
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Mon, May 19, 2014 at 7:42 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>
>> Good summary! We fixed it in branch 0.9 since our production is still in
>> 0.9. I'm porting it to 1.0 now, and hopefully will submit PR for 1.0
>> tonight.
>>
>>
>> Sincerely,
>>
>> DB Tsai
>> -------------------------------------------------------
>> My Blog: https://www.dbtsai.com
>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>
>>
>> On Mon, May 19, 2014 at 7:38 PM, Sandy Ryza <sandy.ryza@cloudera.com>wrote:
>>
>>> It just hit me why this problem is showing up on YARN and not on
>>> standalone.
>>>
>>> The relevant difference between YARN and standalone is that, on YARN, the
>>> app jar is loaded by the system classloader instead of Spark's custom URL
>>> classloader.
>>>
>>> On YARN, the system classloader knows about [the classes in the spark
>>> jars,
>>> the classes in the primary app jar].   The custom classloader knows about
>>> [the classes in secondary app jars] and has the system classloader as its
>>> parent.
>>>
>>> A few relevant facts (mostly redundant with what Sean pointed out):
>>> * Every class has a classloader that loaded it.
>>> * When an object of class B is instantiated inside of class A, the
>>> classloader used for loading B is the classloader that was used for
>>> loading
>>> A.
>>> * When a classloader fails to load a class, it lets its parent classloader
>>> try.  If its parent succeeds, its parent becomes the "classloader that
>>> loaded it".
>>>
>>> So suppose class B is in a secondary app jar and class A is in the primary
>>> app jar:
>>> 1. The custom classloader will try to load class A.
>>> 2. It will fail, because it only knows about the secondary jars.
>>> 3. It will delegate to its parent, the system classloader.
>>> 4. The system classloader will succeed, because it knows about the primary
>>> app jar.
>>> 5. A's classloader will be the system classloader.
>>> 6. A tries to instantiate an instance of class B.
>>> 7. B will be loaded with A's classloader, which is the system classloader.
>>> 8. Loading B will fail, because A's classloader, which is the system
>>> classloader, doesn't know about the secondary app jars.
>>>
>>> In Spark standalone, A and B are both loaded by the custom classloader, so
>>> this issue doesn't come up.
>>>
>>> -Sandy
>>>
>>> On Mon, May 19, 2014 at 7:07 PM, Patrick Wendell <pwendell@gmail.com>
>>> wrote:
>>>
>>> > Having a user add define a custom class inside of an added jar and
>>> > instantiate it directly inside of an executor is definitely supported
>>> > in Spark and has been for a really long time (several years). This is
>>> > something we do all the time in Spark.
>>> >
>>> > DB - I'd hold off on a re-architecting of this until we identify
>>> > exactly what is causing the bug you are running into.
>>> >
>>> > In a nutshell, when the bytecode "new Foo()" is run on the executor,
>>> > it will ask the driver for the class over HTTP using a custom
>>> > classloader. Something in that pipeline is breaking here, possibly
>>> > related to the YARN deployment stuff.
>>> >
>>> >
>>> > On Mon, May 19, 2014 at 12:29 AM, Sean Owen <sowen@cloudera.com> wrote:
>>> > > I don't think a customer classloader is necessary.
>>> > >
>>> > > Well, it occurs to me that this is no new problem. Hadoop, Tomcat, etc
>>> > > all run custom user code that creates new user objects without
>>> > > reflection. I should go see how that's done. Maybe it's totally valid
>>> > > to set the thread's context classloader for just this purpose, and I
>>> > > am not thinking clearly.
>>> > >
>>> > > On Mon, May 19, 2014 at 8:26 AM, Andrew Ash <andrew@andrewash.com>
>>> > wrote:
>>> > >> Sounds like the problem is that classloaders always look in their
>>> > parents
>>> > >> before themselves, and Spark users want executors to pick up classes
>>> > from
>>> > >> their custom code before the ones in Spark plus its dependencies.
>>> > >>
>>> > >> Would a custom classloader that delegates to the parent after first
>>> > >> checking itself fix this up?
>>> > >>
>>> > >>
>>> > >> On Mon, May 19, 2014 at 12:17 AM, DB Tsai <dbtsai@stanford.edu>
>>> wrote:
>>> > >>
>>> > >>> Hi Sean,
>>> > >>>
>>> > >>> It's true that the issue here is classloader, and due to the
>>> > classloader
>>> > >>> delegation model, users have to use reflection in the executors to
>>> > pick up
>>> > >>> the classloader in order to use those classes added by sc.addJars
>>> APIs.
>>> > >>> However, it's very inconvenience for users, and not documented in
>>> > spark.
>>> > >>>
>>> > >>> I'm working on a patch to solve it by calling the protected method
>>> > addURL
>>> > >>> in URLClassLoader to update the current default classloader, so no
>>> > >>> customClassLoader anymore. I wonder if this is an good way to go.
>>> > >>>
>>> > >>>   private def addURL(url: URL, loader: URLClassLoader){
>>> > >>>     try {
>>> > >>>       val method: Method =
>>> > >>> classOf[URLClassLoader].getDeclaredMethod("addURL", classOf[URL])
>>> > >>>       method.setAccessible(true)
>>> > >>>       method.invoke(loader, url)
>>> > >>>     }
>>> > >>>     catch {
>>> > >>>       case t: Throwable => {
>>> > >>>         throw new IOException("Error, could not add URL to system
>>> > >>> classloader")
>>> > >>>       }
>>> > >>>     }
>>> > >>>   }
>>> > >>>
>>> > >>>
>>> > >>>
>>> > >>> Sincerely,
>>> > >>>
>>> > >>> DB Tsai
>>> > >>> -------------------------------------------------------
>>> > >>> My Blog: https://www.dbtsai.com
>>> > >>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>> > >>>
>>> > >>>
>>> > >>> On Sun, May 18, 2014 at 11:57 PM, Sean Owen <sowen@cloudera.com>
>>> > wrote:
>>> > >>>
>>> > >>> > I might be stating the obvious for everyone, but the issue here is
>>> > not
>>> > >>> > reflection or the source of the JAR, but the ClassLoader. The
>>> basic
>>> > >>> > rules are this.
>>> > >>> >
>>> > >>> > "new Foo" will use the ClassLoader that defines Foo. This is
>>> usually
>>> > >>> > the ClassLoader that loaded whatever it is that first referenced
>>> Foo
>>> > >>> > and caused it to be loaded -- usually the ClassLoader holding your
>>> > >>> > other app classes.
>>> > >>> >
>>> > >>> > ClassLoaders can have a parent-child relationship. ClassLoaders
>>> > always
>>> > >>> > look in their parent before themselves.
>>> > >>> >
>>> > >>> > (Careful then -- in contexts like Hadoop or Tomcat where your app
>>> is
>>> > >>> > loaded in a child ClassLoader, and you reference a class that
>>> Hadoop
>>> > >>> > or Tomcat also has (like a lib class) you will get the container's
>>> > >>> > version!)
>>> > >>> >
>>> > >>> > When you load an external JAR it has a separate ClassLoader which
>>> > does
>>> > >>> > not necessarily bear any relation to the one containing your app
>>> > >>> > classes, so yeah it is not generally going to make "new Foo" work.
>>> > >>> >
>>> > >>> > Reflection lets you pick the ClassLoader, yes.
>>> > >>> >
>>> > >>> > I would not call setContextClassLoader.
>>> > >>> >
>>> > >>> > On Mon, May 19, 2014 at 12:00 AM, Sandy Ryza <
>>> > sandy.ryza@cloudera.com>
>>> > >>> > wrote:
>>> > >>> > > I spoke with DB offline about this a little while ago and he
>>> > confirmed
>>> > >>> > that
>>> > >>> > > he was able to access the jar from the driver.
>>> > >>> > >
>>> > >>> > > The issue appears to be a general Java issue: you can't directly
>>> > >>> > > instantiate a class from a dynamically loaded jar.
>>> > >>> > >
>>> > >>> > > I reproduced it locally outside of Spark with:
>>> > >>> > > ---
>>> > >>> > >     URLClassLoader urlClassLoader = new URLClassLoader(new
>>> URL[] {
>>> > new
>>> > >>> > > File("myotherjar.jar").toURI().toURL() }, null);
>>> > >>> > >
>>> Thread.currentThread().setContextClassLoader(urlClassLoader);
>>> > >>> > >     MyClassFromMyOtherJar obj = new MyClassFromMyOtherJar();
>>> > >>> > > ---
>>> > >>> > >
>>> > >>> > > I was able to load the class with reflection.
>>> > >>> >
>>> > >>>
>>> >
>>>
>>
>>

From dev-return-7750-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 02:39:37 2014
Return-Path: <dev-return-7750-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 70FA111EED
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 02:39:37 +0000 (UTC)
Received: (qmail 30600 invoked by uid 500); 21 May 2014 02:39:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30541 invoked by uid 500); 21 May 2014 02:39:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30533 invoked by uid 99); 21 May 2014 02:39:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 02:39:37 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.42 as permitted sender)
Received: from [209.85.160.42] (HELO mail-pb0-f42.google.com) (209.85.160.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 02:39:32 +0000
Received: by mail-pb0-f42.google.com with SMTP id md12so922027pbc.15
        for <dev@spark.apache.org>; Tue, 20 May 2014 19:39:12 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=e1Qf8o1+IsLgqn+SEBHp349Mdj3uNBVr1RC3f//V7AQ=;
        b=G0VJaWvDMI80J3mi+bhOsZYPoks+C0eHpzXa592QtxlpFeZxZCmRiLA++NuxHgt6ni
         JiBaWKK1f/xStGdio36S3tAxmS0rKrLi5O72uWK+RzIZHI9hTTkgp7pMLoEMKNnY3B1J
         c2mbwjqepl+BtuEihdu6nkXmbY4avLsUEetFTgg/7u8QCLgoP1oH+VztGX+ukaI9eWAd
         RHhACpv5vHYXR2OMa8B5nubkf8deRm1iZZ/XNhhsQonSwGtWx11x6fqADGd/Gj44g9Dq
         Z+634aYUW3Ufbq8ISLQW/97BFU1ByBjXDxKLQmdcNAdOvmeKMb31wpS8nZVoG4I8gWMn
         E6NA==
X-Received: by 10.66.121.197 with SMTP id lm5mr8338303pab.118.1400639952420;
        Tue, 20 May 2014 19:39:12 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id ek2sm5405131pbd.30.2014.05.20.19.39.09
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 20 May 2014 19:39:10 -0700 (PDT)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC10)
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAAOnQ7vqt=jhCMAz7p2z1bTy3R-du2uzEDAGLpK_q7WNuPafhg@mail.gmail.com>
Date: Tue, 20 May 2014 19:39:08 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <BBA2B589-2DA3-4C28-8357-70054496BEAE@gmail.com>
References: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com> <CAAOnQ7vqt=jhCMAz7p2z1bTy3R-du2uzEDAGLpK_q7WNuPafhg@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

+1

Tested it on both Windows and Mac OS X, with both Scala and Python. =
Confirmed that the issues in the previous RC were fixed.

Matei

On May 20, 2014, at 5:28 PM, Marcelo Vanzin <vanzin@cloudera.com> wrote:

> +1 (non-binding)
>=20
> I have:
> - checked signatures and checksums of the files
> - built the code from the git repo using both sbt and mvn (against =
hadoop 2.3.0)
> - ran a few simple jobs in local, yarn-client and yarn-cluster mode
>=20
> Haven't explicitly tested any of the recent fixes, streaming nor sql.
>=20
>=20
> On Tue, May 20, 2014 at 1:13 PM, Tathagata Das
> <tathagata.das1565@gmail.com> wrote:
>> Please vote on releasing the following candidate as Apache Spark =
version 1.0.0!
>>=20
>> This has a few bug fixes on top of rc9:
>> SPARK-1875: https://github.com/apache/spark/pull/824
>> SPARK-1876: https://github.com/apache/spark/pull/819
>> SPARK-1878: https://github.com/apache/spark/pull/822
>> SPARK-1879: https://github.com/apache/spark/pull/823
>>=20
>> The tag to be voted on is v1.0.0-rc10 (commit d8070234):
>> =
https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3Dd8070=
23479ce10aec28ef3c1ab646ddefc2e663c
>>=20
>> The release files, including signatures, digests, etc. can be found =
at:
>> http://people.apache.org/~tdas/spark-1.0.0-rc10/
>>=20
>> The release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/tdas.asc
>>=20
>> The staging repository for this release can be found at:
>> =
https://repository.apache.org/content/repositories/orgapachespark-1018/
>>=20
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>=20
>> The full list of changes in this release can be found at:
>> =
https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dblob;f=3DCHANGES=
.txt;h=3Dd21f0ace6326e099360975002797eb7cba9d5273;hb=3Dd807023479ce10aec28=
ef3c1ab646ddefc2e663c
>>=20
>> Please vote on releasing this package as Apache Spark 1.0.0!
>>=20
>> The vote is open until Friday, May 23, at 20:00 UTC and passes if
>> amajority of at least 3 +1 PMC votes are cast.
>>=20
>> [ ] +1 Release this package as Apache Spark 1.0.0
>> [ ] -1 Do not release this package because ...
>>=20
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>>=20
>> =3D=3D=3D=3D=3D=3D API Changes =3D=3D=3D=3D=3D=3D
>> We welcome users to compile Spark applications against 1.0. There are
>> a few API changes in this release. Here are links to the associated
>> upgrade guides - user facing changes have been kept as small as
>> possible.
>>=20
>> Changes to ML vector specification:
>> =
http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/mllib-guide.html#from=
-09-to-10
>>=20
>> Changes to the Java API:
>> =
http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/java-programming-guid=
e.html#upgrading-from-pre-10-versions-of-spark
>>=20
>> Changes to the streaming API:
>> =
http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/streaming-programming=
-guide.html#migration-guide-from-091-or-below-to-1x
>>=20
>> Changes to the GraphX API:
>> =
http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/graphx-programming-gu=
ide.html#upgrade-guide-from-spark-091
>>=20
>> Other changes:
>> coGroup and related functions now return Iterable[T] instead of =
Seq[T]
>> =3D=3D> Call toSeq on the result to restore the old behavior
>>=20
>> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
>> =3D=3D> Call toSeq on the result to restore old behavior
>=20
>=20
>=20
> --=20
> Marcelo


From dev-return-7751-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 06:10:09 2014
Return-Path: <dev-return-7751-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7D62B11624
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 06:10:09 +0000 (UTC)
Received: (qmail 59332 invoked by uid 500); 21 May 2014 06:10:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59271 invoked by uid 500); 21 May 2014 06:10:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59263 invoked by uid 99); 21 May 2014 06:10:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 06:10:09 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of henry.saputra@gmail.com designates 74.125.82.42 as permitted sender)
Received: from [74.125.82.42] (HELO mail-wg0-f42.google.com) (74.125.82.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 06:10:06 +0000
Received: by mail-wg0-f42.google.com with SMTP id y10so1482103wgg.25
        for <dev@spark.apache.org>; Tue, 20 May 2014 23:09:43 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=utBY8N2rMwHklnZHXf/jpIZLhJqaJMyIelrjcoT83LI=;
        b=tNlmZKp3+kjIPtoo6XUneMsHVRF3CVQwjRpusihzyvGMVq/NWhM2NDYy83sJxFxjjj
         RwIaFvXbrrNtn5/WhAhOvBxS7sz72ycC4NabO+8PxxyKzmbAMDkoKkmj+xrFJi1Bziji
         OuCGf4s9685M11ttICKygTVxhvjbbJY0osnYCASt+hW3toE7nh2P4vG42rWJ2SkVZDZz
         kOshvGHyIB5zIk9wm/nauR4thy8ttBfr1rTljZe+jxOnSc/Ecq4jWjIhDHNsL+WKfC5U
         4xyVAtt+1jc1jC2gkKArLs6vpP/lqpHkUr7xHSZbZJzShpLDgc+TTJtLiHNPxbKrNDAp
         7GbQ==
MIME-Version: 1.0
X-Received: by 10.180.72.136 with SMTP id d8mr8239449wiv.36.1400652582982;
 Tue, 20 May 2014 23:09:42 -0700 (PDT)
Received: by 10.216.165.71 with HTTP; Tue, 20 May 2014 23:09:42 -0700 (PDT)
In-Reply-To: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>
References: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>
Date: Tue, 20 May 2014 23:09:42 -0700
Message-ID: <CALuGr6YvTxM+YyXx-+p3XsfcXT2wWE3iJ0T43HqJzKfs6s2UYQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC10)
From: Henry Saputra <henry.saputra@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Signature and hash for source looks good
No external executable package with source - good
Compiled with git and maven - good
Ran examples and sample programs locally and standalone -good

+1

- Henry



On Tue, May 20, 2014 at 1:13 PM, Tathagata Das
<tathagata.das1565@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.0.0!
>
> This has a few bug fixes on top of rc9:
> SPARK-1875: https://github.com/apache/spark/pull/824
> SPARK-1876: https://github.com/apache/spark/pull/819
> SPARK-1878: https://github.com/apache/spark/pull/822
> SPARK-1879: https://github.com/apache/spark/pull/823
>
> The tag to be voted on is v1.0.0-rc10 (commit d8070234):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d807023479ce10aec28ef3c1ab646ddefc2e663c
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~tdas/spark-1.0.0-rc10/
>
> The release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/tdas.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1018/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>
> The full list of changes in this release can be found at:
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=blob;f=CHANGES.txt;h=d21f0ace6326e099360975002797eb7cba9d5273;hb=d807023479ce10aec28ef3c1ab646ddefc2e663c
>
> Please vote on releasing this package as Apache Spark 1.0.0!
>
> The vote is open until Friday, May 23, at 20:00 UTC and passes if
> amajority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.0.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> ====== API Changes ======
> We welcome users to compile Spark applications against 1.0. There are
> a few API changes in this release. Here are links to the associated
> upgrade guides - user facing changes have been kept as small as
> possible.
>
> Changes to ML vector specification:
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/mllib-guide.html#from-09-to-10
>
> Changes to the Java API:
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>
> Changes to the streaming API:
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>
> Changes to the GraphX API:
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>
> Other changes:
> coGroup and related functions now return Iterable[T] instead of Seq[T]
> ==> Call toSeq on the result to restore the old behavior
>
> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> ==> Call toSeq on the result to restore old behavior

From dev-return-7752-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 07:32:11 2014
Return-Path: <dev-return-7752-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B7F1E11819
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 07:32:11 +0000 (UTC)
Received: (qmail 92922 invoked by uid 500); 21 May 2014 07:32:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92868 invoked by uid 500); 21 May 2014 07:32:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92860 invoked by uid 99); 21 May 2014 07:32:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 07:32:11 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of cai.qianwen@hotmail.co.uk does not designate 216.139.250.139 as permitted sender)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 07:32:06 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <cai.qianwen@hotmail.co.uk>)
	id 1Wn0zh-00060P-80
	for dev@spark.incubator.apache.org; Wed, 21 May 2014 00:31:21 -0700
Date: Wed, 21 May 2014 00:31:06 -0700 (PDT)
From: Sue Cai <cai.qianwen@hotmail.co.uk>
To: dev@spark.incubator.apache.org
Message-ID: <1400657466191-6740.post@n3.nabble.com>
Subject: MLlib ALS-- Errors communicating with MapOutputTracker
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hello,

I am currently using MLlib ALS to process a large volume of data, about 1.2
billion Rating(userId, productId, rates) triples. The dataset was sepatated
into 4000 partitions for parallized computation on our yarn clusters. 

I encountered this error "Errors communicating with MapOutputTracker",
when trying to get the prediciton rates [model.predict(userproducts)] after
iterations.

    val predictions = model.predict(usersProducts).map{
      case Rating(user, product, rate) => ((user, product), rate)
    }

I tried to separate the iteration process and the process of culating
prediction rates value by storing the two feature matirces into file system
first; and the loading them for prediction. This time, the error occurred at
the stage of loading userFeatures. 

userfData: userId:[0.3,0.5,0.002,.....]

  val userfTuple =userfData.map{
      case (line) => {
        val arr = line.split(splitmark_1)
        val featureArr = arr(1).split(splitmark_2)
        (arr(0),featureArr)
      }
    }

Here is part of the log:
----------------------------------------------------------------------------------------------------------------------------------------------------------------

14-05-21 14:37:17 WARN [Result resolver thread-0] TaskSetManager: Loss was
due to org.apache.spark.SparkException
org.apache.spark.SparkException: Error communicating with MapOutputTracker
        at
org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:79)
        at
org.apache.spark.MapOutputTracker.getServerStatuses(MapOutputTracker.scala:126)
        at
org.apache.spark.BlockStoreShuffleFetcher.fetch(BlockStoreShuffleFetcher.scala:43)
        at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:61)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:244)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:235)
        at
org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:90)
        at
org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:89)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:727)
        at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:723)
        at
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:884)
        at
org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:884)
        at
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
        at org.apache.spark.scheduler.Task.run(Task.scala:53)
        at
org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:220)
        at
org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:49)
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:184)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)


-------------------------------------------------------------------------------------------------------------------------------------------------------------------

I have tried several methods to solve this problem, one way was to decrease
the number of partitions(from 4000 to 3000), another was to increase the
memory of masters. Both worked, but it is still vital to track the
underneath causes there, right? 

Could anyone help me to check this problem? Thanks a lot. 

Sue Cai




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-ALS-Errors-communicating-with-MapOutputTracker-tp6740.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7753-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 07:46:20 2014
Return-Path: <dev-return-7753-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8EF611188D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 07:46:20 +0000 (UTC)
Received: (qmail 21844 invoked by uid 500); 21 May 2014 07:46:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21785 invoked by uid 500); 21 May 2014 07:46:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21777 invoked by uid 99); 21 May 2014 07:46:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 07:46:20 +0000
X-ASF-Spam-Status: No, hits=3.6 required=10.0
	tests=FROM_EXCESS_BASE64,HTML_MESSAGE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of witgo@qq.com designates 184.105.206.26 as permitted sender)
Received: from [184.105.206.26] (HELO smtpbg303.qq.com) (184.105.206.26)
    by apache.org (qpsmtpd/0.29) with SMTP; Wed, 21 May 2014 07:46:16 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=qq.com; s=s201307;
	t=1400658355; bh=mAuAU0Cetg+D+3rz17cCBe8UGGc84vhtt98GuU7W6ds=;
	h=X-QQ-FEAT:X-QQ-SSF:X-HAS-ATTACH:X-QQ-BUSINESS-ORIGIN:
	 X-Originating-IP:In-Reply-To:References:X-QQ-STYLE:X-QQ-mid:From:To:Subject:Mime-Version:Content-Type:Content-Transfer-Encoding:Date:
	 X-Priority:Message-ID:X-QQ-MIME:X-Mailer:X-QQ-Mailer:
	 X-QQ-ReplyHash:X-QQ-SENDSIZE:X-QQ-FName:X-QQ-LocalIP;
	b=rQvNWP56GV1wVLKjbGm6UHUCfSzDQxvIKCZvrBFkUeffx0M03l7289R9sRbWgrB4F
	 pys4kMAhERGwCngxeGs7nWYvLXkx9slWftDH/Jp67bBjMJMzeQTfIHeEt3X5DNx65O
	 9Z5soEZ6pxJvaeZoaz3vgRr7VM6RjiaLbgluTvBY=
X-QQ-FEAT: kdH+uKxk4IelpkSOKj/K2eS9PxC07LQyED6YsaWFiv3yNnqF8Yrk7+188ev/C
	ba6FU60N19KBJleOUSuATFDpOKbOHBuOdvPMr+rEf3dAkhCcJYHZM4Bdi8a7egvhB2zNmA8
	kJLCza2U+l0dWMpQ+afGq021N72Iu3KQHPjhJaM=
X-QQ-SSF: 000000000000001000000000000000M
X-HAS-ATTACH: no
X-QQ-BUSINESS-ORIGIN: 2
X-Originating-IP: 219.142.170.212
In-Reply-To: <1400657466191-6740.post@n3.nabble.com>
References: <1400657466191-6740.post@n3.nabble.com>
X-QQ-STYLE: 
X-QQ-mid: webmail421t1400658350t7892586
From: "=?ISO-8859-1?B?d2l0Z28=?=" <witgo@qq.com>
To: "=?ISO-8859-1?B?ZGV2?=" <dev@spark.apache.org>
Subject: Re:MLlib ALS-- Errors communicating with MapOutputTracker
Mime-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_537C59AE_08AEE108_7F9C87B9"
Content-Transfer-Encoding: 8Bit
Date: Wed, 21 May 2014 15:45:50 +0800
X-Priority: 3
Message-ID: <tencent_6B37D69C54F76819509A5C4F@qq.com>
X-QQ-MIME: TCMime 1.0 by Tencent
X-Mailer: QQMail 2.x
X-QQ-Mailer: QQMail 2.x
X-QQ-ReplyHash: 2907599427
X-QQ-SENDSIZE: 520
X-QQ-FName: 356F50A85C0E4E31B56D56838E8618D7
X-QQ-LocalIP: 112.95.241.173
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_537C59AE_08AEE108_7F9C87B9
Content-Type: text/plain;
	charset="ISO-8859-1"
Content-Transfer-Encoding: base64
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------=_NextPart_537C59AE_08AEE108_7F9C87B9--


From dev-return-7754-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 12:48:20 2014
Return-Path: <dev-return-7754-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A0D1611204
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 12:48:20 +0000 (UTC)
Received: (qmail 37032 invoked by uid 500); 21 May 2014 12:48:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36960 invoked by uid 500); 21 May 2014 12:48:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36940 invoked by uid 99); 21 May 2014 12:48:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 12:48:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of gerard.maas@gmail.com designates 74.125.82.181 as permitted sender)
Received: from [74.125.82.181] (HELO mail-we0-f181.google.com) (74.125.82.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 12:48:15 +0000
Received: by mail-we0-f181.google.com with SMTP id w61so1952001wes.26
        for <multiple recipients>; Wed, 21 May 2014 05:47:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=zr82tdIoEIy70lTiwEXM488QrhKeJMrJflGIMN39e/E=;
        b=lQSo2EbhZsPIejLIQeKj0AM8GOVjB6JQo4MyJBGCkUlvnoXXqNzGATX732GPBy9V+c
         gCLqwED6TngJGm3FOePrBWxnCh0uAx8Ksx6mifgjHtBx4nYWwhWmcFCQbvKbSk2uXViU
         g4Xn8MbLnEsmnmGcExhp1jQjfeo9+jNv5nJfU1YuxwV9hBPSW7mGrg090O9vR8VAy1qk
         8sBjHpbuS0BQgOY/inzj3BpwRrTyBEgqyPWzNmLdhN2hDH/0Y0HsAAfAfPswC8PqmSqc
         r46KfqVucmp9hObDYoMlLI2WmVERyfveZKoC3mYGTkMCAkFaJ4bbJ3xH4rzIPaajhba4
         bLbw==
X-Received: by 10.194.58.43 with SMTP id n11mr1670039wjq.92.1400676473625;
 Wed, 21 May 2014 05:47:53 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.173.2 with HTTP; Wed, 21 May 2014 05:47:22 -0700 (PDT)
In-Reply-To: <CAPH-c_P58KrkCdzM5MyXcZ6=MdK8ANpzE2VTPneUZAEV33_fNA@mail.gmail.com>
References: <CAPH-c_P58KrkCdzM5MyXcZ6=MdK8ANpzE2VTPneUZAEV33_fNA@mail.gmail.com>
From: Gerard Maas <gerard.maas@gmail.com>
Date: Wed, 21 May 2014 14:47:22 +0200
Message-ID: <CAMc-71m5C9ZMo--NAcq3EavOG5RPYeP-oZf+KetbAp77gT-thA@mail.gmail.com>
Subject: Re: ClassNotFoundException with Spark/Mesos (spark-shell works fine)
To: user@spark.apache.org, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7ba9803aa811ae04f9e867cd
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7ba9803aa811ae04f9e867cd
Content-Type: text/plain; charset=UTF-8

Hi Tobias,

I was curious about this issue and tried to run your example on my local
Mesos. I was able to reproduce your issue using your current config:

[error] (run-main-0) org.apache.spark.SparkException: Job aborted: Task
1.0:4 failed 4 times (most recent failure: Exception failure:
java.lang.ClassNotFoundException: spark.SparkExamplesMinimal$$anonfun$2)
org.apache.spark.SparkException: Job aborted: Task 1.0:4 failed 4 times
(most recent failure: Exception failure: java.lang.ClassNotFoundException:
spark.SparkExamplesMinimal$$anonfun$2)
 at
org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1028)

Creating a simple jar from the job and providing it through the
configuration seems to solve it:

val conf = new SparkConf()
      .setMaster("mesos://<my_ip>:5050/")
*
.setJars(Seq("/sparkexample/target/scala-2.10/sparkexample_2.10-0.1.jar"))*
      .setAppName("SparkExamplesMinimal")

Resulting in:
14/05/21 12:03:45 INFO scheduler.DAGScheduler: Completed ResultTask(1, 1)
14/05/21 12:03:45 INFO scheduler.DAGScheduler: Stage 1 (count at
SparkExamplesMinimal.scala:50) finished in 1.120 s
14/05/21 12:03:45 INFO spark.SparkContext: Job finished: count at
SparkExamplesMinimal.scala:50, took 1.177091435 s
count: 1000000

Why the closure serialization does not work with Mesos is beyond my current
knowledge.
Would be great to hear from the experts (cross-posting to dev for that)

-kr, Gerard.













On Wed, May 21, 2014 at 11:51 AM, Tobias Pfeiffer <tgp@preferred.jp> wrote:

> Hi,
>
> I have set up a cluster with Mesos (backed by Zookeeper) with three
> master and three slave instances. I set up Spark (git HEAD) for use
> with Mesos according to this manual:
> http://people.apache.org/~pwendell/catalyst-docs/running-on-mesos.html
>
> Using the spark-shell, I can connect to this cluster and do simple RDD
> operations, but the same code in a Scala class and executed via sbt
> run-main works only partially. (That is, count() works, count() after
> flatMap() does not.)
>
> Here is my code: https://gist.github.com/tgpfeiffer/7d20a4d59ee6e0088f91
> The file SparkExamplesScript.scala, when pasted into spark-shell,
> outputs the correct count() for the parallelized list comprehension,
> as well as for the flatMapped RDD.
>
> The file SparkExamplesMinimal.scala contains exactly the same code,
> and also the MASTER configuration and the Spark Executor are the same.
> However, while the count() for the parallelized list is displayed
> correctly, I receive the following error when asking for the count()
> of the flatMapped RDD:
>
> -----------------
>
> 14/05/21 09:47:49 INFO scheduler.DAGScheduler: Submitting Stage 1
> (FlatMappedRDD[1] at flatMap at SparkExamplesMinimal.scala:34), which
> has no missing parents
> 14/05/21 09:47:49 INFO scheduler.DAGScheduler: Submitting 8 missing
> tasks from Stage 1 (FlatMappedRDD[1] at flatMap at
> SparkExamplesMinimal.scala:34)
> 14/05/21 09:47:49 INFO scheduler.TaskSchedulerImpl: Adding task set
> 1.0 with 8 tasks
> 14/05/21 09:47:49 INFO scheduler.TaskSetManager: Starting task 1.0:0
> as TID 8 on executor 20140520-102159-2154735808-5050-1108-1: mesos9-1
> (PROCESS_LOCAL)
> 14/05/21 09:47:49 INFO scheduler.TaskSetManager: Serialized task 1.0:0
> as 1779147 bytes in 37 ms
> 14/05/21 09:47:49 WARN scheduler.TaskSetManager: Lost TID 8 (task 1.0:0)
> 14/05/21 09:47:49 WARN scheduler.TaskSetManager: Loss was due to
> java.lang.ClassNotFoundException
> java.lang.ClassNotFoundException: spark.SparkExamplesMinimal$$anonfun$2
> at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
> at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
> at java.security.AccessController.doPrivileged(Native Method)
> at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
> at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
> at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
> at java.lang.Class.forName0(Native Method)
> at java.lang.Class.forName(Class.java:270)
> at
> org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:60)
> at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1612)
> at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
> at
> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
> at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
> at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
> at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
> at
> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
> at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
> at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
> at
> org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)
> at
> org.apache.spark.scheduler.ResultTask$.deserializeInfo(ResultTask.scala:61)
> at org.apache.spark.scheduler.ResultTask.readExternal(ResultTask.scala:141)
> at java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1837)
> at
> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
> at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
> at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
> at
> org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)
> at
> org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:85)
> at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:169)
> at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> at java.lang.Thread.run(Thread.java:745)
>
> -----------------
>
> Can anyone explain to me where this comes from or how I might further
> track the problem down?
>
> Thanks,
> Tobias
>

--047d7ba9803aa811ae04f9e867cd--

From dev-return-7755-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 14:19:58 2014
Return-Path: <dev-return-7755-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 76875115D3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 14:19:58 +0000 (UTC)
Received: (qmail 48992 invoked by uid 500); 21 May 2014 14:19:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48933 invoked by uid 500); 21 May 2014 14:19:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48925 invoked by uid 99); 21 May 2014 14:19:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 14:19:58 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of gerard.maas@gmail.com designates 209.85.212.176 as permitted sender)
Received: from [209.85.212.176] (HELO mail-wi0-f176.google.com) (209.85.212.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 14:19:55 +0000
Received: by mail-wi0-f176.google.com with SMTP id n15so7753527wiw.15
        for <dev@spark.apache.org>; Wed, 21 May 2014 07:19:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:cc:content-type;
        bh=4EOTNRs3a6ErM+Bwk2jqH7SXf4ECEqIgcPGW7gaZx3w=;
        b=ofWXU8v9u28h+A4R9Mge9gr379+whsltX9hOA+92QdfVFMhFfgd5oxpOF2fmOuDBkJ
         IFDrJjET3rJIhKXdKLTkXjool9xa2qCLuOw5Cq+hziuO6WoCzRj7Tyol0yQFYY9AkAu2
         nbes9FNrPTp1wWq+iKl3ivSRlLAAtJbl7Ez8QdLr05shHHEP+IbG7MbydXeSlMZldKTN
         9wewSywXBzxrxxJkpiZVbtSFkU39VFABkX6JBC9KxutkdF2yHE3dnAwlMTGhN+P4zgHt
         At2hHtCuRVyVrr83jO+yjxWRCFOwO3ardi0yhvd5iyOIDydXx9Vca/0jY7Atie6FA0iQ
         8o3A==
X-Received: by 10.194.88.106 with SMTP id bf10mr44638173wjb.26.1400681971818;
 Wed, 21 May 2014 07:19:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.173.2 with HTTP; Wed, 21 May 2014 07:19:01 -0700 (PDT)
From: Gerard Maas <gerard.maas@gmail.com>
Date: Wed, 21 May 2014 16:19:01 +0200
Message-ID: <CAMc-71kjOHyVcL+4GZW=ch8+ZxvkDcXWnW07Uvaa4ePX7n79=A@mail.gmail.com>
Subject: Should SPARK_HOME be needed with Mesos?
To: dev@spark.apache.org
Cc: tgp@preferred.jp
Content-Type: multipart/alternative; boundary=047d7bf198925fa8c604f9e9afd7
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bf198925fa8c604f9e9afd7
Content-Type: text/plain; charset=UTF-8

Spark dev's,

I was looking into a question asked on the user list where a
ClassNotFoundException was thrown when running a job on Mesos. Curious
issue with serialization on Mesos: more details here [1]:

When trying to run that simple example on my Mesos installation, I faced
another issue: I got an error that "SPARK_HOME" was not set. I found that
curious b/c a local spark installation should not be required to run a job
on Mesos. All that's needed is the executor package, being the
assembly.tar.gz on a reachable location (HDFS/S3/HTTP).

I went looking into the code and indeed there's a check on SPARK_HOME [2]
regardless of the presence of the assembly but it's actually only used if
the assembly is not provided (which is a kind-of best-effort recovery
strategy).

Current flow:

if (!SPARK_HOME) fail("No SPARK_HOME")
else if (assembly) { use assembly) }
else { try use SPARK_HOME to build spark_executor }

Should be:
sparkExecutor =  if (assembly) {assembly}
                 else if (SPARK_HOME) {try use SPARK_HOME to build
spark_executor}
                 else { fail("No executor found. Please provide
spark.executor.uri (preferred) or spark.home")

What do you think?

-kr, Gerard.


[1]
http://apache-spark-user-list.1001560.n3.nabble.com/ClassNotFoundException-with-Spark-Mesos-spark-shell-works-fine-td6165.html

[2]
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend.scala#L89

--047d7bf198925fa8c604f9e9afd7--

From dev-return-7756-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 15:33:35 2014
Return-Path: <dev-return-7756-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 45A5D11909
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 15:33:35 +0000 (UTC)
Received: (qmail 635 invoked by uid 500); 21 May 2014 15:33:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 543 invoked by uid 500); 21 May 2014 15:33:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 527 invoked by uid 99); 21 May 2014 15:33:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 15:33:34 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of gerard.maas@gmail.com designates 74.125.82.52 as permitted sender)
Received: from [74.125.82.52] (HELO mail-wg0-f52.google.com) (74.125.82.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 15:33:29 +0000
Received: by mail-wg0-f52.google.com with SMTP id l18so2158078wgh.23
        for <multiple recipients>; Wed, 21 May 2014 08:33:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=fItIQrfRHs77w5IB35q8vQjbqDEiPQk+2mCRje+YvJs=;
        b=iSfOpopoxlFF8mVZQpMFmq8/DwKH7XbAEgs4OHyfn2Z5LoSTBnPpdaUoxxSpy5iFP2
         aJL3OC6a7VPWWV5SjRXVdoawo0NCzGuGpr3qv+4K+G/NIhunoqaO8f9SqLrD16iYywzH
         b87sOLuJztJaRjeI/I3pyCs8gtvhrMigg9JUz5/8sGRii29QSbxyAp5+457vuMHJWpgE
         UIvfxfofwnJb1OkXQK/VuRiaXb+UaLPPCvmlpMPMkurl8+mvl1T0/u0pW14JfUL5MZl9
         33iyIPn4V9fn/pQHxkmnfbAUwRN5oqnEfXuRnbatIpBf4nqyjyj0zo9/CW7nG826gORW
         qbdQ==
X-Received: by 10.194.58.43 with SMTP id n11mr2604080wjq.92.1400686388425;
 Wed, 21 May 2014 08:33:08 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.173.2 with HTTP; Wed, 21 May 2014 08:32:38 -0700 (PDT)
In-Reply-To: <CAMc-71m5C9ZMo--NAcq3EavOG5RPYeP-oZf+KetbAp77gT-thA@mail.gmail.com>
References: <CAPH-c_P58KrkCdzM5MyXcZ6=MdK8ANpzE2VTPneUZAEV33_fNA@mail.gmail.com>
 <CAMc-71m5C9ZMo--NAcq3EavOG5RPYeP-oZf+KetbAp77gT-thA@mail.gmail.com>
From: Gerard Maas <gerard.maas@gmail.com>
Date: Wed, 21 May 2014 17:32:38 +0200
Message-ID: <CAMc-71=CR-PDfrjP0XdCvG7bT3LX-HwNx4irN9ee0tLodnxEtg@mail.gmail.com>
Subject: Re: ClassNotFoundException with Spark/Mesos (spark-shell works fine)
To: user@spark.apache.org, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7ba9803a9fbd2004f9eab67d
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7ba9803a9fbd2004f9eab67d
Content-Type: text/plain; charset=UTF-8

Hi Tobias,

Regarding my comment on closure serialization:

I was discussing it with my fellow Sparkers here and I totally overlooked
the fact that you need the class files to de-serialize the closures (or
whatever) on the workers, so you always need the jar file delivered to the
workers in order for it to work.

The SparkREPL  works differently. It uses some dark magic to send the
working session to the workers.

-kr, Gerard.





On Wed, May 21, 2014 at 2:47 PM, Gerard Maas <gerard.maas@gmail.com> wrote:

> Hi Tobias,
>
> I was curious about this issue and tried to run your example on my local
> Mesos. I was able to reproduce your issue using your current config:
>
> [error] (run-main-0) org.apache.spark.SparkException: Job aborted: Task
> 1.0:4 failed 4 times (most recent failure: Exception failure:
> java.lang.ClassNotFoundException: spark.SparkExamplesMinimal$$anonfun$2)
> org.apache.spark.SparkException: Job aborted: Task 1.0:4 failed 4 times
> (most recent failure: Exception failure: java.lang.ClassNotFoundException:
> spark.SparkExamplesMinimal$$anonfun$2)
>  at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1028)
>
> Creating a simple jar from the job and providing it through the
> configuration seems to solve it:
>
> val conf = new SparkConf()
>       .setMaster("mesos://<my_ip>:5050/")
> *
> .setJars(Seq("/sparkexample/target/scala-2.10/sparkexample_2.10-0.1.jar"))*
>       .setAppName("SparkExamplesMinimal")
>
> Resulting in:
>  14/05/21 12:03:45 INFO scheduler.DAGScheduler: Completed ResultTask(1, 1)
> 14/05/21 12:03:45 INFO scheduler.DAGScheduler: Stage 1 (count at
> SparkExamplesMinimal.scala:50) finished in 1.120 s
> 14/05/21 12:03:45 INFO spark.SparkContext: Job finished: count at
> SparkExamplesMinimal.scala:50, took 1.177091435 s
> count: 1000000
>
> Why the closure serialization does not work with Mesos is beyond my
> current knowledge.
> Would be great to hear from the experts (cross-posting to dev for that)
>
> -kr, Gerard.
>
>
>
>
>
>
>
>
>
>
>
>
>
> On Wed, May 21, 2014 at 11:51 AM, Tobias Pfeiffer <tgp@preferred.jp>wrote:
>
>> Hi,
>>
>> I have set up a cluster with Mesos (backed by Zookeeper) with three
>> master and three slave instances. I set up Spark (git HEAD) for use
>> with Mesos according to this manual:
>> http://people.apache.org/~pwendell/catalyst-docs/running-on-mesos.html
>>
>> Using the spark-shell, I can connect to this cluster and do simple RDD
>> operations, but the same code in a Scala class and executed via sbt
>> run-main works only partially. (That is, count() works, count() after
>> flatMap() does not.)
>>
>> Here is my code: https://gist.github.com/tgpfeiffer/7d20a4d59ee6e0088f91
>> The file SparkExamplesScript.scala, when pasted into spark-shell,
>> outputs the correct count() for the parallelized list comprehension,
>> as well as for the flatMapped RDD.
>>
>> The file SparkExamplesMinimal.scala contains exactly the same code,
>> and also the MASTER configuration and the Spark Executor are the same.
>> However, while the count() for the parallelized list is displayed
>> correctly, I receive the following error when asking for the count()
>> of the flatMapped RDD:
>>
>> -----------------
>>
>> 14/05/21 09:47:49 INFO scheduler.DAGScheduler: Submitting Stage 1
>> (FlatMappedRDD[1] at flatMap at SparkExamplesMinimal.scala:34), which
>> has no missing parents
>> 14/05/21 09:47:49 INFO scheduler.DAGScheduler: Submitting 8 missing
>> tasks from Stage 1 (FlatMappedRDD[1] at flatMap at
>> SparkExamplesMinimal.scala:34)
>> 14/05/21 09:47:49 INFO scheduler.TaskSchedulerImpl: Adding task set
>> 1.0 with 8 tasks
>> 14/05/21 09:47:49 INFO scheduler.TaskSetManager: Starting task 1.0:0
>> as TID 8 on executor 20140520-102159-2154735808-5050-1108-1: mesos9-1
>> (PROCESS_LOCAL)
>> 14/05/21 09:47:49 INFO scheduler.TaskSetManager: Serialized task 1.0:0
>> as 1779147 bytes in 37 ms
>> 14/05/21 09:47:49 WARN scheduler.TaskSetManager: Lost TID 8 (task 1.0:0)
>> 14/05/21 09:47:49 WARN scheduler.TaskSetManager: Loss was due to
>> java.lang.ClassNotFoundException
>> java.lang.ClassNotFoundException: spark.SparkExamplesMinimal$$anonfun$2
>> at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
>> at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
>> at java.security.AccessController.doPrivileged(Native Method)
>> at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
>> at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
>> at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
>> at java.lang.Class.forName0(Native Method)
>> at java.lang.Class.forName(Class.java:270)
>> at
>> org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:60)
>> at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1612)
>> at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
>> at
>> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
>> at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
>> at
>> java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
>> at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
>> at
>> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
>> at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
>> at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
>> at
>> org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)
>> at
>> org.apache.spark.scheduler.ResultTask$.deserializeInfo(ResultTask.scala:61)
>> at
>> org.apache.spark.scheduler.ResultTask.readExternal(ResultTask.scala:141)
>> at java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1837)
>> at
>> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
>> at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
>> at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
>> at
>> org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)
>> at
>> org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:85)
>> at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:169)
>> at
>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>> at
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>> at java.lang.Thread.run(Thread.java:745)
>>
>> -----------------
>>
>> Can anyone explain to me where this comes from or how I might further
>> track the problem down?
>>
>> Thanks,
>> Tobias
>>
>
>

--047d7ba9803a9fbd2004f9eab67d--

From dev-return-7757-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 16:58:59 2014
Return-Path: <dev-return-7757-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 65F4811D18
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 16:58:59 +0000 (UTC)
Received: (qmail 84101 invoked by uid 500); 21 May 2014 16:58:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84046 invoked by uid 500); 21 May 2014 16:58:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84038 invoked by uid 99); 21 May 2014 16:58:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 16:58:58 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mark@clearstorydata.com designates 209.85.192.53 as permitted sender)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 16:58:54 +0000
Received: by mail-qg0-f53.google.com with SMTP id f51so3618691qge.26
        for <dev@spark.apache.org>; Wed, 21 May 2014 09:58:33 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=LUCnow/gzKOHupZu/9wBZ605soJSiDm4RmsNPWuVXZs=;
        b=MssDdYTAUTlQE99MlXViruhFRYv4HM0EPpB0aQYQEXK6kSOjVarOkoxc2zajBcFyuY
         9JY4MMha8YzQ743xF94SXn7OV5sNRtxxy7gX8gwLDyhMlHgWUQWJrAJFbEjcCKSjbSGW
         Iu23zeeGD2rYsqI6TjxMGkKnL/2EpyL/IA2YxnzW307mLj/uzJPEeq7J10EATyx5pkrd
         xIQNDFwvrwgPkDljdaNweCgqKrP9s0fyGHpLv1rI+o45Es7j4SmSmAxZL6XYTNoJWQYn
         k8W4MaBtMv94jiB4+5CguqfcxeOGhDsRncPOKUb0/blm03D/AfFWAZ29Up1M38qy3jWc
         b/3Q==
X-Gm-Message-State: ALoCoQlSVGEPC9Gji7prgnGuRtmVeD1+l9YcbVgHvxFaSygj3Xu8nMW7Olv8iX9QVpX77ZyimxC7
MIME-Version: 1.0
X-Received: by 10.140.27.45 with SMTP id 42mr5524738qgw.94.1400691513148; Wed,
 21 May 2014 09:58:33 -0700 (PDT)
Received: by 10.140.37.40 with HTTP; Wed, 21 May 2014 09:58:33 -0700 (PDT)
In-Reply-To: <CALuGr6YvTxM+YyXx-+p3XsfcXT2wWE3iJ0T43HqJzKfs6s2UYQ@mail.gmail.com>
References: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>
	<CALuGr6YvTxM+YyXx-+p3XsfcXT2wWE3iJ0T43HqJzKfs6s2UYQ@mail.gmail.com>
Date: Wed, 21 May 2014 09:58:33 -0700
Message-ID: <CAAsvFPkk+TVQiAu+_XiDOOgqUr4tck8ccA=YQ5Aiv_SF7hn7Dg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC10)
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c152f414ea8104f9ebe85a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c152f414ea8104f9ebe85a
Content-Type: text/plain; charset=UTF-8

+1


On Tue, May 20, 2014 at 11:09 PM, Henry Saputra <henry.saputra@gmail.com>wrote:

> Signature and hash for source looks good
> No external executable package with source - good
> Compiled with git and maven - good
> Ran examples and sample programs locally and standalone -good
>
> +1
>
> - Henry
>
>
>
> On Tue, May 20, 2014 at 1:13 PM, Tathagata Das
> <tathagata.das1565@gmail.com> wrote:
> > Please vote on releasing the following candidate as Apache Spark version
> 1.0.0!
> >
> > This has a few bug fixes on top of rc9:
> > SPARK-1875: https://github.com/apache/spark/pull/824
> > SPARK-1876: https://github.com/apache/spark/pull/819
> > SPARK-1878: https://github.com/apache/spark/pull/822
> > SPARK-1879: https://github.com/apache/spark/pull/823
> >
> > The tag to be voted on is v1.0.0-rc10 (commit d8070234):
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d807023479ce10aec28ef3c1ab646ddefc2e663c
> >
> > The release files, including signatures, digests, etc. can be found at:
> > http://people.apache.org/~tdas/spark-1.0.0-rc10/
> >
> > The release artifacts are signed with the following key:
> > https://people.apache.org/keys/committer/tdas.asc
> >
> > The staging repository for this release can be found at:
> > https://repository.apache.org/content/repositories/orgapachespark-1018/
> >
> > The documentation corresponding to this release can be found at:
> > http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
> >
> > The full list of changes in this release can be found at:
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=blob;f=CHANGES.txt;h=d21f0ace6326e099360975002797eb7cba9d5273;hb=d807023479ce10aec28ef3c1ab646ddefc2e663c
> >
> > Please vote on releasing this package as Apache Spark 1.0.0!
> >
> > The vote is open until Friday, May 23, at 20:00 UTC and passes if
> > amajority of at least 3 +1 PMC votes are cast.
> >
> > [ ] +1 Release this package as Apache Spark 1.0.0
> > [ ] -1 Do not release this package because ...
> >
> > To learn more about Apache Spark, please see
> > http://spark.apache.org/
> >
> > ====== API Changes ======
> > We welcome users to compile Spark applications against 1.0. There are
> > a few API changes in this release. Here are links to the associated
> > upgrade guides - user facing changes have been kept as small as
> > possible.
> >
> > Changes to ML vector specification:
> >
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/mllib-guide.html#from-09-to-10
> >
> > Changes to the Java API:
> >
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
> >
> > Changes to the streaming API:
> >
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
> >
> > Changes to the GraphX API:
> >
> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
> >
> > Other changes:
> > coGroup and related functions now return Iterable[T] instead of Seq[T]
> > ==> Call toSeq on the result to restore the old behavior
> >
> > SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> > ==> Call toSeq on the result to restore old behavior
>

--001a11c152f414ea8104f9ebe85a--

From dev-return-7758-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 20:14:22 2014
Return-Path: <dev-return-7758-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BC40C117A1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 20:14:22 +0000 (UTC)
Received: (qmail 36225 invoked by uid 500); 21 May 2014 20:14:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36165 invoked by uid 500); 21 May 2014 20:14:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36157 invoked by uid 99); 21 May 2014 20:14:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 20:14:17 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.192.54 as permitted sender)
Received: from [209.85.192.54] (HELO mail-qg0-f54.google.com) (209.85.192.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 20:14:13 +0000
Received: by mail-qg0-f54.google.com with SMTP id q108so4064532qgd.27
        for <dev@spark.apache.org>; Wed, 21 May 2014 13:13:53 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=Gojs3/RjTQ0aPEfZb9X9mVsJKeR159m1hVronJ4rmC0=;
        b=h+W62Hb7sFTNGv/ngZuE7TYZ4RE21XHr/2VT8t/tfPv8tYOA2OVS4GriprktIPDp4U
         F6sbjyuUBsuXbwo2A2ChUR3B1UBUQgZawgRWtuQzyuxVdYSbLKbpR9ov3bDjYqJsgBiL
         525eiUQxiejJ+EUATIaBu85vV0Jisms/fVpOOF97oZZELKAvCgxpfV+dkdYUhLmYCT1t
         oVm5ie1SYfrnIVmrPkLoqFglvLaUWqmS9kfyrzN0+O20F596yYUFe/eMveGOKtqoFk6A
         Edldbx0OHuRuocWR+qGv0uSXPRQLqFlhAQiw82PkYTlcqEzMOqt8Mx/CI0BzEhBBJ52n
         3PsQ==
X-Gm-Message-State: ALoCoQn68MpTla6TgY5vbhcEEz8KYdBCFRTRLIWBcsl2Cy3yqtG8Bn0oPB11vlLYH6+2iJlzjDOj
MIME-Version: 1.0
X-Received: by 10.229.221.194 with SMTP id id2mr55483121qcb.5.1400703232948;
 Wed, 21 May 2014 13:13:52 -0700 (PDT)
Received: by 10.140.106.202 with HTTP; Wed, 21 May 2014 13:13:52 -0700 (PDT)
In-Reply-To: <CAJgQjQ_ifNtFFK4W99FA3cJ3BapRRfLc4FomCWxdgwTqEU+icg@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
	<CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
	<CAMAsSdKxE-biW8fRqkRn6NmqsszFgQ=f1w_Z8yt8cSOj3qBB2Q@mail.gmail.com>
	<CAEYYnxY9UZDuhBoHAP5kD3dCXxd0TwR761tt-kswjTEddQCCKg@mail.gmail.com>
	<CA+-p3AGT8E5caKU+2ExxjbpV-UqaaTot+Ni3DXEdOf37tADLgw@mail.gmail.com>
	<CAMAsSdJVckDE5=3S8wZj9P1-1-D6qBumGnofk9G7A7cksh1RVA@mail.gmail.com>
	<CABPQxstfSyVu6D-FSt7qa-o1co9AkhHsncTHGBrCK4rEM7fymg@mail.gmail.com>
	<CACBYxKLb=OK2EKu=2yS_1tswYz52rBF-59+8DZL+DfG6qHF52Q@mail.gmail.com>
	<CAEYYnxakT5SVD0xxWcyAXq+w-f-tY0rDA2c3YLRXK6oZ3ArF7Q@mail.gmail.com>
	<CAEYYnxaYfaUOes+A4kR4w+nh0LS8fgJytM2Nj7xoBErpHiRwbA@mail.gmail.com>
	<CAJgQjQ_ifNtFFK4W99FA3cJ3BapRRfLc4FomCWxdgwTqEU+icg@mail.gmail.com>
Date: Wed, 21 May 2014 13:13:52 -0700
Message-ID: <CACBYxK+VOs2=m8tF1xijTOAxL917Q2yo=pOKNsNfzK52a7QdhQ@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11343920a2f2fb04f9eea2eb
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11343920a2f2fb04f9eea2eb
Content-Type: text/plain; charset=UTF-8

This will solve the issue for jars added upon application submission, but,
on top of this, we need to make sure that anything dynamically added
through sc.addJar works as well.

To do so, we need to make sure that any jars retrieved via the driver's
HTTP server are loaded by the same classloader that loads the jars given on
app submission.  To achieve this, we need to either use the same
classloader for both system jars and user jars, or make sure that the user
jars given on app submission are under the same classloader used for
dynamically added jars.

On Tue, May 20, 2014 at 5:59 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Talked with Sandy and DB offline. I think the best solution is sending
> the secondary jars to the distributed cache of all containers rather
> than just the master, and set the classpath to include spark jar,
> primary app jar, and secondary jars before executor starts. In this
> way, user only needs to specify secondary jars via --jars instead of
> calling sc.addJar inside the code. It also solves the scalability
> problem of serving all the jars via http.
>
> If this solution sounds good, I can try to make a patch.
>
> Best,
> Xiangrui
>
> On Mon, May 19, 2014 at 10:04 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> > In 1.0, there is a new option for users to choose which classloader has
> > higher priority via spark.files.userClassPathFirst, I decided to submit
> the
> > PR for 0.9 first. We use this patch in our lab and we can use those jars
> > added by sc.addJar without reflection.
> >
> > https://github.com/apache/spark/pull/834
> >
> > Can anyone comment if it's a good approach?
> >
> > Thanks.
> >
> >
> > Sincerely,
> >
> > DB Tsai
> > -------------------------------------------------------
> > My Blog: https://www.dbtsai.com
> > LinkedIn: https://www.linkedin.com/in/dbtsai
> >
> >
> > On Mon, May 19, 2014 at 7:42 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> >
> >> Good summary! We fixed it in branch 0.9 since our production is still in
> >> 0.9. I'm porting it to 1.0 now, and hopefully will submit PR for 1.0
> >> tonight.
> >>
> >>
> >> Sincerely,
> >>
> >> DB Tsai
> >> -------------------------------------------------------
> >> My Blog: https://www.dbtsai.com
> >> LinkedIn: https://www.linkedin.com/in/dbtsai
> >>
> >>
> >> On Mon, May 19, 2014 at 7:38 PM, Sandy Ryza <sandy.ryza@cloudera.com
> >wrote:
> >>
> >>> It just hit me why this problem is showing up on YARN and not on
> >>> standalone.
> >>>
> >>> The relevant difference between YARN and standalone is that, on YARN,
> the
> >>> app jar is loaded by the system classloader instead of Spark's custom
> URL
> >>> classloader.
> >>>
> >>> On YARN, the system classloader knows about [the classes in the spark
> >>> jars,
> >>> the classes in the primary app jar].   The custom classloader knows
> about
> >>> [the classes in secondary app jars] and has the system classloader as
> its
> >>> parent.
> >>>
> >>> A few relevant facts (mostly redundant with what Sean pointed out):
> >>> * Every class has a classloader that loaded it.
> >>> * When an object of class B is instantiated inside of class A, the
> >>> classloader used for loading B is the classloader that was used for
> >>> loading
> >>> A.
> >>> * When a classloader fails to load a class, it lets its parent
> classloader
> >>> try.  If its parent succeeds, its parent becomes the "classloader that
> >>> loaded it".
> >>>
> >>> So suppose class B is in a secondary app jar and class A is in the
> primary
> >>> app jar:
> >>> 1. The custom classloader will try to load class A.
> >>> 2. It will fail, because it only knows about the secondary jars.
> >>> 3. It will delegate to its parent, the system classloader.
> >>> 4. The system classloader will succeed, because it knows about the
> primary
> >>> app jar.
> >>> 5. A's classloader will be the system classloader.
> >>> 6. A tries to instantiate an instance of class B.
> >>> 7. B will be loaded with A's classloader, which is the system
> classloader.
> >>> 8. Loading B will fail, because A's classloader, which is the system
> >>> classloader, doesn't know about the secondary app jars.
> >>>
> >>> In Spark standalone, A and B are both loaded by the custom
> classloader, so
> >>> this issue doesn't come up.
> >>>
> >>> -Sandy
> >>>
> >>> On Mon, May 19, 2014 at 7:07 PM, Patrick Wendell <pwendell@gmail.com>
> >>> wrote:
> >>>
> >>> > Having a user add define a custom class inside of an added jar and
> >>> > instantiate it directly inside of an executor is definitely supported
> >>> > in Spark and has been for a really long time (several years). This is
> >>> > something we do all the time in Spark.
> >>> >
> >>> > DB - I'd hold off on a re-architecting of this until we identify
> >>> > exactly what is causing the bug you are running into.
> >>> >
> >>> > In a nutshell, when the bytecode "new Foo()" is run on the executor,
> >>> > it will ask the driver for the class over HTTP using a custom
> >>> > classloader. Something in that pipeline is breaking here, possibly
> >>> > related to the YARN deployment stuff.
> >>> >
> >>> >
> >>> > On Mon, May 19, 2014 at 12:29 AM, Sean Owen <sowen@cloudera.com>
> wrote:
> >>> > > I don't think a customer classloader is necessary.
> >>> > >
> >>> > > Well, it occurs to me that this is no new problem. Hadoop, Tomcat,
> etc
> >>> > > all run custom user code that creates new user objects without
> >>> > > reflection. I should go see how that's done. Maybe it's totally
> valid
> >>> > > to set the thread's context classloader for just this purpose, and
> I
> >>> > > am not thinking clearly.
> >>> > >
> >>> > > On Mon, May 19, 2014 at 8:26 AM, Andrew Ash <andrew@andrewash.com>
> >>> > wrote:
> >>> > >> Sounds like the problem is that classloaders always look in their
> >>> > parents
> >>> > >> before themselves, and Spark users want executors to pick up
> classes
> >>> > from
> >>> > >> their custom code before the ones in Spark plus its dependencies.
> >>> > >>
> >>> > >> Would a custom classloader that delegates to the parent after
> first
> >>> > >> checking itself fix this up?
> >>> > >>
> >>> > >>
> >>> > >> On Mon, May 19, 2014 at 12:17 AM, DB Tsai <dbtsai@stanford.edu>
> >>> wrote:
> >>> > >>
> >>> > >>> Hi Sean,
> >>> > >>>
> >>> > >>> It's true that the issue here is classloader, and due to the
> >>> > classloader
> >>> > >>> delegation model, users have to use reflection in the executors
> to
> >>> > pick up
> >>> > >>> the classloader in order to use those classes added by sc.addJars
> >>> APIs.
> >>> > >>> However, it's very inconvenience for users, and not documented in
> >>> > spark.
> >>> > >>>
> >>> > >>> I'm working on a patch to solve it by calling the protected
> method
> >>> > addURL
> >>> > >>> in URLClassLoader to update the current default classloader, so
> no
> >>> > >>> customClassLoader anymore. I wonder if this is an good way to go.
> >>> > >>>
> >>> > >>>   private def addURL(url: URL, loader: URLClassLoader){
> >>> > >>>     try {
> >>> > >>>       val method: Method =
> >>> > >>> classOf[URLClassLoader].getDeclaredMethod("addURL", classOf[URL])
> >>> > >>>       method.setAccessible(true)
> >>> > >>>       method.invoke(loader, url)
> >>> > >>>     }
> >>> > >>>     catch {
> >>> > >>>       case t: Throwable => {
> >>> > >>>         throw new IOException("Error, could not add URL to system
> >>> > >>> classloader")
> >>> > >>>       }
> >>> > >>>     }
> >>> > >>>   }
> >>> > >>>
> >>> > >>>
> >>> > >>>
> >>> > >>> Sincerely,
> >>> > >>>
> >>> > >>> DB Tsai
> >>> > >>> -------------------------------------------------------
> >>> > >>> My Blog: https://www.dbtsai.com
> >>> > >>> LinkedIn: https://www.linkedin.com/in/dbtsai
> >>> > >>>
> >>> > >>>
> >>> > >>> On Sun, May 18, 2014 at 11:57 PM, Sean Owen <sowen@cloudera.com>
> >>> > wrote:
> >>> > >>>
> >>> > >>> > I might be stating the obvious for everyone, but the issue
> here is
> >>> > not
> >>> > >>> > reflection or the source of the JAR, but the ClassLoader. The
> >>> basic
> >>> > >>> > rules are this.
> >>> > >>> >
> >>> > >>> > "new Foo" will use the ClassLoader that defines Foo. This is
> >>> usually
> >>> > >>> > the ClassLoader that loaded whatever it is that first
> referenced
> >>> Foo
> >>> > >>> > and caused it to be loaded -- usually the ClassLoader holding
> your
> >>> > >>> > other app classes.
> >>> > >>> >
> >>> > >>> > ClassLoaders can have a parent-child relationship. ClassLoaders
> >>> > always
> >>> > >>> > look in their parent before themselves.
> >>> > >>> >
> >>> > >>> > (Careful then -- in contexts like Hadoop or Tomcat where your
> app
> >>> is
> >>> > >>> > loaded in a child ClassLoader, and you reference a class that
> >>> Hadoop
> >>> > >>> > or Tomcat also has (like a lib class) you will get the
> container's
> >>> > >>> > version!)
> >>> > >>> >
> >>> > >>> > When you load an external JAR it has a separate ClassLoader
> which
> >>> > does
> >>> > >>> > not necessarily bear any relation to the one containing your
> app
> >>> > >>> > classes, so yeah it is not generally going to make "new Foo"
> work.
> >>> > >>> >
> >>> > >>> > Reflection lets you pick the ClassLoader, yes.
> >>> > >>> >
> >>> > >>> > I would not call setContextClassLoader.
> >>> > >>> >
> >>> > >>> > On Mon, May 19, 2014 at 12:00 AM, Sandy Ryza <
> >>> > sandy.ryza@cloudera.com>
> >>> > >>> > wrote:
> >>> > >>> > > I spoke with DB offline about this a little while ago and he
> >>> > confirmed
> >>> > >>> > that
> >>> > >>> > > he was able to access the jar from the driver.
> >>> > >>> > >
> >>> > >>> > > The issue appears to be a general Java issue: you can't
> directly
> >>> > >>> > > instantiate a class from a dynamically loaded jar.
> >>> > >>> > >
> >>> > >>> > > I reproduced it locally outside of Spark with:
> >>> > >>> > > ---
> >>> > >>> > >     URLClassLoader urlClassLoader = new URLClassLoader(new
> >>> URL[] {
> >>> > new
> >>> > >>> > > File("myotherjar.jar").toURI().toURL() }, null);
> >>> > >>> > >
> >>> Thread.currentThread().setContextClassLoader(urlClassLoader);
> >>> > >>> > >     MyClassFromMyOtherJar obj = new MyClassFromMyOtherJar();
> >>> > >>> > > ---
> >>> > >>> > >
> >>> > >>> > > I was able to load the class with reflection.
> >>> > >>> >
> >>> > >>>
> >>> >
> >>>
> >>
> >>
>

--001a11343920a2f2fb04f9eea2eb--

From dev-return-7759-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 20:40:51 2014
Return-Path: <dev-return-7759-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7F502118B6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 20:40:51 +0000 (UTC)
Received: (qmail 98142 invoked by uid 500); 21 May 2014 20:40:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98083 invoked by uid 500); 21 May 2014 20:40:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98075 invoked by uid 99); 21 May 2014 20:40:51 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 20:40:51 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 20:40:47 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id C74F5101AB9
	for <dev@spark.apache.org>; Wed, 21 May 2014 13:40:26 -0700 (PDT)
Received: from mail-qg0-f41.google.com (mail-qg0-f41.google.com [209.85.192.41])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id C35D61011A1
	for <dev@spark.apache.org>; Wed, 21 May 2014 13:40:08 -0700 (PDT)
Received: by mail-qg0-f41.google.com with SMTP id j5so4160006qga.14
        for <dev@spark.apache.org>; Wed, 21 May 2014 13:40:07 -0700 (PDT)
X-Gm-Message-State: ALoCoQncXgvhv/Dqt5HrVfIlPrdIQmsew9XHvsoLa4OYMPzh9OCyLPdAvfLqa5I5MnnZaBiYRcYt
MIME-Version: 1.0
X-Received: by 10.140.90.69 with SMTP id w63mr7356748qgd.52.1400704807915;
 Wed, 21 May 2014 13:40:07 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Wed, 21 May 2014 13:40:07 -0700 (PDT)
In-Reply-To: <CACBYxK+VOs2=m8tF1xijTOAxL917Q2yo=pOKNsNfzK52a7QdhQ@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
	<CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
	<CAMAsSdKxE-biW8fRqkRn6NmqsszFgQ=f1w_Z8yt8cSOj3qBB2Q@mail.gmail.com>
	<CAEYYnxY9UZDuhBoHAP5kD3dCXxd0TwR761tt-kswjTEddQCCKg@mail.gmail.com>
	<CA+-p3AGT8E5caKU+2ExxjbpV-UqaaTot+Ni3DXEdOf37tADLgw@mail.gmail.com>
	<CAMAsSdJVckDE5=3S8wZj9P1-1-D6qBumGnofk9G7A7cksh1RVA@mail.gmail.com>
	<CABPQxstfSyVu6D-FSt7qa-o1co9AkhHsncTHGBrCK4rEM7fymg@mail.gmail.com>
	<CACBYxKLb=OK2EKu=2yS_1tswYz52rBF-59+8DZL+DfG6qHF52Q@mail.gmail.com>
	<CAEYYnxakT5SVD0xxWcyAXq+w-f-tY0rDA2c3YLRXK6oZ3ArF7Q@mail.gmail.com>
	<CAEYYnxaYfaUOes+A4kR4w+nh0LS8fgJytM2Nj7xoBErpHiRwbA@mail.gmail.com>
	<CAJgQjQ_ifNtFFK4W99FA3cJ3BapRRfLc4FomCWxdgwTqEU+icg@mail.gmail.com>
	<CACBYxK+VOs2=m8tF1xijTOAxL917Q2yo=pOKNsNfzK52a7QdhQ@mail.gmail.com>
Date: Wed, 21 May 2014 13:40:07 -0700
Message-ID: <CAEYYnxbBW62gVrVtuQ1MPFPgBc4mRARB+=LyRm8b21by9cmFPQ@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c139ba83210a04f9ef0000
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c139ba83210a04f9ef0000
Content-Type: text/plain; charset=UTF-8

This will be another separate story.

Since in the yarn deployment, as Sandy said, the app.jar will be always in
the systemclassloader which means any object instantiated in app.jar will
have parent loader of systemclassloader instead of custom one. As a result,
the custom class loader in yarn will never work without specifically using
reflection.

Solution will be not using system classloader in the classloader hierarchy,
and add all the resources in system one into custom one. This is the
approach of tomcat takes.

Or we can directly overwirte the system class loader by calling the
protected method `addURL` which will not work and throw exception if the
code is wrapped in security manager.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Wed, May 21, 2014 at 1:13 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:

> This will solve the issue for jars added upon application submission, but,
> on top of this, we need to make sure that anything dynamically added
> through sc.addJar works as well.
>
> To do so, we need to make sure that any jars retrieved via the driver's
> HTTP server are loaded by the same classloader that loads the jars given on
> app submission.  To achieve this, we need to either use the same
> classloader for both system jars and user jars, or make sure that the user
> jars given on app submission are under the same classloader used for
> dynamically added jars.
>
> On Tue, May 20, 2014 at 5:59 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
> > Talked with Sandy and DB offline. I think the best solution is sending
> > the secondary jars to the distributed cache of all containers rather
> > than just the master, and set the classpath to include spark jar,
> > primary app jar, and secondary jars before executor starts. In this
> > way, user only needs to specify secondary jars via --jars instead of
> > calling sc.addJar inside the code. It also solves the scalability
> > problem of serving all the jars via http.
> >
> > If this solution sounds good, I can try to make a patch.
> >
> > Best,
> > Xiangrui
> >
> > On Mon, May 19, 2014 at 10:04 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> > > In 1.0, there is a new option for users to choose which classloader has
> > > higher priority via spark.files.userClassPathFirst, I decided to submit
> > the
> > > PR for 0.9 first. We use this patch in our lab and we can use those
> jars
> > > added by sc.addJar without reflection.
> > >
> > > https://github.com/apache/spark/pull/834
> > >
> > > Can anyone comment if it's a good approach?
> > >
> > > Thanks.
> > >
> > >
> > > Sincerely,
> > >
> > > DB Tsai
> > > -------------------------------------------------------
> > > My Blog: https://www.dbtsai.com
> > > LinkedIn: https://www.linkedin.com/in/dbtsai
> > >
> > >
> > > On Mon, May 19, 2014 at 7:42 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> > >
> > >> Good summary! We fixed it in branch 0.9 since our production is still
> in
> > >> 0.9. I'm porting it to 1.0 now, and hopefully will submit PR for 1.0
> > >> tonight.
> > >>
> > >>
> > >> Sincerely,
> > >>
> > >> DB Tsai
> > >> -------------------------------------------------------
> > >> My Blog: https://www.dbtsai.com
> > >> LinkedIn: https://www.linkedin.com/in/dbtsai
> > >>
> > >>
> > >> On Mon, May 19, 2014 at 7:38 PM, Sandy Ryza <sandy.ryza@cloudera.com
> > >wrote:
> > >>
> > >>> It just hit me why this problem is showing up on YARN and not on
> > >>> standalone.
> > >>>
> > >>> The relevant difference between YARN and standalone is that, on YARN,
> > the
> > >>> app jar is loaded by the system classloader instead of Spark's custom
> > URL
> > >>> classloader.
> > >>>
> > >>> On YARN, the system classloader knows about [the classes in the spark
> > >>> jars,
> > >>> the classes in the primary app jar].   The custom classloader knows
> > about
> > >>> [the classes in secondary app jars] and has the system classloader as
> > its
> > >>> parent.
> > >>>
> > >>> A few relevant facts (mostly redundant with what Sean pointed out):
> > >>> * Every class has a classloader that loaded it.
> > >>> * When an object of class B is instantiated inside of class A, the
> > >>> classloader used for loading B is the classloader that was used for
> > >>> loading
> > >>> A.
> > >>> * When a classloader fails to load a class, it lets its parent
> > classloader
> > >>> try.  If its parent succeeds, its parent becomes the "classloader
> that
> > >>> loaded it".
> > >>>
> > >>> So suppose class B is in a secondary app jar and class A is in the
> > primary
> > >>> app jar:
> > >>> 1. The custom classloader will try to load class A.
> > >>> 2. It will fail, because it only knows about the secondary jars.
> > >>> 3. It will delegate to its parent, the system classloader.
> > >>> 4. The system classloader will succeed, because it knows about the
> > primary
> > >>> app jar.
> > >>> 5. A's classloader will be the system classloader.
> > >>> 6. A tries to instantiate an instance of class B.
> > >>> 7. B will be loaded with A's classloader, which is the system
> > classloader.
> > >>> 8. Loading B will fail, because A's classloader, which is the system
> > >>> classloader, doesn't know about the secondary app jars.
> > >>>
> > >>> In Spark standalone, A and B are both loaded by the custom
> > classloader, so
> > >>> this issue doesn't come up.
> > >>>
> > >>> -Sandy
> > >>>
> > >>> On Mon, May 19, 2014 at 7:07 PM, Patrick Wendell <pwendell@gmail.com
> >
> > >>> wrote:
> > >>>
> > >>> > Having a user add define a custom class inside of an added jar and
> > >>> > instantiate it directly inside of an executor is definitely
> supported
> > >>> > in Spark and has been for a really long time (several years). This
> is
> > >>> > something we do all the time in Spark.
> > >>> >
> > >>> > DB - I'd hold off on a re-architecting of this until we identify
> > >>> > exactly what is causing the bug you are running into.
> > >>> >
> > >>> > In a nutshell, when the bytecode "new Foo()" is run on the
> executor,
> > >>> > it will ask the driver for the class over HTTP using a custom
> > >>> > classloader. Something in that pipeline is breaking here, possibly
> > >>> > related to the YARN deployment stuff.
> > >>> >
> > >>> >
> > >>> > On Mon, May 19, 2014 at 12:29 AM, Sean Owen <sowen@cloudera.com>
> > wrote:
> > >>> > > I don't think a customer classloader is necessary.
> > >>> > >
> > >>> > > Well, it occurs to me that this is no new problem. Hadoop,
> Tomcat,
> > etc
> > >>> > > all run custom user code that creates new user objects without
> > >>> > > reflection. I should go see how that's done. Maybe it's totally
> > valid
> > >>> > > to set the thread's context classloader for just this purpose,
> and
> > I
> > >>> > > am not thinking clearly.
> > >>> > >
> > >>> > > On Mon, May 19, 2014 at 8:26 AM, Andrew Ash <
> andrew@andrewash.com>
> > >>> > wrote:
> > >>> > >> Sounds like the problem is that classloaders always look in
> their
> > >>> > parents
> > >>> > >> before themselves, and Spark users want executors to pick up
> > classes
> > >>> > from
> > >>> > >> their custom code before the ones in Spark plus its
> dependencies.
> > >>> > >>
> > >>> > >> Would a custom classloader that delegates to the parent after
> > first
> > >>> > >> checking itself fix this up?
> > >>> > >>
> > >>> > >>
> > >>> > >> On Mon, May 19, 2014 at 12:17 AM, DB Tsai <dbtsai@stanford.edu>
> > >>> wrote:
> > >>> > >>
> > >>> > >>> Hi Sean,
> > >>> > >>>
> > >>> > >>> It's true that the issue here is classloader, and due to the
> > >>> > classloader
> > >>> > >>> delegation model, users have to use reflection in the executors
> > to
> > >>> > pick up
> > >>> > >>> the classloader in order to use those classes added by
> sc.addJars
> > >>> APIs.
> > >>> > >>> However, it's very inconvenience for users, and not documented
> in
> > >>> > spark.
> > >>> > >>>
> > >>> > >>> I'm working on a patch to solve it by calling the protected
> > method
> > >>> > addURL
> > >>> > >>> in URLClassLoader to update the current default classloader, so
> > no
> > >>> > >>> customClassLoader anymore. I wonder if this is an good way to
> go.
> > >>> > >>>
> > >>> > >>>   private def addURL(url: URL, loader: URLClassLoader){
> > >>> > >>>     try {
> > >>> > >>>       val method: Method =
> > >>> > >>> classOf[URLClassLoader].getDeclaredMethod("addURL",
> classOf[URL])
> > >>> > >>>       method.setAccessible(true)
> > >>> > >>>       method.invoke(loader, url)
> > >>> > >>>     }
> > >>> > >>>     catch {
> > >>> > >>>       case t: Throwable => {
> > >>> > >>>         throw new IOException("Error, could not add URL to
> system
> > >>> > >>> classloader")
> > >>> > >>>       }
> > >>> > >>>     }
> > >>> > >>>   }
> > >>> > >>>
> > >>> > >>>
> > >>> > >>>
> > >>> > >>> Sincerely,
> > >>> > >>>
> > >>> > >>> DB Tsai
> > >>> > >>> -------------------------------------------------------
> > >>> > >>> My Blog: https://www.dbtsai.com
> > >>> > >>> LinkedIn: https://www.linkedin.com/in/dbtsai
> > >>> > >>>
> > >>> > >>>
> > >>> > >>> On Sun, May 18, 2014 at 11:57 PM, Sean Owen <
> sowen@cloudera.com>
> > >>> > wrote:
> > >>> > >>>
> > >>> > >>> > I might be stating the obvious for everyone, but the issue
> > here is
> > >>> > not
> > >>> > >>> > reflection or the source of the JAR, but the ClassLoader. The
> > >>> basic
> > >>> > >>> > rules are this.
> > >>> > >>> >
> > >>> > >>> > "new Foo" will use the ClassLoader that defines Foo. This is
> > >>> usually
> > >>> > >>> > the ClassLoader that loaded whatever it is that first
> > referenced
> > >>> Foo
> > >>> > >>> > and caused it to be loaded -- usually the ClassLoader holding
> > your
> > >>> > >>> > other app classes.
> > >>> > >>> >
> > >>> > >>> > ClassLoaders can have a parent-child relationship.
> ClassLoaders
> > >>> > always
> > >>> > >>> > look in their parent before themselves.
> > >>> > >>> >
> > >>> > >>> > (Careful then -- in contexts like Hadoop or Tomcat where your
> > app
> > >>> is
> > >>> > >>> > loaded in a child ClassLoader, and you reference a class that
> > >>> Hadoop
> > >>> > >>> > or Tomcat also has (like a lib class) you will get the
> > container's
> > >>> > >>> > version!)
> > >>> > >>> >
> > >>> > >>> > When you load an external JAR it has a separate ClassLoader
> > which
> > >>> > does
> > >>> > >>> > not necessarily bear any relation to the one containing your
> > app
> > >>> > >>> > classes, so yeah it is not generally going to make "new Foo"
> > work.
> > >>> > >>> >
> > >>> > >>> > Reflection lets you pick the ClassLoader, yes.
> > >>> > >>> >
> > >>> > >>> > I would not call setContextClassLoader.
> > >>> > >>> >
> > >>> > >>> > On Mon, May 19, 2014 at 12:00 AM, Sandy Ryza <
> > >>> > sandy.ryza@cloudera.com>
> > >>> > >>> > wrote:
> > >>> > >>> > > I spoke with DB offline about this a little while ago and
> he
> > >>> > confirmed
> > >>> > >>> > that
> > >>> > >>> > > he was able to access the jar from the driver.
> > >>> > >>> > >
> > >>> > >>> > > The issue appears to be a general Java issue: you can't
> > directly
> > >>> > >>> > > instantiate a class from a dynamically loaded jar.
> > >>> > >>> > >
> > >>> > >>> > > I reproduced it locally outside of Spark with:
> > >>> > >>> > > ---
> > >>> > >>> > >     URLClassLoader urlClassLoader = new URLClassLoader(new
> > >>> URL[] {
> > >>> > new
> > >>> > >>> > > File("myotherjar.jar").toURI().toURL() }, null);
> > >>> > >>> > >
> > >>> Thread.currentThread().setContextClassLoader(urlClassLoader);
> > >>> > >>> > >     MyClassFromMyOtherJar obj = new
> MyClassFromMyOtherJar();
> > >>> > >>> > > ---
> > >>> > >>> > >
> > >>> > >>> > > I was able to load the class with reflection.
> > >>> > >>> >
> > >>> > >>>
> > >>> >
> > >>>
> > >>
> > >>
> >
>

--001a11c139ba83210a04f9ef0000--

From dev-return-7760-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 21:11:23 2014
Return-Path: <dev-return-7760-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5F44A119A9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 21:11:23 +0000 (UTC)
Received: (qmail 59076 invoked by uid 500); 21 May 2014 21:11:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59021 invoked by uid 500); 21 May 2014 21:11:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59013 invoked by uid 99); 21 May 2014 21:11:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 21:11:23 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 74.125.82.41 as permitted sender)
Received: from [74.125.82.41] (HELO mail-wg0-f41.google.com) (74.125.82.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 21:11:19 +0000
Received: by mail-wg0-f41.google.com with SMTP id z12so2505560wgg.0
        for <dev@spark.apache.org>; Wed, 21 May 2014 14:10:58 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=/048wjj/GsxICdeG6CgLK4AZjCneN8IQN6UDu7Oi5Wo=;
        b=0DHmtR4stgcOCvw5Mgfz+HZ2zVnV1P7NTGwjtsCQzsBxPyKOHNVIdJLUt+zTbMXOE+
         nPOtXkcDSNTke6U384NEfCTJZumSoTioPBnVffUJAaTmcIVXWhIh0aoaKJBgnHY8Dget
         0aZxbRIBhQPRseCedpzL4ZREAGbT88g6jS+G2st/Vs7Y6FIal8PrrgNxjtKRVCuzmv/j
         ogvRVzBNGi/O3ropjP0PbrpDU0KGuDwPENx28ULwSe5lnIo+hWGQLh59m3+5ZT+MjRBX
         2Rs4iMsX5drFwmTOahzmklix5nrPZCSLWukEUd144y/Q3ssW3ja+GJ1C55r+c/ykMCRb
         96kQ==
MIME-Version: 1.0
X-Received: by 10.180.73.201 with SMTP id n9mr12578771wiv.45.1400706658212;
 Wed, 21 May 2014 14:10:58 -0700 (PDT)
Received: by 10.194.187.20 with HTTP; Wed, 21 May 2014 14:10:58 -0700 (PDT)
In-Reply-To: <CAEYYnxbBW62gVrVtuQ1MPFPgBc4mRARB+=LyRm8b21by9cmFPQ@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
	<CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
	<CAMAsSdKxE-biW8fRqkRn6NmqsszFgQ=f1w_Z8yt8cSOj3qBB2Q@mail.gmail.com>
	<CAEYYnxY9UZDuhBoHAP5kD3dCXxd0TwR761tt-kswjTEddQCCKg@mail.gmail.com>
	<CA+-p3AGT8E5caKU+2ExxjbpV-UqaaTot+Ni3DXEdOf37tADLgw@mail.gmail.com>
	<CAMAsSdJVckDE5=3S8wZj9P1-1-D6qBumGnofk9G7A7cksh1RVA@mail.gmail.com>
	<CABPQxstfSyVu6D-FSt7qa-o1co9AkhHsncTHGBrCK4rEM7fymg@mail.gmail.com>
	<CACBYxKLb=OK2EKu=2yS_1tswYz52rBF-59+8DZL+DfG6qHF52Q@mail.gmail.com>
	<CAEYYnxakT5SVD0xxWcyAXq+w-f-tY0rDA2c3YLRXK6oZ3ArF7Q@mail.gmail.com>
	<CAEYYnxaYfaUOes+A4kR4w+nh0LS8fgJytM2Nj7xoBErpHiRwbA@mail.gmail.com>
	<CAJgQjQ_ifNtFFK4W99FA3cJ3BapRRfLc4FomCWxdgwTqEU+icg@mail.gmail.com>
	<CACBYxK+VOs2=m8tF1xijTOAxL917Q2yo=pOKNsNfzK52a7QdhQ@mail.gmail.com>
	<CAEYYnxbBW62gVrVtuQ1MPFPgBc4mRARB+=LyRm8b21by9cmFPQ@mail.gmail.com>
Date: Wed, 21 May 2014 14:10:58 -0700
Message-ID: <CAJgQjQ894Gz_StdJa+GgFXgXuEiq-5Z+4-3B0Bq9XXejzXCFXA@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I think adding jars dynamically should work as long as the primary jar
and the secondary jars do not depend on dynamically added jars, which
should be the correct logic. -Xiangrui

On Wed, May 21, 2014 at 1:40 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> This will be another separate story.
>
> Since in the yarn deployment, as Sandy said, the app.jar will be always in
> the systemclassloader which means any object instantiated in app.jar will
> have parent loader of systemclassloader instead of custom one. As a result,
> the custom class loader in yarn will never work without specifically using
> reflection.
>
> Solution will be not using system classloader in the classloader hierarchy,
> and add all the resources in system one into custom one. This is the
> approach of tomcat takes.
>
> Or we can directly overwirte the system class loader by calling the
> protected method `addURL` which will not work and throw exception if the
> code is wrapped in security manager.
>
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Wed, May 21, 2014 at 1:13 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
>
>> This will solve the issue for jars added upon application submission, but,
>> on top of this, we need to make sure that anything dynamically added
>> through sc.addJar works as well.
>>
>> To do so, we need to make sure that any jars retrieved via the driver's
>> HTTP server are loaded by the same classloader that loads the jars given on
>> app submission.  To achieve this, we need to either use the same
>> classloader for both system jars and user jars, or make sure that the user
>> jars given on app submission are under the same classloader used for
>> dynamically added jars.
>>
>> On Tue, May 20, 2014 at 5:59 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>
>> > Talked with Sandy and DB offline. I think the best solution is sending
>> > the secondary jars to the distributed cache of all containers rather
>> > than just the master, and set the classpath to include spark jar,
>> > primary app jar, and secondary jars before executor starts. In this
>> > way, user only needs to specify secondary jars via --jars instead of
>> > calling sc.addJar inside the code. It also solves the scalability
>> > problem of serving all the jars via http.
>> >
>> > If this solution sounds good, I can try to make a patch.
>> >
>> > Best,
>> > Xiangrui
>> >
>> > On Mon, May 19, 2014 at 10:04 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>> > > In 1.0, there is a new option for users to choose which classloader has
>> > > higher priority via spark.files.userClassPathFirst, I decided to submit
>> > the
>> > > PR for 0.9 first. We use this patch in our lab and we can use those
>> jars
>> > > added by sc.addJar without reflection.
>> > >
>> > > https://github.com/apache/spark/pull/834
>> > >
>> > > Can anyone comment if it's a good approach?
>> > >
>> > > Thanks.
>> > >
>> > >
>> > > Sincerely,
>> > >
>> > > DB Tsai
>> > > -------------------------------------------------------
>> > > My Blog: https://www.dbtsai.com
>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>> > >
>> > >
>> > > On Mon, May 19, 2014 at 7:42 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>> > >
>> > >> Good summary! We fixed it in branch 0.9 since our production is still
>> in
>> > >> 0.9. I'm porting it to 1.0 now, and hopefully will submit PR for 1.0
>> > >> tonight.
>> > >>
>> > >>
>> > >> Sincerely,
>> > >>
>> > >> DB Tsai
>> > >> -------------------------------------------------------
>> > >> My Blog: https://www.dbtsai.com
>> > >> LinkedIn: https://www.linkedin.com/in/dbtsai
>> > >>
>> > >>
>> > >> On Mon, May 19, 2014 at 7:38 PM, Sandy Ryza <sandy.ryza@cloudera.com
>> > >wrote:
>> > >>
>> > >>> It just hit me why this problem is showing up on YARN and not on
>> > >>> standalone.
>> > >>>
>> > >>> The relevant difference between YARN and standalone is that, on YARN,
>> > the
>> > >>> app jar is loaded by the system classloader instead of Spark's custom
>> > URL
>> > >>> classloader.
>> > >>>
>> > >>> On YARN, the system classloader knows about [the classes in the spark
>> > >>> jars,
>> > >>> the classes in the primary app jar].   The custom classloader knows
>> > about
>> > >>> [the classes in secondary app jars] and has the system classloader as
>> > its
>> > >>> parent.
>> > >>>
>> > >>> A few relevant facts (mostly redundant with what Sean pointed out):
>> > >>> * Every class has a classloader that loaded it.
>> > >>> * When an object of class B is instantiated inside of class A, the
>> > >>> classloader used for loading B is the classloader that was used for
>> > >>> loading
>> > >>> A.
>> > >>> * When a classloader fails to load a class, it lets its parent
>> > classloader
>> > >>> try.  If its parent succeeds, its parent becomes the "classloader
>> that
>> > >>> loaded it".
>> > >>>
>> > >>> So suppose class B is in a secondary app jar and class A is in the
>> > primary
>> > >>> app jar:
>> > >>> 1. The custom classloader will try to load class A.
>> > >>> 2. It will fail, because it only knows about the secondary jars.
>> > >>> 3. It will delegate to its parent, the system classloader.
>> > >>> 4. The system classloader will succeed, because it knows about the
>> > primary
>> > >>> app jar.
>> > >>> 5. A's classloader will be the system classloader.
>> > >>> 6. A tries to instantiate an instance of class B.
>> > >>> 7. B will be loaded with A's classloader, which is the system
>> > classloader.
>> > >>> 8. Loading B will fail, because A's classloader, which is the system
>> > >>> classloader, doesn't know about the secondary app jars.
>> > >>>
>> > >>> In Spark standalone, A and B are both loaded by the custom
>> > classloader, so
>> > >>> this issue doesn't come up.
>> > >>>
>> > >>> -Sandy
>> > >>>
>> > >>> On Mon, May 19, 2014 at 7:07 PM, Patrick Wendell <pwendell@gmail.com
>> >
>> > >>> wrote:
>> > >>>
>> > >>> > Having a user add define a custom class inside of an added jar and
>> > >>> > instantiate it directly inside of an executor is definitely
>> supported
>> > >>> > in Spark and has been for a really long time (several years). This
>> is
>> > >>> > something we do all the time in Spark.
>> > >>> >
>> > >>> > DB - I'd hold off on a re-architecting of this until we identify
>> > >>> > exactly what is causing the bug you are running into.
>> > >>> >
>> > >>> > In a nutshell, when the bytecode "new Foo()" is run on the
>> executor,
>> > >>> > it will ask the driver for the class over HTTP using a custom
>> > >>> > classloader. Something in that pipeline is breaking here, possibly
>> > >>> > related to the YARN deployment stuff.
>> > >>> >
>> > >>> >
>> > >>> > On Mon, May 19, 2014 at 12:29 AM, Sean Owen <sowen@cloudera.com>
>> > wrote:
>> > >>> > > I don't think a customer classloader is necessary.
>> > >>> > >
>> > >>> > > Well, it occurs to me that this is no new problem. Hadoop,
>> Tomcat,
>> > etc
>> > >>> > > all run custom user code that creates new user objects without
>> > >>> > > reflection. I should go see how that's done. Maybe it's totally
>> > valid
>> > >>> > > to set the thread's context classloader for just this purpose,
>> and
>> > I
>> > >>> > > am not thinking clearly.
>> > >>> > >
>> > >>> > > On Mon, May 19, 2014 at 8:26 AM, Andrew Ash <
>> andrew@andrewash.com>
>> > >>> > wrote:
>> > >>> > >> Sounds like the problem is that classloaders always look in
>> their
>> > >>> > parents
>> > >>> > >> before themselves, and Spark users want executors to pick up
>> > classes
>> > >>> > from
>> > >>> > >> their custom code before the ones in Spark plus its
>> dependencies.
>> > >>> > >>
>> > >>> > >> Would a custom classloader that delegates to the parent after
>> > first
>> > >>> > >> checking itself fix this up?
>> > >>> > >>
>> > >>> > >>
>> > >>> > >> On Mon, May 19, 2014 at 12:17 AM, DB Tsai <dbtsai@stanford.edu>
>> > >>> wrote:
>> > >>> > >>
>> > >>> > >>> Hi Sean,
>> > >>> > >>>
>> > >>> > >>> It's true that the issue here is classloader, and due to the
>> > >>> > classloader
>> > >>> > >>> delegation model, users have to use reflection in the executors
>> > to
>> > >>> > pick up
>> > >>> > >>> the classloader in order to use those classes added by
>> sc.addJars
>> > >>> APIs.
>> > >>> > >>> However, it's very inconvenience for users, and not documented
>> in
>> > >>> > spark.
>> > >>> > >>>
>> > >>> > >>> I'm working on a patch to solve it by calling the protected
>> > method
>> > >>> > addURL
>> > >>> > >>> in URLClassLoader to update the current default classloader, so
>> > no
>> > >>> > >>> customClassLoader anymore. I wonder if this is an good way to
>> go.
>> > >>> > >>>
>> > >>> > >>>   private def addURL(url: URL, loader: URLClassLoader){
>> > >>> > >>>     try {
>> > >>> > >>>       val method: Method =
>> > >>> > >>> classOf[URLClassLoader].getDeclaredMethod("addURL",
>> classOf[URL])
>> > >>> > >>>       method.setAccessible(true)
>> > >>> > >>>       method.invoke(loader, url)
>> > >>> > >>>     }
>> > >>> > >>>     catch {
>> > >>> > >>>       case t: Throwable => {
>> > >>> > >>>         throw new IOException("Error, could not add URL to
>> system
>> > >>> > >>> classloader")
>> > >>> > >>>       }
>> > >>> > >>>     }
>> > >>> > >>>   }
>> > >>> > >>>
>> > >>> > >>>
>> > >>> > >>>
>> > >>> > >>> Sincerely,
>> > >>> > >>>
>> > >>> > >>> DB Tsai
>> > >>> > >>> -------------------------------------------------------
>> > >>> > >>> My Blog: https://www.dbtsai.com
>> > >>> > >>> LinkedIn: https://www.linkedin.com/in/dbtsai
>> > >>> > >>>
>> > >>> > >>>
>> > >>> > >>> On Sun, May 18, 2014 at 11:57 PM, Sean Owen <
>> sowen@cloudera.com>
>> > >>> > wrote:
>> > >>> > >>>
>> > >>> > >>> > I might be stating the obvious for everyone, but the issue
>> > here is
>> > >>> > not
>> > >>> > >>> > reflection or the source of the JAR, but the ClassLoader. The
>> > >>> basic
>> > >>> > >>> > rules are this.
>> > >>> > >>> >
>> > >>> > >>> > "new Foo" will use the ClassLoader that defines Foo. This is
>> > >>> usually
>> > >>> > >>> > the ClassLoader that loaded whatever it is that first
>> > referenced
>> > >>> Foo
>> > >>> > >>> > and caused it to be loaded -- usually the ClassLoader holding
>> > your
>> > >>> > >>> > other app classes.
>> > >>> > >>> >
>> > >>> > >>> > ClassLoaders can have a parent-child relationship.
>> ClassLoaders
>> > >>> > always
>> > >>> > >>> > look in their parent before themselves.
>> > >>> > >>> >
>> > >>> > >>> > (Careful then -- in contexts like Hadoop or Tomcat where your
>> > app
>> > >>> is
>> > >>> > >>> > loaded in a child ClassLoader, and you reference a class that
>> > >>> Hadoop
>> > >>> > >>> > or Tomcat also has (like a lib class) you will get the
>> > container's
>> > >>> > >>> > version!)
>> > >>> > >>> >
>> > >>> > >>> > When you load an external JAR it has a separate ClassLoader
>> > which
>> > >>> > does
>> > >>> > >>> > not necessarily bear any relation to the one containing your
>> > app
>> > >>> > >>> > classes, so yeah it is not generally going to make "new Foo"
>> > work.
>> > >>> > >>> >
>> > >>> > >>> > Reflection lets you pick the ClassLoader, yes.
>> > >>> > >>> >
>> > >>> > >>> > I would not call setContextClassLoader.
>> > >>> > >>> >
>> > >>> > >>> > On Mon, May 19, 2014 at 12:00 AM, Sandy Ryza <
>> > >>> > sandy.ryza@cloudera.com>
>> > >>> > >>> > wrote:
>> > >>> > >>> > > I spoke with DB offline about this a little while ago and
>> he
>> > >>> > confirmed
>> > >>> > >>> > that
>> > >>> > >>> > > he was able to access the jar from the driver.
>> > >>> > >>> > >
>> > >>> > >>> > > The issue appears to be a general Java issue: you can't
>> > directly
>> > >>> > >>> > > instantiate a class from a dynamically loaded jar.
>> > >>> > >>> > >
>> > >>> > >>> > > I reproduced it locally outside of Spark with:
>> > >>> > >>> > > ---
>> > >>> > >>> > >     URLClassLoader urlClassLoader = new URLClassLoader(new
>> > >>> URL[] {
>> > >>> > new
>> > >>> > >>> > > File("myotherjar.jar").toURI().toURL() }, null);
>> > >>> > >>> > >
>> > >>> Thread.currentThread().setContextClassLoader(urlClassLoader);
>> > >>> > >>> > >     MyClassFromMyOtherJar obj = new
>> MyClassFromMyOtherJar();
>> > >>> > >>> > > ---
>> > >>> > >>> > >
>> > >>> > >>> > > I was able to load the class with reflection.
>> > >>> > >>> >
>> > >>> > >>>
>> > >>> >
>> > >>>
>> > >>
>> > >>
>> >
>>

From dev-return-7761-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 21:16:52 2014
Return-Path: <dev-return-7761-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D1602119D4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 21:16:52 +0000 (UTC)
Received: (qmail 73130 invoked by uid 500); 21 May 2014 21:16:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73070 invoked by uid 500); 21 May 2014 21:16:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73062 invoked by uid 99); 21 May 2014 21:16:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 21:16:52 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 21:16:48 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 417C61019E1
	for <dev@spark.apache.org>; Wed, 21 May 2014 14:16:24 -0700 (PDT)
Received: from mail-qg0-f43.google.com (mail-qg0-f43.google.com [209.85.192.43])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 50564101764
	for <dev@spark.apache.org>; Wed, 21 May 2014 14:16:23 -0700 (PDT)
Received: by mail-qg0-f43.google.com with SMTP id 63so4204106qgz.16
        for <dev@spark.apache.org>; Wed, 21 May 2014 14:16:22 -0700 (PDT)
X-Gm-Message-State: ALoCoQlAM+h3UOyN4DdAWtNSBvznqJ4kkfhBFWZcD/olbSo0I6QXEkb4FCiQy9Yt0rwKcqHtk3n2
MIME-Version: 1.0
X-Received: by 10.140.105.119 with SMTP id b110mr70782382qgf.28.1400706982444;
 Wed, 21 May 2014 14:16:22 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Wed, 21 May 2014 14:16:22 -0700 (PDT)
In-Reply-To: <CAJgQjQ894Gz_StdJa+GgFXgXuEiq-5Z+4-3B0Bq9XXejzXCFXA@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
	<CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
	<CAMAsSdKxE-biW8fRqkRn6NmqsszFgQ=f1w_Z8yt8cSOj3qBB2Q@mail.gmail.com>
	<CAEYYnxY9UZDuhBoHAP5kD3dCXxd0TwR761tt-kswjTEddQCCKg@mail.gmail.com>
	<CA+-p3AGT8E5caKU+2ExxjbpV-UqaaTot+Ni3DXEdOf37tADLgw@mail.gmail.com>
	<CAMAsSdJVckDE5=3S8wZj9P1-1-D6qBumGnofk9G7A7cksh1RVA@mail.gmail.com>
	<CABPQxstfSyVu6D-FSt7qa-o1co9AkhHsncTHGBrCK4rEM7fymg@mail.gmail.com>
	<CACBYxKLb=OK2EKu=2yS_1tswYz52rBF-59+8DZL+DfG6qHF52Q@mail.gmail.com>
	<CAEYYnxakT5SVD0xxWcyAXq+w-f-tY0rDA2c3YLRXK6oZ3ArF7Q@mail.gmail.com>
	<CAEYYnxaYfaUOes+A4kR4w+nh0LS8fgJytM2Nj7xoBErpHiRwbA@mail.gmail.com>
	<CAJgQjQ_ifNtFFK4W99FA3cJ3BapRRfLc4FomCWxdgwTqEU+icg@mail.gmail.com>
	<CACBYxK+VOs2=m8tF1xijTOAxL917Q2yo=pOKNsNfzK52a7QdhQ@mail.gmail.com>
	<CAEYYnxbBW62gVrVtuQ1MPFPgBc4mRARB+=LyRm8b21by9cmFPQ@mail.gmail.com>
	<CAJgQjQ894Gz_StdJa+GgFXgXuEiq-5Z+4-3B0Bq9XXejzXCFXA@mail.gmail.com>
Date: Wed, 21 May 2014 14:16:22 -0700
Message-ID: <CAEYYnxYVdWQ=DQVn57nKotv-nyCYkv5jgorTmrTq0B68zkVAUg@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113944021f978704f9ef823e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113944021f978704f9ef823e
Content-Type: text/plain; charset=UTF-8

How about the jars added dynamically? Those will be in customLoader's
classpath but not in the system one. Unfortunately, when we reference to
those jars added dynamically in primary jar, the default classloader will
be the system one not the custom one.

It works in standalone mode since the primary jar is not in the system
loader but custom one, so when we reference those jars added dynamically,
we can find it without reflection.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Wed, May 21, 2014 at 2:10 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> I think adding jars dynamically should work as long as the primary jar
> and the secondary jars do not depend on dynamically added jars, which
> should be the correct logic. -Xiangrui
>
> On Wed, May 21, 2014 at 1:40 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> > This will be another separate story.
> >
> > Since in the yarn deployment, as Sandy said, the app.jar will be always
> in
> > the systemclassloader which means any object instantiated in app.jar will
> > have parent loader of systemclassloader instead of custom one. As a
> result,
> > the custom class loader in yarn will never work without specifically
> using
> > reflection.
> >
> > Solution will be not using system classloader in the classloader
> hierarchy,
> > and add all the resources in system one into custom one. This is the
> > approach of tomcat takes.
> >
> > Or we can directly overwirte the system class loader by calling the
> > protected method `addURL` which will not work and throw exception if the
> > code is wrapped in security manager.
> >
> >
> > Sincerely,
> >
> > DB Tsai
> > -------------------------------------------------------
> > My Blog: https://www.dbtsai.com
> > LinkedIn: https://www.linkedin.com/in/dbtsai
> >
> >
> > On Wed, May 21, 2014 at 1:13 PM, Sandy Ryza <sandy.ryza@cloudera.com>
> wrote:
> >
> >> This will solve the issue for jars added upon application submission,
> but,
> >> on top of this, we need to make sure that anything dynamically added
> >> through sc.addJar works as well.
> >>
> >> To do so, we need to make sure that any jars retrieved via the driver's
> >> HTTP server are loaded by the same classloader that loads the jars
> given on
> >> app submission.  To achieve this, we need to either use the same
> >> classloader for both system jars and user jars, or make sure that the
> user
> >> jars given on app submission are under the same classloader used for
> >> dynamically added jars.
> >>
> >> On Tue, May 20, 2014 at 5:59 PM, Xiangrui Meng <mengxr@gmail.com>
> wrote:
> >>
> >> > Talked with Sandy and DB offline. I think the best solution is sending
> >> > the secondary jars to the distributed cache of all containers rather
> >> > than just the master, and set the classpath to include spark jar,
> >> > primary app jar, and secondary jars before executor starts. In this
> >> > way, user only needs to specify secondary jars via --jars instead of
> >> > calling sc.addJar inside the code. It also solves the scalability
> >> > problem of serving all the jars via http.
> >> >
> >> > If this solution sounds good, I can try to make a patch.
> >> >
> >> > Best,
> >> > Xiangrui
> >> >
> >> > On Mon, May 19, 2014 at 10:04 PM, DB Tsai <dbtsai@stanford.edu>
> wrote:
> >> > > In 1.0, there is a new option for users to choose which classloader
> has
> >> > > higher priority via spark.files.userClassPathFirst, I decided to
> submit
> >> > the
> >> > > PR for 0.9 first. We use this patch in our lab and we can use those
> >> jars
> >> > > added by sc.addJar without reflection.
> >> > >
> >> > > https://github.com/apache/spark/pull/834
> >> > >
> >> > > Can anyone comment if it's a good approach?
> >> > >
> >> > > Thanks.
> >> > >
> >> > >
> >> > > Sincerely,
> >> > >
> >> > > DB Tsai
> >> > > -------------------------------------------------------
> >> > > My Blog: https://www.dbtsai.com
> >> > > LinkedIn: https://www.linkedin.com/in/dbtsai
> >> > >
> >> > >
> >> > > On Mon, May 19, 2014 at 7:42 PM, DB Tsai <dbtsai@stanford.edu>
> wrote:
> >> > >
> >> > >> Good summary! We fixed it in branch 0.9 since our production is
> still
> >> in
> >> > >> 0.9. I'm porting it to 1.0 now, and hopefully will submit PR for
> 1.0
> >> > >> tonight.
> >> > >>
> >> > >>
> >> > >> Sincerely,
> >> > >>
> >> > >> DB Tsai
> >> > >> -------------------------------------------------------
> >> > >> My Blog: https://www.dbtsai.com
> >> > >> LinkedIn: https://www.linkedin.com/in/dbtsai
> >> > >>
> >> > >>
> >> > >> On Mon, May 19, 2014 at 7:38 PM, Sandy Ryza <
> sandy.ryza@cloudera.com
> >> > >wrote:
> >> > >>
> >> > >>> It just hit me why this problem is showing up on YARN and not on
> >> > >>> standalone.
> >> > >>>
> >> > >>> The relevant difference between YARN and standalone is that, on
> YARN,
> >> > the
> >> > >>> app jar is loaded by the system classloader instead of Spark's
> custom
> >> > URL
> >> > >>> classloader.
> >> > >>>
> >> > >>> On YARN, the system classloader knows about [the classes in the
> spark
> >> > >>> jars,
> >> > >>> the classes in the primary app jar].   The custom classloader
> knows
> >> > about
> >> > >>> [the classes in secondary app jars] and has the system
> classloader as
> >> > its
> >> > >>> parent.
> >> > >>>
> >> > >>> A few relevant facts (mostly redundant with what Sean pointed
> out):
> >> > >>> * Every class has a classloader that loaded it.
> >> > >>> * When an object of class B is instantiated inside of class A, the
> >> > >>> classloader used for loading B is the classloader that was used
> for
> >> > >>> loading
> >> > >>> A.
> >> > >>> * When a classloader fails to load a class, it lets its parent
> >> > classloader
> >> > >>> try.  If its parent succeeds, its parent becomes the "classloader
> >> that
> >> > >>> loaded it".
> >> > >>>
> >> > >>> So suppose class B is in a secondary app jar and class A is in the
> >> > primary
> >> > >>> app jar:
> >> > >>> 1. The custom classloader will try to load class A.
> >> > >>> 2. It will fail, because it only knows about the secondary jars.
> >> > >>> 3. It will delegate to its parent, the system classloader.
> >> > >>> 4. The system classloader will succeed, because it knows about the
> >> > primary
> >> > >>> app jar.
> >> > >>> 5. A's classloader will be the system classloader.
> >> > >>> 6. A tries to instantiate an instance of class B.
> >> > >>> 7. B will be loaded with A's classloader, which is the system
> >> > classloader.
> >> > >>> 8. Loading B will fail, because A's classloader, which is the
> system
> >> > >>> classloader, doesn't know about the secondary app jars.
> >> > >>>
> >> > >>> In Spark standalone, A and B are both loaded by the custom
> >> > classloader, so
> >> > >>> this issue doesn't come up.
> >> > >>>
> >> > >>> -Sandy
> >> > >>>
> >> > >>> On Mon, May 19, 2014 at 7:07 PM, Patrick Wendell <
> pwendell@gmail.com
> >> >
> >> > >>> wrote:
> >> > >>>
> >> > >>> > Having a user add define a custom class inside of an added jar
> and
> >> > >>> > instantiate it directly inside of an executor is definitely
> >> supported
> >> > >>> > in Spark and has been for a really long time (several years).
> This
> >> is
> >> > >>> > something we do all the time in Spark.
> >> > >>> >
> >> > >>> > DB - I'd hold off on a re-architecting of this until we identify
> >> > >>> > exactly what is causing the bug you are running into.
> >> > >>> >
> >> > >>> > In a nutshell, when the bytecode "new Foo()" is run on the
> >> executor,
> >> > >>> > it will ask the driver for the class over HTTP using a custom
> >> > >>> > classloader. Something in that pipeline is breaking here,
> possibly
> >> > >>> > related to the YARN deployment stuff.
> >> > >>> >
> >> > >>> >
> >> > >>> > On Mon, May 19, 2014 at 12:29 AM, Sean Owen <sowen@cloudera.com
> >
> >> > wrote:
> >> > >>> > > I don't think a customer classloader is necessary.
> >> > >>> > >
> >> > >>> > > Well, it occurs to me that this is no new problem. Hadoop,
> >> Tomcat,
> >> > etc
> >> > >>> > > all run custom user code that creates new user objects without
> >> > >>> > > reflection. I should go see how that's done. Maybe it's
> totally
> >> > valid
> >> > >>> > > to set the thread's context classloader for just this purpose,
> >> and
> >> > I
> >> > >>> > > am not thinking clearly.
> >> > >>> > >
> >> > >>> > > On Mon, May 19, 2014 at 8:26 AM, Andrew Ash <
> >> andrew@andrewash.com>
> >> > >>> > wrote:
> >> > >>> > >> Sounds like the problem is that classloaders always look in
> >> their
> >> > >>> > parents
> >> > >>> > >> before themselves, and Spark users want executors to pick up
> >> > classes
> >> > >>> > from
> >> > >>> > >> their custom code before the ones in Spark plus its
> >> dependencies.
> >> > >>> > >>
> >> > >>> > >> Would a custom classloader that delegates to the parent after
> >> > first
> >> > >>> > >> checking itself fix this up?
> >> > >>> > >>
> >> > >>> > >>
> >> > >>> > >> On Mon, May 19, 2014 at 12:17 AM, DB Tsai <
> dbtsai@stanford.edu>
> >> > >>> wrote:
> >> > >>> > >>
> >> > >>> > >>> Hi Sean,
> >> > >>> > >>>
> >> > >>> > >>> It's true that the issue here is classloader, and due to the
> >> > >>> > classloader
> >> > >>> > >>> delegation model, users have to use reflection in the
> executors
> >> > to
> >> > >>> > pick up
> >> > >>> > >>> the classloader in order to use those classes added by
> >> sc.addJars
> >> > >>> APIs.
> >> > >>> > >>> However, it's very inconvenience for users, and not
> documented
> >> in
> >> > >>> > spark.
> >> > >>> > >>>
> >> > >>> > >>> I'm working on a patch to solve it by calling the protected
> >> > method
> >> > >>> > addURL
> >> > >>> > >>> in URLClassLoader to update the current default
> classloader, so
> >> > no
> >> > >>> > >>> customClassLoader anymore. I wonder if this is an good way
> to
> >> go.
> >> > >>> > >>>
> >> > >>> > >>>   private def addURL(url: URL, loader: URLClassLoader){
> >> > >>> > >>>     try {
> >> > >>> > >>>       val method: Method =
> >> > >>> > >>> classOf[URLClassLoader].getDeclaredMethod("addURL",
> >> classOf[URL])
> >> > >>> > >>>       method.setAccessible(true)
> >> > >>> > >>>       method.invoke(loader, url)
> >> > >>> > >>>     }
> >> > >>> > >>>     catch {
> >> > >>> > >>>       case t: Throwable => {
> >> > >>> > >>>         throw new IOException("Error, could not add URL to
> >> system
> >> > >>> > >>> classloader")
> >> > >>> > >>>       }
> >> > >>> > >>>     }
> >> > >>> > >>>   }
> >> > >>> > >>>
> >> > >>> > >>>
> >> > >>> > >>>
> >> > >>> > >>> Sincerely,
> >> > >>> > >>>
> >> > >>> > >>> DB Tsai
> >> > >>> > >>> -------------------------------------------------------
> >> > >>> > >>> My Blog: https://www.dbtsai.com
> >> > >>> > >>> LinkedIn: https://www.linkedin.com/in/dbtsai
> >> > >>> > >>>
> >> > >>> > >>>
> >> > >>> > >>> On Sun, May 18, 2014 at 11:57 PM, Sean Owen <
> >> sowen@cloudera.com>
> >> > >>> > wrote:
> >> > >>> > >>>
> >> > >>> > >>> > I might be stating the obvious for everyone, but the issue
> >> > here is
> >> > >>> > not
> >> > >>> > >>> > reflection or the source of the JAR, but the ClassLoader.
> The
> >> > >>> basic
> >> > >>> > >>> > rules are this.
> >> > >>> > >>> >
> >> > >>> > >>> > "new Foo" will use the ClassLoader that defines Foo. This
> is
> >> > >>> usually
> >> > >>> > >>> > the ClassLoader that loaded whatever it is that first
> >> > referenced
> >> > >>> Foo
> >> > >>> > >>> > and caused it to be loaded -- usually the ClassLoader
> holding
> >> > your
> >> > >>> > >>> > other app classes.
> >> > >>> > >>> >
> >> > >>> > >>> > ClassLoaders can have a parent-child relationship.
> >> ClassLoaders
> >> > >>> > always
> >> > >>> > >>> > look in their parent before themselves.
> >> > >>> > >>> >
> >> > >>> > >>> > (Careful then -- in contexts like Hadoop or Tomcat where
> your
> >> > app
> >> > >>> is
> >> > >>> > >>> > loaded in a child ClassLoader, and you reference a class
> that
> >> > >>> Hadoop
> >> > >>> > >>> > or Tomcat also has (like a lib class) you will get the
> >> > container's
> >> > >>> > >>> > version!)
> >> > >>> > >>> >
> >> > >>> > >>> > When you load an external JAR it has a separate
> ClassLoader
> >> > which
> >> > >>> > does
> >> > >>> > >>> > not necessarily bear any relation to the one containing
> your
> >> > app
> >> > >>> > >>> > classes, so yeah it is not generally going to make "new
> Foo"
> >> > work.
> >> > >>> > >>> >
> >> > >>> > >>> > Reflection lets you pick the ClassLoader, yes.
> >> > >>> > >>> >
> >> > >>> > >>> > I would not call setContextClassLoader.
> >> > >>> > >>> >
> >> > >>> > >>> > On Mon, May 19, 2014 at 12:00 AM, Sandy Ryza <
> >> > >>> > sandy.ryza@cloudera.com>
> >> > >>> > >>> > wrote:
> >> > >>> > >>> > > I spoke with DB offline about this a little while ago
> and
> >> he
> >> > >>> > confirmed
> >> > >>> > >>> > that
> >> > >>> > >>> > > he was able to access the jar from the driver.
> >> > >>> > >>> > >
> >> > >>> > >>> > > The issue appears to be a general Java issue: you can't
> >> > directly
> >> > >>> > >>> > > instantiate a class from a dynamically loaded jar.
> >> > >>> > >>> > >
> >> > >>> > >>> > > I reproduced it locally outside of Spark with:
> >> > >>> > >>> > > ---
> >> > >>> > >>> > >     URLClassLoader urlClassLoader = new
> URLClassLoader(new
> >> > >>> URL[] {
> >> > >>> > new
> >> > >>> > >>> > > File("myotherjar.jar").toURI().toURL() }, null);
> >> > >>> > >>> > >
> >> > >>> Thread.currentThread().setContextClassLoader(urlClassLoader);
> >> > >>> > >>> > >     MyClassFromMyOtherJar obj = new
> >> MyClassFromMyOtherJar();
> >> > >>> > >>> > > ---
> >> > >>> > >>> > >
> >> > >>> > >>> > > I was able to load the class with reflection.
> >> > >>> > >>> >
> >> > >>> > >>>
> >> > >>> >
> >> > >>>
> >> > >>
> >> > >>
> >> >
> >>
>

--001a113944021f978704f9ef823e--

From dev-return-7762-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 21:27:41 2014
Return-Path: <dev-return-7762-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C080C11A35
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 21:27:41 +0000 (UTC)
Received: (qmail 97699 invoked by uid 500); 21 May 2014 21:27:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97639 invoked by uid 500); 21 May 2014 21:27:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 97631 invoked by uid 99); 21 May 2014 21:27:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 21:27:41 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.174 as permitted sender)
Received: from [209.85.216.174] (HELO mail-qc0-f174.google.com) (209.85.216.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 21:27:37 +0000
Received: by mail-qc0-f174.google.com with SMTP id x13so4149467qcv.5
        for <dev@spark.apache.org>; Wed, 21 May 2014 14:27:16 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=jlcPFI5Z5jOVfEQGq90hQ1nWCoSgBHkXWhTV8XWxn9I=;
        b=mPdCQZexkchdBj29fo7sQCsAE+0eiNnpbH7xUflI2k42ZgVZq7BPuUFF9tjmgnTBNx
         QwacCAKlB787mAHtpa5KawpiFzeezBcsS3wg0XdE1VHiw5D1SqAySTma6C26EzLZExaN
         4y3MET6eSe49tQaEguNREV99aeflIBhhvm75TW/7jsMzCwZKq+JV5+rjhrUei3R7BRwD
         I2Dq0Lo8r0WeIHoAx+JxMbYberoEQDa8jU7oMHrAHCscG3aneQy2YiOM/DaO7ecIElxO
         5NuyjMCaKBDTAdY7VEbvVF4GqruX5iwpenxlBzOqYwofH9e3VjDVdZWu2MSQ6CJrxZaB
         D0bg==
X-Gm-Message-State: ALoCoQl8nKJWMwffOhwvbhOQjJefjphKeWv/FbCbckusmTs/fHmDnKNH4v6WXxuKFNyZSr56ewa0
MIME-Version: 1.0
X-Received: by 10.140.94.179 with SMTP id g48mr70381928qge.58.1400707636648;
 Wed, 21 May 2014 14:27:16 -0700 (PDT)
Received: by 10.140.106.202 with HTTP; Wed, 21 May 2014 14:27:16 -0700 (PDT)
In-Reply-To: <CAJgQjQ894Gz_StdJa+GgFXgXuEiq-5Z+4-3B0Bq9XXejzXCFXA@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
	<CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
	<CAMAsSdKxE-biW8fRqkRn6NmqsszFgQ=f1w_Z8yt8cSOj3qBB2Q@mail.gmail.com>
	<CAEYYnxY9UZDuhBoHAP5kD3dCXxd0TwR761tt-kswjTEddQCCKg@mail.gmail.com>
	<CA+-p3AGT8E5caKU+2ExxjbpV-UqaaTot+Ni3DXEdOf37tADLgw@mail.gmail.com>
	<CAMAsSdJVckDE5=3S8wZj9P1-1-D6qBumGnofk9G7A7cksh1RVA@mail.gmail.com>
	<CABPQxstfSyVu6D-FSt7qa-o1co9AkhHsncTHGBrCK4rEM7fymg@mail.gmail.com>
	<CACBYxKLb=OK2EKu=2yS_1tswYz52rBF-59+8DZL+DfG6qHF52Q@mail.gmail.com>
	<CAEYYnxakT5SVD0xxWcyAXq+w-f-tY0rDA2c3YLRXK6oZ3ArF7Q@mail.gmail.com>
	<CAEYYnxaYfaUOes+A4kR4w+nh0LS8fgJytM2Nj7xoBErpHiRwbA@mail.gmail.com>
	<CAJgQjQ_ifNtFFK4W99FA3cJ3BapRRfLc4FomCWxdgwTqEU+icg@mail.gmail.com>
	<CACBYxK+VOs2=m8tF1xijTOAxL917Q2yo=pOKNsNfzK52a7QdhQ@mail.gmail.com>
	<CAEYYnxbBW62gVrVtuQ1MPFPgBc4mRARB+=LyRm8b21by9cmFPQ@mail.gmail.com>
	<CAJgQjQ894Gz_StdJa+GgFXgXuEiq-5Z+4-3B0Bq9XXejzXCFXA@mail.gmail.com>
Date: Wed, 21 May 2014 14:27:16 -0700
Message-ID: <CACBYxKKk5mTDsOevK6J8UgtJMEDC0YFsWrLeXLSChOQfFcBgBg@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ab3ec1e238c04f9efa958
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ab3ec1e238c04f9efa958
Content-Type: text/plain; charset=UTF-8

Is that an assumption we can make?  I think we'd run into an issue in this
situation:

*In primary jar:*
def makeDynamicObject(clazz: String) = Class.forName(clazz).newInstance()

*In app code:*
sc.addJar("dynamicjar.jar")
...
rdd.map(x => makeDynamicObject("some.class.from.DynamicJar"))

It might be fair to say that the user should make sure to use the context
classloader when instantiating dynamic classes, but I think it's weird that
this code would work on Spark standalone but not on YARN.

-Sandy


On Wed, May 21, 2014 at 2:10 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> I think adding jars dynamically should work as long as the primary jar
> and the secondary jars do not depend on dynamically added jars, which
> should be the correct logic. -Xiangrui
>
> On Wed, May 21, 2014 at 1:40 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> > This will be another separate story.
> >
> > Since in the yarn deployment, as Sandy said, the app.jar will be always
> in
> > the systemclassloader which means any object instantiated in app.jar will
> > have parent loader of systemclassloader instead of custom one. As a
> result,
> > the custom class loader in yarn will never work without specifically
> using
> > reflection.
> >
> > Solution will be not using system classloader in the classloader
> hierarchy,
> > and add all the resources in system one into custom one. This is the
> > approach of tomcat takes.
> >
> > Or we can directly overwirte the system class loader by calling the
> > protected method `addURL` which will not work and throw exception if the
> > code is wrapped in security manager.
> >
> >
> > Sincerely,
> >
> > DB Tsai
> > -------------------------------------------------------
> > My Blog: https://www.dbtsai.com
> > LinkedIn: https://www.linkedin.com/in/dbtsai
> >
> >
> > On Wed, May 21, 2014 at 1:13 PM, Sandy Ryza <sandy.ryza@cloudera.com>
> wrote:
> >
> >> This will solve the issue for jars added upon application submission,
> but,
> >> on top of this, we need to make sure that anything dynamically added
> >> through sc.addJar works as well.
> >>
> >> To do so, we need to make sure that any jars retrieved via the driver's
> >> HTTP server are loaded by the same classloader that loads the jars
> given on
> >> app submission.  To achieve this, we need to either use the same
> >> classloader for both system jars and user jars, or make sure that the
> user
> >> jars given on app submission are under the same classloader used for
> >> dynamically added jars.
> >>
> >> On Tue, May 20, 2014 at 5:59 PM, Xiangrui Meng <mengxr@gmail.com>
> wrote:
> >>
> >> > Talked with Sandy and DB offline. I think the best solution is sending
> >> > the secondary jars to the distributed cache of all containers rather
> >> > than just the master, and set the classpath to include spark jar,
> >> > primary app jar, and secondary jars before executor starts. In this
> >> > way, user only needs to specify secondary jars via --jars instead of
> >> > calling sc.addJar inside the code. It also solves the scalability
> >> > problem of serving all the jars via http.
> >> >
> >> > If this solution sounds good, I can try to make a patch.
> >> >
> >> > Best,
> >> > Xiangrui
> >> >
> >> > On Mon, May 19, 2014 at 10:04 PM, DB Tsai <dbtsai@stanford.edu>
> wrote:
> >> > > In 1.0, there is a new option for users to choose which classloader
> has
> >> > > higher priority via spark.files.userClassPathFirst, I decided to
> submit
> >> > the
> >> > > PR for 0.9 first. We use this patch in our lab and we can use those
> >> jars
> >> > > added by sc.addJar without reflection.
> >> > >
> >> > > https://github.com/apache/spark/pull/834
> >> > >
> >> > > Can anyone comment if it's a good approach?
> >> > >
> >> > > Thanks.
> >> > >
> >> > >
> >> > > Sincerely,
> >> > >
> >> > > DB Tsai
> >> > > -------------------------------------------------------
> >> > > My Blog: https://www.dbtsai.com
> >> > > LinkedIn: https://www.linkedin.com/in/dbtsai
> >> > >
> >> > >
> >> > > On Mon, May 19, 2014 at 7:42 PM, DB Tsai <dbtsai@stanford.edu>
> wrote:
> >> > >
> >> > >> Good summary! We fixed it in branch 0.9 since our production is
> still
> >> in
> >> > >> 0.9. I'm porting it to 1.0 now, and hopefully will submit PR for
> 1.0
> >> > >> tonight.
> >> > >>
> >> > >>
> >> > >> Sincerely,
> >> > >>
> >> > >> DB Tsai
> >> > >> -------------------------------------------------------
> >> > >> My Blog: https://www.dbtsai.com
> >> > >> LinkedIn: https://www.linkedin.com/in/dbtsai
> >> > >>
> >> > >>
> >> > >> On Mon, May 19, 2014 at 7:38 PM, Sandy Ryza <
> sandy.ryza@cloudera.com
> >> > >wrote:
> >> > >>
> >> > >>> It just hit me why this problem is showing up on YARN and not on
> >> > >>> standalone.
> >> > >>>
> >> > >>> The relevant difference between YARN and standalone is that, on
> YARN,
> >> > the
> >> > >>> app jar is loaded by the system classloader instead of Spark's
> custom
> >> > URL
> >> > >>> classloader.
> >> > >>>
> >> > >>> On YARN, the system classloader knows about [the classes in the
> spark
> >> > >>> jars,
> >> > >>> the classes in the primary app jar].   The custom classloader
> knows
> >> > about
> >> > >>> [the classes in secondary app jars] and has the system
> classloader as
> >> > its
> >> > >>> parent.
> >> > >>>
> >> > >>> A few relevant facts (mostly redundant with what Sean pointed
> out):
> >> > >>> * Every class has a classloader that loaded it.
> >> > >>> * When an object of class B is instantiated inside of class A, the
> >> > >>> classloader used for loading B is the classloader that was used
> for
> >> > >>> loading
> >> > >>> A.
> >> > >>> * When a classloader fails to load a class, it lets its parent
> >> > classloader
> >> > >>> try.  If its parent succeeds, its parent becomes the "classloader
> >> that
> >> > >>> loaded it".
> >> > >>>
> >> > >>> So suppose class B is in a secondary app jar and class A is in the
> >> > primary
> >> > >>> app jar:
> >> > >>> 1. The custom classloader will try to load class A.
> >> > >>> 2. It will fail, because it only knows about the secondary jars.
> >> > >>> 3. It will delegate to its parent, the system classloader.
> >> > >>> 4. The system classloader will succeed, because it knows about the
> >> > primary
> >> > >>> app jar.
> >> > >>> 5. A's classloader will be the system classloader.
> >> > >>> 6. A tries to instantiate an instance of class B.
> >> > >>> 7. B will be loaded with A's classloader, which is the system
> >> > classloader.
> >> > >>> 8. Loading B will fail, because A's classloader, which is the
> system
> >> > >>> classloader, doesn't know about the secondary app jars.
> >> > >>>
> >> > >>> In Spark standalone, A and B are both loaded by the custom
> >> > classloader, so
> >> > >>> this issue doesn't come up.
> >> > >>>
> >> > >>> -Sandy
> >> > >>>
> >> > >>> On Mon, May 19, 2014 at 7:07 PM, Patrick Wendell <
> pwendell@gmail.com
> >> >
> >> > >>> wrote:
> >> > >>>
> >> > >>> > Having a user add define a custom class inside of an added jar
> and
> >> > >>> > instantiate it directly inside of an executor is definitely
> >> supported
> >> > >>> > in Spark and has been for a really long time (several years).
> This
> >> is
> >> > >>> > something we do all the time in Spark.
> >> > >>> >
> >> > >>> > DB - I'd hold off on a re-architecting of this until we identify
> >> > >>> > exactly what is causing the bug you are running into.
> >> > >>> >
> >> > >>> > In a nutshell, when the bytecode "new Foo()" is run on the
> >> executor,
> >> > >>> > it will ask the driver for the class over HTTP using a custom
> >> > >>> > classloader. Something in that pipeline is breaking here,
> possibly
> >> > >>> > related to the YARN deployment stuff.
> >> > >>> >
> >> > >>> >
> >> > >>> > On Mon, May 19, 2014 at 12:29 AM, Sean Owen <sowen@cloudera.com
> >
> >> > wrote:
> >> > >>> > > I don't think a customer classloader is necessary.
> >> > >>> > >
> >> > >>> > > Well, it occurs to me that this is no new problem. Hadoop,
> >> Tomcat,
> >> > etc
> >> > >>> > > all run custom user code that creates new user objects without
> >> > >>> > > reflection. I should go see how that's done. Maybe it's
> totally
> >> > valid
> >> > >>> > > to set the thread's context classloader for just this purpose,
> >> and
> >> > I
> >> > >>> > > am not thinking clearly.
> >> > >>> > >
> >> > >>> > > On Mon, May 19, 2014 at 8:26 AM, Andrew Ash <
> >> andrew@andrewash.com>
> >> > >>> > wrote:
> >> > >>> > >> Sounds like the problem is that classloaders always look in
> >> their
> >> > >>> > parents
> >> > >>> > >> before themselves, and Spark users want executors to pick up
> >> > classes
> >> > >>> > from
> >> > >>> > >> their custom code before the ones in Spark plus its
> >> dependencies.
> >> > >>> > >>
> >> > >>> > >> Would a custom classloader that delegates to the parent after
> >> > first
> >> > >>> > >> checking itself fix this up?
> >> > >>> > >>
> >> > >>> > >>
> >> > >>> > >> On Mon, May 19, 2014 at 12:17 AM, DB Tsai <
> dbtsai@stanford.edu>
> >> > >>> wrote:
> >> > >>> > >>
> >> > >>> > >>> Hi Sean,
> >> > >>> > >>>
> >> > >>> > >>> It's true that the issue here is classloader, and due to the
> >> > >>> > classloader
> >> > >>> > >>> delegation model, users have to use reflection in the
> executors
> >> > to
> >> > >>> > pick up
> >> > >>> > >>> the classloader in order to use those classes added by
> >> sc.addJars
> >> > >>> APIs.
> >> > >>> > >>> However, it's very inconvenience for users, and not
> documented
> >> in
> >> > >>> > spark.
> >> > >>> > >>>
> >> > >>> > >>> I'm working on a patch to solve it by calling the protected
> >> > method
> >> > >>> > addURL
> >> > >>> > >>> in URLClassLoader to update the current default
> classloader, so
> >> > no
> >> > >>> > >>> customClassLoader anymore. I wonder if this is an good way
> to
> >> go.
> >> > >>> > >>>
> >> > >>> > >>>   private def addURL(url: URL, loader: URLClassLoader){
> >> > >>> > >>>     try {
> >> > >>> > >>>       val method: Method =
> >> > >>> > >>> classOf[URLClassLoader].getDeclaredMethod("addURL",
> >> classOf[URL])
> >> > >>> > >>>       method.setAccessible(true)
> >> > >>> > >>>       method.invoke(loader, url)
> >> > >>> > >>>     }
> >> > >>> > >>>     catch {
> >> > >>> > >>>       case t: Throwable => {
> >> > >>> > >>>         throw new IOException("Error, could not add URL to
> >> system
> >> > >>> > >>> classloader")
> >> > >>> > >>>       }
> >> > >>> > >>>     }
> >> > >>> > >>>   }
> >> > >>> > >>>
> >> > >>> > >>>
> >> > >>> > >>>
> >> > >>> > >>> Sincerely,
> >> > >>> > >>>
> >> > >>> > >>> DB Tsai
> >> > >>> > >>> -------------------------------------------------------
> >> > >>> > >>> My Blog: https://www.dbtsai.com
> >> > >>> > >>> LinkedIn: https://www.linkedin.com/in/dbtsai
> >> > >>> > >>>
> >> > >>> > >>>
> >> > >>> > >>> On Sun, May 18, 2014 at 11:57 PM, Sean Owen <
> >> sowen@cloudera.com>
> >> > >>> > wrote:
> >> > >>> > >>>
> >> > >>> > >>> > I might be stating the obvious for everyone, but the issue
> >> > here is
> >> > >>> > not
> >> > >>> > >>> > reflection or the source of the JAR, but the ClassLoader.
> The
> >> > >>> basic
> >> > >>> > >>> > rules are this.
> >> > >>> > >>> >
> >> > >>> > >>> > "new Foo" will use the ClassLoader that defines Foo. This
> is
> >> > >>> usually
> >> > >>> > >>> > the ClassLoader that loaded whatever it is that first
> >> > referenced
> >> > >>> Foo
> >> > >>> > >>> > and caused it to be loaded -- usually the ClassLoader
> holding
> >> > your
> >> > >>> > >>> > other app classes.
> >> > >>> > >>> >
> >> > >>> > >>> > ClassLoaders can have a parent-child relationship.
> >> ClassLoaders
> >> > >>> > always
> >> > >>> > >>> > look in their parent before themselves.
> >> > >>> > >>> >
> >> > >>> > >>> > (Careful then -- in contexts like Hadoop or Tomcat where
> your
> >> > app
> >> > >>> is
> >> > >>> > >>> > loaded in a child ClassLoader, and you reference a class
> that
> >> > >>> Hadoop
> >> > >>> > >>> > or Tomcat also has (like a lib class) you will get the
> >> > container's
> >> > >>> > >>> > version!)
> >> > >>> > >>> >
> >> > >>> > >>> > When you load an external JAR it has a separate
> ClassLoader
> >> > which
> >> > >>> > does
> >> > >>> > >>> > not necessarily bear any relation to the one containing
> your
> >> > app
> >> > >>> > >>> > classes, so yeah it is not generally going to make "new
> Foo"
> >> > work.
> >> > >>> > >>> >
> >> > >>> > >>> > Reflection lets you pick the ClassLoader, yes.
> >> > >>> > >>> >
> >> > >>> > >>> > I would not call setContextClassLoader.
> >> > >>> > >>> >
> >> > >>> > >>> > On Mon, May 19, 2014 at 12:00 AM, Sandy Ryza <
> >> > >>> > sandy.ryza@cloudera.com>
> >> > >>> > >>> > wrote:
> >> > >>> > >>> > > I spoke with DB offline about this a little while ago
> and
> >> he
> >> > >>> > confirmed
> >> > >>> > >>> > that
> >> > >>> > >>> > > he was able to access the jar from the driver.
> >> > >>> > >>> > >
> >> > >>> > >>> > > The issue appears to be a general Java issue: you can't
> >> > directly
> >> > >>> > >>> > > instantiate a class from a dynamically loaded jar.
> >> > >>> > >>> > >
> >> > >>> > >>> > > I reproduced it locally outside of Spark with:
> >> > >>> > >>> > > ---
> >> > >>> > >>> > >     URLClassLoader urlClassLoader = new
> URLClassLoader(new
> >> > >>> URL[] {
> >> > >>> > new
> >> > >>> > >>> > > File("myotherjar.jar").toURI().toURL() }, null);
> >> > >>> > >>> > >
> >> > >>> Thread.currentThread().setContextClassLoader(urlClassLoader);
> >> > >>> > >>> > >     MyClassFromMyOtherJar obj = new
> >> MyClassFromMyOtherJar();
> >> > >>> > >>> > > ---
> >> > >>> > >>> > >
> >> > >>> > >>> > > I was able to load the class with reflection.
> >> > >>> > >>> >
> >> > >>> > >>>
> >> > >>> >
> >> > >>>
> >> > >>
> >> > >>
> >> >
> >>
>

--001a113ab3ec1e238c04f9efa958--

From dev-return-7763-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 21:41:41 2014
Return-Path: <dev-return-7763-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 41A2511AAE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 21:41:41 +0000 (UTC)
Received: (qmail 30652 invoked by uid 500); 21 May 2014 21:41:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30610 invoked by uid 500); 21 May 2014 21:41:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30602 invoked by uid 99); 21 May 2014 21:41:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 21:41:40 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 74.125.82.47 as permitted sender)
Received: from [74.125.82.47] (HELO mail-wg0-f47.google.com) (74.125.82.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 21:41:37 +0000
Received: by mail-wg0-f47.google.com with SMTP id x12so2521386wgg.18
        for <dev@spark.apache.org>; Wed, 21 May 2014 14:41:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=PaNrZ9MH/JsjQIYBTGY1iezajgsdKrbLqKn0B0d2l+w=;
        b=gRyrMTh3i5KLSVkb+lL3+9f1hBR6XyDGnqGbUqgkCEpr5uce9pa3cjLF6y38D7CfZZ
         GrvAhSo6vK+m2i6XfOkrt+FYQNzl2BzWqv3cEZ5BNxJKCwhNbBVf6Axy23MIPo68a6+9
         EFo+2HZA1jx9NknXw/+ZnXTWNf52RddX4UKm7AkY6QhCO9YyJ9/jvLNH8C9w0cnBgAuN
         05X0zwCFoBlv6cRQQH4FfUyfzAIcvVrlShaHtjuhjMrrspd8S+hMfLFwSIzFNqzHIPMR
         iYc7NFwA/DzKtQg2JdYehWoQzp5Tvf0f5NkGhbGduNzz4HrDVdTHBG28wsfVNlFjNJ5r
         lH9Q==
MIME-Version: 1.0
X-Received: by 10.194.108.5 with SMTP id hg5mr14424829wjb.57.1400708475725;
 Wed, 21 May 2014 14:41:15 -0700 (PDT)
Received: by 10.194.187.20 with HTTP; Wed, 21 May 2014 14:41:15 -0700 (PDT)
In-Reply-To: <CACBYxKKk5mTDsOevK6J8UgtJMEDC0YFsWrLeXLSChOQfFcBgBg@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
	<CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
	<CAMAsSdKxE-biW8fRqkRn6NmqsszFgQ=f1w_Z8yt8cSOj3qBB2Q@mail.gmail.com>
	<CAEYYnxY9UZDuhBoHAP5kD3dCXxd0TwR761tt-kswjTEddQCCKg@mail.gmail.com>
	<CA+-p3AGT8E5caKU+2ExxjbpV-UqaaTot+Ni3DXEdOf37tADLgw@mail.gmail.com>
	<CAMAsSdJVckDE5=3S8wZj9P1-1-D6qBumGnofk9G7A7cksh1RVA@mail.gmail.com>
	<CABPQxstfSyVu6D-FSt7qa-o1co9AkhHsncTHGBrCK4rEM7fymg@mail.gmail.com>
	<CACBYxKLb=OK2EKu=2yS_1tswYz52rBF-59+8DZL+DfG6qHF52Q@mail.gmail.com>
	<CAEYYnxakT5SVD0xxWcyAXq+w-f-tY0rDA2c3YLRXK6oZ3ArF7Q@mail.gmail.com>
	<CAEYYnxaYfaUOes+A4kR4w+nh0LS8fgJytM2Nj7xoBErpHiRwbA@mail.gmail.com>
	<CAJgQjQ_ifNtFFK4W99FA3cJ3BapRRfLc4FomCWxdgwTqEU+icg@mail.gmail.com>
	<CACBYxK+VOs2=m8tF1xijTOAxL917Q2yo=pOKNsNfzK52a7QdhQ@mail.gmail.com>
	<CAEYYnxbBW62gVrVtuQ1MPFPgBc4mRARB+=LyRm8b21by9cmFPQ@mail.gmail.com>
	<CAJgQjQ894Gz_StdJa+GgFXgXuEiq-5Z+4-3B0Bq9XXejzXCFXA@mail.gmail.com>
	<CACBYxKKk5mTDsOevK6J8UgtJMEDC0YFsWrLeXLSChOQfFcBgBg@mail.gmail.com>
Date: Wed, 21 May 2014 14:41:15 -0700
Message-ID: <CAJgQjQ-Of38yGJ0SP416Rxz7e935+69aUTGyi6-DLSH0aO0_2A@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

That's a good example. If we really want to cover that case, there are
two solutions:

1. Follow DB's patch, adding jars to the system classloader. Then we
cannot put a user class in front of an existing class.
2. Do not send the primary jar and secondary jars to executors'
distributed cache. Instead, add them to "spark.jars" in SparkSubmit
and serve them via http by called sc.addJar in SparkContext.

What is your preference?

On Wed, May 21, 2014 at 2:27 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
> Is that an assumption we can make?  I think we'd run into an issue in this
> situation:
>
> *In primary jar:*
> def makeDynamicObject(clazz: String) = Class.forName(clazz).newInstance()
>
> *In app code:*
> sc.addJar("dynamicjar.jar")
> ...
> rdd.map(x => makeDynamicObject("some.class.from.DynamicJar"))
>
> It might be fair to say that the user should make sure to use the context
> classloader when instantiating dynamic classes, but I think it's weird that
> this code would work on Spark standalone but not on YARN.
>
> -Sandy
>
>
> On Wed, May 21, 2014 at 2:10 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
>> I think adding jars dynamically should work as long as the primary jar
>> and the secondary jars do not depend on dynamically added jars, which
>> should be the correct logic. -Xiangrui
>>
>> On Wed, May 21, 2014 at 1:40 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>> > This will be another separate story.
>> >
>> > Since in the yarn deployment, as Sandy said, the app.jar will be always
>> in
>> > the systemclassloader which means any object instantiated in app.jar will
>> > have parent loader of systemclassloader instead of custom one. As a
>> result,
>> > the custom class loader in yarn will never work without specifically
>> using
>> > reflection.
>> >
>> > Solution will be not using system classloader in the classloader
>> hierarchy,
>> > and add all the resources in system one into custom one. This is the
>> > approach of tomcat takes.
>> >
>> > Or we can directly overwirte the system class loader by calling the
>> > protected method `addURL` which will not work and throw exception if the
>> > code is wrapped in security manager.
>> >
>> >
>> > Sincerely,
>> >
>> > DB Tsai
>> > -------------------------------------------------------
>> > My Blog: https://www.dbtsai.com
>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>> >
>> >
>> > On Wed, May 21, 2014 at 1:13 PM, Sandy Ryza <sandy.ryza@cloudera.com>
>> wrote:
>> >
>> >> This will solve the issue for jars added upon application submission,
>> but,
>> >> on top of this, we need to make sure that anything dynamically added
>> >> through sc.addJar works as well.
>> >>
>> >> To do so, we need to make sure that any jars retrieved via the driver's
>> >> HTTP server are loaded by the same classloader that loads the jars
>> given on
>> >> app submission.  To achieve this, we need to either use the same
>> >> classloader for both system jars and user jars, or make sure that the
>> user
>> >> jars given on app submission are under the same classloader used for
>> >> dynamically added jars.
>> >>
>> >> On Tue, May 20, 2014 at 5:59 PM, Xiangrui Meng <mengxr@gmail.com>
>> wrote:
>> >>
>> >> > Talked with Sandy and DB offline. I think the best solution is sending
>> >> > the secondary jars to the distributed cache of all containers rather
>> >> > than just the master, and set the classpath to include spark jar,
>> >> > primary app jar, and secondary jars before executor starts. In this
>> >> > way, user only needs to specify secondary jars via --jars instead of
>> >> > calling sc.addJar inside the code. It also solves the scalability
>> >> > problem of serving all the jars via http.
>> >> >
>> >> > If this solution sounds good, I can try to make a patch.
>> >> >
>> >> > Best,
>> >> > Xiangrui
>> >> >
>> >> > On Mon, May 19, 2014 at 10:04 PM, DB Tsai <dbtsai@stanford.edu>
>> wrote:
>> >> > > In 1.0, there is a new option for users to choose which classloader
>> has
>> >> > > higher priority via spark.files.userClassPathFirst, I decided to
>> submit
>> >> > the
>> >> > > PR for 0.9 first. We use this patch in our lab and we can use those
>> >> jars
>> >> > > added by sc.addJar without reflection.
>> >> > >
>> >> > > https://github.com/apache/spark/pull/834
>> >> > >
>> >> > > Can anyone comment if it's a good approach?
>> >> > >
>> >> > > Thanks.
>> >> > >
>> >> > >
>> >> > > Sincerely,
>> >> > >
>> >> > > DB Tsai
>> >> > > -------------------------------------------------------
>> >> > > My Blog: https://www.dbtsai.com
>> >> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>> >> > >
>> >> > >
>> >> > > On Mon, May 19, 2014 at 7:42 PM, DB Tsai <dbtsai@stanford.edu>
>> wrote:
>> >> > >
>> >> > >> Good summary! We fixed it in branch 0.9 since our production is
>> still
>> >> in
>> >> > >> 0.9. I'm porting it to 1.0 now, and hopefully will submit PR for
>> 1.0
>> >> > >> tonight.
>> >> > >>
>> >> > >>
>> >> > >> Sincerely,
>> >> > >>
>> >> > >> DB Tsai
>> >> > >> -------------------------------------------------------
>> >> > >> My Blog: https://www.dbtsai.com
>> >> > >> LinkedIn: https://www.linkedin.com/in/dbtsai
>> >> > >>
>> >> > >>
>> >> > >> On Mon, May 19, 2014 at 7:38 PM, Sandy Ryza <
>> sandy.ryza@cloudera.com
>> >> > >wrote:
>> >> > >>
>> >> > >>> It just hit me why this problem is showing up on YARN and not on
>> >> > >>> standalone.
>> >> > >>>
>> >> > >>> The relevant difference between YARN and standalone is that, on
>> YARN,
>> >> > the
>> >> > >>> app jar is loaded by the system classloader instead of Spark's
>> custom
>> >> > URL
>> >> > >>> classloader.
>> >> > >>>
>> >> > >>> On YARN, the system classloader knows about [the classes in the
>> spark
>> >> > >>> jars,
>> >> > >>> the classes in the primary app jar].   The custom classloader
>> knows
>> >> > about
>> >> > >>> [the classes in secondary app jars] and has the system
>> classloader as
>> >> > its
>> >> > >>> parent.
>> >> > >>>
>> >> > >>> A few relevant facts (mostly redundant with what Sean pointed
>> out):
>> >> > >>> * Every class has a classloader that loaded it.
>> >> > >>> * When an object of class B is instantiated inside of class A, the
>> >> > >>> classloader used for loading B is the classloader that was used
>> for
>> >> > >>> loading
>> >> > >>> A.
>> >> > >>> * When a classloader fails to load a class, it lets its parent
>> >> > classloader
>> >> > >>> try.  If its parent succeeds, its parent becomes the "classloader
>> >> that
>> >> > >>> loaded it".
>> >> > >>>
>> >> > >>> So suppose class B is in a secondary app jar and class A is in the
>> >> > primary
>> >> > >>> app jar:
>> >> > >>> 1. The custom classloader will try to load class A.
>> >> > >>> 2. It will fail, because it only knows about the secondary jars.
>> >> > >>> 3. It will delegate to its parent, the system classloader.
>> >> > >>> 4. The system classloader will succeed, because it knows about the
>> >> > primary
>> >> > >>> app jar.
>> >> > >>> 5. A's classloader will be the system classloader.
>> >> > >>> 6. A tries to instantiate an instance of class B.
>> >> > >>> 7. B will be loaded with A's classloader, which is the system
>> >> > classloader.
>> >> > >>> 8. Loading B will fail, because A's classloader, which is the
>> system
>> >> > >>> classloader, doesn't know about the secondary app jars.
>> >> > >>>
>> >> > >>> In Spark standalone, A and B are both loaded by the custom
>> >> > classloader, so
>> >> > >>> this issue doesn't come up.
>> >> > >>>
>> >> > >>> -Sandy
>> >> > >>>
>> >> > >>> On Mon, May 19, 2014 at 7:07 PM, Patrick Wendell <
>> pwendell@gmail.com
>> >> >
>> >> > >>> wrote:
>> >> > >>>
>> >> > >>> > Having a user add define a custom class inside of an added jar
>> and
>> >> > >>> > instantiate it directly inside of an executor is definitely
>> >> supported
>> >> > >>> > in Spark and has been for a really long time (several years).
>> This
>> >> is
>> >> > >>> > something we do all the time in Spark.
>> >> > >>> >
>> >> > >>> > DB - I'd hold off on a re-architecting of this until we identify
>> >> > >>> > exactly what is causing the bug you are running into.
>> >> > >>> >
>> >> > >>> > In a nutshell, when the bytecode "new Foo()" is run on the
>> >> executor,
>> >> > >>> > it will ask the driver for the class over HTTP using a custom
>> >> > >>> > classloader. Something in that pipeline is breaking here,
>> possibly
>> >> > >>> > related to the YARN deployment stuff.
>> >> > >>> >
>> >> > >>> >
>> >> > >>> > On Mon, May 19, 2014 at 12:29 AM, Sean Owen <sowen@cloudera.com
>> >
>> >> > wrote:
>> >> > >>> > > I don't think a customer classloader is necessary.
>> >> > >>> > >
>> >> > >>> > > Well, it occurs to me that this is no new problem. Hadoop,
>> >> Tomcat,
>> >> > etc
>> >> > >>> > > all run custom user code that creates new user objects without
>> >> > >>> > > reflection. I should go see how that's done. Maybe it's
>> totally
>> >> > valid
>> >> > >>> > > to set the thread's context classloader for just this purpose,
>> >> and
>> >> > I
>> >> > >>> > > am not thinking clearly.
>> >> > >>> > >
>> >> > >>> > > On Mon, May 19, 2014 at 8:26 AM, Andrew Ash <
>> >> andrew@andrewash.com>
>> >> > >>> > wrote:
>> >> > >>> > >> Sounds like the problem is that classloaders always look in
>> >> their
>> >> > >>> > parents
>> >> > >>> > >> before themselves, and Spark users want executors to pick up
>> >> > classes
>> >> > >>> > from
>> >> > >>> > >> their custom code before the ones in Spark plus its
>> >> dependencies.
>> >> > >>> > >>
>> >> > >>> > >> Would a custom classloader that delegates to the parent after
>> >> > first
>> >> > >>> > >> checking itself fix this up?
>> >> > >>> > >>
>> >> > >>> > >>
>> >> > >>> > >> On Mon, May 19, 2014 at 12:17 AM, DB Tsai <
>> dbtsai@stanford.edu>
>> >> > >>> wrote:
>> >> > >>> > >>
>> >> > >>> > >>> Hi Sean,
>> >> > >>> > >>>
>> >> > >>> > >>> It's true that the issue here is classloader, and due to the
>> >> > >>> > classloader
>> >> > >>> > >>> delegation model, users have to use reflection in the
>> executors
>> >> > to
>> >> > >>> > pick up
>> >> > >>> > >>> the classloader in order to use those classes added by
>> >> sc.addJars
>> >> > >>> APIs.
>> >> > >>> > >>> However, it's very inconvenience for users, and not
>> documented
>> >> in
>> >> > >>> > spark.
>> >> > >>> > >>>
>> >> > >>> > >>> I'm working on a patch to solve it by calling the protected
>> >> > method
>> >> > >>> > addURL
>> >> > >>> > >>> in URLClassLoader to update the current default
>> classloader, so
>> >> > no
>> >> > >>> > >>> customClassLoader anymore. I wonder if this is an good way
>> to
>> >> go.
>> >> > >>> > >>>
>> >> > >>> > >>>   private def addURL(url: URL, loader: URLClassLoader){
>> >> > >>> > >>>     try {
>> >> > >>> > >>>       val method: Method =
>> >> > >>> > >>> classOf[URLClassLoader].getDeclaredMethod("addURL",
>> >> classOf[URL])
>> >> > >>> > >>>       method.setAccessible(true)
>> >> > >>> > >>>       method.invoke(loader, url)
>> >> > >>> > >>>     }
>> >> > >>> > >>>     catch {
>> >> > >>> > >>>       case t: Throwable => {
>> >> > >>> > >>>         throw new IOException("Error, could not add URL to
>> >> system
>> >> > >>> > >>> classloader")
>> >> > >>> > >>>       }
>> >> > >>> > >>>     }
>> >> > >>> > >>>   }
>> >> > >>> > >>>
>> >> > >>> > >>>
>> >> > >>> > >>>
>> >> > >>> > >>> Sincerely,
>> >> > >>> > >>>
>> >> > >>> > >>> DB Tsai
>> >> > >>> > >>> -------------------------------------------------------
>> >> > >>> > >>> My Blog: https://www.dbtsai.com
>> >> > >>> > >>> LinkedIn: https://www.linkedin.com/in/dbtsai
>> >> > >>> > >>>
>> >> > >>> > >>>
>> >> > >>> > >>> On Sun, May 18, 2014 at 11:57 PM, Sean Owen <
>> >> sowen@cloudera.com>
>> >> > >>> > wrote:
>> >> > >>> > >>>
>> >> > >>> > >>> > I might be stating the obvious for everyone, but the issue
>> >> > here is
>> >> > >>> > not
>> >> > >>> > >>> > reflection or the source of the JAR, but the ClassLoader.
>> The
>> >> > >>> basic
>> >> > >>> > >>> > rules are this.
>> >> > >>> > >>> >
>> >> > >>> > >>> > "new Foo" will use the ClassLoader that defines Foo. This
>> is
>> >> > >>> usually
>> >> > >>> > >>> > the ClassLoader that loaded whatever it is that first
>> >> > referenced
>> >> > >>> Foo
>> >> > >>> > >>> > and caused it to be loaded -- usually the ClassLoader
>> holding
>> >> > your
>> >> > >>> > >>> > other app classes.
>> >> > >>> > >>> >
>> >> > >>> > >>> > ClassLoaders can have a parent-child relationship.
>> >> ClassLoaders
>> >> > >>> > always
>> >> > >>> > >>> > look in their parent before themselves.
>> >> > >>> > >>> >
>> >> > >>> > >>> > (Careful then -- in contexts like Hadoop or Tomcat where
>> your
>> >> > app
>> >> > >>> is
>> >> > >>> > >>> > loaded in a child ClassLoader, and you reference a class
>> that
>> >> > >>> Hadoop
>> >> > >>> > >>> > or Tomcat also has (like a lib class) you will get the
>> >> > container's
>> >> > >>> > >>> > version!)
>> >> > >>> > >>> >
>> >> > >>> > >>> > When you load an external JAR it has a separate
>> ClassLoader
>> >> > which
>> >> > >>> > does
>> >> > >>> > >>> > not necessarily bear any relation to the one containing
>> your
>> >> > app
>> >> > >>> > >>> > classes, so yeah it is not generally going to make "new
>> Foo"
>> >> > work.
>> >> > >>> > >>> >
>> >> > >>> > >>> > Reflection lets you pick the ClassLoader, yes.
>> >> > >>> > >>> >
>> >> > >>> > >>> > I would not call setContextClassLoader.
>> >> > >>> > >>> >
>> >> > >>> > >>> > On Mon, May 19, 2014 at 12:00 AM, Sandy Ryza <
>> >> > >>> > sandy.ryza@cloudera.com>
>> >> > >>> > >>> > wrote:
>> >> > >>> > >>> > > I spoke with DB offline about this a little while ago
>> and
>> >> he
>> >> > >>> > confirmed
>> >> > >>> > >>> > that
>> >> > >>> > >>> > > he was able to access the jar from the driver.
>> >> > >>> > >>> > >
>> >> > >>> > >>> > > The issue appears to be a general Java issue: you can't
>> >> > directly
>> >> > >>> > >>> > > instantiate a class from a dynamically loaded jar.
>> >> > >>> > >>> > >
>> >> > >>> > >>> > > I reproduced it locally outside of Spark with:
>> >> > >>> > >>> > > ---
>> >> > >>> > >>> > >     URLClassLoader urlClassLoader = new
>> URLClassLoader(new
>> >> > >>> URL[] {
>> >> > >>> > new
>> >> > >>> > >>> > > File("myotherjar.jar").toURI().toURL() }, null);
>> >> > >>> > >>> > >
>> >> > >>> Thread.currentThread().setContextClassLoader(urlClassLoader);
>> >> > >>> > >>> > >     MyClassFromMyOtherJar obj = new
>> >> MyClassFromMyOtherJar();
>> >> > >>> > >>> > > ---
>> >> > >>> > >>> > >
>> >> > >>> > >>> > > I was able to load the class with reflection.
>> >> > >>> > >>> >
>> >> > >>> > >>>
>> >> > >>> >
>> >> > >>>
>> >> > >>
>> >> > >>
>> >> >
>> >>
>>

From dev-return-7764-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 21:57:43 2014
Return-Path: <dev-return-7764-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9176211B33
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 21:57:43 +0000 (UTC)
Received: (qmail 64438 invoked by uid 500); 21 May 2014 21:57:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64376 invoked by uid 500); 21 May 2014 21:57:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64362 invoked by uid 99); 21 May 2014 21:57:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 21:57:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.212.182] (HELO mail-wi0-f182.google.com) (209.85.212.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 21:57:39 +0000
Received: by mail-wi0-f182.google.com with SMTP id r20so3421880wiv.3
        for <dev@spark.apache.org>; Wed, 21 May 2014 14:57:16 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=vr9CFdLQK/0ouJw//C7KaHfA7WOMK+mcAbSEy+lhk/o=;
        b=UXET9T059Hh/O0xFC0EN5AEaAufQWE4djZalyiGCZTys2VZIbJCG/34C+VjbWhywsV
         YmjKELmONh5wA7hCGPQd0lj9oc1V2mLBUicZCkthuJuDQeNNbqF+bMuyWJpRIv1o0+20
         vIgUCO8TgC2kS8zOWxU6roMuJDgtzkpBXOOoaz3bnSvEDB1rfcP5sfFpSo5aFvnKlvoS
         W+Idgt93SdG0zeVGSrTN7RYH+AWHuIJt9aG4UYrNmdiXbphZ9let8NmjETclrSyYdHEr
         m0sshD8y9n4o2q/UhG7yz/ZjhjtWCsqSEG13A6wNDlLEbxDJSRB7zgjOzuljRWIk8+Xr
         XY8w==
X-Gm-Message-State: ALoCoQl8wgLvDpCnveddUK/8s2lRGWMh4YHqBdttlOt9qeT3PygxlekcBjk1G/o5zHs1prcyAsVR
MIME-Version: 1.0
X-Received: by 10.180.87.165 with SMTP id az5mr12923636wib.10.1400709436266;
 Wed, 21 May 2014 14:57:16 -0700 (PDT)
Received: by 10.217.66.129 with HTTP; Wed, 21 May 2014 14:57:16 -0700 (PDT)
X-Originating-IP: [209.150.41.132]
In-Reply-To: <CAEYYnxaYfaUOes+A4kR4w+nh0LS8fgJytM2Nj7xoBErpHiRwbA@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
	<CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
	<CAMAsSdKxE-biW8fRqkRn6NmqsszFgQ=f1w_Z8yt8cSOj3qBB2Q@mail.gmail.com>
	<CAEYYnxY9UZDuhBoHAP5kD3dCXxd0TwR761tt-kswjTEddQCCKg@mail.gmail.com>
	<CA+-p3AGT8E5caKU+2ExxjbpV-UqaaTot+Ni3DXEdOf37tADLgw@mail.gmail.com>
	<CAMAsSdJVckDE5=3S8wZj9P1-1-D6qBumGnofk9G7A7cksh1RVA@mail.gmail.com>
	<CABPQxstfSyVu6D-FSt7qa-o1co9AkhHsncTHGBrCK4rEM7fymg@mail.gmail.com>
	<CACBYxKLb=OK2EKu=2yS_1tswYz52rBF-59+8DZL+DfG6qHF52Q@mail.gmail.com>
	<CAEYYnxakT5SVD0xxWcyAXq+w-f-tY0rDA2c3YLRXK6oZ3ArF7Q@mail.gmail.com>
	<CAEYYnxaYfaUOes+A4kR4w+nh0LS8fgJytM2Nj7xoBErpHiRwbA@mail.gmail.com>
Date: Wed, 21 May 2014 17:57:16 -0400
Message-ID: <CANx3uAjGpOC+MXbQQCWWdTgyKAN3wm6y095R_+-+=PzAaaAHng@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: Koert Kuipers <koert@tresata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=14dae9cc955261f2db04f9f014c0
X-Virus-Checked: Checked by ClamAV on apache.org

--14dae9cc955261f2db04f9f014c0
Content-Type: text/plain; charset=UTF-8

db tsai, i do not think userClassPathFirst is working, unless the classes
you load dont reference any classes already loaded by the parent
classloader (a mostly hypothetical situation)... i filed a jira for this
here:
https://issues.apache.org/jira/browse/SPARK-1863



On Tue, May 20, 2014 at 1:04 AM, DB Tsai <dbtsai@stanford.edu> wrote:

> In 1.0, there is a new option for users to choose which classloader has
> higher priority via spark.files.userClassPathFirst, I decided to submit the
> PR for 0.9 first. We use this patch in our lab and we can use those jars
> added by sc.addJar without reflection.
>
> https://github.com/apache/spark/pull/834
>
> Can anyone comment if it's a good approach?
>
> Thanks.
>
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Mon, May 19, 2014 at 7:42 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>
> > Good summary! We fixed it in branch 0.9 since our production is still in
> > 0.9. I'm porting it to 1.0 now, and hopefully will submit PR for 1.0
> > tonight.
> >
> >
> > Sincerely,
> >
> > DB Tsai
> > -------------------------------------------------------
> > My Blog: https://www.dbtsai.com
> > LinkedIn: https://www.linkedin.com/in/dbtsai
> >
> >
> > On Mon, May 19, 2014 at 7:38 PM, Sandy Ryza <sandy.ryza@cloudera.com
> >wrote:
> >
> >> It just hit me why this problem is showing up on YARN and not on
> >> standalone.
> >>
> >> The relevant difference between YARN and standalone is that, on YARN,
> the
> >> app jar is loaded by the system classloader instead of Spark's custom
> URL
> >> classloader.
> >>
> >> On YARN, the system classloader knows about [the classes in the spark
> >> jars,
> >> the classes in the primary app jar].   The custom classloader knows
> about
> >> [the classes in secondary app jars] and has the system classloader as
> its
> >> parent.
> >>
> >> A few relevant facts (mostly redundant with what Sean pointed out):
> >> * Every class has a classloader that loaded it.
> >> * When an object of class B is instantiated inside of class A, the
> >> classloader used for loading B is the classloader that was used for
> >> loading
> >> A.
> >> * When a classloader fails to load a class, it lets its parent
> classloader
> >> try.  If its parent succeeds, its parent becomes the "classloader that
> >> loaded it".
> >>
> >> So suppose class B is in a secondary app jar and class A is in the
> primary
> >> app jar:
> >> 1. The custom classloader will try to load class A.
> >> 2. It will fail, because it only knows about the secondary jars.
> >> 3. It will delegate to its parent, the system classloader.
> >> 4. The system classloader will succeed, because it knows about the
> primary
> >> app jar.
> >> 5. A's classloader will be the system classloader.
> >> 6. A tries to instantiate an instance of class B.
> >> 7. B will be loaded with A's classloader, which is the system
> classloader.
> >> 8. Loading B will fail, because A's classloader, which is the system
> >> classloader, doesn't know about the secondary app jars.
> >>
> >> In Spark standalone, A and B are both loaded by the custom classloader,
> so
> >> this issue doesn't come up.
> >>
> >> -Sandy
> >>
> >> On Mon, May 19, 2014 at 7:07 PM, Patrick Wendell <pwendell@gmail.com>
> >> wrote:
> >>
> >> > Having a user add define a custom class inside of an added jar and
> >> > instantiate it directly inside of an executor is definitely supported
> >> > in Spark and has been for a really long time (several years). This is
> >> > something we do all the time in Spark.
> >> >
> >> > DB - I'd hold off on a re-architecting of this until we identify
> >> > exactly what is causing the bug you are running into.
> >> >
> >> > In a nutshell, when the bytecode "new Foo()" is run on the executor,
> >> > it will ask the driver for the class over HTTP using a custom
> >> > classloader. Something in that pipeline is breaking here, possibly
> >> > related to the YARN deployment stuff.
> >> >
> >> >
> >> > On Mon, May 19, 2014 at 12:29 AM, Sean Owen <sowen@cloudera.com>
> wrote:
> >> > > I don't think a customer classloader is necessary.
> >> > >
> >> > > Well, it occurs to me that this is no new problem. Hadoop, Tomcat,
> etc
> >> > > all run custom user code that creates new user objects without
> >> > > reflection. I should go see how that's done. Maybe it's totally
> valid
> >> > > to set the thread's context classloader for just this purpose, and I
> >> > > am not thinking clearly.
> >> > >
> >> > > On Mon, May 19, 2014 at 8:26 AM, Andrew Ash <andrew@andrewash.com>
> >> > wrote:
> >> > >> Sounds like the problem is that classloaders always look in their
> >> > parents
> >> > >> before themselves, and Spark users want executors to pick up
> classes
> >> > from
> >> > >> their custom code before the ones in Spark plus its dependencies.
> >> > >>
> >> > >> Would a custom classloader that delegates to the parent after first
> >> > >> checking itself fix this up?
> >> > >>
> >> > >>
> >> > >> On Mon, May 19, 2014 at 12:17 AM, DB Tsai <dbtsai@stanford.edu>
> >> wrote:
> >> > >>
> >> > >>> Hi Sean,
> >> > >>>
> >> > >>> It's true that the issue here is classloader, and due to the
> >> > classloader
> >> > >>> delegation model, users have to use reflection in the executors to
> >> > pick up
> >> > >>> the classloader in order to use those classes added by sc.addJars
> >> APIs.
> >> > >>> However, it's very inconvenience for users, and not documented in
> >> > spark.
> >> > >>>
> >> > >>> I'm working on a patch to solve it by calling the protected method
> >> > addURL
> >> > >>> in URLClassLoader to update the current default classloader, so no
> >> > >>> customClassLoader anymore. I wonder if this is an good way to go.
> >> > >>>
> >> > >>>   private def addURL(url: URL, loader: URLClassLoader){
> >> > >>>     try {
> >> > >>>       val method: Method =
> >> > >>> classOf[URLClassLoader].getDeclaredMethod("addURL", classOf[URL])
> >> > >>>       method.setAccessible(true)
> >> > >>>       method.invoke(loader, url)
> >> > >>>     }
> >> > >>>     catch {
> >> > >>>       case t: Throwable => {
> >> > >>>         throw new IOException("Error, could not add URL to system
> >> > >>> classloader")
> >> > >>>       }
> >> > >>>     }
> >> > >>>   }
> >> > >>>
> >> > >>>
> >> > >>>
> >> > >>> Sincerely,
> >> > >>>
> >> > >>> DB Tsai
> >> > >>> -------------------------------------------------------
> >> > >>> My Blog: https://www.dbtsai.com
> >> > >>> LinkedIn: https://www.linkedin.com/in/dbtsai
> >> > >>>
> >> > >>>
> >> > >>> On Sun, May 18, 2014 at 11:57 PM, Sean Owen <sowen@cloudera.com>
> >> > wrote:
> >> > >>>
> >> > >>> > I might be stating the obvious for everyone, but the issue here
> is
> >> > not
> >> > >>> > reflection or the source of the JAR, but the ClassLoader. The
> >> basic
> >> > >>> > rules are this.
> >> > >>> >
> >> > >>> > "new Foo" will use the ClassLoader that defines Foo. This is
> >> usually
> >> > >>> > the ClassLoader that loaded whatever it is that first referenced
> >> Foo
> >> > >>> > and caused it to be loaded -- usually the ClassLoader holding
> your
> >> > >>> > other app classes.
> >> > >>> >
> >> > >>> > ClassLoaders can have a parent-child relationship. ClassLoaders
> >> > always
> >> > >>> > look in their parent before themselves.
> >> > >>> >
> >> > >>> > (Careful then -- in contexts like Hadoop or Tomcat where your
> app
> >> is
> >> > >>> > loaded in a child ClassLoader, and you reference a class that
> >> Hadoop
> >> > >>> > or Tomcat also has (like a lib class) you will get the
> container's
> >> > >>> > version!)
> >> > >>> >
> >> > >>> > When you load an external JAR it has a separate ClassLoader
> which
> >> > does
> >> > >>> > not necessarily bear any relation to the one containing your app
> >> > >>> > classes, so yeah it is not generally going to make "new Foo"
> work.
> >> > >>> >
> >> > >>> > Reflection lets you pick the ClassLoader, yes.
> >> > >>> >
> >> > >>> > I would not call setContextClassLoader.
> >> > >>> >
> >> > >>> > On Mon, May 19, 2014 at 12:00 AM, Sandy Ryza <
> >> > sandy.ryza@cloudera.com>
> >> > >>> > wrote:
> >> > >>> > > I spoke with DB offline about this a little while ago and he
> >> > confirmed
> >> > >>> > that
> >> > >>> > > he was able to access the jar from the driver.
> >> > >>> > >
> >> > >>> > > The issue appears to be a general Java issue: you can't
> directly
> >> > >>> > > instantiate a class from a dynamically loaded jar.
> >> > >>> > >
> >> > >>> > > I reproduced it locally outside of Spark with:
> >> > >>> > > ---
> >> > >>> > >     URLClassLoader urlClassLoader = new URLClassLoader(new
> >> URL[] {
> >> > new
> >> > >>> > > File("myotherjar.jar").toURI().toURL() }, null);
> >> > >>> > >
> >> Thread.currentThread().setContextClassLoader(urlClassLoader);
> >> > >>> > >     MyClassFromMyOtherJar obj = new MyClassFromMyOtherJar();
> >> > >>> > > ---
> >> > >>> > >
> >> > >>> > > I was able to load the class with reflection.
> >> > >>> >
> >> > >>>
> >> >
> >>
> >
> >
>

--14dae9cc955261f2db04f9f014c0--

From dev-return-7765-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 22:15:53 2014
Return-Path: <dev-return-7765-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C1EDF11BCD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 22:15:53 +0000 (UTC)
Received: (qmail 89860 invoked by uid 500); 21 May 2014 22:15:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89804 invoked by uid 500); 21 May 2014 22:15:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89796 invoked by uid 99); 21 May 2014 22:15:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 22:15:53 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.178 as permitted sender)
Received: from [209.85.214.178] (HELO mail-ob0-f178.google.com) (209.85.214.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 22:15:49 +0000
Received: by mail-ob0-f178.google.com with SMTP id va2so2895495obc.37
        for <dev@spark.apache.org>; Wed, 21 May 2014 15:15:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=9/BjMZu53sfu4q+yZrMck1e7efOCd/dZG83eq23JtCg=;
        b=iVDMkLQQ8kDD3xwmH7+j22dPBiLhwy7g0CFTICPe2SoNsudFLm7RW+aDo9l+u7tbGN
         xbGrBYkPdZmDz5/z/mcxALyNOl7MtI9tbOU9CvGqO8tcB1GgFwxATn19MxAg+j+Unqny
         mIZBpdPKgu/IM1WUH3WBB5nPAsnpVQOmUPmxip9AeNJ67Pp3+qE32bnJOc0nQGkHx4dY
         ZFiB4LGYMN1aDCvJ1sRGcBWXEurAqB9Sf13inwit4Wm2zr1n93/Re0GUPZjnzZUBRAoO
         4qnt2A9rYypykFzcQYazKkEKfQUV0LwyUl+i4Xj0jXRDS40RMd5ZJGCLyu8Mq8/C8kQA
         nVeQ==
MIME-Version: 1.0
X-Received: by 10.60.44.100 with SMTP id d4mr44488085oem.6.1400710528723; Wed,
 21 May 2014 15:15:28 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Wed, 21 May 2014 15:15:28 -0700 (PDT)
In-Reply-To: <CAJgQjQ-Of38yGJ0SP416Rxz7e935+69aUTGyi6-DLSH0aO0_2A@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
	<CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
	<CAMAsSdKxE-biW8fRqkRn6NmqsszFgQ=f1w_Z8yt8cSOj3qBB2Q@mail.gmail.com>
	<CAEYYnxY9UZDuhBoHAP5kD3dCXxd0TwR761tt-kswjTEddQCCKg@mail.gmail.com>
	<CA+-p3AGT8E5caKU+2ExxjbpV-UqaaTot+Ni3DXEdOf37tADLgw@mail.gmail.com>
	<CAMAsSdJVckDE5=3S8wZj9P1-1-D6qBumGnofk9G7A7cksh1RVA@mail.gmail.com>
	<CABPQxstfSyVu6D-FSt7qa-o1co9AkhHsncTHGBrCK4rEM7fymg@mail.gmail.com>
	<CACBYxKLb=OK2EKu=2yS_1tswYz52rBF-59+8DZL+DfG6qHF52Q@mail.gmail.com>
	<CAEYYnxakT5SVD0xxWcyAXq+w-f-tY0rDA2c3YLRXK6oZ3ArF7Q@mail.gmail.com>
	<CAEYYnxaYfaUOes+A4kR4w+nh0LS8fgJytM2Nj7xoBErpHiRwbA@mail.gmail.com>
	<CAJgQjQ_ifNtFFK4W99FA3cJ3BapRRfLc4FomCWxdgwTqEU+icg@mail.gmail.com>
	<CACBYxK+VOs2=m8tF1xijTOAxL917Q2yo=pOKNsNfzK52a7QdhQ@mail.gmail.com>
	<CAEYYnxbBW62gVrVtuQ1MPFPgBc4mRARB+=LyRm8b21by9cmFPQ@mail.gmail.com>
	<CAJgQjQ894Gz_StdJa+GgFXgXuEiq-5Z+4-3B0Bq9XXejzXCFXA@mail.gmail.com>
	<CACBYxKKk5mTDsOevK6J8UgtJMEDC0YFsWrLeXLSChOQfFcBgBg@mail.gmail.com>
	<CAJgQjQ-Of38yGJ0SP416Rxz7e935+69aUTGyi6-DLSH0aO0_2A@mail.gmail.com>
Date: Wed, 21 May 2014 15:15:28 -0700
Message-ID: <CABPQxsvq4L2M56MgjDo5RO_XFxAU86Pf2+x43UP+PeaXsDPP5A@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Of these two solutions I'd definitely prefer 2 in the short term. I'd
imagine the fix is very straightforward (it would mostly just be
remove code), and we'd be making this more consistent with the
standalone mode which makes things way easier to reason about.

In the long term we'll definitely want to exploit the distributed
cache more, but at this point it's premature optimization at a high
complexity cost. Writing stuff to HDFS through is so slow anyways I'd
guess that serving it directly from the driver is still faster in most
cases (though for very large jar sizes or very large clusters, yes,
we'll need the distributed cache).

- Patrick

On Wed, May 21, 2014 at 2:41 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
> That's a good example. If we really want to cover that case, there are
> two solutions:
>
> 1. Follow DB's patch, adding jars to the system classloader. Then we
> cannot put a user class in front of an existing class.
> 2. Do not send the primary jar and secondary jars to executors'
> distributed cache. Instead, add them to "spark.jars" in SparkSubmit
> and serve them via http by called sc.addJar in SparkContext.
>
> What is your preference?
>
> On Wed, May 21, 2014 at 2:27 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
>> Is that an assumption we can make?  I think we'd run into an issue in this
>> situation:
>>
>> *In primary jar:*
>> def makeDynamicObject(clazz: String) = Class.forName(clazz).newInstance()
>>
>> *In app code:*
>> sc.addJar("dynamicjar.jar")
>> ...
>> rdd.map(x => makeDynamicObject("some.class.from.DynamicJar"))
>>
>> It might be fair to say that the user should make sure to use the context
>> classloader when instantiating dynamic classes, but I think it's weird that
>> this code would work on Spark standalone but not on YARN.
>>
>> -Sandy
>>
>>
>> On Wed, May 21, 2014 at 2:10 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>
>>> I think adding jars dynamically should work as long as the primary jar
>>> and the secondary jars do not depend on dynamically added jars, which
>>> should be the correct logic. -Xiangrui
>>>
>>> On Wed, May 21, 2014 at 1:40 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>> > This will be another separate story.
>>> >
>>> > Since in the yarn deployment, as Sandy said, the app.jar will be always
>>> in
>>> > the systemclassloader which means any object instantiated in app.jar will
>>> > have parent loader of systemclassloader instead of custom one. As a
>>> result,
>>> > the custom class loader in yarn will never work without specifically
>>> using
>>> > reflection.
>>> >
>>> > Solution will be not using system classloader in the classloader
>>> hierarchy,
>>> > and add all the resources in system one into custom one. This is the
>>> > approach of tomcat takes.
>>> >
>>> > Or we can directly overwirte the system class loader by calling the
>>> > protected method `addURL` which will not work and throw exception if the
>>> > code is wrapped in security manager.
>>> >
>>> >
>>> > Sincerely,
>>> >
>>> > DB Tsai
>>> > -------------------------------------------------------
>>> > My Blog: https://www.dbtsai.com
>>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>>> >
>>> >
>>> > On Wed, May 21, 2014 at 1:13 PM, Sandy Ryza <sandy.ryza@cloudera.com>
>>> wrote:
>>> >
>>> >> This will solve the issue for jars added upon application submission,
>>> but,
>>> >> on top of this, we need to make sure that anything dynamically added
>>> >> through sc.addJar works as well.
>>> >>
>>> >> To do so, we need to make sure that any jars retrieved via the driver's
>>> >> HTTP server are loaded by the same classloader that loads the jars
>>> given on
>>> >> app submission.  To achieve this, we need to either use the same
>>> >> classloader for both system jars and user jars, or make sure that the
>>> user
>>> >> jars given on app submission are under the same classloader used for
>>> >> dynamically added jars.
>>> >>
>>> >> On Tue, May 20, 2014 at 5:59 PM, Xiangrui Meng <mengxr@gmail.com>
>>> wrote:
>>> >>
>>> >> > Talked with Sandy and DB offline. I think the best solution is sending
>>> >> > the secondary jars to the distributed cache of all containers rather
>>> >> > than just the master, and set the classpath to include spark jar,
>>> >> > primary app jar, and secondary jars before executor starts. In this
>>> >> > way, user only needs to specify secondary jars via --jars instead of
>>> >> > calling sc.addJar inside the code. It also solves the scalability
>>> >> > problem of serving all the jars via http.
>>> >> >
>>> >> > If this solution sounds good, I can try to make a patch.
>>> >> >
>>> >> > Best,
>>> >> > Xiangrui
>>> >> >
>>> >> > On Mon, May 19, 2014 at 10:04 PM, DB Tsai <dbtsai@stanford.edu>
>>> wrote:
>>> >> > > In 1.0, there is a new option for users to choose which classloader
>>> has
>>> >> > > higher priority via spark.files.userClassPathFirst, I decided to
>>> submit
>>> >> > the
>>> >> > > PR for 0.9 first. We use this patch in our lab and we can use those
>>> >> jars
>>> >> > > added by sc.addJar without reflection.
>>> >> > >
>>> >> > > https://github.com/apache/spark/pull/834
>>> >> > >
>>> >> > > Can anyone comment if it's a good approach?
>>> >> > >
>>> >> > > Thanks.
>>> >> > >
>>> >> > >
>>> >> > > Sincerely,
>>> >> > >
>>> >> > > DB Tsai
>>> >> > > -------------------------------------------------------
>>> >> > > My Blog: https://www.dbtsai.com
>>> >> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>> >> > >
>>> >> > >
>>> >> > > On Mon, May 19, 2014 at 7:42 PM, DB Tsai <dbtsai@stanford.edu>
>>> wrote:
>>> >> > >
>>> >> > >> Good summary! We fixed it in branch 0.9 since our production is
>>> still
>>> >> in
>>> >> > >> 0.9. I'm porting it to 1.0 now, and hopefully will submit PR for
>>> 1.0
>>> >> > >> tonight.
>>> >> > >>
>>> >> > >>
>>> >> > >> Sincerely,
>>> >> > >>
>>> >> > >> DB Tsai
>>> >> > >> -------------------------------------------------------
>>> >> > >> My Blog: https://www.dbtsai.com
>>> >> > >> LinkedIn: https://www.linkedin.com/in/dbtsai
>>> >> > >>
>>> >> > >>
>>> >> > >> On Mon, May 19, 2014 at 7:38 PM, Sandy Ryza <
>>> sandy.ryza@cloudera.com
>>> >> > >wrote:
>>> >> > >>
>>> >> > >>> It just hit me why this problem is showing up on YARN and not on
>>> >> > >>> standalone.
>>> >> > >>>
>>> >> > >>> The relevant difference between YARN and standalone is that, on
>>> YARN,
>>> >> > the
>>> >> > >>> app jar is loaded by the system classloader instead of Spark's
>>> custom
>>> >> > URL
>>> >> > >>> classloader.
>>> >> > >>>
>>> >> > >>> On YARN, the system classloader knows about [the classes in the
>>> spark
>>> >> > >>> jars,
>>> >> > >>> the classes in the primary app jar].   The custom classloader
>>> knows
>>> >> > about
>>> >> > >>> [the classes in secondary app jars] and has the system
>>> classloader as
>>> >> > its
>>> >> > >>> parent.
>>> >> > >>>
>>> >> > >>> A few relevant facts (mostly redundant with what Sean pointed
>>> out):
>>> >> > >>> * Every class has a classloader that loaded it.
>>> >> > >>> * When an object of class B is instantiated inside of class A, the
>>> >> > >>> classloader used for loading B is the classloader that was used
>>> for
>>> >> > >>> loading
>>> >> > >>> A.
>>> >> > >>> * When a classloader fails to load a class, it lets its parent
>>> >> > classloader
>>> >> > >>> try.  If its parent succeeds, its parent becomes the "classloader
>>> >> that
>>> >> > >>> loaded it".
>>> >> > >>>
>>> >> > >>> So suppose class B is in a secondary app jar and class A is in the
>>> >> > primary
>>> >> > >>> app jar:
>>> >> > >>> 1. The custom classloader will try to load class A.
>>> >> > >>> 2. It will fail, because it only knows about the secondary jars.
>>> >> > >>> 3. It will delegate to its parent, the system classloader.
>>> >> > >>> 4. The system classloader will succeed, because it knows about the
>>> >> > primary
>>> >> > >>> app jar.
>>> >> > >>> 5. A's classloader will be the system classloader.
>>> >> > >>> 6. A tries to instantiate an instance of class B.
>>> >> > >>> 7. B will be loaded with A's classloader, which is the system
>>> >> > classloader.
>>> >> > >>> 8. Loading B will fail, because A's classloader, which is the
>>> system
>>> >> > >>> classloader, doesn't know about the secondary app jars.
>>> >> > >>>
>>> >> > >>> In Spark standalone, A and B are both loaded by the custom
>>> >> > classloader, so
>>> >> > >>> this issue doesn't come up.
>>> >> > >>>
>>> >> > >>> -Sandy
>>> >> > >>>
>>> >> > >>> On Mon, May 19, 2014 at 7:07 PM, Patrick Wendell <
>>> pwendell@gmail.com
>>> >> >
>>> >> > >>> wrote:
>>> >> > >>>
>>> >> > >>> > Having a user add define a custom class inside of an added jar
>>> and
>>> >> > >>> > instantiate it directly inside of an executor is definitely
>>> >> supported
>>> >> > >>> > in Spark and has been for a really long time (several years).
>>> This
>>> >> is
>>> >> > >>> > something we do all the time in Spark.
>>> >> > >>> >
>>> >> > >>> > DB - I'd hold off on a re-architecting of this until we identify
>>> >> > >>> > exactly what is causing the bug you are running into.
>>> >> > >>> >
>>> >> > >>> > In a nutshell, when the bytecode "new Foo()" is run on the
>>> >> executor,
>>> >> > >>> > it will ask the driver for the class over HTTP using a custom
>>> >> > >>> > classloader. Something in that pipeline is breaking here,
>>> possibly
>>> >> > >>> > related to the YARN deployment stuff.
>>> >> > >>> >
>>> >> > >>> >
>>> >> > >>> > On Mon, May 19, 2014 at 12:29 AM, Sean Owen <sowen@cloudera.com
>>> >
>>> >> > wrote:
>>> >> > >>> > > I don't think a customer classloader is necessary.
>>> >> > >>> > >
>>> >> > >>> > > Well, it occurs to me that this is no new problem. Hadoop,
>>> >> Tomcat,
>>> >> > etc
>>> >> > >>> > > all run custom user code that creates new user objects without
>>> >> > >>> > > reflection. I should go see how that's done. Maybe it's
>>> totally
>>> >> > valid
>>> >> > >>> > > to set the thread's context classloader for just this purpose,
>>> >> and
>>> >> > I
>>> >> > >>> > > am not thinking clearly.
>>> >> > >>> > >
>>> >> > >>> > > On Mon, May 19, 2014 at 8:26 AM, Andrew Ash <
>>> >> andrew@andrewash.com>
>>> >> > >>> > wrote:
>>> >> > >>> > >> Sounds like the problem is that classloaders always look in
>>> >> their
>>> >> > >>> > parents
>>> >> > >>> > >> before themselves, and Spark users want executors to pick up
>>> >> > classes
>>> >> > >>> > from
>>> >> > >>> > >> their custom code before the ones in Spark plus its
>>> >> dependencies.
>>> >> > >>> > >>
>>> >> > >>> > >> Would a custom classloader that delegates to the parent after
>>> >> > first
>>> >> > >>> > >> checking itself fix this up?
>>> >> > >>> > >>
>>> >> > >>> > >>
>>> >> > >>> > >> On Mon, May 19, 2014 at 12:17 AM, DB Tsai <
>>> dbtsai@stanford.edu>
>>> >> > >>> wrote:
>>> >> > >>> > >>
>>> >> > >>> > >>> Hi Sean,
>>> >> > >>> > >>>
>>> >> > >>> > >>> It's true that the issue here is classloader, and due to the
>>> >> > >>> > classloader
>>> >> > >>> > >>> delegation model, users have to use reflection in the
>>> executors
>>> >> > to
>>> >> > >>> > pick up
>>> >> > >>> > >>> the classloader in order to use those classes added by
>>> >> sc.addJars
>>> >> > >>> APIs.
>>> >> > >>> > >>> However, it's very inconvenience for users, and not
>>> documented
>>> >> in
>>> >> > >>> > spark.
>>> >> > >>> > >>>
>>> >> > >>> > >>> I'm working on a patch to solve it by calling the protected
>>> >> > method
>>> >> > >>> > addURL
>>> >> > >>> > >>> in URLClassLoader to update the current default
>>> classloader, so
>>> >> > no
>>> >> > >>> > >>> customClassLoader anymore. I wonder if this is an good way
>>> to
>>> >> go.
>>> >> > >>> > >>>
>>> >> > >>> > >>>   private def addURL(url: URL, loader: URLClassLoader){
>>> >> > >>> > >>>     try {
>>> >> > >>> > >>>       val method: Method =
>>> >> > >>> > >>> classOf[URLClassLoader].getDeclaredMethod("addURL",
>>> >> classOf[URL])
>>> >> > >>> > >>>       method.setAccessible(true)
>>> >> > >>> > >>>       method.invoke(loader, url)
>>> >> > >>> > >>>     }
>>> >> > >>> > >>>     catch {
>>> >> > >>> > >>>       case t: Throwable => {
>>> >> > >>> > >>>         throw new IOException("Error, could not add URL to
>>> >> system
>>> >> > >>> > >>> classloader")
>>> >> > >>> > >>>       }
>>> >> > >>> > >>>     }
>>> >> > >>> > >>>   }
>>> >> > >>> > >>>
>>> >> > >>> > >>>
>>> >> > >>> > >>>
>>> >> > >>> > >>> Sincerely,
>>> >> > >>> > >>>
>>> >> > >>> > >>> DB Tsai
>>> >> > >>> > >>> -------------------------------------------------------
>>> >> > >>> > >>> My Blog: https://www.dbtsai.com
>>> >> > >>> > >>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>> >> > >>> > >>>
>>> >> > >>> > >>>
>>> >> > >>> > >>> On Sun, May 18, 2014 at 11:57 PM, Sean Owen <
>>> >> sowen@cloudera.com>
>>> >> > >>> > wrote:
>>> >> > >>> > >>>
>>> >> > >>> > >>> > I might be stating the obvious for everyone, but the issue
>>> >> > here is
>>> >> > >>> > not
>>> >> > >>> > >>> > reflection or the source of the JAR, but the ClassLoader.
>>> The
>>> >> > >>> basic
>>> >> > >>> > >>> > rules are this.
>>> >> > >>> > >>> >
>>> >> > >>> > >>> > "new Foo" will use the ClassLoader that defines Foo. This
>>> is
>>> >> > >>> usually
>>> >> > >>> > >>> > the ClassLoader that loaded whatever it is that first
>>> >> > referenced
>>> >> > >>> Foo
>>> >> > >>> > >>> > and caused it to be loaded -- usually the ClassLoader
>>> holding
>>> >> > your
>>> >> > >>> > >>> > other app classes.
>>> >> > >>> > >>> >
>>> >> > >>> > >>> > ClassLoaders can have a parent-child relationship.
>>> >> ClassLoaders
>>> >> > >>> > always
>>> >> > >>> > >>> > look in their parent before themselves.
>>> >> > >>> > >>> >
>>> >> > >>> > >>> > (Careful then -- in contexts like Hadoop or Tomcat where
>>> your
>>> >> > app
>>> >> > >>> is
>>> >> > >>> > >>> > loaded in a child ClassLoader, and you reference a class
>>> that
>>> >> > >>> Hadoop
>>> >> > >>> > >>> > or Tomcat also has (like a lib class) you will get the
>>> >> > container's
>>> >> > >>> > >>> > version!)
>>> >> > >>> > >>> >
>>> >> > >>> > >>> > When you load an external JAR it has a separate
>>> ClassLoader
>>> >> > which
>>> >> > >>> > does
>>> >> > >>> > >>> > not necessarily bear any relation to the one containing
>>> your
>>> >> > app
>>> >> > >>> > >>> > classes, so yeah it is not generally going to make "new
>>> Foo"
>>> >> > work.
>>> >> > >>> > >>> >
>>> >> > >>> > >>> > Reflection lets you pick the ClassLoader, yes.
>>> >> > >>> > >>> >
>>> >> > >>> > >>> > I would not call setContextClassLoader.
>>> >> > >>> > >>> >
>>> >> > >>> > >>> > On Mon, May 19, 2014 at 12:00 AM, Sandy Ryza <
>>> >> > >>> > sandy.ryza@cloudera.com>
>>> >> > >>> > >>> > wrote:
>>> >> > >>> > >>> > > I spoke with DB offline about this a little while ago
>>> and
>>> >> he
>>> >> > >>> > confirmed
>>> >> > >>> > >>> > that
>>> >> > >>> > >>> > > he was able to access the jar from the driver.
>>> >> > >>> > >>> > >
>>> >> > >>> > >>> > > The issue appears to be a general Java issue: you can't
>>> >> > directly
>>> >> > >>> > >>> > > instantiate a class from a dynamically loaded jar.
>>> >> > >>> > >>> > >
>>> >> > >>> > >>> > > I reproduced it locally outside of Spark with:
>>> >> > >>> > >>> > > ---
>>> >> > >>> > >>> > >     URLClassLoader urlClassLoader = new
>>> URLClassLoader(new
>>> >> > >>> URL[] {
>>> >> > >>> > new
>>> >> > >>> > >>> > > File("myotherjar.jar").toURI().toURL() }, null);
>>> >> > >>> > >>> > >
>>> >> > >>> Thread.currentThread().setContextClassLoader(urlClassLoader);
>>> >> > >>> > >>> > >     MyClassFromMyOtherJar obj = new
>>> >> MyClassFromMyOtherJar();
>>> >> > >>> > >>> > > ---
>>> >> > >>> > >>> > >
>>> >> > >>> > >>> > > I was able to load the class with reflection.
>>> >> > >>> > >>> >
>>> >> > >>> > >>>
>>> >> > >>> >
>>> >> > >>>
>>> >> > >>
>>> >> > >>
>>> >> >
>>> >>
>>>

From dev-return-7766-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 22:22:57 2014
Return-Path: <dev-return-7766-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CA87E11BF6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 22:22:57 +0000 (UTC)
Received: (qmail 697 invoked by uid 500); 21 May 2014 22:22:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 638 invoked by uid 500); 21 May 2014 22:22:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 630 invoked by uid 99); 21 May 2014 22:22:57 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 22:22:57 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.180 as permitted sender)
Received: from [209.85.214.180] (HELO mail-ob0-f180.google.com) (209.85.214.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 22:22:53 +0000
Received: by mail-ob0-f180.google.com with SMTP id va2so2856964obc.25
        for <dev@spark.apache.org>; Wed, 21 May 2014 15:22:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=JhbcmntCZWXUOmB+lbZ4BIiXxf5x1ACzIKNKI4zvFJ4=;
        b=jPcDkJk6xIPoUFRO3lJBCqwDfHlpYAqBGGP8ytwu70XDJRrOyDdPZqFjMeXAkTKZZc
         rnRJCwAuPBclVOoHcwc/shD3gzIhoLJ4loRKHBCurhEr7hJC2TXbtFp6DprxBDI2aCbe
         S+qmFX6Ev0ej/VwUb4yVkPYxw1jvXbO3YUWV5p57r6KpLoA1ZclcQ68gdKDZM5HP0ZBG
         sJx39/6udYBjRw5KZ8Hn8db5TQpHaBhfN0uice9DQ1SPFgc6SvkkLykEbnjYrS3O/3Vy
         4wSIgIZKUH68zbgFJ7WQRLbXmmJDhikkSAUjMBQQ45rHfZGAhyeEV0tHncBapt/ga0WB
         1Qyw==
MIME-Version: 1.0
X-Received: by 10.60.54.38 with SMTP id g6mr14182060oep.79.1400710953090; Wed,
 21 May 2014 15:22:33 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Wed, 21 May 2014 15:22:32 -0700 (PDT)
In-Reply-To: <CABPQxsvq4L2M56MgjDo5RO_XFxAU86Pf2+x43UP+PeaXsDPP5A@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
	<CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
	<CAMAsSdKxE-biW8fRqkRn6NmqsszFgQ=f1w_Z8yt8cSOj3qBB2Q@mail.gmail.com>
	<CAEYYnxY9UZDuhBoHAP5kD3dCXxd0TwR761tt-kswjTEddQCCKg@mail.gmail.com>
	<CA+-p3AGT8E5caKU+2ExxjbpV-UqaaTot+Ni3DXEdOf37tADLgw@mail.gmail.com>
	<CAMAsSdJVckDE5=3S8wZj9P1-1-D6qBumGnofk9G7A7cksh1RVA@mail.gmail.com>
	<CABPQxstfSyVu6D-FSt7qa-o1co9AkhHsncTHGBrCK4rEM7fymg@mail.gmail.com>
	<CACBYxKLb=OK2EKu=2yS_1tswYz52rBF-59+8DZL+DfG6qHF52Q@mail.gmail.com>
	<CAEYYnxakT5SVD0xxWcyAXq+w-f-tY0rDA2c3YLRXK6oZ3ArF7Q@mail.gmail.com>
	<CAEYYnxaYfaUOes+A4kR4w+nh0LS8fgJytM2Nj7xoBErpHiRwbA@mail.gmail.com>
	<CAJgQjQ_ifNtFFK4W99FA3cJ3BapRRfLc4FomCWxdgwTqEU+icg@mail.gmail.com>
	<CACBYxK+VOs2=m8tF1xijTOAxL917Q2yo=pOKNsNfzK52a7QdhQ@mail.gmail.com>
	<CAEYYnxbBW62gVrVtuQ1MPFPgBc4mRARB+=LyRm8b21by9cmFPQ@mail.gmail.com>
	<CAJgQjQ894Gz_StdJa+GgFXgXuEiq-5Z+4-3B0Bq9XXejzXCFXA@mail.gmail.com>
	<CACBYxKKk5mTDsOevK6J8UgtJMEDC0YFsWrLeXLSChOQfFcBgBg@mail.gmail.com>
	<CAJgQjQ-Of38yGJ0SP416Rxz7e935+69aUTGyi6-DLSH0aO0_2A@mail.gmail.com>
	<CABPQxsvq4L2M56MgjDo5RO_XFxAU86Pf2+x43UP+PeaXsDPP5A@mail.gmail.com>
Date: Wed, 21 May 2014 15:22:32 -0700
Message-ID: <CABPQxsvuv1TM27mViZ5Mmsdf_AWpw5OvWZTMMR8vfT9FDp2qhg@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey I just looked at the fix here:
https://github.com/apache/spark/pull/848

Given that this is quite simple, maybe it's best to just go with this
and just explain that we don't support adding jars dynamically in YARN
in Spark 1.0. That seems like a reasonable thing to do.

On Wed, May 21, 2014 at 3:15 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Of these two solutions I'd definitely prefer 2 in the short term. I'd
> imagine the fix is very straightforward (it would mostly just be
> remove code), and we'd be making this more consistent with the
> standalone mode which makes things way easier to reason about.
>
> In the long term we'll definitely want to exploit the distributed
> cache more, but at this point it's premature optimization at a high
> complexity cost. Writing stuff to HDFS through is so slow anyways I'd
> guess that serving it directly from the driver is still faster in most
> cases (though for very large jar sizes or very large clusters, yes,
> we'll need the distributed cache).
>
> - Patrick
>
> On Wed, May 21, 2014 at 2:41 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>> That's a good example. If we really want to cover that case, there are
>> two solutions:
>>
>> 1. Follow DB's patch, adding jars to the system classloader. Then we
>> cannot put a user class in front of an existing class.
>> 2. Do not send the primary jar and secondary jars to executors'
>> distributed cache. Instead, add them to "spark.jars" in SparkSubmit
>> and serve them via http by called sc.addJar in SparkContext.
>>
>> What is your preference?
>>
>> On Wed, May 21, 2014 at 2:27 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
>>> Is that an assumption we can make?  I think we'd run into an issue in this
>>> situation:
>>>
>>> *In primary jar:*
>>> def makeDynamicObject(clazz: String) = Class.forName(clazz).newInstance()
>>>
>>> *In app code:*
>>> sc.addJar("dynamicjar.jar")
>>> ...
>>> rdd.map(x => makeDynamicObject("some.class.from.DynamicJar"))
>>>
>>> It might be fair to say that the user should make sure to use the context
>>> classloader when instantiating dynamic classes, but I think it's weird that
>>> this code would work on Spark standalone but not on YARN.
>>>
>>> -Sandy
>>>
>>>
>>> On Wed, May 21, 2014 at 2:10 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>>
>>>> I think adding jars dynamically should work as long as the primary jar
>>>> and the secondary jars do not depend on dynamically added jars, which
>>>> should be the correct logic. -Xiangrui
>>>>
>>>> On Wed, May 21, 2014 at 1:40 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>>>> > This will be another separate story.
>>>> >
>>>> > Since in the yarn deployment, as Sandy said, the app.jar will be always
>>>> in
>>>> > the systemclassloader which means any object instantiated in app.jar will
>>>> > have parent loader of systemclassloader instead of custom one. As a
>>>> result,
>>>> > the custom class loader in yarn will never work without specifically
>>>> using
>>>> > reflection.
>>>> >
>>>> > Solution will be not using system classloader in the classloader
>>>> hierarchy,
>>>> > and add all the resources in system one into custom one. This is the
>>>> > approach of tomcat takes.
>>>> >
>>>> > Or we can directly overwirte the system class loader by calling the
>>>> > protected method `addURL` which will not work and throw exception if the
>>>> > code is wrapped in security manager.
>>>> >
>>>> >
>>>> > Sincerely,
>>>> >
>>>> > DB Tsai
>>>> > -------------------------------------------------------
>>>> > My Blog: https://www.dbtsai.com
>>>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>> >
>>>> >
>>>> > On Wed, May 21, 2014 at 1:13 PM, Sandy Ryza <sandy.ryza@cloudera.com>
>>>> wrote:
>>>> >
>>>> >> This will solve the issue for jars added upon application submission,
>>>> but,
>>>> >> on top of this, we need to make sure that anything dynamically added
>>>> >> through sc.addJar works as well.
>>>> >>
>>>> >> To do so, we need to make sure that any jars retrieved via the driver's
>>>> >> HTTP server are loaded by the same classloader that loads the jars
>>>> given on
>>>> >> app submission.  To achieve this, we need to either use the same
>>>> >> classloader for both system jars and user jars, or make sure that the
>>>> user
>>>> >> jars given on app submission are under the same classloader used for
>>>> >> dynamically added jars.
>>>> >>
>>>> >> On Tue, May 20, 2014 at 5:59 PM, Xiangrui Meng <mengxr@gmail.com>
>>>> wrote:
>>>> >>
>>>> >> > Talked with Sandy and DB offline. I think the best solution is sending
>>>> >> > the secondary jars to the distributed cache of all containers rather
>>>> >> > than just the master, and set the classpath to include spark jar,
>>>> >> > primary app jar, and secondary jars before executor starts. In this
>>>> >> > way, user only needs to specify secondary jars via --jars instead of
>>>> >> > calling sc.addJar inside the code. It also solves the scalability
>>>> >> > problem of serving all the jars via http.
>>>> >> >
>>>> >> > If this solution sounds good, I can try to make a patch.
>>>> >> >
>>>> >> > Best,
>>>> >> > Xiangrui
>>>> >> >
>>>> >> > On Mon, May 19, 2014 at 10:04 PM, DB Tsai <dbtsai@stanford.edu>
>>>> wrote:
>>>> >> > > In 1.0, there is a new option for users to choose which classloader
>>>> has
>>>> >> > > higher priority via spark.files.userClassPathFirst, I decided to
>>>> submit
>>>> >> > the
>>>> >> > > PR for 0.9 first. We use this patch in our lab and we can use those
>>>> >> jars
>>>> >> > > added by sc.addJar without reflection.
>>>> >> > >
>>>> >> > > https://github.com/apache/spark/pull/834
>>>> >> > >
>>>> >> > > Can anyone comment if it's a good approach?
>>>> >> > >
>>>> >> > > Thanks.
>>>> >> > >
>>>> >> > >
>>>> >> > > Sincerely,
>>>> >> > >
>>>> >> > > DB Tsai
>>>> >> > > -------------------------------------------------------
>>>> >> > > My Blog: https://www.dbtsai.com
>>>> >> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>>>> >> > >
>>>> >> > >
>>>> >> > > On Mon, May 19, 2014 at 7:42 PM, DB Tsai <dbtsai@stanford.edu>
>>>> wrote:
>>>> >> > >
>>>> >> > >> Good summary! We fixed it in branch 0.9 since our production is
>>>> still
>>>> >> in
>>>> >> > >> 0.9. I'm porting it to 1.0 now, and hopefully will submit PR for
>>>> 1.0
>>>> >> > >> tonight.
>>>> >> > >>
>>>> >> > >>
>>>> >> > >> Sincerely,
>>>> >> > >>
>>>> >> > >> DB Tsai
>>>> >> > >> -------------------------------------------------------
>>>> >> > >> My Blog: https://www.dbtsai.com
>>>> >> > >> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>> >> > >>
>>>> >> > >>
>>>> >> > >> On Mon, May 19, 2014 at 7:38 PM, Sandy Ryza <
>>>> sandy.ryza@cloudera.com
>>>> >> > >wrote:
>>>> >> > >>
>>>> >> > >>> It just hit me why this problem is showing up on YARN and not on
>>>> >> > >>> standalone.
>>>> >> > >>>
>>>> >> > >>> The relevant difference between YARN and standalone is that, on
>>>> YARN,
>>>> >> > the
>>>> >> > >>> app jar is loaded by the system classloader instead of Spark's
>>>> custom
>>>> >> > URL
>>>> >> > >>> classloader.
>>>> >> > >>>
>>>> >> > >>> On YARN, the system classloader knows about [the classes in the
>>>> spark
>>>> >> > >>> jars,
>>>> >> > >>> the classes in the primary app jar].   The custom classloader
>>>> knows
>>>> >> > about
>>>> >> > >>> [the classes in secondary app jars] and has the system
>>>> classloader as
>>>> >> > its
>>>> >> > >>> parent.
>>>> >> > >>>
>>>> >> > >>> A few relevant facts (mostly redundant with what Sean pointed
>>>> out):
>>>> >> > >>> * Every class has a classloader that loaded it.
>>>> >> > >>> * When an object of class B is instantiated inside of class A, the
>>>> >> > >>> classloader used for loading B is the classloader that was used
>>>> for
>>>> >> > >>> loading
>>>> >> > >>> A.
>>>> >> > >>> * When a classloader fails to load a class, it lets its parent
>>>> >> > classloader
>>>> >> > >>> try.  If its parent succeeds, its parent becomes the "classloader
>>>> >> that
>>>> >> > >>> loaded it".
>>>> >> > >>>
>>>> >> > >>> So suppose class B is in a secondary app jar and class A is in the
>>>> >> > primary
>>>> >> > >>> app jar:
>>>> >> > >>> 1. The custom classloader will try to load class A.
>>>> >> > >>> 2. It will fail, because it only knows about the secondary jars.
>>>> >> > >>> 3. It will delegate to its parent, the system classloader.
>>>> >> > >>> 4. The system classloader will succeed, because it knows about the
>>>> >> > primary
>>>> >> > >>> app jar.
>>>> >> > >>> 5. A's classloader will be the system classloader.
>>>> >> > >>> 6. A tries to instantiate an instance of class B.
>>>> >> > >>> 7. B will be loaded with A's classloader, which is the system
>>>> >> > classloader.
>>>> >> > >>> 8. Loading B will fail, because A's classloader, which is the
>>>> system
>>>> >> > >>> classloader, doesn't know about the secondary app jars.
>>>> >> > >>>
>>>> >> > >>> In Spark standalone, A and B are both loaded by the custom
>>>> >> > classloader, so
>>>> >> > >>> this issue doesn't come up.
>>>> >> > >>>
>>>> >> > >>> -Sandy
>>>> >> > >>>
>>>> >> > >>> On Mon, May 19, 2014 at 7:07 PM, Patrick Wendell <
>>>> pwendell@gmail.com
>>>> >> >
>>>> >> > >>> wrote:
>>>> >> > >>>
>>>> >> > >>> > Having a user add define a custom class inside of an added jar
>>>> and
>>>> >> > >>> > instantiate it directly inside of an executor is definitely
>>>> >> supported
>>>> >> > >>> > in Spark and has been for a really long time (several years).
>>>> This
>>>> >> is
>>>> >> > >>> > something we do all the time in Spark.
>>>> >> > >>> >
>>>> >> > >>> > DB - I'd hold off on a re-architecting of this until we identify
>>>> >> > >>> > exactly what is causing the bug you are running into.
>>>> >> > >>> >
>>>> >> > >>> > In a nutshell, when the bytecode "new Foo()" is run on the
>>>> >> executor,
>>>> >> > >>> > it will ask the driver for the class over HTTP using a custom
>>>> >> > >>> > classloader. Something in that pipeline is breaking here,
>>>> possibly
>>>> >> > >>> > related to the YARN deployment stuff.
>>>> >> > >>> >
>>>> >> > >>> >
>>>> >> > >>> > On Mon, May 19, 2014 at 12:29 AM, Sean Owen <sowen@cloudera.com
>>>> >
>>>> >> > wrote:
>>>> >> > >>> > > I don't think a customer classloader is necessary.
>>>> >> > >>> > >
>>>> >> > >>> > > Well, it occurs to me that this is no new problem. Hadoop,
>>>> >> Tomcat,
>>>> >> > etc
>>>> >> > >>> > > all run custom user code that creates new user objects without
>>>> >> > >>> > > reflection. I should go see how that's done. Maybe it's
>>>> totally
>>>> >> > valid
>>>> >> > >>> > > to set the thread's context classloader for just this purpose,
>>>> >> and
>>>> >> > I
>>>> >> > >>> > > am not thinking clearly.
>>>> >> > >>> > >
>>>> >> > >>> > > On Mon, May 19, 2014 at 8:26 AM, Andrew Ash <
>>>> >> andrew@andrewash.com>
>>>> >> > >>> > wrote:
>>>> >> > >>> > >> Sounds like the problem is that classloaders always look in
>>>> >> their
>>>> >> > >>> > parents
>>>> >> > >>> > >> before themselves, and Spark users want executors to pick up
>>>> >> > classes
>>>> >> > >>> > from
>>>> >> > >>> > >> their custom code before the ones in Spark plus its
>>>> >> dependencies.
>>>> >> > >>> > >>
>>>> >> > >>> > >> Would a custom classloader that delegates to the parent after
>>>> >> > first
>>>> >> > >>> > >> checking itself fix this up?
>>>> >> > >>> > >>
>>>> >> > >>> > >>
>>>> >> > >>> > >> On Mon, May 19, 2014 at 12:17 AM, DB Tsai <
>>>> dbtsai@stanford.edu>
>>>> >> > >>> wrote:
>>>> >> > >>> > >>
>>>> >> > >>> > >>> Hi Sean,
>>>> >> > >>> > >>>
>>>> >> > >>> > >>> It's true that the issue here is classloader, and due to the
>>>> >> > >>> > classloader
>>>> >> > >>> > >>> delegation model, users have to use reflection in the
>>>> executors
>>>> >> > to
>>>> >> > >>> > pick up
>>>> >> > >>> > >>> the classloader in order to use those classes added by
>>>> >> sc.addJars
>>>> >> > >>> APIs.
>>>> >> > >>> > >>> However, it's very inconvenience for users, and not
>>>> documented
>>>> >> in
>>>> >> > >>> > spark.
>>>> >> > >>> > >>>
>>>> >> > >>> > >>> I'm working on a patch to solve it by calling the protected
>>>> >> > method
>>>> >> > >>> > addURL
>>>> >> > >>> > >>> in URLClassLoader to update the current default
>>>> classloader, so
>>>> >> > no
>>>> >> > >>> > >>> customClassLoader anymore. I wonder if this is an good way
>>>> to
>>>> >> go.
>>>> >> > >>> > >>>
>>>> >> > >>> > >>>   private def addURL(url: URL, loader: URLClassLoader){
>>>> >> > >>> > >>>     try {
>>>> >> > >>> > >>>       val method: Method =
>>>> >> > >>> > >>> classOf[URLClassLoader].getDeclaredMethod("addURL",
>>>> >> classOf[URL])
>>>> >> > >>> > >>>       method.setAccessible(true)
>>>> >> > >>> > >>>       method.invoke(loader, url)
>>>> >> > >>> > >>>     }
>>>> >> > >>> > >>>     catch {
>>>> >> > >>> > >>>       case t: Throwable => {
>>>> >> > >>> > >>>         throw new IOException("Error, could not add URL to
>>>> >> system
>>>> >> > >>> > >>> classloader")
>>>> >> > >>> > >>>       }
>>>> >> > >>> > >>>     }
>>>> >> > >>> > >>>   }
>>>> >> > >>> > >>>
>>>> >> > >>> > >>>
>>>> >> > >>> > >>>
>>>> >> > >>> > >>> Sincerely,
>>>> >> > >>> > >>>
>>>> >> > >>> > >>> DB Tsai
>>>> >> > >>> > >>> -------------------------------------------------------
>>>> >> > >>> > >>> My Blog: https://www.dbtsai.com
>>>> >> > >>> > >>> LinkedIn: https://www.linkedin.com/in/dbtsai
>>>> >> > >>> > >>>
>>>> >> > >>> > >>>
>>>> >> > >>> > >>> On Sun, May 18, 2014 at 11:57 PM, Sean Owen <
>>>> >> sowen@cloudera.com>
>>>> >> > >>> > wrote:
>>>> >> > >>> > >>>
>>>> >> > >>> > >>> > I might be stating the obvious for everyone, but the issue
>>>> >> > here is
>>>> >> > >>> > not
>>>> >> > >>> > >>> > reflection or the source of the JAR, but the ClassLoader.
>>>> The
>>>> >> > >>> basic
>>>> >> > >>> > >>> > rules are this.
>>>> >> > >>> > >>> >
>>>> >> > >>> > >>> > "new Foo" will use the ClassLoader that defines Foo. This
>>>> is
>>>> >> > >>> usually
>>>> >> > >>> > >>> > the ClassLoader that loaded whatever it is that first
>>>> >> > referenced
>>>> >> > >>> Foo
>>>> >> > >>> > >>> > and caused it to be loaded -- usually the ClassLoader
>>>> holding
>>>> >> > your
>>>> >> > >>> > >>> > other app classes.
>>>> >> > >>> > >>> >
>>>> >> > >>> > >>> > ClassLoaders can have a parent-child relationship.
>>>> >> ClassLoaders
>>>> >> > >>> > always
>>>> >> > >>> > >>> > look in their parent before themselves.
>>>> >> > >>> > >>> >
>>>> >> > >>> > >>> > (Careful then -- in contexts like Hadoop or Tomcat where
>>>> your
>>>> >> > app
>>>> >> > >>> is
>>>> >> > >>> > >>> > loaded in a child ClassLoader, and you reference a class
>>>> that
>>>> >> > >>> Hadoop
>>>> >> > >>> > >>> > or Tomcat also has (like a lib class) you will get the
>>>> >> > container's
>>>> >> > >>> > >>> > version!)
>>>> >> > >>> > >>> >
>>>> >> > >>> > >>> > When you load an external JAR it has a separate
>>>> ClassLoader
>>>> >> > which
>>>> >> > >>> > does
>>>> >> > >>> > >>> > not necessarily bear any relation to the one containing
>>>> your
>>>> >> > app
>>>> >> > >>> > >>> > classes, so yeah it is not generally going to make "new
>>>> Foo"
>>>> >> > work.
>>>> >> > >>> > >>> >
>>>> >> > >>> > >>> > Reflection lets you pick the ClassLoader, yes.
>>>> >> > >>> > >>> >
>>>> >> > >>> > >>> > I would not call setContextClassLoader.
>>>> >> > >>> > >>> >
>>>> >> > >>> > >>> > On Mon, May 19, 2014 at 12:00 AM, Sandy Ryza <
>>>> >> > >>> > sandy.ryza@cloudera.com>
>>>> >> > >>> > >>> > wrote:
>>>> >> > >>> > >>> > > I spoke with DB offline about this a little while ago
>>>> and
>>>> >> he
>>>> >> > >>> > confirmed
>>>> >> > >>> > >>> > that
>>>> >> > >>> > >>> > > he was able to access the jar from the driver.
>>>> >> > >>> > >>> > >
>>>> >> > >>> > >>> > > The issue appears to be a general Java issue: you can't
>>>> >> > directly
>>>> >> > >>> > >>> > > instantiate a class from a dynamically loaded jar.
>>>> >> > >>> > >>> > >
>>>> >> > >>> > >>> > > I reproduced it locally outside of Spark with:
>>>> >> > >>> > >>> > > ---
>>>> >> > >>> > >>> > >     URLClassLoader urlClassLoader = new
>>>> URLClassLoader(new
>>>> >> > >>> URL[] {
>>>> >> > >>> > new
>>>> >> > >>> > >>> > > File("myotherjar.jar").toURI().toURL() }, null);
>>>> >> > >>> > >>> > >
>>>> >> > >>> Thread.currentThread().setContextClassLoader(urlClassLoader);
>>>> >> > >>> > >>> > >     MyClassFromMyOtherJar obj = new
>>>> >> MyClassFromMyOtherJar();
>>>> >> > >>> > >>> > > ---
>>>> >> > >>> > >>> > >
>>>> >> > >>> > >>> > > I was able to load the class with reflection.
>>>> >> > >>> > >>> >
>>>> >> > >>> > >>>
>>>> >> > >>> >
>>>> >> > >>>
>>>> >> > >>
>>>> >> > >>
>>>> >> >
>>>> >>
>>>>

From dev-return-7767-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 22:35:17 2014
Return-Path: <dev-return-7767-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 13A7B11C5A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 22:35:17 +0000 (UTC)
Received: (qmail 25867 invoked by uid 500); 21 May 2014 22:35:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25761 invoked by uid 500); 21 May 2014 22:35:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25673 invoked by uid 99); 21 May 2014 22:35:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 22:35:16 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of kevin.markey@oracle.com designates 156.151.31.81 as permitted sender)
Received: from [156.151.31.81] (HELO userp1040.oracle.com) (156.151.31.81)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 22:35:10 +0000
Received: from acsinet21.oracle.com (acsinet21.oracle.com [141.146.126.237])
	by userp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2) with ESMTP id s4LMYm6M024467
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=OK)
	for <dev@spark.apache.org>; Wed, 21 May 2014 22:34:49 GMT
Received: from aserz7021.oracle.com (aserz7021.oracle.com [141.146.126.230])
	by acsinet21.oracle.com (8.14.4+Sun/8.14.4) with ESMTP id s4LMYlSS029691
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=NO)
	for <dev@spark.apache.org>; Wed, 21 May 2014 22:34:48 GMT
Received: from abhmp0017.oracle.com (abhmp0017.oracle.com [141.146.116.23])
	by aserz7021.oracle.com (8.14.4+Sun/8.14.4) with ESMTP id s4LMYlAb029686
	for <dev@spark.apache.org>; Wed, 21 May 2014 22:34:47 GMT
Received: from [10.135.123.92] (/10.135.123.92)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Wed, 21 May 2014 15:34:47 -0700
Message-ID: <537D2A07.4030702@oracle.com>
Date: Wed, 21 May 2014 16:34:47 -0600
From: Kevin Markey <kevin.markey@oracle.com>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20100101 Thunderbird/24.5.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC10)
References: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>	<CALuGr6YvTxM+YyXx-+p3XsfcXT2wWE3iJ0T43HqJzKfs6s2UYQ@mail.gmail.com> <CAAsvFPkk+TVQiAu+_XiDOOgqUr4tck8ccA=YQ5Aiv_SF7hn7Dg@mail.gmail.com>
In-Reply-To: <CAAsvFPkk+TVQiAu+_XiDOOgqUr4tck8ccA=YQ5Aiv_SF7hn7Dg@mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed
Content-Transfer-Encoding: 7bit
X-Source-IP: acsinet21.oracle.com [141.146.126.237]
X-Virus-Checked: Checked by ClamAV on apache.org

0

Abstaining because I'm not sure if my failures are due to Spark, 
configuration, or other factors...

Compiled and deployed RC10 for YARN, Hadoop 2.3 per Spark 1.0.0 Yarn 
documentation.  No problems.
Rebuilt applications against RC10 and Hadoop 2.3.0 (plain vanilla Apache 
release).
Updated scripts for various applications.
Application had successfully compiled and run against Spark 0.9.1 and 
Hadoop 2.3.0.
Ran in "yarn-cluster" mode.
Application ran to conclusion except that it ultimately failed because 
of an exception when Spark tried to clean up the staging directory.  
Also, where before Yarn would report the running program as "RUNNING", 
it only reported this application as "ACCEPTED".  It appeared to run two 
containers when the first instance never reported that it was RUNNING.

I will post a separate note to the USER list about the specifics.

Thanks
Kevin Markey


On 05/21/2014 10:58 AM, Mark Hamstra wrote:
> +1
>
>
> On Tue, May 20, 2014 at 11:09 PM, Henry Saputra <henry.saputra@gmail.com>wrote:
>
>> Signature and hash for source looks good
>> No external executable package with source - good
>> Compiled with git and maven - good
>> Ran examples and sample programs locally and standalone -good
>>
>> +1
>>
>> - Henry
>>
>>
>>
>> On Tue, May 20, 2014 at 1:13 PM, Tathagata Das
>> <tathagata.das1565@gmail.com> wrote:
>>> Please vote on releasing the following candidate as Apache Spark version
>> 1.0.0!
>>> This has a few bug fixes on top of rc9:
>>> SPARK-1875: https://github.com/apache/spark/pull/824
>>> SPARK-1876: https://github.com/apache/spark/pull/819
>>> SPARK-1878: https://github.com/apache/spark/pull/822
>>> SPARK-1879: https://github.com/apache/spark/pull/823
>>>
>>> The tag to be voted on is v1.0.0-rc10 (commit d8070234):
>>>
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=d807023479ce10aec28ef3c1ab646ddefc2e663c
>>> The release files, including signatures, digests, etc. can be found at:
>>> http://people.apache.org/~tdas/spark-1.0.0-rc10/
>>>
>>> The release artifacts are signed with the following key:
>>> https://people.apache.org/keys/committer/tdas.asc
>>>
>>> The staging repository for this release can be found at:
>>> https://repository.apache.org/content/repositories/orgapachespark-1018/
>>>
>>> The documentation corresponding to this release can be found at:
>>> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>>
>>> The full list of changes in this release can be found at:
>>>
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=blob;f=CHANGES.txt;h=d21f0ace6326e099360975002797eb7cba9d5273;hb=d807023479ce10aec28ef3c1ab646ddefc2e663c
>>> Please vote on releasing this package as Apache Spark 1.0.0!
>>>
>>> The vote is open until Friday, May 23, at 20:00 UTC and passes if
>>> amajority of at least 3 +1 PMC votes are cast.
>>>
>>> [ ] +1 Release this package as Apache Spark 1.0.0
>>> [ ] -1 Do not release this package because ...
>>>
>>> To learn more about Apache Spark, please see
>>> http://spark.apache.org/
>>>
>>> ====== API Changes ======
>>> We welcome users to compile Spark applications against 1.0. There are
>>> a few API changes in this release. Here are links to the associated
>>> upgrade guides - user facing changes have been kept as small as
>>> possible.
>>>
>>> Changes to ML vector specification:
>>>
>> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/mllib-guide.html#from-09-to-10
>>> Changes to the Java API:
>>>
>> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>>> Changes to the streaming API:
>>>
>> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>>> Changes to the GraphX API:
>>>
>> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>>> Other changes:
>>> coGroup and related functions now return Iterable[T] instead of Seq[T]
>>> ==> Call toSeq on the result to restore the old behavior
>>>
>>> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
>>> ==> Call toSeq on the result to restore old behavior


From dev-return-7768-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 22:50:37 2014
Return-Path: <dev-return-7768-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 69C0D11CDE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 22:50:37 +0000 (UTC)
Received: (qmail 52916 invoked by uid 500); 21 May 2014 22:50:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52847 invoked by uid 500); 21 May 2014 22:50:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52839 invoked by uid 99); 21 May 2014 22:50:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 22:50:37 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 22:50:33 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 7362010134B
	for <dev@spark.apache.org>; Wed, 21 May 2014 15:50:11 -0700 (PDT)
Received: from mail-qg0-f51.google.com (mail-qg0-f51.google.com [209.85.192.51])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 7781E101B01
	for <dev@spark.apache.org>; Wed, 21 May 2014 15:50:10 -0700 (PDT)
Received: by mail-qg0-f51.google.com with SMTP id q107so4393742qgd.10
        for <dev@spark.apache.org>; Wed, 21 May 2014 15:50:09 -0700 (PDT)
X-Gm-Message-State: ALoCoQnM1cgvkKoebNU0eFtKUPl0zxgMu4zW562x9DhmFq+lhHGdP13OHksiDKRM+dUzYMXPuF5i
MIME-Version: 1.0
X-Received: by 10.140.47.167 with SMTP id m36mr71448838qga.21.1400712609675;
 Wed, 21 May 2014 15:50:09 -0700 (PDT)
Received: by 10.229.96.201 with HTTP; Wed, 21 May 2014 15:50:09 -0700 (PDT)
In-Reply-To: <CANx3uAjGpOC+MXbQQCWWdTgyKAN3wm6y095R_+-+=PzAaaAHng@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
	<CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
	<CAMAsSdKxE-biW8fRqkRn6NmqsszFgQ=f1w_Z8yt8cSOj3qBB2Q@mail.gmail.com>
	<CAEYYnxY9UZDuhBoHAP5kD3dCXxd0TwR761tt-kswjTEddQCCKg@mail.gmail.com>
	<CA+-p3AGT8E5caKU+2ExxjbpV-UqaaTot+Ni3DXEdOf37tADLgw@mail.gmail.com>
	<CAMAsSdJVckDE5=3S8wZj9P1-1-D6qBumGnofk9G7A7cksh1RVA@mail.gmail.com>
	<CABPQxstfSyVu6D-FSt7qa-o1co9AkhHsncTHGBrCK4rEM7fymg@mail.gmail.com>
	<CACBYxKLb=OK2EKu=2yS_1tswYz52rBF-59+8DZL+DfG6qHF52Q@mail.gmail.com>
	<CAEYYnxakT5SVD0xxWcyAXq+w-f-tY0rDA2c3YLRXK6oZ3ArF7Q@mail.gmail.com>
	<CAEYYnxaYfaUOes+A4kR4w+nh0LS8fgJytM2Nj7xoBErpHiRwbA@mail.gmail.com>
	<CANx3uAjGpOC+MXbQQCWWdTgyKAN3wm6y095R_+-+=PzAaaAHng@mail.gmail.com>
Date: Wed, 21 May 2014 15:50:09 -0700
Message-ID: <CAEYYnxZhLxdjBQD0BqEr9Su4UZ89ZnABQWoKYCqooO52Gm9Psw@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c1641e8850dd04f9f0d110
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1641e8850dd04f9f0d110
Content-Type: text/plain; charset=UTF-8

@Xiangrui
How about we send the primary jar and secondary jars into distributed cache
without adding them into the system classloader of executors. Then we add
them using custom classloader so we don't need to call secondary jars
through reflection in primary jar. This will be consistent to what we do in
standalone mode, and also solve the scalability of jar distribution issue.

@Koert
Yes, that's why I suggest we can either ignore the parent classloader of
custom class loader to solve this as you say. In this case, we need add the
all the classpath of the system loader into our custom one (which doesn't
have parent) so we will not miss the default java classes. This is how
tomcat works.

@Patrick
I agree that we should have the fix by Xiangrui first, since it solves most
of the use case. I don't know when people will use dynamical addJar in Yarn
since it's most useful for interactive environment.


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Wed, May 21, 2014 at 2:57 PM, Koert Kuipers <koert@tresata.com> wrote:

> db tsai, i do not think userClassPathFirst is working, unless the classes
> you load dont reference any classes already loaded by the parent
> classloader (a mostly hypothetical situation)... i filed a jira for this
> here:
> https://issues.apache.org/jira/browse/SPARK-1863
>
>
>
> On Tue, May 20, 2014 at 1:04 AM, DB Tsai <dbtsai@stanford.edu> wrote:
>
> > In 1.0, there is a new option for users to choose which classloader has
> > higher priority via spark.files.userClassPathFirst, I decided to submit
> the
> > PR for 0.9 first. We use this patch in our lab and we can use those jars
> > added by sc.addJar without reflection.
> >
> > https://github.com/apache/spark/pull/834
> >
> > Can anyone comment if it's a good approach?
> >
> > Thanks.
> >
> >
> > Sincerely,
> >
> > DB Tsai
> > -------------------------------------------------------
> > My Blog: https://www.dbtsai.com
> > LinkedIn: https://www.linkedin.com/in/dbtsai
> >
> >
> > On Mon, May 19, 2014 at 7:42 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> >
> > > Good summary! We fixed it in branch 0.9 since our production is still
> in
> > > 0.9. I'm porting it to 1.0 now, and hopefully will submit PR for 1.0
> > > tonight.
> > >
> > >
> > > Sincerely,
> > >
> > > DB Tsai
> > > -------------------------------------------------------
> > > My Blog: https://www.dbtsai.com
> > > LinkedIn: https://www.linkedin.com/in/dbtsai
> > >
> > >
> > > On Mon, May 19, 2014 at 7:38 PM, Sandy Ryza <sandy.ryza@cloudera.com
> > >wrote:
> > >
> > >> It just hit me why this problem is showing up on YARN and not on
> > >> standalone.
> > >>
> > >> The relevant difference between YARN and standalone is that, on YARN,
> > the
> > >> app jar is loaded by the system classloader instead of Spark's custom
> > URL
> > >> classloader.
> > >>
> > >> On YARN, the system classloader knows about [the classes in the spark
> > >> jars,
> > >> the classes in the primary app jar].   The custom classloader knows
> > about
> > >> [the classes in secondary app jars] and has the system classloader as
> > its
> > >> parent.
> > >>
> > >> A few relevant facts (mostly redundant with what Sean pointed out):
> > >> * Every class has a classloader that loaded it.
> > >> * When an object of class B is instantiated inside of class A, the
> > >> classloader used for loading B is the classloader that was used for
> > >> loading
> > >> A.
> > >> * When a classloader fails to load a class, it lets its parent
> > classloader
> > >> try.  If its parent succeeds, its parent becomes the "classloader that
> > >> loaded it".
> > >>
> > >> So suppose class B is in a secondary app jar and class A is in the
> > primary
> > >> app jar:
> > >> 1. The custom classloader will try to load class A.
> > >> 2. It will fail, because it only knows about the secondary jars.
> > >> 3. It will delegate to its parent, the system classloader.
> > >> 4. The system classloader will succeed, because it knows about the
> > primary
> > >> app jar.
> > >> 5. A's classloader will be the system classloader.
> > >> 6. A tries to instantiate an instance of class B.
> > >> 7. B will be loaded with A's classloader, which is the system
> > classloader.
> > >> 8. Loading B will fail, because A's classloader, which is the system
> > >> classloader, doesn't know about the secondary app jars.
> > >>
> > >> In Spark standalone, A and B are both loaded by the custom
> classloader,
> > so
> > >> this issue doesn't come up.
> > >>
> > >> -Sandy
> > >>
> > >> On Mon, May 19, 2014 at 7:07 PM, Patrick Wendell <pwendell@gmail.com>
> > >> wrote:
> > >>
> > >> > Having a user add define a custom class inside of an added jar and
> > >> > instantiate it directly inside of an executor is definitely
> supported
> > >> > in Spark and has been for a really long time (several years). This
> is
> > >> > something we do all the time in Spark.
> > >> >
> > >> > DB - I'd hold off on a re-architecting of this until we identify
> > >> > exactly what is causing the bug you are running into.
> > >> >
> > >> > In a nutshell, when the bytecode "new Foo()" is run on the executor,
> > >> > it will ask the driver for the class over HTTP using a custom
> > >> > classloader. Something in that pipeline is breaking here, possibly
> > >> > related to the YARN deployment stuff.
> > >> >
> > >> >
> > >> > On Mon, May 19, 2014 at 12:29 AM, Sean Owen <sowen@cloudera.com>
> > wrote:
> > >> > > I don't think a customer classloader is necessary.
> > >> > >
> > >> > > Well, it occurs to me that this is no new problem. Hadoop, Tomcat,
> > etc
> > >> > > all run custom user code that creates new user objects without
> > >> > > reflection. I should go see how that's done. Maybe it's totally
> > valid
> > >> > > to set the thread's context classloader for just this purpose,
> and I
> > >> > > am not thinking clearly.
> > >> > >
> > >> > > On Mon, May 19, 2014 at 8:26 AM, Andrew Ash <andrew@andrewash.com
> >
> > >> > wrote:
> > >> > >> Sounds like the problem is that classloaders always look in their
> > >> > parents
> > >> > >> before themselves, and Spark users want executors to pick up
> > classes
> > >> > from
> > >> > >> their custom code before the ones in Spark plus its dependencies.
> > >> > >>
> > >> > >> Would a custom classloader that delegates to the parent after
> first
> > >> > >> checking itself fix this up?
> > >> > >>
> > >> > >>
> > >> > >> On Mon, May 19, 2014 at 12:17 AM, DB Tsai <dbtsai@stanford.edu>
> > >> wrote:
> > >> > >>
> > >> > >>> Hi Sean,
> > >> > >>>
> > >> > >>> It's true that the issue here is classloader, and due to the
> > >> > classloader
> > >> > >>> delegation model, users have to use reflection in the executors
> to
> > >> > pick up
> > >> > >>> the classloader in order to use those classes added by
> sc.addJars
> > >> APIs.
> > >> > >>> However, it's very inconvenience for users, and not documented
> in
> > >> > spark.
> > >> > >>>
> > >> > >>> I'm working on a patch to solve it by calling the protected
> method
> > >> > addURL
> > >> > >>> in URLClassLoader to update the current default classloader, so
> no
> > >> > >>> customClassLoader anymore. I wonder if this is an good way to
> go.
> > >> > >>>
> > >> > >>>   private def addURL(url: URL, loader: URLClassLoader){
> > >> > >>>     try {
> > >> > >>>       val method: Method =
> > >> > >>> classOf[URLClassLoader].getDeclaredMethod("addURL",
> classOf[URL])
> > >> > >>>       method.setAccessible(true)
> > >> > >>>       method.invoke(loader, url)
> > >> > >>>     }
> > >> > >>>     catch {
> > >> > >>>       case t: Throwable => {
> > >> > >>>         throw new IOException("Error, could not add URL to
> system
> > >> > >>> classloader")
> > >> > >>>       }
> > >> > >>>     }
> > >> > >>>   }
> > >> > >>>
> > >> > >>>
> > >> > >>>
> > >> > >>> Sincerely,
> > >> > >>>
> > >> > >>> DB Tsai
> > >> > >>> -------------------------------------------------------
> > >> > >>> My Blog: https://www.dbtsai.com
> > >> > >>> LinkedIn: https://www.linkedin.com/in/dbtsai
> > >> > >>>
> > >> > >>>
> > >> > >>> On Sun, May 18, 2014 at 11:57 PM, Sean Owen <sowen@cloudera.com
> >
> > >> > wrote:
> > >> > >>>
> > >> > >>> > I might be stating the obvious for everyone, but the issue
> here
> > is
> > >> > not
> > >> > >>> > reflection or the source of the JAR, but the ClassLoader. The
> > >> basic
> > >> > >>> > rules are this.
> > >> > >>> >
> > >> > >>> > "new Foo" will use the ClassLoader that defines Foo. This is
> > >> usually
> > >> > >>> > the ClassLoader that loaded whatever it is that first
> referenced
> > >> Foo
> > >> > >>> > and caused it to be loaded -- usually the ClassLoader holding
> > your
> > >> > >>> > other app classes.
> > >> > >>> >
> > >> > >>> > ClassLoaders can have a parent-child relationship.
> ClassLoaders
> > >> > always
> > >> > >>> > look in their parent before themselves.
> > >> > >>> >
> > >> > >>> > (Careful then -- in contexts like Hadoop or Tomcat where your
> > app
> > >> is
> > >> > >>> > loaded in a child ClassLoader, and you reference a class that
> > >> Hadoop
> > >> > >>> > or Tomcat also has (like a lib class) you will get the
> > container's
> > >> > >>> > version!)
> > >> > >>> >
> > >> > >>> > When you load an external JAR it has a separate ClassLoader
> > which
> > >> > does
> > >> > >>> > not necessarily bear any relation to the one containing your
> app
> > >> > >>> > classes, so yeah it is not generally going to make "new Foo"
> > work.
> > >> > >>> >
> > >> > >>> > Reflection lets you pick the ClassLoader, yes.
> > >> > >>> >
> > >> > >>> > I would not call setContextClassLoader.
> > >> > >>> >
> > >> > >>> > On Mon, May 19, 2014 at 12:00 AM, Sandy Ryza <
> > >> > sandy.ryza@cloudera.com>
> > >> > >>> > wrote:
> > >> > >>> > > I spoke with DB offline about this a little while ago and he
> > >> > confirmed
> > >> > >>> > that
> > >> > >>> > > he was able to access the jar from the driver.
> > >> > >>> > >
> > >> > >>> > > The issue appears to be a general Java issue: you can't
> > directly
> > >> > >>> > > instantiate a class from a dynamically loaded jar.
> > >> > >>> > >
> > >> > >>> > > I reproduced it locally outside of Spark with:
> > >> > >>> > > ---
> > >> > >>> > >     URLClassLoader urlClassLoader = new URLClassLoader(new
> > >> URL[] {
> > >> > new
> > >> > >>> > > File("myotherjar.jar").toURI().toURL() }, null);
> > >> > >>> > >
> > >> Thread.currentThread().setContextClassLoader(urlClassLoader);
> > >> > >>> > >     MyClassFromMyOtherJar obj = new MyClassFromMyOtherJar();
> > >> > >>> > > ---
> > >> > >>> > >
> > >> > >>> > > I was able to load the class with reflection.
> > >> > >>> >
> > >> > >>>
> > >> >
> > >>
> > >
> > >
> >
>

--001a11c1641e8850dd04f9f0d110--

From dev-return-7769-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 21 23:19:30 2014
Return-Path: <dev-return-7769-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 276A811DDE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 May 2014 23:19:30 +0000 (UTC)
Received: (qmail 15253 invoked by uid 500); 21 May 2014 23:19:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15195 invoked by uid 500); 21 May 2014 23:19:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15172 invoked by uid 99); 21 May 2014 23:19:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 23:19:28 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rarecactus@gmail.com designates 209.85.212.176 as permitted sender)
Received: from [209.85.212.176] (HELO mail-wi0-f176.google.com) (209.85.212.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 May 2014 23:19:24 +0000
Received: by mail-wi0-f176.google.com with SMTP id n15so8417391wiw.9
        for <dev@spark.apache.org>; Wed, 21 May 2014 16:19:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=COcnQMa+tQ/73zjt7IrmauZr/hf4x0XMyokjxRcWVyg=;
        b=N5HejMWDAAi7fppb9seTIsiZZ8KIrqgxGs4jR3d8toI/B2Na+3wl2Oq93gV3EPvPV1
         lDeuRkjkYjpCvfGB9dTsT0E1hmm0CUDYyXh0qnBaoyl1/pcv05/eKlzCDPR2uJ/J6wqD
         SAtdBCSL48qNwS8sZCxPyHn+dqtdGTHBxJXKAhSvP2o7Wa99Jzr3TG52T4redodBnKEk
         MRy2Mgt332yz+PPIrLxvel00r33vCtFM9ndyE9d88TDNRuTeTFY/lRhI4+3gp1rP4gYy
         RGyPv+QNZCRfpDxgreALbvrlXGkls2zEOYhnPptHSsjE1iRGAGZV63ML4NJLQOuWaYoW
         zGAg==
MIME-Version: 1.0
X-Received: by 10.180.187.111 with SMTP id fr15mr12928522wic.57.1400714341605;
 Wed, 21 May 2014 16:19:01 -0700 (PDT)
Sender: rarecactus@gmail.com
Received: by 10.194.122.135 with HTTP; Wed, 21 May 2014 16:19:01 -0700 (PDT)
In-Reply-To: <537D2A07.4030702@oracle.com>
References: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>
	<CALuGr6YvTxM+YyXx-+p3XsfcXT2wWE3iJ0T43HqJzKfs6s2UYQ@mail.gmail.com>
	<CAAsvFPkk+TVQiAu+_XiDOOgqUr4tck8ccA=YQ5Aiv_SF7hn7Dg@mail.gmail.com>
	<537D2A07.4030702@oracle.com>
Date: Wed, 21 May 2014 16:19:01 -0700
X-Google-Sender-Auth: tberkos3enyRINmYUlmItSWUrZo
Message-ID: <CA+qbEUNKVF2xb3SM-EXwiTom7+qX+ZTbeNS6nHVvfp3-_a=fNw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC10)
From: Colin McCabe <cmccabe@alumni.cmu.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c265dec36a0e04f9f13811
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c265dec36a0e04f9f13811
Content-Type: text/plain; charset=UTF-8

Hi Kevin,

Can you try https://issues.apache.org/jira/browse/SPARK-1898 to see if it
fixes your issue?

Running in YARN cluster mode, I had a similar issue where Spark was able to
create a Driver and an Executor via YARN, but then it stopped making any
progress.

Note: I was using a pre-release version of CDH5.1.0, not 2.3 like you were
using.

best,
Colin


On Wed, May 21, 2014 at 3:34 PM, Kevin Markey <kevin.markey@oracle.com>wrote:

> 0
>
> Abstaining because I'm not sure if my failures are due to Spark,
> configuration, or other factors...
>
> Compiled and deployed RC10 for YARN, Hadoop 2.3 per Spark 1.0.0 Yarn
> documentation.  No problems.
> Rebuilt applications against RC10 and Hadoop 2.3.0 (plain vanilla Apache
> release).
> Updated scripts for various applications.
> Application had successfully compiled and run against Spark 0.9.1 and
> Hadoop 2.3.0.
> Ran in "yarn-cluster" mode.
> Application ran to conclusion except that it ultimately failed because of
> an exception when Spark tried to clean up the staging directory.  Also,
> where before Yarn would report the running program as "RUNNING", it only
> reported this application as "ACCEPTED".  It appeared to run two containers
> when the first instance never reported that it was RUNNING.
>
> I will post a separate note to the USER list about the specifics.
>
> Thanks
> Kevin Markey
>
>
>
> On 05/21/2014 10:58 AM, Mark Hamstra wrote:
>
>> +1
>>
>>
>> On Tue, May 20, 2014 at 11:09 PM, Henry Saputra <henry.saputra@gmail.com>
>> wrote:
>>
>>  Signature and hash for source looks good
>>> No external executable package with source - good
>>> Compiled with git and maven - good
>>> Ran examples and sample programs locally and standalone -good
>>>
>>> +1
>>>
>>> - Henry
>>>
>>>
>>>
>>> On Tue, May 20, 2014 at 1:13 PM, Tathagata Das
>>> <tathagata.das1565@gmail.com> wrote:
>>>
>>>> Please vote on releasing the following candidate as Apache Spark version
>>>>
>>> 1.0.0!
>>>
>>>> This has a few bug fixes on top of rc9:
>>>> SPARK-1875: https://github.com/apache/spark/pull/824
>>>> SPARK-1876: https://github.com/apache/spark/pull/819
>>>> SPARK-1878: https://github.com/apache/spark/pull/822
>>>> SPARK-1879: https://github.com/apache/spark/pull/823
>>>>
>>>> The tag to be voted on is v1.0.0-rc10 (commit d8070234):
>>>>
>>>>  https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=
>>> d807023479ce10aec28ef3c1ab646ddefc2e663c
>>>
>>>> The release files, including signatures, digests, etc. can be found at:
>>>> http://people.apache.org/~tdas/spark-1.0.0-rc10/
>>>>
>>>> The release artifacts are signed with the following key:
>>>> https://people.apache.org/keys/committer/tdas.asc
>>>>
>>>> The staging repository for this release can be found at:
>>>> https://repository.apache.org/content/repositories/orgapachespark-1018/
>>>>
>>>> The documentation corresponding to this release can be found at:
>>>> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>>>
>>>> The full list of changes in this release can be found at:
>>>>
>>>>  https://git-wip-us.apache.org/repos/asf?p=spark.git;a=blob;
>>> f=CHANGES.txt;h=d21f0ace6326e099360975002797eb7cba9d5273;hb=
>>> d807023479ce10aec28ef3c1ab646ddefc2e663c
>>>
>>>> Please vote on releasing this package as Apache Spark 1.0.0!
>>>>
>>>> The vote is open until Friday, May 23, at 20:00 UTC and passes if
>>>> amajority of at least 3 +1 PMC votes are cast.
>>>>
>>>> [ ] +1 Release this package as Apache Spark 1.0.0
>>>> [ ] -1 Do not release this package because ...
>>>>
>>>> To learn more about Apache Spark, please see
>>>> http://spark.apache.org/
>>>>
>>>> ====== API Changes ======
>>>> We welcome users to compile Spark applications against 1.0. There are
>>>> a few API changes in this release. Here are links to the associated
>>>> upgrade guides - user facing changes have been kept as small as
>>>> possible.
>>>>
>>>> Changes to ML vector specification:
>>>>
>>>>  http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>> mllib-guide.html#from-09-to-10
>>>
>>>> Changes to the Java API:
>>>>
>>>>  http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>> java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>>>
>>>> Changes to the streaming API:
>>>>
>>>>  http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>> streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>>>
>>>> Changes to the GraphX API:
>>>>
>>>>  http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>> graphx-programming-guide.html#upgrade-guide-from-spark-091
>>>
>>>> Other changes:
>>>> coGroup and related functions now return Iterable[T] instead of Seq[T]
>>>> ==> Call toSeq on the result to restore the old behavior
>>>>
>>>> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
>>>> ==> Call toSeq on the result to restore old behavior
>>>>
>>>
>

--001a11c265dec36a0e04f9f13811--

From dev-return-7770-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 02:48:19 2014
Return-Path: <dev-return-7770-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E982811363
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 02:48:19 +0000 (UTC)
Received: (qmail 70413 invoked by uid 500); 22 May 2014 02:48:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70359 invoked by uid 500); 22 May 2014 02:48:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70351 invoked by uid 99); 22 May 2014 02:48:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 02:48:19 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of cai.qianwen@hotmail.co.uk does not designate 216.139.250.139 as permitted sender)
Received: from [216.139.250.139] (HELO joe.nabble.com) (216.139.250.139)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 02:48:16 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by joe.nabble.com with esmtp (Exim 4.72)
	(envelope-from <cai.qianwen@hotmail.co.uk>)
	id 1WnJ2V-0002Hk-9c
	for dev@spark.incubator.apache.org; Wed, 21 May 2014 19:47:27 -0700
Date: Wed, 21 May 2014 19:47:12 -0700 (PDT)
From: Sue Cai <cai.qianwen@hotmail.co.uk>
To: dev@spark.incubator.apache.org
Message-ID: <1400726832277-6758.post@n3.nabble.com>
In-Reply-To: <tencent_6B37D69C54F76819509A5C4F@qq.com>
References: <1400657466191-6740.post@n3.nabble.com> <tencent_6B37D69C54F76819509A5C4F@qq.com>
Subject: Re: Re:MLlib ALS-- Errors communicating with MapOutputTracker
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Witgo,

     Thanks a lot for your reply.

     In my second test, the user features and product features were loaded
from the file system directly,which means I did not use ALS here, and this
problem happened at the loading data stage. The way I am asking the question
was a little bit miss leading, this problem might not be ALS specific.

    As far as I had discovered, it might related to the usage of master
memory, since I could solve this problem by increase the master memory size,
or reduce the number of partitions. But I am still waiting for a better
explanation :)

   Thank you again.

Sue   



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-ALS-Errors-communicating-with-MapOutputTracker-tp6740p6758.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7771-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 02:51:56 2014
Return-Path: <dev-return-7771-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 25D9211369
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 02:51:56 +0000 (UTC)
Received: (qmail 72278 invoked by uid 500); 22 May 2014 02:51:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72204 invoked by uid 500); 22 May 2014 02:51:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72196 invoked by uid 99); 22 May 2014 02:51:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 02:51:55 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tgraves_cs@yahoo.com designates 98.139.212.165 as permitted sender)
Received: from [98.139.212.165] (HELO nm6.bullet.mail.bf1.yahoo.com) (98.139.212.165)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 02:51:50 +0000
Received: from [98.139.214.32] by nm6.bullet.mail.bf1.yahoo.com with NNFMP; 22 May 2014 02:51:26 -0000
Received: from [98.139.212.237] by tm15.bullet.mail.bf1.yahoo.com with NNFMP; 22 May 2014 02:51:26 -0000
Received: from [127.0.0.1] by omp1046.mail.bf1.yahoo.com with NNFMP; 22 May 2014 02:51:26 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 888215.8300.bm@omp1046.mail.bf1.yahoo.com
Received: (qmail 74240 invoked by uid 60001); 22 May 2014 02:51:26 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.com; s=s1024; t=1400727086; bh=XAnrvXzaSH2+raCDgI1bsjqfosW7ts0/w16pQH86jAQ=; h=References:Message-ID:Date:From:Reply-To:Subject:To:In-Reply-To:MIME-Version:Content-Type; b=dqfFOzl0rPwzrsKfBcYvclXg2dUu+dxKDRmKsSPleXCGM1njSGNPxohVkENHaGRVcfnnshfA2As6PuY6nDsF2CLYIO3jdOkzj/bJgtxUGPMGYNhobcfsZZv7NPnUrnMufCM525oGQf55GHs+IO18AuJ4mHETgW/pXhzIBZ2D2+U=
X-YMail-OSG: 3epExN8VM1lCOwJUPqfHlkw9R5M7hv.O62eJgBssbVVNFcJ
 K6gsTnXe77oUxmLlKNz6lt9Tw_HAS8ZN8VGPpjk0US2Sq9QSvjzGX676wBk_
 X.Gn74hyKoHQ2Vx3E2b52098s1a30_0oFyrFeNgphXUXR3r3C4HP4xOviadR
 DJgq31gfVyHJNR8NHUbKa0Uh.URm0mHPORA5iidxmmLn4C5mzpT0xT1Ezv8d
 qDL6bvEOKqvBmH1OxEPONZgvi2.X7dHPu7eUURat8PV1XUAOSWM6ncIXkSG3
 U0JVOfRAitq4vbR9S5oY13LGu_.WZEOaZX.V2nJ.iEXsaPTHs5tDgy5VQq0t
 dqA3MFUv2Oet7te4qUOzU1q.zt6kGisXahbEIfEpbyq_QX0thZY4DFW957U4
 vszIKYFO7ElM3u2dvCOBB4reY_FzO1iHd0jA.QM8D4NANfOxg5ZXvGO3uuOn
 ATyCjQYvv.vSGk82Vcwr2S_oGDFYeitKSVSRTL3nSuK1UoSEmp_DLoaf.XVK
 H8oofUswylErsXHPE_CjnnHc6LUgDx0KHoxImpBwXQtLsiVzMT89TLg8Tnos
 SzT3atzDd671zhbIhh0sTt4BkM1sT9n2vBlX3XtMXpugXLm494MtjhT3UO0H
 7T8jgJo0NIbUh6s8CUj2fAoP918IHJNPQvw4ZgHxj_8pt7p7C9.ZcWOuKTDp
 kL8ESKw8-
Received: from [209.131.62.115] by web140102.mail.bf1.yahoo.com via HTTP; Wed, 21 May 2014 19:51:26 PDT
X-Rocket-MIMEInfo: 002.001,SGFzIGFueW9uZSB0cmllZCBweXNwYXJrIG9uIHlhcm4gYW5kIGdvdCBpdCB0byB3b3JrPyDCoEkgd2FzIGhhdmluZyBpc3N1ZXMgd2hlbiBJIGJ1aWx0IHNwYXJrIG9uIHJlZGhhdCBidXQgd2hlbiBJIGJ1aWx0IG9uIG15IG1hYyBpdCBoYWQgd29ya2VkLCDCoGJ1dCBub3cgd2hlbiBJIGJ1aWxkIGl0IG9uIG15IG1hYyBpdCBhbHNvIGRvZXNuJ3Qgd29yay4KClRvbQoKCgoKT24gVHVlc2RheSwgTWF5IDIwLCAyMDE0IDM6MTQgUE0sIFRhdGhhZ2F0YSBEYXMgPHRhdGhhZ2F0YS5kYXMxNTY1QGdtYWlsLmNvbT4BMAEBAQE-
X-Mailer: YahooMailWebService/0.8.188.663
References: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>
Message-ID: <1400727086.66643.YahooMailNeo@web140102.mail.bf1.yahoo.com>
Date: Wed, 21 May 2014 19:51:26 -0700 (PDT)
From: Tom Graves <tgraves_cs@yahoo.com>
Reply-To: Tom Graves <tgraves_cs@yahoo.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC10)
To: "dev@spark.apache.org" <dev@spark.apache.org>
In-Reply-To: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="-559291896-96606630-1400727086=:66643"
X-Virus-Checked: Checked by ClamAV on apache.org

---559291896-96606630-1400727086=:66643
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

Has anyone tried pyspark on yarn and got it to work? =A0I was having issues=
 when I built spark on redhat but when I built on my mac it had worked, =A0=
but now when I build it on my mac it also doesn't work.=0A=0ATom=0A=0A=0A=
=0A=0AOn Tuesday, May 20, 2014 3:14 PM, Tathagata Das <tathagata.das1565@gm=
ail.com> wrote:=0A =0A=0A=0APlease vote on releasing the following candidat=
e as Apache Spark version 1.0.0!=0A=0AThis has a few bug fixes on top of rc=
9:=0ASPARK-1875: https://github.com/apache/spark/pull/824=0ASPARK-1876: htt=
ps://github.com/apache/spark/pull/819=0ASPARK-1878: https://github.com/apac=
he/spark/pull/822=0ASPARK-1879: https://github.com/apache/spark/pull/823=0A=
=0AThe tag to be voted on is v1.0.0-rc10 (commit d8070234):=0Ahttps://git-w=
ip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3Dd807023479ce10aec28=
ef3c1ab646ddefc2e663c=0A=0AThe release files, including signatures, digests=
, etc. can be found at:=0Ahttp://people.apache.org/~tdas/spark-1.0.0-rc10/=
=0A=0AThe release artifacts are signed with the following key:=0Ahttps://pe=
ople.apache.org/keys/committer/tdas.asc=0A=0AThe staging repository for thi=
s release can be found at:=0Ahttps://repository.apache.org/content/reposito=
ries/orgapachespark-1018/=0A=0AThe documentation corresponding to this rele=
ase can be found at:=0Ahttp://people.apache.org/~tdas/spark-1.0.0-rc10-docs=
/=0A=0AThe full list of changes in this release can be found at:=0Ahttps://=
git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dblob;f=3DCHANGES.txt;h=3D=
d21f0ace6326e099360975002797eb7cba9d5273;hb=3Dd807023479ce10aec28ef3c1ab646=
ddefc2e663c=0A=0APlease vote on releasing this package as Apache Spark 1.0.=
0!=0A=0AThe vote is open until Friday, May 23, at 20:00 UTC and passes if=
=0Aamajority of at least 3 +1 PMC votes are cast.=0A=0A[ ] +1 Release this =
package as Apache Spark 1.0.0=0A[ ] -1 Do not release this package because =
...=0A=0ATo learn more about Apache Spark, please see=0Ahttp://spark.apache=
.org/=0A=0A=3D=3D=3D=3D=3D=3D API Changes =3D=3D=3D=3D=3D=3D=0AWe welcome u=
sers to compile Spark applications against 1.0. There are=0Aa few API chang=
es in this release. Here are links to the associated=0Aupgrade guides - use=
r facing changes have been kept as small as=0Apossible.=0A=0AChanges to ML =
vector specification:=0Ahttp://people.apache.org/~tdas/spark-1.0.0-rc10-doc=
s/mllib-guide.html#from-09-to-10=0A=0AChanges to the Java API:=0Ahttp://peo=
ple.apache.org/~tdas/spark-1.0.0-rc10-docs/java-programming-guide.html#upgr=
ading-from-pre-10-versions-of-spark=0A=0AChanges to the streaming API:=0Aht=
tp://people.apache.org/~tdas/spark-1.0.0-rc10-docs/streaming-programming-gu=
ide.html#migration-guide-from-091-or-below-to-1x=0A=0AChanges to the GraphX=
 API:=0Ahttp://people.apache.org/~tdas/spark-1.0.0-rc10-docs/graphx-program=
ming-guide.html#upgrade-guide-from-spark-091=0A=0AOther changes:=0AcoGroup =
and related functions now return Iterable[T] instead of Seq[T]=0A=3D=3D> Ca=
ll toSeq on the result to restore the old behavior=0A=0ASparkContext.jarOfC=
lass returns Option[String] instead of Seq[String]=0A=3D=3D> Call toSeq on =
the result to restore old behavior
---559291896-96606630-1400727086=:66643--

From dev-return-7772-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 03:19:19 2014
Return-Path: <dev-return-7772-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 00CD91141A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 03:19:19 +0000 (UTC)
Received: (qmail 97328 invoked by uid 500); 22 May 2014 03:19:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97264 invoked by uid 500); 22 May 2014 03:19:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 97253 invoked by uid 99); 22 May 2014 03:19:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 03:19:18 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tgraves_cs@yahoo.com designates 216.109.115.78 as permitted sender)
Received: from [216.109.115.78] (HELO nm45-vm7.bullet.mail.bf1.yahoo.com) (216.109.115.78)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 03:19:13 +0000
Received: from [98.139.212.153] by nm45.bullet.mail.bf1.yahoo.com with NNFMP; 22 May 2014 03:18:49 -0000
Received: from [98.139.212.217] by tm10.bullet.mail.bf1.yahoo.com with NNFMP; 22 May 2014 03:18:49 -0000
Received: from [127.0.0.1] by omp1026.mail.bf1.yahoo.com with NNFMP; 22 May 2014 03:18:49 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 570517.5229.bm@omp1026.mail.bf1.yahoo.com
Received: (qmail 45486 invoked by uid 60001); 22 May 2014 03:18:49 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.com; s=s1024; t=1400728729; bh=2r3Uk7ksRL7lJeUuMhVNa+qU9uym0VmR2BMOvU/D+Ck=; h=References:Message-ID:Date:From:Reply-To:Subject:To:In-Reply-To:MIME-Version:Content-Type; b=ystK3xcVekECWRaNIezHL+Qb8sKMNeCLspGwUxon/sTfbVvkm8RnXs3se0oZs443HErjGVIZzawWg4ynPGymiVnLezxFL5M8ZF0nfZlsFy5iTWwR0IrTswwSDmdLlvLHAZURDE+oSA5C68HiCtXjz6t3wteMO4nv4v5ifXjdMJc=
X-YMail-OSG: C7bKU4oVM1lnn5n7gGQlvkIWSUMiU5uHRD82zWbXXxwRIkx
 zn_qZnCM7XVpkOPrdUQ9Emjb3w8ipufDM5wUVbiWVp3idIpAFa_N2FEs3xhg
 XAVd19UQ8gMGANOyYZREFC0gGzuifY.LPUEaPHaleU2YexWkdoCxq4a0srtb
 6udxOfR__sTHJD28TV8SSmp3uqG82_Ax1PTOzVIpxt_3JZCYZETzK1rbFhTk
 rZqMGPCeuNC4f4qTdfz6IgBahrM1p7rOxXp10t272ycnT0KB8ongr3B5u0Hl
 d96jAVs97cxLR79oMl_EpEq_.YVwtMfKCIU3HvclBdln4_yvNTrrCyxA_VG5
 9qPw0XrkYstupd4Ls5uLk51PNekcUL1s2tYBB6GTC4BElkakJKW.QroNX7Mq
 R4wGrVxGUoz22Tsf3_Q.2o.m8_aAkX_xAK1QoQPNHEV7.7O7jIco7akvRCBf
 7bT8gmEp6BNGOzrCQ_ROwDxoQi.qOeagzqbxMJB6wZE7CQky8fo5Avq7C3HT
 LmdNk_Bl.K9phQ7thV1lBT1ESzDnqJtMBcYW68yO5Yf_PAy7vylD_SFQBx.w
 bAWrfNKjL2V.TEBKyd1NvNfGg63uAKeR_Iab7DV6DnxUr_NBMp7Ity_4ClU2
 m_TZDU7L806Zw.nfi1uEl5jxBYsnPriAsspAohIpqaMH5wkACWsxQ4rv6N5C
 IUoxVs3uNr_TZ
Received: from [209.131.62.115] by web140101.mail.bf1.yahoo.com via HTTP; Wed, 21 May 2014 20:18:49 PDT
X-Rocket-MIMEInfo: 002.001,SSBkb24ndCB0aGluayBLZXZpbidzIGlzc3VlIHdvdWxkIGJlIHdpdGggYW4gYXBpIGNoYW5nZSBpbiBZYXJuQ2xpZW50SW1wbCBzaW5jZSBpbiBib3RoIGNhc2VzIGhlIHNheXMgaGUgaXMgdXNpbmcgaGFkb29wIDIuMy4wLiDCoEknbGwgdGFrZSBhIGxvb2sgYXQgaGlzIHBvc3QgaW4gdGhlIHVzZXIgbGlzdC4KClRvbQoKCgoKT24gV2VkbmVzZGF5LCBNYXkgMjEsIDIwMTQgNzowMSBQTSwgQ29saW4gTWNDYWJlIDxjbWNjYWJlQGFsdW1uaS5jbXUuZWR1PiB3cm90ZToKIAoKCkhpIEtldmluLAoKQ2FuIHlvdSABMAEBAQE-
X-Mailer: YahooMailWebService/0.8.188.663
References: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>	<CALuGr6YvTxM+YyXx-+p3XsfcXT2wWE3iJ0T43HqJzKfs6s2UYQ@mail.gmail.com>	<CAAsvFPkk+TVQiAu+_XiDOOgqUr4tck8ccA=YQ5Aiv_SF7hn7Dg@mail.gmail.com>	<537D2A07.4030702@oracle.com> <CA+qbEUNKVF2xb3SM-EXwiTom7+qX+ZTbeNS6nHVvfp3-_a=fNw@mail.gmail.com> 
Message-ID: <1400728729.27373.YahooMailNeo@web140101.mail.bf1.yahoo.com>
Date: Wed, 21 May 2014 20:18:49 -0700 (PDT)
From: Tom Graves <tgraves_cs@yahoo.com>
Reply-To: Tom Graves <tgraves_cs@yahoo.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC10)
To: "dev@spark.apache.org" <dev@spark.apache.org>
In-Reply-To: <CA+qbEUNKVF2xb3SM-EXwiTom7+qX+ZTbeNS6nHVvfp3-_a=fNw@mail.gmail.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="584511794-1709950576-1400728729=:27373"
X-Virus-Checked: Checked by ClamAV on apache.org

--584511794-1709950576-1400728729=:27373
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

I don't think Kevin's issue would be with an api change in YarnClientImpl s=
ince in both cases he says he is using hadoop 2.3.0. =A0I'll take a look at=
 his post in the user list.=0A=0ATom=0A=0A=0A=0A=0AOn Wednesday, May 21, 20=
14 7:01 PM, Colin McCabe <cmccabe@alumni.cmu.edu> wrote:=0A =0A=0A=0AHi Kev=
in,=0A=0ACan you try https://issues.apache.org/jira/browse/SPARK-1898 to se=
e if it=0Afixes your issue?=0A=0ARunning in YARN cluster mode, I had a simi=
lar issue where Spark was able to=0Acreate a Driver and an Executor via YAR=
N, but then it stopped making any=0Aprogress.=0A=0ANote: I was using a pre-=
release version of CDH5.1.0, not 2.3 like you were=0Ausing.=0A=0Abest,=0ACo=
lin=0A=0A=0A=0AOn Wed, May 21, 2014 at 3:34 PM, Kevin Markey <kevin.markey@=
oracle.com>wrote:=0A=0A> 0=0A>=0A> Abstaining because I'm not sure if my fa=
ilures are due to Spark,=0A> configuration, or other factors...=0A>=0A> Com=
piled and deployed RC10 for YARN, Hadoop 2.3=0A per Spark 1.0.0 Yarn=0A> do=
cumentation.=A0 No problems.=0A> Rebuilt applications against RC10 and Hado=
op 2.3.0 (plain vanilla Apache=0A> release).=0A> Updated scripts for variou=
s applications.=0A> Application had successfully compiled and run against S=
park 0.9.1 and=0A> Hadoop 2.3.0.=0A> Ran in "yarn-cluster" mode.=0A> Applic=
ation ran to conclusion except that it ultimately failed because of=0A> an =
exception when Spark tried to clean up the staging directory.=A0 Also,=0A> =
where before Yarn would report the running program as "RUNNING", it only=0A=
> reported this application as "ACCEPTED".=A0 It appeared to run two contai=
ners=0A> when the first instance never reported that it was RUNNING.=0A>=0A=
> I will post a=0A separate note to the USER list about the specifics.=0A>=
=0A> Thanks=0A> Kevin Markey=0A>=0A>=0A>=0A> On 05/21/2014 10:58 AM, Mark H=
amstra wrote:=0A>=0A>> +1=0A>>=0A>>=0A>> On Tue, May 20, 2014 at 11:09 PM, =
Henry Saputra <henry.saputra@gmail.com>=0A>> wrote:=0A>>=0A>>=A0 Signature =
and hash for source looks good=0A>>> No external executable package with so=
urce - good=0A>>> Compiled with git and maven - good=0A>>> Ran examples and=
 sample programs locally and standalone -good=0A>>>=0A>>> +1=0A>>>=0A>>> - =
Henry=0A>>>=0A>>>=0A>>>=0A>>> On Tue, May 20, 2014 at 1:13 PM, Tathagata Da=
s=0A>>> <tathagata.das1565@gmail.com> wrote:=0A>>>=0A>>>> Please vote on re=
leasing the following candidate as Apache Spark version=0A>>>>=0A>>> 1.0.0!=
=0A>>>=0A>>>> This has a few bug fixes on top of rc9:=0A>>>> SPARK-1875: ht=
tps://github.com/apache/spark/pull/824=0A>>>> SPARK-1876: https://github.co=
m/apache/spark/pull/819=0A>>>> SPARK-1878: https://github.com/apache/spark/=
pull/822=0A>>>> SPARK-1879: https://github.com/apache/spark/pull/823=0A>>>>=
=0A>>>> The tag to be voted on is v1.0.0-rc10 (commit d8070234):=0A>>>>=0A>=
>>>=A0 https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=
=3D=0A>>> d807023479ce10aec28ef3c1ab646ddefc2e663c=0A>>>=0A>>>> The=0A rele=
ase files, including signatures, digests, etc. can be found at:=0A>>>> http=
://people.apache.org/~tdas/spark-1.0.0-rc10/=0A>>>>=0A>>>> The release arti=
facts are signed with the following key:=0A>>>> https://people.apache.org/k=
eys/committer/tdas.asc=0A>>>>=0A>>>> The staging repository for this releas=
e can be found at:=0A>>>> https://repository.apache.org/content/repositorie=
s/orgapachespark-1018/=0A>>>>=0A>>>> The documentation=0A corresponding to =
this release can be found at:=0A>>>> http://people.apache.org/~tdas/spark-1=
.0.0-rc10-docs/=0A>>>>=0A>>>> The full list of changes in this release can =
be found at:=0A>>>>=0A>>>>=A0 https://git-wip-us.apache.org/repos/asf?p=3Ds=
park.git;a=3Dblob;=0A>>> f=3DCHANGES.txt;h=3Dd21f0ace6326e099360975002797eb=
7cba9d5273;hb=3D=0A>>> d807023479ce10aec28ef3c1ab646ddefc2e663c=0A>>>=0A>>>=
> Please vote on releasing this package as Apache Spark 1.0.0!=0A>>>>=0A>>>=
> The vote is open until=0A Friday, May 23, at 20:00 UTC and passes if=0A>>=
>> amajority of at least 3 +1 PMC votes are cast.=0A>>>>=0A>>>> [ ] +1 Rele=
ase this package as Apache Spark 1.0.0=0A>>>> [ ] -1 Do not release this pa=
ckage because ...=0A>>>>=0A>>>> To learn more about Apache Spark, please se=
e=0A>>>> http://spark.apache.org/=0A>>>>=0A>>>> =3D=3D=3D=3D=3D=3D API Chan=
ges =3D=3D=3D=3D=3D=3D=0A>>>> We welcome users to compile Spark application=
s against 1.0. There are=0A>>>> a few API changes in this release. Here are=
 links to the associated=0A>>>> upgrade guides - user facing changes have b=
een kept as small as=0A>>>> possible.=0A>>>>=0A>>>> Changes to ML vector sp=
ecification:=0A>>>>=0A>>>>=A0 http://people.apache.org/~tdas/spark-1.0.0-rc=
10-docs/=0A>>> mllib-guide.html#from-09-to-10=0A>>>=0A>>>> Changes to the J=
ava API:=0A>>>>=0A>>>>=A0 http://people.apache.org/~tdas/spark-1.0.0-rc10-d=
ocs/=0A>>> java-programming-guide.html#upgrading-from-pre-10-versions-of-sp=
ark=0A>>>=0A>>>> Changes to the streaming API:=0A>>>>=0A>>>>=A0 http://peop=
le.apache.org/~tdas/spark-1.0.0-rc10-docs/=0A>>> streaming-programming-guid=
e.html#migration-guide-from-091-or-below-to-1x=0A>>>=0A>>>> Changes to the =
GraphX API:=0A>>>>=0A>>>>=A0 http://people.apache.org/~tdas/spark-1.0.0-rc1=
0-docs/=0A>>> graphx-programming-guide.html#upgrade-guide-from-spark-091=0A=
>>>=0A>>>> Other changes:=0A>>>> coGroup and related functions now return I=
terable[T] instead of Seq[T]=0A>>>> =3D=3D> Call toSeq on the result to res=
tore the old behavior=0A>>>>=0A>>>> SparkContext.jarOfClass returns Option[=
String] instead of Seq[String]=0A>>>> =3D=3D> Call toSeq on the result to r=
estore old behavior=0A>>>>=0A>>>=0A>
--584511794-1709950576-1400728729=:27373--

From dev-return-7773-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 04:20:31 2014
Return-Path: <dev-return-7773-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EC35A11561
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 04:20:30 +0000 (UTC)
Received: (qmail 81294 invoked by uid 500); 22 May 2014 04:20:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81232 invoked by uid 500); 22 May 2014 04:20:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81224 invoked by uid 99); 22 May 2014 04:20:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 04:20:30 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.128.182] (HELO mail-ve0-f182.google.com) (209.85.128.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 04:20:26 +0000
Received: by mail-ve0-f182.google.com with SMTP id sa20so3762729veb.27
        for <dev@spark.apache.org>; Wed, 21 May 2014 21:20:03 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=a4Z1Nc80Fgc0LsS7mKgM9KSc9oUHdgqyX9TVdSzux5c=;
        b=e3CcP+vJLnpF0wYgGtBR8YjY3o1pFg4HO7F7XZzSFCffNObc0hJc31YUCafADAIDw8
         Dfgv08+L2phkm9nFmu3xOEJAD9Gg1QgKmpll89ePimf+791BD/2nKUi+Bf9dB+tsDuVl
         oq08x4Aenl0kqPNfUPn8XbY+q4iVgrP1OPZXiXl8FjPt/3xUErRC47GN8z+57UCpzAZK
         YzNWAMEBmHoshWPPK/Qy/4Imi5wJ2GuBgbxj+uKIb0lpmrfZnzCHsp3ZcCjyWiglQBEj
         /soGXvTYJ8JHirx5EcOvC7lJR8jBhlVvaEsRSsQUm2EbInw5eaK3CwLb3zdg2ViA73cl
         5yIA==
X-Gm-Message-State: ALoCoQn6vNDxTXbcsKosZIc1Aeva4prT4VtzB8RyPaxxxmuHUV+K5M6/hmn1slcHBFziUOzzuIuU
X-Received: by 10.52.113.1 with SMTP id iu1mr11450586vdb.35.1400732403097;
        Wed, 21 May 2014 21:20:03 -0700 (PDT)
Received: from mail-vc0-f173.google.com (mail-vc0-f173.google.com [209.85.220.173])
        by mx.google.com with ESMTPSA id 5sm6803435vdg.8.2014.05.21.21.20.01
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 21 May 2014 21:20:01 -0700 (PDT)
Received: by mail-vc0-f173.google.com with SMTP id il7so3762106vcb.32
        for <dev@spark.apache.org>; Wed, 21 May 2014 21:20:01 -0700 (PDT)
X-Received: by 10.52.99.168 with SMTP id er8mr11563902vdb.26.1400732401490;
 Wed, 21 May 2014 21:20:01 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Wed, 21 May 2014 21:19:40 -0700 (PDT)
In-Reply-To: <CAMc-71kjOHyVcL+4GZW=ch8+ZxvkDcXWnW07Uvaa4ePX7n79=A@mail.gmail.com>
References: <CAMc-71kjOHyVcL+4GZW=ch8+ZxvkDcXWnW07Uvaa4ePX7n79=A@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Thu, 22 May 2014 00:19:40 -0400
Message-ID: <CA+-p3AGerFkP_9bsVDw4tu9QUhNH_--aEiWL4NqB+r3xWsyQjA@mail.gmail.com>
Subject: Re: Should SPARK_HOME be needed with Mesos?
To: dev@spark.apache.org
Cc: tgp@preferred.jp
Content-Type: multipart/alternative; boundary=20cf3071cab03761d404f9f56d5e
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf3071cab03761d404f9f56d5e
Content-Type: text/plain; charset=UTF-8

Hi Gerard,

I agree that your second option seems preferred.  You shouldn't have to
specify a SPARK_HOME if the executor is going to use the spark.executor.uri
instead.  Can you send in a pull request that includes your proposed
changes?

Andrew


On Wed, May 21, 2014 at 10:19 AM, Gerard Maas <gerard.maas@gmail.com> wrote:

> Spark dev's,
>
> I was looking into a question asked on the user list where a
> ClassNotFoundException was thrown when running a job on Mesos. Curious
> issue with serialization on Mesos: more details here [1]:
>
> When trying to run that simple example on my Mesos installation, I faced
> another issue: I got an error that "SPARK_HOME" was not set. I found that
> curious b/c a local spark installation should not be required to run a job
> on Mesos. All that's needed is the executor package, being the
> assembly.tar.gz on a reachable location (HDFS/S3/HTTP).
>
> I went looking into the code and indeed there's a check on SPARK_HOME [2]
> regardless of the presence of the assembly but it's actually only used if
> the assembly is not provided (which is a kind-of best-effort recovery
> strategy).
>
> Current flow:
>
> if (!SPARK_HOME) fail("No SPARK_HOME")
> else if (assembly) { use assembly) }
> else { try use SPARK_HOME to build spark_executor }
>
> Should be:
> sparkExecutor =  if (assembly) {assembly}
>                  else if (SPARK_HOME) {try use SPARK_HOME to build
> spark_executor}
>                  else { fail("No executor found. Please provide
> spark.executor.uri (preferred) or spark.home")
>
> What do you think?
>
> -kr, Gerard.
>
>
> [1]
>
> http://apache-spark-user-list.1001560.n3.nabble.com/ClassNotFoundException-with-Spark-Mesos-spark-shell-works-fine-td6165.html
>
> [2]
>
> https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend.scala#L89
>

--20cf3071cab03761d404f9f56d5e--

From dev-return-7774-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 05:59:25 2014
Return-Path: <dev-return-7774-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6FD4A117A3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 05:59:25 +0000 (UTC)
Received: (qmail 14164 invoked by uid 500); 22 May 2014 05:59:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14108 invoked by uid 500); 22 May 2014 05:59:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14100 invoked by uid 99); 22 May 2014 05:59:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 05:59:25 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of nitinpanj@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 05:59:20 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <nitinpanj@gmail.com>)
	id 1WnM1s-0000zB-1a
	for dev@spark.incubator.apache.org; Wed, 21 May 2014 22:59:00 -0700
Date: Wed, 21 May 2014 22:59:00 -0700 (PDT)
From: npanj <nitinpanj@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1400738340022-6762.post@n3.nabble.com>
Subject: Graphx: GraphLoader.edgeListFile with edge weight
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

For my project I needed to load a graph with edge weight; for this I have
updated GraphLoader.edgeListFile to consider third column in input file. I
will like to submit my patch for review so that it can be merged with master
branch. What is the process for submitting patches? 



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Graphx-GraphLoader-edgeListFile-with-edge-weight-tp6762.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7775-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 06:05:16 2014
Return-Path: <dev-return-7775-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0BFC4117D6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 06:05:16 +0000 (UTC)
Received: (qmail 20008 invoked by uid 500); 22 May 2014 06:05:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19946 invoked by uid 500); 22 May 2014 06:05:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19938 invoked by uid 99); 22 May 2014 06:05:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 06:05:15 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.180] (HELO mail-qc0-f180.google.com) (209.85.216.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 06:05:12 +0000
Received: by mail-qc0-f180.google.com with SMTP id i17so4811581qcy.25
        for <dev@spark.apache.org>; Wed, 21 May 2014 23:04:49 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=s0KW7vB/PvFzQVQVfT0ViJMQ3aEdVpvlV1mmBekdyXc=;
        b=SBuI4c3c3x8YtO3RAjI96A2+UYAgEUiNU76MxXvIvogxZMAfIvkffEBYb8PbbL9nj4
         sZDtp/LR4ZbHhv2Q6XMzGSa3CSSxxj0NQn1Fven598IvXJainR/yzovHnv7wRHLkwa8K
         FmjXWSgd6F15o5DsdoorA6Ods9ObPcQ4TNkPwUHTn1TGp/Xy58LuZhAeJCMmZ7VXjo/M
         pY2SMQU3rCj8TvfXnceafxRPsyUIQaC61HEKT+sDEtBqCVSMxYUb6Jxq4ppF/8DexXR0
         jomrIe0QdWSs3kFU/tdguWUi7trKuzPSU/5QhqIBRcz5+4oMJ7VkTgGmU2ZI8CarlzlW
         e++A==
X-Gm-Message-State: ALoCoQnEM9Pmsax6AFs3ue5770yVeBz01fkW+VMVYGtYvWOE0R1IjH2zm2jVuHUhA2+Z6LZcPD20
X-Received: by 10.140.81.146 with SMTP id f18mr74378492qgd.47.1400738689261;
 Wed, 21 May 2014 23:04:49 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.126.1 with HTTP; Wed, 21 May 2014 23:04:29 -0700 (PDT)
In-Reply-To: <1400738340022-6762.post@n3.nabble.com>
References: <1400738340022-6762.post@n3.nabble.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 21 May 2014 23:04:29 -0700
Message-ID: <CAPh_B=b9y+ik2DFRaTA6KVVtpLpT0iGb6Szv63yQ7M2GwNyjWw@mail.gmail.com>
Subject: Re: Graphx: GraphLoader.edgeListFile with edge weight
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c1299cff35ac04f9f6e342
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1299cff35ac04f9f6e342
Content-Type: text/plain; charset=UTF-8

You can submit a pull request on the github mirror:
https://github.com/apache/spark

Thanks.


On Wed, May 21, 2014 at 10:59 PM, npanj <nitinpanj@gmail.com> wrote:

> Hi,
>
> For my project I needed to load a graph with edge weight; for this I have
> updated GraphLoader.edgeListFile to consider third column in input file. I
> will like to submit my patch for review so that it can be merged with
> master
> branch. What is the process for submitting patches?
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Graphx-GraphLoader-edgeListFile-with-edge-weight-tp6762.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c1299cff35ac04f9f6e342--

From dev-return-7776-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 07:33:28 2014
Return-Path: <dev-return-7776-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 874F41199B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 07:33:28 +0000 (UTC)
Received: (qmail 63407 invoked by uid 500); 22 May 2014 07:33:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63344 invoked by uid 500); 22 May 2014 07:33:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63336 invoked by uid 99); 22 May 2014 07:33:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 07:33:28 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of kevin.markey@oracle.com designates 156.151.31.81 as permitted sender)
Received: from [156.151.31.81] (HELO userp1040.oracle.com) (156.151.31.81)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 07:33:24 +0000
Received: from ucsinet21.oracle.com (ucsinet21.oracle.com [156.151.31.93])
	by userp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2) with ESMTP id s4M7Wx0d007159
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=OK)
	for <dev@spark.apache.org>; Thu, 22 May 2014 07:33:00 GMT
Received: from userz7021.oracle.com (userz7021.oracle.com [156.151.31.85])
	by ucsinet21.oracle.com (8.14.4+Sun/8.14.4) with ESMTP id s4M7WwaM019021
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=FAIL)
	for <dev@spark.apache.org>; Thu, 22 May 2014 07:32:59 GMT
Received: from abhmp0011.oracle.com (abhmp0011.oracle.com [141.146.116.17])
	by userz7021.oracle.com (8.14.4+Sun/8.14.4) with ESMTP id s4M7Wwbm019006
	for <dev@spark.apache.org>; Thu, 22 May 2014 07:32:58 GMT
Received: from [192.168.1.18] (/71.229.205.50)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Thu, 22 May 2014 00:32:58 -0700
Message-ID: <537DA829.9030000@oracle.com>
Date: Thu, 22 May 2014 01:32:57 -0600
From: Kevin Markey <kevin.markey@oracle.com>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20100101 Thunderbird/24.5.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC10)
References: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>	<CALuGr6YvTxM+YyXx-+p3XsfcXT2wWE3iJ0T43HqJzKfs6s2UYQ@mail.gmail.com>	<CAAsvFPkk+TVQiAu+_XiDOOgqUr4tck8ccA=YQ5Aiv_SF7hn7Dg@mail.gmail.com>	<537D2A07.4030702@oracle.com> <CA+qbEUNKVF2xb3SM-EXwiTom7+qX+ZTbeNS6nHVvfp3-_a=fNw@mail.gmail.com> <1400728729.27373.YahooMailNeo@web140101.mail.bf1.yahoo.com>
In-Reply-To: <1400728729.27373.YahooMailNeo@web140101.mail.bf1.yahoo.com>
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Source-IP: ucsinet21.oracle.com [156.151.31.93]
X-Virus-Checked: Checked by ClamAV on apache.org

I've discovered that one of the anomalies I encountered was due to a 
(embarrassing? humorous?) user error.  See the user list thread "Failed 
RC-10 yarn-cluster job for FS closed error when cleaning up staging 
directory" for my discussion.  With the user error corrected, the FS 
closed exception only prevents deletion of the staging directory, but 
does not affect completion with "SUCCESS." The FS closed exception still 
needs some investigation at least by me.

I tried the patch reported by SPARK-1898, but it didn't fix the problem 
without fixing the user error.  I did not attempt to test my fix without 
the patch, so I can't pass judgment on the patch.

Although this is merely a pseudocluster based test -- I can't 
reconfigure our cluster with RC-10 -- I'll now change my vote to...

+1.

Thanks all who helped.
Kevin



On 05/21/2014 09:18 PM, Tom Graves wrote:
> I don't think Kevin's issue would be with an api change in YarnClientImpl since in both cases he says he is using hadoop 2.3.0.  I'll take a look at his post in the user list.
>
> Tom
>
>
>
>
> On Wednesday, May 21, 2014 7:01 PM, Colin McCabe <cmccabe@alumni.cmu.edu> wrote:
>   
>
>
> Hi Kevin,
>
> Can you try https://issues.apache.org/jira/browse/SPARK-1898 to see if it
> fixes your issue?
>
> Running in YARN cluster mode, I had a similar issue where Spark was able to
> create a Driver and an Executor via YARN, but then it stopped making any
> progress.
>
> Note: I was using a pre-release version of CDH5.1.0, not 2.3 like you were
> using.
>
> best,
> Colin
>
>
>
> On Wed, May 21, 2014 at 3:34 PM, Kevin Markey <kevin.markey@oracle.com>wrote:
>
>> 0
>>
>> Abstaining because I'm not sure if my failures are due to Spark,
>> configuration, or other factors...
>>
>> Compiled and deployed RC10 for YARN, Hadoop 2.3
>   per Spark 1.0.0 Yarn
>> documentation.  No problems.
>> Rebuilt applications against RC10 and Hadoop 2.3.0 (plain vanilla Apache
>> release).
>> Updated scripts for various applications.
>> Application had successfully compiled and run against Spark 0.9.1 and
>> Hadoop 2.3.0.
>> Ran in "yarn-cluster" mode.
>> Application ran to conclusion except that it ultimately failed because of
>> an exception when Spark tried to clean up the staging directory.  Also,
>> where before Yarn would report the running program as "RUNNING", it only
>> reported this application as "ACCEPTED".  It appeared to run two containers
>> when the first instance never reported that it was RUNNING.
>>
>> I will post a
>   separate note to the USER list about the specifics.
>> Thanks
>> Kevin Markey
>>
>>
>>
>> On 05/21/2014 10:58 AM, Mark Hamstra wrote:
>>
>>> +1
>>>
>>>
>>> On Tue, May 20, 2014 at 11:09 PM, Henry Saputra <henry.saputra@gmail.com>
>>> wrote:
>>>
>>>    Signature and hash for source looks good
>>>> No external executable package with source - good
>>>> Compiled with git and maven - good
>>>> Ran examples and sample programs locally and standalone -good
>>>>
>>>> +1
>>>>
>>>> - Henry
>>>>
>>>>
>>>>
>>>> On Tue, May 20, 2014 at 1:13 PM, Tathagata Das
>>>> <tathagata.das1565@gmail.com> wrote:
>>>>
>>>>> Please vote on releasing the following candidate as Apache Spark version
>>>>>
>>>> 1.0.0!
>>>>
>>>>> This has a few bug fixes on top of rc9:
>>>>> SPARK-1875: https://github.com/apache/spark/pull/824
>>>>> SPARK-1876: https://github.com/apache/spark/pull/819
>>>>> SPARK-1878: https://github.com/apache/spark/pull/822
>>>>> SPARK-1879: https://github.com/apache/spark/pull/823
>>>>>
>>>>> The tag to be voted on is v1.0.0-rc10 (commit d8070234):
>>>>>
>>>>>    https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=
>>>> d807023479ce10aec28ef3c1ab646ddefc2e663c
>>>>
>>>>> The
>   release files, including signatures, digests, etc. can be found at:
>>>>> http://people.apache.org/~tdas/spark-1.0.0-rc10/
>>>>>
>>>>> The release artifacts are signed with the following key:
>>>>> https://people.apache.org/keys/committer/tdas.asc
>>>>>
>>>>> The staging repository for this release can be found at:
>>>>> https://repository.apache.org/content/repositories/orgapachespark-1018/
>>>>>
>>>>> The documentation
>   corresponding to this release can be found at:
>>>>> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>>>>
>>>>> The full list of changes in this release can be found at:
>>>>>
>>>>>    https://git-wip-us.apache.org/repos/asf?p=spark.git;a=blob;
>>>> f=CHANGES.txt;h=d21f0ace6326e099360975002797eb7cba9d5273;hb=
>>>> d807023479ce10aec28ef3c1ab646ddefc2e663c
>>>>
>>>>> Please vote on releasing this package as Apache Spark 1.0.0!
>>>>>
>>>>> The vote is open until
>   Friday, May 23, at 20:00 UTC and passes if
>>>>> amajority of at least 3 +1 PMC votes are cast.
>>>>>
>>>>> [ ] +1 Release this package as Apache Spark 1.0.0
>>>>> [ ] -1 Do not release this package because ...
>>>>>
>>>>> To learn more about Apache Spark, please see
>>>>> http://spark.apache.org/
>>>>>
>>>>> ====== API Changes ======
>>>>> We welcome users to compile Spark applications against 1.0. There are
>>>>> a few API changes in this release. Here are links to the associated
>>>>> upgrade guides - user facing changes have been kept as small as
>>>>> possible.
>>>>>
>>>>> Changes to ML vector specification:
>>>>>
>>>>>    http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>>> mllib-guide.html#from-09-to-10
>>>>
>>>>> Changes to the Java API:
>>>>>
>>>>>    http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>>> java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>>>>
>>>>> Changes to the streaming API:
>>>>>
>>>>>    http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>>> streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>>>>
>>>>> Changes to the GraphX API:
>>>>>
>>>>>    http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>>> graphx-programming-guide.html#upgrade-guide-from-spark-091
>>>>
>>>>> Other changes:
>>>>> coGroup and related functions now return Iterable[T] instead of Seq[T]
>>>>> ==> Call toSeq on the result to restore the old behavior
>>>>>
>>>>> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
>>>>> ==> Call toSeq on the result to restore old behavior
>>>>>


From dev-return-7777-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 08:34:29 2014
Return-Path: <dev-return-7777-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 44A4811AE6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 08:34:29 +0000 (UTC)
Received: (qmail 73275 invoked by uid 500); 22 May 2014 08:34:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73214 invoked by uid 500); 22 May 2014 08:34:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73206 invoked by uid 99); 22 May 2014 08:34:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 08:34:28 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 74.125.82.181 as permitted sender)
Received: from [74.125.82.181] (HELO mail-we0-f181.google.com) (74.125.82.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 08:34:25 +0000
Received: by mail-we0-f181.google.com with SMTP id w61so3046644wes.26
        for <dev@spark.apache.org>; Thu, 22 May 2014 01:34:04 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=pczZxfgHP6atk7nbkxkaI+dZ9hhMbrVcDe8Xjnj5bMU=;
        b=YwwY1u/Z94usF5ouLaLhPE9BOZr2KKUSpB41tfx1Y8+AaeeWdyS1dZDaODimmwc7CG
         14QbKVo0TSPidL88bmyYmeiSojSclOfPREEXtA8uuaA7a7q8qAmOzlokovAyejufsqU3
         Xuzb49i2+qAbErP52GY1M4aVHeaf0gFp7pQsxaI9ibBQvnEDiGokgFqH4nsxL5Jj+FML
         bI3sAWqjsAY2eGgrlCaNOluQjtrlnGaPz6NH69ElwUMseHT70Prk86SoW4N2zGyZtaBQ
         WPwLKMKSNfF4+PYy0mpnYBCiJU8e3sD7MuBjYNuQVIW9Ivpf970hAAZcCp5X01hWQqBy
         Sy5g==
MIME-Version: 1.0
X-Received: by 10.180.73.201 with SMTP id n9mr15074838wiv.45.1400747643871;
 Thu, 22 May 2014 01:34:03 -0700 (PDT)
Received: by 10.194.187.20 with HTTP; Thu, 22 May 2014 01:34:03 -0700 (PDT)
In-Reply-To: <CAEYYnxZhLxdjBQD0BqEr9Su4UZ89ZnABQWoKYCqooO52Gm9Psw@mail.gmail.com>
References: <CAEYYnxZic-qc0KuoKszmXxYBF-WoBEeDMiFyqBwQUzZL03Mnmg@mail.gmail.com>
	<CAJiQeYK+oUfRKDAd2exew5oMbaxVKDYA3UZ+k7B3JkD11COBsg@mail.gmail.com>
	<CAJgQjQ8QDN3dZ3-rK+aK51qy784ebkDw9xROwkOYgqk4BJbvPg@mail.gmail.com>
	<CAJgQjQ9JWt59eHYFzG54R8bYOfL8-+EFhjXZjF69qxNFXapz_Q@mail.gmail.com>
	<CAJgQjQ8ncqp+QNZpevBSBzyxTVDPahhwUmkOwXUoDyUws8Fo2A@mail.gmail.com>
	<CABPQxsuQUYXExpoKA=7KO7aTMsda2s5F=9vbsAarT-rSmxrL_Q@mail.gmail.com>
	<CABPQxsuXjjVHDqkgUkNZRoUkF+BbJdE-eyq9pxFnWso79hOpMw@mail.gmail.com>
	<CACBYxKJo+er88da3c3ahUW+Pu61D001oVJd+rVg4HJz2oq8Vpw@mail.gmail.com>
	<CAMAsSdKxE-biW8fRqkRn6NmqsszFgQ=f1w_Z8yt8cSOj3qBB2Q@mail.gmail.com>
	<CAEYYnxY9UZDuhBoHAP5kD3dCXxd0TwR761tt-kswjTEddQCCKg@mail.gmail.com>
	<CA+-p3AGT8E5caKU+2ExxjbpV-UqaaTot+Ni3DXEdOf37tADLgw@mail.gmail.com>
	<CAMAsSdJVckDE5=3S8wZj9P1-1-D6qBumGnofk9G7A7cksh1RVA@mail.gmail.com>
	<CABPQxstfSyVu6D-FSt7qa-o1co9AkhHsncTHGBrCK4rEM7fymg@mail.gmail.com>
	<CACBYxKLb=OK2EKu=2yS_1tswYz52rBF-59+8DZL+DfG6qHF52Q@mail.gmail.com>
	<CAEYYnxakT5SVD0xxWcyAXq+w-f-tY0rDA2c3YLRXK6oZ3ArF7Q@mail.gmail.com>
	<CAEYYnxaYfaUOes+A4kR4w+nh0LS8fgJytM2Nj7xoBErpHiRwbA@mail.gmail.com>
	<CANx3uAjGpOC+MXbQQCWWdTgyKAN3wm6y095R_+-+=PzAaaAHng@mail.gmail.com>
	<CAEYYnxZhLxdjBQD0BqEr9Su4UZ89ZnABQWoKYCqooO52Gm9Psw@mail.gmail.com>
Date: Thu, 22 May 2014 01:34:03 -0700
Message-ID: <CAJgQjQ9vrkasAQga1_adVGKgD9WLyO5-C8mSk6mip2grnGKp5A@mail.gmail.com>
Subject: Re: Calling external classes added by sc.addJar needs to be through reflection
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi DB,

I found it is a little hard to implement the solution I mentioned:

> Do not send the primary jar and secondary jars to executors'
> distributed cache. Instead, add them to "spark.jars" in SparkSubmit
> and serve them via http by called sc.addJar in SparkContext.

If you look at ApplicationMaster code, which is entry point in
yarn-cluster mode. It actually creates a thread of the user class
first and waits the user class to create a spark context. It means the
user class has to be on the classpath at that time. I think we need to
add the primary jar and secondary jars twice, once to system
classpath, and then to the executor classloader.

Best,
Xiangrui

On Wed, May 21, 2014 at 3:50 PM, DB Tsai <dbtsai@stanford.edu> wrote:
> @Xiangrui
> How about we send the primary jar and secondary jars into distributed cache
> without adding them into the system classloader of executors. Then we add
> them using custom classloader so we don't need to call secondary jars
> through reflection in primary jar. This will be consistent to what we do in
> standalone mode, and also solve the scalability of jar distribution issue.
>
> @Koert
> Yes, that's why I suggest we can either ignore the parent classloader of
> custom class loader to solve this as you say. In this case, we need add the
> all the classpath of the system loader into our custom one (which doesn't
> have parent) so we will not miss the default java classes. This is how
> tomcat works.
>
> @Patrick
> I agree that we should have the fix by Xiangrui first, since it solves most
> of the use case. I don't know when people will use dynamical addJar in Yarn
> since it's most useful for interactive environment.
>
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai
>
>
> On Wed, May 21, 2014 at 2:57 PM, Koert Kuipers <koert@tresata.com> wrote:
>
>> db tsai, i do not think userClassPathFirst is working, unless the classes
>> you load dont reference any classes already loaded by the parent
>> classloader (a mostly hypothetical situation)... i filed a jira for this
>> here:
>> https://issues.apache.org/jira/browse/SPARK-1863
>>
>>
>>
>> On Tue, May 20, 2014 at 1:04 AM, DB Tsai <dbtsai@stanford.edu> wrote:
>>
>> > In 1.0, there is a new option for users to choose which classloader has
>> > higher priority via spark.files.userClassPathFirst, I decided to submit
>> the
>> > PR for 0.9 first. We use this patch in our lab and we can use those jars
>> > added by sc.addJar without reflection.
>> >
>> > https://github.com/apache/spark/pull/834
>> >
>> > Can anyone comment if it's a good approach?
>> >
>> > Thanks.
>> >
>> >
>> > Sincerely,
>> >
>> > DB Tsai
>> > -------------------------------------------------------
>> > My Blog: https://www.dbtsai.com
>> > LinkedIn: https://www.linkedin.com/in/dbtsai
>> >
>> >
>> > On Mon, May 19, 2014 at 7:42 PM, DB Tsai <dbtsai@stanford.edu> wrote:
>> >
>> > > Good summary! We fixed it in branch 0.9 since our production is still
>> in
>> > > 0.9. I'm porting it to 1.0 now, and hopefully will submit PR for 1.0
>> > > tonight.
>> > >
>> > >
>> > > Sincerely,
>> > >
>> > > DB Tsai
>> > > -------------------------------------------------------
>> > > My Blog: https://www.dbtsai.com
>> > > LinkedIn: https://www.linkedin.com/in/dbtsai
>> > >
>> > >
>> > > On Mon, May 19, 2014 at 7:38 PM, Sandy Ryza <sandy.ryza@cloudera.com
>> > >wrote:
>> > >
>> > >> It just hit me why this problem is showing up on YARN and not on
>> > >> standalone.
>> > >>
>> > >> The relevant difference between YARN and standalone is that, on YARN,
>> > the
>> > >> app jar is loaded by the system classloader instead of Spark's custom
>> > URL
>> > >> classloader.
>> > >>
>> > >> On YARN, the system classloader knows about [the classes in the spark
>> > >> jars,
>> > >> the classes in the primary app jar].   The custom classloader knows
>> > about
>> > >> [the classes in secondary app jars] and has the system classloader as
>> > its
>> > >> parent.
>> > >>
>> > >> A few relevant facts (mostly redundant with what Sean pointed out):
>> > >> * Every class has a classloader that loaded it.
>> > >> * When an object of class B is instantiated inside of class A, the
>> > >> classloader used for loading B is the classloader that was used for
>> > >> loading
>> > >> A.
>> > >> * When a classloader fails to load a class, it lets its parent
>> > classloader
>> > >> try.  If its parent succeeds, its parent becomes the "classloader that
>> > >> loaded it".
>> > >>
>> > >> So suppose class B is in a secondary app jar and class A is in the
>> > primary
>> > >> app jar:
>> > >> 1. The custom classloader will try to load class A.
>> > >> 2. It will fail, because it only knows about the secondary jars.
>> > >> 3. It will delegate to its parent, the system classloader.
>> > >> 4. The system classloader will succeed, because it knows about the
>> > primary
>> > >> app jar.
>> > >> 5. A's classloader will be the system classloader.
>> > >> 6. A tries to instantiate an instance of class B.
>> > >> 7. B will be loaded with A's classloader, which is the system
>> > classloader.
>> > >> 8. Loading B will fail, because A's classloader, which is the system
>> > >> classloader, doesn't know about the secondary app jars.
>> > >>
>> > >> In Spark standalone, A and B are both loaded by the custom
>> classloader,
>> > so
>> > >> this issue doesn't come up.
>> > >>
>> > >> -Sandy
>> > >>
>> > >> On Mon, May 19, 2014 at 7:07 PM, Patrick Wendell <pwendell@gmail.com>
>> > >> wrote:
>> > >>
>> > >> > Having a user add define a custom class inside of an added jar and
>> > >> > instantiate it directly inside of an executor is definitely
>> supported
>> > >> > in Spark and has been for a really long time (several years). This
>> is
>> > >> > something we do all the time in Spark.
>> > >> >
>> > >> > DB - I'd hold off on a re-architecting of this until we identify
>> > >> > exactly what is causing the bug you are running into.
>> > >> >
>> > >> > In a nutshell, when the bytecode "new Foo()" is run on the executor,
>> > >> > it will ask the driver for the class over HTTP using a custom
>> > >> > classloader. Something in that pipeline is breaking here, possibly
>> > >> > related to the YARN deployment stuff.
>> > >> >
>> > >> >
>> > >> > On Mon, May 19, 2014 at 12:29 AM, Sean Owen <sowen@cloudera.com>
>> > wrote:
>> > >> > > I don't think a customer classloader is necessary.
>> > >> > >
>> > >> > > Well, it occurs to me that this is no new problem. Hadoop, Tomcat,
>> > etc
>> > >> > > all run custom user code that creates new user objects without
>> > >> > > reflection. I should go see how that's done. Maybe it's totally
>> > valid
>> > >> > > to set the thread's context classloader for just this purpose,
>> and I
>> > >> > > am not thinking clearly.
>> > >> > >
>> > >> > > On Mon, May 19, 2014 at 8:26 AM, Andrew Ash <andrew@andrewash.com
>> >
>> > >> > wrote:
>> > >> > >> Sounds like the problem is that classloaders always look in their
>> > >> > parents
>> > >> > >> before themselves, and Spark users want executors to pick up
>> > classes
>> > >> > from
>> > >> > >> their custom code before the ones in Spark plus its dependencies.
>> > >> > >>
>> > >> > >> Would a custom classloader that delegates to the parent after
>> first
>> > >> > >> checking itself fix this up?
>> > >> > >>
>> > >> > >>
>> > >> > >> On Mon, May 19, 2014 at 12:17 AM, DB Tsai <dbtsai@stanford.edu>
>> > >> wrote:
>> > >> > >>
>> > >> > >>> Hi Sean,
>> > >> > >>>
>> > >> > >>> It's true that the issue here is classloader, and due to the
>> > >> > classloader
>> > >> > >>> delegation model, users have to use reflection in the executors
>> to
>> > >> > pick up
>> > >> > >>> the classloader in order to use those classes added by
>> sc.addJars
>> > >> APIs.
>> > >> > >>> However, it's very inconvenience for users, and not documented
>> in
>> > >> > spark.
>> > >> > >>>
>> > >> > >>> I'm working on a patch to solve it by calling the protected
>> method
>> > >> > addURL
>> > >> > >>> in URLClassLoader to update the current default classloader, so
>> no
>> > >> > >>> customClassLoader anymore. I wonder if this is an good way to
>> go.
>> > >> > >>>
>> > >> > >>>   private def addURL(url: URL, loader: URLClassLoader){
>> > >> > >>>     try {
>> > >> > >>>       val method: Method =
>> > >> > >>> classOf[URLClassLoader].getDeclaredMethod("addURL",
>> classOf[URL])
>> > >> > >>>       method.setAccessible(true)
>> > >> > >>>       method.invoke(loader, url)
>> > >> > >>>     }
>> > >> > >>>     catch {
>> > >> > >>>       case t: Throwable => {
>> > >> > >>>         throw new IOException("Error, could not add URL to
>> system
>> > >> > >>> classloader")
>> > >> > >>>       }
>> > >> > >>>     }
>> > >> > >>>   }
>> > >> > >>>
>> > >> > >>>
>> > >> > >>>
>> > >> > >>> Sincerely,
>> > >> > >>>
>> > >> > >>> DB Tsai
>> > >> > >>> -------------------------------------------------------
>> > >> > >>> My Blog: https://www.dbtsai.com
>> > >> > >>> LinkedIn: https://www.linkedin.com/in/dbtsai
>> > >> > >>>
>> > >> > >>>
>> > >> > >>> On Sun, May 18, 2014 at 11:57 PM, Sean Owen <sowen@cloudera.com
>> >
>> > >> > wrote:
>> > >> > >>>
>> > >> > >>> > I might be stating the obvious for everyone, but the issue
>> here
>> > is
>> > >> > not
>> > >> > >>> > reflection or the source of the JAR, but the ClassLoader. The
>> > >> basic
>> > >> > >>> > rules are this.
>> > >> > >>> >
>> > >> > >>> > "new Foo" will use the ClassLoader that defines Foo. This is
>> > >> usually
>> > >> > >>> > the ClassLoader that loaded whatever it is that first
>> referenced
>> > >> Foo
>> > >> > >>> > and caused it to be loaded -- usually the ClassLoader holding
>> > your
>> > >> > >>> > other app classes.
>> > >> > >>> >
>> > >> > >>> > ClassLoaders can have a parent-child relationship.
>> ClassLoaders
>> > >> > always
>> > >> > >>> > look in their parent before themselves.
>> > >> > >>> >
>> > >> > >>> > (Careful then -- in contexts like Hadoop or Tomcat where your
>> > app
>> > >> is
>> > >> > >>> > loaded in a child ClassLoader, and you reference a class that
>> > >> Hadoop
>> > >> > >>> > or Tomcat also has (like a lib class) you will get the
>> > container's
>> > >> > >>> > version!)
>> > >> > >>> >
>> > >> > >>> > When you load an external JAR it has a separate ClassLoader
>> > which
>> > >> > does
>> > >> > >>> > not necessarily bear any relation to the one containing your
>> app
>> > >> > >>> > classes, so yeah it is not generally going to make "new Foo"
>> > work.
>> > >> > >>> >
>> > >> > >>> > Reflection lets you pick the ClassLoader, yes.
>> > >> > >>> >
>> > >> > >>> > I would not call setContextClassLoader.
>> > >> > >>> >
>> > >> > >>> > On Mon, May 19, 2014 at 12:00 AM, Sandy Ryza <
>> > >> > sandy.ryza@cloudera.com>
>> > >> > >>> > wrote:
>> > >> > >>> > > I spoke with DB offline about this a little while ago and he
>> > >> > confirmed
>> > >> > >>> > that
>> > >> > >>> > > he was able to access the jar from the driver.
>> > >> > >>> > >
>> > >> > >>> > > The issue appears to be a general Java issue: you can't
>> > directly
>> > >> > >>> > > instantiate a class from a dynamically loaded jar.
>> > >> > >>> > >
>> > >> > >>> > > I reproduced it locally outside of Spark with:
>> > >> > >>> > > ---
>> > >> > >>> > >     URLClassLoader urlClassLoader = new URLClassLoader(new
>> > >> URL[] {
>> > >> > new
>> > >> > >>> > > File("myotherjar.jar").toURI().toURL() }, null);
>> > >> > >>> > >
>> > >> Thread.currentThread().setContextClassLoader(urlClassLoader);
>> > >> > >>> > >     MyClassFromMyOtherJar obj = new MyClassFromMyOtherJar();
>> > >> > >>> > > ---
>> > >> > >>> > >
>> > >> > >>> > > I was able to load the class with reflection.
>> > >> > >>> >
>> > >> > >>>
>> > >> >
>> > >>
>> > >
>> > >
>> >
>>

From dev-return-7778-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 10:06:29 2014
Return-Path: <dev-return-7778-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 22B5C11DCC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 10:06:29 +0000 (UTC)
Received: (qmail 48427 invoked by uid 500); 22 May 2014 10:06:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48365 invoked by uid 500); 22 May 2014 10:06:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48357 invoked by uid 99); 22 May 2014 10:06:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 10:06:28 +0000
X-ASF-Spam-Status: No, hits=2.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FREEMAIL_REPLYTO_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [106.10.151.71] (HELO nm23-vm8.bullet.mail.sg3.yahoo.com) (106.10.151.71)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 10:06:22 +0000
Received: from [106.10.166.62] by nm23.bullet.mail.sg3.yahoo.com with NNFMP; 22 May 2014 10:06:00 -0000
Received: from [106.10.151.171] by tm19.bullet.mail.sg3.yahoo.com with NNFMP; 22 May 2014 10:06:00 -0000
Received: from [127.0.0.1] by omp1011.mail.sg3.yahoo.com with NNFMP; 22 May 2014 10:06:00 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 516370.41001.bm@omp1011.mail.sg3.yahoo.com
Received: (qmail 2594 invoked by uid 60001); 22 May 2014 10:06:00 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.co.in; s=s1024; t=1400753160; bh=rdtt+Aqo8rdek2QqPwFG/cVWrbqSMkq1r0WudCp6yhQ=; h=References:Message-ID:Date:From:Reply-To:Subject:To:MIME-Version:Content-Type; b=2HqppFmr+lBI7fFo/EuCVVDNthyPL6qKA/mUIEA4OfQdtF8B9kzZqMx76MbXEDJSYVNuAl9ZnAFpStJPTGMt0cwdpXvcSBa5yzhzByxFun0FQU7eqtfjBd8PfoFQKXugoVGtJ/DTc/7AGStNG0wE/2rr/R7nT4ot0d/A/kdTUrk=
X-YMail-OSG: Yu2Rj6QVM1lRQU888RfVnf3i4phOvqRIItzS4pl1fU7Tc2e
 Azon0XXc.yt9AodSL_Nbvst3h3WALD3E7OHs.SM5qE3RQmmQur7mB0BkizJA
 UCc9VsZvW7QxH3sbus88OQegdbDc2RyUU6pQ5pAwG2f_7VTqFbYk7T7IQmb8
 mB.47VnqeOlpA8cysfdrleayCR3YYqXE8y3_wvB0YxNB7jd4v7WaWI_tbDYR
 O0_2UAjaDi9ponxFfkN8V_VKUn.RDQEsbhyjr3wG44yTgGA8h2mQ6oSVl.a_
 enXDxZdZ7Fi.z6xl0vafsLfzibVvR9VooDUwFiaLN_V1bvW43tU6Wfwe_eQc
 BDCSK58Z_Ap6V6CIr8HIbKErzPNBFPhVoz_Bk0xhDRxLd7L7YSaAK8caRMLV
 6mi7G5n3X.Zw.rvo_P.4zwyyVBE0w_e6Gqu3EMzsM6lS15fdw11iHySrIze1
 CDKJ.WBXdT8hnpux9hnoNOCrW7Fm7re9vMolN.qsz5.EBQa86hLVchFU4dhW
 5HyULeEuXsQmQq4waZE_8Fg4zlIuhsSDKw4ESNjcJ2Oa9nYN_
Received: from [125.17.228.30] by web190406.mail.sg3.yahoo.com via HTTP; Thu, 22 May 2014 18:06:00 SGT
X-Rocket-MIMEInfo: 002.001,SGksCgoKSSB3b3VsZCBsaWtlIHRvIGRvIHNvbWUgY29udHJpYnV0aW9ucyB0b3dhcmRzIHRoZSBNTGxpYiAuSSd2ZSBhIGZldyBjb25jZXJucyByZWdhcmRpbmcgdGhlIHNhbWUuCgoxLiBJcyB0aGVyZSBhbnkgcmVhc29uIGZvciBpbXBsZW1lbnRpbmcgdGhlIGFsZ29yaXRobXMgc3VwcG9ydGVkIMKgYnkgTUxsaWIgaW4gU2NhbGEKMi4gV2lsbCB5b3UgYWNjZXB0IGlmIMKgdGhlIGNvbnRyaWJ1dGlvbnMgYXJlIGRvbmUgaW4gUHl0aG9uIG9yIEphdmEKClRoYW5rcywKTWVldGh1IE0BMAEBAQE-
X-Mailer: YahooMailWebService/0.8.188.663
References: 
Message-ID: <1400753160.5626.YahooMailNeo@web190406.mail.sg3.yahoo.com>
Date: Thu, 22 May 2014 18:06:00 +0800 (SGT)
From: MEETHU MATHEW <meethu2006@yahoo.co.in>
Reply-To: MEETHU MATHEW <meethu2006@yahoo.co.in>
Subject: Contributions to MLlib
To: "dev@spark.apache.org" <dev@spark.apache.org>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="1886548795-1214514491-1400753160=:5626"
X-Virus-Checked: Checked by ClamAV on apache.org

--1886548795-1214514491-1400753160=:5626
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

Hi,=0A=0A=0AI would like to do some contributions towards the MLlib .I've a=
 few concerns regarding the same.=0A=0A1. Is there any reason for implement=
ing the algorithms supported =A0by MLlib in Scala=0A2. Will you accept if =
=A0the contributions are done in Python or Java=0A=0AThanks,=0AMeethu M
--1886548795-1214514491-1400753160=:5626--

From dev-return-7779-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 15:04:27 2014
Return-Path: <dev-return-7779-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0D6FA117DA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 15:04:27 +0000 (UTC)
Received: (qmail 9264 invoked by uid 500); 22 May 2014 15:04:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9195 invoked by uid 500); 22 May 2014 15:04:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9187 invoked by uid 99); 22 May 2014 15:04:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 15:04:26 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of gerard.maas@gmail.com designates 74.125.82.178 as permitted sender)
Received: from [74.125.82.178] (HELO mail-we0-f178.google.com) (74.125.82.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 15:04:22 +0000
Received: by mail-we0-f178.google.com with SMTP id u56so3519847wes.37
        for <dev@spark.apache.org>; Thu, 22 May 2014 08:04:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=uxc+RtQKcgNkKPKq0b+aG/ty0nj0Ep2Sofelz1yBrI0=;
        b=b7o30+dZC5yLAp50OUlKFezcRi1zeFISQRXgFMwQoUWauldKyhJuQZNOQPZuxj40fl
         MRyeibV4DJ6NoKefr9XJyc+fs9xTBqC9IrBRt5iChtIdv/FrsbObJ6xhfslvMFoKRwz3
         O7QKLOLVCkXnQRqSn1yW1v4ykXky5bu0sDuQkxc6Ru71JKmQM/fyPJtj5k6gnORrtXfb
         WX4B9QWj9eRIEGM/WqKsOu6qbgwNMUtmCjzKeEYKO7MxFSdO/xxokquVx4m8LjySRH+J
         OSJIlQ7JVx2t8zt82YTmR65BtWQoCnuiHwN0M8Wm9AlbX38yOp/AQPFJobYMOB0tOhck
         1dLg==
X-Received: by 10.180.109.69 with SMTP id hq5mr16882224wib.30.1400771041731;
 Thu, 22 May 2014 08:04:01 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.173.2 with HTTP; Thu, 22 May 2014 08:03:31 -0700 (PDT)
In-Reply-To: <CA+-p3AGerFkP_9bsVDw4tu9QUhNH_--aEiWL4NqB+r3xWsyQjA@mail.gmail.com>
References: <CAMc-71kjOHyVcL+4GZW=ch8+ZxvkDcXWnW07Uvaa4ePX7n79=A@mail.gmail.com>
 <CA+-p3AGerFkP_9bsVDw4tu9QUhNH_--aEiWL4NqB+r3xWsyQjA@mail.gmail.com>
From: Gerard Maas <gerard.maas@gmail.com>
Date: Thu, 22 May 2014 17:03:31 +0200
Message-ID: <CAMc-71nLz_EpxkJzkzQPVyBP-b2z8Gofehvex5DNc8kamoLmpA@mail.gmail.com>
Subject: Re: Should SPARK_HOME be needed with Mesos?
To: dev@spark.apache.org, andrew@andrewash.com
Cc: pwendell@gmail.com
Content-Type: multipart/alternative; boundary=e89a8f3baa075aa99d04f9fe6cac
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f3baa075aa99d04f9fe6cac
Content-Type: text/plain; charset=UTF-8

Sure.  Should I create a Jira as well?

I saw there's already a broader ticket regarding the ambiguous use of
SPARK_HOME [1]  (cc: Patrick as owner of that ticket)

I don't know if it would be more relevant to remove the use of SPARK_HOME
when using mesos and have the assembly as the only way forward, or whether
that's a too radical change that might break some existing systems.

>From a real-world ops perspective, the assembly should be the way to go. I
don't see installing and configuring Spark distros on a mesos master as a
way to have the mesos executor in place.

-kr, Gerard.

[1] https://issues.apache.org/jira/browse/SPARK-1110


On Thu, May 22, 2014 at 6:19 AM, Andrew Ash <andrew@andrewash.com> wrote:

> Hi Gerard,
>
> I agree that your second option seems preferred.  You shouldn't have to
> specify a SPARK_HOME if the executor is going to use the spark.executor.uri
> instead.  Can you send in a pull request that includes your proposed
> changes?
>
> Andrew
>
>
> On Wed, May 21, 2014 at 10:19 AM, Gerard Maas <gerard.maas@gmail.com>
> wrote:
>
> > Spark dev's,
> >
> > I was looking into a question asked on the user list where a
> > ClassNotFoundException was thrown when running a job on Mesos. Curious
> > issue with serialization on Mesos: more details here [1]:
> >
> > When trying to run that simple example on my Mesos installation, I faced
> > another issue: I got an error that "SPARK_HOME" was not set. I found that
> > curious b/c a local spark installation should not be required to run a
> job
> > on Mesos. All that's needed is the executor package, being the
> > assembly.tar.gz on a reachable location (HDFS/S3/HTTP).
> >
> > I went looking into the code and indeed there's a check on SPARK_HOME [2]
> > regardless of the presence of the assembly but it's actually only used if
> > the assembly is not provided (which is a kind-of best-effort recovery
> > strategy).
> >
> > Current flow:
> >
> > if (!SPARK_HOME) fail("No SPARK_HOME")
> > else if (assembly) { use assembly) }
> > else { try use SPARK_HOME to build spark_executor }
> >
> > Should be:
> > sparkExecutor =  if (assembly) {assembly}
> >                  else if (SPARK_HOME) {try use SPARK_HOME to build
> > spark_executor}
> >                  else { fail("No executor found. Please provide
> > spark.executor.uri (preferred) or spark.home")
> >
> > What do you think?
> >
> > -kr, Gerard.
> >
> >
> > [1]
> >
> >
> http://apache-spark-user-list.1001560.n3.nabble.com/ClassNotFoundException-with-Spark-Mesos-spark-shell-works-fine-td6165.html
> >
> > [2]
> >
> >
> https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend.scala#L89
> >
>

--e89a8f3baa075aa99d04f9fe6cac--

From dev-return-7780-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 16:50:12 2014
Return-Path: <dev-return-7780-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 06D8411CC7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 16:50:12 +0000 (UTC)
Received: (qmail 85296 invoked by uid 500); 22 May 2014 16:50:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85237 invoked by uid 500); 22 May 2014 16:50:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85229 invoked by uid 99); 22 May 2014 16:50:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 16:50:11 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of kevin.markey@oracle.com designates 141.146.126.69 as permitted sender)
Received: from [141.146.126.69] (HELO aserp1040.oracle.com) (141.146.126.69)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 16:50:07 +0000
Received: from acsinet21.oracle.com (acsinet21.oracle.com [141.146.126.237])
	by aserp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2) with ESMTP id s4MGnhKR020415
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=OK)
	for <dev@spark.apache.org>; Thu, 22 May 2014 16:49:43 GMT
Received: from userz7021.oracle.com (userz7021.oracle.com [156.151.31.85])
	by acsinet21.oracle.com (8.14.4+Sun/8.14.4) with ESMTP id s4MGngsP015463
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=FAIL)
	for <dev@spark.apache.org>; Thu, 22 May 2014 16:49:43 GMT
Received: from abhmp0005.oracle.com (abhmp0005.oracle.com [141.146.116.11])
	by userz7021.oracle.com (8.14.4+Sun/8.14.4) with ESMTP id s4MGnfSQ020572
	for <dev@spark.apache.org>; Thu, 22 May 2014 16:49:42 GMT
Received: from [10.10.210.197] (/107.1.138.30)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Thu, 22 May 2014 09:49:41 -0700
Message-ID: <537E2AA4.8070303@oracle.com>
Date: Thu, 22 May 2014 10:49:40 -0600
From: Kevin Markey <kevin.markey@oracle.com>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20100101 Thunderbird/24.5.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC10)
References: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>	<CALuGr6YvTxM+YyXx-+p3XsfcXT2wWE3iJ0T43HqJzKfs6s2UYQ@mail.gmail.com>	<CAAsvFPkk+TVQiAu+_XiDOOgqUr4tck8ccA=YQ5Aiv_SF7hn7Dg@mail.gmail.com>	<537D2A07.4030702@oracle.com> <CA+qbEUNKVF2xb3SM-EXwiTom7+qX+ZTbeNS6nHVvfp3-_a=fNw@mail.gmail.com> <1400728729.27373.YahooMailNeo@web140101.mail.bf1.yahoo.com> <537DA829.9030000@oracle.com>
In-Reply-To: <537DA829.9030000@oracle.com>
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Source-IP: acsinet21.oracle.com [141.146.126.237]
X-Virus-Checked: Checked by ClamAV on apache.org

I retested several different cases...

1. FS closed exception shows up ONLY in RC-10, not in Spark 0.9.1, with 
both Hadoop 2.2 and 2.3.
2. SPARK-1898 has no effect for my use cases.
3. The failure to report that the underlying application is "RUNNING" 
and that it has succeeded is due ONLY to my user error.

The FS closed exception only effects the cleanup of the staging 
directory, not the final success or failure.  I've not yet tested the 
effect of changing my application's initialization, use, or closing of 
FileSystem.

Thanks again.
Kevin

On 05/22/2014 01:32 AM, Kevin Markey wrote:
> I've discovered that one of the anomalies I encountered was due to a 
> (embarrassing? humorous?) user error.  See the user list thread 
> "Failed RC-10 yarn-cluster job for FS closed error when cleaning up 
> staging directory" for my discussion.  With the user error corrected, 
> the FS closed exception only prevents deletion of the staging 
> directory, but does not affect completion with "SUCCESS." The FS 
> closed exception still needs some investigation at least by me.
>
> I tried the patch reported by SPARK-1898, but it didn't fix the 
> problem without fixing the user error.  I did not attempt to test my 
> fix without the patch, so I can't pass judgment on the patch.
>
> Although this is merely a pseudocluster based test -- I can't 
> reconfigure our cluster with RC-10 -- I'll now change my vote to...
>
> +1.
>
> Thanks all who helped.
> Kevin
>
>
>
> On 05/21/2014 09:18 PM, Tom Graves wrote:
>> I don't think Kevin's issue would be with an api change in 
>> YarnClientImpl since in both cases he says he is using hadoop 2.3.0.  
>> I'll take a look at his post in the user list.
>>
>> Tom
>>
>>
>>
>>
>> On Wednesday, May 21, 2014 7:01 PM, Colin McCabe 
>> <cmccabe@alumni.cmu.edu> wrote:
>>
>>
>> Hi Kevin,
>>
>> Can you try https://issues.apache.org/jira/browse/SPARK-1898 to see 
>> if it
>> fixes your issue?
>>
>> Running in YARN cluster mode, I had a similar issue where Spark was 
>> able to
>> create a Driver and an Executor via YARN, but then it stopped making any
>> progress.
>>
>> Note: I was using a pre-release version of CDH5.1.0, not 2.3 like you 
>> were
>> using.
>>
>> best,
>> Colin
>>
>>
>>
>> On Wed, May 21, 2014 at 3:34 PM, Kevin Markey 
>> <kevin.markey@oracle.com>wrote:
>>
>>> 0
>>>
>>> Abstaining because I'm not sure if my failures are due to Spark,
>>> configuration, or other factors...
>>>
>>> Compiled and deployed RC10 for YARN, Hadoop 2.3
>>   per Spark 1.0.0 Yarn
>>> documentation.  No problems.
>>> Rebuilt applications against RC10 and Hadoop 2.3.0 (plain vanilla 
>>> Apache
>>> release).
>>> Updated scripts for various applications.
>>> Application had successfully compiled and run against Spark 0.9.1 and
>>> Hadoop 2.3.0.
>>> Ran in "yarn-cluster" mode.
>>> Application ran to conclusion except that it ultimately failed 
>>> because of
>>> an exception when Spark tried to clean up the staging directory.  Also,
>>> where before Yarn would report the running program as "RUNNING", it 
>>> only
>>> reported this application as "ACCEPTED".  It appeared to run two 
>>> containers
>>> when the first instance never reported that it was RUNNING.
>>>
>>> I will post a
>>   separate note to the USER list about the specifics.
>>> Thanks
>>> Kevin Markey
>>>
>>>
>>>
>>> On 05/21/2014 10:58 AM, Mark Hamstra wrote:
>>>
>>>> +1
>>>>
>>>>
>>>> On Tue, May 20, 2014 at 11:09 PM, Henry Saputra 
>>>> <henry.saputra@gmail.com>
>>>> wrote:
>>>>
>>>>    Signature and hash for source looks good
>>>>> No external executable package with source - good
>>>>> Compiled with git and maven - good
>>>>> Ran examples and sample programs locally and standalone -good
>>>>>
>>>>> +1
>>>>>
>>>>> - Henry
>>>>>
>>>>>
>>>>>
>>>>> On Tue, May 20, 2014 at 1:13 PM, Tathagata Das
>>>>> <tathagata.das1565@gmail.com> wrote:
>>>>>
>>>>>> Please vote on releasing the following candidate as Apache Spark 
>>>>>> version
>>>>>>
>>>>> 1.0.0!
>>>>>
>>>>>> This has a few bug fixes on top of rc9:
>>>>>> SPARK-1875: https://github.com/apache/spark/pull/824
>>>>>> SPARK-1876: https://github.com/apache/spark/pull/819
>>>>>> SPARK-1878: https://github.com/apache/spark/pull/822
>>>>>> SPARK-1879: https://github.com/apache/spark/pull/823
>>>>>>
>>>>>> The tag to be voted on is v1.0.0-rc10 (commit d8070234):
>>>>>>
>>>>>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=
>>>>> d807023479ce10aec28ef3c1ab646ddefc2e663c
>>>>>
>>>>>> The
>>   release files, including signatures, digests, etc. can be found at:
>>>>>> http://people.apache.org/~tdas/spark-1.0.0-rc10/
>>>>>>
>>>>>> The release artifacts are signed with the following key:
>>>>>> https://people.apache.org/keys/committer/tdas.asc
>>>>>>
>>>>>> The staging repository for this release can be found at:
>>>>>> https://repository.apache.org/content/repositories/orgapachespark-1018/ 
>>>>>>
>>>>>>
>>>>>> The documentation
>>   corresponding to this release can be found at:
>>>>>> http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>>>>>
>>>>>> The full list of changes in this release can be found at:
>>>>>>
>>>>>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=blob;
>>>>> f=CHANGES.txt;h=d21f0ace6326e099360975002797eb7cba9d5273;hb=
>>>>> d807023479ce10aec28ef3c1ab646ddefc2e663c
>>>>>
>>>>>> Please vote on releasing this package as Apache Spark 1.0.0!
>>>>>>
>>>>>> The vote is open until
>>   Friday, May 23, at 20:00 UTC and passes if
>>>>>> amajority of at least 3 +1 PMC votes are cast.
>>>>>>
>>>>>> [ ] +1 Release this package as Apache Spark 1.0.0
>>>>>> [ ] -1 Do not release this package because ...
>>>>>>
>>>>>> To learn more about Apache Spark, please see
>>>>>> http://spark.apache.org/
>>>>>>
>>>>>> ====== API Changes ======
>>>>>> We welcome users to compile Spark applications against 1.0. There 
>>>>>> are
>>>>>> a few API changes in this release. Here are links to the associated
>>>>>> upgrade guides - user facing changes have been kept as small as
>>>>>> possible.
>>>>>>
>>>>>> Changes to ML vector specification:
>>>>>>
>>>>>>    http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>>>> mllib-guide.html#from-09-to-10
>>>>>
>>>>>> Changes to the Java API:
>>>>>>
>>>>>>    http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>>>> java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>>>>>
>>>>>> Changes to the streaming API:
>>>>>>
>>>>>>    http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>>>> streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x 
>>>>>
>>>>>
>>>>>> Changes to the GraphX API:
>>>>>>
>>>>>>    http://people.apache.org/~tdas/spark-1.0.0-rc10-docs/
>>>>> graphx-programming-guide.html#upgrade-guide-from-spark-091
>>>>>
>>>>>> Other changes:
>>>>>> coGroup and related functions now return Iterable[T] instead of 
>>>>>> Seq[T]
>>>>>> ==> Call toSeq on the result to restore the old behavior
>>>>>>
>>>>>> SparkContext.jarOfClass returns Option[String] instead of 
>>>>>> Seq[String]
>>>>>> ==> Call toSeq on the result to restore old behavior
>>>>>>
>


From dev-return-7781-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 17:06:50 2014
Return-Path: <dev-return-7781-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A64B311D8C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 17:06:50 +0000 (UTC)
Received: (qmail 21467 invoked by uid 500); 22 May 2014 17:06:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21408 invoked by uid 500); 22 May 2014 17:06:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21400 invoked by uid 99); 22 May 2014 17:06:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 17:06:50 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of vanzin@cloudera.com designates 209.85.192.46 as permitted sender)
Received: from [209.85.192.46] (HELO mail-qg0-f46.google.com) (209.85.192.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 17:06:46 +0000
Received: by mail-qg0-f46.google.com with SMTP id q108so6001596qgd.5
        for <dev@spark.apache.org>; Thu, 22 May 2014 10:06:25 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=e2I0NOlw0hnoaKI3s0lbkcMOEf2epg+1PrmekAvkICM=;
        b=jzpa6BRCnvCHg1t1DNtIeK0eky3NpUNRpeDlIKiheWxyUl0Hqfnx//NAC65BzCVjhy
         fWS9SuwrkbCr0OnVE1oEioJb12B4Nq5+440s8Ke4ojmhdbK3inxHJZYrmGH2FRLtMQFY
         k0/XTd9hLvO/vYNC1D7NO1tYFNoOzIUR8AeadeTYC3jmpuxsaAUDsUsQ+jnJfNJox+8z
         f5J/RW/B7L2m+HmuvUI1IziCazGKzxR/9I6/nD6pkRIgBh/rWqm4dirrCLmV0dFrfgrl
         0YtM4zlewPzURARq6QHBMXfErdNHVieNQLuRIJhev3xnqssKI9G0sSo6GbSocE0KVQcS
         TxGA==
X-Gm-Message-State: ALoCoQl3V0oczYy9GFejdnRBGNpBmFJDjdblMrZkjTAz1dQ2SFEO6CyIq3UotqTDRopJqdX/qql9
MIME-Version: 1.0
X-Received: by 10.229.83.131 with SMTP id f3mr15882043qcl.19.1400778385400;
 Thu, 22 May 2014 10:06:25 -0700 (PDT)
Received: by 10.229.55.6 with HTTP; Thu, 22 May 2014 10:06:25 -0700 (PDT)
In-Reply-To: <537E2AA4.8070303@oracle.com>
References: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>
	<CALuGr6YvTxM+YyXx-+p3XsfcXT2wWE3iJ0T43HqJzKfs6s2UYQ@mail.gmail.com>
	<CAAsvFPkk+TVQiAu+_XiDOOgqUr4tck8ccA=YQ5Aiv_SF7hn7Dg@mail.gmail.com>
	<537D2A07.4030702@oracle.com>
	<CA+qbEUNKVF2xb3SM-EXwiTom7+qX+ZTbeNS6nHVvfp3-_a=fNw@mail.gmail.com>
	<1400728729.27373.YahooMailNeo@web140101.mail.bf1.yahoo.com>
	<537DA829.9030000@oracle.com>
	<537E2AA4.8070303@oracle.com>
Date: Thu, 22 May 2014 10:06:25 -0700
Message-ID: <CAAOnQ7t9OkOj9mh==RYzHdOka_wwgVo50ZAgutfBO4b3-e58wg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC10)
From: Marcelo Vanzin <vanzin@cloudera.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Kevin,

On Thu, May 22, 2014 at 9:49 AM, Kevin Markey <kevin.markey@oracle.com> wrote:
> The FS closed exception only effects the cleanup of the staging directory,
> not the final success or failure.  I've not yet tested the effect of
> changing my application's initialization, use, or closing of FileSystem.

Without going and reading more of the Spark code, if your app is
explicitly close()'ing the FileSystem instance, it may be causing the
exception. If Spark is caching the FileSystem instance, your app is
probably closing that same instance (which it got from the HDFS
library's internal cache).

It would be nice if you could test that theory; it might be worth
knowing that's the case so that we can tell people not to do that.

-- 
Marcelo

From dev-return-7782-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 17:16:48 2014
Return-Path: <dev-return-7782-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 73C9511E2C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 17:16:48 +0000 (UTC)
Received: (qmail 46985 invoked by uid 500); 22 May 2014 17:16:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46926 invoked by uid 500); 22 May 2014 17:16:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46917 invoked by uid 99); 22 May 2014 17:16:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 17:16:48 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.212.173 as permitted sender)
Received: from [209.85.212.173] (HELO mail-wi0-f173.google.com) (209.85.212.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 17:16:45 +0000
Received: by mail-wi0-f173.google.com with SMTP id bs8so9629014wib.0
        for <dev@spark.apache.org>; Thu, 22 May 2014 10:16:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=zJ90DXCyH4x3vUP5cGfn6orgp+WcrCNBIfEMVrI6iRA=;
        b=mWa+UTvORL9BeT9HGhmb9J0feOIvWLqf3D1MNzkb8N93WAuQAwhHuZ5RFi9EIuwzNE
         qO6eJfohyngZGk3G3UBE0BexIo6ZAElYJm/zrfAUNdX40PrUPiq/AlaPNTu4kIb4i7Kg
         LWE1KDCrcly+Y4+st8yQ8MoerGrSLqiDK6Kk2E/Z87IE8839puFe+ce/s0mvdln14WZk
         GI+couLj9exYpBihDaiI8NXm7uUXF+xBBAEISG7B/4elSIZZoDOyy/kd4bOtD5k5ZD3h
         eO+41KQ9qLmCtZA7MEwaciJTjf9EOkLPulwI4ASj9avG0b4kKjXmbAYzAYvlbEwNqG/g
         6jbQ==
MIME-Version: 1.0
X-Received: by 10.180.73.201 with SMTP id n9mr17630468wiv.45.1400778982371;
 Thu, 22 May 2014 10:16:22 -0700 (PDT)
Received: by 10.194.187.20 with HTTP; Thu, 22 May 2014 10:16:22 -0700 (PDT)
In-Reply-To: <1400753160.5626.YahooMailNeo@web190406.mail.sg3.yahoo.com>
References: <1400753160.5626.YahooMailNeo@web190406.mail.sg3.yahoo.com>
Date: Thu, 22 May 2014 10:16:22 -0700
Message-ID: <CAJgQjQ9=0==tjf0cd_Tv-pWy2kbDhg2DfO69=GVJ09thFrFNyA@mail.gmail.com>
Subject: Re: Contributions to MLlib
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org, MEETHU MATHEW <meethu2006@yahoo.co.in>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Meethu,

Thanks for asking! Scala is the native language in Spark. Implementing
algorithms in Scala can utilize the full power of Spark Core. Also,
Scala's syntax is very concise. Implementing ML algorithms using
different languages would increase the maintenance cost. However,
there are still much work to be done in the Python/Java land. For
example, we currently do not support distributed matrix and decision
tree in Python, and those interfaces may not be friendly for Java
users. If you would like to contribute to MLlib in Python or Java, it
would be a good place to start. Thanks!

Best,
Xiangrui

On Thu, May 22, 2014 at 3:06 AM, MEETHU MATHEW <meethu2006@yahoo.co.in> wrote:
> Hi,
>
>
> I would like to do some contributions towards the MLlib .I've a few concerns regarding the same.
>
> 1. Is there any reason for implementing the algorithms supported  by MLlib in Scala
> 2. Will you accept if  the contributions are done in Python or Java
>
> Thanks,
> Meethu M

From dev-return-7783-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 19:06:43 2014
Return-Path: <dev-return-7783-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A58DC115B1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 19:06:43 +0000 (UTC)
Received: (qmail 48280 invoked by uid 500); 22 May 2014 19:06:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48217 invoked by uid 500); 22 May 2014 19:06:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48209 invoked by uid 99); 22 May 2014 19:06:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 19:06:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rarecactus@gmail.com designates 209.85.212.181 as permitted sender)
Received: from [209.85.212.181] (HELO mail-wi0-f181.google.com) (209.85.212.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 19:06:39 +0000
Received: by mail-wi0-f181.google.com with SMTP id n15so4896810wiw.2
        for <dev@spark.apache.org>; Thu, 22 May 2014 12:06:18 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=YCC0NtJWjomBE4XluEA0FbNSf6HhMaYu2A6W9ah14Jw=;
        b=xq2iCxd3wJqFqmjDNZFTnaECoBT/f1wx7GHGPxh25xH7JZR+T0r54iSuv9gVm0EjB6
         jMuiVtj+lCJB3czpHqTk5HdyNiz9g1hN4s5PemqhSCeTIIQHaOliYj9F6s2/4hbdu/O9
         6boLu5/VDOjnXQ6fB2YHhdt7yUmtLRaLGzWbYWdggBnZkoxT/4NIVUwYqXOIizwxmfbF
         AvKVl5OqrqyX5fOMC4v1CQR90KxSYnC2UDPDHL+j0o1HkOahhXjRsSjtNk9D+19TYTaT
         wcInh09oEc71I9BTbK2wcDn9PWbMnz39iS/2c15jMt0ar2ETrjpSa2h12+/GutSmyFw8
         PCIA==
MIME-Version: 1.0
X-Received: by 10.194.187.107 with SMTP id fr11mr3914689wjc.70.1400785578291;
 Thu, 22 May 2014 12:06:18 -0700 (PDT)
Sender: rarecactus@gmail.com
Received: by 10.194.122.135 with HTTP; Thu, 22 May 2014 12:06:18 -0700 (PDT)
In-Reply-To: <CAAOnQ7t9OkOj9mh==RYzHdOka_wwgVo50ZAgutfBO4b3-e58wg@mail.gmail.com>
References: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>
	<CALuGr6YvTxM+YyXx-+p3XsfcXT2wWE3iJ0T43HqJzKfs6s2UYQ@mail.gmail.com>
	<CAAsvFPkk+TVQiAu+_XiDOOgqUr4tck8ccA=YQ5Aiv_SF7hn7Dg@mail.gmail.com>
	<537D2A07.4030702@oracle.com>
	<CA+qbEUNKVF2xb3SM-EXwiTom7+qX+ZTbeNS6nHVvfp3-_a=fNw@mail.gmail.com>
	<1400728729.27373.YahooMailNeo@web140101.mail.bf1.yahoo.com>
	<537DA829.9030000@oracle.com>
	<537E2AA4.8070303@oracle.com>
	<CAAOnQ7t9OkOj9mh==RYzHdOka_wwgVo50ZAgutfBO4b3-e58wg@mail.gmail.com>
Date: Thu, 22 May 2014 12:06:18 -0700
X-Google-Sender-Auth: CRac2wRuE-Zf4GdqWJRyqP0coK4
Message-ID: <CA+qbEUPLi_oVnYBeuak0fgxZadbjMO73rMEjx3ZzcHYL1_32jg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC10)
From: Colin McCabe <cmccabe@alumni.cmu.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bb03f60ccf8a204fa01ce3c
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bb03f60ccf8a204fa01ce3c
Content-Type: text/plain; charset=UTF-8

The FileSystem cache is something that has caused a lot of pain over the
years.  Unfortunately we (in Hadoop core) can't change the way it works now
because there are too many users depending on the current behavior.

Basically, the idea is that when you request a FileSystem with certain
options with FileSystem#get, you might get a reference to an FS object that
already exists, from our FS cache cache singleton.  Unfortunately, this
also means that someone else can change the working directory on you or
close the FS underneath you.  The FS is basically shared mutable state, and
you don't know whom you're sharing with.

It might be better for Spark to call FileSystem#newInstance, which bypasses
the FileSystem cache and always creates a new object.  If Spark can hang on
to the FS for a while, it can get the benefits of caching without the
downsides.  In HDFS, multiple FS instances can also share things like the
socket cache between them.

best,
Colin


On Thu, May 22, 2014 at 10:06 AM, Marcelo Vanzin <vanzin@cloudera.com>wrote:

> Hi Kevin,
>
> On Thu, May 22, 2014 at 9:49 AM, Kevin Markey <kevin.markey@oracle.com>
> wrote:
> > The FS closed exception only effects the cleanup of the staging
> directory,
> > not the final success or failure.  I've not yet tested the effect of
> > changing my application's initialization, use, or closing of FileSystem.
>
> Without going and reading more of the Spark code, if your app is
> explicitly close()'ing the FileSystem instance, it may be causing the
> exception. If Spark is caching the FileSystem instance, your app is
> probably closing that same instance (which it got from the HDFS
> library's internal cache).
>
> It would be nice if you could test that theory; it might be worth
> knowing that's the case so that we can tell people not to do that.
>
> --
> Marcelo
>

--047d7bb03f60ccf8a204fa01ce3c--

From dev-return-7784-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 19:27:13 2014
Return-Path: <dev-return-7784-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B2CE61169B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 19:27:13 +0000 (UTC)
Received: (qmail 99732 invoked by uid 500); 22 May 2014 19:27:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99657 invoked by uid 500); 22 May 2014 19:27:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99649 invoked by uid 99); 22 May 2014 19:27:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 19:27:13 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.128.174] (HELO mail-ve0-f174.google.com) (209.85.128.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 19:27:10 +0000
Received: by mail-ve0-f174.google.com with SMTP id jw12so5035409veb.19
        for <dev@spark.apache.org>; Thu, 22 May 2014 12:26:46 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=9wbXrFHbgRI8E1WrslSxcqnZA7o9hVCvGYJbo2Heygg=;
        b=XyAjyTqGWhL+L+OxSYyXxHUQmYWSOHKBLx//Ap7Joidwj5pIZk5MaxP3dlQRo1ETPH
         bovRXVHvvBsIi66MmyUSgBldhLMHvTzXJFVb8DdTrDo/HKD3DU37IehRaHpcP2Hmz1nt
         cl3ntciY9sb3BJoqpXAjPPR2tX7qws/SBEosDMnHeZ14wSspzRa5XmAnuALTvO/iFtOO
         7qZ//YnhOU35pG8YYTVi2LpNpp3TPgt1u4K2CnuLUMpRn+mAxa4V6X4P5uGqO7AhywXn
         NTpt97ZRJbYoYeZwFP1KcWDDGPguJbK94X92BGUf638TVK3walZKeqM1UIZeBsRRCznj
         1kWw==
X-Gm-Message-State: ALoCoQmannapuEPdhYDjPamqwTKNXpoOyG7df5l7g7knKGRayvDQngbjSSk85PCZ8jC9l3acd+3t
X-Received: by 10.58.182.129 with SMTP id ee1mr45830684vec.14.1400786805864;
        Thu, 22 May 2014 12:26:45 -0700 (PDT)
Received: from mail-ve0-f181.google.com (mail-ve0-f181.google.com [209.85.128.181])
        by mx.google.com with ESMTPSA id ew15sm754889veb.4.2014.05.22.12.26.44
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 22 May 2014 12:26:44 -0700 (PDT)
Received: by mail-ve0-f181.google.com with SMTP id pa12so4970642veb.40
        for <dev@spark.apache.org>; Thu, 22 May 2014 12:26:43 -0700 (PDT)
X-Received: by 10.58.119.167 with SMTP id kv7mr83984veb.78.1400786803861; Thu,
 22 May 2014 12:26:43 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Thu, 22 May 2014 12:26:23 -0700 (PDT)
In-Reply-To: <CAMc-71nLz_EpxkJzkzQPVyBP-b2z8Gofehvex5DNc8kamoLmpA@mail.gmail.com>
References: <CAMc-71kjOHyVcL+4GZW=ch8+ZxvkDcXWnW07Uvaa4ePX7n79=A@mail.gmail.com>
 <CA+-p3AGerFkP_9bsVDw4tu9QUhNH_--aEiWL4NqB+r3xWsyQjA@mail.gmail.com> <CAMc-71nLz_EpxkJzkzQPVyBP-b2z8Gofehvex5DNc8kamoLmpA@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Thu, 22 May 2014 15:26:23 -0400
Message-ID: <CA+-p3AE6LfPSqzDBW0x7dPkzR0K955vfeGkonnnUrrGx29tYAw@mail.gmail.com>
Subject: Re: Should SPARK_HOME be needed with Mesos?
To: Gerard Maas <gerard.maas@gmail.com>
Cc: dev@spark.apache.org, Patrick Wendell <pwendell@gmail.com>
Content-Type: multipart/alternative; boundary=001a11c3a4a2d9b34f04fa021730
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3a4a2d9b34f04fa021730
Content-Type: text/plain; charset=UTF-8

Fixing the immediate issue of requiring SPARK_HOME to be set when it's not
actually used is a separate ticket in my mind from a larger cleanup of what
SPARK_HOME means across the cluster.

I think you should file a new ticket for just this particular issue.


On Thu, May 22, 2014 at 11:03 AM, Gerard Maas <gerard.maas@gmail.com> wrote:

> Sure.  Should I create a Jira as well?
>
> I saw there's already a broader ticket regarding the ambiguous use of
> SPARK_HOME [1]  (cc: Patrick as owner of that ticket)
>
> I don't know if it would be more relevant to remove the use of SPARK_HOME
> when using mesos and have the assembly as the only way forward, or whether
> that's a too radical change that might break some existing systems.
>
> From a real-world ops perspective, the assembly should be the way to go. I
> don't see installing and configuring Spark distros on a mesos master as a
> way to have the mesos executor in place.
>
> -kr, Gerard.
>
> [1] https://issues.apache.org/jira/browse/SPARK-1110
>
>
> On Thu, May 22, 2014 at 6:19 AM, Andrew Ash <andrew@andrewash.com> wrote:
>
>> Hi Gerard,
>>
>> I agree that your second option seems preferred.  You shouldn't have to
>> specify a SPARK_HOME if the executor is going to use the
>> spark.executor.uri
>> instead.  Can you send in a pull request that includes your proposed
>> changes?
>>
>> Andrew
>>
>>
>> On Wed, May 21, 2014 at 10:19 AM, Gerard Maas <gerard.maas@gmail.com>
>> wrote:
>>
>> > Spark dev's,
>> >
>> > I was looking into a question asked on the user list where a
>> > ClassNotFoundException was thrown when running a job on Mesos. Curious
>> > issue with serialization on Mesos: more details here [1]:
>> >
>> > When trying to run that simple example on my Mesos installation, I faced
>> > another issue: I got an error that "SPARK_HOME" was not set. I found
>> that
>> > curious b/c a local spark installation should not be required to run a
>> job
>> > on Mesos. All that's needed is the executor package, being the
>> > assembly.tar.gz on a reachable location (HDFS/S3/HTTP).
>> >
>> > I went looking into the code and indeed there's a check on SPARK_HOME
>> [2]
>> > regardless of the presence of the assembly but it's actually only used
>> if
>> > the assembly is not provided (which is a kind-of best-effort recovery
>> > strategy).
>> >
>> > Current flow:
>> >
>> > if (!SPARK_HOME) fail("No SPARK_HOME")
>> > else if (assembly) { use assembly) }
>> > else { try use SPARK_HOME to build spark_executor }
>> >
>> > Should be:
>> > sparkExecutor =  if (assembly) {assembly}
>> >                  else if (SPARK_HOME) {try use SPARK_HOME to build
>> > spark_executor}
>> >                  else { fail("No executor found. Please provide
>> > spark.executor.uri (preferred) or spark.home")
>> >
>> > What do you think?
>> >
>> > -kr, Gerard.
>> >
>> >
>> > [1]
>> >
>> >
>> http://apache-spark-user-list.1001560.n3.nabble.com/ClassNotFoundException-with-Spark-Mesos-spark-shell-works-fine-td6165.html
>> >
>> > [2]
>> >
>> >
>> https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend.scala#L89
>> >
>>
>
>

--001a11c3a4a2d9b34f04fa021730--

From dev-return-7785-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 19:49:08 2014
Return-Path: <dev-return-7785-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1E58E11786
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 19:49:08 +0000 (UTC)
Received: (qmail 38175 invoked by uid 500); 22 May 2014 19:49:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38115 invoked by uid 500); 22 May 2014 19:49:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38107 invoked by uid 99); 22 May 2014 19:49:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 19:49:07 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ilikerps@gmail.com designates 209.85.216.175 as permitted sender)
Received: from [209.85.216.175] (HELO mail-qc0-f175.google.com) (209.85.216.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 19:49:04 +0000
Received: by mail-qc0-f175.google.com with SMTP id w7so6565465qcr.34
        for <dev@spark.apache.org>; Thu, 22 May 2014 12:48:43 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=5hBdgoKc4haEE7Cpg9wAAoxuFkd7CJk5Al8hKefahHI=;
        b=u5hIJfTeFtgM6dF6Pom6MLTfxWaEnoWPBCGhTviYE6f8ty0Lw1LL05lSsLm4M7/jeB
         Mjt2QeLIfldu9gbRQ9OBwfP1TuBVMyw/razP+u1vs0fR0cPXfNgHjQ7XhXVzvaPoMjiz
         rzYditLaYymzQOje1cfKM3nPP5zLhvkydUIdD7B49QQWiD+W7OeYVkJ3BQU5orN3HkUP
         V6gtlnxmV0DpA0H0E4vIfgG1TqBYsZPY48SmPMmWe59JWZt0vdQRUXoHcxhK46lKh/93
         zJiAZaPlOjO/Gqnpab4fjPFonw4oaVRcBqscOVETXexQZ/CgLq2TT1DhbAzUYzgu7Th6
         MbqQ==
X-Received: by 10.224.67.131 with SMTP id r3mr82673175qai.75.1400788123418;
 Thu, 22 May 2014 12:48:43 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.31.35 with HTTP; Thu, 22 May 2014 12:48:23 -0700 (PDT)
In-Reply-To: <CA+qbEUPLi_oVnYBeuak0fgxZadbjMO73rMEjx3ZzcHYL1_32jg@mail.gmail.com>
References: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>
 <CALuGr6YvTxM+YyXx-+p3XsfcXT2wWE3iJ0T43HqJzKfs6s2UYQ@mail.gmail.com>
 <CAAsvFPkk+TVQiAu+_XiDOOgqUr4tck8ccA=YQ5Aiv_SF7hn7Dg@mail.gmail.com>
 <537D2A07.4030702@oracle.com> <CA+qbEUNKVF2xb3SM-EXwiTom7+qX+ZTbeNS6nHVvfp3-_a=fNw@mail.gmail.com>
 <1400728729.27373.YahooMailNeo@web140101.mail.bf1.yahoo.com>
 <537DA829.9030000@oracle.com> <537E2AA4.8070303@oracle.com>
 <CAAOnQ7t9OkOj9mh==RYzHdOka_wwgVo50ZAgutfBO4b3-e58wg@mail.gmail.com> <CA+qbEUPLi_oVnYBeuak0fgxZadbjMO73rMEjx3ZzcHYL1_32jg@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Thu, 22 May 2014 12:48:23 -0700
Message-ID: <CANGvG8q+YLSUna4qAXGtoNDGNBmh1KfmE0cjH7s1CFPbmqcYsA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC10)
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c3cefc8085ab04fa0266f8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3cefc8085ab04fa0266f8
Content-Type: text/plain; charset=UTF-8

In Spark 0.9.0 and 0.9.1, we stopped using the FileSystem cache correctly,
and we just recently resumed using it in 1.0 (and in 0.9.2) when this issue
was fixed: https://issues.apache.org/jira/browse/SPARK-1676

Prior to this fix, each Spark task created and cached its own FileSystems
due to a bug in how the FS cache handles UGIs. The big problem that arose
was that these FileSystems were never closed, so they just kept piling up.
There were two solutions we considered, with the following effects: (1)
Share the FS cache among all tasks and (2) Each task effectively gets its
own FS cache, and closes all of its FSes after the task completes.

We chose solution (1) for 3 reasons:
 - It does not rely on the behavior of a bug in HDFS.
 - It is the most performant option.
 - It is most consistent with the semantics of the (albeit broken) FS cache.

Since this behavior was changed in 1.0, it could be considered a
regression. We should consider the exact behavior we want out of the FS
cache. For Spark's purposes, it seems fine to cache FileSystems across
tasks, as Spark does not close FileSystems. The issue that comes up is that
user code which uses FileSystem.get() but then closes the FileSystem can
screw up Spark processes which were using that FileSystem. The workaround
for users would be to use FileSystem.newInstance() if they want full
control over the lifecycle of their FileSystems.


On Thu, May 22, 2014 at 12:06 PM, Colin McCabe <cmccabe@alumni.cmu.edu>wrote:

> The FileSystem cache is something that has caused a lot of pain over the
> years.  Unfortunately we (in Hadoop core) can't change the way it works now
> because there are too many users depending on the current behavior.
>
> Basically, the idea is that when you request a FileSystem with certain
> options with FileSystem#get, you might get a reference to an FS object that
> already exists, from our FS cache cache singleton.  Unfortunately, this
> also means that someone else can change the working directory on you or
> close the FS underneath you.  The FS is basically shared mutable state, and
> you don't know whom you're sharing with.
>
> It might be better for Spark to call FileSystem#newInstance, which bypasses
> the FileSystem cache and always creates a new object.  If Spark can hang on
> to the FS for a while, it can get the benefits of caching without the
> downsides.  In HDFS, multiple FS instances can also share things like the
> socket cache between them.
>
> best,
> Colin
>
>
> On Thu, May 22, 2014 at 10:06 AM, Marcelo Vanzin <vanzin@cloudera.com
> >wrote:
>
> > Hi Kevin,
> >
> > On Thu, May 22, 2014 at 9:49 AM, Kevin Markey <kevin.markey@oracle.com>
> > wrote:
> > > The FS closed exception only effects the cleanup of the staging
> > directory,
> > > not the final success or failure.  I've not yet tested the effect of
> > > changing my application's initialization, use, or closing of
> FileSystem.
> >
> > Without going and reading more of the Spark code, if your app is
> > explicitly close()'ing the FileSystem instance, it may be causing the
> > exception. If Spark is caching the FileSystem instance, your app is
> > probably closing that same instance (which it got from the HDFS
> > library's internal cache).
> >
> > It would be nice if you could test that theory; it might be worth
> > knowing that's the case so that we can tell people not to do that.
> >
> > --
> > Marcelo
> >
>

--001a11c3cefc8085ab04fa0266f8--

From dev-return-7786-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 20:26:17 2014
Return-Path: <dev-return-7786-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5EA2A11890
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 20:26:17 +0000 (UTC)
Received: (qmail 2896 invoked by uid 500); 22 May 2014 20:26:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2836 invoked by uid 500); 22 May 2014 20:26:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2828 invoked by uid 99); 22 May 2014 20:26:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 20:26:17 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of kevin.markey@oracle.com designates 141.146.126.69 as permitted sender)
Received: from [141.146.126.69] (HELO aserp1040.oracle.com) (141.146.126.69)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 20:26:13 +0000
Received: from ucsinet22.oracle.com (ucsinet22.oracle.com [156.151.31.94])
	by aserp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2) with ESMTP id s4MKPmoS021662
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=OK)
	for <dev@spark.apache.org>; Thu, 22 May 2014 20:25:49 GMT
Received: from aserz7021.oracle.com (aserz7021.oracle.com [141.146.126.230])
	by ucsinet22.oracle.com (8.14.5+Sun/8.14.5) with ESMTP id s4MKPloi021588
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=NO)
	for <dev@spark.apache.org>; Thu, 22 May 2014 20:25:48 GMT
Received: from abhmp0010.oracle.com (abhmp0010.oracle.com [141.146.116.16])
	by aserz7021.oracle.com (8.14.4+Sun/8.14.4) with ESMTP id s4MKPl31012736
	for <dev@spark.apache.org>; Thu, 22 May 2014 20:25:47 GMT
Received: from [10.135.123.92] (/10.135.123.92)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Thu, 22 May 2014 13:25:47 -0700
Message-ID: <537E5D4A.1040607@oracle.com>
Date: Thu, 22 May 2014 14:25:46 -0600
From: Kevin Markey <kevin.markey@oracle.com>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20100101 Thunderbird/24.5.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC10)
References: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com> <CALuGr6YvTxM+YyXx-+p3XsfcXT2wWE3iJ0T43HqJzKfs6s2UYQ@mail.gmail.com> <CAAsvFPkk+TVQiAu+_XiDOOgqUr4tck8ccA=YQ5Aiv_SF7hn7Dg@mail.gmail.com> <537D2A07.4030702@oracle.com> <CA+qbEUNKVF2xb3SM-EXwiTom7+qX+ZTbeNS6nHVvfp3-_a=fNw@mail.gmail.com> <1400728729.27373.YahooMailNeo@web140101.mail.bf1.yahoo.com> <537DA829.9030000@oracle.com> <537E2AA4.8070303@oracle.com> <CAAOnQ7t9OkOj9mh==RYzHdOka_wwgVo50ZAgutfBO4b3-e58wg@mail.gmail.com> <CA+qbEUPLi_oVnYBeuak0fgxZadbjMO73rMEjx3ZzcHYL1_32jg@mail.gmail.com> <CANGvG8q+YLSUna4qAXGtoNDGNBmh1KfmE0cjH7s1CFPbmqcYsA@mail.gmail.com>
In-Reply-To: <CANGvG8q+YLSUna4qAXGtoNDGNBmh1KfmE0cjH7s1CFPbmqcYsA@mail.gmail.com>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed
Content-Transfer-Encoding: 7bit
X-Source-IP: ucsinet22.oracle.com [156.151.31.94]
X-Virus-Checked: Checked by ClamAV on apache.org

Thank you, all!  This is quite helpful.

We have been arguing how to handle this issue across a growing 
application.  Unfortunately the Hadoop FileSystem java doc should say 
all this but doesn't!

Kevin

On 05/22/2014 01:48 PM, Aaron Davidson wrote:
> In Spark 0.9.0 and 0.9.1, we stopped using the FileSystem cache correctly,
> and we just recently resumed using it in 1.0 (and in 0.9.2) when this issue
> was fixed: https://issues.apache.org/jira/browse/SPARK-1676
>
> Prior to this fix, each Spark task created and cached its own FileSystems
> due to a bug in how the FS cache handles UGIs. The big problem that arose
> was that these FileSystems were never closed, so they just kept piling up.
> There were two solutions we considered, with the following effects: (1)
> Share the FS cache among all tasks and (2) Each task effectively gets its
> own FS cache, and closes all of its FSes after the task completes.
>
> We chose solution (1) for 3 reasons:
>   - It does not rely on the behavior of a bug in HDFS.
>   - It is the most performant option.
>   - It is most consistent with the semantics of the (albeit broken) FS cache.
>
> Since this behavior was changed in 1.0, it could be considered a
> regression. We should consider the exact behavior we want out of the FS
> cache. For Spark's purposes, it seems fine to cache FileSystems across
> tasks, as Spark does not close FileSystems. The issue that comes up is that
> user code which uses FileSystem.get() but then closes the FileSystem can
> screw up Spark processes which were using that FileSystem. The workaround
> for users would be to use FileSystem.newInstance() if they want full
> control over the lifecycle of their FileSystems.
>
>
> On Thu, May 22, 2014 at 12:06 PM, Colin McCabe <cmccabe@alumni.cmu.edu>wrote:
>
>> The FileSystem cache is something that has caused a lot of pain over the
>> years.  Unfortunately we (in Hadoop core) can't change the way it works now
>> because there are too many users depending on the current behavior.
>>
>> Basically, the idea is that when you request a FileSystem with certain
>> options with FileSystem#get, you might get a reference to an FS object that
>> already exists, from our FS cache cache singleton.  Unfortunately, this
>> also means that someone else can change the working directory on you or
>> close the FS underneath you.  The FS is basically shared mutable state, and
>> you don't know whom you're sharing with.
>>
>> It might be better for Spark to call FileSystem#newInstance, which bypasses
>> the FileSystem cache and always creates a new object.  If Spark can hang on
>> to the FS for a while, it can get the benefits of caching without the
>> downsides.  In HDFS, multiple FS instances can also share things like the
>> socket cache between them.
>>
>> best,
>> Colin
>>
>>
>> On Thu, May 22, 2014 at 10:06 AM, Marcelo Vanzin <vanzin@cloudera.com
>>> wrote:
>>> Hi Kevin,
>>>
>>> On Thu, May 22, 2014 at 9:49 AM, Kevin Markey <kevin.markey@oracle.com>
>>> wrote:
>>>> The FS closed exception only effects the cleanup of the staging
>>> directory,
>>>> not the final success or failure.  I've not yet tested the effect of
>>>> changing my application's initialization, use, or closing of
>> FileSystem.
>>> Without going and reading more of the Spark code, if your app is
>>> explicitly close()'ing the FileSystem instance, it may be causing the
>>> exception. If Spark is caching the FileSystem instance, your app is
>>> probably closing that same instance (which it got from the HDFS
>>> library's internal cache).
>>>
>>> It would be nice if you could test that theory; it might be worth
>>> knowing that's the case so that we can tell people not to do that.
>>>
>>> --
>>> Marcelo
>>>


From dev-return-7787-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 21:07:31 2014
Return-Path: <dev-return-7787-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3B082119E9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 21:07:31 +0000 (UTC)
Received: (qmail 95965 invoked by uid 500); 22 May 2014 21:06:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95927 invoked by uid 500); 22 May 2014 21:06:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95917 invoked by uid 99); 22 May 2014 21:06:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 21:06:53 +0000
X-ASF-Spam-Status: No, hits=-0.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.220.179 as permitted sender)
Received: from [209.85.220.179] (HELO mail-vc0-f179.google.com) (209.85.220.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 21:06:49 +0000
Received: by mail-vc0-f179.google.com with SMTP id im17so5196292vcb.10
        for <dev@spark.apache.org>; Thu, 22 May 2014 14:06:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=WyP2y7DAf01EOXIrY89n80o3Z7ZYcPIh9JZvUVPdY40=;
        b=Dr87GZ482cgpDLyCgSxUuItAaXlr32r7aJsMZHDC1g8bw8MKQkG54nbaReoFJC1Ffc
         dm+MnmUQa5v7WymLbdm+S/O+IAKye3RDsdzZLcJud/YvsyGjKDs8P/XoV9S6S/ImNT6w
         6j58q320djdvdCgPlJ8th0YwkBHTJkWe+04rPKTV9pBP/OarMVtLObP48qiydp/hGHuv
         sAhtiXL0sDnm9KFCl3+lW9giUhuSEaiiOTcxKWKv67DUoqK7mMoWsVJu/Ng3VTCNOT8Z
         +lImNp0qydPSNkcRFPS26ApTOX12o6N31WOxT5MuWrE7NXeBA4vDlWi3OQeU5Fec/vW0
         JtsA==
X-Received: by 10.52.72.4 with SMTP id z4mr67872vdu.71.1400792789088; Thu, 22
 May 2014 14:06:29 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.169.131 with HTTP; Thu, 22 May 2014 14:05:59 -0700 (PDT)
In-Reply-To: <537E5D4A.1040607@oracle.com>
References: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>
 <CALuGr6YvTxM+YyXx-+p3XsfcXT2wWE3iJ0T43HqJzKfs6s2UYQ@mail.gmail.com>
 <CAAsvFPkk+TVQiAu+_XiDOOgqUr4tck8ccA=YQ5Aiv_SF7hn7Dg@mail.gmail.com>
 <537D2A07.4030702@oracle.com> <CA+qbEUNKVF2xb3SM-EXwiTom7+qX+ZTbeNS6nHVvfp3-_a=fNw@mail.gmail.com>
 <1400728729.27373.YahooMailNeo@web140101.mail.bf1.yahoo.com>
 <537DA829.9030000@oracle.com> <537E2AA4.8070303@oracle.com>
 <CAAOnQ7t9OkOj9mh==RYzHdOka_wwgVo50ZAgutfBO4b3-e58wg@mail.gmail.com>
 <CA+qbEUPLi_oVnYBeuak0fgxZadbjMO73rMEjx3ZzcHYL1_32jg@mail.gmail.com>
 <CANGvG8q+YLSUna4qAXGtoNDGNBmh1KfmE0cjH7s1CFPbmqcYsA@mail.gmail.com> <537E5D4A.1040607@oracle.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Thu, 22 May 2014 14:05:59 -0700
Message-ID: <CAMwrk0nK7G7e38Ccbk+3Q5ihhG_xCF5MTbnM5J1y7QgeeWfj_Q@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC10)
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hey all,

On further testing, I came across a bug that breaks execution of
pyspark scripts on YARN.
https://issues.apache.org/jira/browse/SPARK-1900
This is a blocker and worth cutting a new RC.

We also found a fix for a known issue that prevents additional jar
files to be specified through spark-submit on YARN.
https://issues.apache.org/jira/browse/SPARK-1870
The has been fixed and will be in the next RC.

We are canceling this vote for now. We will post RC11 shortly. Thanks
everyone for testing!

TD

On Thu, May 22, 2014 at 1:25 PM, Kevin Markey <kevin.markey@oracle.com> wrote:
> Thank you, all!  This is quite helpful.
>
> We have been arguing how to handle this issue across a growing application.
> Unfortunately the Hadoop FileSystem java doc should say all this but
> doesn't!
>
> Kevin
>
>
> On 05/22/2014 01:48 PM, Aaron Davidson wrote:
>>
>> In Spark 0.9.0 and 0.9.1, we stopped using the FileSystem cache correctly,
>> and we just recently resumed using it in 1.0 (and in 0.9.2) when this
>> issue
>> was fixed: https://issues.apache.org/jira/browse/SPARK-1676
>>
>> Prior to this fix, each Spark task created and cached its own FileSystems
>> due to a bug in how the FS cache handles UGIs. The big problem that arose
>> was that these FileSystems were never closed, so they just kept piling up.
>> There were two solutions we considered, with the following effects: (1)
>> Share the FS cache among all tasks and (2) Each task effectively gets its
>> own FS cache, and closes all of its FSes after the task completes.
>>
>> We chose solution (1) for 3 reasons:
>>   - It does not rely on the behavior of a bug in HDFS.
>>   - It is the most performant option.
>>   - It is most consistent with the semantics of the (albeit broken) FS
>> cache.
>>
>> Since this behavior was changed in 1.0, it could be considered a
>> regression. We should consider the exact behavior we want out of the FS
>> cache. For Spark's purposes, it seems fine to cache FileSystems across
>> tasks, as Spark does not close FileSystems. The issue that comes up is
>> that
>> user code which uses FileSystem.get() but then closes the FileSystem can
>> screw up Spark processes which were using that FileSystem. The workaround
>> for users would be to use FileSystem.newInstance() if they want full
>> control over the lifecycle of their FileSystems.
>>
>>
>> On Thu, May 22, 2014 at 12:06 PM, Colin McCabe
>> <cmccabe@alumni.cmu.edu>wrote:
>>
>>> The FileSystem cache is something that has caused a lot of pain over the
>>> years.  Unfortunately we (in Hadoop core) can't change the way it works
>>> now
>>> because there are too many users depending on the current behavior.
>>>
>>> Basically, the idea is that when you request a FileSystem with certain
>>> options with FileSystem#get, you might get a reference to an FS object
>>> that
>>> already exists, from our FS cache cache singleton.  Unfortunately, this
>>> also means that someone else can change the working directory on you or
>>> close the FS underneath you.  The FS is basically shared mutable state,
>>> and
>>> you don't know whom you're sharing with.
>>>
>>> It might be better for Spark to call FileSystem#newInstance, which
>>> bypasses
>>> the FileSystem cache and always creates a new object.  If Spark can hang
>>> on
>>> to the FS for a while, it can get the benefits of caching without the
>>> downsides.  In HDFS, multiple FS instances can also share things like the
>>> socket cache between them.
>>>
>>> best,
>>> Colin
>>>
>>>
>>> On Thu, May 22, 2014 at 10:06 AM, Marcelo Vanzin <vanzin@cloudera.com
>>>>
>>>> wrote:
>>>> Hi Kevin,
>>>>
>>>> On Thu, May 22, 2014 at 9:49 AM, Kevin Markey <kevin.markey@oracle.com>
>>>> wrote:
>>>>>
>>>>> The FS closed exception only effects the cleanup of the staging
>>>>
>>>> directory,
>>>>>
>>>>> not the final success or failure.  I've not yet tested the effect of
>>>>> changing my application's initialization, use, or closing of
>>>
>>> FileSystem.
>>>>
>>>> Without going and reading more of the Spark code, if your app is
>>>> explicitly close()'ing the FileSystem instance, it may be causing the
>>>> exception. If Spark is caching the FileSystem instance, your app is
>>>> probably closing that same instance (which it got from the HDFS
>>>> library's internal cache).
>>>>
>>>> It would be nice if you could test that theory; it might be worth
>>>> knowing that's the case so that we can tell people not to do that.
>>>>
>>>> --
>>>> Marcelo
>>>>
>

From dev-return-7788-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 22:05:25 2014
Return-Path: <dev-return-7788-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 92A9811C2C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 22:05:25 +0000 (UTC)
Received: (qmail 27541 invoked by uid 500); 22 May 2014 22:05:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27482 invoked by uid 500); 22 May 2014 22:05:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27472 invoked by uid 99); 22 May 2014 22:05:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 22:05:25 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rarecactus@gmail.com designates 74.125.82.45 as permitted sender)
Received: from [74.125.82.45] (HELO mail-wg0-f45.google.com) (74.125.82.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 22:05:22 +0000
Received: by mail-wg0-f45.google.com with SMTP id m15so3941203wgh.28
        for <dev@spark.apache.org>; Thu, 22 May 2014 15:04:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=q4AG+deUBXvTWu8p3ebtD7lGu/mW/bcmzw5kyaMvHk8=;
        b=t6XfN29GOEic5xFbmttmwE4wVoR8AnlcJe6q7YaM0Q5EG34WPC+rrEG5DN+f752nPK
         XljGlR8KOSPHPEVo+2YrCM1jhVFsdUIDd/o/4TYvX1A3DQNErUITdirBPBYcTkmwfuIf
         16e6U9xlCungnEiMDIB5ASRKcKoQh4jIuKEX+NPfT0kv1vX8Ao0x/pRPRLvfUJJeiNYi
         6FtfXo3hHzMxKjf5r/YJgrclu760NrSD9iD5tdiO5y7mM0hDraicGuoYe6nbBsTEweFX
         1DC6dsqqm4q3XxpjQeo2SF9hgnrzeyl0ktTpVp5671M2fkhpxv4/KVZxYjQztNXr4QvI
         OMvQ==
MIME-Version: 1.0
X-Received: by 10.180.91.162 with SMTP id cf2mr971082wib.57.1400796298891;
 Thu, 22 May 2014 15:04:58 -0700 (PDT)
Sender: rarecactus@gmail.com
Received: by 10.194.122.135 with HTTP; Thu, 22 May 2014 15:04:58 -0700 (PDT)
In-Reply-To: <CANGvG8q+YLSUna4qAXGtoNDGNBmh1KfmE0cjH7s1CFPbmqcYsA@mail.gmail.com>
References: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>
	<CALuGr6YvTxM+YyXx-+p3XsfcXT2wWE3iJ0T43HqJzKfs6s2UYQ@mail.gmail.com>
	<CAAsvFPkk+TVQiAu+_XiDOOgqUr4tck8ccA=YQ5Aiv_SF7hn7Dg@mail.gmail.com>
	<537D2A07.4030702@oracle.com>
	<CA+qbEUNKVF2xb3SM-EXwiTom7+qX+ZTbeNS6nHVvfp3-_a=fNw@mail.gmail.com>
	<1400728729.27373.YahooMailNeo@web140101.mail.bf1.yahoo.com>
	<537DA829.9030000@oracle.com>
	<537E2AA4.8070303@oracle.com>
	<CAAOnQ7t9OkOj9mh==RYzHdOka_wwgVo50ZAgutfBO4b3-e58wg@mail.gmail.com>
	<CA+qbEUPLi_oVnYBeuak0fgxZadbjMO73rMEjx3ZzcHYL1_32jg@mail.gmail.com>
	<CANGvG8q+YLSUna4qAXGtoNDGNBmh1KfmE0cjH7s1CFPbmqcYsA@mail.gmail.com>
Date: Thu, 22 May 2014 15:04:58 -0700
X-Google-Sender-Auth: esOUg_iF_lpvrIMd9-c_mb8q_rw
Message-ID: <CA+qbEUMs55pjo+HhdvqA0DgWx=t=2zt707q8L28q4eJpT+vLKA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC10)
From: Colin McCabe <cmccabe@alumni.cmu.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d0438933bcc566e04fa044dde
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0438933bcc566e04fa044dde
Content-Type: text/plain; charset=UTF-8

On Thu, May 22, 2014 at 12:48 PM, Aaron Davidson <ilikerps@gmail.com> wrote:

> In Spark 0.9.0 and 0.9.1, we stopped using the FileSystem cache correctly,
> and we just recently resumed using it in 1.0 (and in 0.9.2) when this issue
> was fixed: https://issues.apache.org/jira/browse/SPARK-1676
>

Interesting...


> Prior to this fix, each Spark task created and cached its own FileSystems
> due to a bug in how the FS cache handles UGIs. The big problem that arose
> was that these FileSystems were never closed, so they just kept piling up.
> There were two solutions we considered, with the following effects: (1)
> Share the FS cache among all tasks and (2) Each task effectively gets its
> own FS cache, and closes all of its FSes after the task completes.
>

Since the FS cache is in hadoop-common-project, it's not so much a bug in
HDFS as a bug in Hadoop.  So even if you're using, say, Lustre, you'll
still get the same issues with org.apache.hadoop.fs.FileSystem and its
global cache.

We chose solution (1) for 3 reasons:
>  - It does not rely on the behavior of a bug in HDFS.

 - It is the most performant option.
>  - It is most consistent with the semantics of the (albeit broken) FS
> cache.
>
> Since this behavior was changed in 1.0, it could be considered a
> regression. We should consider the exact behavior we want out of the FS
> cache. For Spark's purposes, it seems fine to cache FileSystems across
> tasks, as Spark does not close FileSystems. The issue that comes up is that
> user code which uses FileSystem.get() but then closes the FileSystem can
> screw up Spark processes which were using that FileSystem. The workaround
> for users would be to use FileSystem.newInstance() if they want full
> control over the lifecycle of their FileSystems.
>

The current solution seems reasonable, as long as Spark processes:
1. don't change the current working directory (doing so isn't thread-safe
and will affect all other users of that FS object)
2. don't close the FileSystem object

Another solution would be to use newInstance and build your own FS cache,
essentially.  I don't think it would be that much code.  This might be
nicer because you could implement things like closing FileSystem objects
that haven't been used in a while.

cheers,
Colin



> On Thu, May 22, 2014 at 12:06 PM, Colin McCabe <cmccabe@alumni.cmu.edu
> >wrote:
>
> > The FileSystem cache is something that has caused a lot of pain over the
> > years.  Unfortunately we (in Hadoop core) can't change the way it works
> now
> > because there are too many users depending on the current behavior.
> >
> > Basically, the idea is that when you request a FileSystem with certain
> > options with FileSystem#get, you might get a reference to an FS object
> that
> > already exists, from our FS cache cache singleton.  Unfortunately, this
> > also means that someone else can change the working directory on you or
> > close the FS underneath you.  The FS is basically shared mutable state,
> and
> > you don't know whom you're sharing with.
> >
> > It might be better for Spark to call FileSystem#newInstance, which
> bypasses
> > the FileSystem cache and always creates a new object.  If Spark can hang
> on
> > to the FS for a while, it can get the benefits of caching without the
> > downsides.  In HDFS, multiple FS instances can also share things like the
> > socket cache between them.
> >
> > best,
> > Colin
> >
> >
> > On Thu, May 22, 2014 at 10:06 AM, Marcelo Vanzin <vanzin@cloudera.com
> > >wrote:
> >
> > > Hi Kevin,
> > >
> > > On Thu, May 22, 2014 at 9:49 AM, Kevin Markey <kevin.markey@oracle.com
> >
> > > wrote:
> > > > The FS closed exception only effects the cleanup of the staging
> > > directory,
> > > > not the final success or failure.  I've not yet tested the effect of
> > > > changing my application's initialization, use, or closing of
> > FileSystem.
> > >
> > > Without going and reading more of the Spark code, if your app is
> > > explicitly close()'ing the FileSystem instance, it may be causing the
> > > exception. If Spark is caching the FileSystem instance, your app is
> > > probably closing that same instance (which it got from the HDFS
> > > library's internal cache).
> > >
> > > It would be nice if you could test that theory; it might be worth
> > > knowing that's the case so that we can tell people not to do that.
> > >
> > > --
> > > Marcelo
> > >
> >
>

--f46d0438933bcc566e04fa044dde--

From dev-return-7789-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 22:07:35 2014
Return-Path: <dev-return-7789-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CC8F711C46
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 22:07:35 +0000 (UTC)
Received: (qmail 31229 invoked by uid 500); 22 May 2014 22:07:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31163 invoked by uid 500); 22 May 2014 22:07:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31155 invoked by uid 99); 22 May 2014 22:07:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 22:07:35 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of henry.saputra@gmail.com designates 209.85.212.178 as permitted sender)
Received: from [209.85.212.178] (HELO mail-wi0-f178.google.com) (209.85.212.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 22:07:30 +0000
Received: by mail-wi0-f178.google.com with SMTP id cc10so5058437wib.11
        for <dev@spark.apache.org>; Thu, 22 May 2014 15:07:09 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=pNMa7M0KgBLXIKX9X8W3kxujFmk8TqUn/q16yC//4NM=;
        b=jwHoiuhEA9iVDviZUWrJWPZ1hyPVyg4eMjR6pNaZRTev8Zi8tGQUCxKcQi2QUC6FKO
         QT3TPDfBsMKomOgAcJLaC87lFYVLg6xXFOW25SSjgkEijgAVjmIUSmo3z5TyWjnuMyFz
         rYPVmu3/gti5UXd61Utt+iL+mfaJUDg9Xv8ZC+tjuAHz0ui/fhf4naVUC9hj4DUMFVBY
         EwrMKW7DdwO4/gL2XKIKw0OhVCgQe7RefoadWWJH9qUpcw4mHcFl+qiC+gx0niQVroIQ
         X8XJ2fQa1vZkDWsGml104OZ6UvzKi4dJLlG1O09/6f1823xb6orL1YttqEpstHwmurjZ
         Up6A==
MIME-Version: 1.0
X-Received: by 10.194.6.166 with SMTP id c6mr443185wja.64.1400796428999; Thu,
 22 May 2014 15:07:08 -0700 (PDT)
Received: by 10.216.165.71 with HTTP; Thu, 22 May 2014 15:07:08 -0700 (PDT)
In-Reply-To: <CAMwrk0nK7G7e38Ccbk+3Q5ihhG_xCF5MTbnM5J1y7QgeeWfj_Q@mail.gmail.com>
References: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>
	<CALuGr6YvTxM+YyXx-+p3XsfcXT2wWE3iJ0T43HqJzKfs6s2UYQ@mail.gmail.com>
	<CAAsvFPkk+TVQiAu+_XiDOOgqUr4tck8ccA=YQ5Aiv_SF7hn7Dg@mail.gmail.com>
	<537D2A07.4030702@oracle.com>
	<CA+qbEUNKVF2xb3SM-EXwiTom7+qX+ZTbeNS6nHVvfp3-_a=fNw@mail.gmail.com>
	<1400728729.27373.YahooMailNeo@web140101.mail.bf1.yahoo.com>
	<537DA829.9030000@oracle.com>
	<537E2AA4.8070303@oracle.com>
	<CAAOnQ7t9OkOj9mh==RYzHdOka_wwgVo50ZAgutfBO4b3-e58wg@mail.gmail.com>
	<CA+qbEUPLi_oVnYBeuak0fgxZadbjMO73rMEjx3ZzcHYL1_32jg@mail.gmail.com>
	<CANGvG8q+YLSUna4qAXGtoNDGNBmh1KfmE0cjH7s1CFPbmqcYsA@mail.gmail.com>
	<537E5D4A.1040607@oracle.com>
	<CAMwrk0nK7G7e38Ccbk+3Q5ihhG_xCF5MTbnM5J1y7QgeeWfj_Q@mail.gmail.com>
Date: Thu, 22 May 2014 15:07:08 -0700
Message-ID: <CALuGr6b=nsXGO+YFgJ=SNXizU=Tw8g-GchEs6WY1UB73CREo9g@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC10)
From: Henry Saputra <henry.saputra@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Looks like SPARK-1900 is a blocker for YARN and might as well add
SPARK-1870 while at it.

TD or Patrick, could you kindly send [CANCEL] prefixed in the subject
email out for the RC10 Vote to help people follow the active VOTE
threads? The VOTE emails are getting a bit hard to follow.


- Henry


On Thu, May 22, 2014 at 2:05 PM, Tathagata Das
<tathagata.das1565@gmail.com> wrote:
> Hey all,
>
> On further testing, I came across a bug that breaks execution of
> pyspark scripts on YARN.
> https://issues.apache.org/jira/browse/SPARK-1900
> This is a blocker and worth cutting a new RC.
>
> We also found a fix for a known issue that prevents additional jar
> files to be specified through spark-submit on YARN.
> https://issues.apache.org/jira/browse/SPARK-1870
> The has been fixed and will be in the next RC.
>
> We are canceling this vote for now. We will post RC11 shortly. Thanks
> everyone for testing!
>
> TD
>

From dev-return-7790-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 22:26:55 2014
Return-Path: <dev-return-7790-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A960411CD3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 22:26:55 +0000 (UTC)
Received: (qmail 50705 invoked by uid 500); 22 May 2014 22:26:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50638 invoked by uid 500); 22 May 2014 22:26:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50630 invoked by uid 99); 22 May 2014 22:26:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 22:26:55 +0000
X-ASF-Spam-Status: No, hits=-0.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.220.178 as permitted sender)
Received: from [209.85.220.178] (HELO mail-vc0-f178.google.com) (209.85.220.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 22:26:50 +0000
Received: by mail-vc0-f178.google.com with SMTP id ij19so1897229vcb.37
        for <dev@spark.apache.org>; Thu, 22 May 2014 15:26:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=yKFN6miDZnnJHH2zIIdeLyqdUv3eirOdqOSNNHTYyoQ=;
        b=erAijAyAkSzc9ay2cFP0fam0UuAY96PhuoJTiVggqnDG75W5v1biLMiGXOXBx6dd/Y
         hMNsAFmtUxMVJNXkkl9s4gvL2HtkcJtu3TAsoc0ZSkqexIHwl4J5e2sxcIXwlar0T6iW
         pTtttNUD3BHa/Q37Sin81PvHnt8OLzXOlAC+4TCqpk6hCVHs1rQ5veEG57fQpLtGMhCD
         ws8aKFQHGbSP5VlYfHmfFAubANHywey5hyCAiViiyIoVu5bWdGXbzNh0bxzkU3Xzzb7p
         2HHQKIDN4R6Y4FOH7xVmEuQWy+Y+yptKd7ug5l1MZzibi/QfdxQEho9w1M8lo+gWD5pT
         8sbw==
X-Received: by 10.58.136.168 with SMTP id qb8mr452043veb.21.1400797589338;
 Thu, 22 May 2014 15:26:29 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.169.131 with HTTP; Thu, 22 May 2014 15:25:59 -0700 (PDT)
In-Reply-To: <CALuGr6b=nsXGO+YFgJ=SNXizU=Tw8g-GchEs6WY1UB73CREo9g@mail.gmail.com>
References: <CAMwrk0=eOfLLdXYVArE4HKmFkwN__3gnYMTtK-aDAiLiGiyA3A@mail.gmail.com>
 <CALuGr6YvTxM+YyXx-+p3XsfcXT2wWE3iJ0T43HqJzKfs6s2UYQ@mail.gmail.com>
 <CAAsvFPkk+TVQiAu+_XiDOOgqUr4tck8ccA=YQ5Aiv_SF7hn7Dg@mail.gmail.com>
 <537D2A07.4030702@oracle.com> <CA+qbEUNKVF2xb3SM-EXwiTom7+qX+ZTbeNS6nHVvfp3-_a=fNw@mail.gmail.com>
 <1400728729.27373.YahooMailNeo@web140101.mail.bf1.yahoo.com>
 <537DA829.9030000@oracle.com> <537E2AA4.8070303@oracle.com>
 <CAAOnQ7t9OkOj9mh==RYzHdOka_wwgVo50ZAgutfBO4b3-e58wg@mail.gmail.com>
 <CA+qbEUPLi_oVnYBeuak0fgxZadbjMO73rMEjx3ZzcHYL1_32jg@mail.gmail.com>
 <CANGvG8q+YLSUna4qAXGtoNDGNBmh1KfmE0cjH7s1CFPbmqcYsA@mail.gmail.com>
 <537E5D4A.1040607@oracle.com> <CAMwrk0nK7G7e38Ccbk+3Q5ihhG_xCF5MTbnM5J1y7QgeeWfj_Q@mail.gmail.com>
 <CALuGr6b=nsXGO+YFgJ=SNXizU=Tw8g-GchEs6WY1UB73CREo9g@mail.gmail.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Thu, 22 May 2014 15:25:59 -0700
Message-ID: <CAMwrk0nFn39b08EAVQZB42CxgG6foViGs4k43hVamz66c5t9fA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC10)
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Right! Doing that.

TD

On Thu, May 22, 2014 at 3:07 PM, Henry Saputra <henry.saputra@gmail.com> wrote:
> Looks like SPARK-1900 is a blocker for YARN and might as well add
> SPARK-1870 while at it.
>
> TD or Patrick, could you kindly send [CANCEL] prefixed in the subject
> email out for the RC10 Vote to help people follow the active VOTE
> threads? The VOTE emails are getting a bit hard to follow.
>
>
> - Henry
>
>
> On Thu, May 22, 2014 at 2:05 PM, Tathagata Das
> <tathagata.das1565@gmail.com> wrote:
>> Hey all,
>>
>> On further testing, I came across a bug that breaks execution of
>> pyspark scripts on YARN.
>> https://issues.apache.org/jira/browse/SPARK-1900
>> This is a blocker and worth cutting a new RC.
>>
>> We also found a fix for a known issue that prevents additional jar
>> files to be specified through spark-submit on YARN.
>> https://issues.apache.org/jira/browse/SPARK-1870
>> The has been fixed and will be in the next RC.
>>
>> We are canceling this vote for now. We will post RC11 shortly. Thanks
>> everyone for testing!
>>
>> TD
>>

From dev-return-7791-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 22:48:47 2014
Return-Path: <dev-return-7791-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6682011DD3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 22:48:47 +0000 (UTC)
Received: (qmail 93659 invoked by uid 500); 22 May 2014 22:48:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93596 invoked by uid 500); 22 May 2014 22:48:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93587 invoked by uid 99); 22 May 2014 22:48:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 22:48:46 +0000
X-ASF-Spam-Status: No, hits=-0.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.128.169 as permitted sender)
Received: from [209.85.128.169] (HELO mail-ve0-f169.google.com) (209.85.128.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 22:48:41 +0000
Received: by mail-ve0-f169.google.com with SMTP id jx11so5298694veb.14
        for <dev@spark.apache.org>; Thu, 22 May 2014 15:48:20 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=k24Cq2+mpAmSENnvj25h8RgovmWCWA5c2vqsO79a5Og=;
        b=SeKv4McMlMvUEMCPoB7LvOBNhursGquIHKinnUb86xbkJk36tZP3/C3dAoSTtC1mfD
         nZChlvKphO5V8fiWmPSablbY6lkNitl5bo5m2p+UiYTntih4BGgCWfSrfSc8xDvABN+s
         3auzLDDOQYI3+/gfB6qFwBGzJz16ZllirrkMVOBsjqNRvK5DkQK20v5fiTv13191cGx3
         zxZaP0Wt2ijp975tcAn2AWzIjN5meRUDNb2Qbv+vJ9MIxBtmAx0PMLtoyy6snsR1gg7C
         3KOptKTU6JhAcwHmXi5XSlLMzRc1YXPkAaaj2j0rr8K7OMg8/D5Jdzasct8WVUCwdm4l
         IriA==
X-Received: by 10.52.134.202 with SMTP id pm10mr414472vdb.55.1400798900777;
 Thu, 22 May 2014 15:48:20 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.118.9 with HTTP; Thu, 22 May 2014 15:47:50 -0700 (PDT)
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Thu, 22 May 2014 15:47:50 -0700
Message-ID: <CAMwrk0npjTJcJ22npvfYLjCKN3k1_=d8sEvBhqMJxkM00_mkkA@mail.gmail.com>
Subject: [CANCEL][VOTE] Release Apache Spark 1.0.0 (RC10)
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hey all,

We are canceling the vote on RC10 because of a blocker bug in pyspark on Yarn.
https://issues.apache.org/jira/browse/SPARK-1900

Thanks everyone for testing! We will post RC11 soon.

TD

From dev-return-7792-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 22 23:00:46 2014
Return-Path: <dev-return-7792-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4A40711E47
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 22 May 2014 23:00:46 +0000 (UTC)
Received: (qmail 19059 invoked by uid 500); 22 May 2014 23:00:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18997 invoked by uid 500); 22 May 2014 23:00:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18989 invoked by uid 99); 22 May 2014 23:00:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 23:00:45 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of gerard.maas@gmail.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 22 May 2014 23:00:42 +0000
Received: by mail-wi0-f175.google.com with SMTP id f8so10041626wiw.14
        for <dev@spark.apache.org>; Thu, 22 May 2014 16:00:20 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=duSuqrFqnkF3iEjSdxFxZ0kS0s1xXDRFDKSofkCZZGs=;
        b=dkNPYXiCtbHPNTTlWQUi553YiyF1KVCoFC8gfzNkKUTxyjmpu8R/jntFpd1J9qzozs
         9wQQmqmmyEW1vKaHwJ2zbWuiskUXK7sYiBKnMk93Bgf0xBwND17r3JItJgj7wgBWcGHC
         WTBnlDqzhI2UdEjpiASDUdEa4NldrJSAnVQ8fH1d4P+UbQYAP0JM8St7zvwwHxy4WzDO
         CzZsZuiOsHHg9ehFIry3rZIzkbtHBK01uVBdrHSRYy4IXXoy2HqNtOz9ddxBCCTZThP5
         DC6XMe744DNZ62AqpUefhIaANhJLucovaezDicw8yFvtiKPFVj/pCQY7JIyoCfH8syD/
         oMCg==
X-Received: by 10.180.182.19 with SMTP id ea19mr1364862wic.14.1400799620897;
 Thu, 22 May 2014 16:00:20 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.173.2 with HTTP; Thu, 22 May 2014 15:59:50 -0700 (PDT)
In-Reply-To: <CA+-p3AE6LfPSqzDBW0x7dPkzR0K955vfeGkonnnUrrGx29tYAw@mail.gmail.com>
References: <CAMc-71kjOHyVcL+4GZW=ch8+ZxvkDcXWnW07Uvaa4ePX7n79=A@mail.gmail.com>
 <CA+-p3AGerFkP_9bsVDw4tu9QUhNH_--aEiWL4NqB+r3xWsyQjA@mail.gmail.com>
 <CAMc-71nLz_EpxkJzkzQPVyBP-b2z8Gofehvex5DNc8kamoLmpA@mail.gmail.com> <CA+-p3AE6LfPSqzDBW0x7dPkzR0K955vfeGkonnnUrrGx29tYAw@mail.gmail.com>
From: Gerard Maas <gerard.maas@gmail.com>
Date: Fri, 23 May 2014 00:59:50 +0200
Message-ID: <CAMc-71n8Q_7SBynaOyktZvSOa5Mogp0Xgm=QmTGeuMbWH9yG6w@mail.gmail.com>
Subject: Re: Should SPARK_HOME be needed with Mesos?
To: Andrew Ash <andrew@andrewash.com>
Cc: dev@spark.apache.org, Patrick Wendell <pwendell@gmail.com>
Content-Type: multipart/alternative; boundary=047d7b6225ccce22ad04fa051333
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6225ccce22ad04fa051333
Content-Type: text/plain; charset=UTF-8

ack


On Thu, May 22, 2014 at 9:26 PM, Andrew Ash <andrew@andrewash.com> wrote:

> Fixing the immediate issue of requiring SPARK_HOME to be set when it's not
> actually used is a separate ticket in my mind from a larger cleanup of what
> SPARK_HOME means across the cluster.
>
> I think you should file a new ticket for just this particular issue.
>
>
> On Thu, May 22, 2014 at 11:03 AM, Gerard Maas <gerard.maas@gmail.com>wrote:
>
>> Sure.  Should I create a Jira as well?
>>
>> I saw there's already a broader ticket regarding the ambiguous use of
>> SPARK_HOME [1]  (cc: Patrick as owner of that ticket)
>>
>> I don't know if it would be more relevant to remove the use of SPARK_HOME
>> when using mesos and have the assembly as the only way forward, or whether
>> that's a too radical change that might break some existing systems.
>>
>> From a real-world ops perspective, the assembly should be the way to go.
>> I don't see installing and configuring Spark distros on a mesos master as a
>> way to have the mesos executor in place.
>>
>> -kr, Gerard.
>>
>> [1] https://issues.apache.org/jira/browse/SPARK-1110
>>
>>
>> On Thu, May 22, 2014 at 6:19 AM, Andrew Ash <andrew@andrewash.com> wrote:
>>
>>> Hi Gerard,
>>>
>>> I agree that your second option seems preferred.  You shouldn't have to
>>> specify a SPARK_HOME if the executor is going to use the
>>> spark.executor.uri
>>> instead.  Can you send in a pull request that includes your proposed
>>> changes?
>>>
>>> Andrew
>>>
>>>
>>> On Wed, May 21, 2014 at 10:19 AM, Gerard Maas <gerard.maas@gmail.com>
>>> wrote:
>>>
>>> > Spark dev's,
>>> >
>>> > I was looking into a question asked on the user list where a
>>> > ClassNotFoundException was thrown when running a job on Mesos. Curious
>>> > issue with serialization on Mesos: more details here [1]:
>>> >
>>> > When trying to run that simple example on my Mesos installation, I
>>> faced
>>> > another issue: I got an error that "SPARK_HOME" was not set. I found
>>> that
>>> > curious b/c a local spark installation should not be required to run a
>>> job
>>> > on Mesos. All that's needed is the executor package, being the
>>> > assembly.tar.gz on a reachable location (HDFS/S3/HTTP).
>>> >
>>> > I went looking into the code and indeed there's a check on SPARK_HOME
>>> [2]
>>> > regardless of the presence of the assembly but it's actually only used
>>> if
>>> > the assembly is not provided (which is a kind-of best-effort recovery
>>> > strategy).
>>> >
>>> > Current flow:
>>> >
>>> > if (!SPARK_HOME) fail("No SPARK_HOME")
>>> > else if (assembly) { use assembly) }
>>> > else { try use SPARK_HOME to build spark_executor }
>>> >
>>> > Should be:
>>> > sparkExecutor =  if (assembly) {assembly}
>>> >                  else if (SPARK_HOME) {try use SPARK_HOME to build
>>> > spark_executor}
>>> >                  else { fail("No executor found. Please provide
>>> > spark.executor.uri (preferred) or spark.home")
>>> >
>>> > What do you think?
>>> >
>>> > -kr, Gerard.
>>> >
>>> >
>>> > [1]
>>> >
>>> >
>>> http://apache-spark-user-list.1001560.n3.nabble.com/ClassNotFoundException-with-Spark-Mesos-spark-shell-works-fine-td6165.html
>>> >
>>> > [2]
>>> >
>>> >
>>> https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosSchedulerBackend.scala#L89
>>> >
>>>
>>
>>
>

--047d7b6225ccce22ad04fa051333--

From dev-return-7793-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 23 04:53:23 2014
Return-Path: <dev-return-7793-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 095DB10736
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 23 May 2014 04:53:23 +0000 (UTC)
Received: (qmail 18169 invoked by uid 500); 23 May 2014 04:53:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18075 invoked by uid 500); 23 May 2014 04:53:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18059 invoked by uid 99); 23 May 2014 04:53:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 May 2014 04:53:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of prabsmails@gmail.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 May 2014 04:53:18 +0000
Received: by mail-wi0-f175.google.com with SMTP id f8so233755wiw.14
        for <multiple recipients>; Thu, 22 May 2014 21:52:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=ATeRU/vkaJBfAcBasVFws7mEApIPo8eTM3zakUwrDvI=;
        b=e7+zKvO1EtJJpUTj9d1ywx7ZB97zSmS9la5Ywg+uf/bL51n9e5tpIaSAk+2Dw1N22p
         ++eOjKyaneaQiuwEOtWQnx2SjG/NAyKZ2ED5SgDwVG5F13W9a6uy+9G1dDMc9IMfS3lg
         Jqw2i+aP2ksU1jArhKMSNEvS9FYE8qf5mjv/AOemkP23LySYpuuNCKPrHFM4GGO3EjaO
         6flhOlJ/Y3WTVGD4h0FpUjW44709SP1qkSxRHT6UYWlsl48W5CKObqbYU9++BZ1mRm3P
         t/cbyabhWPUGV7QXvO3flRezxQ0EbRJbbDCdGMYMvcYinRPlL+E6uEBlxbfOzoICDwNn
         FEtA==
X-Received: by 10.180.228.100 with SMTP id sh4mr1010896wic.40.1400820774910;
 Thu, 22 May 2014 21:52:54 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.217.143.148 with HTTP; Thu, 22 May 2014 21:52:14 -0700 (PDT)
From: prabeesh k <prabsmails@gmail.com>
Date: Fri, 23 May 2014 10:22:14 +0530
Message-ID: <CAPdPcW2QhRL_15s6w+VU0tOsL5fmAQAcpnhaDT+Pi91fYJd5rA@mail.gmail.com>
Subject: java.lang.OutOfMemoryError while running Shark on Mesos
To: user@spark.apache.org, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1135e81aaec14e04fa0a0023
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1135e81aaec14e04fa0a0023
Content-Type: text/plain; charset=UTF-8

Hi,

I am trying to apply  inner join in shark using 64MB and 27MB files. I am
able to run the following queris on Mesos


   - "SELECT * FROM geoLocation1 "



   - """ SELECT * FROM geoLocation1  WHERE  country =  '"US"' """


But while trying inner join as

 "SELECT * FROM geoLocation1 g1 INNER JOIN geoBlocks1 g2 ON (g1.locId =
g2.locId)"



I am getting following error as follows.


Exception in thread "main" org.apache.spark.SparkException: Job aborted:
Task 1.0:7 failed 4 times (most recent failure: Exception failure:
java.lang.OutOfMemoryError: Java heap space)
 at
org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1020)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1018)
 at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
 at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1018)
 at
org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
 at scala.Option.foreach(Option.scala:236)
at
org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:604)
 at
org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:190)
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
 at akka.actor.ActorCell.invoke(ActorCell.scala:456)
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
 at akka.dispatch.Mailbox.run(Mailbox.scala:219)
at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
 at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
 at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


Please help me to resolve this.

Thanks in adv

regards,
prabeesh

--001a1135e81aaec14e04fa0a0023--

From dev-return-7794-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 23 06:49:28 2014
Return-Path: <dev-return-7794-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 26A6B10ABD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 23 May 2014 06:49:28 +0000 (UTC)
Received: (qmail 14358 invoked by uid 500); 23 May 2014 06:49:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14296 invoked by uid 500); 23 May 2014 06:49:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 71978 invoked by uid 99); 23 May 2014 06:28:22 -0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=G2l24Uw76VE1mRHcoWFoK9jAolJNVGqFwvOXGBGORt0=;
        b=UnX/9J1n1hk8GwIn312bAPUU+ktGHoEgsY620TVymrm4LW4r390aFE2MrxmV1CTfn/
         Zqrx+jfwB5a//hBf9qEHEowiWfuT0kKNssO9PgcHHzzihBBrgrpctVaSl94z7fQQJsqr
         1uQHp9wz4qmtopUtzO424afjvqcevQO4hrsy7TvZmCO9aZYD0UX6NmhsYfCTsDvf9O76
         9VPKBZPCxOFmw2kwsTTQcKBsPqtn2qP1961KITYCYZR4YJ3MVoAHypHnT9wmiE2R8DcH
         ktcUdVYhxrUuM75VHarM0ceEcquA2ODt3OKgK6IM+PEcg41ovHTMjmTC4LY5jh6pakfr
         ikSQ==
X-Gm-Message-State: ALoCoQnNnNS9hxs153y9uPPGGmSyoKlMaxBGBmcij4Qr7Vyvz4QoUnxFRCO7kTgYHcCaJSVjo5N2
MIME-Version: 1.0
X-Received: by 10.112.139.65 with SMTP id qw1mr1799069lbb.25.1400826475764;
 Thu, 22 May 2014 23:27:55 -0700 (PDT)
In-Reply-To: <CAPdPcW2QhRL_15s6w+VU0tOsL5fmAQAcpnhaDT+Pi91fYJd5rA@mail.gmail.com>
References: <CAPdPcW2QhRL_15s6w+VU0tOsL5fmAQAcpnhaDT+Pi91fYJd5rA@mail.gmail.com>
Date: Fri, 23 May 2014 11:57:55 +0530
Message-ID: <CAHUQ+_Y8U6BBfPgFiesyEqj9fowDoOC2bpg+EdEvet+ACJn6gA@mail.gmail.com>
Subject: Re: java.lang.OutOfMemoryError while running Shark on Mesos
From: Akhil Das <akhil@sigmoidanalytics.com>
To: user@spark.apache.org
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01182f3c7aee9304fa0b54ee
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01182f3c7aee9304fa0b54ee
Content-Type: text/plain; charset=UTF-8

Hi Prabeesh,

Do a export _JAVA_OPTIONS="-Xmx10g" before starting the shark. Also you can
do a ps aux | grep shark and see how much memory it is being allocated,
mostly it should be 512mb, in that case increase the limit.

Thanks
Best Regards


On Fri, May 23, 2014 at 10:22 AM, prabeesh k <prabsmails@gmail.com> wrote:

>
> Hi,
>
> I am trying to apply  inner join in shark using 64MB and 27MB files. I am
> able to run the following queris on Mesos
>
>
>    - "SELECT * FROM geoLocation1 "
>
>
>
>    - """ SELECT * FROM geoLocation1  WHERE  country =  '"US"' """
>
>
> But while trying inner join as
>
>  "SELECT * FROM geoLocation1 g1 INNER JOIN geoBlocks1 g2 ON (g1.locId =
> g2.locId)"
>
>
>
> I am getting following error as follows.
>
>
> Exception in thread "main" org.apache.spark.SparkException: Job aborted:
> Task 1.0:7 failed 4 times (most recent failure: Exception failure:
> java.lang.OutOfMemoryError: Java heap space)
>  at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1020)
> at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1018)
>  at
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>  at org.apache.spark.scheduler.DAGScheduler.org
> $apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1018)
>  at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
> at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
>  at scala.Option.foreach(Option.scala:236)
> at
> org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:604)
>  at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:190)
> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>  at akka.actor.ActorCell.invoke(ActorCell.scala:456)
> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>  at akka.dispatch.Mailbox.run(Mailbox.scala:219)
> at
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>  at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
> at
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>  at
> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
> at
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>
>
> Please help me to resolve this.
>
> Thanks in adv
>
> regards,
> prabeesh
>

--089e01182f3c7aee9304fa0b54ee--

From dev-return-7795-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 23 21:58:23 2014
Return-Path: <dev-return-7795-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9F2BD11847
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 23 May 2014 21:58:23 +0000 (UTC)
Received: (qmail 11115 invoked by uid 500); 23 May 2014 21:58:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11051 invoked by uid 500); 23 May 2014 21:58:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11043 invoked by uid 99); 23 May 2014 21:58:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 May 2014 21:58:23 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: unknown (athena.apache.org: error in processing during lookup of jdonahue@adobe.com)
Received: from [207.46.163.212] (HELO na01-bl2-obe.outbound.protection.outlook.com) (207.46.163.212)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 23 May 2014 21:58:16 +0000
Received: from BLUPR02MB408.namprd02.prod.outlook.com (10.141.80.21) by
 BLUPR02MB130.namprd02.prod.outlook.com (10.242.189.23) with Microsoft SMTP
 Server (TLS) id 15.0.949.11; Fri, 23 May 2014 21:57:42 +0000
Received: from BLUPR02MB405.namprd02.prod.outlook.com (10.141.80.13) by
 BLUPR02MB408.namprd02.prod.outlook.com (10.141.80.21) with Microsoft SMTP
 Server (TLS) id 15.0.944.11; Fri, 23 May 2014 21:57:41 +0000
Received: from BLUPR02MB405.namprd02.prod.outlook.com ([10.141.80.13]) by
 BLUPR02MB405.namprd02.prod.outlook.com ([10.141.80.13]) with mapi id
 15.00.0949.001; Fri, 23 May 2014 21:57:41 +0000
From: Jim Donahue <jdonahue@adobe.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: No output from Spark Streaming program with  Spark 1.0
Thread-Topic: No output from Spark Streaming program with  Spark 1.0
Thread-Index: AQHPdtIEV0YxvuUVjki33D7IqBk7rw==
Date: Fri, 23 May 2014 21:57:40 +0000
Message-ID: <CFA50D86.1415A%jdonahue@adobe.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
user-agent: Microsoft-MacOutlook/14.4.1.140326
x-originating-ip: [192.150.10.209]
x-forefront-prvs: 0220D4B98D
x-forefront-antispam-report: SFV:NSPM;SFS:(6009001)(428001)(199002)(189002)(164054003)(64706001)(81342001)(74502001)(21056001)(66066001)(80022001)(36756003)(4396001)(81542001)(20776003)(31966008)(83072002)(74662001)(86362001)(87936001)(2656002)(79102001)(92566001)(83322001)(19580395003)(77096999)(54356999)(46102001)(85852003)(77982001)(99286001)(92726001)(76482001)(99396002)(101416001)(50986999);DIR:OUT;SFP:;SCL:1;SRVR:BLUPR02MB408;H:BLUPR02MB405.namprd02.prod.outlook.com;FPR:;MLV:sfv;PTR:InfoNoRecords;A:1;MX:1;LANG:en;
received-spf: None (: adobe.com does not designate permitted sender hosts)
authentication-results: spf=none (sender IP is )
 smtp.mailfrom=jdonahue@adobe.com; 
Content-Type: text/plain; charset="Windows-1252"
Content-ID: <39541E7FF80CFA4699E2AED039898314@namprd02.prod.outlook.com>
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-OriginatorOrg: adobe.com
X-Virus-Checked: Checked by ClamAV on apache.org

I=B9m trying out 1.0 on a set of small Spark Streaming tests and am running
into problems.  Here=B9s one of the little programs I=B9ve used for a long
time =8B it reads a Kafka stream that contains Twitter JSON tweets and does
some simple counting.  The program starts OK (it connects to the Kafka
stream fine) and generates a stream of INFO logging messages, but never
generates any output. :-(

I=B9m running this in Eclipse, so there may be some class loading issue
(loading the wrong class or something like that), but I=B9m not seeing
anything in the console output.

Thanks,

Jim Donahue
Adobe



val kafka_messages =3D
      KafkaUtils.createStream[Array[Byte], Array[Byte],
kafka.serializer.DefaultDecoder, kafka.serializer.DefaultDecoder](ssc,
propsMap, topicMap, StorageLevel.MEMORY_AND_DISK)

   =20
     val messages =3D kafka_messages.map(_._2)

    =20
     val total =3D ssc.sparkContext.accumulator(0)

    =20
     val startTime =3D new java.util.Date().getTime()

    =20
     val jsonstream =3D messages.map[JSONObject](message =3D>
      {val string =3D new String(message);
      val json =3D new JSONObject(string);
      total +=3D 1
      json
      }
    )

   =20
    val deleted =3D ssc.sparkContext.accumulator(0)

   =20
    val msgstream =3D jsonstream.filter(json =3D>
      if (!json.has("delete")) true else { deleted +=3D 1; false}
      )

   =20
    msgstream.foreach(rdd =3D> {
      if(rdd.count() > 0){
      val data =3D rdd.map(json =3D> (json.has("entities"),
json.length())).collect()
      val entities: Double =3D data.count(t =3D> t._1)
      val fieldCounts =3D data.sortBy(_._2)
      val minFields =3D fieldCounts(0)._2
      val maxFields =3D fieldCounts(fieldCounts.size - 1)._2
      val now =3D new java.util.Date()
      val interval =3D (now.getTime() - startTime) / 1000
      System.out.println(now.toString)
      System.out.println("processing time: " + interval + " seconds")
      System.out.println("total messages: " + total.value)
      System.out.println("deleted messages: " + deleted.value)
      System.out.println("message receipt rate: " + (total.value/interval)
+ " per second")
      System.out.println("messages this interval: " + data.length)
      System.out.println("message fields varied between: " + minFields + "
and " + maxFields)
      System.out.println("fraction with entities is " + (entities /
data.length))
      }
    }
    )
   =20
    ssc.start()


From dev-return-7796-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 24 01:01:12 2014
Return-Path: <dev-return-7796-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8292411D22
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 24 May 2014 01:01:12 +0000 (UTC)
Received: (qmail 94592 invoked by uid 500); 24 May 2014 01:01:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94537 invoked by uid 500); 24 May 2014 01:01:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94529 invoked by uid 99); 24 May 2014 01:01:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 24 May 2014 01:01:11 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.54 as permitted sender)
Received: from [209.85.219.54] (HELO mail-oa0-f54.google.com) (209.85.219.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 24 May 2014 01:01:06 +0000
Received: by mail-oa0-f54.google.com with SMTP id j17so6377744oag.41
        for <dev@spark.apache.org>; Fri, 23 May 2014 18:00:45 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type:content-transfer-encoding;
        bh=0VEuQ2nYk3KMnmp33JPjVuXCseWZaIity0iwlLO+BOo=;
        b=Gh2TNS1mBOel0xxVQemoNu8WjFW70Xl8E7qa7OAd4lQ0bKd1xw5FleAc59b6jTueoi
         mSypTVqGd9L/9MGw+SmsaclSg5vdL5/TrVnp04xKvw+xTpg203mldJqYbAPC4wPbUzUb
         pu2XA8XLRjTjrHsGZ+48/TNDyjMO17ObOyp527sCXGZPkQiVunmlTET/DDHBrn+ANmsf
         qhnDEbeMKgnoas+0ksDQ1iSqlmqklxEpE8Qkhg4z2Uc63GaaXsQ0AV0XYE3MHN1IpvTY
         AA90L4Y7GmCFl3WUFSDAGTFfOW5PLtwthYMhir9i19twi2JkfkEYWBy0DQQHHMI5JmC1
         s5Eg==
MIME-Version: 1.0
X-Received: by 10.60.33.199 with SMTP id t7mr7228983oei.73.1400893245593; Fri,
 23 May 2014 18:00:45 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Fri, 23 May 2014 18:00:45 -0700 (PDT)
In-Reply-To: <CFA50D86.1415A%jdonahue@adobe.com>
References: <CFA50D86.1415A%jdonahue@adobe.com>
Date: Fri, 23 May 2014 18:00:45 -0700
Message-ID: <CABPQxsvo=HPxP-tNumvkJMk=84_8iLC-f9Jqbs_mr_EtFh8e-A@mail.gmail.com>
Subject: Re: No output from Spark Streaming program with Spark 1.0
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Jim,

Do you see the same behavior if you run this outside of eclipse?

Also, what happens if you print something to standard out when setting
up your streams (i.e. not inside of the foreach) do you see that? This
could be a streaming issue, but it could also be something related to
the way it's running in eclipse.

- Patrick

On Fri, May 23, 2014 at 2:57 PM, Jim Donahue <jdonahue@adobe.com> wrote:
> I=C2=B9m trying out 1.0 on a set of small Spark Streaming tests and am ru=
nning
> into problems.  Here=C2=B9s one of the little programs I=C2=B9ve used for=
 a long
> time =E2=80=B9 it reads a Kafka stream that contains Twitter JSON tweets =
and does
> some simple counting.  The program starts OK (it connects to the Kafka
> stream fine) and generates a stream of INFO logging messages, but never
> generates any output. :-(
>
> I=C2=B9m running this in Eclipse, so there may be some class loading issu=
e
> (loading the wrong class or something like that), but I=C2=B9m not seeing
> anything in the console output.
>
> Thanks,
>
> Jim Donahue
> Adobe
>
>
>
> val kafka_messages =3D
>       KafkaUtils.createStream[Array[Byte], Array[Byte],
> kafka.serializer.DefaultDecoder, kafka.serializer.DefaultDecoder](ssc,
> propsMap, topicMap, StorageLevel.MEMORY_AND_DISK)
>
>
>      val messages =3D kafka_messages.map(_._2)
>
>
>      val total =3D ssc.sparkContext.accumulator(0)
>
>
>      val startTime =3D new java.util.Date().getTime()
>
>
>      val jsonstream =3D messages.map[JSONObject](message =3D>
>       {val string =3D new String(message);
>       val json =3D new JSONObject(string);
>       total +=3D 1
>       json
>       }
>     )
>
>
>     val deleted =3D ssc.sparkContext.accumulator(0)
>
>
>     val msgstream =3D jsonstream.filter(json =3D>
>       if (!json.has("delete")) true else { deleted +=3D 1; false}
>       )
>
>
>     msgstream.foreach(rdd =3D> {
>       if(rdd.count() > 0){
>       val data =3D rdd.map(json =3D> (json.has("entities"),
> json.length())).collect()
>       val entities: Double =3D data.count(t =3D> t._1)
>       val fieldCounts =3D data.sortBy(_._2)
>       val minFields =3D fieldCounts(0)._2
>       val maxFields =3D fieldCounts(fieldCounts.size - 1)._2
>       val now =3D new java.util.Date()
>       val interval =3D (now.getTime() - startTime) / 1000
>       System.out.println(now.toString)
>       System.out.println("processing time: " + interval + " seconds")
>       System.out.println("total messages: " + total.value)
>       System.out.println("deleted messages: " + deleted.value)
>       System.out.println("message receipt rate: " + (total.value/interval=
)
> + " per second")
>       System.out.println("messages this interval: " + data.length)
>       System.out.println("message fields varied between: " + minFields + =
"
> and " + maxFields)
>       System.out.println("fraction with entities is " + (entities /
> data.length))
>       }
>     }
>     )
>
>     ssc.start()
>

From dev-return-7797-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 24 01:02:28 2014
Return-Path: <dev-return-7797-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DD1EA11D3C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 24 May 2014 01:02:27 +0000 (UTC)
Received: (qmail 98656 invoked by uid 500); 24 May 2014 01:02:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98594 invoked by uid 500); 24 May 2014 01:02:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98586 invoked by uid 99); 24 May 2014 01:02:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 24 May 2014 01:02:27 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.43 as permitted sender)
Received: from [209.85.219.43] (HELO mail-oa0-f43.google.com) (209.85.219.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 24 May 2014 01:02:21 +0000
Received: by mail-oa0-f43.google.com with SMTP id l6so6342009oag.30
        for <dev@spark.apache.org>; Fri, 23 May 2014 18:02:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type:content-transfer-encoding;
        bh=dtyEcz6/bTXboMLU20lDs0gOya31bN7F6QxCyyjoYFU=;
        b=RZo4mVlajM8c5icMUyJmPkO7MPBd2NqU0cwAw3n1zUOZH09ga7aFPQSDkwVVDCWehU
         Zb/UXu2rIpRA7OhFu4KElJXBuVellOxMGOGAt6fQ+8YgAZRaQ6p55Y8cFx51WyH1iK6g
         TXac8NdBEW9ivlbVxMyLDOTqFbSd72uVeDugC0VE9388c3ouxbqsd7R8ZtxgxItmz7SO
         DDJBd6if7rAmhTnQDU38mm48wWjBXhhbqLPoS2EjWxC7ZtE8tHs+S2af4sT/+9Pzx+Rv
         IuoMAFkpoGMVlvQshlmhraAOgl5wglY/JUN42hMBcow6Y58i7GTrVDMiDhLeNQb3YV4M
         UiEA==
MIME-Version: 1.0
X-Received: by 10.182.47.196 with SMTP id f4mr8949660obn.50.1400893321314;
 Fri, 23 May 2014 18:02:01 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Fri, 23 May 2014 18:02:01 -0700 (PDT)
In-Reply-To: <CABPQxsvo=HPxP-tNumvkJMk=84_8iLC-f9Jqbs_mr_EtFh8e-A@mail.gmail.com>
References: <CFA50D86.1415A%jdonahue@adobe.com>
	<CABPQxsvo=HPxP-tNumvkJMk=84_8iLC-f9Jqbs_mr_EtFh8e-A@mail.gmail.com>
Date: Fri, 23 May 2014 18:02:01 -0700
Message-ID: <CABPQxssvZ2-hEy2=qb+Lhrn7fkwLU22p0b+CZe-XaE0m4gT4GA@mail.gmail.com>
Subject: Re: No output from Spark Streaming program with Spark 1.0
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Also one other thing to try, try removing all of the logic form inside
of foreach and just printing something. It could be that somehow an
exception is being triggered inside of your foreach block and as a
result the output goes away.

On Fri, May 23, 2014 at 6:00 PM, Patrick Wendell <pwendell@gmail.com> wrote=
:
> Hey Jim,
>
> Do you see the same behavior if you run this outside of eclipse?
>
> Also, what happens if you print something to standard out when setting
> up your streams (i.e. not inside of the foreach) do you see that? This
> could be a streaming issue, but it could also be something related to
> the way it's running in eclipse.
>
> - Patrick
>
> On Fri, May 23, 2014 at 2:57 PM, Jim Donahue <jdonahue@adobe.com> wrote:
>> I=C2=B9m trying out 1.0 on a set of small Spark Streaming tests and am r=
unning
>> into problems.  Here=C2=B9s one of the little programs I=C2=B9ve used fo=
r a long
>> time =E2=80=B9 it reads a Kafka stream that contains Twitter JSON tweets=
 and does
>> some simple counting.  The program starts OK (it connects to the Kafka
>> stream fine) and generates a stream of INFO logging messages, but never
>> generates any output. :-(
>>
>> I=C2=B9m running this in Eclipse, so there may be some class loading iss=
ue
>> (loading the wrong class or something like that), but I=C2=B9m not seein=
g
>> anything in the console output.
>>
>> Thanks,
>>
>> Jim Donahue
>> Adobe
>>
>>
>>
>> val kafka_messages =3D
>>       KafkaUtils.createStream[Array[Byte], Array[Byte],
>> kafka.serializer.DefaultDecoder, kafka.serializer.DefaultDecoder](ssc,
>> propsMap, topicMap, StorageLevel.MEMORY_AND_DISK)
>>
>>
>>      val messages =3D kafka_messages.map(_._2)
>>
>>
>>      val total =3D ssc.sparkContext.accumulator(0)
>>
>>
>>      val startTime =3D new java.util.Date().getTime()
>>
>>
>>      val jsonstream =3D messages.map[JSONObject](message =3D>
>>       {val string =3D new String(message);
>>       val json =3D new JSONObject(string);
>>       total +=3D 1
>>       json
>>       }
>>     )
>>
>>
>>     val deleted =3D ssc.sparkContext.accumulator(0)
>>
>>
>>     val msgstream =3D jsonstream.filter(json =3D>
>>       if (!json.has("delete")) true else { deleted +=3D 1; false}
>>       )
>>
>>
>>     msgstream.foreach(rdd =3D> {
>>       if(rdd.count() > 0){
>>       val data =3D rdd.map(json =3D> (json.has("entities"),
>> json.length())).collect()
>>       val entities: Double =3D data.count(t =3D> t._1)
>>       val fieldCounts =3D data.sortBy(_._2)
>>       val minFields =3D fieldCounts(0)._2
>>       val maxFields =3D fieldCounts(fieldCounts.size - 1)._2
>>       val now =3D new java.util.Date()
>>       val interval =3D (now.getTime() - startTime) / 1000
>>       System.out.println(now.toString)
>>       System.out.println("processing time: " + interval + " seconds")
>>       System.out.println("total messages: " + total.value)
>>       System.out.println("deleted messages: " + deleted.value)
>>       System.out.println("message receipt rate: " + (total.value/interva=
l)
>> + " per second")
>>       System.out.println("messages this interval: " + data.length)
>>       System.out.println("message fields varied between: " + minFields +=
 "
>> and " + maxFields)
>>       System.out.println("fraction with entities is " + (entities /
>> data.length))
>>       }
>>     }
>>     )
>>
>>     ssc.start()
>>

From dev-return-7798-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 24 02:30:18 2014
Return-Path: <dev-return-7798-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 91F2C11E64
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 24 May 2014 02:30:18 +0000 (UTC)
Received: (qmail 63543 invoked by uid 500); 24 May 2014 02:30:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63488 invoked by uid 500); 24 May 2014 02:30:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63479 invoked by uid 99); 24 May 2014 02:30:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 24 May 2014 02:30:18 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.128.178 as permitted sender)
Received: from [209.85.128.178] (HELO mail-ve0-f178.google.com) (209.85.128.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 24 May 2014 02:30:12 +0000
Received: by mail-ve0-f178.google.com with SMTP id sa20so7106804veb.23
        for <dev@spark.apache.org>; Fri, 23 May 2014 19:29:52 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=jQbr/lBwVkIj9bOYtdVb2K95ojotH2We5F2emOyav58=;
        b=CAJDduRbXWNjYn7Vqz1p5hNJfgpWF57BxEeA9bOvLVNdDUzsov2IYru50KINqivAkc
         pDoX9MAjCjJgkGnf97WZg7OX43xXzXLCn0e2kZfrjSE58yEimbrO2BJVmk38+C0LWSoc
         4izTOT8G1FDUrnTsFOGn44KqJNgfesMNhhUqDBrSNEkSqfG4JHUJ7TxahTd7WnTwGwJc
         hsxRUQuDaobW6H+f6XFIz15Di8z3LM5nXrP60lPQLErGiRG5LoZcCo32D6r+AxG4jBDn
         Pg34TE6mp12ontZ30AkHLA/lX6c0HLsNfqlTK/LpyNbRbvwQScgAuZmZQqYMsn4ueLur
         8iQg==
X-Received: by 10.58.246.132 with SMTP id xw4mr7651976vec.2.1400898591819;
 Fri, 23 May 2014 19:29:51 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.118.9 with HTTP; Fri, 23 May 2014 19:29:21 -0700 (PDT)
In-Reply-To: <CABPQxssvZ2-hEy2=qb+Lhrn7fkwLU22p0b+CZe-XaE0m4gT4GA@mail.gmail.com>
References: <CFA50D86.1415A%jdonahue@adobe.com> <CABPQxsvo=HPxP-tNumvkJMk=84_8iLC-f9Jqbs_mr_EtFh8e-A@mail.gmail.com>
 <CABPQxssvZ2-hEy2=qb+Lhrn7fkwLU22p0b+CZe-XaE0m4gT4GA@mail.gmail.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Fri, 23 May 2014 19:29:21 -0700
Message-ID: <CAMwrk0nHNjv+StpBO+MKbDvK4DFLPYbxP56APdAYwJP-x-ua9g@mail.gmail.com>
Subject: Re: No output from Spark Streaming program with Spark 1.0
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bd8fe94ee90ec04fa1c1e7a
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd8fe94ee90ec04fa1c1e7a
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Few more suggestions.
1. See the web ui, is the system running any jobs? If not, then you may
need to give the system more nodes. Basically the system should have more
cores than the number of receivers.
2. Furthermore there is a streaming specific web ui which gives more
streaming specific data.


On Fri, May 23, 2014 at 6:02 PM, Patrick Wendell <pwendell@gmail.com> wrote=
:

> Also one other thing to try, try removing all of the logic form inside
> of foreach and just printing something. It could be that somehow an
> exception is being triggered inside of your foreach block and as a
> result the output goes away.
>
> On Fri, May 23, 2014 at 6:00 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > Hey Jim,
> >
> > Do you see the same behavior if you run this outside of eclipse?
> >
> > Also, what happens if you print something to standard out when setting
> > up your streams (i.e. not inside of the foreach) do you see that? This
> > could be a streaming issue, but it could also be something related to
> > the way it's running in eclipse.
> >
> > - Patrick
> >
> > On Fri, May 23, 2014 at 2:57 PM, Jim Donahue <jdonahue@adobe.com> wrote=
:
> >> I=C2=B9m trying out 1.0 on a set of small Spark Streaming tests and am
> running
> >> into problems.  Here=C2=B9s one of the little programs I=C2=B9ve used =
for a long
> >> time =E2=80=B9 it reads a Kafka stream that contains Twitter JSON twee=
ts and
> does
> >> some simple counting.  The program starts OK (it connects to the Kafka
> >> stream fine) and generates a stream of INFO logging messages, but neve=
r
> >> generates any output. :-(
> >>
> >> I=C2=B9m running this in Eclipse, so there may be some class loading i=
ssue
> >> (loading the wrong class or something like that), but I=C2=B9m not see=
ing
> >> anything in the console output.
> >>
> >> Thanks,
> >>
> >> Jim Donahue
> >> Adobe
> >>
> >>
> >>
> >> val kafka_messages =3D
> >>       KafkaUtils.createStream[Array[Byte], Array[Byte],
> >> kafka.serializer.DefaultDecoder, kafka.serializer.DefaultDecoder](ssc,
> >> propsMap, topicMap, StorageLevel.MEMORY_AND_DISK)
> >>
> >>
> >>      val messages =3D kafka_messages.map(_._2)
> >>
> >>
> >>      val total =3D ssc.sparkContext.accumulator(0)
> >>
> >>
> >>      val startTime =3D new java.util.Date().getTime()
> >>
> >>
> >>      val jsonstream =3D messages.map[JSONObject](message =3D>
> >>       {val string =3D new String(message);
> >>       val json =3D new JSONObject(string);
> >>       total +=3D 1
> >>       json
> >>       }
> >>     )
> >>
> >>
> >>     val deleted =3D ssc.sparkContext.accumulator(0)
> >>
> >>
> >>     val msgstream =3D jsonstream.filter(json =3D>
> >>       if (!json.has("delete")) true else { deleted +=3D 1; false}
> >>       )
> >>
> >>
> >>     msgstream.foreach(rdd =3D> {
> >>       if(rdd.count() > 0){
> >>       val data =3D rdd.map(json =3D> (json.has("entities"),
> >> json.length())).collect()
> >>       val entities: Double =3D data.count(t =3D> t._1)
> >>       val fieldCounts =3D data.sortBy(_._2)
> >>       val minFields =3D fieldCounts(0)._2
> >>       val maxFields =3D fieldCounts(fieldCounts.size - 1)._2
> >>       val now =3D new java.util.Date()
> >>       val interval =3D (now.getTime() - startTime) / 1000
> >>       System.out.println(now.toString)
> >>       System.out.println("processing time: " + interval + " seconds")
> >>       System.out.println("total messages: " + total.value)
> >>       System.out.println("deleted messages: " + deleted.value)
> >>       System.out.println("message receipt rate: " +
> (total.value/interval)
> >> + " per second")
> >>       System.out.println("messages this interval: " + data.length)
> >>       System.out.println("message fields varied between: " + minFields
> + "
> >> and " + maxFields)
> >>       System.out.println("fraction with entities is " + (entities /
> >> data.length))
> >>       }
> >>     }
> >>     )
> >>
> >>     ssc.start()
> >>
>

--047d7bd8fe94ee90ec04fa1c1e7a--

From dev-return-7799-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 24 15:33:24 2014
Return-Path: <dev-return-7799-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 53A9E11987
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 24 May 2014 15:33:24 +0000 (UTC)
Received: (qmail 93874 invoked by uid 500); 24 May 2014 15:33:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93804 invoked by uid 500); 24 May 2014 15:33:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93794 invoked by uid 99); 24 May 2014 15:33:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 24 May 2014 15:33:23 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 24 May 2014 15:33:21 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <nilesh@nileshc.com>)
	id 1WoDwP-0007sv-A4
	for dev@spark.incubator.apache.org; Sat, 24 May 2014 08:32:57 -0700
Date: Sat, 24 May 2014 08:32:57 -0700 (PDT)
From: Nilesh <nilesh@nileshc.com>
To: dev@spark.incubator.apache.org
Message-ID: <1400945577261-6787.post@n3.nabble.com>
Subject: Kryo serialization for closures: a workaround
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_79512_19964469.1400945577300"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_79512_19964469.1400945577300
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit

Suppose my mappers can be functions (def) that internally call other classes
and create objects and do different things inside. (Or they can even be
classes that extend (Foo) => Bar and do the processing in their apply method
- but let's ignore this case for now)

Spark supports only Java Serialization for closures and forces all the
classes inside to implement Serializable and coughs up errors when forced to
use Kryo for closures. But one cannot expect all 3rd party libraries to have
all classes extend Serializable!

Here's a workaround that I thought I'd share in case anyone comes across
this problem:

You simply need to serialize the objects before passing through the closure,
and de-serialize afterwards. This approach just works, even if your classes
aren't Serializable, because it uses Kryo behind the scenes. All you need is
some curry. ;) Here's an example of how I did it:

def genMapper(kryoWrapper: KryoSerializationWrapper[(Foo => Bar)])              
(foo: Foo) : Bar = {    kryoWrapper.value.apply(foo)}val mapper =
genMapper(KryoSerializationWrapper(new Blah(abc)))
_rdd.flatMap(mapper).collectAsMap()object Blah(abc: ABC) extends (Foo =>
Bar) {    def apply(foo: Foo) : Bar = { //This is the real function }}
Feel free to make Blah as complicated as you want, class, companion object,
nested classes, references to multiple 3rd party libs.

KryoSerializationWrapper refers to  this wrapper from amplab/shark
<https://github.com/amplab/shark/blob/master/src/main/scala/shark/execution/serialization/KryoSerializationWrapper.scala>  

Don't you think it's a good idea to have something like this inside the
framework itself? :)



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Kryo-serialization-for-closures-a-workaround-tp6787.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
------=_Part_79512_19964469.1400945577300--

From dev-return-7800-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 24 17:28:55 2014
Return-Path: <dev-return-7800-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C210911B7A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 24 May 2014 17:28:55 +0000 (UTC)
Received: (qmail 76472 invoked by uid 500); 24 May 2014 17:28:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76413 invoked by uid 500); 24 May 2014 17:28:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76405 invoked by uid 99); 24 May 2014 17:28:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 24 May 2014 17:28:54 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.169] (HELO mail-qc0-f169.google.com) (209.85.216.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 24 May 2014 17:28:52 +0000
Received: by mail-qc0-f169.google.com with SMTP id e16so10079474qcx.14
        for <dev@spark.apache.org>; Sat, 24 May 2014 10:28:28 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=hLeQYc1WL1VWczbC+ZDkWPAcBWlRq1CkevcVxG7OrYk=;
        b=DtCiSoP/BRoiCJTiTeXg5Uoqcjn5Awju+nHlqGegw4Q5r3HRFT0oufmvpthwO4pgl5
         Vri9vjkGmLLaScWbn/g5UUaNIPss/yD5ue1JJF+oUgU4nd3ooRVzlzQT2PN42x3vCAsV
         Fh0aedJwjl8lKgqxtxODdQV1UVzFbCdGwN5jm3Th6CCzL4A3IO77kJsSQgMTIYyYJET6
         t1KYkCV3CGyJazV3/6z1bQa7CHn+mp+wzJ6wVlDnUxj55RBhzphz7qaJ9xrchjTieDUb
         GnU0D2xpusI6TXE054SioZEZgclfehdF4svHUChcbOkMSY6yFcV2yUaHQeVSMOjgwvJh
         7b0Q==
X-Gm-Message-State: ALoCoQlF/q4cBqBVgHxrDigwz+GnMJ6NJ2LVfGFeEJXSPAaIv1xf1fgvzEoTKY9Y1npu5yPL2EcC
X-Received: by 10.140.26.179 with SMTP id 48mr17020002qgv.51.1400952507985;
 Sat, 24 May 2014 10:28:27 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.126.1 with HTTP; Sat, 24 May 2014 10:28:07 -0700 (PDT)
In-Reply-To: <1400945577261-6787.post@n3.nabble.com>
References: <1400945577261-6787.post@n3.nabble.com>
From: Reynold Xin <rxin@databricks.com>
Date: Sat, 24 May 2014 10:28:07 -0700
Message-ID: <CAPh_B=Yq6fJ14b0aSo_brjXBgnVpVrzT29i8N=sOEgEEhExb_w@mail.gmail.com>
Subject: Re: Kryo serialization for closures: a workaround
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c035f496075104fa28ac8e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c035f496075104fa28ac8e
Content-Type: text/plain; charset=UTF-8

Thanks for sending this in.

The ASF list doesn't support html so the formatting of the code is a little
messed up. For those who want to see the code in clearly formatted text, go
to
http://apache-spark-developers-list.1001551.n3.nabble.com/Kryo-serialization-for-closures-a-workaround-tp6787.html



On Sat, May 24, 2014 at 8:32 AM, Nilesh <nilesh@nileshc.com> wrote:

> Suppose my mappers can be functions (def) that internally call other
> classes
> and create objects and do different things inside. (Or they can even be
> classes that extend (Foo) => Bar and do the processing in their apply
> method
> - but let's ignore this case for now)
>
> Spark supports only Java Serialization for closures and forces all the
> classes inside to implement Serializable and coughs up errors when forced
> to
> use Kryo for closures. But one cannot expect all 3rd party libraries to
> have
> all classes extend Serializable!
>
> Here's a workaround that I thought I'd share in case anyone comes across
> this problem:
>
> You simply need to serialize the objects before passing through the
> closure,
> and de-serialize afterwards. This approach just works, even if your classes
> aren't Serializable, because it uses Kryo behind the scenes. All you need
> is
> some curry. ;) Here's an example of how I did it:
>
> def genMapper(kryoWrapper: KryoSerializationWrapper[(Foo => Bar)])
> (foo: Foo) : Bar = {    kryoWrapper.value.apply(foo)}val mapper =
> genMapper(KryoSerializationWrapper(new Blah(abc)))
> _rdd.flatMap(mapper).collectAsMap()object Blah(abc: ABC) extends (Foo =>
> Bar) {    def apply(foo: Foo) : Bar = { //This is the real function }}
> Feel free to make Blah as complicated as you want, class, companion object,
> nested classes, references to multiple 3rd party libs.
>
> KryoSerializationWrapper refers to  this wrapper from amplab/shark
> <
> https://github.com/amplab/shark/blob/master/src/main/scala/shark/execution/serialization/KryoSerializationWrapper.scala
> >
>
> Don't you think it's a good idea to have something like this inside the
> framework itself? :)
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Kryo-serialization-for-closures-a-workaround-tp6787.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.

--001a11c035f496075104fa28ac8e--

From dev-return-7801-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 24 23:58:34 2014
Return-Path: <dev-return-7801-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2ED5011127
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 24 May 2014 23:58:33 +0000 (UTC)
Received: (qmail 82226 invoked by uid 500); 24 May 2014 23:58:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82160 invoked by uid 500); 24 May 2014 23:58:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82152 invoked by uid 99); 24 May 2014 23:58:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 24 May 2014 23:58:32 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_HELO_PASS
X-Spam-Check-By: apache.org
Received-SPF: unknown (athena.apache.org: error in processing during lookup of jdonahue@adobe.com)
Received: from [207.46.163.143] (HELO na01-bn1-obe.outbound.protection.outlook.com) (207.46.163.143)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 24 May 2014 23:58:26 +0000
Received: from BLUPR02MB405.namprd02.prod.outlook.com (10.141.80.13) by
 BLUPR02MB406.namprd02.prod.outlook.com (10.141.80.16) with Microsoft SMTP
 Server (TLS) id 15.0.949.11; Sat, 24 May 2014 23:58:03 +0000
Received: from BLUPR02MB405.namprd02.prod.outlook.com ([10.141.80.13]) by
 BLUPR02MB405.namprd02.prod.outlook.com ([10.141.80.13]) with mapi id
 15.00.0949.001; Sat, 24 May 2014 23:58:03 +0000
From: Jim Donahue <jdonahue@adobe.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: No output from Spark Streaming program with Spark 1.0
Thread-Topic: No output from Spark Streaming program with Spark 1.0
Thread-Index: AQHPduvC1MFLMYQqc02m9Td2IfwRAJtO6bKAgAAYZoCAAPKxAA==
Date: Sat, 24 May 2014 23:58:03 +0000
Message-ID: <CFA67E04.141EE%jdonahue@adobe.com>
References: <CFA50D86.1415A%jdonahue@adobe.com>
 <CABPQxsvo=HPxP-tNumvkJMk=84_8iLC-f9Jqbs_mr_EtFh8e-A@mail.gmail.com>
 <CABPQxssvZ2-hEy2=qb+Lhrn7fkwLU22p0b+CZe-XaE0m4gT4GA@mail.gmail.com>
 <CAMwrk0nHNjv+StpBO+MKbDvK4DFLPYbxP56APdAYwJP-x-ua9g@mail.gmail.com>
In-Reply-To: <CAMwrk0nHNjv+StpBO+MKbDvK4DFLPYbxP56APdAYwJP-x-ua9g@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
user-agent: Microsoft-MacOutlook/14.4.1.140326
x-originating-ip: [192.150.10.202]
x-forefront-prvs: 02213C82F8
x-forefront-antispam-report: SFV:NSPM;SFS:(6009001)(428001)(377454003)(199002)(189002)(51704005)(479174003)(24454002)(164054003)(66066001)(80022001)(83506001)(20776003)(77982001)(79102001)(64706001)(74502001)(4396001)(36756003)(81342001)(50986999)(76482001)(99286001)(19580395003)(92566001)(19580405001)(46102001)(83322001)(99396002)(54356999)(77096999)(2656002)(31966008)(76176999)(86362001)(101416001)(81542001)(92726001)(74662001)(21056001)(87936001)(83072002)(85852003);DIR:OUT;SFP:;SCL:1;SRVR:BLUPR02MB406;H:BLUPR02MB405.namprd02.prod.outlook.com;FPR:;MLV:sfv;PTR:InfoNoRecords;A:1;MX:1;LANG:en;
received-spf: None (: adobe.com does not designate permitted sender hosts)
authentication-results: spf=none (sender IP is )
 smtp.mailfrom=jdonahue@adobe.com; 
Content-Type: text/plain; charset="utf-8"
Content-ID: <75B4354484A964489DEE51278CA28561@namprd02.prod.outlook.com>
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-OriginatorOrg: adobe.com
X-Virus-Checked: Checked by ClamAV on apache.org

SSBsb29rZWQgYXQgdGhlIFN0cmVhbWluZyBVSSBmb3IgbXkgam9iIGFuZCBpdCByZXBvcnRzIHRo
YXQgaXQgaGFzDQpwcm9jZXNzZWQgbWFueSBiYXRjaGVzLCBidXQgdGhhdCBub25lIG9mIHRoZSBi
YXRjaGVzIGhhZCBhbnkgcmVjb3JkcyBpbg0KdGhlbS4gVW5mb3J0dW5hdGVseSwgdGhhdOKAmXMg
d2hhdCBJIGV4cGVjdGVkLiAgOi0oDQoNCknigJl2ZSB0cmllZCBtdWx0aXBsZSB0ZXN0IHByb2dy
YW1zIGFuZCBJ4oCZbSBzZWVpbmcgdGhlIHNhbWUgdGhpbmcuICBUaGUNCkthZmthIHNvdXJjZXMg
YXJlIGFsaXZlIGFuZCB3ZWxsIGFuZCB0aGUgcHJvZ3JhbXMgYWxsIHdvcmtlZCBvbiAwLjkgZnJv
bQ0KRWNsaXBzZS4gIEFuZCB0aGVyZeKAmXMgbm8gaW5kaWNhdGlvbiBvZiBhbnkgZmFpbHVyZSDi
gJQganVzdCBubyByZWNvcmRzIGFyZQ0KYmVpbmcgZGVsaXZlcmVkLg0KDQpBbnkgaWRlYXMgd291
bGQgYmUgbXVjaCBhcHByZWNpYXRlZCDigKYNCg0KDQpUaGFua3MsDQoNCkppbQ0KDQoNCk9uIDUv
MjMvMTQsIDc6MjkgUE0sICJUYXRoYWdhdGEgRGFzIiA8dGF0aGFnYXRhLmRhczE1NjVAZ21haWwu
Y29tPiB3cm90ZToNCg0KPkZldyBtb3JlIHN1Z2dlc3Rpb25zLg0KPjEuIFNlZSB0aGUgd2ViIHVp
LCBpcyB0aGUgc3lzdGVtIHJ1bm5pbmcgYW55IGpvYnM/IElmIG5vdCwgdGhlbiB5b3UgbWF5DQo+
bmVlZCB0byBnaXZlIHRoZSBzeXN0ZW0gbW9yZSBub2Rlcy4gQmFzaWNhbGx5IHRoZSBzeXN0ZW0g
c2hvdWxkIGhhdmUgbW9yZQ0KPmNvcmVzIHRoYW4gdGhlIG51bWJlciBvZiByZWNlaXZlcnMuDQo+
Mi4gRnVydGhlcm1vcmUgdGhlcmUgaXMgYSBzdHJlYW1pbmcgc3BlY2lmaWMgd2ViIHVpIHdoaWNo
IGdpdmVzIG1vcmUNCj5zdHJlYW1pbmcgc3BlY2lmaWMgZGF0YS4NCj4NCj4NCj5PbiBGcmksIE1h
eSAyMywgMjAxNCBhdCA2OjAyIFBNLCBQYXRyaWNrIFdlbmRlbGwgPHB3ZW5kZWxsQGdtYWlsLmNv
bT4NCj53cm90ZToNCj4NCj4+IEFsc28gb25lIG90aGVyIHRoaW5nIHRvIHRyeSwgdHJ5IHJlbW92
aW5nIGFsbCBvZiB0aGUgbG9naWMgZm9ybSBpbnNpZGUNCj4+IG9mIGZvcmVhY2ggYW5kIGp1c3Qg
cHJpbnRpbmcgc29tZXRoaW5nLiBJdCBjb3VsZCBiZSB0aGF0IHNvbWVob3cgYW4NCj4+IGV4Y2Vw
dGlvbiBpcyBiZWluZyB0cmlnZ2VyZWQgaW5zaWRlIG9mIHlvdXIgZm9yZWFjaCBibG9jayBhbmQg
YXMgYQ0KPj4gcmVzdWx0IHRoZSBvdXRwdXQgZ29lcyBhd2F5Lg0KPj4NCj4+IE9uIEZyaSwgTWF5
IDIzLCAyMDE0IGF0IDY6MDAgUE0sIFBhdHJpY2sgV2VuZGVsbCA8cHdlbmRlbGxAZ21haWwuY29t
Pg0KPj4gd3JvdGU6DQo+PiA+IEhleSBKaW0sDQo+PiA+DQo+PiA+IERvIHlvdSBzZWUgdGhlIHNh
bWUgYmVoYXZpb3IgaWYgeW91IHJ1biB0aGlzIG91dHNpZGUgb2YgZWNsaXBzZT8NCj4+ID4NCj4+
ID4gQWxzbywgd2hhdCBoYXBwZW5zIGlmIHlvdSBwcmludCBzb21ldGhpbmcgdG8gc3RhbmRhcmQg
b3V0IHdoZW4gc2V0dGluZw0KPj4gPiB1cCB5b3VyIHN0cmVhbXMgKGkuZS4gbm90IGluc2lkZSBv
ZiB0aGUgZm9yZWFjaCkgZG8geW91IHNlZSB0aGF0PyBUaGlzDQo+PiA+IGNvdWxkIGJlIGEgc3Ry
ZWFtaW5nIGlzc3VlLCBidXQgaXQgY291bGQgYWxzbyBiZSBzb21ldGhpbmcgcmVsYXRlZCB0bw0K
Pj4gPiB0aGUgd2F5IGl0J3MgcnVubmluZyBpbiBlY2xpcHNlLg0KPj4gPg0KPj4gPiAtIFBhdHJp
Y2sNCj4+ID4NCj4+ID4gT24gRnJpLCBNYXkgMjMsIDIwMTQgYXQgMjo1NyBQTSwgSmltIERvbmFo
dWUgPGpkb25haHVlQGFkb2JlLmNvbT4NCj4+d3JvdGU6DQo+PiA+PiBJwrltIHRyeWluZyBvdXQg
MS4wIG9uIGEgc2V0IG9mIHNtYWxsIFNwYXJrIFN0cmVhbWluZyB0ZXN0cyBhbmQgYW0NCj4+IHJ1
bm5pbmcNCj4+ID4+IGludG8gcHJvYmxlbXMuICBIZXJlwrlzIG9uZSBvZiB0aGUgbGl0dGxlIHBy
b2dyYW1zIEnCuXZlIHVzZWQgZm9yIGENCj4+bG9uZw0KPj4gPj4gdGltZSDigLkgaXQgcmVhZHMg
YSBLYWZrYSBzdHJlYW0gdGhhdCBjb250YWlucyBUd2l0dGVyIEpTT04gdHdlZXRzIGFuZA0KPj4g
ZG9lcw0KPj4gPj4gc29tZSBzaW1wbGUgY291bnRpbmcuICBUaGUgcHJvZ3JhbSBzdGFydHMgT0sg
KGl0IGNvbm5lY3RzIHRvIHRoZQ0KPj5LYWZrYQ0KPj4gPj4gc3RyZWFtIGZpbmUpIGFuZCBnZW5l
cmF0ZXMgYSBzdHJlYW0gb2YgSU5GTyBsb2dnaW5nIG1lc3NhZ2VzLCBidXQNCj4+bmV2ZXINCj4+
ID4+IGdlbmVyYXRlcyBhbnkgb3V0cHV0LiA6LSgNCj4+ID4+DQo+PiA+PiBJwrltIHJ1bm5pbmcg
dGhpcyBpbiBFY2xpcHNlLCBzbyB0aGVyZSBtYXkgYmUgc29tZSBjbGFzcyBsb2FkaW5nIGlzc3Vl
DQo+PiA+PiAobG9hZGluZyB0aGUgd3JvbmcgY2xhc3Mgb3Igc29tZXRoaW5nIGxpa2UgdGhhdCks
IGJ1dCBJwrltIG5vdCBzZWVpbmcNCj4+ID4+IGFueXRoaW5nIGluIHRoZSBjb25zb2xlIG91dHB1
dC4NCj4+ID4+DQo+PiA+PiBUaGFua3MsDQo+PiA+Pg0KPj4gPj4gSmltIERvbmFodWUNCj4+ID4+
IEFkb2JlDQo+PiA+Pg0KPj4gPj4NCj4+ID4+DQo+PiA+PiB2YWwga2Fma2FfbWVzc2FnZXMgPQ0K
Pj4gPj4gICAgICAgS2Fma2FVdGlscy5jcmVhdGVTdHJlYW1bQXJyYXlbQnl0ZV0sIEFycmF5W0J5
dGVdLA0KPj4gPj4ga2Fma2Euc2VyaWFsaXplci5EZWZhdWx0RGVjb2RlciwNCj4+a2Fma2Euc2Vy
aWFsaXplci5EZWZhdWx0RGVjb2Rlcl0oc3NjLA0KPj4gPj4gcHJvcHNNYXAsIHRvcGljTWFwLCBT
dG9yYWdlTGV2ZWwuTUVNT1JZX0FORF9ESVNLKQ0KPj4gPj4NCj4+ID4+DQo+PiA+PiAgICAgIHZh
bCBtZXNzYWdlcyA9IGthZmthX21lc3NhZ2VzLm1hcChfLl8yKQ0KPj4gPj4NCj4+ID4+DQo+PiA+
PiAgICAgIHZhbCB0b3RhbCA9IHNzYy5zcGFya0NvbnRleHQuYWNjdW11bGF0b3IoMCkNCj4+ID4+
DQo+PiA+Pg0KPj4gPj4gICAgICB2YWwgc3RhcnRUaW1lID0gbmV3IGphdmEudXRpbC5EYXRlKCku
Z2V0VGltZSgpDQo+PiA+Pg0KPj4gPj4NCj4+ID4+ICAgICAgdmFsIGpzb25zdHJlYW0gPSBtZXNz
YWdlcy5tYXBbSlNPTk9iamVjdF0obWVzc2FnZSA9Pg0KPj4gPj4gICAgICAge3ZhbCBzdHJpbmcg
PSBuZXcgU3RyaW5nKG1lc3NhZ2UpOw0KPj4gPj4gICAgICAgdmFsIGpzb24gPSBuZXcgSlNPTk9i
amVjdChzdHJpbmcpOw0KPj4gPj4gICAgICAgdG90YWwgKz0gMQ0KPj4gPj4gICAgICAganNvbg0K
Pj4gPj4gICAgICAgfQ0KPj4gPj4gICAgICkNCj4+ID4+DQo+PiA+Pg0KPj4gPj4gICAgIHZhbCBk
ZWxldGVkID0gc3NjLnNwYXJrQ29udGV4dC5hY2N1bXVsYXRvcigwKQ0KPj4gPj4NCj4+ID4+DQo+
PiA+PiAgICAgdmFsIG1zZ3N0cmVhbSA9IGpzb25zdHJlYW0uZmlsdGVyKGpzb24gPT4NCj4+ID4+
ICAgICAgIGlmICghanNvbi5oYXMoImRlbGV0ZSIpKSB0cnVlIGVsc2UgeyBkZWxldGVkICs9IDE7
IGZhbHNlfQ0KPj4gPj4gICAgICAgKQ0KPj4gPj4NCj4+ID4+DQo+PiA+PiAgICAgbXNnc3RyZWFt
LmZvcmVhY2gocmRkID0+IHsNCj4+ID4+ICAgICAgIGlmKHJkZC5jb3VudCgpID4gMCl7DQo+PiA+
PiAgICAgICB2YWwgZGF0YSA9IHJkZC5tYXAoanNvbiA9PiAoanNvbi5oYXMoImVudGl0aWVzIiks
DQo+PiA+PiBqc29uLmxlbmd0aCgpKSkuY29sbGVjdCgpDQo+PiA+PiAgICAgICB2YWwgZW50aXRp
ZXM6IERvdWJsZSA9IGRhdGEuY291bnQodCA9PiB0Ll8xKQ0KPj4gPj4gICAgICAgdmFsIGZpZWxk
Q291bnRzID0gZGF0YS5zb3J0QnkoXy5fMikNCj4+ID4+ICAgICAgIHZhbCBtaW5GaWVsZHMgPSBm
aWVsZENvdW50cygwKS5fMg0KPj4gPj4gICAgICAgdmFsIG1heEZpZWxkcyA9IGZpZWxkQ291bnRz
KGZpZWxkQ291bnRzLnNpemUgLSAxKS5fMg0KPj4gPj4gICAgICAgdmFsIG5vdyA9IG5ldyBqYXZh
LnV0aWwuRGF0ZSgpDQo+PiA+PiAgICAgICB2YWwgaW50ZXJ2YWwgPSAobm93LmdldFRpbWUoKSAt
IHN0YXJ0VGltZSkgLyAxMDAwDQo+PiA+PiAgICAgICBTeXN0ZW0ub3V0LnByaW50bG4obm93LnRv
U3RyaW5nKQ0KPj4gPj4gICAgICAgU3lzdGVtLm91dC5wcmludGxuKCJwcm9jZXNzaW5nIHRpbWU6
ICIgKyBpbnRlcnZhbCArICIgc2Vjb25kcyIpDQo+PiA+PiAgICAgICBTeXN0ZW0ub3V0LnByaW50
bG4oInRvdGFsIG1lc3NhZ2VzOiAiICsgdG90YWwudmFsdWUpDQo+PiA+PiAgICAgICBTeXN0ZW0u
b3V0LnByaW50bG4oImRlbGV0ZWQgbWVzc2FnZXM6ICIgKyBkZWxldGVkLnZhbHVlKQ0KPj4gPj4g
ICAgICAgU3lzdGVtLm91dC5wcmludGxuKCJtZXNzYWdlIHJlY2VpcHQgcmF0ZTogIiArDQo+PiAo
dG90YWwudmFsdWUvaW50ZXJ2YWwpDQo+PiA+PiArICIgcGVyIHNlY29uZCIpDQo+PiA+PiAgICAg
ICBTeXN0ZW0ub3V0LnByaW50bG4oIm1lc3NhZ2VzIHRoaXMgaW50ZXJ2YWw6ICIgKyBkYXRhLmxl
bmd0aCkNCj4+ID4+ICAgICAgIFN5c3RlbS5vdXQucHJpbnRsbigibWVzc2FnZSBmaWVsZHMgdmFy
aWVkIGJldHdlZW46ICIgKw0KPj5taW5GaWVsZHMNCj4+ICsgIg0KPj4gPj4gYW5kICIgKyBtYXhG
aWVsZHMpDQo+PiA+PiAgICAgICBTeXN0ZW0ub3V0LnByaW50bG4oImZyYWN0aW9uIHdpdGggZW50
aXRpZXMgaXMgIiArIChlbnRpdGllcyAvDQo+PiA+PiBkYXRhLmxlbmd0aCkpDQo+PiA+PiAgICAg
ICB9DQo+PiA+PiAgICAgfQ0KPj4gPj4gICAgICkNCj4+ID4+DQo+PiA+PiAgICAgc3NjLnN0YXJ0
KCkNCj4+ID4+DQo+Pg0KDQo=

From dev-return-7802-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 25 02:47:47 2014
Return-Path: <dev-return-7802-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 66D64112CE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 25 May 2014 02:47:47 +0000 (UTC)
Received: (qmail 38979 invoked by uid 500); 25 May 2014 02:47:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38915 invoked by uid 500); 25 May 2014 02:47:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38907 invoked by uid 99); 25 May 2014 02:47:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 25 May 2014 02:47:46 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.220.170 as permitted sender)
Received: from [209.85.220.170] (HELO mail-vc0-f170.google.com) (209.85.220.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 25 May 2014 02:47:41 +0000
Received: by mail-vc0-f170.google.com with SMTP id lf12so7880107vcb.15
        for <dev@spark.apache.org>; Sat, 24 May 2014 19:47:18 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=LI8ieiERGjQPsQ3lQuz0HN6zwpVkO7wnndV7RR2BSwY=;
        b=vWNfSnYNB03RXtU53KBX9oDkElwe62wE4+ckGY4db4Hl0TzZWl14tHY9NQXHYxw5+3
         Ghg0vm2o8hbGENAYiuZ9GB0qkkf5d2GCg8I3TDK27Bt8mvBFOdtO1Fa31M0SBe2Y6ygH
         wgiLXwwdZ0ij6NDKUokxK76faJXsE+c1P7jOmsL8ECgMVUh8HsrnpIQ2CJfqfpLckeHP
         llRKDrjMDDHp2B1Gqf/uialDJ2YP6k1X5JHfOh4RCWuaPkuzXDGUJsX00zDRnQKV9FCe
         nI84tk78wwsTnwCtc2kTHIxbVYafu/K8fsGukAc5gHQJp6D/KkXYyRRACJzGip6UtKc/
         F4XQ==
X-Received: by 10.220.167.2 with SMTP id o2mr12705682vcy.8.1400986037942; Sat,
 24 May 2014 19:47:17 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.118.9 with HTTP; Sat, 24 May 2014 19:46:47 -0700 (PDT)
In-Reply-To: <CFA67E04.141EE%jdonahue@adobe.com>
References: <CFA50D86.1415A%jdonahue@adobe.com> <CABPQxsvo=HPxP-tNumvkJMk=84_8iLC-f9Jqbs_mr_EtFh8e-A@mail.gmail.com>
 <CABPQxssvZ2-hEy2=qb+Lhrn7fkwLU22p0b+CZe-XaE0m4gT4GA@mail.gmail.com>
 <CAMwrk0nHNjv+StpBO+MKbDvK4DFLPYbxP56APdAYwJP-x-ua9g@mail.gmail.com> <CFA67E04.141EE%jdonahue@adobe.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Sat, 24 May 2014 19:46:47 -0700
Message-ID: <CAMwrk0mV6Uw+TZL0Fcjs9ctwSAYaTZkc0L7w_=rv3XDy-LQ3dw@mail.gmail.com>
Subject: Re: No output from Spark Streaming program with Spark 1.0
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e011618ce2080b804fa307b54
X-Virus-Checked: Checked by ClamAV on apache.org

--089e011618ce2080b804fa307b54
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

What does the kafka receiver status on the streaming UI say when you are
connected to the Kafka sources? Does it show any error?

Can you find out which machine the receiver is running and see the worker
logs for any exceptions / error messages? Try turning on the DEBUG level in
log4j.

TD
On May 24, 2014 4:58 PM, "Jim Donahue" <jdonahue@adobe.com> wrote:

> I looked at the Streaming UI for my job and it reports that it has
> processed many batches, but that none of the batches had any records in
> them. Unfortunately, that=E2=80=99s what I expected.  :-(
>
> I=E2=80=99ve tried multiple test programs and I=E2=80=99m seeing the same=
 thing.  The
> Kafka sources are alive and well and the programs all worked on 0.9 from
> Eclipse.  And there=E2=80=99s no indication of any failure =E2=80=94 just=
 no records are
> being delivered.
>
> Any ideas would be much appreciated =E2=80=A6
>
>
> Thanks,
>
> Jim
>
>
> On 5/23/14, 7:29 PM, "Tathagata Das" <tathagata.das1565@gmail.com> wrote:
>
> >Few more suggestions.
> >1. See the web ui, is the system running any jobs? If not, then you may
> >need to give the system more nodes. Basically the system should have mor=
e
> >cores than the number of receivers.
> >2. Furthermore there is a streaming specific web ui which gives more
> >streaming specific data.
> >
> >
> >On Fri, May 23, 2014 at 6:02 PM, Patrick Wendell <pwendell@gmail.com>
> >wrote:
> >
> >> Also one other thing to try, try removing all of the logic form inside
> >> of foreach and just printing something. It could be that somehow an
> >> exception is being triggered inside of your foreach block and as a
> >> result the output goes away.
> >>
> >> On Fri, May 23, 2014 at 6:00 PM, Patrick Wendell <pwendell@gmail.com>
> >> wrote:
> >> > Hey Jim,
> >> >
> >> > Do you see the same behavior if you run this outside of eclipse?
> >> >
> >> > Also, what happens if you print something to standard out when setti=
ng
> >> > up your streams (i.e. not inside of the foreach) do you see that? Th=
is
> >> > could be a streaming issue, but it could also be something related t=
o
> >> > the way it's running in eclipse.
> >> >
> >> > - Patrick
> >> >
> >> > On Fri, May 23, 2014 at 2:57 PM, Jim Donahue <jdonahue@adobe.com>
> >>wrote:
> >> >> I=C2=B9m trying out 1.0 on a set of small Spark Streaming tests and=
 am
> >> running
> >> >> into problems.  Here=C2=B9s one of the little programs I=C2=B9ve us=
ed for a
> >>long
> >> >> time =E2=80=B9 it reads a Kafka stream that contains Twitter JSON t=
weets and
> >> does
> >> >> some simple counting.  The program starts OK (it connects to the
> >>Kafka
> >> >> stream fine) and generates a stream of INFO logging messages, but
> >>never
> >> >> generates any output. :-(
> >> >>
> >> >> I=C2=B9m running this in Eclipse, so there may be some class loadin=
g issue
> >> >> (loading the wrong class or something like that), but I=C2=B9m not =
seeing
> >> >> anything in the console output.
> >> >>
> >> >> Thanks,
> >> >>
> >> >> Jim Donahue
> >> >> Adobe
> >> >>
> >> >>
> >> >>
> >> >> val kafka_messages =3D
> >> >>       KafkaUtils.createStream[Array[Byte], Array[Byte],
> >> >> kafka.serializer.DefaultDecoder,
> >>kafka.serializer.DefaultDecoder](ssc,
> >> >> propsMap, topicMap, StorageLevel.MEMORY_AND_DISK)
> >> >>
> >> >>
> >> >>      val messages =3D kafka_messages.map(_._2)
> >> >>
> >> >>
> >> >>      val total =3D ssc.sparkContext.accumulator(0)
> >> >>
> >> >>
> >> >>      val startTime =3D new java.util.Date().getTime()
> >> >>
> >> >>
> >> >>      val jsonstream =3D messages.map[JSONObject](message =3D>
> >> >>       {val string =3D new String(message);
> >> >>       val json =3D new JSONObject(string);
> >> >>       total +=3D 1
> >> >>       json
> >> >>       }
> >> >>     )
> >> >>
> >> >>
> >> >>     val deleted =3D ssc.sparkContext.accumulator(0)
> >> >>
> >> >>
> >> >>     val msgstream =3D jsonstream.filter(json =3D>
> >> >>       if (!json.has("delete")) true else { deleted +=3D 1; false}
> >> >>       )
> >> >>
> >> >>
> >> >>     msgstream.foreach(rdd =3D> {
> >> >>       if(rdd.count() > 0){
> >> >>       val data =3D rdd.map(json =3D> (json.has("entities"),
> >> >> json.length())).collect()
> >> >>       val entities: Double =3D data.count(t =3D> t._1)
> >> >>       val fieldCounts =3D data.sortBy(_._2)
> >> >>       val minFields =3D fieldCounts(0)._2
> >> >>       val maxFields =3D fieldCounts(fieldCounts.size - 1)._2
> >> >>       val now =3D new java.util.Date()
> >> >>       val interval =3D (now.getTime() - startTime) / 1000
> >> >>       System.out.println(now.toString)
> >> >>       System.out.println("processing time: " + interval + " seconds=
")
> >> >>       System.out.println("total messages: " + total.value)
> >> >>       System.out.println("deleted messages: " + deleted.value)
> >> >>       System.out.println("message receipt rate: " +
> >> (total.value/interval)
> >> >> + " per second")
> >> >>       System.out.println("messages this interval: " + data.length)
> >> >>       System.out.println("message fields varied between: " +
> >>minFields
> >> + "
> >> >> and " + maxFields)
> >> >>       System.out.println("fraction with entities is " + (entities /
> >> >> data.length))
> >> >>       }
> >> >>     }
> >> >>     )
> >> >>
> >> >>     ssc.start()
> >> >>
> >>
>
>

--089e011618ce2080b804fa307b54--

From dev-return-7803-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 25 22:56:16 2014
Return-Path: <dev-return-7803-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 006E4C4BC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 25 May 2014 22:56:16 +0000 (UTC)
Received: (qmail 55350 invoked by uid 500); 25 May 2014 22:56:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55290 invoked by uid 500); 25 May 2014 22:56:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55282 invoked by uid 99); 25 May 2014 22:56:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 25 May 2014 22:56:15 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 25 May 2014 22:56:11 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <nilesh@nileshc.com>)
	id 1WohKX-0000De-TQ
	for dev@spark.incubator.apache.org; Sun, 25 May 2014 15:55:49 -0700
Date: Sun, 25 May 2014 15:55:49 -0700 (PDT)
From: Nilesh <nilesh@nileshc.com>
To: dev@spark.incubator.apache.org
Message-ID: <1401058549889-6791.post@n3.nabble.com>
In-Reply-To: <FBFD63BC-23CC-44D2-A114-5BE07B641A82@gmail.com>
References: <CACBYxKKWvS5FpVyWSDJG=o3xgDm2XabwNHYuzxRg8CmOV7+K8A@mail.gmail.com> <CAJiQeY+=QUjcF4vKckRnvv=G_k-QTmZ_0CCUHF=amyVjUfjNrw@mail.gmail.com> <CACBYxK+b_JyBt-D20+UUz2Sdwift0w6NaJ0_yQM75KRHDsVPEQ@mail.gmail.com> <FBFD63BC-23CC-44D2-A114-5BE07B641A82@gmail.com>
Subject: Re: all values for a key must fit in memory
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I would like to clarify something. Matei mentioned that in Spark 1.0 groupBy
returns an (Key, Iterable[Value]) instead of (Key, Seq[Value]). Does this
also automatically assure us that the whole Iterable[Value] is not in fact
stored in memory? That is to say, with 1.0, will it be possible to do
groupByKey().values.map(x => while(x.hasNext) ... ) assuming x :
Iterable[Value] is larger than the RAM on a single machine? Or will this be
possible later, in subsequent versions?

Could you please propose a workaround for this for the meantime? I'm out of
ideas.

Thanks,
Nilesh



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/all-values-for-a-key-must-fit-in-memory-tp6342p6791.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7804-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 25 23:28:07 2014
Return-Path: <dev-return-7804-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F052CC54F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 25 May 2014 23:28:06 +0000 (UTC)
Received: (qmail 71823 invoked by uid 500); 25 May 2014 23:28:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71751 invoked by uid 500); 25 May 2014 23:28:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71737 invoked by uid 99); 25 May 2014 23:28:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 25 May 2014 23:28:05 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.177] (HELO mail-vc0-f177.google.com) (209.85.220.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 25 May 2014 23:28:01 +0000
Received: by mail-vc0-f177.google.com with SMTP id hq11so2654854vcb.22
        for <dev@spark.incubator.apache.org>; Sun, 25 May 2014 16:27:40 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=/CYOq+3gLDkMDvXBZMAmNY5GcX+pNMz5vqsPDcQWb14=;
        b=e4tXtYCLwvYhQiedDPLZLoFYy/LaQR1h33JB6kQeiq1cuIPnBGhuFgifeXIKcmHe47
         cGtOpduqo/iqgDM68sjt6EpcFiRXe0W9S1FQnb3L6FcLHxUMMGWqRdIbvs4L/ytn/b+g
         HHdZ4euKmjOgy/KgOQS9T0JALPzgLG3Te2h2NoFsquoMvk9XIUmG+zT95iNiMEo5c6DY
         qh2AJYFwtrZXmANfYfLwqOQJ91ClzA1R2yXPq5vekdwJdGiffsKxoMYuAvrn+X76dnVN
         gW3wWPTNKrCW9mIwhrrA5CUh+w5iEQMm0O9BaXUJwqSHeBxnQfUF6BWcYaScfmDgLog2
         XwdA==
X-Gm-Message-State: ALoCoQn7nGMx2WS46pzXwPsr9wko1ysYS6ZB2N4wGSu1h4OJkL1v+vDleVsEu0GyXdRgo0LahXIB
X-Received: by 10.52.53.69 with SMTP id z5mr3440209vdo.42.1401060460777;
        Sun, 25 May 2014 16:27:40 -0700 (PDT)
Received: from mail-ve0-f177.google.com (mail-ve0-f177.google.com [209.85.128.177])
        by mx.google.com with ESMTPSA id m8sm10940959veh.14.2014.05.25.16.27.39
        for <dev@spark.incubator.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 25 May 2014 16:27:39 -0700 (PDT)
Received: by mail-ve0-f177.google.com with SMTP id db11so8452214veb.36
        for <dev@spark.incubator.apache.org>; Sun, 25 May 2014 16:27:39 -0700 (PDT)
X-Received: by 10.220.139.198 with SMTP id f6mr3088437vcu.47.1401060459279;
 Sun, 25 May 2014 16:27:39 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Sun, 25 May 2014 16:27:19 -0700 (PDT)
In-Reply-To: <1401058549889-6791.post@n3.nabble.com>
References: <CACBYxKKWvS5FpVyWSDJG=o3xgDm2XabwNHYuzxRg8CmOV7+K8A@mail.gmail.com>
 <CAJiQeY+=QUjcF4vKckRnvv=G_k-QTmZ_0CCUHF=amyVjUfjNrw@mail.gmail.com>
 <CACBYxK+b_JyBt-D20+UUz2Sdwift0w6NaJ0_yQM75KRHDsVPEQ@mail.gmail.com>
 <FBFD63BC-23CC-44D2-A114-5BE07B641A82@gmail.com> <1401058549889-6791.post@n3.nabble.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Sun, 25 May 2014 19:27:19 -0400
Message-ID: <CA+-p3AGS6Bkvp5q4WOZ82UYtyooeFfxP5aBgoBCFhMEP6WLa1A@mail.gmail.com>
Subject: Re: all values for a key must fit in memory
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=047d7b343940fbfb5f04fa41ce57
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b343940fbfb5f04fa41ce57
Content-Type: text/plain; charset=UTF-8

Hi Nilesh,

That change from Matei to change (Key, Seq[Value]) into (Key,
Iterable[Value]) was to enable the optimization in future releases without
breaking the API.  Currently though, all values on a single key are still
held in memory on a single machine.

The way I've gotten around this is by introducing another value to my Key
that goes from (Key) to (Key, randomValue % 10) for example.  Using this
you can further shard an individual key and keep from holding as much data
in memory at once.  The workaround is an ugly hack, but if it works then it
works.

Hope that helps!
Andrew


On Sun, May 25, 2014 at 6:55 PM, Nilesh <nilesh@nileshc.com> wrote:

> I would like to clarify something. Matei mentioned that in Spark 1.0
> groupBy
> returns an (Key, Iterable[Value]) instead of (Key, Seq[Value]). Does this
> also automatically assure us that the whole Iterable[Value] is not in fact
> stored in memory? That is to say, with 1.0, will it be possible to do
> groupByKey().values.map(x => while(x.hasNext) ... ) assuming x :
> Iterable[Value] is larger than the RAM on a single machine? Or will this be
> possible later, in subsequent versions?
>
> Could you please propose a workaround for this for the meantime? I'm out of
> ideas.
>
> Thanks,
> Nilesh
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/all-values-for-a-key-must-fit-in-memory-tp6342p6791.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--047d7b343940fbfb5f04fa41ce57--

From dev-return-7805-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun May 25 23:28:08 2014
Return-Path: <dev-return-7805-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F0D36C550
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 25 May 2014 23:28:06 +0000 (UTC)
Received: (qmail 71918 invoked by uid 500); 25 May 2014 23:28:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71755 invoked by uid 500); 25 May 2014 23:28:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71743 invoked by uid 99); 25 May 2014 23:28:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 25 May 2014 23:28:05 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.170] (HELO mail-vc0-f170.google.com) (209.85.220.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 25 May 2014 23:28:01 +0000
Received: by mail-vc0-f170.google.com with SMTP id lf12so8444324vcb.15
        for <dev@spark.apache.org>; Sun, 25 May 2014 16:27:41 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=/CYOq+3gLDkMDvXBZMAmNY5GcX+pNMz5vqsPDcQWb14=;
        b=QnBlv+j3y5+oakXkxQB0hF64hIl+xAsuiDJkkJMcAeg1qs9ZgK6nyxuLGwInMMPH3N
         vme0h4UYVQKdJd4aI15AQ2GXwM+4fadx/3ljBSOQaPjp/XSlw9o1bNtDKpz3+27aDt2+
         Aoo1EvNti/oHZoxxKTjzWVGxEU4dTZpPHe8PnQ7rUxePxNZ4kdLqmVxXzY6E2jQvGwkY
         nKwTa8pOAda1VOJmvuh8+uffH+UZxI+efdtWJ+eydrsxvbfZ3SO8qm7fWmw/fRe1izVC
         KeygFVSZKmCwu4lCPkEwoULYsrc6A0xda2s6nIO7HfkDe7mserVCv/YnwojuEo74JaZl
         ftlg==
X-Gm-Message-State: ALoCoQm/w507Slcw7TEJXsR3lj2cwMyaNXZ+GIjJhWQN7cRvp2D3Z1TuGn7uyOaXr9PSqxwLVuMf
X-Received: by 10.58.136.168 with SMTP id qb8mr17691350veb.21.1401060460750;
        Sun, 25 May 2014 16:27:40 -0700 (PDT)
Received: from mail-ve0-f174.google.com (mail-ve0-f174.google.com [209.85.128.174])
        by mx.google.com with ESMTPSA id qh2sm14704420vdb.4.2014.05.25.16.27.39
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 25 May 2014 16:27:39 -0700 (PDT)
Received: by mail-ve0-f174.google.com with SMTP id jw12so8509639veb.19
        for <dev@spark.apache.org>; Sun, 25 May 2014 16:27:39 -0700 (PDT)
X-Received: by 10.220.139.198 with SMTP id f6mr3088437vcu.47.1401060459279;
 Sun, 25 May 2014 16:27:39 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Sun, 25 May 2014 16:27:19 -0700 (PDT)
In-Reply-To: <1401058549889-6791.post@n3.nabble.com>
References: <CACBYxKKWvS5FpVyWSDJG=o3xgDm2XabwNHYuzxRg8CmOV7+K8A@mail.gmail.com>
 <CAJiQeY+=QUjcF4vKckRnvv=G_k-QTmZ_0CCUHF=amyVjUfjNrw@mail.gmail.com>
 <CACBYxK+b_JyBt-D20+UUz2Sdwift0w6NaJ0_yQM75KRHDsVPEQ@mail.gmail.com>
 <FBFD63BC-23CC-44D2-A114-5BE07B641A82@gmail.com> <1401058549889-6791.post@n3.nabble.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Sun, 25 May 2014 19:27:19 -0400
Message-ID: <CA+-p3AGS6Bkvp5q4WOZ82UYtyooeFfxP5aBgoBCFhMEP6WLa1A@mail.gmail.com>
Subject: Re: all values for a key must fit in memory
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=047d7b343940fbfb5f04fa41ce57
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b343940fbfb5f04fa41ce57
Content-Type: text/plain; charset=UTF-8

Hi Nilesh,

That change from Matei to change (Key, Seq[Value]) into (Key,
Iterable[Value]) was to enable the optimization in future releases without
breaking the API.  Currently though, all values on a single key are still
held in memory on a single machine.

The way I've gotten around this is by introducing another value to my Key
that goes from (Key) to (Key, randomValue % 10) for example.  Using this
you can further shard an individual key and keep from holding as much data
in memory at once.  The workaround is an ugly hack, but if it works then it
works.

Hope that helps!
Andrew


On Sun, May 25, 2014 at 6:55 PM, Nilesh <nilesh@nileshc.com> wrote:

> I would like to clarify something. Matei mentioned that in Spark 1.0
> groupBy
> returns an (Key, Iterable[Value]) instead of (Key, Seq[Value]). Does this
> also automatically assure us that the whole Iterable[Value] is not in fact
> stored in memory? That is to say, with 1.0, will it be possible to do
> groupByKey().values.map(x => while(x.hasNext) ... ) assuming x :
> Iterable[Value] is larger than the RAM on a single machine? Or will this be
> possible later, in subsequent versions?
>
> Could you please propose a workaround for this for the meantime? I'm out of
> ideas.
>
> Thanks,
> Nilesh
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/all-values-for-a-key-must-fit-in-memory-tp6342p6791.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--047d7b343940fbfb5f04fa41ce57--

From dev-return-7806-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 26 00:40:07 2014
Return-Path: <dev-return-7806-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 332D8C66B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 26 May 2014 00:40:07 +0000 (UTC)
Received: (qmail 8947 invoked by uid 500); 26 May 2014 00:40:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8884 invoked by uid 500); 26 May 2014 00:40:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8875 invoked by uid 99); 26 May 2014 00:40:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 May 2014 00:40:06 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of shihaoliang@huawei.com designates 119.145.14.66 as permitted sender)
Received: from [119.145.14.66] (HELO szxga03-in.huawei.com) (119.145.14.66)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 May 2014 00:40:02 +0000
Received: from 172.24.2.119 (EHLO szxeml211-edg.china.huawei.com) ([172.24.2.119])
	by szxrg03-dlp.huawei.com (MOS 4.4.3-GA FastPath queued)
	with ESMTP id APD03730;
	Mon, 26 May 2014 08:39:36 +0800 (CST)
Received: from SZXEML417-HUB.china.huawei.com (10.82.67.156) by
 szxeml211-edg.china.huawei.com (172.24.2.182) with Microsoft SMTP Server
 (TLS) id 14.3.158.1; Mon, 26 May 2014 08:39:36 +0800
Received: from NKGEML406-HUB.china.huawei.com (10.98.56.37) by
 szxeml417-hub.china.huawei.com (10.82.67.156) with Microsoft SMTP Server
 (TLS) id 14.3.158.1; Mon, 26 May 2014 08:39:35 +0800
Received: from NKGEML511-MBX.china.huawei.com ([169.254.5.36]) by
 nkgeml406-hub.china.huawei.com ([10.98.56.37]) with mapi id 14.03.0158.001;
 Mon, 26 May 2014 08:39:32 +0800
From: "Shihaoliang (Shihaoliang)" <shihaoliang@huawei.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: credential transfer question
Thread-Topic: credential transfer question
Thread-Index: Ac94evSZh4mXYzQ4TJ+uNFDTY9l6xg==
Date: Mon, 26 May 2014 00:39:30 +0000
Message-ID: <FE502ABB7E0C0F48B64DA686C8210F19561C35AB@nkgeml511-mbx.china.huawei.com>
Accept-Language: zh-CN, en-US
Content-Language: zh-CN
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [10.177.16.59]
Content-Type: multipart/alternative;
	boundary="_000_FE502ABB7E0C0F48B64DA686C8210F19561C35ABnkgeml511mbxchi_"
MIME-Version: 1.0
X-CFilter-Loop: Reflected
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_FE502ABB7E0C0F48B64DA686C8210F19561C35ABnkgeml511mbxchi_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable



Hi,



I have view the code about UGI in spark. If spark interactive with kerberos=
 HDFS, The spark will apply delegate token in scheduler side, and stored as=
 credential into the UGI; And the credential will be transferred to spark e=
xecutor so that they can authenticate the HDFS. My question is how does UGI=
 do these credential transfer? does the credential transferred in encrypted=
 way?



Thanks in advance.

Peter Shi


--_000_FE502ABB7E0C0F48B64DA686C8210F19561C35ABnkgeml511mbxchi_--

From dev-return-7807-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 26 01:35:28 2014
Return-Path: <dev-return-7807-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7FC87C792
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 26 May 2014 01:35:28 +0000 (UTC)
Received: (qmail 48341 invoked by uid 500); 26 May 2014 01:35:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48279 invoked by uid 500); 26 May 2014 01:35:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48271 invoked by uid 99); 26 May 2014 01:35:28 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 May 2014 01:35:28 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 May 2014 01:35:25 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <nilesh@nileshc.com>)
	id 1Wojob-0000Z7-Fi
	for dev@spark.incubator.apache.org; Sun, 25 May 2014 18:35:01 -0700
Date: Sun, 25 May 2014 18:35:01 -0700 (PDT)
From: Nilesh <nilesh@nileshc.com>
To: dev@spark.incubator.apache.org
Message-ID: <1401068101479-6794.post@n3.nabble.com>
In-Reply-To: <CA+-p3AGS6Bkvp5q4WOZ82UYtyooeFfxP5aBgoBCFhMEP6WLa1A@mail.gmail.com>
References: <CACBYxKKWvS5FpVyWSDJG=o3xgDm2XabwNHYuzxRg8CmOV7+K8A@mail.gmail.com> <CAJiQeY+=QUjcF4vKckRnvv=G_k-QTmZ_0CCUHF=amyVjUfjNrw@mail.gmail.com> <CACBYxK+b_JyBt-D20+UUz2Sdwift0w6NaJ0_yQM75KRHDsVPEQ@mail.gmail.com> <FBFD63BC-23CC-44D2-A114-5BE07B641A82@gmail.com> <1401058549889-6791.post@n3.nabble.com> <CA+-p3AGS6Bkvp5q4WOZ82UYtyooeFfxP5aBgoBCFhMEP6WLa1A@mail.gmail.com>
Subject: Re: all values for a key must fit in memory
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Andrew,

Thanks for the reply!

It's clearer about the API part now. That's what I wanted to know.

Wow, tuples, why didn't that occur to me. That's a lovely ugly hack. :) I
also came across something that solved my real problem though - the
RDD.toLocalIterator method from 1.0, the logic of which thankfully works
with 0.9.1 too, no new API changes there.

Cheers,
Nilesh



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/all-values-for-a-key-must-fit-in-memory-tp6342p6794.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7808-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 26 02:27:16 2014
Return-Path: <dev-return-7808-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 11A18C83C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 26 May 2014 02:27:16 +0000 (UTC)
Received: (qmail 76081 invoked by uid 500); 26 May 2014 02:27:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76011 invoked by uid 500); 26 May 2014 02:27:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75995 invoked by uid 99); 26 May 2014 02:27:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 May 2014 02:27:14 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.174 as permitted sender)
Received: from [209.85.214.174] (HELO mail-ob0-f174.google.com) (209.85.214.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 May 2014 02:27:11 +0000
Received: by mail-ob0-f174.google.com with SMTP id uz6so7487307obc.5
        for <dev@spark.apache.org>; Sun, 25 May 2014 19:26:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Qb10plG5o1kOJlCCG3T0GiVybudw1Gq7hMCLUlLeVfo=;
        b=pbbLRJz9yCP+v+sQRitxy2v4TX/P9jLzYHYI0KXiSl6FhXEglFacitTZz68pcNz/08
         +205QrDIwJhlHMfiM43sYBE2uVSr8ICjUR7ot20QT3542BBrZeLdbTzW/L9GBme88mb3
         TNaMOnM375r3CD9nlhwCLTqlTmNRKYDtrD6rQdONqcnRiqXMyZGWB8o01xhYfjzc3jWx
         4/gf9daERBw3b7ICoDLd/mxSjKhHIsgMIs3JEA7u4iq3UPzcQHVLp9oR9Mowz9x3r1XI
         YcOXiOZVRC/ZBAiE4WZZEZTzTD5b0Kc30tfJuKVb8vOPjq8uVVme8O5XoxsHc0F+94vs
         4XCQ==
MIME-Version: 1.0
X-Received: by 10.60.173.228 with SMTP id bn4mr22137695oec.27.1401071210354;
 Sun, 25 May 2014 19:26:50 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Sun, 25 May 2014 19:26:50 -0700 (PDT)
In-Reply-To: <1401068101479-6794.post@n3.nabble.com>
References: <CACBYxKKWvS5FpVyWSDJG=o3xgDm2XabwNHYuzxRg8CmOV7+K8A@mail.gmail.com>
	<CAJiQeY+=QUjcF4vKckRnvv=G_k-QTmZ_0CCUHF=amyVjUfjNrw@mail.gmail.com>
	<CACBYxK+b_JyBt-D20+UUz2Sdwift0w6NaJ0_yQM75KRHDsVPEQ@mail.gmail.com>
	<FBFD63BC-23CC-44D2-A114-5BE07B641A82@gmail.com>
	<1401058549889-6791.post@n3.nabble.com>
	<CA+-p3AGS6Bkvp5q4WOZ82UYtyooeFfxP5aBgoBCFhMEP6WLa1A@mail.gmail.com>
	<1401068101479-6794.post@n3.nabble.com>
Date: Sun, 25 May 2014 19:26:50 -0700
Message-ID: <CABPQxsvyiZ89kHba4pg9Zbjm2xpDWQLYD9zBkHevSmeW_AqSXA@mail.gmail.com>
Subject: Re: all values for a key must fit in memory
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Nilesh - out of curiosity - what operation are you doing on the values
for the key?

On Sun, May 25, 2014 at 6:35 PM, Nilesh <nilesh@nileshc.com> wrote:
> Hi Andrew,
>
> Thanks for the reply!
>
> It's clearer about the API part now. That's what I wanted to know.
>
> Wow, tuples, why didn't that occur to me. That's a lovely ugly hack. :) I
> also came across something that solved my real problem though - the
> RDD.toLocalIterator method from 1.0, the logic of which thankfully works
> with 0.9.1 too, no new API changes there.
>
> Cheers,
> Nilesh
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/all-values-for-a-key-must-fit-in-memory-tp6342p6794.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7809-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 26 02:27:16 2014
Return-Path: <dev-return-7809-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2B89FC83E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 26 May 2014 02:27:16 +0000 (UTC)
Received: (qmail 76210 invoked by uid 500); 26 May 2014 02:27:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76013 invoked by uid 500); 26 May 2014 02:27:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75996 invoked by uid 99); 26 May 2014 02:27:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 May 2014 02:27:14 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.171 as permitted sender)
Received: from [209.85.214.171] (HELO mail-ob0-f171.google.com) (209.85.214.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 May 2014 02:27:11 +0000
Received: by mail-ob0-f171.google.com with SMTP id wn1so7550818obc.16
        for <dev@spark.incubator.apache.org>; Sun, 25 May 2014 19:26:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Qb10plG5o1kOJlCCG3T0GiVybudw1Gq7hMCLUlLeVfo=;
        b=pbbLRJz9yCP+v+sQRitxy2v4TX/P9jLzYHYI0KXiSl6FhXEglFacitTZz68pcNz/08
         +205QrDIwJhlHMfiM43sYBE2uVSr8ICjUR7ot20QT3542BBrZeLdbTzW/L9GBme88mb3
         TNaMOnM375r3CD9nlhwCLTqlTmNRKYDtrD6rQdONqcnRiqXMyZGWB8o01xhYfjzc3jWx
         4/gf9daERBw3b7ICoDLd/mxSjKhHIsgMIs3JEA7u4iq3UPzcQHVLp9oR9Mowz9x3r1XI
         YcOXiOZVRC/ZBAiE4WZZEZTzTD5b0Kc30tfJuKVb8vOPjq8uVVme8O5XoxsHc0F+94vs
         4XCQ==
MIME-Version: 1.0
X-Received: by 10.60.173.228 with SMTP id bn4mr22137695oec.27.1401071210354;
 Sun, 25 May 2014 19:26:50 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Sun, 25 May 2014 19:26:50 -0700 (PDT)
In-Reply-To: <1401068101479-6794.post@n3.nabble.com>
References: <CACBYxKKWvS5FpVyWSDJG=o3xgDm2XabwNHYuzxRg8CmOV7+K8A@mail.gmail.com>
	<CAJiQeY+=QUjcF4vKckRnvv=G_k-QTmZ_0CCUHF=amyVjUfjNrw@mail.gmail.com>
	<CACBYxK+b_JyBt-D20+UUz2Sdwift0w6NaJ0_yQM75KRHDsVPEQ@mail.gmail.com>
	<FBFD63BC-23CC-44D2-A114-5BE07B641A82@gmail.com>
	<1401058549889-6791.post@n3.nabble.com>
	<CA+-p3AGS6Bkvp5q4WOZ82UYtyooeFfxP5aBgoBCFhMEP6WLa1A@mail.gmail.com>
	<1401068101479-6794.post@n3.nabble.com>
Date: Sun, 25 May 2014 19:26:50 -0700
Message-ID: <CABPQxsvyiZ89kHba4pg9Zbjm2xpDWQLYD9zBkHevSmeW_AqSXA@mail.gmail.com>
Subject: Re: all values for a key must fit in memory
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Nilesh - out of curiosity - what operation are you doing on the values
for the key?

On Sun, May 25, 2014 at 6:35 PM, Nilesh <nilesh@nileshc.com> wrote:
> Hi Andrew,
>
> Thanks for the reply!
>
> It's clearer about the API part now. That's what I wanted to know.
>
> Wow, tuples, why didn't that occur to me. That's a lovely ugly hack. :) I
> also came across something that solved my real problem though - the
> RDD.toLocalIterator method from 1.0, the logic of which thankfully works
> with 0.9.1 too, no new API changes there.
>
> Cheers,
> Nilesh
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/all-values-for-a-key-must-fit-in-memory-tp6342p6794.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7810-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 26 03:14:44 2014
Return-Path: <dev-return-7810-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3D41AC9B1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 26 May 2014 03:14:44 +0000 (UTC)
Received: (qmail 27259 invoked by uid 500); 26 May 2014 03:14:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27200 invoked by uid 500); 26 May 2014 03:14:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27192 invoked by uid 99); 26 May 2014 03:14:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 May 2014 03:14:43 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 May 2014 03:14:39 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <nilesh@nileshc.com>)
	id 1WolMg-0008NY-GG
	for dev@spark.incubator.apache.org; Sun, 25 May 2014 20:14:18 -0700
Date: Sun, 25 May 2014 20:14:18 -0700 (PDT)
From: Nilesh <nilesh@nileshc.com>
To: dev@spark.incubator.apache.org
Message-ID: <1401074058497-6796.post@n3.nabble.com>
In-Reply-To: <CABPQxsvyiZ89kHba4pg9Zbjm2xpDWQLYD9zBkHevSmeW_AqSXA@mail.gmail.com>
References: <CACBYxKKWvS5FpVyWSDJG=o3xgDm2XabwNHYuzxRg8CmOV7+K8A@mail.gmail.com> <CAJiQeY+=QUjcF4vKckRnvv=G_k-QTmZ_0CCUHF=amyVjUfjNrw@mail.gmail.com> <CACBYxK+b_JyBt-D20+UUz2Sdwift0w6NaJ0_yQM75KRHDsVPEQ@mail.gmail.com> <FBFD63BC-23CC-44D2-A114-5BE07B641A82@gmail.com> <1401058549889-6791.post@n3.nabble.com> <CA+-p3AGS6Bkvp5q4WOZ82UYtyooeFfxP5aBgoBCFhMEP6WLa1A@mail.gmail.com> <1401068101479-6794.post@n3.nabble.com> <CABPQxsvyiZ89kHba4pg9Zbjm2xpDWQLYD9zBkHevSmeW_AqSXA@mail.gmail.com>
Subject: Re: all values for a key must fit in memory
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Patrick,

In this particular case, at the end of my tasks I have X different types of
keys. I need to write their values to X different files respectively. For
now I'm writing everything to the driver node's local FS.

While the number of key-value pairs can grow to millions (billions?), X is
more or less fixed at 25-30. A groupByKey followed by a map(x:
Iterable[Value] => x.foreach(destination.write(x)) would be great. But then
again, I'm not too sure about serialization issues and more likely that not
this idea would fail, but I'll try it out.

So the toLocalIterator implementation works OK for me here, though it might
turn out to be slow.

Cheers,
Nilesh

PS: Can't wait for 1.0! ^_^ Looks like it's been RC10 till now.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/all-values-for-a-key-must-fit-in-memory-tp6342p6796.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7811-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon May 26 14:39:07 2014
Return-Path: <dev-return-7811-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 422D7FB7F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 26 May 2014 14:39:07 +0000 (UTC)
Received: (qmail 40188 invoked by uid 500); 26 May 2014 14:39:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40126 invoked by uid 500); 26 May 2014 14:39:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40118 invoked by uid 99); 26 May 2014 14:39:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 May 2014 14:39:06 +0000
X-ASF-Spam-Status: No, hits=-0.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.128.177 as permitted sender)
Received: from [209.85.128.177] (HELO mail-ve0-f177.google.com) (209.85.128.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 26 May 2014 14:39:04 +0000
Received: by mail-ve0-f177.google.com with SMTP id db11so9173687veb.8
        for <dev@spark.apache.org>; Mon, 26 May 2014 07:38:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=WYEAL7fVhen72ye/U1S+8tabREjIULIEbEHw95AR1Uc=;
        b=eoefB5tTRb33TccisZr3L8kJsIXa71mW/4cLme1lbo9OwuwybebI13MHrIlZJ0CxF5
         TW/arXrCEgNuDHCHPr5KI/VW+TTnoxm7IYm13K0OZ4lR7qxe79ykV8VOd8NO6vn/7wmy
         ElK7diVGJ8gI/OJbK4bO0rKlvcQRWU9011UCsz6kssiJFO4gFc2FbKWK/p3z0Ll+qqKC
         JouAexvlyXZivscS/CS3aDLAC925U9B/W6I62lMrklpp3aIHN5U5TzZ+a9Ze0zto0Bxo
         L84+cTfhyzEU65iWFUQog1sdblJegSf0tjGhFi6HRJV30t1lCZnSBQ/yicSKehgVBzXF
         t4gA==
X-Received: by 10.58.182.129 with SMTP id ee1mr22149934vec.14.1401115120302;
 Mon, 26 May 2014 07:38:40 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.118.9 with HTTP; Mon, 26 May 2014 07:38:10 -0700 (PDT)
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Mon, 26 May 2014 07:38:10 -0700
Message-ID: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
Subject: [VOTE] Release Apache Spark 1.0.0 (RC11)
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Please vote on releasing the following candidate as Apache Spark version 1.0.0!

This has a few important bug fixes on top of rc10:
SPARK-1900 and SPARK-1918: https://github.com/apache/spark/pull/853
SPARK-1870: https://github.com/apache/spark/pull/848
SPARK-1897: https://github.com/apache/spark/pull/849

The tag to be voted on is v1.0.0-rc11 (commit c69d97cd):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=c69d97cdb42f809cb71113a1db4194c21372242a

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~tdas/spark-1.0.0-rc11/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/tdas.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1019/

The documentation corresponding to this release can be found at:
http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/

Please vote on releasing this package as Apache Spark 1.0.0!

The vote is open until Thursday, May 29, at 16:00 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.0.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== API Changes ==
We welcome users to compile Spark applications against 1.0. There are
a few API changes in this release. Here are links to the associated
upgrade guides - user facing changes have been kept as small as
possible.

Changes to ML vector specification:
http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/mllib-guide.html#from-09-to-10

Changes to the Java API:
http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark

Changes to the streaming API:
http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x

Changes to the GraphX API:
http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091

Other changes:
coGroup and related functions now return Iterable[T] instead of Seq[T]
==> Call toSeq on the result to restore the old behavior

SparkContext.jarOfClass returns Option[String] instead of Seq[String]
==> Call toSeq on the result to restore old behavior

From dev-return-7812-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 27 00:52:24 2014
Return-Path: <dev-return-7812-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7BE8610C17
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 May 2014 00:52:24 +0000 (UTC)
Received: (qmail 97211 invoked by uid 500); 27 May 2014 00:52:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97158 invoked by uid 500); 27 May 2014 00:52:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 97150 invoked by uid 99); 27 May 2014 00:52:23 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 00:52:23 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.47 as permitted sender)
Received: from [209.85.220.47] (HELO mail-pa0-f47.google.com) (209.85.220.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 00:52:20 +0000
Received: by mail-pa0-f47.google.com with SMTP id lf10so8248905pab.20
        for <dev@spark.apache.org>; Mon, 26 May 2014 17:51:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=IWMOniSTEV1QLN8krIlklbm90WDrFks31gQ7xCvyQ00=;
        b=COCF941RCJw4k4/OCkHLCBXuwXuKmB8zZbb5EmRGLBvK3HahUH130uQH1/HPUJCDM2
         C2lpmcfA5HfvPHIj8j3Mu6Sw991JSi38lyKbFaGYKKFcoWbIjRnn31skgufyTwnEeNrG
         YnD+zaZVNR483HKr8D3G5AlszQ0Xn5qQkeQCxGBMndBRsBbTrOxSEG9S+3RkqdsmZhHt
         hvSHxBgLzbQ1r1ShFEc6doabZChEBigAn2hQ8ymzt4b4AbPeKxAbwCZcp43dEza/9kW9
         rZaxyzVAnSCJdwKNs/exnhUpQee6k1PBM+RoJQLu+8BuvWiii5OQPU+lvz8hA0VZEmTo
         r6Ug==
X-Received: by 10.66.122.101 with SMTP id lr5mr31982375pab.130.1401151915855;
        Mon, 26 May 2014 17:51:55 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id iz2sm20188501pbb.95.2014.05.26.17.51.54
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 26 May 2014 17:51:54 -0700 (PDT)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
Date: Mon, 26 May 2014 17:51:51 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <40E7D8BA-8C70-4DE4-B10C-56693F09C411@gmail.com>
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.2)
X-Virus-Checked: Checked by ClamAV on apache.org

+1

Tested on Mac OS X and Windows.

Matei


On May 26, 2014, at 7:38 AM, Tathagata Das <tathagata.das1565@gmail.com> =
wrote:

> Please vote on releasing the following candidate as Apache Spark =
version 1.0.0!
>=20
> This has a few important bug fixes on top of rc10:
> SPARK-1900 and SPARK-1918: https://github.com/apache/spark/pull/853
> SPARK-1870: https://github.com/apache/spark/pull/848
> SPARK-1897: https://github.com/apache/spark/pull/849
>=20
> The tag to be voted on is v1.0.0-rc11 (commit c69d97cd):
> =
https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3Dc69d9=
7cdb42f809cb71113a1db4194c21372242a
>=20
> The release files, including signatures, digests, etc. can be found =
at:
> http://people.apache.org/~tdas/spark-1.0.0-rc11/
>=20
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/tdas.asc
>=20
> The staging repository for this release can be found at:
> =
https://repository.apache.org/content/repositories/orgapachespark-1019/
>=20
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/
>=20
> Please vote on releasing this package as Apache Spark 1.0.0!
>=20
> The vote is open until Thursday, May 29, at 16:00 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>=20
> [ ] +1 Release this package as Apache Spark 1.0.0
> [ ] -1 Do not release this package because ...
>=20
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>=20
> =3D=3D API Changes =3D=3D
> We welcome users to compile Spark applications against 1.0. There are
> a few API changes in this release. Here are links to the associated
> upgrade guides - user facing changes have been kept as small as
> possible.
>=20
> Changes to ML vector specification:
> =
http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/mllib-guide.html#from=
-09-to-10
>=20
> Changes to the Java API:
> =
http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/java-programming-guid=
e.html#upgrading-from-pre-10-versions-of-spark
>=20
> Changes to the streaming API:
> =
http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/streaming-programming=
-guide.html#migration-guide-from-091-or-below-to-1x
>=20
> Changes to the GraphX API:
> =
http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/graphx-programming-gu=
ide.html#upgrade-guide-from-spark-091
>=20
> Other changes:
> coGroup and related functions now return Iterable[T] instead of Seq[T]
> =3D=3D> Call toSeq on the result to restore the old behavior
>=20
> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> =3D=3D> Call toSeq on the result to restore old behavior


From dev-return-7813-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 27 01:12:52 2014
Return-Path: <dev-return-7813-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A976010CB5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 May 2014 01:12:52 +0000 (UTC)
Received: (qmail 19265 invoked by uid 500); 27 May 2014 01:12:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19205 invoked by uid 500); 27 May 2014 01:12:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19197 invoked by uid 99); 27 May 2014 01:12:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 01:12:52 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of nitinpanj@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 01:12:49 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <nitinpanj@gmail.com>)
	id 1Wp5wH-0004aP-OX
	for dev@spark.incubator.apache.org; Mon, 26 May 2014 18:12:25 -0700
Date: Mon, 26 May 2014 18:12:25 -0700 (PDT)
From: npanj <nitinpanj@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1401153145738-6799.post@n3.nabble.com>
Subject: Spark 1.0: outerJoinVertices seems to return null for vertex
 attributes when input was partitioned and vertex attribute type is changed
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I am seeing something strange with outerJoinVertices(and triangle count that
relies on this api):

Here is what I am doing:
1) Created a Graph with multiple partitions i.e created a graph with
minEdgePartitions(in api GraphLoader.edgeListFile), where minEdgePartitions
>=1; and use partitionBy(PartitionStrategy.RandomVertexCut) on generated
graph. Note: vertex attribute type is Int in this case
2) next I am building neighborhood ids by calling collectNeighborIds i.e.
returned vertex attribute type is Array[VertexId] ;
VertexRDD[Array[VertexId]]
3) finally join vertex ids from 2 to graph (generated in step 1) via
outerJoinVertices
4) Create a subgraph on joined graph from 3 where I only keep the edges with
ed.srcAttr != -1 && ed.dstAttr != -1 i.e. filter out null attr vertices
5) Finally checked the number edges left in subgraph from step4

I ran this program in a loop where minEdgePartitions is changed in each
iteration. When minEdgePartitions == 1 I see correct number of edges. When
minEdgePartitions == 2 result is ~1/2 number of edges; when
minEdgePartitions == 3 result is ~1/3 number of edges and so on

It seems that outerJoinVertices is returning srcAttr(and dstAtt) = nulll for
many attributes; and from numbers it seems that it might be returning null
for vertices residing on other partitions ?

Environment : I am using RC5; and 22 executers.

BUT I get correct number of edges in each iteration when I repeated my
experiment by keeping the vertex attribute type Int in step 2 (i.e. just
kept the number of vertices instead of array of vertices), which is same as
the type vertex attribute in graph before join.

Is this a know bug fixed recently? or are we supposed to set some flags when
updating the vertex attribute type? 



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-1-0-outerJoinVertices-seems-to-return-null-for-vertex-attributes-when-input-was-partitioned-and-tp6799.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7814-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 27 01:26:30 2014
Return-Path: <dev-return-7814-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9D1BB10CEA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 May 2014 01:26:30 +0000 (UTC)
Received: (qmail 36297 invoked by uid 500); 27 May 2014 01:26:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36238 invoked by uid 500); 27 May 2014 01:26:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36230 invoked by uid 99); 27 May 2014 01:26:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 01:26:29 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of nitinpanj@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 01:26:26 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <nitinpanj@gmail.com>)
	id 1Wp69S-00059J-BL
	for dev@spark.incubator.apache.org; Mon, 26 May 2014 18:26:02 -0700
Date: Mon, 26 May 2014 18:26:02 -0700 (PDT)
From: npanj <nitinpanj@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1401153962341-6800.post@n3.nabble.com>
In-Reply-To: <1401153145738-6799.post@n3.nabble.com>
References: <1401153145738-6799.post@n3.nabble.com>
Subject: Re: Spark 1.0: outerJoinVertices seems to return null for vertex
 attributes when input was partitioned and vertex attribute type is changed
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Correction: in step 4) predicate is  ed.srcAttr != null && ed.dstAttr != null
(used -1, when when changed attr type to Int )



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-1-0-outerJoinVertices-seems-to-return-null-for-vertex-attributes-when-input-was-partitioned-and-tp6799p6800.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7815-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 27 01:32:56 2014
Return-Path: <dev-return-7815-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B98A210D1A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 May 2014 01:32:56 +0000 (UTC)
Received: (qmail 42635 invoked by uid 500); 27 May 2014 01:32:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42576 invoked by uid 500); 27 May 2014 01:32:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42568 invoked by uid 99); 27 May 2014 01:32:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 01:32:56 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ankurdave@gmail.com designates 209.85.128.178 as permitted sender)
Received: from [209.85.128.178] (HELO mail-ve0-f178.google.com) (209.85.128.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 01:32:52 +0000
Received: by mail-ve0-f178.google.com with SMTP id sa20so9977285veb.37
        for <dev@spark.incubator.apache.org>; Mon, 26 May 2014 18:32:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=SJpmF835taulE8kEreBqF3lWDnHcndYMKIoQIQ3lXec=;
        b=m2Pvl3MgrnNIsBeVF8Wx4r67WzA6vxtWR7MDXnivg1SD7v6Ks7b7nAzFQY7F1b5v5o
         VGuHEw/HKfo3/sDNuub0ipOI2Snjiukvy/o5xdn0nqQ1svAhYqnNxfHeslaEVWFqGO4J
         +ysvc3NtvUj/9s/qsQEZ7+yoEWS5bsU9kr4MhMJRxVo2eFX1wQEtNunPhBUn45lV1yL9
         RMWrU7VkeF1Tl0z9b1bzde1k93iPmJikMITHZyHQbzHQFjgvBjfnEwHVFQrfa8nN+oOm
         PZK5cUTb0Ds63m/GrnE2poO0ENL/MpOWiYZPdmzjLqO/VpI0lC8ZCZsSTtx81Fd9uzDt
         LXkA==
X-Received: by 10.52.13.98 with SMTP id g2mr8785942vdc.46.1401154351539; Mon,
 26 May 2014 18:32:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.99.36 with HTTP; Mon, 26 May 2014 18:32:11 -0700 (PDT)
In-Reply-To: <1401153962341-6800.post@n3.nabble.com>
References: <1401153145738-6799.post@n3.nabble.com> <1401153962341-6800.post@n3.nabble.com>
From: Ankur Dave <ankurdave@gmail.com>
Date: Mon, 26 May 2014 18:32:11 -0700
Message-ID: <CAK1A71yXrj4P2UA6Li39cZ=-spzN8CCEuLsJuEcEre4YefMThA@mail.gmail.com>
Subject: Re: Spark 1.0: outerJoinVertices seems to return null for vertex
 attributes when input was partitioned and vertex attribute type is changed
To: dev@spark.apache.org
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=20cf302ef544662d1904fa57ab37
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf302ef544662d1904fa57ab37
Content-Type: text/plain; charset=UTF-8

This is probably due to
SPARK-1931<https://issues.apache.org/jira/browse/SPARK-1931>,
which I just fixed in PR #885 <https://github.com/apache/spark/pull/885>.
Is the problem resolved if you use the current Spark master?

Ankur <http://www.ankurdave.com/>

--20cf302ef544662d1904fa57ab37--

From dev-return-7816-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 27 01:32:58 2014
Return-Path: <dev-return-7816-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EA7BF10D1C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 May 2014 01:32:57 +0000 (UTC)
Received: (qmail 43378 invoked by uid 500); 27 May 2014 01:32:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43328 invoked by uid 500); 27 May 2014 01:32:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43320 invoked by uid 99); 27 May 2014 01:32:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 01:32:57 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ankurdave@gmail.com designates 209.85.128.170 as permitted sender)
Received: from [209.85.128.170] (HELO mail-ve0-f170.google.com) (209.85.128.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 01:32:55 +0000
Received: by mail-ve0-f170.google.com with SMTP id db11so10004636veb.29
        for <dev@spark.apache.org>; Mon, 26 May 2014 18:32:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=SJpmF835taulE8kEreBqF3lWDnHcndYMKIoQIQ3lXec=;
        b=m2Pvl3MgrnNIsBeVF8Wx4r67WzA6vxtWR7MDXnivg1SD7v6Ks7b7nAzFQY7F1b5v5o
         VGuHEw/HKfo3/sDNuub0ipOI2Snjiukvy/o5xdn0nqQ1svAhYqnNxfHeslaEVWFqGO4J
         +ysvc3NtvUj/9s/qsQEZ7+yoEWS5bsU9kr4MhMJRxVo2eFX1wQEtNunPhBUn45lV1yL9
         RMWrU7VkeF1Tl0z9b1bzde1k93iPmJikMITHZyHQbzHQFjgvBjfnEwHVFQrfa8nN+oOm
         PZK5cUTb0Ds63m/GrnE2poO0ENL/MpOWiYZPdmzjLqO/VpI0lC8ZCZsSTtx81Fd9uzDt
         LXkA==
X-Received: by 10.52.13.98 with SMTP id g2mr8785942vdc.46.1401154351539; Mon,
 26 May 2014 18:32:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.99.36 with HTTP; Mon, 26 May 2014 18:32:11 -0700 (PDT)
In-Reply-To: <1401153962341-6800.post@n3.nabble.com>
References: <1401153145738-6799.post@n3.nabble.com> <1401153962341-6800.post@n3.nabble.com>
From: Ankur Dave <ankurdave@gmail.com>
Date: Mon, 26 May 2014 18:32:11 -0700
Message-ID: <CAK1A71yXrj4P2UA6Li39cZ=-spzN8CCEuLsJuEcEre4YefMThA@mail.gmail.com>
Subject: Re: Spark 1.0: outerJoinVertices seems to return null for vertex
 attributes when input was partitioned and vertex attribute type is changed
To: dev@spark.apache.org
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=20cf302ef544662d1904fa57ab37
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf302ef544662d1904fa57ab37
Content-Type: text/plain; charset=UTF-8

This is probably due to
SPARK-1931<https://issues.apache.org/jira/browse/SPARK-1931>,
which I just fixed in PR #885 <https://github.com/apache/spark/pull/885>.
Is the problem resolved if you use the current Spark master?

Ankur <http://www.ankurdave.com/>

--20cf302ef544662d1904fa57ab37--

From dev-return-7817-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 27 01:51:16 2014
Return-Path: <dev-return-7817-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6117510DD0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 May 2014 01:51:16 +0000 (UTC)
Received: (qmail 59706 invoked by uid 500); 27 May 2014 01:51:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59645 invoked by uid 500); 27 May 2014 01:51:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59637 invoked by uid 99); 27 May 2014 01:51:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 01:51:15 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of ankurdave@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 01:51:11 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <ankurdave@gmail.com>)
	id 1Wp6XT-0006Bi-7v
	for dev@spark.incubator.apache.org; Mon, 26 May 2014 18:50:51 -0700
Date: Mon, 26 May 2014 18:50:51 -0700 (PDT)
From: ankurdave <ankurdave@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1401155451235-6802.post@n3.nabble.com>
In-Reply-To: <40E7D8BA-8C70-4DE4-B10C-56693F09C411@gmail.com>
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com> <40E7D8BA-8C70-4DE4-B10C-56693F09C411@gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

-1

I just fixed  SPARK-1931 <https://issues.apache.org/jira/browse/SPARK-1931> 
, which was a critical bug in Graph#partitionBy. Since this is an important
part of the GraphX API, I think Spark 1.0.0 should include the fix:
https://github.com/apache/spark/pull/885.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-0-0-RC11-tp6797p6802.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7818-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 27 05:47:54 2014
Return-Path: <dev-return-7818-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C8E9A102D7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 May 2014 05:47:54 +0000 (UTC)
Received: (qmail 72287 invoked by uid 500); 27 May 2014 05:47:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72237 invoked by uid 500); 27 May 2014 05:47:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72229 invoked by uid 99); 27 May 2014 05:47:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 05:47:54 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.178 as permitted sender)
Received: from [209.85.214.178] (HELO mail-ob0-f178.google.com) (209.85.214.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 05:47:50 +0000
Received: by mail-ob0-f178.google.com with SMTP id va2so8822261obc.23
        for <dev@spark.apache.org>; Mon, 26 May 2014 22:47:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=jFx2dPZiGh69tHPy5L3AsY+fBrSPECduoZ0fpUVhBhA=;
        b=kSnocYSvo3U/+h2cSWhhJzCC6NpsApSojhtySp7srCD//Qi2PTJ2oQiLIqaceEBgxo
         F8ZHo6RFFYCctGXJvnm+3IRPkwPFGfarxWxPU6bNNFQ1swapzuEbLFTi30eVhfvdC5hH
         cIYw9H60IIdTvfzLNwePzw88sJhf+6/Oor5UHWJKCxBpJinBNpwW3wGTGmDhKU8KHrmJ
         wezKNW5rPjrX2GqdEbDDqCds8YT/OW39GKA26W4CEkkOBjJMfRWT5rNkHxK67SQPLwWL
         AwDA8SOxugva9+/p1DoH+qfAGK4NP8tgMIcueKU+BDbMGHg1czufsjiY1ZrXoGTcervF
         8Ylg==
MIME-Version: 1.0
X-Received: by 10.60.44.243 with SMTP id h19mr29872863oem.46.1401169649748;
 Mon, 26 May 2014 22:47:29 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Mon, 26 May 2014 22:47:29 -0700 (PDT)
In-Reply-To: <1401155451235-6802.post@n3.nabble.com>
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
	<40E7D8BA-8C70-4DE4-B10C-56693F09C411@gmail.com>
	<1401155451235-6802.post@n3.nabble.com>
Date: Mon, 26 May 2014 22:47:29 -0700
Message-ID: <CABPQxst3qp7NzK18NdfsX1JD7fYM2dfP5Qq=X-b49kFe6u_crg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Ankur,

That does seem like a good fix, but right now we are only blocking the
release on major regressions that affect all components. So I don't
think this is sufficient to block it from going forward and cutting a
new candidate. This is because we are in the very late stage of the
release.

We can slot that for the 1.0.1 release and merge it into the 1.0
branch so people can get access to the fix easily.

On Mon, May 26, 2014 at 6:50 PM, ankurdave <ankurdave@gmail.com> wrote:
> -1
>
> I just fixed  SPARK-1931 <https://issues.apache.org/jira/browse/SPARK-1931>
> , which was a critical bug in Graph#partitionBy. Since this is an important
> part of the GraphX API, I think Spark 1.0.0 should include the fix:
> https://github.com/apache/spark/pull/885.
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apache-Spark-1-0-0-RC11-tp6797p6802.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7819-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 27 05:58:10 2014
Return-Path: <dev-return-7819-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8B95710333
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 May 2014 05:58:10 +0000 (UTC)
Received: (qmail 85052 invoked by uid 500); 27 May 2014 05:58:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84997 invoked by uid 500); 27 May 2014 05:58:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84989 invoked by uid 99); 27 May 2014 05:58:10 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 05:58:10 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.43 as permitted sender)
Received: from [209.85.160.43] (HELO mail-pb0-f43.google.com) (209.85.160.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 05:58:07 +0000
Received: by mail-pb0-f43.google.com with SMTP id up15so8714373pbc.30
        for <dev@spark.apache.org>; Mon, 26 May 2014 22:57:43 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=Psh/zktO2LmVLSY/GLYUlWwfx/fCMEfwMyK23px6YvI=;
        b=IPOMwzFFpMPj67C4Ab7qWp5qECv+WK5gWmizuFdcfeGZKWiloT5KxlXVU36TpEE53+
         aGPkWR6eJckSwAoOzoJ2KS0tDeGbwYbwx5SDgtqc+1V+IxEPUQco/wJahJ1CrbfCXfH2
         +eTg00IqJpaIMCpDv7fDjJ9r/yocNcLX5mwpibMS/GudxCDr2nqJY7U37iomT+nKcfEO
         QaUYG2z0ibr2Xqb85WjCx2qljryv4Xa5iZlNNOQV5N3A/ZtGyYkSXyMILulLPqZa06MV
         w0logYdKFI00pd4l+S7zsuBvBDwEwADD5DuaCrMcyljDsV5SULoVN7lGlnecJrqNyTKr
         ZXlg==
X-Received: by 10.66.250.166 with SMTP id zd6mr33283171pac.7.1401170263160;
        Mon, 26 May 2014 22:57:43 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id lr3sm68350579pab.4.2014.05.26.22.57.40
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 26 May 2014 22:57:41 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CABPQxst3qp7NzK18NdfsX1JD7fYM2dfP5Qq=X-b49kFe6u_crg@mail.gmail.com>
Date: Mon, 26 May 2014 22:57:39 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <45C8C94C-FF9D-4511-B988-2E89DA6B7191@gmail.com>
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com> <40E7D8BA-8C70-4DE4-B10C-56693F09C411@gmail.com> <1401155451235-6802.post@n3.nabble.com> <CABPQxst3qp7NzK18NdfsX1JD7fYM2dfP5Qq=X-b49kFe6u_crg@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.2)
X-Virus-Checked: Checked by ClamAV on apache.org

I think the question for me would be does this only happen when you call =
partitionBy, or always? And how common do you expect calls to =
partitionBy to be? If we can wait for 1.0.1 then I=92d wait on this one.

Matei


On May 26, 2014, at 10:47 PM, Patrick Wendell <pwendell@gmail.com> =
wrote:

> Hey Ankur,
>=20
> That does seem like a good fix, but right now we are only blocking the
> release on major regressions that affect all components. So I don't
> think this is sufficient to block it from going forward and cutting a
> new candidate. This is because we are in the very late stage of the
> release.
>=20
> We can slot that for the 1.0.1 release and merge it into the 1.0
> branch so people can get access to the fix easily.
>=20
> On Mon, May 26, 2014 at 6:50 PM, ankurdave <ankurdave@gmail.com> =
wrote:
>> -1
>>=20
>> I just fixed  SPARK-1931 =
<https://issues.apache.org/jira/browse/SPARK-1931>
>> , which was a critical bug in Graph#partitionBy. Since this is an =
important
>> part of the GraphX API, I think Spark 1.0.0 should include the fix:
>> https://github.com/apache/spark/pull/885.
>>=20
>>=20
>>=20
>> --
>> View this message in context: =
http://apache-spark-developers-list.1001551.n3.nabble.com/VOTE-Release-Apa=
che-Spark-1-0-0-RC11-tp6797p6802.html
>> Sent from the Apache Spark Developers List mailing list archive at =
Nabble.com.


From dev-return-7820-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 27 05:58:28 2014
Return-Path: <dev-return-7820-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CD3E310334
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 May 2014 05:58:28 +0000 (UTC)
Received: (qmail 85830 invoked by uid 500); 27 May 2014 05:58:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85773 invoked by uid 500); 27 May 2014 05:58:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85765 invoked by uid 99); 27 May 2014 05:58:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 05:58:28 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.171 as permitted sender)
Received: from [209.85.216.171] (HELO mail-qc0-f171.google.com) (209.85.216.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 05:58:25 +0000
Received: by mail-qc0-f171.google.com with SMTP id c9so5123266qcz.2
        for <dev@spark.apache.org>; Mon, 26 May 2014 22:58:01 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=2MopWYnbyliC8hr40FEz0rbkVpdZ6OlgyNWqEpGdCic=;
        b=KzM+zymVcUvQlrsRAm5QuVjPYjwztqdRmDSQxOKehLuvK+pjpQAtT2335KfIG8Pfiy
         ZOR/nFFK/12rQbGeQaTh+j2u7F7RB7NMArRXs3sPXGjqCxL4UBJKgvmfAG6SlZSfZngl
         D7RRdod0BNBM7kZ3lUa49a+R7XIn3ENQXdihh8KUsnlPa3dpTEao6PJEeNlryyGPSxjE
         qqJU4/81FgZ6/f3itv6pByGBISLxBEIX8OnG8/12pukxA0JkLuJNZ0ukqXa3saUREVbH
         ea39JBs2KF/YnL8jg1/EYfd98QnASL+nmPoDyTOFG0hfkMR14gC7MXllFBypkfXbsBlW
         fRlA==
X-Gm-Message-State: ALoCoQljhxvFiFf9ulY0K40e0wM2FiUR+neIxXCA+JFWaAHcL9zAaRbUZkvQNrSbMPmA5M0KiFwj
MIME-Version: 1.0
X-Received: by 10.140.94.179 with SMTP id g48mr36653506qge.58.1401170281678;
 Mon, 26 May 2014 22:58:01 -0700 (PDT)
Received: by 10.140.106.202 with HTTP; Mon, 26 May 2014 22:58:01 -0700 (PDT)
In-Reply-To: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
Date: Mon, 26 May 2014 22:58:01 -0700
Message-ID: <CACBYxKLDAaqV7Hc5Z03uDBeqST+x=0dsiUnDBss3QjJq_ksbDg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ab3ece8de9b04fa5b6087
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ab3ece8de9b04fa5b6087
Content-Type: text/plain; charset=UTF-8

+1


On Mon, May 26, 2014 at 7:38 AM, Tathagata Das
<tathagata.das1565@gmail.com>wrote:

> Please vote on releasing the following candidate as Apache Spark version
> 1.0.0!
>
> This has a few important bug fixes on top of rc10:
> SPARK-1900 and SPARK-1918: https://github.com/apache/spark/pull/853
> SPARK-1870 <https://github.com/apache/spark/pull/853SPARK-1870>:
> https://github.com/apache/spark/pull/848
> SPARK-1897: https://github.com/apache/spark/pull/849
>
> The tag to be voted on is v1.0.0-rc11 (commit c69d97cd):
>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=c69d97cdb42f809cb71113a1db4194c21372242a
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~tdas/spark-1.0.0-rc11/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/tdas.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1019/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/
>
> Please vote on releasing this package as Apache Spark 1.0.0!
>
> The vote is open until Thursday, May 29, at 16:00 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.0.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == API Changes ==
> We welcome users to compile Spark applications against 1.0. There are
> a few API changes in this release. Here are links to the associated
> upgrade guides - user facing changes have been kept as small as
> possible.
>
> Changes to ML vector specification:
>
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/mllib-guide.html#from-09-to-10
>
> Changes to the Java API:
>
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>
> Changes to the streaming API:
>
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>
> Changes to the GraphX API:
>
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>
> Other changes:
> coGroup and related functions now return Iterable[T] instead of Seq[T]
> ==> Call toSeq on the result to restore the old behavior
>
> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> ==> Call toSeq on the result to restore old behavior
>

--001a113ab3ece8de9b04fa5b6087--

From dev-return-7821-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 27 06:04:10 2014
Return-Path: <dev-return-7821-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1B7881036A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 May 2014 06:04:10 +0000 (UTC)
Received: (qmail 94561 invoked by uid 500); 27 May 2014 06:04:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94502 invoked by uid 500); 27 May 2014 06:04:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94494 invoked by uid 99); 27 May 2014 06:04:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 06:04:09 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of nitinpanj@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 06:04:07 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <nitinpanj@gmail.com>)
	id 1WpAUB-0008Jk-Bu
	for dev@spark.incubator.apache.org; Mon, 26 May 2014 23:03:43 -0700
Date: Mon, 26 May 2014 23:03:43 -0700 (PDT)
From: npanj <nitinpanj@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1401170623342-6806.post@n3.nabble.com>
In-Reply-To: <CAK1A71yXrj4P2UA6Li39cZ=-spzN8CCEuLsJuEcEre4YefMThA@mail.gmail.com>
References: <1401153145738-6799.post@n3.nabble.com> <1401153962341-6800.post@n3.nabble.com> <CAK1A71yXrj4P2UA6Li39cZ=-spzN8CCEuLsJuEcEre4YefMThA@mail.gmail.com>
Subject: Re: Spark 1.0: outerJoinVertices seems to return null for vertex
 attributes when input was partitioned and vertex attribute type is changed
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks Ankur. With your fix I see expected results.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-1-0-outerJoinVertices-seems-to-return-null-for-vertex-attributes-when-input-was-partitioned-and-tp6799p6806.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7822-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 27 16:27:10 2014
Return-Path: <dev-return-7822-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 249FF10A88
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 May 2014 16:27:10 +0000 (UTC)
Received: (qmail 80515 invoked by uid 500); 27 May 2014 16:27:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80453 invoked by uid 500); 27 May 2014 16:27:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80445 invoked by uid 99); 27 May 2014 16:27:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 16:27:09 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ankurdave@gmail.com designates 209.85.220.178 as permitted sender)
Received: from [209.85.220.178] (HELO mail-vc0-f178.google.com) (209.85.220.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 16:27:05 +0000
Received: by mail-vc0-f178.google.com with SMTP id ij19so7640659vcb.9
        for <dev@spark.apache.org>; Tue, 27 May 2014 09:26:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=3MQlek83y072ulpv8eBwlHZ6f1y7TqLoSMb9YrkleeI=;
        b=M+Qp1SUYAud0qtVzocIZqJWQECO4lzvPLaWS+vj7zsNcuZ8cPaAvW85NdNyR0GqFnB
         9E0T9XOAGZbuRdjFF+ojFzEh2vrboEZRTB3yhHhYgocFh3nECREOEyBbvFjrW1n5ztPq
         Am+Qx8f5tZ10ktJmbcqwsNwzTB6tl8yNt36uOycY5/MCDebWb297v264EWQtuNJSKZH0
         rzcdu3V6I5mdU4ljpp0AqcC/Fe4Cm3dmtIE26+cODF6c0wMwP0SAS3RAdJWqASgbLP9C
         glpKwKPmBFSlmLmN1lom94TFuBhQfq8frX8qSu/yBXoYht2RwcwofYzeNwWK9xeUPFYk
         dHcw==
X-Received: by 10.58.25.6 with SMTP id y6mr2429845vef.48.1401208004481; Tue,
 27 May 2014 09:26:44 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.99.36 with HTTP; Tue, 27 May 2014 09:26:24 -0700 (PDT)
In-Reply-To: <CACBYxKLDAaqV7Hc5Z03uDBeqST+x=0dsiUnDBss3QjJq_ksbDg@mail.gmail.com>
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
 <CACBYxKLDAaqV7Hc5Z03uDBeqST+x=0dsiUnDBss3QjJq_ksbDg@mail.gmail.com>
From: Ankur Dave <ankurdave@gmail.com>
Date: Tue, 27 May 2014 09:26:24 -0700
Message-ID: <CAK1A71xb+cU3yQ_mCr8b+X5tjWwmpxzp5U8K66r5Z_YC4xXojA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c3a2fc5d185f04fa642968
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3a2fc5d185f04fa642968
Content-Type: text/plain; charset=UTF-8

0

OK, I withdraw my downvote.

Ankur <http://www.ankurdave.com/>

--001a11c3a2fc5d185f04fa642968--

From dev-return-7823-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 27 22:01:34 2014
Return-Path: <dev-return-7823-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5500C10E59
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 May 2014 22:01:34 +0000 (UTC)
Received: (qmail 70418 invoked by uid 500); 27 May 2014 22:01:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70361 invoked by uid 500); 27 May 2014 22:01:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70352 invoked by uid 99); 27 May 2014 22:01:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 22:01:29 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.220.177 as permitted sender)
Received: from [209.85.220.177] (HELO mail-vc0-f177.google.com) (209.85.220.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 22:01:26 +0000
Received: by mail-vc0-f177.google.com with SMTP id hq11so5583336vcb.36
        for <dev@spark.apache.org>; Tue, 27 May 2014 15:01:02 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=fdwzx8LI5o8l5W3ujx8gJNTAossR7YYh96xSZQ4LI70=;
        b=hvjk2qPizhhMWk9SLBsmqWcESeSR/o6Xptpsc9ykIzP0E8Hmbzuv9z4kXwF9xFO7d3
         SQHcqU/Pj8IMBVUAOVNibheGZ2j3o7j6sUpVswK9AW/K33OOZ8oEiV7wKKGNHwaCvPMc
         KxhJJbNFwviUOpLRETtBDUAJV1jwO40B0h9pdLcW2qfOGPXPGCqG2Spnqln7f+DCl9nk
         SwDD5bdeWPDo0oNHMiLZnUE5P8kgce+iH0jw4WudZgkky/Obgc6SXHTEBmzBrUG3mRBD
         Jzw++7B5SxMkrgdxUKw2eFVSW/iQHhXoVjeLcZk6RrwywuyXxaN1nuwfxhJmFQWt/aVv
         ksDA==
X-Gm-Message-State: ALoCoQlixKznEAvA47nNPYID62TadxTyzSMdhdkI4qQiEG5IL6AsueoVs+o2wEELxDblJihTcxTU
X-Received: by 10.58.178.115 with SMTP id cx19mr46734vec.70.1401228062625;
 Tue, 27 May 2014 15:01:02 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.121.6 with HTTP; Tue, 27 May 2014 15:00:42 -0700 (PDT)
From: Sean Owen <sowen@cloudera.com>
Date: Tue, 27 May 2014 23:00:42 +0100
Message-ID: <CAMAsSdLzS6ihcTxepUsphRyXxA-wp26ZGBxx83sM6niRo0q4Rg@mail.gmail.com>
Subject: Kafka + Spark Streaming and NoSuchMethodError, related to Manifest / reflection?
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I'd like to resurrect this thread:
http://mail-archives.apache.org/mod_mbox/spark-user/201403.mbox/%3C6D657D19-1ECF-4E92-BF15-CC4762EF98BF@thekratos.com%3E

Basically when you call this particular Java-flavored overloading of
KafkaUtils.createStream:
https://github.com/apache/spark/blob/master/external/kafka/src/main/scala/org/apache/spark/streaming/kafka/KafkaUtils.scala#L133

... you get

java.lang.NoSuchMethodException:
java.lang.Object.<init>(kafka.utils.VerifiableProperties)
        at java.lang.Class.getConstructor0(Class.java:2763)
        at java.lang.Class.getConstructor(Class.java:1693)
        at org.apache.spark.streaming.kafka.KafkaReceiver.onStart(KafkaInputDStream.scala:108)

This doesn't appear to be a version issue. It doesn't appear when
calling other versions of this method. Other overloadings work (well,
have other issues).

Something is making it try to instantiate java.lang.Object as if it's
a Decoder class.

I am wondering about this code at
https://github.com/apache/spark/blob/master/external/kafka/src/main/scala/org/apache/spark/streaming/kafka/KafkaUtils.scala#L148

    implicit val keyCmd: Manifest[U] =
implicitly[Manifest[AnyRef]].asInstanceOf[Manifest[U]]
    implicit val valueCmd: Manifest[T] =
implicitly[Manifest[AnyRef]].asInstanceOf[Manifest[T]]

... where U and T are key/value Decoder types. I don't know enough
Scala to fully understand this, but is it possible this causes the
reflective call later to lose the type and try to instantiate Object?
The AnyRef made me wonder.

@tdas I'm hoping you might have some insight as it came in this commit
in January:
https://github.com/apache/spark/commit/aa99f226a691ddcb4442d60f4cd4908f434cc4ce

I'll file a JIRA if it's legitimate; just asking first.

From dev-return-7824-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 27 22:01:45 2014
Return-Path: <dev-return-7824-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8F48910E60
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 May 2014 22:01:45 +0000 (UTC)
Received: (qmail 71749 invoked by uid 500); 27 May 2014 22:01:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71696 invoked by uid 500); 27 May 2014 22:01:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71688 invoked by uid 99); 27 May 2014 22:01:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 22:01:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of suren.hiraman@sociocast.com designates 209.85.128.178 as permitted sender)
Received: from [209.85.128.178] (HELO mail-ve0-f178.google.com) (209.85.128.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 22:01:41 +0000
Received: by mail-ve0-f178.google.com with SMTP id sa20so11550733veb.37
        for <dev@spark.apache.org>; Tue, 27 May 2014 15:01:20 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=Fgpwg9HZh6dd07jSGVu3e/6Ti/jtWlMfoqrt1u/RroE=;
        b=NCfUgdMJ5F5RcRwfxr3fGZy4g4+5CjLjXu+IMpTJnAQhwnul9jOkIhxZWxqQI+80xR
         kkPxtxP71C4rb5bsnx6U8QKHB0i5n8SKC4oBlF7e1J9JaD1TQpmioiwxZFEXupvQDtrg
         SMiQs441Jqsb1qLaQphdPw1z5wJSCj1BboDDrYcYd+2tKBNYV7bHpizNxEHmQCGslWsV
         Yr8i/pAkWpMDMKzp7tP/HmC3TQ94gGBor7O6abzyhVz5GCboWqnh+vt5bDXIDQGW8+6L
         5aS9Vl8GuiShOdmf4u9UflZghgnZhZqDiv9uLdepmrTOKe34V2tG1UqeVprzYDBUiapy
         M3sA==
X-Gm-Message-State: ALoCoQkftUJ5xEKYDA2g+GJxfHebecrWgvvfEakcqkee+FBBusKQ6w8Woll9VWH/bSth7xjPJ8hh
MIME-Version: 1.0
X-Received: by 10.52.173.165 with SMTP id bl5mr25587125vdc.13.1401228080387;
 Tue, 27 May 2014 15:01:20 -0700 (PDT)
Received: by 10.58.31.231 with HTTP; Tue, 27 May 2014 15:01:20 -0700 (PDT)
Date: Tue, 27 May 2014 18:01:20 -0400
Message-ID: <CALWDz_ufcOm=P-hbBqecKXV=430OjwPaEeJfWRvShKPORVi+rA@mail.gmail.com>
Subject: Clearspring Analytics Version
From: Surendranauth Hiraman <suren.hiraman@velos.io>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=bcaec51b9cfdfb31db04fa68d51e
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec51b9cfdfb31db04fa68d51e
Content-Type: text/plain; charset=UTF-8

Hi,

It looks like the version of Clearspring's stream analytics class in 1.0
branch and master is 2.5

There were some significant bug fixes in 2.6 and version 2.7 is just out
now as well.

Are there any plans to upgrade?

The QDigest deserialization code in 2.5 seems to have bugs that are fixed
in 2.6 and higher.


      <dependency>
        <groupId>com.clearspring.analytics</groupId>
        <artifactId>stream</artifactId>
        <version>2.5.1</version>-
      </dependency>



SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io

--bcaec51b9cfdfb31db04fa68d51e--

From dev-return-7825-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 27 22:03:37 2014
Return-Path: <dev-return-7825-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B1BA010E8A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 May 2014 22:03:37 +0000 (UTC)
Received: (qmail 83188 invoked by uid 500); 27 May 2014 22:03:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83124 invoked by uid 500); 27 May 2014 22:03:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83116 invoked by uid 99); 27 May 2014 22:03:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 22:03:37 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.46] (HELO mail-qg0-f46.google.com) (209.85.192.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 22:03:33 +0000
Received: by mail-qg0-f46.google.com with SMTP id q108so14842122qgd.5
        for <dev@spark.apache.org>; Tue, 27 May 2014 15:03:12 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=Yy4PaRZ2RfQhIAoie84vOWa4/S2xCK268cwm1QmOSSE=;
        b=DJpFbm2ltBQSeYWUspwReNKWSeDeFqC56r1ihrARr95Zde0bmAhyYbiV00ngqdjNIo
         6pU7gLK7SPD3SrBeh7XQbB5beS0SDl90jNVbZZNnaw6fk8aB4HkTgpmyAnfMleBwt0wf
         JiUW0j2S5fPxzX3WBgb93RJt3FgvQGSIrhSsxxG1tGrSkPOyP7Dt7kjZleKpFFd/gzfl
         /EUXLPHS/57zFPhUywtgDhDQUqwbWb+YWjptloVgtI8WmVvDzJoD7RJr0N+58rUYIou8
         Uhv9Hf6WhqrXW0xBormiUOUPz6Kecq+DpD1tmoAHG5wC/K+ythq9LzBnSZ1HdPjCc7SU
         EDlg==
X-Gm-Message-State: ALoCoQk4i+zRJTjD0vc0j9WArMVUadXfmovcAiAuXDyGu0CIZWyP68JUImI1FUMSSsZo71gYXfqP
X-Received: by 10.140.26.179 with SMTP id 48mr45531937qgv.51.1401228192480;
 Tue, 27 May 2014 15:03:12 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.49.231 with HTTP; Tue, 27 May 2014 15:02:52 -0700 (PDT)
In-Reply-To: <CALWDz_ufcOm=P-hbBqecKXV=430OjwPaEeJfWRvShKPORVi+rA@mail.gmail.com>
References: <CALWDz_ufcOm=P-hbBqecKXV=430OjwPaEeJfWRvShKPORVi+rA@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 27 May 2014 15:02:52 -0700
Message-ID: <CAPh_B=YWjB4gAKF6Z7+yqRb6AxrG4zhVExS565jULsaqAPTEzA@mail.gmail.com>
Subject: Re: Clearspring Analytics Version
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c035f4a9902904fa68dcbf
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c035f4a9902904fa68dcbf
Content-Type: text/plain; charset=UTF-8

Would you like to submit a pull request to update it?

Also in the latest version HyperLogLog is serializable. That means we can
get rid of the SerializableHyperLogLog class. (and move to use
HyperLogLogPlus).






On Tue, May 27, 2014 at 3:01 PM, Surendranauth Hiraman <
suren.hiraman@velos.io> wrote:

> Hi,
>
> It looks like the version of Clearspring's stream analytics class in 1.0
> branch and master is 2.5
>
> There were some significant bug fixes in 2.6 and version 2.7 is just out
> now as well.
>
> Are there any plans to upgrade?
>
> The QDigest deserialization code in 2.5 seems to have bugs that are fixed
> in 2.6 and higher.
>
>
>       <dependency>
>         <groupId>com.clearspring.analytics</groupId>
>         <artifactId>stream</artifactId>
>         <version>2.5.1</version>-
>       </dependency>
>
>
>
> SUREN HIRAMAN, VP TECHNOLOGY
> Velos
> Accelerating Machine Learning
>
> 440 NINTH AVENUE, 11TH FLOOR
> NEW YORK, NY 10001
> O: (917) 525-2466 ext. 105
> F: 646.349.4063
> E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
> W: www.velos.io
>

--001a11c035f4a9902904fa68dcbf--

From dev-return-7826-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 27 22:11:32 2014
Return-Path: <dev-return-7826-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 320B61000D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 May 2014 22:11:32 +0000 (UTC)
Received: (qmail 10436 invoked by uid 500); 27 May 2014 22:11:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10374 invoked by uid 500); 27 May 2014 22:11:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10366 invoked by uid 99); 27 May 2014 22:11:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 22:11:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of suren.hiraman@sociocast.com designates 209.85.128.177 as permitted sender)
Received: from [209.85.128.177] (HELO mail-ve0-f177.google.com) (209.85.128.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 22:11:22 +0000
Received: by mail-ve0-f177.google.com with SMTP id db11so11388644veb.22
        for <dev@spark.apache.org>; Tue, 27 May 2014 15:11:01 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=AVHUoUP9foC+Dw1QwT++OB5FrD13/u/vbxBu3g4HOxw=;
        b=HopAWQGCPEblg5k4SJ4Mpnnv5eP9H2GhmlM1+Vhdc38o79GJoKc/jbFpCjG1asx1wp
         hH2b32XZxjNg8w4Ab0XR+AcEqz6puXb+D8Ka1ZoGTWTgXcXkU3qm0Q/IzEZjTnNQS/lq
         hThqIqSNqp1gpU2I0tK3RJdReVV0bdX5ksYCi9XA82R+gsatAIV5GIiyT/j6f1Qc0qko
         mVTMAv+M45+pGO1ochdEncor4USqcSwIuX83WTdbzqTkaxGvavQ4Pjze72jKEOkYR0CQ
         Q6IGRyADi4Ynokm9zIowM5r2zjKUsHzrCEXxEeyRhubatUQlhP+6iu59gZGPqR35zkri
         grSQ==
X-Gm-Message-State: ALoCoQluY0Lfh6KtqzVZgGu2//pvSWzASmXAigTj7iw4+FrE43N3Wv9H/Sx7BFRxC5Rvw/gcoRl6
MIME-Version: 1.0
X-Received: by 10.52.72.4 with SMTP id z4mr3281276vdu.71.1401228661670; Tue,
 27 May 2014 15:11:01 -0700 (PDT)
Received: by 10.58.31.231 with HTTP; Tue, 27 May 2014 15:11:01 -0700 (PDT)
In-Reply-To: <CAPh_B=YWjB4gAKF6Z7+yqRb6AxrG4zhVExS565jULsaqAPTEzA@mail.gmail.com>
References: <CALWDz_ufcOm=P-hbBqecKXV=430OjwPaEeJfWRvShKPORVi+rA@mail.gmail.com>
	<CAPh_B=YWjB4gAKF6Z7+yqRb6AxrG4zhVExS565jULsaqAPTEzA@mail.gmail.com>
Date: Tue, 27 May 2014 18:11:01 -0400
Message-ID: <CALWDz_vmjDnQf39HttwpEscOfdY5ptNmeyJkYJnzn4F3JRx87A@mail.gmail.com>
Subject: Re: Clearspring Analytics Version
From: Surendranauth Hiraman <suren.hiraman@velos.io>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=20cf307f3b1aa0dd4504fa68f82a
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf307f3b1aa0dd4504fa68f82a
Content-Type: text/plain; charset=UTF-8

Great, I will submit the pull request.

Any objection to version 2.7?

-Suren



On Tue, May 27, 2014 at 6:02 PM, Reynold Xin <rxin@databricks.com> wrote:

> Would you like to submit a pull request to update it?
>
> Also in the latest version HyperLogLog is serializable. That means we can
> get rid of the SerializableHyperLogLog class. (and move to use
> HyperLogLogPlus).
>
>
>
>
>
>
> On Tue, May 27, 2014 at 3:01 PM, Surendranauth Hiraman <
> suren.hiraman@velos.io> wrote:
>
> > Hi,
> >
> > It looks like the version of Clearspring's stream analytics class in 1.0
> > branch and master is 2.5
> >
> > There were some significant bug fixes in 2.6 and version 2.7 is just out
> > now as well.
> >
> > Are there any plans to upgrade?
> >
> > The QDigest deserialization code in 2.5 seems to have bugs that are fixed
> > in 2.6 and higher.
> >
> >
> >       <dependency>
> >         <groupId>com.clearspring.analytics</groupId>
> >         <artifactId>stream</artifactId>
> >         <version>2.5.1</version>-
> >       </dependency>
> >
> >
> >
> > SUREN HIRAMAN, VP TECHNOLOGY
> > Velos
> > Accelerating Machine Learning
> >
> > 440 NINTH AVENUE, 11TH FLOOR
> > NEW YORK, NY 10001
> > O: (917) 525-2466 ext. 105
> > F: 646.349.4063
> > E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
> > W: www.velos.io
> >
>



-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io

--20cf307f3b1aa0dd4504fa68f82a--

From dev-return-7827-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 27 22:13:54 2014
Return-Path: <dev-return-7827-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A790810078
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 May 2014 22:13:54 +0000 (UTC)
Received: (qmail 18333 invoked by uid 500); 27 May 2014 22:13:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18274 invoked by uid 500); 27 May 2014 22:13:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18266 invoked by uid 99); 27 May 2014 22:13:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 22:13:53 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.49] (HELO mail-qg0-f49.google.com) (209.85.192.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 22:13:51 +0000
Received: by mail-qg0-f49.google.com with SMTP id a108so14974993qge.8
        for <dev@spark.apache.org>; Tue, 27 May 2014 15:13:27 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=d1vUl4KrH5u/3MSQYQLcfYM9IfBUW7GbkVIOUZe7rBI=;
        b=JsgX4R0JiiwMhcn2WypztQ9ABwoTCRkxNCm16ArvZSrQBZAfS32zWsbQZz6UQ/t43g
         P9hQTDonAPNqjLpTNNpgVR4XcDkU7GPdlI7RGF7ErIb9d40DdMs/ZWB4GY2z1l1+lJcb
         JDy0PkIbR7MA4+jmYbLFhIiA3VmWknAF0oSSy4/0Z6V3s1VFnfuQMyigQwhmslqzKqGv
         WgMmCvOCEqvQWn/eidn8lCfY+/2PKDa0UgaPH1aa/4epXAxAoTrh51NkhpfGKr4qs34B
         5nlw/ooXb5kS469C7QWucKbsz0ZFjA7pPsVfVPCOi6CRCPUMtS9RpvPDA+kx8dmSAQuk
         o8AA==
X-Gm-Message-State: ALoCoQl10PmFBs7EZ69aQM1uYWF5xyVJCjHs9652vyQqzUn3u4dU6plsTBbwxYQrRn8bD7LedjC4
X-Received: by 10.140.97.116 with SMTP id l107mr45376522qge.19.1401228807185;
 Tue, 27 May 2014 15:13:27 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.49.231 with HTTP; Tue, 27 May 2014 15:13:07 -0700 (PDT)
In-Reply-To: <CALWDz_vmjDnQf39HttwpEscOfdY5ptNmeyJkYJnzn4F3JRx87A@mail.gmail.com>
References: <CALWDz_ufcOm=P-hbBqecKXV=430OjwPaEeJfWRvShKPORVi+rA@mail.gmail.com>
 <CAPh_B=YWjB4gAKF6Z7+yqRb6AxrG4zhVExS565jULsaqAPTEzA@mail.gmail.com> <CALWDz_vmjDnQf39HttwpEscOfdY5ptNmeyJkYJnzn4F3JRx87A@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 27 May 2014 15:13:07 -0700
Message-ID: <CAPh_B=Z7-sPe=J1MPSRxN4R5+Z8w9mZg9xMAk6qsSh_5stPbmQ@mail.gmail.com>
Subject: Re: Clearspring Analytics Version
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1138f6564d80db04fa6901b3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1138f6564d80db04fa6901b3
Content-Type: text/plain; charset=UTF-8

2.7 sounds good. I was actually waiting for 2.7 to come out to post a JIRA
(mainly for the serializable HyperLogLogPlus class).


On Tue, May 27, 2014 at 3:11 PM, Surendranauth Hiraman <
suren.hiraman@velos.io> wrote:

> Great, I will submit the pull request.
>
> Any objection to version 2.7?
>
> -Suren
>
>
>
> On Tue, May 27, 2014 at 6:02 PM, Reynold Xin <rxin@databricks.com> wrote:
>
> > Would you like to submit a pull request to update it?
> >
> > Also in the latest version HyperLogLog is serializable. That means we can
> > get rid of the SerializableHyperLogLog class. (and move to use
> > HyperLogLogPlus).
> >
> >
> >
> >
> >
> >
> > On Tue, May 27, 2014 at 3:01 PM, Surendranauth Hiraman <
> > suren.hiraman@velos.io> wrote:
> >
> > > Hi,
> > >
> > > It looks like the version of Clearspring's stream analytics class in
> 1.0
> > > branch and master is 2.5
> > >
> > > There were some significant bug fixes in 2.6 and version 2.7 is just
> out
> > > now as well.
> > >
> > > Are there any plans to upgrade?
> > >
> > > The QDigest deserialization code in 2.5 seems to have bugs that are
> fixed
> > > in 2.6 and higher.
> > >
> > >
> > >       <dependency>
> > >         <groupId>com.clearspring.analytics</groupId>
> > >         <artifactId>stream</artifactId>
> > >         <version>2.5.1</version>-
> > >       </dependency>
> > >
> > >
> > >
> > > SUREN HIRAMAN, VP TECHNOLOGY
> > > Velos
> > > Accelerating Machine Learning
> > >
> > > 440 NINTH AVENUE, 11TH FLOOR
> > > NEW YORK, NY 10001
> > > O: (917) 525-2466 ext. 105
> > > F: 646.349.4063
> > > E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
> > > W: www.velos.io
> > >
> >
>
>
>
> --
>
> SUREN HIRAMAN, VP TECHNOLOGY
> Velos
> Accelerating Machine Learning
>
> 440 NINTH AVENUE, 11TH FLOOR
> NEW YORK, NY 10001
> O: (917) 525-2466 ext. 105
> F: 646.349.4063
> E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
> W: www.velos.io
>

--001a1138f6564d80db04fa6901b3--

From dev-return-7828-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 27 22:37:16 2014
Return-Path: <dev-return-7828-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 274381047A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 May 2014 22:37:16 +0000 (UTC)
Received: (qmail 71217 invoked by uid 500); 27 May 2014 22:37:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71159 invoked by uid 500); 27 May 2014 22:37:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71151 invoked by uid 99); 27 May 2014 22:37:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 22:37:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of suren.hiraman@sociocast.com designates 209.85.220.180 as permitted sender)
Received: from [209.85.220.180] (HELO mail-vc0-f180.google.com) (209.85.220.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 22:37:12 +0000
Received: by mail-vc0-f180.google.com with SMTP id hy4so11376871vcb.39
        for <dev@spark.apache.org>; Tue, 27 May 2014 15:36:51 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=FsCu1/12qQXloaF6IQ0eA8FKAN3VfjzWbrEX6ye7+34=;
        b=Z9tIE9GT4H3LoLDvbhfDJavgf8YWfXYKWENcb3/TGmZInK9NB28tzke6lft3kET8qh
         J8TEsOUnHfQAyIeW2SD+5vrZ8IGSNqCHOaAjBpZSP0ut5C/YWDfvdVp12lqbY+u7F4Wb
         K2P2/XsR506oEWExK4iZudJtAZgeZWTvD74rqqt6VkMSzJ2K4ISXY5YeRP1Wv3a4oeF2
         bTdD6cn8YQIioWJlbIkMVS5GgUu+KaXdaSGVh5u2BKPs872YV+KOV+3Cui7BMjQvNtrG
         z08YG/KkzMHyvDNonv+yIpjDEer28XOQnMqhokH7XWgEndB+xF6gOSIcvPU6UWft7wie
         6fvA==
X-Gm-Message-State: ALoCoQl69odYM+CS6acLfKI1gZY6VEeSCNH8dSwR8Tc7QUzitypXdQGaHMgm8p+ryRJirUIVWceI
MIME-Version: 1.0
X-Received: by 10.52.2.229 with SMTP id 5mr25418226vdx.24.1401230211186; Tue,
 27 May 2014 15:36:51 -0700 (PDT)
Received: by 10.58.31.231 with HTTP; Tue, 27 May 2014 15:36:51 -0700 (PDT)
In-Reply-To: <CAPh_B=Z7-sPe=J1MPSRxN4R5+Z8w9mZg9xMAk6qsSh_5stPbmQ@mail.gmail.com>
References: <CALWDz_ufcOm=P-hbBqecKXV=430OjwPaEeJfWRvShKPORVi+rA@mail.gmail.com>
	<CAPh_B=YWjB4gAKF6Z7+yqRb6AxrG4zhVExS565jULsaqAPTEzA@mail.gmail.com>
	<CALWDz_vmjDnQf39HttwpEscOfdY5ptNmeyJkYJnzn4F3JRx87A@mail.gmail.com>
	<CAPh_B=Z7-sPe=J1MPSRxN4R5+Z8w9mZg9xMAk6qsSh_5stPbmQ@mail.gmail.com>
Date: Tue, 27 May 2014 18:36:51 -0400
Message-ID: <CALWDz_vNq=wsfpKAcvvbemafP9bUAjUKw5SPyaKuBUDrYKbNTA@mail.gmail.com>
Subject: Re: Clearspring Analytics Version
From: Surendranauth Hiraman <suren.hiraman@velos.io>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=20cf302d4ae0fcbd5304fa6954f6
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf302d4ae0fcbd5304fa6954f6
Content-Type: text/plain; charset=UTF-8

Reynold,

My environment is not quite set up to build Spark core properly and I'm
having various dependency issues running sbt/sbt assembly.

Any change you could go ahead and submit a pull request for this if it's
easy for you? :-)

-Suren



On Tue, May 27, 2014 at 6:13 PM, Reynold Xin <rxin@databricks.com> wrote:

> 2.7 sounds good. I was actually waiting for 2.7 to come out to post a JIRA
> (mainly for the serializable HyperLogLogPlus class).
>
>
> On Tue, May 27, 2014 at 3:11 PM, Surendranauth Hiraman <
> suren.hiraman@velos.io> wrote:
>
> > Great, I will submit the pull request.
> >
> > Any objection to version 2.7?
> >
> > -Suren
> >
> >
> >
> > On Tue, May 27, 2014 at 6:02 PM, Reynold Xin <rxin@databricks.com>
> wrote:
> >
> > > Would you like to submit a pull request to update it?
> > >
> > > Also in the latest version HyperLogLog is serializable. That means we
> can
> > > get rid of the SerializableHyperLogLog class. (and move to use
> > > HyperLogLogPlus).
> > >
> > >
> > >
> > >
> > >
> > >
> > > On Tue, May 27, 2014 at 3:01 PM, Surendranauth Hiraman <
> > > suren.hiraman@velos.io> wrote:
> > >
> > > > Hi,
> > > >
> > > > It looks like the version of Clearspring's stream analytics class in
> > 1.0
> > > > branch and master is 2.5
> > > >
> > > > There were some significant bug fixes in 2.6 and version 2.7 is just
> > out
> > > > now as well.
> > > >
> > > > Are there any plans to upgrade?
> > > >
> > > > The QDigest deserialization code in 2.5 seems to have bugs that are
> > fixed
> > > > in 2.6 and higher.
> > > >
> > > >
> > > >       <dependency>
> > > >         <groupId>com.clearspring.analytics</groupId>
> > > >         <artifactId>stream</artifactId>
> > > >         <version>2.5.1</version>-
> > > >       </dependency>
> > > >
> > > >
> > > >
> > > > SUREN HIRAMAN, VP TECHNOLOGY
> > > > Velos
> > > > Accelerating Machine Learning
> > > >
> > > > 440 NINTH AVENUE, 11TH FLOOR
> > > > NEW YORK, NY 10001
> > > > O: (917) 525-2466 ext. 105
> > > > F: 646.349.4063
> > > > E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
> > > > W: www.velos.io
> > > >
> > >
> >
> >
> >
> > --
> >
> > SUREN HIRAMAN, VP TECHNOLOGY
> > Velos
> > Accelerating Machine Learning
> >
> > 440 NINTH AVENUE, 11TH FLOOR
> > NEW YORK, NY 10001
> > O: (917) 525-2466 ext. 105
> > F: 646.349.4063
> > E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
> > W: www.velos.io
> >
>



-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io

--20cf302d4ae0fcbd5304fa6954f6--

From dev-return-7829-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 27 22:53:48 2014
Return-Path: <dev-return-7829-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 59425106C2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 May 2014 22:53:48 +0000 (UTC)
Received: (qmail 11617 invoked by uid 500); 27 May 2014 22:53:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11557 invoked by uid 500); 27 May 2014 22:53:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11549 invoked by uid 99); 27 May 2014 22:53:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 22:53:47 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mark@clearstorydata.com designates 74.125.82.169 as permitted sender)
Received: from [74.125.82.169] (HELO mail-we0-f169.google.com) (74.125.82.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 22:53:43 +0000
Received: by mail-we0-f169.google.com with SMTP id u56so10492702wes.0
        for <dev@spark.apache.org>; Tue, 27 May 2014 15:53:21 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=2h4iXGL9xXVdBOauoOd51y1W2/rnc04WkXe1Thq0zWE=;
        b=dXCLtApDIbjI2FLPPlhkc7lEkk79Q+boBwngHMsrqjNbrEckluT3oTqawDWgXGW3sY
         J6irmSvbgC3MNyoPL/2VrvFCpygk6jyDWcBpeblmxDmw9HVKkw/RlDuzZdrBs6gK/Icq
         KcSl1dx3SN0WczQq2ZtGrwK8xz8tnjPYbZZwLqyhyI/km9nx+BPxdS0h+RpDCy7AO028
         YpbQOc10lGGiKOPbCZyVtZ750mvMenELNyGAZGniHGUDhChC9Nnl61FJgpr+FT00zXGN
         wFd/35CuCEBd4Nli/PdqiFCoGvaJ/n16Ok4DxPK9nCfZbazsP4w8NRObL4oSuKtap0I5
         0BuA==
X-Gm-Message-State: ALoCoQmCjTRlxEcCcVC3ABJKbEtKizggDt5il0MPs7Ms1Inb8Vv1YKcnsjbfpa0Txm6md9npMXCi
MIME-Version: 1.0
X-Received: by 10.194.157.68 with SMTP id wk4mr45744949wjb.42.1401231201789;
 Tue, 27 May 2014 15:53:21 -0700 (PDT)
Received: by 10.216.161.67 with HTTP; Tue, 27 May 2014 15:53:21 -0700 (PDT)
In-Reply-To: <CAK1A71xb+cU3yQ_mCr8b+X5tjWwmpxzp5U8K66r5Z_YC4xXojA@mail.gmail.com>
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
	<CACBYxKLDAaqV7Hc5Z03uDBeqST+x=0dsiUnDBss3QjJq_ksbDg@mail.gmail.com>
	<CAK1A71xb+cU3yQ_mCr8b+X5tjWwmpxzp5U8K66r5Z_YC4xXojA@mail.gmail.com>
Date: Tue, 27 May 2014 15:53:21 -0700
Message-ID: <CAAsvFP=vMC0bfSwzY5o9ZTJVz2hAPnM2+NPT8ewXmziEv+vOxA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0112cbfc08019c04fa699079
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0112cbfc08019c04fa699079
Content-Type: text/plain; charset=UTF-8

+1


On Tue, May 27, 2014 at 9:26 AM, Ankur Dave <ankurdave@gmail.com> wrote:

> 0
>
> OK, I withdraw my downvote.
>
> Ankur <http://www.ankurdave.com/>
>

--089e0112cbfc08019c04fa699079--

From dev-return-7830-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 27 23:09:30 2014
Return-Path: <dev-return-7830-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AC0EC10892
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 May 2014 23:09:30 +0000 (UTC)
Received: (qmail 34118 invoked by uid 500); 27 May 2014 23:09:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34047 invoked by uid 500); 27 May 2014 23:09:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34038 invoked by uid 99); 27 May 2014 23:09:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 23:09:25 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of holden.karau@gmail.com designates 209.85.220.181 as permitted sender)
Received: from [209.85.220.181] (HELO mail-vc0-f181.google.com) (209.85.220.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 23:09:21 +0000
Received: by mail-vc0-f181.google.com with SMTP id hy4so8125877vcb.12
        for <dev@spark.apache.org>; Tue, 27 May 2014 16:09:00 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=vfNESvX+GKBT7KMH1wYtFElgW7Xm/10i4m0o/b7m4QI=;
        b=LU+FiRcwkm4RE1JCAWxwVIsIos2Bq6oAufA1EDSHeRSozuSIvJZk8X3xWN4a+aatKS
         RvRpDqp53GKckShyIRV3fcg0/EAUe00gNi5jXh4RyykwNevJG9U6gEvKUTe116/vLh7E
         5bTktqoG211WYN52qX56ROUGcavHylrXKhVm3QC/75BT/0hE5GosKUTjPCk68Kay2njo
         Fb1qmZyRe9Euou5zvFrb4k4V99VkpEtlFKUHKZFWvVq1Jcw51ncKIVlJ86zCbvNGm/9f
         P5leUcjW5vQQu3rr2BxTRB+A0Q2X//oI6L14cGTwjZ5sM1zzRzs4GR3VfgkffLya7Gqv
         xZpQ==
MIME-Version: 1.0
X-Received: by 10.221.37.1 with SMTP id tc1mr20774885vcb.32.1401232140636;
 Tue, 27 May 2014 16:09:00 -0700 (PDT)
Sender: holden.karau@gmail.com
Received: by 10.58.217.230 with HTTP; Tue, 27 May 2014 16:09:00 -0700 (PDT)
In-Reply-To: <CAAsvFP=vMC0bfSwzY5o9ZTJVz2hAPnM2+NPT8ewXmziEv+vOxA@mail.gmail.com>
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
	<CACBYxKLDAaqV7Hc5Z03uDBeqST+x=0dsiUnDBss3QjJq_ksbDg@mail.gmail.com>
	<CAK1A71xb+cU3yQ_mCr8b+X5tjWwmpxzp5U8K66r5Z_YC4xXojA@mail.gmail.com>
	<CAAsvFP=vMC0bfSwzY5o9ZTJVz2hAPnM2+NPT8ewXmziEv+vOxA@mail.gmail.com>
Date: Tue, 27 May 2014 16:09:00 -0700
X-Google-Sender-Auth: xnKZTNOzVZgRS2HcntbDHkXzmOs
Message-ID: <CAJLcJd-iaCEsF3OvfpY5fYgbEb0TMHw1tGFX-+8yJrRK9Rm7uA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
From: Holden Karau <holden@pigscanfly.ca>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11334c38fd981904fa69c7ff
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11334c38fd981904fa69c7ff
Content-Type: text/plain; charset=UTF-8

+1 (I did some very basic testing with PySpark & Pandas on rc11)


On Tue, May 27, 2014 at 3:53 PM, Mark Hamstra <mark@clearstorydata.com>wrote:

> +1
>
>
> On Tue, May 27, 2014 at 9:26 AM, Ankur Dave <ankurdave@gmail.com> wrote:
>
> > 0
> >
> > OK, I withdraw my downvote.
> >
> > Ankur <http://www.ankurdave.com/>
> >
>



-- 
Cell : 425-233-8271

--001a11334c38fd981904fa69c7ff--

From dev-return-7831-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue May 27 23:57:00 2014
Return-Path: <dev-return-7831-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4070610BC5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 27 May 2014 23:57:00 +0000 (UTC)
Received: (qmail 97911 invoked by uid 500); 27 May 2014 23:56:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97841 invoked by uid 500); 27 May 2014 23:56:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 97833 invoked by uid 99); 27 May 2014 23:56:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 23:56:59 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.41] (HELO mail-qg0-f41.google.com) (209.85.192.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 27 May 2014 23:56:56 +0000
Received: by mail-qg0-f41.google.com with SMTP id j5so15713944qga.28
        for <dev@spark.apache.org>; Tue, 27 May 2014 16:56:32 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=tCBxswl9D3XMrJO+i6JjW6SPNZE4xGjG/YWeWkEZn2o=;
        b=OKevNvoM5pVY+EFxFley8Gf2UbSsheXkIJ4yA+iTV0voCoJ4/aO3Oz4MkHNiJ5exEe
         /ToI9gJCOV9uBtiN++yfVdYsW08Je6oGj+BVSkg6kHuMex9SIWoFcawwc68KhA32vsJs
         aeFYVjfX2Y8MxvhiAQgwQheLd4dqGRu3LSET2dIVWiEwYmuBl2N3vUvmxMnaCMXCaBed
         g0xFqnfmDwWa7Tgi1aGnCFnP67+CeWDdmVD/EWK8M4A5et0YaRLQuYT+v9uL14UtfdGi
         XAzojFHZdgmJPQiyiAEwUc3CaY9T/I54yOw8Nku85wZIN4F2pHs3jh6Xvj6BzqwuOvZV
         8iXQ==
X-Gm-Message-State: ALoCoQmygf7MSNr2nU7D9DmBG853n0XKZ0N0POwi5N/UBJaUWpEaaYypAa7p5ZVt0sQEwKFV39gw
X-Received: by 10.224.49.67 with SMTP id u3mr47741942qaf.63.1401234992710;
 Tue, 27 May 2014 16:56:32 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.49.231 with HTTP; Tue, 27 May 2014 16:56:12 -0700 (PDT)
In-Reply-To: <CALWDz_vNq=wsfpKAcvvbemafP9bUAjUKw5SPyaKuBUDrYKbNTA@mail.gmail.com>
References: <CALWDz_ufcOm=P-hbBqecKXV=430OjwPaEeJfWRvShKPORVi+rA@mail.gmail.com>
 <CAPh_B=YWjB4gAKF6Z7+yqRb6AxrG4zhVExS565jULsaqAPTEzA@mail.gmail.com>
 <CALWDz_vmjDnQf39HttwpEscOfdY5ptNmeyJkYJnzn4F3JRx87A@mail.gmail.com>
 <CAPh_B=Z7-sPe=J1MPSRxN4R5+Z8w9mZg9xMAk6qsSh_5stPbmQ@mail.gmail.com> <CALWDz_vNq=wsfpKAcvvbemafP9bUAjUKw5SPyaKuBUDrYKbNTA@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 27 May 2014 16:56:12 -0700
Message-ID: <CAPh_B=Yc2tPTVeMQK+9d+3AkaL0qyohg3VDFyKKm1HauzDtUag@mail.gmail.com>
Subject: Re: Clearspring Analytics Version
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2ef80fcd91c04fa6a716c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ef80fcd91c04fa6a716c
Content-Type: text/plain; charset=UTF-8

Posted a question on streamlib user group about API compatibility:

https://groups.google.com/forum/#!topic/stream-lib-user/4VDeKcPiTJU


On Tue, May 27, 2014 at 3:36 PM, Surendranauth Hiraman <
suren.hiraman@velos.io> wrote:

> Reynold,
>
> My environment is not quite set up to build Spark core properly and I'm
> having various dependency issues running sbt/sbt assembly.
>
> Any change you could go ahead and submit a pull request for this if it's
> easy for you? :-)
>
> -Suren
>
>
>
> On Tue, May 27, 2014 at 6:13 PM, Reynold Xin <rxin@databricks.com> wrote:
>
> > 2.7 sounds good. I was actually waiting for 2.7 to come out to post a
> JIRA
> > (mainly for the serializable HyperLogLogPlus class).
> >
> >
> > On Tue, May 27, 2014 at 3:11 PM, Surendranauth Hiraman <
> > suren.hiraman@velos.io> wrote:
> >
> > > Great, I will submit the pull request.
> > >
> > > Any objection to version 2.7?
> > >
> > > -Suren
> > >
> > >
> > >
> > > On Tue, May 27, 2014 at 6:02 PM, Reynold Xin <rxin@databricks.com>
> > wrote:
> > >
> > > > Would you like to submit a pull request to update it?
> > > >
> > > > Also in the latest version HyperLogLog is serializable. That means we
> > can
> > > > get rid of the SerializableHyperLogLog class. (and move to use
> > > > HyperLogLogPlus).
> > > >
> > > >
> > > >
> > > >
> > > >
> > > >
> > > > On Tue, May 27, 2014 at 3:01 PM, Surendranauth Hiraman <
> > > > suren.hiraman@velos.io> wrote:
> > > >
> > > > > Hi,
> > > > >
> > > > > It looks like the version of Clearspring's stream analytics class
> in
> > > 1.0
> > > > > branch and master is 2.5
> > > > >
> > > > > There were some significant bug fixes in 2.6 and version 2.7 is
> just
> > > out
> > > > > now as well.
> > > > >
> > > > > Are there any plans to upgrade?
> > > > >
> > > > > The QDigest deserialization code in 2.5 seems to have bugs that are
> > > fixed
> > > > > in 2.6 and higher.
> > > > >
> > > > >
> > > > >       <dependency>
> > > > >         <groupId>com.clearspring.analytics</groupId>
> > > > >         <artifactId>stream</artifactId>
> > > > >         <version>2.5.1</version>-
> > > > >       </dependency>
> > > > >
> > > > >
> > > > >
> > > > > SUREN HIRAMAN, VP TECHNOLOGY
> > > > > Velos
> > > > > Accelerating Machine Learning
> > > > >
> > > > > 440 NINTH AVENUE, 11TH FLOOR
> > > > > NEW YORK, NY 10001
> > > > > O: (917) 525-2466 ext. 105
> > > > > F: 646.349.4063
> > > > > E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
> > > > > W: www.velos.io
> > > > >
> > > >
> > >
> > >
> > >
> > > --
> > >
> > > SUREN HIRAMAN, VP TECHNOLOGY
> > > Velos
> > > Accelerating Machine Learning
> > >
> > > 440 NINTH AVENUE, 11TH FLOOR
> > > NEW YORK, NY 10001
> > > O: (917) 525-2466 ext. 105
> > > F: 646.349.4063
> > > E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
> > > W: www.velos.io
> > >
> >
>
>
>
> --
>
> SUREN HIRAMAN, VP TECHNOLOGY
> Velos
> Accelerating Machine Learning
>
> 440 NINTH AVENUE, 11TH FLOOR
> NEW YORK, NY 10001
> O: (917) 525-2466 ext. 105
> F: 646.349.4063
> E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
> W: www.velos.io
>

--001a11c2ef80fcd91c04fa6a716c--

From dev-return-7832-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 28 00:25:27 2014
Return-Path: <dev-return-7832-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6AE2110E93
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 May 2014 00:25:27 +0000 (UTC)
Received: (qmail 50638 invoked by uid 500); 28 May 2014 00:25:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50585 invoked by uid 500); 28 May 2014 00:25:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50577 invoked by uid 99); 28 May 2014 00:25:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 00:25:27 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE
X-Spam-Check-By: apache.org
Received-SPF: unknown (nike.apache.org: error in processing during lookup of taeyun.kim@innowireless.co.kr)
Received: from [59.12.193.79] (HELO mail.innowireless.co.kr) (59.12.193.79)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 00:25:21 +0000
Received: from INNOC358 (218.154.28.162) by mail.innowireless.co.kr
 (59.12.193.79) with Microsoft SMTP Server id 8.2.255.0; Wed, 28 May 2014
 09:16:56 +0900
From: innowireless TaeYun Kim <taeyun.kim@innowireless.co.kr>
To: <dev@spark.apache.org>
Subject: About JIRA SPARK-1825
Date: Wed, 28 May 2014 09:25:00 +0900
Message-ID: <005d01cf7a0b$431d8230$c9588690$@innowireless.co.kr>
MIME-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_000_005E_01CF7A56.B3081060"
X-Mailer: Microsoft Outlook 14.0
Thread-Index: Ac96Cec+O4pgQQG1SpKe0fiBQ1f1hg==
Content-Language: ko
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_000_005E_01CF7A56.B3081060
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: 7bit

Could somebody please review and fix
https://issues.apache.org/jira/browse/SPARK-1825 ?

It's a cross-platform issue.

I've fixed the Spark source code based on rc5 and it's working for me now.
but totally not sure whether I've done correctly, since I'm almost new to
Spark and don't know much about the source code or API of both Spark and
YARN.

 

It would be nice that the issue be resolved before the 1.0.0 release.

Without the fix, developing a Spark application on Windows will be
inconvenient.

 

I have the source code I've fixed, but I don't' know how to upload it to be
shown by other, more Spark-capable developers. (And I'm not sure that my fix
is correct)

 

Thanks in advance. 

 


------=_NextPart_000_005E_01CF7A56.B3081060--

From dev-return-7833-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 28 00:53:23 2014
Return-Path: <dev-return-7833-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BDCE41008C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 May 2014 00:53:23 +0000 (UTC)
Received: (qmail 93158 invoked by uid 500); 28 May 2014 00:53:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93089 invoked by uid 500); 28 May 2014 00:53:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93081 invoked by uid 99); 28 May 2014 00:53:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 00:53:23 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.41 as permitted sender)
Received: from [209.85.160.41] (HELO mail-pb0-f41.google.com) (209.85.160.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 00:53:18 +0000
Received: by mail-pb0-f41.google.com with SMTP id uo5so10255166pbc.0
        for <dev@spark.apache.org>; Tue, 27 May 2014 17:52:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=Y1v3poxLB6t1qr1otJ/hHBXg0r3lJyEnm7pPCdI4KjM=;
        b=DglvR9VTO4UjX9admtzcdupjL9AUGXLSh2p+RWwc3WVZLtT/odbdK8qBGKL2gww4gU
         5O3vpLR8peK7+HsnR7ehlbxcTv3MS8Cmaoyf4CYFbpwPUvyC8QogilqNmxXKkvJBH+pE
         9VjmmikuyQHDqg1n4z/YJThUI+yru3fnM8JHLgOBZ3ivjr5k24FkUaixG1A0y+kFWxo7
         LYkWzvIpcamulUM8vNjyU8mCLeJLj6GPUSC5RAgj+SmsKrARC79ChntqzU/3fG0gr8TZ
         M81QqlOQeeM7BxkeJdOWHBmhKfnQYk+XH9IyLjxv7i9Q6+K5KdiYTENHx/AfzdAi9Tcy
         /ocg==
X-Received: by 10.67.1.39 with SMTP id bd7mr41969811pad.15.1401238377655;
        Tue, 27 May 2014 17:52:57 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id iq10sm25346207pbc.14.2014.05.27.17.52.54
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 27 May 2014 17:52:55 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
Subject: Re: About JIRA SPARK-1825
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <005d01cf7a0b$431d8230$c9588690$@innowireless.co.kr>
Date: Tue, 27 May 2014 17:52:52 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <56F6D2C3-73D0-4C30-8277-D44D9A39833F@gmail.com>
References: <005d01cf7a0b$431d8230$c9588690$@innowireless.co.kr>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.2)
X-Virus-Checked: Checked by ClamAV on apache.org

Hei Taeyun, have you sent a pull request for this fix? We can review it =
there.

It=92s too late to merge anything but blockers for 1.0.0 but we can do =
it for 1.0.1 or 1.1, depending how big the patch is.

Matei

On May 27, 2014, at 5:25 PM, innowireless TaeYun Kim =
<taeyun.kim@innowireless.co.kr> wrote:

> Could somebody please review and fix
> https://issues.apache.org/jira/browse/SPARK-1825 ?
>=20
> It's a cross-platform issue.
>=20
> I've fixed the Spark source code based on rc5 and it's working for me =
now.
> but totally not sure whether I've done correctly, since I'm almost new =
to
> Spark and don't know much about the source code or API of both Spark =
and
> YARN.
>=20
>=20
>=20
> It would be nice that the issue be resolved before the 1.0.0 release.
>=20
> Without the fix, developing a Spark application on Windows will be
> inconvenient.
>=20
>=20
>=20
> I have the source code I've fixed, but I don't' know how to upload it =
to be
> shown by other, more Spark-capable developers. (And I'm not sure that =
my fix
> is correct)
>=20
>=20
>=20
> Thanks in advance.=20
>=20
>=20
>=20


From dev-return-7834-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 28 01:11:05 2014
Return-Path: <dev-return-7834-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 43E78101B7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 May 2014 01:11:05 +0000 (UTC)
Received: (qmail 28726 invoked by uid 500); 28 May 2014 01:11:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28651 invoked by uid 500); 28 May 2014 01:11:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28643 invoked by uid 99); 28 May 2014 01:11:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 01:11:04 +0000
X-ASF-Spam-Status: No, hits=0.0 required=10.0
	tests=
X-Spam-Check-By: apache.org
Received-SPF: unknown (nike.apache.org: error in processing during lookup of taeyun.kim@innowireless.co.kr)
Received: from [59.12.193.79] (HELO mail.innowireless.co.kr) (59.12.193.79)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 01:11:01 +0000
Received: from INNOC358 (218.154.28.162) by mail.innowireless.co.kr
 (59.12.193.79) with Microsoft SMTP Server id 8.2.255.0; Wed, 28 May 2014
 10:02:34 +0900
From: innowireless TaeYun Kim <taeyun.kim@innowireless.co.kr>
To: <dev@spark.apache.org>
References: <005d01cf7a0b$431d8230$c9588690$@innowireless.co.kr> <56F6D2C3-73D0-4C30-8277-D44D9A39833F@gmail.com>
In-Reply-To: <56F6D2C3-73D0-4C30-8277-D44D9A39833F@gmail.com>
Subject: RE: About JIRA SPARK-1825
Date: Wed, 28 May 2014 10:10:38 +0900
Message-ID: <007001cf7a11$a2e65330$e8b2f990$@innowireless.co.kr>
MIME-Version: 1.0
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: 7bit
X-Mailer: Microsoft Outlook 14.0
Thread-Index: AQLSxGmTWtOlCm4S7xynBt/J0E/1vgHuVls5mT8mcrA=
Content-Language: ko
X-Virus-Checked: Checked by ClamAV on apache.org

I'm afraid I don't know how to send a 'pull request'. (Sorry for my
ignorance)
I've modified the source code on my PC.
I can learn how to send a pull request, or I can attach the modified source
code on the mail right now(only 2 scala files).
Which way is preferable?


-----Original Message-----
From: Matei Zaharia [mailto:matei.zaharia@gmail.com] 
Sent: Wednesday, May 28, 2014 9:53 AM
To: dev@spark.apache.org
Subject: Re: About JIRA SPARK-1825

Hei Taeyun, have you sent a pull request for this fix? We can review it
there.

It's too late to merge anything but blockers for 1.0.0 but we can do it for
1.0.1 or 1.1, depending how big the patch is.

Matei

On May 27, 2014, at 5:25 PM, innowireless TaeYun Kim
<taeyun.kim@innowireless.co.kr> wrote:

> Could somebody please review and fix
> https://issues.apache.org/jira/browse/SPARK-1825 ?
> 
> It's a cross-platform issue.
> 
> I've fixed the Spark source code based on rc5 and it's working for me now.
> but totally not sure whether I've done correctly, since I'm almost new 
> to Spark and don't know much about the source code or API of both 
> Spark and YARN.
> 
> 
> 
> It would be nice that the issue be resolved before the 1.0.0 release.
> 
> Without the fix, developing a Spark application on Windows will be 
> inconvenient.
> 
> 
> 
> I have the source code I've fixed, but I don't' know how to upload it 
> to be shown by other, more Spark-capable developers. (And I'm not sure 
> that my fix is correct)
> 
> 
> 
> Thanks in advance. 
> 
> 
> 


From dev-return-7835-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 28 01:13:04 2014
Return-Path: <dev-return-7835-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 64D3E101E1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 May 2014 01:13:04 +0000 (UTC)
Received: (qmail 31802 invoked by uid 500); 28 May 2014 01:13:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31745 invoked by uid 500); 28 May 2014 01:13:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31737 invoked by uid 99); 28 May 2014 01:13:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 01:13:03 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.41] (HELO mail-qg0-f41.google.com) (209.85.192.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 01:12:59 +0000
Received: by mail-qg0-f41.google.com with SMTP id j5so15840601qga.14
        for <dev@spark.apache.org>; Tue, 27 May 2014 18:12:39 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=qNlzrVU3cUHDNsrAZeXqpYo4aHOPnsojqmOgzd3jnPQ=;
        b=OeTK47SFhK9iM7ZXN+BeXl/Qe8x3iOJoJpFwfraP78xGl/S255ZM01ZkL9MakxFCfi
         6vaoFLxuJioGv0B4LMBNSr3FEBwFEBUpTfU90uDBxpC2aSskl4YCzuGBoM2PwTW49NJV
         suo4tMTYHHya7DcwlqDN5373uS2wcfuHzzaa+mjT/cF5oNIJkjNHAjUhVv3npWvtSOHC
         MLGECcqJCHowcmYlFX61ru1LYl/VuUkem6hbde11UZC03TTLcQdYHQsmOSD0GxEFGIP5
         l/aXHSQ18kwX7PpxNCHFlcI6PabJyyuasErbNPmL/EahqH5geeQ5t0sYF/OnPPUiwnCW
         g5hg==
X-Gm-Message-State: ALoCoQk9M3OuDUguhYN9ujNRf2M8yqvwaVhEomJZzQVbjhmVHJDzCryFEtl0nkhZVrcftEXUCgTY
X-Received: by 10.140.26.179 with SMTP id 48mr46651572qgv.51.1401239558940;
 Tue, 27 May 2014 18:12:38 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.49.231 with HTTP; Tue, 27 May 2014 18:12:18 -0700 (PDT)
In-Reply-To: <007001cf7a11$a2e65330$e8b2f990$@innowireless.co.kr>
References: <005d01cf7a0b$431d8230$c9588690$@innowireless.co.kr>
 <56F6D2C3-73D0-4C30-8277-D44D9A39833F@gmail.com> <007001cf7a11$a2e65330$e8b2f990$@innowireless.co.kr>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 27 May 2014 18:12:18 -0700
Message-ID: <CAPh_B=bahm+76gc1vT6ijhOAOmT_XakxTDzC0KR3xdhczO0_ew@mail.gmail.com>
Subject: Re: About JIRA SPARK-1825
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c035f427faca04fa6b8236
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c035f427faca04fa6b8236
Content-Type: text/plain; charset=UTF-8

It is actually pretty simple. You will first need to fork Spark on github,
and then push your changes to it, and then follow:
https://help.github.com/articles/using-pull-requests



On Tue, May 27, 2014 at 6:10 PM, innowireless TaeYun Kim <
taeyun.kim@innowireless.co.kr> wrote:

> I'm afraid I don't know how to send a 'pull request'. (Sorry for my
> ignorance)
> I've modified the source code on my PC.
> I can learn how to send a pull request, or I can attach the modified source
> code on the mail right now(only 2 scala files).
> Which way is preferable?
>
>
> -----Original Message-----
> From: Matei Zaharia [mailto:matei.zaharia@gmail.com]
> Sent: Wednesday, May 28, 2014 9:53 AM
> To: dev@spark.apache.org
> Subject: Re: About JIRA SPARK-1825
>
> Hei Taeyun, have you sent a pull request for this fix? We can review it
> there.
>
> It's too late to merge anything but blockers for 1.0.0 but we can do it for
> 1.0.1 or 1.1, depending how big the patch is.
>
> Matei
>
> On May 27, 2014, at 5:25 PM, innowireless TaeYun Kim
> <taeyun.kim@innowireless.co.kr> wrote:
>
> > Could somebody please review and fix
> > https://issues.apache.org/jira/browse/SPARK-1825 ?
> >
> > It's a cross-platform issue.
> >
> > I've fixed the Spark source code based on rc5 and it's working for me
> now.
> > but totally not sure whether I've done correctly, since I'm almost new
> > to Spark and don't know much about the source code or API of both
> > Spark and YARN.
> >
> >
> >
> > It would be nice that the issue be resolved before the 1.0.0 release.
> >
> > Without the fix, developing a Spark application on Windows will be
> > inconvenient.
> >
> >
> >
> > I have the source code I've fixed, but I don't' know how to upload it
> > to be shown by other, more Spark-capable developers. (And I'm not sure
> > that my fix is correct)
> >
> >
> >
> > Thanks in advance.
> >
> >
> >
>
>

--001a11c035f427faca04fa6b8236--

From dev-return-7836-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 28 05:02:17 2014
Return-Path: <dev-return-7836-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A07F9101AF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 May 2014 05:02:17 +0000 (UTC)
Received: (qmail 12152 invoked by uid 500); 28 May 2014 05:02:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12097 invoked by uid 500); 28 May 2014 05:02:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12089 invoked by uid 99); 28 May 2014 05:02:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 05:02:17 +0000
X-ASF-Spam-Status: No, hits=0.0 required=10.0
	tests=
X-Spam-Check-By: apache.org
Received-SPF: unknown (nike.apache.org: error in processing during lookup of taeyun.kim@innowireless.co.kr)
Received: from [59.12.193.79] (HELO mail.innowireless.co.kr) (59.12.193.79)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 05:02:14 +0000
Received: from INNOC358 (218.154.28.162) by mail.innowireless.co.kr
 (59.12.193.79) with Microsoft SMTP Server id 8.2.255.0; Wed, 28 May 2014
 13:53:39 +0900
From: innowireless TaeYun Kim <taeyun.kim@innowireless.co.kr>
To: <dev@spark.apache.org>
References: <005d01cf7a0b$431d8230$c9588690$@innowireless.co.kr> <56F6D2C3-73D0-4C30-8277-D44D9A39833F@gmail.com> <007001cf7a11$a2e65330$e8b2f990$@innowireless.co.kr> <CAPh_B=bahm+76gc1vT6ijhOAOmT_XakxTDzC0KR3xdhczO0_ew@mail.gmail.com>
In-Reply-To: <CAPh_B=bahm+76gc1vT6ijhOAOmT_XakxTDzC0KR3xdhczO0_ew@mail.gmail.com>
Subject: RE: About JIRA SPARK-1825
Date: Wed, 28 May 2014 14:01:44 +0900
Message-ID: <000901cf7a31$ec0217a0$c40646e0$@innowireless.co.kr>
MIME-Version: 1.0
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
X-Mailer: Microsoft Outlook 14.0
Thread-Index: AQLSxGmTWtOlCm4S7xynBt/J0E/1vgHuVls5AuVfXakBqMgVSpka9obA
Content-Language: ko
X-Virus-Checked: Checked by ClamAV on apache.org

Sent a pull request. (https://github.com/apache/spark/pull/899)
Thanks.

-----Original Message-----
From: Reynold Xin [mailto:rxin@databricks.com]=20
Sent: Wednesday, May 28, 2014 10:12 AM
To: dev@spark.apache.org
Subject: Re: About JIRA SPARK-1825

It is actually pretty simple. You will first need to fork Spark on =
github, and then push your changes to it, and then follow:
https://help.github.com/articles/using-pull-requests



On Tue, May 27, 2014 at 6:10 PM, innowireless TaeYun Kim < =
taeyun.kim@innowireless.co.kr> wrote:

> I'm afraid I don't know how to send a 'pull request'. (Sorry for my
> ignorance)
> I've modified the source code on my PC.
> I can learn how to send a pull request, or I can attach the modified=20
> source code on the mail right now(only 2 scala files).
> Which way is preferable?
>
>
> -----Original Message-----
> From: Matei Zaharia [mailto:matei.zaharia@gmail.com]
> Sent: Wednesday, May 28, 2014 9:53 AM
> To: dev@spark.apache.org
> Subject: Re: About JIRA SPARK-1825
>
> Hei Taeyun, have you sent a pull request for this fix? We can review=20
> it there.
>
> It's too late to merge anything but blockers for 1.0.0 but we can do=20
> it for
> 1.0.1 or 1.1, depending how big the patch is.
>
> Matei
>
> On May 27, 2014, at 5:25 PM, innowireless TaeYun Kim=20
> <taeyun.kim@innowireless.co.kr> wrote:
>
> > Could somebody please review and fix
> > https://issues.apache.org/jira/browse/SPARK-1825 ?
> >
> > It's a cross-platform issue.
> >
> > I've fixed the Spark source code based on rc5 and it's working for=20
> > me
> now.
> > but totally not sure whether I've done correctly, since I'm almost=20
> > new to Spark and don't know much about the source code or API of=20
> > both Spark and YARN.
> >
> >
> >
> > It would be nice that the issue be resolved before the 1.0.0 =
release.
> >
> > Without the fix, developing a Spark application on Windows will be=20
> > inconvenient.
> >
> >
> >
> > I have the source code I've fixed, but I don't' know how to upload=20
> > it to be shown by other, more Spark-capable developers. (And I'm not =

> > sure that my fix is correct)
> >
> >
> >
> > Thanks in advance.
> >
> >
> >
>
>


From dev-return-7837-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 28 05:08:45 2014
Return-Path: <dev-return-7837-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 50BEC101EE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 May 2014 05:08:45 +0000 (UTC)
Received: (qmail 19810 invoked by uid 500); 28 May 2014 05:08:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19750 invoked by uid 500); 28 May 2014 05:08:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19742 invoked by uid 99); 28 May 2014 05:08:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 05:08:44 +0000
X-ASF-Spam-Status: No, hits=0.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_SOFTFAIL
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of keo@eecs.berkeley.edu does not designate 169.229.218.146 as permitted sender)
Received: from [169.229.218.146] (HELO cm05fe.IST.Berkeley.EDU) (169.229.218.146)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 05:08:39 +0000
Received: from mail-yk0-f176.google.com ([209.85.160.176])
	by cm05fe.ist.berkeley.edu with esmtpsa (TLSv1:RC4-SHA:128)
	(Exim 4.76)
	(auth plain:keo@eecs.berkeley.edu)
	(envelope-from <keo@eecs.berkeley.edu>)
	id 1WpW61-0005At-Gs
	for dev@spark.apache.org; Tue, 27 May 2014 22:08:14 -0700
Received: by mail-yk0-f176.google.com with SMTP id q9so7947594ykb.35
        for <dev@spark.apache.org>; Tue, 27 May 2014 22:08:12 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.236.20.19 with SMTP id o19mr54206305yho.41.1401253692458;
 Tue, 27 May 2014 22:08:12 -0700 (PDT)
Received: by 10.170.51.18 with HTTP; Tue, 27 May 2014 22:08:12 -0700 (PDT)
Date: Tue, 27 May 2014 22:08:12 -0700
Message-ID: <CAKJXNjGccvXb3anhYZiyohjpQBtvJCaAtCp4yfWos2odk4FmDw@mail.gmail.com>
Subject: FYI -- javax.servlet dependency issue workaround
From: Kay Ousterhout <keo@eecs.berkeley.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c1bb82944b6e04fa6ecc9a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1bb82944b6e04fa6ecc9a
Content-Type: text/plain; charset=UTF-8

Hi all,

I had some trouble compiling an application (Shark) against Spark 1.0,
where Shark had a runtime exception (at the bottom of this message) because
it couldn't find the javax.servlet classes.  SBT seemed to have trouble
downloading the servlet APIs that are dependencies of Jetty (used by the
Spark web UI), so I had to manually add them to the application's build
file:

libraryDependencies += "org.mortbay.jetty" % "servlet-api" % "3.0.20100224"

Not exactly sure why this happens but thought it might be useful in case
others run into the same problem.

-Kay

-------------

Exception in thread "main" java.lang.NoClassDefFoundError:
javax/servlet/FilterRegistration

at
org.eclipse.jetty.servlet.ServletContextHandler.<init>(ServletContextHandler.java:136)

at
org.eclipse.jetty.servlet.ServletContextHandler.<init>(ServletContextHandler.java:129)

at
org.eclipse.jetty.servlet.ServletContextHandler.<init>(ServletContextHandler.java:98)

at org.apache.spark.ui.JettyUtils$.createServletHandler(JettyUtils.scala:98)

at org.apache.spark.ui.JettyUtils$.createServletHandler(JettyUtils.scala:89)

at org.apache.spark.ui.WebUI.attachPage(WebUI.scala:64)

at org.apache.spark.ui.WebUI$anonfun$attachTab$1.apply(WebUI.scala:57)

at org.apache.spark.ui.WebUI$anonfun$attachTab$1.apply(WebUI.scala:57)

at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)

at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

at org.apache.spark.ui.WebUI.attachTab(WebUI.scala:57)

at org.apache.spark.ui.SparkUI.initialize(SparkUI.scala:66)

at org.apache.spark.ui.SparkUI.<init>(SparkUI.scala:60)

at org.apache.spark.ui.SparkUI.<init>(SparkUI.scala:42)

at org.apache.spark.SparkContext.<init>(SparkContext.scala:222)

at org.apache.spark.SparkContext.<init>(SparkContext.scala:85)

at shark.SharkContext.<init>(SharkContext.scala:42)

at shark.SharkContext.<init>(SharkContext.scala:61)

at shark.SharkEnv$.initWithSharkContext(SharkEnv.scala:78)

at shark.SharkEnv$.init(SharkEnv.scala:38)

at shark.SharkCliDriver.<init>(SharkCliDriver.scala:280)

at shark.SharkCliDriver$.main(SharkCliDriver.scala:162)

at shark.SharkCliDriver.main(SharkCliDriver.scala)

Caused by: java.lang.ClassNotFoundException:
javax.servlet.FilterRegistration

at java.net.URLClassLoader$1.run(URLClassLoader.java:366)

at java.net.URLClassLoader$1.run(URLClassLoader.java:355)

at java.security.AccessController.doPrivileged(Native Method)

at java.net.URLClassLoader.findClass(URLClassLoader.java:354)

at java.lang.ClassLoader.loadClass(ClassLoader.java:425)

at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)

at java.lang.ClassLoader.loadClass(ClassLoader.java:358)

... 23 more

--001a11c1bb82944b6e04fa6ecc9a--

From dev-return-7838-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 28 05:18:22 2014
Return-Path: <dev-return-7838-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0BD5A10233
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 May 2014 05:18:22 +0000 (UTC)
Received: (qmail 28977 invoked by uid 500); 28 May 2014 05:18:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28916 invoked by uid 500); 28 May 2014 05:18:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28908 invoked by uid 99); 28 May 2014 05:18:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 05:18:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of scrapcodes@gmail.com designates 209.85.128.179 as permitted sender)
Received: from [209.85.128.179] (HELO mail-ve0-f179.google.com) (209.85.128.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 05:18:17 +0000
Received: by mail-ve0-f179.google.com with SMTP id oy12so11734071veb.24
        for <dev@spark.apache.org>; Tue, 27 May 2014 22:17:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=9st17JC5Vh0/QVS2db39BUv5dio2yJ+BrmDTtqSvj6Q=;
        b=0R3J9nhheB7BckSQ51MqEbiE/PlabfxbRqjSN9vTnHhDpX9PLWwaC8yjZ9UvcGLlx7
         LmleUlV1zFRE+jMyw4TH5GDnZK0BtgpYSu9mOLJnOMeuFjtbngMUdcO1QdwlLwpzan5F
         TXr/rTpUpBBrS7Z6IO6LTu5zy6uWTvWWeTpuzwdLnm0jzMk3nMbHPWvbZmWPXeq0lqRn
         XXivO+gJi/QqRdx/GznnwUCxd+HugBBooGuT4aTCCr+IR13Kh7O91LIbLfk+r7L5Fxe5
         nD6HoaIkqJ1/DRm/fOEq/7vUEhAkIoUZHwdKKhOrjRNnPubEUIyKYxytlV2/uBcWzmNJ
         z8fQ==
X-Received: by 10.220.249.198 with SMTP id ml6mr6504534vcb.36.1401254276658;
 Tue, 27 May 2014 22:17:56 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.19.164 with HTTP; Tue, 27 May 2014 22:17:36 -0700 (PDT)
In-Reply-To: <CAKJXNjGccvXb3anhYZiyohjpQBtvJCaAtCp4yfWos2odk4FmDw@mail.gmail.com>
References: <CAKJXNjGccvXb3anhYZiyohjpQBtvJCaAtCp4yfWos2odk4FmDw@mail.gmail.com>
From: Prashant Sharma <scrapcodes@gmail.com>
Date: Wed, 28 May 2014 10:47:36 +0530
Message-ID: <CAOYDGoA1whrYJjk2w6Hwa2WGwDHJOTK2B3cNrUmCa4BM2cvLgA@mail.gmail.com>
Subject: Re: FYI -- javax.servlet dependency issue workaround
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01182ebe667c4704fa6eef43
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01182ebe667c4704fa6eef43
Content-Type: text/plain; charset=UTF-8

Also just for sake of completeness, sometimes the desired dependency might
just be an older version in that case even if you include it like above it
may get evicted (Sbt's default strategy for conflict manager is to choose
the latest version).

So to further ensure that it does include it. We can
libraryDependencies += "org.mortbay.jetty" % "servlet-api" % "3.0.20100224"
*force()*

force it.

Thanks

Prashant Sharma


On Wed, May 28, 2014 at 10:38 AM, Kay Ousterhout <keo@eecs.berkeley.edu>wrote:

> Hi all,
>
> I had some trouble compiling an application (Shark) against Spark 1.0,
> where Shark had a runtime exception (at the bottom of this message) because
> it couldn't find the javax.servlet classes.  SBT seemed to have trouble
> downloading the servlet APIs that are dependencies of Jetty (used by the
> Spark web UI), so I had to manually add them to the application's build
> file:
>
> libraryDependencies += "org.mortbay.jetty" % "servlet-api" % "3.0.20100224"
>
> Not exactly sure why this happens but thought it might be useful in case
> others run into the same problem.
>
> -Kay
>
> -------------
>
> Exception in thread "main" java.lang.NoClassDefFoundError:
> javax/servlet/FilterRegistration
>
> at
>
> org.eclipse.jetty.servlet.ServletContextHandler.<init>(ServletContextHandler.java:136)
>
> at
>
> org.eclipse.jetty.servlet.ServletContextHandler.<init>(ServletContextHandler.java:129)
>
> at
>
> org.eclipse.jetty.servlet.ServletContextHandler.<init>(ServletContextHandler.java:98)
>
> at
> org.apache.spark.ui.JettyUtils$.createServletHandler(JettyUtils.scala:98)
>
> at
> org.apache.spark.ui.JettyUtils$.createServletHandler(JettyUtils.scala:89)
>
> at org.apache.spark.ui.WebUI.attachPage(WebUI.scala:64)
>
> at org.apache.spark.ui.WebUI$anonfun$attachTab$1.apply(WebUI.scala:57)
>
> at org.apache.spark.ui.WebUI$anonfun$attachTab$1.apply(WebUI.scala:57)
>
> at
>
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>
> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>
> at org.apache.spark.ui.WebUI.attachTab(WebUI.scala:57)
>
> at org.apache.spark.ui.SparkUI.initialize(SparkUI.scala:66)
>
> at org.apache.spark.ui.SparkUI.<init>(SparkUI.scala:60)
>
> at org.apache.spark.ui.SparkUI.<init>(SparkUI.scala:42)
>
> at org.apache.spark.SparkContext.<init>(SparkContext.scala:222)
>
> at org.apache.spark.SparkContext.<init>(SparkContext.scala:85)
>
> at shark.SharkContext.<init>(SharkContext.scala:42)
>
> at shark.SharkContext.<init>(SharkContext.scala:61)
>
> at shark.SharkEnv$.initWithSharkContext(SharkEnv.scala:78)
>
> at shark.SharkEnv$.init(SharkEnv.scala:38)
>
> at shark.SharkCliDriver.<init>(SharkCliDriver.scala:280)
>
> at shark.SharkCliDriver$.main(SharkCliDriver.scala:162)
>
> at shark.SharkCliDriver.main(SharkCliDriver.scala)
>
> Caused by: java.lang.ClassNotFoundException:
> javax.servlet.FilterRegistration
>
> at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
>
> at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
>
> at java.security.AccessController.doPrivileged(Native Method)
>
> at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
>
> at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
>
> at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
>
> at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
>
> ... 23 more
>

--089e01182ebe667c4704fa6eef43--

From dev-return-7839-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 28 09:08:55 2014
Return-Path: <dev-return-7839-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EC3BA104BB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 May 2014 09:08:55 +0000 (UTC)
Received: (qmail 82713 invoked by uid 500); 28 May 2014 09:08:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82650 invoked by uid 500); 28 May 2014 09:08:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82640 invoked by uid 99); 28 May 2014 09:08:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 09:08:55 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.128.170 as permitted sender)
Received: from [209.85.128.170] (HELO mail-ve0-f170.google.com) (209.85.128.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 09:08:51 +0000
Received: by mail-ve0-f170.google.com with SMTP id db11so12210482veb.29
        for <dev@spark.apache.org>; Wed, 28 May 2014 02:08:30 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=TnYQtvVARlkmd1idQ4WKdrwSJ3xEAs404gvA3ppafNQ=;
        b=fQim3rk6SMoGdWxKv0fa6sYVbdz/q17ekmuOystPwdMLPjR7YQqLi7L3OTI9eemz2D
         NMPJPNq5oSMtTUA5ilRDcCTS5s/8ngJRP/HPlj3/8WI+YZIHyGtbNSjtW8XHrW8H4lsn
         9+TwE3RSPcO5iqB4HQ+yXLv69qymkjshEoESrpkzi3EE3wOoCIUxCOs6qnoAV/q2cAff
         FRW1Qlgfwx+RBbEm5REzwoyNc0hZ5Xn1hdgpg/3h5HhaoEt4OvJnzd5sOc1ZO2Sx9Qhq
         o+y+n9uSs6pslj9UVqQN2lKbQzsbqRfaF1FBD8l7/zMECZ278Yg1anbdKWaMecEaAlKs
         I0yA==
X-Gm-Message-State: ALoCoQluwsrUME7v2ZR0r+LiD5umSQaTST+GiFIpIXPbz/bxY4gmwlmCVwCEEcJcpDhfpVci/StF
X-Received: by 10.52.143.6 with SMTP id sa6mr27353559vdb.22.1401268110487;
 Wed, 28 May 2014 02:08:30 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.121.6 with HTTP; Wed, 28 May 2014 02:08:10 -0700 (PDT)
In-Reply-To: <CAKJXNjGccvXb3anhYZiyohjpQBtvJCaAtCp4yfWos2odk4FmDw@mail.gmail.com>
References: <CAKJXNjGccvXb3anhYZiyohjpQBtvJCaAtCp4yfWos2odk4FmDw@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Wed, 28 May 2014 10:08:10 +0100
Message-ID: <CAMAsSdJHgcgME0bcqyxhrEBg5n+w_fMQ4U1eg5rr51XZrNRfXg@mail.gmail.com>
Subject: Re: FYI -- javax.servlet dependency issue workaround
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

This class was introduced in Servlet 3.0. We have in the dependency
tree some references to Servlet 2.5 and Servlet 3.0. The latter is a
superset of the former. So we standardized on depending on Servlet
3.0.

At least, that seems to have been successful in the Maven build, but
this is just evidence that the SBT build ends up including Servlet 2.5
dependencies.

You shouldn't have to work around it in this way. Let me see if I can
debug and propose the right exclusion for SBT.

(Related: is the SBT build going to continue to live separately from
Maven, or is it going to be auto-generated? that is -- worth fixing
this?)


On Wed, May 28, 2014 at 6:08 AM, Kay Ousterhout <keo@eecs.berkeley.edu> wrote:
> Hi all,
>
> I had some trouble compiling an application (Shark) against Spark 1.0,
> where Shark had a runtime exception (at the bottom of this message) because
> it couldn't find the javax.servlet classes.  SBT seemed to have trouble
> downloading the servlet APIs that are dependencies of Jetty (used by the
> Spark web UI), so I had to manually add them to the application's build
> file:
>
> libraryDependencies += "org.mortbay.jetty" % "servlet-api" % "3.0.20100224"
>
> Not exactly sure why this happens but thought it might be useful in case
> others run into the same problem.
>
> -Kay
>
> -------------
>
> Exception in thread "main" java.lang.NoClassDefFoundError:
> javax/servlet/FilterRegistration
>
> at
> org.eclipse.jetty.servlet.ServletContextHandler.<init>(ServletContextHandler.java:136)
>
> at
> org.eclipse.jetty.servlet.ServletContextHandler.<init>(ServletContextHandler.java:129)
>
> at
> org.eclipse.jetty.servlet.ServletContextHandler.<init>(ServletContextHandler.java:98)
>
> at org.apache.spark.ui.JettyUtils$.createServletHandler(JettyUtils.scala:98)
>
> at org.apache.spark.ui.JettyUtils$.createServletHandler(JettyUtils.scala:89)
>
> at org.apache.spark.ui.WebUI.attachPage(WebUI.scala:64)
>
> at org.apache.spark.ui.WebUI$anonfun$attachTab$1.apply(WebUI.scala:57)
>
> at org.apache.spark.ui.WebUI$anonfun$attachTab$1.apply(WebUI.scala:57)
>
> at
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>
> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>
> at org.apache.spark.ui.WebUI.attachTab(WebUI.scala:57)
>
> at org.apache.spark.ui.SparkUI.initialize(SparkUI.scala:66)
>
> at org.apache.spark.ui.SparkUI.<init>(SparkUI.scala:60)
>
> at org.apache.spark.ui.SparkUI.<init>(SparkUI.scala:42)
>
> at org.apache.spark.SparkContext.<init>(SparkContext.scala:222)
>
> at org.apache.spark.SparkContext.<init>(SparkContext.scala:85)
>
> at shark.SharkContext.<init>(SharkContext.scala:42)
>
> at shark.SharkContext.<init>(SharkContext.scala:61)
>
> at shark.SharkEnv$.initWithSharkContext(SharkEnv.scala:78)
>
> at shark.SharkEnv$.init(SharkEnv.scala:38)
>
> at shark.SharkCliDriver.<init>(SharkCliDriver.scala:280)
>
> at shark.SharkCliDriver$.main(SharkCliDriver.scala:162)
>
> at shark.SharkCliDriver.main(SharkCliDriver.scala)
>
> Caused by: java.lang.ClassNotFoundException:
> javax.servlet.FilterRegistration
>
> at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
>
> at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
>
> at java.security.AccessController.doPrivileged(Native Method)
>
> at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
>
> at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
>
> at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
>
> at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
>
> ... 23 more

From dev-return-7840-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 28 09:47:36 2014
Return-Path: <dev-return-7840-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ADDD11065F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 May 2014 09:47:36 +0000 (UTC)
Received: (qmail 34103 invoked by uid 500); 28 May 2014 09:47:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34045 invoked by uid 500); 28 May 2014 09:47:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34037 invoked by uid 99); 28 May 2014 09:47:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 09:47:36 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nick.pentreath@gmail.com designates 209.85.219.44 as permitted sender)
Received: from [209.85.219.44] (HELO mail-oa0-f44.google.com) (209.85.219.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 09:47:32 +0000
Received: by mail-oa0-f44.google.com with SMTP id o6so10823718oag.17
        for <dev@spark.apache.org>; Wed, 28 May 2014 02:47:11 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=lbUvH7M9z7pWsS6zOaAayOh1w88hm2T2kAcWE7JSNx8=;
        b=KAwL4lBjvbrreeHaImBlDb40kPAnNCrWmceiPBpMXg0zSF10j/32SwJ6BJv+YSFxkq
         dELpJITJTYBJ5FpMVybCrp9WC2Dajg4t97hFFSLO2+qAIwOxCWaMGr7pJepygxoL0je8
         YdvFnsOTaM+0JyZAhbldUHMc2r7e9FIhKtrG4ruP4jpiZx211lvHgdG2Nb40FzUBtb2d
         0KSkgxBVKrkDqlxVseQz7LOnYGppotcz0WdUQxrHzMHcXvZAtNOw/rvjNCLzh+ikHs3N
         VOV4n+/cxjrUiJeg9N2pxq5/QPQICof4AJw1JrR9u8vk7h5YWOMlt04YgtpF/WjmJXTi
         oxIw==
MIME-Version: 1.0
X-Received: by 10.60.45.4 with SMTP id i4mr40073154oem.49.1401270431731; Wed,
 28 May 2014 02:47:11 -0700 (PDT)
Received: by 10.182.95.103 with HTTP; Wed, 28 May 2014 02:47:11 -0700 (PDT)
In-Reply-To: <CAJLcJd-iaCEsF3OvfpY5fYgbEb0TMHw1tGFX-+8yJrRK9Rm7uA@mail.gmail.com>
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
	<CACBYxKLDAaqV7Hc5Z03uDBeqST+x=0dsiUnDBss3QjJq_ksbDg@mail.gmail.com>
	<CAK1A71xb+cU3yQ_mCr8b+X5tjWwmpxzp5U8K66r5Z_YC4xXojA@mail.gmail.com>
	<CAAsvFP=vMC0bfSwzY5o9ZTJVz2hAPnM2+NPT8ewXmziEv+vOxA@mail.gmail.com>
	<CAJLcJd-iaCEsF3OvfpY5fYgbEb0TMHw1tGFX-+8yJrRK9Rm7uA@mail.gmail.com>
Date: Wed, 28 May 2014 11:47:11 +0200
Message-ID: <CALD+6GPYhbvJDgKSDqmcO19puyR+giVqwjVD+n0KGM3_iDMQQQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
From: Nick Pentreath <nick.pentreath@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0149cb9c51562204fa72b223
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149cb9c51562204fa72b223
Content-Type: text/plain; charset=UTF-8

+1
Built and tested locally on Mac OS X
Built and tested on AWS Ubuntu, with and without Hive support
Ran production jobs including MLlib and SparkSQL/HiveContext successfully


On Wed, May 28, 2014 at 1:09 AM, Holden Karau <holden@pigscanfly.ca> wrote:

> +1 (I did some very basic testing with PySpark & Pandas on rc11)
>
>
> On Tue, May 27, 2014 at 3:53 PM, Mark Hamstra <mark@clearstorydata.com
> >wrote:
>
> > +1
> >
> >
> > On Tue, May 27, 2014 at 9:26 AM, Ankur Dave <ankurdave@gmail.com> wrote:
> >
> > > 0
> > >
> > > OK, I withdraw my downvote.
> > >
> > > Ankur <http://www.ankurdave.com/>
> > >
> >
>
>
>
> --
> Cell : 425-233-8271
>

--089e0149cb9c51562204fa72b223--

From dev-return-7841-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 28 14:18:16 2014
Return-Path: <dev-return-7841-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0429BFBFE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 May 2014 14:18:16 +0000 (UTC)
Received: (qmail 17598 invoked by uid 500); 28 May 2014 14:18:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17547 invoked by uid 500); 28 May 2014 14:18:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 24445 invoked by uid 99); 28 May 2014 12:04:10 -0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of rickett.stephanie@gmail.com does not designate 216.139.236.26 as permitted sender)
Date: Wed, 28 May 2014 05:03:45 -0700 (PDT)
From: dataginjaninja <rickett.stephanie@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1401278625683-6826.post@n3.nabble.com>
Subject: Standard preprocessing/scaling
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I searched on this, but didn't find anything general so I apologize if this
has been addressed. 

Many algorithms (SGD, SVM...) either will not converge or will run forever
if the data is not scaled. Sci-kit has  preprocessing
<http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html>  
that will subtract the mean and divide by standard dev. Of course there are
a few options with it as well.

Is there something in the works for this?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Standard-preprocessing-scaling-tp6826.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7842-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 28 16:35:23 2014
Return-Path: <dev-return-7842-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4EB7D10CC2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 May 2014 16:35:23 +0000 (UTC)
Received: (qmail 55772 invoked by uid 500); 28 May 2014 16:35:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55725 invoked by uid 500); 28 May 2014 16:35:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55717 invoked by uid 99); 28 May 2014 16:35:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 16:35:22 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of wibenton@redhat.com designates 209.132.183.25 as permitted sender)
Received: from [209.132.183.25] (HELO mx4-phx2.redhat.com) (209.132.183.25)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 16:35:20 +0000
Received: from zmail14.collab.prod.int.phx2.redhat.com (zmail14.collab.prod.int.phx2.redhat.com [10.5.83.16])
	by mx4-phx2.redhat.com (8.13.8/8.13.8) with ESMTP id s4SGYsB9020502
	for <dev@spark.apache.org>; Wed, 28 May 2014 12:34:54 -0400
Date: Wed, 28 May 2014 12:34:53 -0400 (EDT)
From: Will Benton <willb@redhat.com>
To: dev@spark.apache.org
Message-ID: <1497101516.10787277.1401294893435.JavaMail.zimbra@redhat.com>
In-Reply-To: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.11]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - FF29 (Mac)/8.0.6_GA_5922)
Thread-Topic: Release Apache Spark 1.0.0 (RC11)
Thread-Index: xX/G1X/gW/V72jFlrtVY1vIeyXGcYg==
X-Virus-Checked: Checked by ClamAV on apache.org

+1

I made the necessary interface changes to my apps that use MLLib and tested all of my code against rc11 on Fedora 20 and OS X 10.9.3.  (The Fedora Rawhide package remains at 0.9.1 pending some additional dependency packaging work.)


best,
wb


----- Original Message -----
> From: "Tathagata Das" <tathagata.das1565@gmail.com>
> To: dev@spark.apache.org
> Sent: Monday, May 26, 2014 9:38:10 AM
> Subject: [VOTE] Release Apache Spark 1.0.0 (RC11)
> 
> Please vote on releasing the following candidate as Apache Spark version
> 1.0.0!
> 
> This has a few important bug fixes on top of rc10:
> SPARK-1900 and SPARK-1918: https://github.com/apache/spark/pull/853
> SPARK-1870: https://github.com/apache/spark/pull/848
> SPARK-1897: https://github.com/apache/spark/pull/849
> 
> The tag to be voted on is v1.0.0-rc11 (commit c69d97cd):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=c69d97cdb42f809cb71113a1db4194c21372242a
> 
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~tdas/spark-1.0.0-rc11/
> 
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/tdas.asc
> 
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1019/
> 
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/
> 
> Please vote on releasing this package as Apache Spark 1.0.0!
> 
> The vote is open until Thursday, May 29, at 16:00 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
> 
> [ ] +1 Release this package as Apache Spark 1.0.0
> [ ] -1 Do not release this package because ...
> 
> To learn more about Apache Spark, please see
> http://spark.apache.org/
> 
> == API Changes ==
> We welcome users to compile Spark applications against 1.0. There are
> a few API changes in this release. Here are links to the associated
> upgrade guides - user facing changes have been kept as small as
> possible.
> 
> Changes to ML vector specification:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/mllib-guide.html#from-09-to-10
> 
> Changes to the Java API:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
> 
> Changes to the streaming API:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
> 
> Changes to the GraphX API:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
> 
> Other changes:
> coGroup and related functions now return Iterable[T] instead of Seq[T]
> ==> Call toSeq on the result to restore the old behavior
> 
> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> ==> Call toSeq on the result to restore old behavior
> 

From dev-return-7843-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 28 16:49:23 2014
Return-Path: <dev-return-7843-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C141B10DBE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 May 2014 16:49:23 +0000 (UTC)
Received: (qmail 87914 invoked by uid 500); 28 May 2014 16:49:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87851 invoked by uid 500); 28 May 2014 16:49:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87843 invoked by uid 99); 28 May 2014 16:49:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 16:49:23 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of henry.saputra@gmail.com designates 74.125.82.172 as permitted sender)
Received: from [74.125.82.172] (HELO mail-we0-f172.google.com) (74.125.82.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 16:49:18 +0000
Received: by mail-we0-f172.google.com with SMTP id k48so11737381wev.17
        for <dev@spark.apache.org>; Wed, 28 May 2014 09:48:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=5NovHwXuyMH+fFiN3C0JfbqPWi5LBMInG7z7yRvQe3A=;
        b=gc0mxPfhON4i5Vh9Oz4AuhTZl2AabUOsTNRAJwSxX83+u0K5C3J36g192m6IP2vlYI
         MxSIipf1r4hqYOmdTi+yqzOr+BhHsNY8WmRgeyuw/SxrLAwEjGSkT/nW/HaVcj7uWXd8
         NoeR3s7xisCi9E2PIhW+YkaT07vDU37Z3FMrKA0kQvzZN0zoenHCzpoW5A0dOMkmmozN
         4Kzvt7AZ24L06B2LC53vrHE1JBXwEWnw8gdan+eq6edHtiyF7bYRFOYTgqdPCzGeGyhC
         Y/iI54GpMjjLxD6qeRGwIixk1cfVZ/ACibptXw+P+sEFYnhysvexnAFv+yLcsnwCnWiB
         9M+g==
MIME-Version: 1.0
X-Received: by 10.180.72.136 with SMTP id d8mr49618952wiv.36.1401295736880;
 Wed, 28 May 2014 09:48:56 -0700 (PDT)
Received: by 10.216.165.71 with HTTP; Wed, 28 May 2014 09:48:56 -0700 (PDT)
In-Reply-To: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
Date: Wed, 28 May 2014 09:48:56 -0700
Message-ID: <CALuGr6as48f3T5eRd2s5QFTh+jwngfRyMp-FQ5+zjQ29Lnp7dg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
From: Henry Saputra <henry.saputra@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

NOTICE and LICENSE files look good
Signatures look good.
Hashes look good
No external executables in the source distributions
Source compiled with sbt
Run local and standalone examples look good.

+1


- Henry

On Mon, May 26, 2014 at 7:38 AM, Tathagata Das
<tathagata.das1565@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.0.0!
>
> This has a few important bug fixes on top of rc10:
> SPARK-1900 and SPARK-1918: https://github.com/apache/spark/pull/853
> SPARK-1870: https://github.com/apache/spark/pull/848
> SPARK-1897: https://github.com/apache/spark/pull/849
>
> The tag to be voted on is v1.0.0-rc11 (commit c69d97cd):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=c69d97cdb42f809cb71113a1db4194c21372242a
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~tdas/spark-1.0.0-rc11/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/tdas.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1019/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/
>
> Please vote on releasing this package as Apache Spark 1.0.0!
>
> The vote is open until Thursday, May 29, at 16:00 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.0.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == API Changes ==
> We welcome users to compile Spark applications against 1.0. There are
> a few API changes in this release. Here are links to the associated
> upgrade guides - user facing changes have been kept as small as
> possible.
>
> Changes to ML vector specification:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/mllib-guide.html#from-09-to-10
>
> Changes to the Java API:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>
> Changes to the streaming API:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>
> Changes to the GraphX API:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>
> Other changes:
> coGroup and related functions now return Iterable[T] instead of Seq[T]
> ==> Call toSeq on the result to restore the old behavior
>
> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> ==> Call toSeq on the result to restore old behavior

From dev-return-7844-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 28 18:19:13 2014
Return-Path: <dev-return-7844-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8F4F610715
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 May 2014 18:19:13 +0000 (UTC)
Received: (qmail 30818 invoked by uid 500); 28 May 2014 18:19:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30762 invoked by uid 500); 28 May 2014 18:19:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30754 invoked by uid 99); 28 May 2014 18:19:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 18:19:13 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of reachbach@gmail.com designates 209.85.212.170 as permitted sender)
Received: from [209.85.212.170] (HELO mail-wi0-f170.google.com) (209.85.212.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 18:19:09 +0000
Received: by mail-wi0-f170.google.com with SMTP id bs8so3982891wib.5
        for <dev@spark.apache.org>; Wed, 28 May 2014 11:18:46 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=0dQxTxTcoRlOBZEugV9nbm4+rDx5xm4dz3KfYGeR6+4=;
        b=eDixNAJcENSdwUs6hWTPmSykuI17VfSvbkb7Ew08GZp+B3YtGkG+LUlEna8ismOQFy
         KM3vybBcx9kIW6EKmLTGRkVKGRmry26DLX3UBeSz0fgVL2GJCgfc7ir4LKN5f58oFmTK
         8sXChIW+0nxtlFf/Zf5QXSWkbF7uaQJX/PQVQ1DFPgWEHORo/mY/+Cc/GATNAs8OPAHC
         XrS+bKbpIikz7fTVP+k1IFBd0KzPI+1Ai9b07MQ56u3psu9+ivxbvQobrwfcoC1SBQVZ
         C3mff7jmdbaFhEslgW50/NIK7vTBIwErPX9wMEexes2rbq15+SrmletM4oAD9ST2ZPYP
         GNhg==
MIME-Version: 1.0
X-Received: by 10.194.8.200 with SMTP id t8mr1782210wja.19.1401301125958; Wed,
 28 May 2014 11:18:45 -0700 (PDT)
Received: by 10.194.164.202 with HTTP; Wed, 28 May 2014 11:18:45 -0700 (PDT)
Date: Wed, 28 May 2014 23:48:45 +0530
Message-ID: <CALxMP-BaL9tnJVcnyRUxpOv0ZqWsPjyKdJkCLp=FgEH-tsfmGw@mail.gmail.com>
Subject: LogisticRegression: Predicting continuous outcomes
From: Bharath Ravi Kumar <reachbach@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b5d441ad6152c04fa79d7cd
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b5d441ad6152c04fa79d7cd
Content-Type: text/plain; charset=UTF-8

I'm looking to reuse the LogisticRegression model (with SGD) to predict a
real-valued outcome variable. (I understand that logistic regression is
generally applied to predict binary outcome, but for various reasons, this
model suits our needs better than LinearRegression). Related to that I have
the following questions:

1) Can the current LogisticRegression model be used as is to train based on
binary input (i.e. explanatory) features, or is there an assumption that
the explanatory features must be continuous?

2) I intend to reuse the current class to train a model on LabeledPoints
where the label is a real value (and not 0 / 1). I'd like to know if
invoking setValidateData(false) would suffice or if one must override the
validator to achieve this.

3) I recall seeing an experimental method on the class (
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/classification/LogisticRegression.scala)
that clears the threshold separating positive & negative predictions. Once
the model is trained on real valued labels, would clearing this flag
suffice to predict an outcome that is continous in nature?

Thanks,
Bharath

P.S: I'm writing to dev@ and not user@ assuming that lib changes might be
necessary. Apologies if the mailing list is incorrect.

--047d7b5d441ad6152c04fa79d7cd--

From dev-return-7845-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 28 20:07:26 2014
Return-Path: <dev-return-7845-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CB0901020E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 May 2014 20:07:26 +0000 (UTC)
Received: (qmail 26743 invoked by uid 500); 28 May 2014 20:07:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26693 invoked by uid 500); 28 May 2014 20:07:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26685 invoked by uid 99); 28 May 2014 20:07:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 20:07:26 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of Sean.McNamara@webtrends.com designates 216.64.169.22 as permitted sender)
Received: from [216.64.169.22] (HELO pdxmta01.webtrends.com) (216.64.169.22)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 20:07:22 +0000
Received: from pdxex2.webtrends.corp (Not Verified[10.61.2.221]) by pdxmta01.webtrends.com with MailMarshal (v7,2,1,6300) (using TLS: SSLv23)
	id <B538641f80000>; Wed, 28 May 2014 20:07:20 +0000
Received: from PDXEX1.WebTrends.corp ([172.27.5.220]) by pdxex2.webtrends.corp
 ([172.27.3.221]) with mapi id 14.03.0181.006; Wed, 28 May 2014 20:07:01 +0000
From: Sean McNamara <Sean.McNamara@Webtrends.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
Thread-Topic: [VOTE] Release Apache Spark 1.0.0 (RC11)
Thread-Index: AQHPePBADkQ8nnKEGkCvZYR1wJ7ZrZtWbs+A
Date: Wed, 28 May 2014 20:07:00 +0000
Message-ID: <BA089183-FAF4-4B65-8118-9DDB7E744AFB@webtrends.com>
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
In-Reply-To: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.61.2.4]
Content-Type: text/plain; charset="us-ascii"
Content-ID: <D2AEB806B7133E488ADC4CAF5FF3CC66@WebTrends.com>
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Pulled down, compiled, and tested examples on OS X and ubuntu.
Deployed app we are building on spark and poured data through it.

+1

Sean


On May 26, 2014, at 8:39 AM, Tathagata Das <tathagata.das1565@gmail.com> wr=
ote:

> Please vote on releasing the following candidate as Apache Spark version =
1.0.0!
>=20
> This has a few important bug fixes on top of rc10:
> SPARK-1900 and SPARK-1918: https://github.com/apache/spark/pull/853
> SPARK-1870: https://github.com/apache/spark/pull/848
> SPARK-1897: https://github.com/apache/spark/pull/849
>=20
> The tag to be voted on is v1.0.0-rc11 (commit c69d97cd):
> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3Dc69d=
97cdb42f809cb71113a1db4194c21372242a
>=20
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~tdas/spark-1.0.0-rc11/
>=20
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/tdas.asc
>=20
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1019/
>=20
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/
>=20
> Please vote on releasing this package as Apache Spark 1.0.0!
>=20
> The vote is open until Thursday, May 29, at 16:00 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>=20
> [ ] +1 Release this package as Apache Spark 1.0.0
> [ ] -1 Do not release this package because ...
>=20
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>=20
> =3D=3D API Changes =3D=3D
> We welcome users to compile Spark applications against 1.0. There are
> a few API changes in this release. Here are links to the associated
> upgrade guides - user facing changes have been kept as small as
> possible.
>=20
> Changes to ML vector specification:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/mllib-guide.html#fro=
m-09-to-10
>=20
> Changes to the Java API:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/java-programming-gui=
de.html#upgrading-from-pre-10-versions-of-spark
>=20
> Changes to the streaming API:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/streaming-programmin=
g-guide.html#migration-guide-from-091-or-below-to-1x
>=20
> Changes to the GraphX API:
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/graphx-programming-g=
uide.html#upgrade-guide-from-spark-091
>=20
> Other changes:
> coGroup and related functions now return Iterable[T] instead of Seq[T]
> =3D=3D> Call toSeq on the result to restore the old behavior
>=20
> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> =3D=3D> Call toSeq on the result to restore old behavior


From dev-return-7846-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 28 20:13:09 2014
Return-Path: <dev-return-7846-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B0FD61028E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 May 2014 20:13:09 +0000 (UTC)
Received: (qmail 36434 invoked by uid 500); 28 May 2014 20:13:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36370 invoked by uid 500); 28 May 2014 20:13:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36362 invoked by uid 99); 28 May 2014 20:13:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 20:13:09 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of wibenton@redhat.com designates 209.132.183.37 as permitted sender)
Received: from [209.132.183.37] (HELO mx5-phx2.redhat.com) (209.132.183.37)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 20:13:04 +0000
Received: from zmail14.collab.prod.int.phx2.redhat.com (zmail14.collab.prod.int.phx2.redhat.com [10.5.83.16])
	by mx5-phx2.redhat.com (8.14.4/8.14.4) with ESMTP id s4SKChCw003934
	for <dev@spark.apache.org>; Wed, 28 May 2014 16:12:43 -0400
Date: Wed, 28 May 2014 16:12:43 -0400 (EDT)
From: Will Benton <willb@redhat.com>
To: dev@spark.apache.org
Message-ID: <1662612895.10936720.1401307963457.JavaMail.zimbra@redhat.com>
In-Reply-To: <1884051835.10830452.1401298343359.JavaMail.zimbra@redhat.com>
Subject: ContextCleaner, weak references, and serialization
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.11]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - FF29 (Mac)/8.0.6_GA_5922)
Thread-Topic: ContextCleaner, weak references, and serialization
Thread-Index: S44rvqt6cxGL7g4P7azKOyVJF0/D+Q==
X-Virus-Checked: Checked by ClamAV on apache.org

Friends, 

For context (so to speak), I did some work in the 0.9 timeframe to fix SPARK-897 (provide immediate feedback when closures aren't serializable) and SPARK-729 (make sure that free variables in closures are captured when the RDD transformations are declared).

I currently have a branch addressing SPARK-897 that builds and tests out against 0.9, 1.0, and master last I checked (https://github.com/apache/spark/pull/143).  My branch addressing SPARK-729 builds on my SPARK-897 branch, and passed the test suite in 0.9[1].  However, some things that changed or were added in 1.0 wound up depending on the old behavior.  I've been working on other things lately but would like to get these issues fixed after 1.0 goes final so I was hoping to get a bit of discussion on the best way to go forward with an issue that I haven't solved yet:

ContextCleaner uses weak references to track broadcast variables.  Because weak references obviously don't track cloned objects (or those that have been serialized and deserialized), capturing free variables in closures in the obvious way (i.e. by replacing the closure with a copy that has been serialized and deserialized) results in an undesirable situation:  we might have, e.g., live HTTP broadcast variable objects referring to filesystem resources that could be cleaned at any time because the objects that they were cloned from have become only weakly reachable.

To be clear, this isn't a problem now; it's only a problem for the way I'm proposing to fix SPARK-729.  With that said, I'm wondering if it would make more sense to fix this problem by adding a layer of indirection to reference count external and persisting resources rather than the objects that putatively own them, or if it would make more sense to take a more sophisticated (but also more potentially fragile) approach to ensuring variable capture.



thanks,
wb


[1] Serializing closures also created or uncovered a PySpark issue in 0.9 (and presumably in later versions as well) that requires further investigation, but my patch did include a workaround; here are the details: https://issues.apache.org/jira/browse/SPARK-1454

From dev-return-7847-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 28 20:13:34 2014
Return-Path: <dev-return-7847-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 033421029F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 May 2014 20:13:34 +0000 (UTC)
Received: (qmail 37362 invoked by uid 500); 28 May 2014 20:13:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37300 invoked by uid 500); 28 May 2014 20:13:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37292 invoked by uid 99); 28 May 2014 20:13:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 20:13:33 +0000
X-ASF-Spam-Status: No, hits=-3.7 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_HELO_PASS,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of wibenton@redhat.com designates 209.132.183.24 as permitted sender)
Received: from [209.132.183.24] (HELO mx3-phx2.redhat.com) (209.132.183.24)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 20:13:31 +0000
Received: from zmail14.collab.prod.int.phx2.redhat.com (zmail14.collab.prod.int.phx2.redhat.com [10.5.83.16])
	by mx3-phx2.redhat.com (8.13.8/8.13.8) with ESMTP id s4SKD6J6020407
	for <dev@spark.apache.org>; Wed, 28 May 2014 16:13:06 -0400
Date: Wed, 28 May 2014 16:13:06 -0400 (EDT)
From: Will Benton <willb@redhat.com>
To: dev@spark.apache.org
Message-ID: <1359060085.10936811.1401307986271.JavaMail.zimbra@redhat.com>
In-Reply-To: <1400945577261-6787.post@n3.nabble.com>
References: <1400945577261-6787.post@n3.nabble.com>
Subject: Re: Kryo serialization for closures: a workaround
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.11]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - FF29 (Mac)/8.0.6_GA_5922)
Thread-Topic: Kryo serialization for closures: a workaround
Thread-Index: pBP+MNoMFl9IKxjc2SmEBCCfLBSfJA==
X-Virus-Checked: Checked by ClamAV on apache.org

This is an interesting approach, Nilesh!

Someone will correct me if I'm wrong, but I don't think this could go into ClosureCleaner as a default behavior (since Kryo apparently breaks on some classes that depend on custom Java serializers, as has come up on the list recently).  But it does seem like having a function in Spark that did this for closures more transparently (to be called explicitly by clients in problem cases) could be pretty useful.


best,
wb


----- Original Message -----
> From: "Nilesh" <nilesh@nileshc.com>
> To: dev@spark.incubator.apache.org
> Sent: Saturday, May 24, 2014 10:32:57 AM
> Subject: Kryo serialization for closures: a workaround
> 
> Suppose my mappers can be functions (def) that internally call other classes
> and create objects and do different things inside. (Or they can even be
> classes that extend (Foo) => Bar and do the processing in their apply method
> - but let's ignore this case for now)
> 
> Spark supports only Java Serialization for closures and forces all the
> classes inside to implement Serializable and coughs up errors when forced to
> use Kryo for closures. But one cannot expect all 3rd party libraries to have
> all classes extend Serializable!
> 
> Here's a workaround that I thought I'd share in case anyone comes across
> this problem:
> 
> You simply need to serialize the objects before passing through the closure,
> and de-serialize afterwards. This approach just works, even if your classes
> aren't Serializable, because it uses Kryo behind the scenes. All you need is
> some curry. ;) Here's an example of how I did it:
> 
> def genMapper(kryoWrapper: KryoSerializationWrapper[(Foo => Bar)])
> (foo: Foo) : Bar = {    kryoWrapper.value.apply(foo)}val mapper =
> genMapper(KryoSerializationWrapper(new Blah(abc)))
> _rdd.flatMap(mapper).collectAsMap()object Blah(abc: ABC) extends (Foo =>
> Bar) {    def apply(foo: Foo) : Bar = { //This is the real function }}
> Feel free to make Blah as complicated as you want, class, companion object,
> nested classes, references to multiple 3rd party libs.
> 
> KryoSerializationWrapper refers to  this wrapper from amplab/shark
> <https://github.com/amplab/shark/blob/master/src/main/scala/shark/execution/serialization/KryoSerializationWrapper.scala>
> 
> Don't you think it's a good idea to have something like this inside the
> framework itself? :)
> 
> 
> 
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Kryo-serialization-for-closures-a-workaround-tp6787.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.

From dev-return-7848-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed May 28 21:13:26 2014
Return-Path: <dev-return-7848-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 015EB1084B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 28 May 2014 21:13:26 +0000 (UTC)
Received: (qmail 78930 invoked by uid 500); 28 May 2014 21:13:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78873 invoked by uid 500); 28 May 2014 21:13:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78865 invoked by uid 99); 28 May 2014 21:13:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 21:13:25 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tgraves_cs@yahoo.com designates 216.109.114.170 as permitted sender)
Received: from [216.109.114.170] (HELO nm43-vm9.bullet.mail.bf1.yahoo.com) (216.109.114.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 28 May 2014 21:13:20 +0000
Received: from [98.139.212.153] by nm43.bullet.mail.bf1.yahoo.com with NNFMP; 28 May 2014 21:12:56 -0000
Received: from [98.139.212.194] by tm10.bullet.mail.bf1.yahoo.com with NNFMP; 28 May 2014 21:12:56 -0000
Received: from [127.0.0.1] by omp1003.mail.bf1.yahoo.com with NNFMP; 28 May 2014 21:12:56 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 326480.83815.bm@omp1003.mail.bf1.yahoo.com
Received: (qmail 62317 invoked by uid 60001); 28 May 2014 21:12:56 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.com; s=s1024; t=1401311576; bh=4FCJy5NQ6yXI12P4LZZJJhPdb5T/WkSVdXECbt+bYM4=; h=References:Message-ID:Date:From:Reply-To:Subject:To:In-Reply-To:MIME-Version:Content-Type; b=6khh/YrYR1VVw31CsS8swiIyKZRAa+38KeizbH36eP6HNoQBcqQZ+C7Ub+4DtDlgcZ7RTBuQ8Zbm6XHaznRllrK3ayDfg7Cw/AqD4sypTqJWXHINFhVRVuMCWB1b+h/Klssd3hUUexgKIcO/a4pEGXp+gr3uc2X8x2ElyIeAZF0=
X-YMail-OSG: V.nCaSoVM1mGcDLUxb7Y1T62sQvxssz2Eaam39LBn0QV_CF
 49UXJHpL4dyxLfYnprUDD7lurbVTU89STJ3Hca6rAnKRADdnaqhOSXttxsaW
 mZ7WaogEMAVt8qEl8NJL3Z6z.x2pOgv4jozIPv55NX8ILHe3lowY0j6Osdns
 g239jHqfkzBX2pC1gTciqCm3E5WMQ6FUNDzEwd67U_VINh.XYqwtCsS2QYNI
 bzYdjXEhPKTH1SM8Pm8iRhHGxfVJkIxe_SWVpZy.oTL46bv0flv0.4BYygKP
 gDO75ICqhiMRIEJWBeBjVKFaeMfbcaLG02GMXbmSvFdN4JF1GETlNvmOhr0_
 r2ppUVjTE_OYLdcleX8vuWEWCl4vDBgavqa7P3uZFdGCTHs1lG8eX605ynCE
 BRXjRm0imGLOHiHJM6F4P5mkoG1RdXCwhjYZEq1TzHzY_WUq378y1f80Af5b
 ebMoVoiJrXqU9sbuDNgsXviwrIrvXxQu3LqcboA17ZlBnVRnSbC68pzWISnK
 mKLYCbcLjuVoMYfagIld1JYYe6D0QWEs6FF7t14VN1ocCC3dF6meaAvCS0ik
 ySef208fPqrSFJ_ukF2CnrZewQW3PXQ54Va_wu_bqXnMRWSU5GouQeZVPj3D
 S6v7.Jh0P2ltezE.v0S8HpOi6jdFJDnuYtYCtMr.l0E5b6ouoSoIF.BMJ6B0
 4V1CPWA--
Received: from [204.11.79.50] by web140105.mail.bf1.yahoo.com via HTTP; Wed, 28 May 2014 14:12:56 PDT
X-Rocket-MIMEInfo: 002.001,KzEuIFRlc3RlZCBzcGFyayBvbiB5YXJuIChjbHVzdGVyIG1vZGUsIGNsaWVudCBtb2RlLCBweXNwYXJrLCBzcGFyay1zaGVsbCkgb24gaGFkb29wIDAuMjMgYW5kIDIuNC7CoAoKVG9tCgoKT24gV2VkbmVzZGF5LCBNYXkgMjgsIDIwMTQgMzowNyBQTSwgU2VhbiBNY05hbWFyYSA8U2Vhbi5NY05hbWFyYUBXZWJ0cmVuZHMuY29tPiB3cm90ZToKIAoKClB1bGxlZCBkb3duLCBjb21waWxlZCwgYW5kIHRlc3RlZCBleGFtcGxlcyBvbiBPUyBYIGFuZCB1YnVudHUuCkRlcGxveWVkIGFwcCB3ZSBhcmUgYnVpbGRpbmcBMAEBAQE-
X-Mailer: YahooMailWebService/0.8.188.663
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com> <BA089183-FAF4-4B65-8118-9DDB7E744AFB@webtrends.com> 
Message-ID: <1401311576.69780.YahooMailNeo@web140105.mail.bf1.yahoo.com>
Date: Wed, 28 May 2014 14:12:56 -0700 (PDT)
From: Tom Graves <tgraves_cs@yahoo.com>
Reply-To: Tom Graves <tgraves_cs@yahoo.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
To: "dev@spark.apache.org" <dev@spark.apache.org>
In-Reply-To: <BA089183-FAF4-4B65-8118-9DDB7E744AFB@webtrends.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="66961340-458033912-1401311576=:69780"
X-Virus-Checked: Checked by ClamAV on apache.org

--66961340-458033912-1401311576=:69780
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

+1. Tested spark on yarn (cluster mode, client mode, pyspark, spark-shell) =
on hadoop 0.23 and 2.4.=A0=0A=0ATom=0A=0A=0AOn Wednesday, May 28, 2014 3:07=
 PM, Sean McNamara <Sean.McNamara@Webtrends.com> wrote:=0A =0A=0A=0APulled =
down, compiled, and tested examples on OS X and ubuntu.=0ADeployed app we a=
re building on spark and poured data through it.=0A=0A+1=0A=0ASean=0A=0A=0A=
=0AOn May 26, 2014, at 8:39 AM, Tathagata Das <tathagata.das1565@gmail.com>=
 wrote:=0A=0A> Please vote on releasing the following candidate as Apache S=
park version 1.0.0!=0A> =0A> This has a few important bug fixes on top of r=
c10:=0A> SPARK-1900 and SPARK-1918: https://github.com/apache/spark/pull/85=
3=0A> SPARK-1870: https://github.com/apache/spark/pull/848=0A> SPARK-1897: =
https://github.com/apache/spark/pull/849=0A> =0A> The tag to be voted on is=
 v1.0.0-rc11 (commit c69d97cd):=0A> https://git-wip-us.apache.org/repos/asf=
?p=3Dspark.git;a=3Dcommit;h=3Dc69d97cdb42f809cb71113a1db4194c21372242a=0A> =
=0A> The release files, including signatures, digests, etc. can be found at=
:=0A> http://people.apache.org/~tdas/spark-1.0.0-rc11/=0A> =0A> Release=0A =
artifacts are signed with the following key:=0A> https://people.apache.org/=
keys/committer/tdas.asc=0A> =0A> The staging repository for this release ca=
n be found at:=0A> https://repository.apache.org/content/repositories/orgap=
achespark-1019/=0A> =0A> The documentation corresponding to this release ca=
n be found at:=0A> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/=0A=
> =0A> Please vote on releasing this package as Apache Spark 1.0.0!=0A> =0A=
> The vote is open until=0A Thursday, May 29, at 16:00 UTC and passes if=0A=
> a majority of at least 3 +1 PMC votes are cast.=0A> =0A> [ ] +1 Release t=
his package as Apache Spark 1.0.0=0A> [ ] -1 Do not release this package be=
cause ...=0A> =0A> To learn more about Apache Spark, please see=0A> http://=
spark.apache.org/=0A> =0A> =3D=3D API Changes =3D=3D=0A> We welcome users t=
o compile Spark applications against 1.0. There are=0A> a few API changes i=
n this release. Here are links to the associated=0A> upgrade guides - user =
facing changes have been kept as small as=0A> possible.=0A> =0A> Changes to=
 ML vector specification:=0A> http://people.apache.org/~tdas/spark-1.0.0-rc=
11-docs/mllib-guide.html#from-09-to-10=0A> =0A> Changes to the Java API:=0A=
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/java-programming-gui=
de.html#upgrading-from-pre-10-versions-of-spark=0A> =0A> Changes to the str=
eaming API:=0A> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/stream=
ing-programming-guide.html#migration-guide-from-091-or-below-to-1x=0A> =0A>=
 Changes to the GraphX API:=0A> http://people.apache.org/~tdas/spark-1.0.0-=
rc11-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091=0A> =
=0A> Other changes:=0A> coGroup and related functions now return Iterable[T=
] instead of Seq[T]=0A> =3D=3D> Call toSeq on the result to restore the old=
 behavior=0A> =0A> SparkContext.jarOfClass returns Option[String] instead o=
f Seq[String]=0A> =3D=3D> Call toSeq on the result to restore old behavior
--66961340-458033912-1401311576=:69780--

From dev-return-7849-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 01:58:47 2014
Return-Path: <dev-return-7849-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CE93B10BDA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 01:58:47 +0000 (UTC)
Received: (qmail 5236 invoked by uid 500); 29 May 2014 01:58:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5176 invoked by uid 500); 29 May 2014 01:58:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5168 invoked by uid 99); 29 May 2014 01:58:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 01:58:47 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 74.125.82.177 as permitted sender)
Received: from [74.125.82.177] (HELO mail-we0-f177.google.com) (74.125.82.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 01:58:44 +0000
Received: by mail-we0-f177.google.com with SMTP id x48so11782351wes.8
        for <dev@spark.apache.org>; Wed, 28 May 2014 18:58:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=th7TZ8sOHm0Ou2eKo1Sr4s08EF822YqcE04PCzcy2CE=;
        b=o3dnQqNC8cQyHZhFiukaBZOc5Bei0ojfOG7XqEiI72WmLejuXBvKht3USlJy7Yr5ce
         y6Te9mEqlMZngyyOrt8g6dr7OCY7VVey2bLMDIdFZYlB5ZgmaGjYAl6LLWBO3h/O8sWQ
         SVBTIcYYHJc+NVzcENSBxWcim2atKu+zUkW2tQXIth/P9NM6sVuUii1bd8yRzDFDO3LB
         wfw9lWD5qr/Rwnt+RW3Crz7b/iWhu0SIpA0UBBxuKWAGIPyOmF957tYnmRnKy0S+6KMI
         3oF9aYNfr6dKCodP1OYkuZzNuhGGLSE8DIVSmUk/RnWqkeLXItofaouOOxPRYUz3UZDU
         UGIA==
MIME-Version: 1.0
X-Received: by 10.194.242.136 with SMTP id wq8mr5540858wjc.4.1401328701306;
 Wed, 28 May 2014 18:58:21 -0700 (PDT)
Received: by 10.194.169.227 with HTTP; Wed, 28 May 2014 18:58:21 -0700 (PDT)
In-Reply-To: <CALxMP-BaL9tnJVcnyRUxpOv0ZqWsPjyKdJkCLp=FgEH-tsfmGw@mail.gmail.com>
References: <CALxMP-BaL9tnJVcnyRUxpOv0ZqWsPjyKdJkCLp=FgEH-tsfmGw@mail.gmail.com>
Date: Wed, 28 May 2014 18:58:21 -0700
Message-ID: <CAJgQjQ_V6_ckku4Yd=iTMyOaVTWscfQEhsE67wGvpab6U8kWbw@mail.gmail.com>
Subject: Re: LogisticRegression: Predicting continuous outcomes
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Please find my comments inline. -Xiangrui

On Wed, May 28, 2014 at 11:18 AM, Bharath Ravi Kumar
<reachbach@gmail.com> wrote:
> I'm looking to reuse the LogisticRegression model (with SGD) to predict a
> real-valued outcome variable. (I understand that logistic regression is
> generally applied to predict binary outcome, but for various reasons, this
> model suits our needs better than LinearRegression). Related to that I have
> the following questions:
>
> 1) Can the current LogisticRegression model be used as is to train based on
> binary input (i.e. explanatory) features, or is there an assumption that
> the explanatory features must be continuous?
>

Binary features should be okay.

> 2) I intend to reuse the current class to train a model on LabeledPoints
> where the label is a real value (and not 0 / 1). I'd like to know if
> invoking setValidateData(false) would suffice or if one must override the
> validator to achieve this.
>

I'm not sure whether the loss function makes sense with real valued
labels. We may use the assumption that the label is binary to simplify
the computation of loss. You can take a look at the code and see
whether the loss function fits your model.

> 3) I recall seeing an experimental method on the class (
> https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/classification/LogisticRegression.scala)
> that clears the threshold separating positive & negative predictions. Once
> the model is trained on real valued labels, would clearing this flag
> suffice to predict an outcome that is continous in nature?
>

If you clear the threshold, it outputs the raw scores from the
logistic function.

> Thanks,
> Bharath
>
> P.S: I'm writing to dev@ and not user@ assuming that lib changes might be
> necessary. Apologies if the mailing list is incorrect.

From dev-return-7850-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 02:03:50 2014
Return-Path: <dev-return-7850-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3FED810C12
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 02:03:50 +0000 (UTC)
Received: (qmail 10890 invoked by uid 500); 29 May 2014 02:03:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10824 invoked by uid 500); 29 May 2014 02:03:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10815 invoked by uid 99); 29 May 2014 02:03:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 02:03:49 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 74.125.82.178 as permitted sender)
Received: from [74.125.82.178] (HELO mail-we0-f178.google.com) (74.125.82.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 02:03:45 +0000
Received: by mail-we0-f178.google.com with SMTP id u56so12043487wes.37
        for <dev@spark.incubator.apache.org>; Wed, 28 May 2014 19:03:24 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=ggf13veKtRkasJzAvSTGZRacDPTPfMrluS86Z9H2P5E=;
        b=i+isy2i1qw+X8o4oOVNuKBTLvgSND+Rzof9bx1649MvYG40PRKDpHUHiTEv/xv11ee
         w/HPTq65/ahoaftK8vffCg9YxaimIG0CLXgpX+crl4ht5FJOEGT2wJ4+bV0QDxeCBIHv
         yN9eKZY1ZrZ+EzZvtDlZWZgbUeaJkCnet48HDDi2eyB47GfmLW4lUvl0sZIP6nWWpMLd
         fW9U5yn1KlaEiVLvcdqgbeZqSNGNLN1ksifkRpjXA1Cqm8r/jBPZUnrZVV06JTP7TUyB
         sjszYoaNDdaDV1KBWHLgPjxCq120FymV+qxJxGFeajl0H154ZP1uLTgmpB1xl6dKhiXR
         EDyg==
MIME-Version: 1.0
X-Received: by 10.180.212.77 with SMTP id ni13mr6444286wic.5.1401329004171;
 Wed, 28 May 2014 19:03:24 -0700 (PDT)
Received: by 10.194.169.227 with HTTP; Wed, 28 May 2014 19:03:24 -0700 (PDT)
In-Reply-To: <1401278625683-6826.post@n3.nabble.com>
References: <1401278625683-6826.post@n3.nabble.com>
Date: Wed, 28 May 2014 19:03:24 -0700
Message-ID: <CAJgQjQ_R8w6WXsp5UVjsXMQuBS1zOR3aE-6KBp3R0WFsL0Mksg@mail.gmail.com>
Subject: Re: Standard preprocessing/scaling
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

RowMatrix has a method to compute column summary statistics. There is
a trade-off here because centering may densify the data. A utility
function that centers data would be useful for dense datasets.
-Xiangrui

On Wed, May 28, 2014 at 5:03 AM, dataginjaninja
<rickett.stephanie@gmail.com> wrote:
> I searched on this, but didn't find anything general so I apologize if this
> has been addressed.
>
> Many algorithms (SGD, SVM...) either will not converge or will run forever
> if the data is not scaled. Sci-kit has  preprocessing
> <http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html>
> that will subtract the mean and divide by standard dev. Of course there are
> a few options with it as well.
>
> Is there something in the works for this?
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Standard-preprocessing-scaling-tp6826.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7851-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 02:03:50 2014
Return-Path: <dev-return-7851-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A857910C13
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 02:03:50 +0000 (UTC)
Received: (qmail 11630 invoked by uid 500); 29 May 2014 02:03:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11570 invoked by uid 500); 29 May 2014 02:03:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11500 invoked by uid 99); 29 May 2014 02:03:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 02:03:49 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 74.125.82.180 as permitted sender)
Received: from [74.125.82.180] (HELO mail-we0-f180.google.com) (74.125.82.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 02:03:47 +0000
Received: by mail-we0-f180.google.com with SMTP id q58so3414359wes.11
        for <dev@spark.apache.org>; Wed, 28 May 2014 19:03:24 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=ggf13veKtRkasJzAvSTGZRacDPTPfMrluS86Z9H2P5E=;
        b=i+isy2i1qw+X8o4oOVNuKBTLvgSND+Rzof9bx1649MvYG40PRKDpHUHiTEv/xv11ee
         w/HPTq65/ahoaftK8vffCg9YxaimIG0CLXgpX+crl4ht5FJOEGT2wJ4+bV0QDxeCBIHv
         yN9eKZY1ZrZ+EzZvtDlZWZgbUeaJkCnet48HDDi2eyB47GfmLW4lUvl0sZIP6nWWpMLd
         fW9U5yn1KlaEiVLvcdqgbeZqSNGNLN1ksifkRpjXA1Cqm8r/jBPZUnrZVV06JTP7TUyB
         sjszYoaNDdaDV1KBWHLgPjxCq120FymV+qxJxGFeajl0H154ZP1uLTgmpB1xl6dKhiXR
         EDyg==
MIME-Version: 1.0
X-Received: by 10.180.212.77 with SMTP id ni13mr6444286wic.5.1401329004171;
 Wed, 28 May 2014 19:03:24 -0700 (PDT)
Received: by 10.194.169.227 with HTTP; Wed, 28 May 2014 19:03:24 -0700 (PDT)
In-Reply-To: <1401278625683-6826.post@n3.nabble.com>
References: <1401278625683-6826.post@n3.nabble.com>
Date: Wed, 28 May 2014 19:03:24 -0700
Message-ID: <CAJgQjQ_R8w6WXsp5UVjsXMQuBS1zOR3aE-6KBp3R0WFsL0Mksg@mail.gmail.com>
Subject: Re: Standard preprocessing/scaling
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

RowMatrix has a method to compute column summary statistics. There is
a trade-off here because centering may densify the data. A utility
function that centers data would be useful for dense datasets.
-Xiangrui

On Wed, May 28, 2014 at 5:03 AM, dataginjaninja
<rickett.stephanie@gmail.com> wrote:
> I searched on this, but didn't find anything general so I apologize if this
> has been addressed.
>
> Many algorithms (SGD, SVM...) either will not converge or will run forever
> if the data is not scaled. Sci-kit has  preprocessing
> <http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html>
> that will subtract the mean and divide by standard dev. Of course there are
> a few options with it as well.
>
> Is there something in the works for this?
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Standard-preprocessing-scaling-tp6826.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7852-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 02:05:22 2014
Return-Path: <dev-return-7852-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BED1F10C28
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 02:05:22 +0000 (UTC)
Received: (qmail 14048 invoked by uid 500); 29 May 2014 02:05:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13991 invoked by uid 500); 29 May 2014 02:05:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13983 invoked by uid 99); 29 May 2014 02:05:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 02:05:22 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 74.125.82.177 as permitted sender)
Received: from [74.125.82.177] (HELO mail-we0-f177.google.com) (74.125.82.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 02:05:18 +0000
Received: by mail-we0-f177.google.com with SMTP id x48so12245957wes.36
        for <dev@spark.apache.org>; Wed, 28 May 2014 19:04:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=On7zaBd28DgF489hxP9G2QIdhIAVcWXH5VGMhsH767Q=;
        b=VtQVeCyTYBxOT2TcbUm2gBAAjfhj3+/PjI8Zs9aE3kRWR/JOd060mxmp0vG/Q6G+Dj
         XKpi7NItQtZ6NmLsVXHP4vLg72KpwXs7ae+X5SPRhr0SHey1RwsJlN8v7CIho+T0iw8i
         RHspyV2JkpRkQYayqAiwsstVXVcQBsJ6giiauOQzuhLCBll2gqo+/w3PfGPCkoXIz0x5
         mk0Xajv66thtGqpp9Eo83h3bR8MrN3oUL1t6xcq+3+BLuSs0powyzTBw4h4FtLd1exEb
         jT4aW80i7HqEMixciZ68hAPvYaDn24htsvp161BFEGrrWx0aseuS1iVM8699ZqEW2OMK
         NCww==
MIME-Version: 1.0
X-Received: by 10.180.221.229 with SMTP id qh5mr6316163wic.33.1401329097243;
 Wed, 28 May 2014 19:04:57 -0700 (PDT)
Received: by 10.194.169.227 with HTTP; Wed, 28 May 2014 19:04:57 -0700 (PDT)
In-Reply-To: <BA089183-FAF4-4B65-8118-9DDB7E744AFB@webtrends.com>
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
	<BA089183-FAF4-4B65-8118-9DDB7E744AFB@webtrends.com>
Date: Wed, 28 May 2014 19:04:57 -0700
Message-ID: <CAJgQjQ9z3V4L7HDD-Ksck1JyuAbgt0xcqoeTO27RGq1DKWJtQA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

+1

Tested apps with standalone client mode and yarn cluster and client modes.

Xiangrui

On Wed, May 28, 2014 at 1:07 PM, Sean McNamara
<Sean.McNamara@webtrends.com> wrote:
> Pulled down, compiled, and tested examples on OS X and ubuntu.
> Deployed app we are building on spark and poured data through it.
>
> +1
>
> Sean
>
>
> On May 26, 2014, at 8:39 AM, Tathagata Das <tathagata.das1565@gmail.com> wrote:
>
>> Please vote on releasing the following candidate as Apache Spark version 1.0.0!
>>
>> This has a few important bug fixes on top of rc10:
>> SPARK-1900 and SPARK-1918: https://github.com/apache/spark/pull/853
>> SPARK-1870: https://github.com/apache/spark/pull/848
>> SPARK-1897: https://github.com/apache/spark/pull/849
>>
>> The tag to be voted on is v1.0.0-rc11 (commit c69d97cd):
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=c69d97cdb42f809cb71113a1db4194c21372242a
>>
>> The release files, including signatures, digests, etc. can be found at:
>> http://people.apache.org/~tdas/spark-1.0.0-rc11/
>>
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/tdas.asc
>>
>> The staging repository for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1019/
>>
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/
>>
>> Please vote on releasing this package as Apache Spark 1.0.0!
>>
>> The vote is open until Thursday, May 29, at 16:00 UTC and passes if
>> a majority of at least 3 +1 PMC votes are cast.
>>
>> [ ] +1 Release this package as Apache Spark 1.0.0
>> [ ] -1 Do not release this package because ...
>>
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>>
>> == API Changes ==
>> We welcome users to compile Spark applications against 1.0. There are
>> a few API changes in this release. Here are links to the associated
>> upgrade guides - user facing changes have been kept as small as
>> possible.
>>
>> Changes to ML vector specification:
>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/mllib-guide.html#from-09-to-10
>>
>> Changes to the Java API:
>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>>
>> Changes to the streaming API:
>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>>
>> Changes to the GraphX API:
>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>>
>> Other changes:
>> coGroup and related functions now return Iterable[T] instead of Seq[T]
>> ==> Call toSeq on the result to restore the old behavior
>>
>> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
>> ==> Call toSeq on the result to restore old behavior
>

From dev-return-7853-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 02:37:03 2014
Return-Path: <dev-return-7853-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1E37510D72
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 02:37:03 +0000 (UTC)
Received: (qmail 62889 invoked by uid 500); 29 May 2014 02:37:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62837 invoked by uid 500); 29 May 2014 02:37:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62829 invoked by uid 99); 29 May 2014 02:37:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 02:37:02 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of andykonwinski@gmail.com designates 209.85.215.49 as permitted sender)
Received: from [209.85.215.49] (HELO mail-la0-f49.google.com) (209.85.215.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 02:36:59 +0000
Received: by mail-la0-f49.google.com with SMTP id pv20so7692407lab.22
        for <dev@spark.apache.org>; Wed, 28 May 2014 19:36:36 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=uSejzR5/SigZ67m5tuD7eANjxFtpoRJA+kVSnurYukY=;
        b=zSu+MglpLbSDB1a57gg66rYswQmliz0qRJjTAIa1S9H5xcFlq99M2/WW0Tr173iSUm
         s2rX3imbR80IpKTXaBgNmPdEYFUDwQY09oi3j4ToqzBmT0O8NHmYcF7Vjk81URwSTy7t
         WLCVwiJgxW9D3oaQC/OyhGrG4Jc3ggSyjutm5VLwRSGwYMYuy565GbfBsjLbmsnu+L1N
         26ENgqZrehzwEnMJTm+2w1wueFdHFWKop3zP23CXTP/5i1PqPkPGJfz4uPboR2tg2gtT
         1yQ32nYKqdXdW9IWdrWOtRXM58hEQQnjGKylStFkJt8rAzFqeg6JQQBx/Icf8jIvSgAh
         i6Sg==
MIME-Version: 1.0
X-Received: by 10.152.43.135 with SMTP id w7mr3501479lal.32.1401330996412;
 Wed, 28 May 2014 19:36:36 -0700 (PDT)
Received: by 10.112.147.4 with HTTP; Wed, 28 May 2014 19:36:33 -0700 (PDT)
Received: by 10.112.147.4 with HTTP; Wed, 28 May 2014 19:36:33 -0700 (PDT)
In-Reply-To: <CAJgQjQ9z3V4L7HDD-Ksck1JyuAbgt0xcqoeTO27RGq1DKWJtQA@mail.gmail.com>
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
	<BA089183-FAF4-4B65-8118-9DDB7E744AFB@webtrends.com>
	<CAJgQjQ9z3V4L7HDD-Ksck1JyuAbgt0xcqoeTO27RGq1DKWJtQA@mail.gmail.com>
Date: Wed, 28 May 2014 19:36:33 -0700
Message-ID: <CALEZFQyQu=mgxgF-wRC6kpOd0Yq+rVHXvCcLxEkmQh9xVtLErA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
From: Andy Konwinski <andykonwinski@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c225684105a304fa80cc69
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c225684105a304fa80cc69
Content-Type: text/plain; charset=UTF-8

+1
On May 28, 2014 7:05 PM, "Xiangrui Meng" <mengxr@gmail.com> wrote:

> +1
>
> Tested apps with standalone client mode and yarn cluster and client modes.
>
> Xiangrui
>
> On Wed, May 28, 2014 at 1:07 PM, Sean McNamara
> <Sean.McNamara@webtrends.com> wrote:
> > Pulled down, compiled, and tested examples on OS X and ubuntu.
> > Deployed app we are building on spark and poured data through it.
> >
> > +1
> >
> > Sean
> >
> >
> > On May 26, 2014, at 8:39 AM, Tathagata Das <tathagata.das1565@gmail.com>
> wrote:
> >
> >> Please vote on releasing the following candidate as Apache Spark
> version 1.0.0!
> >>
> >> This has a few important bug fixes on top of rc10:
> >> SPARK-1900 and SPARK-1918: https://github.com/apache/spark/pull/853
> >> SPARK-1870: https://github.com/apache/spark/pull/848
> >> SPARK-1897: https://github.com/apache/spark/pull/849
> >>
> >> The tag to be voted on is v1.0.0-rc11 (commit c69d97cd):
> >>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=c69d97cdb42f809cb71113a1db4194c21372242a
> >>
> >> The release files, including signatures, digests, etc. can be found at:
> >> http://people.apache.org/~tdas/spark-1.0.0-rc11/
> >>
> >> Release artifacts are signed with the following key:
> >> https://people.apache.org/keys/committer/tdas.asc
> >>
> >> The staging repository for this release can be found at:
> >> https://repository.apache.org/content/repositories/orgapachespark-1019/
> >>
> >> The documentation corresponding to this release can be found at:
> >> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/
> >>
> >> Please vote on releasing this package as Apache Spark 1.0.0!
> >>
> >> The vote is open until Thursday, May 29, at 16:00 UTC and passes if
> >> a majority of at least 3 +1 PMC votes are cast.
> >>
> >> [ ] +1 Release this package as Apache Spark 1.0.0
> >> [ ] -1 Do not release this package because ...
> >>
> >> To learn more about Apache Spark, please see
> >> http://spark.apache.org/
> >>
> >> == API Changes ==
> >> We welcome users to compile Spark applications against 1.0. There are
> >> a few API changes in this release. Here are links to the associated
> >> upgrade guides - user facing changes have been kept as small as
> >> possible.
> >>
> >> Changes to ML vector specification:
> >>
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/mllib-guide.html#from-09-to-10
> >>
> >> Changes to the Java API:
> >>
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
> >>
> >> Changes to the streaming API:
> >>
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
> >>
> >> Changes to the GraphX API:
> >>
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
> >>
> >> Other changes:
> >> coGroup and related functions now return Iterable[T] instead of Seq[T]
> >> ==> Call toSeq on the result to restore the old behavior
> >>
> >> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> >> ==> Call toSeq on the result to restore old behavior
> >
>

--001a11c225684105a304fa80cc69--

From dev-return-7854-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 02:47:15 2014
Return-Path: <dev-return-7854-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1393B10E19
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 02:47:15 +0000 (UTC)
Received: (qmail 71566 invoked by uid 500); 29 May 2014 02:47:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71501 invoked by uid 500); 29 May 2014 02:47:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71493 invoked by uid 99); 29 May 2014 02:47:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 02:47:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ctn@adatao.com designates 209.85.213.178 as permitted sender)
Received: from [209.85.213.178] (HELO mail-ig0-f178.google.com) (209.85.213.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 02:47:10 +0000
Received: by mail-ig0-f178.google.com with SMTP id hl10so3326976igb.11
        for <dev@spark.apache.org>; Wed, 28 May 2014 19:46:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=adatao.com; s=google;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=HNEdJPI0EWSJkbRPvaZuJ/YcLUWieRDCyMJ+m+W4jMI=;
        b=fpqb3/Q3w0fz5X4eABtyoVBXqbPbP+zYDb/rb+fTw0ymytAYsn1GMl3wrVPH2guhyY
         Ig60zj+zbAkMpsEJuZwIKnb+dVbgNi+ZDWWUK8IqlVyMB3kH87ByaM9UzbLbisjAICdc
         gPaqqf6aKiC4FR0V6OIXqrAvD/ytTxYBO09tc=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=HNEdJPI0EWSJkbRPvaZuJ/YcLUWieRDCyMJ+m+W4jMI=;
        b=f8o7yyRCsDC2qwjaKPMEJh/zhlYCPseQKfuSME0gIMzMfckWTKVi5kj2dt7o9v9Ezh
         FljaZ/jXKzC6O0FCZJ3RHv6bEaEZVsQISWezosc1ynfUeiVd55RDxM9XHULBbZv9dU98
         azATNcjU5ZTlMOGAFxpBdQrmWwuuOw6Ps27MJulZdNCfbG+ApvtpYK5d/KeuUu75Iarf
         W+DIKVI4lo9NVoWzkK3zXFok1HmCCp976SFBTjadxSsXVAUUq64Ilgh3l9mxg2R+yQ0M
         QcXj7bi71j/XOqLnbXbnCf9VdJ9XZ9HrF8NVC7kFGro91NBlrdcOZGGaPWNlXuAyYA5I
         iihA==
X-Gm-Message-State: ALoCoQlfAFgfLK8CM/5Y+HsqHCULb0gWplThOjxMSLQ+5LDF+rmeOs7fGgIwynhE5ct2n6qu+Fw0
X-Received: by 10.50.97.68 with SMTP id dy4mr6612348igb.8.1401331607653; Wed,
 28 May 2014 19:46:47 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.64.7.80 with HTTP; Wed, 28 May 2014 19:46:27 -0700 (PDT)
X-Originating-IP: [50.184.3.125]
In-Reply-To: <CALxMP-BaL9tnJVcnyRUxpOv0ZqWsPjyKdJkCLp=FgEH-tsfmGw@mail.gmail.com>
References: <CALxMP-BaL9tnJVcnyRUxpOv0ZqWsPjyKdJkCLp=FgEH-tsfmGw@mail.gmail.com>
From: Christopher Nguyen <ctn@adatao.com>
Date: Wed, 28 May 2014 19:46:27 -0700
Message-ID: <CAGh_TuO1iy84eN6Y_fNbF8wjcZik-KZaguooYqq23wRRGjV=Sw@mail.gmail.com>
Subject: Re: LogisticRegression: Predicting continuous outcomes
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b10c78fafe6dc04fa80f050
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b10c78fafe6dc04fa80f050
Content-Type: text/plain; charset=UTF-8

Bharath, (apologies if you're already familiar with the theory): the
proposed approach may or may not be appropriate depending on the overall
transfer function in your data. In general, a single logistic regressor
cannot approximate arbitrary non-linear functions (of linear combinations
of the inputs). You can review works by, e.g., Hornik and Cybenko in the
late 80's to see if you need something more, such as a simple, one
hidden-layer neural network.

This is a good summary:
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.2647&rep=rep1&type=pdf

--
Christopher T. Nguyen
Co-founder & CEO, Adatao <http://adatao.com>
linkedin.com/in/ctnguyen



On Wed, May 28, 2014 at 11:18 AM, Bharath Ravi Kumar <reachbach@gmail.com>wrote:

> I'm looking to reuse the LogisticRegression model (with SGD) to predict a
> real-valued outcome variable. (I understand that logistic regression is
> generally applied to predict binary outcome, but for various reasons, this
> model suits our needs better than LinearRegression). Related to that I have
> the following questions:
>
> 1) Can the current LogisticRegression model be used as is to train based on
> binary input (i.e. explanatory) features, or is there an assumption that
> the explanatory features must be continuous?
>
> 2) I intend to reuse the current class to train a model on LabeledPoints
> where the label is a real value (and not 0 / 1). I'd like to know if
> invoking setValidateData(false) would suffice or if one must override the
> validator to achieve this.
>
> 3) I recall seeing an experimental method on the class (
>
> https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/classification/LogisticRegression.scala
> )
> that clears the threshold separating positive & negative predictions. Once
> the model is trained on real valued labels, would clearing this flag
> suffice to predict an outcome that is continous in nature?
>
> Thanks,
> Bharath
>
> P.S: I'm writing to dev@ and not user@ assuming that lib changes might be
> necessary. Apologies if the mailing list is incorrect.
>

--047d7b10c78fafe6dc04fa80f050--

From dev-return-7855-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 03:55:58 2014
Return-Path: <dev-return-7855-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 55AC010256
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 03:55:58 +0000 (UTC)
Received: (qmail 61244 invoked by uid 500); 29 May 2014 03:55:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61186 invoked by uid 500); 29 May 2014 03:55:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61178 invoked by uid 99); 29 May 2014 03:55:58 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 03:55:58 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ksankar42@gmail.com designates 209.85.160.41 as permitted sender)
Received: from [209.85.160.41] (HELO mail-pb0-f41.google.com) (209.85.160.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 03:55:55 +0000
Received: by mail-pb0-f41.google.com with SMTP id uo5so12261335pbc.14
        for <dev@spark.apache.org>; Wed, 28 May 2014 20:55:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=JfGb/b+vEydUikc4pkx9Q4YrHz0bfCUiOzgvSKmbafc=;
        b=keloeifIJbTRIKPd1C1VTkz4ChScukXnhYPUsczCiO3yBAj8qSwY0A+sKARmJ2qkMX
         K3uULzjBhig9uV8cRdI3HGXvL6hc8BfkCCbNZmOOz9TvolYy2Uy7fhbY4fzJA7f552aC
         uNsjRgPQnqdg39ZkGmXBIUWwZQ8yxlmcOjYiiILj69X/DtYTGoUbs74Z2IzAKZ0vbxSf
         cpkpiNVJu2kbRYaY3U1oS5nmUxwot66Za9XxYlZcWU+co01THbPy13NVR4eDCIHUs112
         uqlqBhlhRpsm2mef8rM00ANJxecurcGwuCkzoiXt+E5YrALGJ/ZtMr+HbGfJ/GnX9kBC
         ZuQg==
MIME-Version: 1.0
X-Received: by 10.67.22.33 with SMTP id hp1mr5059593pad.134.1401335730799;
 Wed, 28 May 2014 20:55:30 -0700 (PDT)
Received: by 10.70.22.69 with HTTP; Wed, 28 May 2014 20:55:30 -0700 (PDT)
In-Reply-To: <CALEZFQyQu=mgxgF-wRC6kpOd0Yq+rVHXvCcLxEkmQh9xVtLErA@mail.gmail.com>
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
	<BA089183-FAF4-4B65-8118-9DDB7E744AFB@webtrends.com>
	<CAJgQjQ9z3V4L7HDD-Ksck1JyuAbgt0xcqoeTO27RGq1DKWJtQA@mail.gmail.com>
	<CALEZFQyQu=mgxgF-wRC6kpOd0Yq+rVHXvCcLxEkmQh9xVtLErA@mail.gmail.com>
Date: Wed, 28 May 2014 20:55:30 -0700
Message-ID: <CAOTBr2m1-wuwahrZUS8A0izqOHQJWCr9KywPWXi00-9fBAiPbQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
From: Krishna Sankar <ksankar42@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b5d60be720a0c04fa81e621
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b5d60be720a0c04fa81e621
Content-Type: text/plain; charset=UTF-8

+1
Pulled & built on MacOS X, EC2 Amazon Linux
Ran test programs on OS X, 5 node c3.4xlarge cluster
Cheers
<k/>


On Wed, May 28, 2014 at 7:36 PM, Andy Konwinski <andykonwinski@gmail.com>wrote:

> +1
> On May 28, 2014 7:05 PM, "Xiangrui Meng" <mengxr@gmail.com> wrote:
>
> > +1
> >
> > Tested apps with standalone client mode and yarn cluster and client
> modes.
> >
> > Xiangrui
> >
> > On Wed, May 28, 2014 at 1:07 PM, Sean McNamara
> > <Sean.McNamara@webtrends.com> wrote:
> > > Pulled down, compiled, and tested examples on OS X and ubuntu.
> > > Deployed app we are building on spark and poured data through it.
> > >
> > > +1
> > >
> > > Sean
> > >
> > >
> > > On May 26, 2014, at 8:39 AM, Tathagata Das <
> tathagata.das1565@gmail.com>
> > wrote:
> > >
> > >> Please vote on releasing the following candidate as Apache Spark
> > version 1.0.0!
> > >>
> > >> This has a few important bug fixes on top of rc10:
> > >> SPARK-1900 and SPARK-1918: https://github.com/apache/spark/pull/853
> > >> SPARK-1870: https://github.com/apache/spark/pull/848
> > >> SPARK-1897: https://github.com/apache/spark/pull/849
> > >>
> > >> The tag to be voted on is v1.0.0-rc11 (commit c69d97cd):
> > >>
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=c69d97cdb42f809cb71113a1db4194c21372242a
> > >>
> > >> The release files, including signatures, digests, etc. can be found
> at:
> > >> http://people.apache.org/~tdas/spark-1.0.0-rc11/
> > >>
> > >> Release artifacts are signed with the following key:
> > >> https://people.apache.org/keys/committer/tdas.asc
> > >>
> > >> The staging repository for this release can be found at:
> > >>
> https://repository.apache.org/content/repositories/orgapachespark-1019/
> > >>
> > >> The documentation corresponding to this release can be found at:
> > >> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/
> > >>
> > >> Please vote on releasing this package as Apache Spark 1.0.0!
> > >>
> > >> The vote is open until Thursday, May 29, at 16:00 UTC and passes if
> > >> a majority of at least 3 +1 PMC votes are cast.
> > >>
> > >> [ ] +1 Release this package as Apache Spark 1.0.0
> > >> [ ] -1 Do not release this package because ...
> > >>
> > >> To learn more about Apache Spark, please see
> > >> http://spark.apache.org/
> > >>
> > >> == API Changes ==
> > >> We welcome users to compile Spark applications against 1.0. There are
> > >> a few API changes in this release. Here are links to the associated
> > >> upgrade guides - user facing changes have been kept as small as
> > >> possible.
> > >>
> > >> Changes to ML vector specification:
> > >>
> >
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/mllib-guide.html#from-09-to-10
> > >>
> > >> Changes to the Java API:
> > >>
> >
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
> > >>
> > >> Changes to the streaming API:
> > >>
> >
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
> > >>
> > >> Changes to the GraphX API:
> > >>
> >
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
> > >>
> > >> Other changes:
> > >> coGroup and related functions now return Iterable[T] instead of Seq[T]
> > >> ==> Call toSeq on the result to restore the old behavior
> > >>
> > >> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
> > >> ==> Call toSeq on the result to restore old behavior
> > >
> >
>

--047d7b5d60be720a0c04fa81e621--

From dev-return-7856-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 04:23:43 2014
Return-Path: <dev-return-7856-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CDE86103E0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 04:23:43 +0000 (UTC)
Received: (qmail 2679 invoked by uid 500); 29 May 2014 04:23:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2615 invoked by uid 500); 29 May 2014 04:23:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2606 invoked by uid 99); 29 May 2014 04:23:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 04:23:43 +0000
X-ASF-Spam-Status: No, hits=1.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.83 as permitted sender)
Received: from [171.67.219.83] (HELO smtp.stanford.edu) (171.67.219.83)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 04:23:38 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 292A5101363
	for <dev@spark.incubator.apache.org>; Wed, 28 May 2014 21:23:18 -0700 (PDT)
Received: from mail-qg0-f53.google.com (mail-qg0-f53.google.com [209.85.192.53])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 9DA91100F0D
	for <dev@spark.incubator.apache.org>; Wed, 28 May 2014 21:23:16 -0700 (PDT)
Received: by mail-qg0-f53.google.com with SMTP id f51so20713002qge.12
        for <dev@spark.incubator.apache.org>; Wed, 28 May 2014 21:23:15 -0700 (PDT)
X-Gm-Message-State: ALoCoQkJ8wqKVlFnhH/tukOKM4dO2ziCrQAQNN6NvFpt+cpyeVyrRW5nEeZ2bdPGdzXl27KMhZX0
MIME-Version: 1.0
X-Received: by 10.140.105.119 with SMTP id b110mr6000750qgf.28.1401337395743;
 Wed, 28 May 2014 21:23:15 -0700 (PDT)
Received: by 10.229.220.7 with HTTP; Wed, 28 May 2014 21:23:15 -0700 (PDT)
Received: by 10.229.220.7 with HTTP; Wed, 28 May 2014 21:23:15 -0700 (PDT)
In-Reply-To: <CAJgQjQ_R8w6WXsp5UVjsXMQuBS1zOR3aE-6KBp3R0WFsL0Mksg@mail.gmail.com>
References: <1401278625683-6826.post@n3.nabble.com>
	<CAJgQjQ_R8w6WXsp5UVjsXMQuBS1zOR3aE-6KBp3R0WFsL0Mksg@mail.gmail.com>
Date: Wed, 28 May 2014 21:23:15 -0700
Message-ID: <CAEYYnxaAAHnaDSUK0r2GD2kJ38+6q5Y5=MPwfoYXWxL_KAiE9A@mail.gmail.com>
Subject: Re: Standard preprocessing/scaling
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11394402af2b5204fa8249a5
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11394402af2b5204fa8249a5
Content-Type: text/plain; charset=UTF-8

Sometimes for this case, I will just standardize without centerization. I
still get good result.

Sent from my Google Nexus 5
On May 28, 2014 7:03 PM, "Xiangrui Meng" <mengxr@gmail.com> wrote:

> RowMatrix has a method to compute column summary statistics. There is
> a trade-off here because centering may densify the data. A utility
> function that centers data would be useful for dense datasets.
> -Xiangrui
>
> On Wed, May 28, 2014 at 5:03 AM, dataginjaninja
> <rickett.stephanie@gmail.com> wrote:
> > I searched on this, but didn't find anything general so I apologize if
> this
> > has been addressed.
> >
> > Many algorithms (SGD, SVM...) either will not converge or will run
> forever
> > if the data is not scaled. Sci-kit has  preprocessing
> > <
> http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html
> >
> > that will subtract the mean and divide by standard dev. Of course there
> are
> > a few options with it as well.
> >
> > Is there something in the works for this?
> >
> >
> >
> > --
> > View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Standard-preprocessing-scaling-tp6826.html
> > Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11394402af2b5204fa8249a5--

From dev-return-7857-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 04:23:46 2014
Return-Path: <dev-return-7857-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 41303103E1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 04:23:46 +0000 (UTC)
Received: (qmail 3467 invoked by uid 500); 29 May 2014 04:23:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3416 invoked by uid 500); 29 May 2014 04:23:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 3408 invoked by uid 99); 29 May 2014 04:23:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 04:23:45 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.81 as permitted sender)
Received: from [171.67.219.81] (HELO smtp.stanford.edu) (171.67.219.81)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 04:23:42 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 3C06521832
	for <dev@spark.apache.org>; Wed, 28 May 2014 21:23:17 -0700 (PDT)
Received: from mail-qg0-f46.google.com (mail-qg0-f46.google.com [209.85.192.46])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id 9EC8C21705
	for <dev@spark.apache.org>; Wed, 28 May 2014 21:23:16 -0700 (PDT)
Received: by mail-qg0-f46.google.com with SMTP id q108so20194401qgd.5
        for <dev@spark.apache.org>; Wed, 28 May 2014 21:23:15 -0700 (PDT)
X-Gm-Message-State: ALoCoQmPO+Sm5G+s3ciJZOF9prPcottaCrv6SegAe3ojjUgVior+eRHdKdL1DfRb7X9lKVcoKsvP
MIME-Version: 1.0
X-Received: by 10.140.105.119 with SMTP id b110mr6000750qgf.28.1401337395743;
 Wed, 28 May 2014 21:23:15 -0700 (PDT)
Received: by 10.229.220.7 with HTTP; Wed, 28 May 2014 21:23:15 -0700 (PDT)
Received: by 10.229.220.7 with HTTP; Wed, 28 May 2014 21:23:15 -0700 (PDT)
In-Reply-To: <CAJgQjQ_R8w6WXsp5UVjsXMQuBS1zOR3aE-6KBp3R0WFsL0Mksg@mail.gmail.com>
References: <1401278625683-6826.post@n3.nabble.com>
	<CAJgQjQ_R8w6WXsp5UVjsXMQuBS1zOR3aE-6KBp3R0WFsL0Mksg@mail.gmail.com>
Date: Wed, 28 May 2014 21:23:15 -0700
Message-ID: <CAEYYnxaAAHnaDSUK0r2GD2kJ38+6q5Y5=MPwfoYXWxL_KAiE9A@mail.gmail.com>
Subject: Re: Standard preprocessing/scaling
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11394402af2b5204fa8249a5
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11394402af2b5204fa8249a5
Content-Type: text/plain; charset=UTF-8

Sometimes for this case, I will just standardize without centerization. I
still get good result.

Sent from my Google Nexus 5
On May 28, 2014 7:03 PM, "Xiangrui Meng" <mengxr@gmail.com> wrote:

> RowMatrix has a method to compute column summary statistics. There is
> a trade-off here because centering may densify the data. A utility
> function that centers data would be useful for dense datasets.
> -Xiangrui
>
> On Wed, May 28, 2014 at 5:03 AM, dataginjaninja
> <rickett.stephanie@gmail.com> wrote:
> > I searched on this, but didn't find anything general so I apologize if
> this
> > has been addressed.
> >
> > Many algorithms (SGD, SVM...) either will not converge or will run
> forever
> > if the data is not scaled. Sci-kit has  preprocessing
> > <
> http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.scale.html
> >
> > that will subtract the mean and divide by standard dev. Of course there
> are
> > a few options with it as well.
> >
> > Is there something in the works for this?
> >
> >
> >
> > --
> > View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Standard-preprocessing-scaling-tp6826.html
> > Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11394402af2b5204fa8249a5--

From dev-return-7858-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 04:39:00 2014
Return-Path: <dev-return-7858-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DA4AA10422
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 04:39:00 +0000 (UTC)
Received: (qmail 24275 invoked by uid 500); 29 May 2014 04:39:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24219 invoked by uid 500); 29 May 2014 04:39:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24210 invoked by uid 99); 29 May 2014 04:39:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 04:39:00 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of kevin.markey@oracle.com designates 156.151.31.81 as permitted sender)
Received: from [156.151.31.81] (HELO userp1040.oracle.com) (156.151.31.81)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 04:38:55 +0000
Received: from ucsinet22.oracle.com (ucsinet22.oracle.com [156.151.31.94])
	by userp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2) with ESMTP id s4T4cYic019530
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=OK)
	for <dev@spark.apache.org>; Thu, 29 May 2014 04:38:34 GMT
Received: from aserz7021.oracle.com (aserz7021.oracle.com [141.146.126.230])
	by ucsinet22.oracle.com (8.14.5+Sun/8.14.5) with ESMTP id s4T4cXCV011243
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=NO)
	for <dev@spark.apache.org>; Thu, 29 May 2014 04:38:33 GMT
Received: from abhmp0001.oracle.com (abhmp0001.oracle.com [141.146.116.7])
	by aserz7021.oracle.com (8.14.4+Sun/8.14.4) with ESMTP id s4T4cXPe017686
	for <dev@spark.apache.org>; Thu, 29 May 2014 04:38:33 GMT
Received: from [192.168.1.18] (/71.229.205.50)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Wed, 28 May 2014 21:38:32 -0700
Message-ID: <5386B9C8.6060608@oracle.com>
Date: Wed, 28 May 2014 22:38:32 -0600
From: Kevin Markey <kevin.markey@oracle.com>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20100101 Thunderbird/24.5.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>	<BA089183-FAF4-4B65-8118-9DDB7E744AFB@webtrends.com>	<CAJgQjQ9z3V4L7HDD-Ksck1JyuAbgt0xcqoeTO27RGq1DKWJtQA@mail.gmail.com>	<CALEZFQyQu=mgxgF-wRC6kpOd0Yq+rVHXvCcLxEkmQh9xVtLErA@mail.gmail.com> <CAOTBr2m1-wuwahrZUS8A0izqOHQJWCr9KywPWXi00-9fBAiPbQ@mail.gmail.com>
In-Reply-To: <CAOTBr2m1-wuwahrZUS8A0izqOHQJWCr9KywPWXi00-9fBAiPbQ@mail.gmail.com>
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Source-IP: ucsinet22.oracle.com [156.151.31.94]
X-Virus-Checked: Checked by ClamAV on apache.org

+1

Built -Pyarn -Phadoop-2.3 -Dhadoop.version=2.3.0
Ran current version of one of my applications on 1-node pseudocluster 
(sorry, unable to test on full cluster).
yarn-cluster mode
Ran regression tests.

Thanks
Kevin

On 05/28/2014 09:55 PM, Krishna Sankar wrote:
> +1
> Pulled & built on MacOS X, EC2 Amazon Linux
> Ran test programs on OS X, 5 node c3.4xlarge cluster
> Cheers
> <k/>
>
>
> On Wed, May 28, 2014 at 7:36 PM, Andy Konwinski <andykonwinski@gmail.com>wrote:
>
>> +1
>> On May 28, 2014 7:05 PM, "Xiangrui Meng" <mengxr@gmail.com> wrote:
>>
>>> +1
>>>
>>> Tested apps with standalone client mode and yarn cluster and client
>> modes.
>>> Xiangrui
>>>
>>> On Wed, May 28, 2014 at 1:07 PM, Sean McNamara
>>> <Sean.McNamara@webtrends.com> wrote:
>>>> Pulled down, compiled, and tested examples on OS X and ubuntu.
>>>> Deployed app we are building on spark and poured data through it.
>>>>
>>>> +1
>>>>
>>>> Sean
>>>>
>>>>
>>>> On May 26, 2014, at 8:39 AM, Tathagata Das <
>> tathagata.das1565@gmail.com>
>>> wrote:
>>>>> Please vote on releasing the following candidate as Apache Spark
>>> version 1.0.0!
>>>>> This has a few important bug fixes on top of rc10:
>>>>> SPARK-1900 and SPARK-1918: https://github.com/apache/spark/pull/853
>>>>> SPARK-1870: https://github.com/apache/spark/pull/848
>>>>> SPARK-1897: https://github.com/apache/spark/pull/849
>>>>>
>>>>> The tag to be voted on is v1.0.0-rc11 (commit c69d97cd):
>>>>>
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=c69d97cdb42f809cb71113a1db4194c21372242a
>>>>> The release files, including signatures, digests, etc. can be found
>> at:
>>>>> http://people.apache.org/~tdas/spark-1.0.0-rc11/
>>>>>
>>>>> Release artifacts are signed with the following key:
>>>>> https://people.apache.org/keys/committer/tdas.asc
>>>>>
>>>>> The staging repository for this release can be found at:
>>>>>
>> https://repository.apache.org/content/repositories/orgapachespark-1019/
>>>>> The documentation corresponding to this release can be found at:
>>>>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/
>>>>>
>>>>> Please vote on releasing this package as Apache Spark 1.0.0!
>>>>>
>>>>> The vote is open until Thursday, May 29, at 16:00 UTC and passes if
>>>>> a majority of at least 3 +1 PMC votes are cast.
>>>>>
>>>>> [ ] +1 Release this package as Apache Spark 1.0.0
>>>>> [ ] -1 Do not release this package because ...
>>>>>
>>>>> To learn more about Apache Spark, please see
>>>>> http://spark.apache.org/
>>>>>
>>>>> == API Changes ==
>>>>> We welcome users to compile Spark applications against 1.0. There are
>>>>> a few API changes in this release. Here are links to the associated
>>>>> upgrade guides - user facing changes have been kept as small as
>>>>> possible.
>>>>>
>>>>> Changes to ML vector specification:
>>>>>
>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/mllib-guide.html#from-09-to-10
>>>>> Changes to the Java API:
>>>>>
>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>>>>> Changes to the streaming API:
>>>>>
>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>>>>> Changes to the GraphX API:
>>>>>
>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>>>>> Other changes:
>>>>> coGroup and related functions now return Iterable[T] instead of Seq[T]
>>>>> ==> Call toSeq on the result to restore the old behavior
>>>>>
>>>>> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
>>>>> ==> Call toSeq on the result to restore old behavior


From dev-return-7859-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 06:46:43 2014
Return-Path: <dev-return-7859-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C96A910972
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 06:46:43 +0000 (UTC)
Received: (qmail 73172 invoked by uid 500); 29 May 2014 06:46:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73114 invoked by uid 500); 29 May 2014 06:46:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73106 invoked by uid 99); 29 May 2014 06:46:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 06:46:43 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE
X-Spam-Check-By: apache.org
Received-SPF: unknown (athena.apache.org: error in processing during lookup of taeyun.kim@innowireless.co.kr)
Received: from [59.12.193.79] (HELO mail.innowireless.co.kr) (59.12.193.79)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 06:46:36 +0000
Received: from INNOC358 (218.154.28.162) by mail.innowireless.co.kr
 (59.12.193.79) with Microsoft SMTP Server id 8.2.255.0; Thu, 29 May 2014
 15:39:04 +0900
From: innowireless TaeYun Kim <taeyun.kim@innowireless.co.kr>
To: <dev@spark.apache.org>
Subject: Suggestion: RDD cache depth
Date: Thu, 29 May 2014 15:46:18 +0900
Message-ID: <000001cf7b09$b1bb54c0$1531fe40$@innowireless.co.kr>
MIME-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_000_0001_01CF7B55.21A371F0"
X-Mailer: Microsoft Outlook 14.0
Thread-Index: Ac97B7BIKoI5WHmsQsO/dH0cY2bTfA==
Content-Language: ko
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_000_0001_01CF7B55.21A371F0
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: 7bit

It would be nice if the RDD cache() method incorporate a depth information.

That is,

 

void test()
{

JavaRDD<.> rdd = .;

 

rdd.cache();  // to depth 1. actual caching happens.

rdd.cache();  // to depth 2. Nop as long as the storage level is the same.
Else, exception.

.

rdd.uncache();  // to depth 1. Nop.

rdd.uncache();  // to depth 0. Actual unpersist happens.

}

 

This can be useful when writing code in modular way.

When a function receives an rdd as an argument, it doesn't necessarily know
the cache status of the rdd.

But it could want to cache the rdd, since it will use the rdd multiple
times.

But with the current RDD API, it cannot determine whether it should
unpersist it or leave it alone (so that caller can continue to use that rdd
without rebuilding).

 

Thanks.

 


------=_NextPart_000_0001_01CF7B55.21A371F0--

From dev-return-7860-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 06:48:59 2014
Return-Path: <dev-return-7860-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5CB471097B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 06:48:59 +0000 (UTC)
Received: (qmail 76721 invoked by uid 500); 29 May 2014 06:48:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76661 invoked by uid 500); 29 May 2014 06:48:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76653 invoked by uid 99); 29 May 2014 06:48:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 06:48:58 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of michaelmalak@yahoo.com designates 98.139.212.175 as permitted sender)
Received: from [98.139.212.175] (HELO nm16.bullet.mail.bf1.yahoo.com) (98.139.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 06:48:50 +0000
Received: from [98.139.212.150] by nm16.bullet.mail.bf1.yahoo.com with NNFMP; 29 May 2014 06:48:29 -0000
Received: from [98.139.212.197] by tm7.bullet.mail.bf1.yahoo.com with NNFMP; 29 May 2014 06:48:29 -0000
Received: from [127.0.0.1] by omp1006.mail.bf1.yahoo.com with NNFMP; 29 May 2014 06:48:29 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 501605.80562.bm@omp1006.mail.bf1.yahoo.com
Received: (qmail 93091 invoked by uid 60001); 29 May 2014 06:48:29 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.com; s=s1024; t=1401346109; bh=G3lMl9L5I0fy/zhuZQua9kC/hugyV4rIgha0inIXAFA=; h=Message-ID:Date:From:Reply-To:Subject:To:MIME-Version:Content-Type; b=OSWw30P0VGEcg+UxbxofS09WgA2jT+y+Akrd69PG0lAoAPfDO1eLmZKMYPdPErw+OPPdDsFnUfQZzSiKaiCxFj38yXEZqjkKqAGVIeN480nQcGhLt3acbDYN79MONl3Ubrf+X2hsIxh2VxPvH4j1UXTJ3ErSvoTsnYe1Ekxo7jc=
X-YMail-OSG: gu2DabkVM1lz1rCruiir2xGR.q07wskoDFGrxoHu1SrAD3C
 GKJlirwgCo6cKnC2b59FJ3mD3kV_PE6ABW1P5wpTZcSi1jsFGTglwBKLgaLQ
 WDl9f_kBvbnHf6QohBx4HKFvbFSor9RMn_BCy_zDKto69.R1okq2DKbZeVOA
 tc1.tYxya7HNNY.gwyIJhj5KpIoQzIgUrY0ONEQEzMkXDnqY8E_B9pdhja2S
 bKUroPeIo2KGFZkJtZVUhWmnM5t56vAwqQDQh5xjzXinstGFedgUZLKDEcf5
 B0KV1hd9xiIAAnnHaAA4SD5YQ1jG2EDyo1EaEddJ7ecAiobG2lwT_EVBOBUr
 yM8zQN70VfJJN70W16HAsNKEjLXez6f9Y7R0VFgkbzk1oDlSYcdw9rkDGngd
 HhP719P4pHEBGzliwSGFu8_tJ72IfN_r035wggP2PrkrKFi3TZM51yZj2p1j
 BYC.L7SDFcYEWmsgwpqzrHoKyr2TaTCGsChxpMezzj5gW.3HR6c3_4cgP7oO
 iJG27bDf4m8bmM8v7LxGVNOj_DMRmgQlQC6TFfnkO8VAoXI0-
Received: from [148.87.67.197] by web140404.mail.bf1.yahoo.com via HTTP; Wed, 28 May 2014 23:48:29 PDT
X-Rocket-MIMEInfo: 002.001,U2hvdWxkbid0IEkgYmUgc2VlaW5nIE4yIGFuZCBONCBpbiB0aGUgb3V0cHV0IGJlbG93PyAoU3BhcmsgMC45LjAgUkVQTCkgT3IgYW0gSSBtaXNzaW5nIHNvbWV0aGluZyBmdW5kYW1lbnRhbD8KCgp2YWwgbm9kZXMgPSBzYy5wYXJhbGxlbGl6ZShBcnJheSgoMUwsICJOMSIpLCAoMkwsICJOMiIpLCAoM0wsICJOMyIpLCAoNEwsICJONCIpLCAoNUwsICJONSIpKSkgCnZhbCBlZGdlcyA9IHNjLnBhcmFsbGVsaXplKEFycmF5KEVkZ2UoMUwsIDJMLCAiRTEiKSwgRWRnZSgxTCwgM0wsICJFMiIpLCBFZGdlKDJMLCABMAEBAQE-
X-Mailer: YahooMailWebService/0.8.188.663
Message-ID: <1401346109.29269.YahooMailNeo@web140404.mail.bf1.yahoo.com>
Date: Wed, 28 May 2014 23:48:29 -0700 (PDT)
From: Michael Malak <michaelmalak@yahoo.com>
Reply-To: Michael Malak <michaelmalak@yahoo.com>
Subject: GraphX triplets on 5-node graph
To: "dev@spark.apache.org" <dev@spark.apache.org>
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
X-Virus-Checked: Checked by ClamAV on apache.org

Shouldn't I be seeing N2 and N4 in the output below? (Spark 0.9.0 REPL) Or am I missing something fundamental?


val nodes = sc.parallelize(Array((1L, "N1"), (2L, "N2"), (3L, "N3"), (4L, "N4"), (5L, "N5"))) 
val edges = sc.parallelize(Array(Edge(1L, 2L, "E1"), Edge(1L, 3L, "E2"), Edge(2L, 4L, "E3"), Edge(3L, 5L, "E4"))) 
Graph(nodes, edges).triplets.collect 
res1: Array[org.apache.spark.graphx.EdgeTriplet[String,String]] = Array(((1,N1),(3,N3),E2), ((1,N1),(3,N3),E2), ((3,N3),(5,N5),E4), ((3,N3),(5,N5),E4))

From dev-return-7861-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 06:54:09 2014
Return-Path: <dev-return-7861-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7F13B1099F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 06:54:09 +0000 (UTC)
Received: (qmail 85995 invoked by uid 500); 29 May 2014 06:54:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85937 invoked by uid 500); 29 May 2014 06:54:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85929 invoked by uid 99); 29 May 2014 06:54:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 06:54:09 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.41 as permitted sender)
Received: from [209.85.220.41] (HELO mail-pa0-f41.google.com) (209.85.220.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 06:54:05 +0000
Received: by mail-pa0-f41.google.com with SMTP id kx10so2514437pab.28
        for <dev@spark.apache.org>; Wed, 28 May 2014 23:53:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=eBIlthsezfflytntDMLvpfcivOuH0Auci3RIkngI4B8=;
        b=wNYFSs6RTAHH7fMiVfodpzoKOKoN/BlVM7WPnOB/L4aRo2g7iE9173cI0WsXX4dNFU
         Q3rjSn4Vyv/PLBOGCfQNR02Iemo3uvX5AgWrpUCu78EsS+1qoQH1ksQ52sZeG3A0D0wL
         7r5ZUFTIXE1WdKPlCtiDX0GNNwM6Udf4bDyjKzkFKr4sYX08zkY91H0010hXay445AHF
         00WDbVmFYfTtzak7PphWvdaULLqHmAfCOzk7Lyv19uTGH8inGO6RIc+VhZnC374j0A5T
         cNi4mC9BTn1c+vxPmJ4swBh/em7FgTtrTvnfawGuJqJ24Dw2l67CGYW7N64GzLl9kEKl
         nYGw==
X-Received: by 10.66.163.70 with SMTP id yg6mr6055821pab.23.1401346421135;
        Wed, 28 May 2014 23:53:41 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id pv4sm99743362pac.14.2014.05.28.23.53.38
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 28 May 2014 23:53:38 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
Subject: Re: Suggestion: RDD cache depth
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <000001cf7b09$b1bb54c0$1531fe40$@innowireless.co.kr>
Date: Wed, 28 May 2014 23:53:37 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <5C5009DC-3E03-4A3D-8C5D-31B6CF0E4C62@gmail.com>
References: <000001cf7b09$b1bb54c0$1531fe40$@innowireless.co.kr>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.2)
X-Virus-Checked: Checked by ClamAV on apache.org

This is a pretty cool idea =97 instead of cache depth I=92d call it =
something like reference counting. Would you mind opening a JIRA issue =
about it?

The issue of really composing together libraries that use RDDs nicely =
isn=92t fully explored, but this is certainly one thing that would help =
with it. I=92d love to look at other ones too, e.g. how to allow =
libraries to share scans over the same dataset.

Unfortunately using multiple cache() calls for this is probably not =
feasible because it would change the current meaning of multiple calls. =
But we can add a new API, or a parameter to the method.

Matei

On May 28, 2014, at 11:46 PM, innowireless TaeYun Kim =
<taeyun.kim@innowireless.co.kr> wrote:

> It would be nice if the RDD cache() method incorporate a depth =
information.
>=20
> That is,
>=20
>=20
>=20
> void test()
> {
>=20
> JavaRDD<.> rdd =3D .;
>=20
>=20
>=20
> rdd.cache();  // to depth 1. actual caching happens.
>=20
> rdd.cache();  // to depth 2. Nop as long as the storage level is the =
same.
> Else, exception.
>=20
> .
>=20
> rdd.uncache();  // to depth 1. Nop.
>=20
> rdd.uncache();  // to depth 0. Actual unpersist happens.
>=20
> }
>=20
>=20
>=20
> This can be useful when writing code in modular way.
>=20
> When a function receives an rdd as an argument, it doesn't necessarily =
know
> the cache status of the rdd.
>=20
> But it could want to cache the rdd, since it will use the rdd multiple
> times.
>=20
> But with the current RDD API, it cannot determine whether it should
> unpersist it or leave it alone (so that caller can continue to use =
that rdd
> without rebuilding).
>=20
>=20
>=20
> Thanks.
>=20
>=20
>=20


From dev-return-7862-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 06:54:19 2014
Return-Path: <dev-return-7862-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 61AD1109A0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 06:54:19 +0000 (UTC)
Received: (qmail 86790 invoked by uid 500); 29 May 2014 06:54:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86732 invoked by uid 500); 29 May 2014 06:54:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86724 invoked by uid 99); 29 May 2014 06:54:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 06:54:19 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.43] (HELO mail-qg0-f43.google.com) (209.85.192.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 06:54:13 +0000
Received: by mail-qg0-f43.google.com with SMTP id 63so20964079qgz.2
        for <dev@spark.apache.org>; Wed, 28 May 2014 23:53:52 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=8jIU/J9rbbvJ/ZGNy1LUEIZm3h0DFtOxlFRlr9b/uAg=;
        b=es0j5uB1lxi5vAKgktOWqCcUalHie/aEe9sflqDZHG6E/Q7MaZ3gkoWu5omx+lE/LR
         THp8vsEQyAcUUP09HRMnx0A3Le/a7BkSvKzimdTazel/s5+WY06VcZdYsPIpTRbhsgrq
         pnnKT0qllB6Hylp4OPsOI9pyu6E00FrVOQoY2aFTe4Kv5UC1bY8OBZVNIx5TmLfgLbpm
         em+RKWyOxz4gd0lJzx9NVH4ul0RfwqREU1rnB5mOUUSYijd3fSd5eb9twBljc4XiqWDA
         JQ8/mCj53qnodNQe6sHGP6nk+R2IBkUMC1bC7SExzv1oTEqtAQnn2J7QerwkVXBqJaO3
         QRag==
X-Gm-Message-State: ALoCoQkXeBgQwIhzRiCH5RAVzs//aHmSpk3TUTUJDy0DcGror7iEbl5a4oTwBiKwTXg/dlzpvdBE
X-Received: by 10.224.104.5 with SMTP id m5mr7001959qao.9.1401346432600; Wed,
 28 May 2014 23:53:52 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.49.231 with HTTP; Wed, 28 May 2014 23:53:32 -0700 (PDT)
In-Reply-To: <1401346109.29269.YahooMailNeo@web140404.mail.bf1.yahoo.com>
References: <1401346109.29269.YahooMailNeo@web140404.mail.bf1.yahoo.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 28 May 2014 23:53:32 -0700
Message-ID: <CAPh_B=bJvJjd6fNNfM2MwFkUxR45rTQue8oysKwOHDgq3mb2PQ@mail.gmail.com>
Subject: Re: GraphX triplets on 5-node graph
To: dev@spark.apache.org, Michael Malak <michaelmalak@yahoo.com>
Content-Type: multipart/alternative; boundary=001a1132f48852a7e704fa8464e8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1132f48852a7e704fa8464e8
Content-Type: text/plain; charset=UTF-8

Take a look at this one: https://issues.apache.org/jira/browse/SPARK-1188

It was an optimization that added user inconvenience. We got rid of that
now in Spark 1.0.



On Wed, May 28, 2014 at 11:48 PM, Michael Malak <michaelmalak@yahoo.com>wrote:

> Shouldn't I be seeing N2 and N4 in the output below? (Spark 0.9.0 REPL) Or
> am I missing something fundamental?
>
>
> val nodes = sc.parallelize(Array((1L, "N1"), (2L, "N2"), (3L, "N3"), (4L,
> "N4"), (5L, "N5")))
> val edges = sc.parallelize(Array(Edge(1L, 2L, "E1"), Edge(1L, 3L, "E2"),
> Edge(2L, 4L, "E3"), Edge(3L, 5L, "E4")))
> Graph(nodes, edges).triplets.collect
> res1: Array[org.apache.spark.graphx.EdgeTriplet[String,String]] =
> Array(((1,N1),(3,N3),E2), ((1,N1),(3,N3),E2), ((3,N3),(5,N5),E4),
> ((3,N3),(5,N5),E4))
>

--001a1132f48852a7e704fa8464e8--

From dev-return-7863-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 07:26:17 2014
Return-Path: <dev-return-7863-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 101FF10AFD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 07:26:17 +0000 (UTC)
Received: (qmail 24563 invoked by uid 500); 29 May 2014 07:26:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24502 invoked by uid 500); 29 May 2014 07:26:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24490 invoked by uid 99); 29 May 2014 07:26:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 07:26:16 +0000
X-ASF-Spam-Status: No, hits=0.0 required=10.0
	tests=
X-Spam-Check-By: apache.org
Received-SPF: unknown (athena.apache.org: error in processing during lookup of taeyun.kim@innowireless.co.kr)
Received: from [59.12.193.79] (HELO mail.innowireless.co.kr) (59.12.193.79)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 07:26:11 +0000
Received: from INNOC358 (218.154.28.162) by mail.innowireless.co.kr
 (59.12.193.79) with Microsoft SMTP Server id 8.2.255.0; Thu, 29 May 2014
 16:12:12 +0900
From: innowireless TaeYun Kim <taeyun.kim@innowireless.co.kr>
To: <dev@spark.apache.org>
References: <000001cf7b09$b1bb54c0$1531fe40$@innowireless.co.kr> <5C5009DC-3E03-4A3D-8C5D-31B6CF0E4C62@gmail.com>
In-Reply-To: <5C5009DC-3E03-4A3D-8C5D-31B6CF0E4C62@gmail.com>
Subject: RE: Suggestion: RDD cache depth
Date: Thu, 29 May 2014 16:19:26 +0900
Message-ID: <000501cf7b0e$52bdc200$f8394600$@innowireless.co.kr>
MIME-Version: 1.0
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: 7bit
X-Mailer: Microsoft Outlook 14.0
Thread-Index: AQL0Q+udhlT4HtWX2dzWZQ3YErgEvABybqsGmQoA+oA=
Content-Language: ko
X-Virus-Checked: Checked by ClamAV on apache.org

Opened a JIRA issue. (https://issues.apache.org/jira/browse/SPARK-1962)

Thanks.


-----Original Message-----
From: Matei Zaharia [mailto:matei.zaharia@gmail.com] 
Sent: Thursday, May 29, 2014 3:54 PM
To: dev@spark.apache.org
Subject: Re: Suggestion: RDD cache depth

This is a pretty cool idea - instead of cache depth I'd call it something
like reference counting. Would you mind opening a JIRA issue about it?

The issue of really composing together libraries that use RDDs nicely isn't
fully explored, but this is certainly one thing that would help with it. I'd
love to look at other ones too, e.g. how to allow libraries to share scans
over the same dataset.

Unfortunately using multiple cache() calls for this is probably not feasible
because it would change the current meaning of multiple calls. But we can
add a new API, or a parameter to the method.

Matei

On May 28, 2014, at 11:46 PM, innowireless TaeYun Kim
<taeyun.kim@innowireless.co.kr> wrote:

> It would be nice if the RDD cache() method incorporate a depth
information.
> 
> That is,
> 
> 
> 
> void test()
> {
> 
> JavaRDD<.> rdd = .;
> 
> 
> 
> rdd.cache();  // to depth 1. actual caching happens.
> 
> rdd.cache();  // to depth 2. Nop as long as the storage level is the same.
> Else, exception.
> 
> .
> 
> rdd.uncache();  // to depth 1. Nop.
> 
> rdd.uncache();  // to depth 0. Actual unpersist happens.
> 
> }
> 
> 
> 
> This can be useful when writing code in modular way.
> 
> When a function receives an rdd as an argument, it doesn't necessarily 
> know the cache status of the rdd.
> 
> But it could want to cache the rdd, since it will use the rdd multiple 
> times.
> 
> But with the current RDD API, it cannot determine whether it should 
> unpersist it or leave it alone (so that caller can continue to use 
> that rdd without rebuilding).
> 
> 
> 
> Thanks.
> 
> 
> 


From dev-return-7864-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 10:12:12 2014
Return-Path: <dev-return-7864-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5E40F1035F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 10:12:12 +0000 (UTC)
Received: (qmail 3763 invoked by uid 500); 29 May 2014 10:12:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3707 invoked by uid 500); 29 May 2014 10:12:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 3699 invoked by uid 99); 29 May 2014 10:12:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 10:12:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of reachbach@gmail.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 10:12:09 +0000
Received: by mail-wi0-f175.google.com with SMTP id f8so5230103wiw.2
        for <dev@spark.apache.org>; Thu, 29 May 2014 03:11:45 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=8UEPOMVMW8NMhkMWrhcZIcnIj36EJ7XmV5649hS47ZY=;
        b=L6CpHF1Qy3Sdl/iRfKonrYg9iynsC5EpAA3DXLNJxgRKuAInepGcJAEymLs+xcFw64
         8u15iqTMmPBc/+q8uhZhqbGiyL75wkFBhi7TJDkRSYaDxXwOz5PK/6BhQAaBhE1poyc9
         5rs5EdIuVbZREFblE9Wh8LZ6s4Xl7GHzbK9+DVEWzp8TRICNqDExjK26hDgywS2+z6qO
         H4dcexFgB/14OgYl8IN6qk43De7w91nciyhXsfnpME8Syu1P7de6qVXrgcbZlUkPEXgS
         C0tKrbWpboGGLy6ocamSALD39aFcJ/ZZbiue6iNO5bD/z+sdJ+XPebw4n5vl3AJ7yCwT
         ZdEw==
MIME-Version: 1.0
X-Received: by 10.180.210.238 with SMTP id mx14mr57172560wic.61.1401358304599;
 Thu, 29 May 2014 03:11:44 -0700 (PDT)
Received: by 10.194.164.202 with HTTP; Thu, 29 May 2014 03:11:44 -0700 (PDT)
In-Reply-To: <CAGh_TuO1iy84eN6Y_fNbF8wjcZik-KZaguooYqq23wRRGjV=Sw@mail.gmail.com>
References: <CALxMP-BaL9tnJVcnyRUxpOv0ZqWsPjyKdJkCLp=FgEH-tsfmGw@mail.gmail.com>
	<CAGh_TuO1iy84eN6Y_fNbF8wjcZik-KZaguooYqq23wRRGjV=Sw@mail.gmail.com>
Date: Thu, 29 May 2014 15:41:44 +0530
Message-ID: <CALxMP-BE-xMp4zz1cormME1vRmj=YeRR4_sM-LKXcXD=ZLAWXQ@mail.gmail.com>
Subject: Re: LogisticRegression: Predicting continuous outcomes
From: Bharath Ravi Kumar <reachbach@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c26a42f3078104fa8727ba
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c26a42f3078104fa8727ba
Content-Type: text/plain; charset=UTF-8

Xiangrui, Christopher,

Thanks for responding.  I'll  go through the code in detail to evaluate if
the loss function used is suitable to our dataset. I'll also go through the
referred paper since I was unaware of the underlying theory. Thanks again.

-Bharath


On Thu, May 29, 2014 at 8:16 AM, Christopher Nguyen <ctn@adatao.com> wrote:

> Bharath, (apologies if you're already familiar with the theory): the
> proposed approach may or may not be appropriate depending on the overall
> transfer function in your data. In general, a single logistic regressor
> cannot approximate arbitrary non-linear functions (of linear combinations
> of the inputs). You can review works by, e.g., Hornik and Cybenko in the
> late 80's to see if you need something more, such as a simple, one
> hidden-layer neural network.
>
> This is a good summary:
>
> http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.101.2647&rep=rep1&type=pdf
>
> --
> Christopher T. Nguyen
> Co-founder & CEO, Adatao <http://adatao.com>
> linkedin.com/in/ctnguyen
>
>
>
> On Wed, May 28, 2014 at 11:18 AM, Bharath Ravi Kumar <reachbach@gmail.com
> >wrote:
>
> > I'm looking to reuse the LogisticRegression model (with SGD) to predict a
> > real-valued outcome variable. (I understand that logistic regression is
> > generally applied to predict binary outcome, but for various reasons,
> this
> > model suits our needs better than LinearRegression). Related to that I
> have
> > the following questions:
> >
> > 1) Can the current LogisticRegression model be used as is to train based
> on
> > binary input (i.e. explanatory) features, or is there an assumption that
> > the explanatory features must be continuous?
> >
> > 2) I intend to reuse the current class to train a model on LabeledPoints
> > where the label is a real value (and not 0 / 1). I'd like to know if
> > invoking setValidateData(false) would suffice or if one must override the
> > validator to achieve this.
> >
> > 3) I recall seeing an experimental method on the class (
> >
> >
> https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/classification/LogisticRegression.scala
> > )
> > that clears the threshold separating positive & negative predictions.
> Once
> > the model is trained on real valued labels, would clearing this flag
> > suffice to predict an outcome that is continous in nature?
> >
> > Thanks,
> > Bharath
> >
> > P.S: I'm writing to dev@ and not user@ assuming that lib changes might
> be
> > necessary. Apologies if the mailing list is incorrect.
> >
>

--001a11c26a42f3078104fa8727ba--

From dev-return-7865-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 12:30:10 2014
Return-Path: <dev-return-7865-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 63FDA109FE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 12:30:10 +0000 (UTC)
Received: (qmail 81379 invoked by uid 500); 29 May 2014 12:30:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81319 invoked by uid 500); 29 May 2014 12:30:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81311 invoked by uid 99); 29 May 2014 12:30:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 12:30:09 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zhengbing.li@huawei.com designates 119.145.14.65 as permitted sender)
Received: from [119.145.14.65] (HELO szxga02-in.huawei.com) (119.145.14.65)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 12:30:05 +0000
Received: from 172.24.2.119 (EHLO szxeml214-edg.china.huawei.com) ([172.24.2.119])
	by szxrg02-dlp.huawei.com (MOS 4.3.7-GA FastPath queued)
	with ESMTP id BUL21928;
	Thu, 29 May 2014 20:29:38 +0800 (CST)
Received: from SZXEML449-HUB.china.huawei.com (10.82.67.192) by
 szxeml214-edg.china.huawei.com (172.24.2.29) with Microsoft SMTP Server (TLS)
 id 14.3.158.1; Thu, 29 May 2014 20:29:35 +0800
Received: from SZXEMA407-HUB.china.huawei.com (10.82.72.39) by
 szxeml449-hub.china.huawei.com (10.82.67.192) with Microsoft SMTP Server
 (TLS) id 14.3.158.1; Thu, 29 May 2014 20:29:37 +0800
Received: from SZXEMA501-MBX.china.huawei.com ([169.254.1.67]) by
 SZXEMA407-HUB.china.huawei.com ([10.82.72.39]) with mapi id 14.03.0158.001;
 Thu, 29 May 2014 20:29:33 +0800
From: "Lizhengbing (bing, BIPA)" <zhengbing.li@huawei.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Please change instruction about  "Launching Applications Inside the
 Cluster"
Thread-Topic: Please change instruction about  "Launching Applications
 Inside the Cluster"
Thread-Index: Ac97OaQXJvsTfLbTQCKoSPrp6HrgIg==
Date: Thu, 29 May 2014 12:29:33 +0000
Message-ID: <49229E870391FC49BBBED818C268753D704BAE59@SZXEMA501-MBX.china.huawei.com>
Accept-Language: zh-CN, en-US
Content-Language: zh-CN
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [10.66.170.84]
Content-Type: multipart/alternative;
	boundary="_000_49229E870391FC49BBBED818C268753D704BAE59SZXEMA501MBXchi_"
MIME-Version: 1.0
X-CFilter-Loop: Reflected
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_49229E870391FC49BBBED818C268753D704BAE59SZXEMA501MBXchi_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

The instruction address is in http://spark.apache.org/docs/0.9.0/spark-stan=
dalone.html#launching-applications-inside-the-cluster or http://spark.apach=
e.org/docs/0.9.1/spark-standalone.html#launching-applications-inside-the-cl=
uster

Origin instruction is:
"./bin/spark-class org.apache.spark.deploy.Client launch
   [client-options] \
   <cluster-url> <application-jar-url> <main-class> \
   [application-options] "

If I follow this instruction, I will not run my program deployed in a spark=
 standalone cluster properly.

Based on source code, This instruction should be changed to
"./bin/spark-class org.apache.spark.deploy.Client [client-options] launch \
   <cluster-url> <application-jar-url> <main-class> \
   [application-options] "

That is to say: [client-options] must be put ahead of launch

--_000_49229E870391FC49BBBED818C268753D704BAE59SZXEMA501MBXchi_--

From dev-return-7866-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 12:32:55 2014
Return-Path: <dev-return-7866-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7948D10A25
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 12:32:55 +0000 (UTC)
Received: (qmail 85587 invoked by uid 500); 29 May 2014 12:32:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85516 invoked by uid 500); 29 May 2014 12:32:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85508 invoked by uid 99); 29 May 2014 12:32:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 12:32:54 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of rickett.stephanie@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 12:32:52 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <rickett.stephanie@gmail.com>)
	id 1WpzVI-0003p4-8m
	for dev@spark.incubator.apache.org; Thu, 29 May 2014 05:32:16 -0700
Date: Thu, 29 May 2014 05:32:16 -0700 (PDT)
From: dataginjaninja <rickett.stephanie@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1401366736239-6849.post@n3.nabble.com>
In-Reply-To: <CAEYYnxaAAHnaDSUK0r2GD2kJ38+6q5Y5=MPwfoYXWxL_KAiE9A@mail.gmail.com>
References: <1401278625683-6826.post@n3.nabble.com> <CAJgQjQ_R8w6WXsp5UVjsXMQuBS1zOR3aE-6KBp3R0WFsL0Mksg@mail.gmail.com> <CAEYYnxaAAHnaDSUK0r2GD2kJ38+6q5Y5=MPwfoYXWxL_KAiE9A@mail.gmail.com>
Subject: Re: Standard preprocessing/scaling
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I do see the issue for centering sparse data. Actually, the centering is less
important than the scaling by the standard deviation. Not having unit
variance causes the convergence issues and long runtimes. 

RowMatrix will compute variance of a column?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-Standard-preprocessing-scaling-tp6826p6849.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7867-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 15:55:19 2014
Return-Path: <dev-return-7867-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9F24D10018
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 15:55:19 +0000 (UTC)
Received: (qmail 20710 invoked by uid 500); 29 May 2014 15:55:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20653 invoked by uid 500); 29 May 2014 15:55:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20645 invoked by uid 99); 29 May 2014 15:55:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 15:55:19 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of rickett.stephanie@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 15:55:15 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <rickett.stephanie@gmail.com>)
	id 1Wq2fP-0000xj-3F
	for dev@spark.incubator.apache.org; Thu, 29 May 2014 08:54:55 -0700
Date: Thu, 29 May 2014 08:54:55 -0700 (PDT)
From: dataginjaninja <rickett.stephanie@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1401378895078-6850.post@n3.nabble.com>
Subject: Timestamp support in v1.0
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Can anyone verify which rc  [SPARK-1360] Add Timestamp Support for SQL #275
<https://github.com/apache/spark/pull/275>   is included in? I am running
rc3, but receiving errors with TIMESTAMP as a datatype in my Hive tables
when trying to use them in pyspark.

*The error I get:
*
14/05/29 15:44:47 INFO ParseDriver: Parsing command: SELECT COUNT(*) FROM
aol
14/05/29 15:44:48 INFO ParseDriver: Parse Completed
14/05/29 15:44:48 INFO metastore: Trying to connect to metastore with URI
thrift:
14/05/29 15:44:48 INFO metastore: Waiting 1 seconds before next connection
attempt.
14/05/29 15:44:49 INFO metastore: Connected to metastore.
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/opt/spark-1.0.0-rc3/python/pyspark/sql.py", line 189, in hql
    return self.hiveql(hqlQuery)
  File "/opt/spark-1.0.0-rc3/python/pyspark/sql.py", line 183, in hiveql
    return SchemaRDD(self._ssql_ctx.hiveql(hqlQuery), self)
  File
"/opt/spark-1.0.0-rc3/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py",
line 537, in __call__
  File
"/opt/spark-1.0.0-rc3/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py", line
300, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o14.hiveql.
: java.lang.RuntimeException: Unsupported dataType: timestamp

*The table I loaded:*
DROP TABLE IF EXISTS aol; 
CREATE EXTERNAL TABLE aol (
	userid STRING,
	query STRING,
	query_time TIMESTAMP,
	item_rank INT,
	click_url STRING)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY '\t'
LOCATION '/tmp/data/aol';

*The pyspark commands:*
from pyspark.sql import HiveContext
hctx= HiveContext(sc)
results = hctx.hql("SELECT COUNT(*) FROM aol").collect()






--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Timestamp-support-in-v1-0-tp6850.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7869-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 16:10:33 2014
Return-Path: <dev-return-7869-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E83B91012A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 16:10:33 +0000 (UTC)
Received: (qmail 72680 invoked by uid 500); 29 May 2014 16:10:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72633 invoked by uid 500); 29 May 2014 16:10:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72625 invoked by uid 99); 29 May 2014 16:10:33 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 16:10:33 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.45] (HELO mail-qg0-f45.google.com) (209.85.192.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 16:10:29 +0000
Received: by mail-qg0-f45.google.com with SMTP id z60so1621204qgd.32
        for <dev@spark.incubator.apache.org>; Thu, 29 May 2014 09:10:08 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=302UEmQu9SGds21jxjGzGd9rm7oonaFyX3XjuOTRJsI=;
        b=UDhkM+JiouoqzwbhE3TtEsKu6+MbdxCeZHXImrhFgc9mbVn6sG5udZ/zlIDxwVZrR8
         J09DRKcPyM0e6Z1suYCxiT5nbAFFXJW7QDELKPdItsXa0I2t0jv38Nav2kmA874UDTcQ
         AhChJXVNYPNFThpehQvCst9AU6U3AxzMta+lCIj/c0bGmWowTbiKssMVm3hm0OpTGLYf
         /64L6YCqVJauG564tHuuBtQz8xNGhdZgzLWvMTs5IOdRLFJyZJxI+vb/8zNhS+aDcdI9
         OKGMm80lEjHf7LErbAEVnDNkypjwL4nPkzbZJ6b2WVBqolYPeWFQuPv9JI1buCR02waZ
         9iKQ==
X-Gm-Message-State: ALoCoQkUT+Vs6Jwj1YDjtihHKFSSlva2ndT3qJncSU5NwLhMT+7QBBMKdiLwchKNZRGAQ15FjU+h
X-Received: by 10.224.71.145 with SMTP id h17mr12113875qaj.74.1401379807915;
        Thu, 29 May 2014 09:10:07 -0700 (PDT)
Received: from mail-qg0-f42.google.com (mail-qg0-f42.google.com [209.85.192.42])
        by mx.google.com with ESMTPSA id o16sm1605726qax.23.2014.05.29.09.10.06
        for <dev@spark.incubator.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 29 May 2014 09:10:06 -0700 (PDT)
Received: by mail-qg0-f42.google.com with SMTP id q107so1635514qgd.15
        for <dev@spark.incubator.apache.org>; Thu, 29 May 2014 09:10:06 -0700 (PDT)
X-Received: by 10.140.28.131 with SMTP id 3mr10991294qgz.49.1401379806184;
 Thu, 29 May 2014 09:10:06 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.41.99 with HTTP; Thu, 29 May 2014 09:09:46 -0700 (PDT)
In-Reply-To: <1401378895078-6850.post@n3.nabble.com>
References: <1401378895078-6850.post@n3.nabble.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Thu, 29 May 2014 09:09:46 -0700
Message-ID: <CA+-p3AGYXHGT2wJaohx8qCsUUEDN1WAf4jUAFaAVJqFY8bmXwQ@mail.gmail.com>
Subject: Re: Timestamp support in v1.0
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a113a5a9a8b0f9404fa8c29c2
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a5a9a8b0f9404fa8c29c2
Content-Type: text/plain; charset=UTF-8

I can confirm that the commit is included in the 1.0.0 release candidates
(it was committed before branch-1.0 split off from master), but I can't
confirm that it works in PySpark.  Generally the Python and Java interfaces
lag a little behind the Scala interface to Spark, but we're working to keep
that diff much smaller going forward.

Can you try the same thing in Scala?


On Thu, May 29, 2014 at 8:54 AM, dataginjaninja <rickett.stephanie@gmail.com
> wrote:

> Can anyone verify which rc  [SPARK-1360] Add Timestamp Support for SQL #275
> <https://github.com/apache/spark/pull/275>   is included in? I am running
> rc3, but receiving errors with TIMESTAMP as a datatype in my Hive tables
> when trying to use them in pyspark.
>
> *The error I get:
> *
> 14/05/29 15:44:47 INFO ParseDriver: Parsing command: SELECT COUNT(*) FROM
> aol
> 14/05/29 15:44:48 INFO ParseDriver: Parse Completed
> 14/05/29 15:44:48 INFO metastore: Trying to connect to metastore with URI
> thrift:
> 14/05/29 15:44:48 INFO metastore: Waiting 1 seconds before next connection
> attempt.
> 14/05/29 15:44:49 INFO metastore: Connected to metastore.
> Traceback (most recent call last):
>   File "<stdin>", line 1, in <module>
>   File "/opt/spark-1.0.0-rc3/python/pyspark/sql.py", line 189, in hql
>     return self.hiveql(hqlQuery)
>   File "/opt/spark-1.0.0-rc3/python/pyspark/sql.py", line 183, in hiveql
>     return SchemaRDD(self._ssql_ctx.hiveql(hqlQuery), self)
>   File
> "/opt/spark-1.0.0-rc3/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py",
> line 537, in __call__
>   File
> "/opt/spark-1.0.0-rc3/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py", line
> 300, in get_return_value
> py4j.protocol.Py4JJavaError: An error occurred while calling o14.hiveql.
> : java.lang.RuntimeException: Unsupported dataType: timestamp
>
> *The table I loaded:*
> DROP TABLE IF EXISTS aol;
> CREATE EXTERNAL TABLE aol (
>         userid STRING,
>         query STRING,
>         query_time TIMESTAMP,
>         item_rank INT,
>         click_url STRING)
> ROW FORMAT DELIMITED
> FIELDS TERMINATED BY '\t'
> LOCATION '/tmp/data/aol';
>
> *The pyspark commands:*
> from pyspark.sql import HiveContext
> hctx= HiveContext(sc)
> results = hctx.hql("SELECT COUNT(*) FROM aol").collect()
>
>
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Timestamp-support-in-v1-0-tp6850.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a113a5a9a8b0f9404fa8c29c2--

From dev-return-7868-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 16:10:36 2014
Return-Path: <dev-return-7868-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 584DA1012B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 16:10:36 +0000 (UTC)
Received: (qmail 71927 invoked by uid 500); 29 May 2014 16:10:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71874 invoked by uid 500); 29 May 2014 16:10:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71865 invoked by uid 99); 29 May 2014 16:10:33 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 16:10:33 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.49] (HELO mail-qg0-f49.google.com) (209.85.192.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 16:10:29 +0000
Received: by mail-qg0-f49.google.com with SMTP id a108so1584689qge.22
        for <dev@spark.apache.org>; Thu, 29 May 2014 09:10:08 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=302UEmQu9SGds21jxjGzGd9rm7oonaFyX3XjuOTRJsI=;
        b=kh9uY7rWb7C3WdzTEindnxbsHcIKDMpcFDoimMPWmEBfJcRQqwSmg6Y8NG8GbdkK09
         YDSNRgf+pXlNcBbAv/+FZjeaiZc75TFLy3Mu/qOy6TYkBzYVdKKPiyNF/Pw+1SSw1CkW
         ySFXMQHd+2wasssuJ9JdHWXE2znLZiJFqSxaHKTDBLIWiXCDUi4gaWay7Sg7eXzXJM3Z
         h908m8rQl5xUyErRThTDTv5RMEMSKa16GQI42pnGoXtfHeQkKLaK4K2Y6t1lWve0vf3R
         wWEL+M/C+uDnA18flqL/1NcfbuZ/+ciPw9yda5HNcTkYs5vCstt/Vj9sYSiSQMHW5UH6
         b+jQ==
X-Gm-Message-State: ALoCoQmKdbUQd8fhxDeDYl3DRqGKaBllIraSccSdCFdsyYjJDJhcpDheY7CskZq7Grq3ddpzMHAj
X-Received: by 10.224.69.130 with SMTP id z2mr11753693qai.87.1401379807944;
        Thu, 29 May 2014 09:10:07 -0700 (PDT)
Received: from mail-qg0-f49.google.com (mail-qg0-f49.google.com [209.85.192.49])
        by mx.google.com with ESMTPSA id b5sm1618395qar.1.2014.05.29.09.10.06
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 29 May 2014 09:10:06 -0700 (PDT)
Received: by mail-qg0-f49.google.com with SMTP id a108so1593017qge.36
        for <dev@spark.apache.org>; Thu, 29 May 2014 09:10:06 -0700 (PDT)
X-Received: by 10.140.28.131 with SMTP id 3mr10991294qgz.49.1401379806184;
 Thu, 29 May 2014 09:10:06 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.41.99 with HTTP; Thu, 29 May 2014 09:09:46 -0700 (PDT)
In-Reply-To: <1401378895078-6850.post@n3.nabble.com>
References: <1401378895078-6850.post@n3.nabble.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Thu, 29 May 2014 09:09:46 -0700
Message-ID: <CA+-p3AGYXHGT2wJaohx8qCsUUEDN1WAf4jUAFaAVJqFY8bmXwQ@mail.gmail.com>
Subject: Re: Timestamp support in v1.0
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a113a5a9a8b0f9404fa8c29c2
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a5a9a8b0f9404fa8c29c2
Content-Type: text/plain; charset=UTF-8

I can confirm that the commit is included in the 1.0.0 release candidates
(it was committed before branch-1.0 split off from master), but I can't
confirm that it works in PySpark.  Generally the Python and Java interfaces
lag a little behind the Scala interface to Spark, but we're working to keep
that diff much smaller going forward.

Can you try the same thing in Scala?


On Thu, May 29, 2014 at 8:54 AM, dataginjaninja <rickett.stephanie@gmail.com
> wrote:

> Can anyone verify which rc  [SPARK-1360] Add Timestamp Support for SQL #275
> <https://github.com/apache/spark/pull/275>   is included in? I am running
> rc3, but receiving errors with TIMESTAMP as a datatype in my Hive tables
> when trying to use them in pyspark.
>
> *The error I get:
> *
> 14/05/29 15:44:47 INFO ParseDriver: Parsing command: SELECT COUNT(*) FROM
> aol
> 14/05/29 15:44:48 INFO ParseDriver: Parse Completed
> 14/05/29 15:44:48 INFO metastore: Trying to connect to metastore with URI
> thrift:
> 14/05/29 15:44:48 INFO metastore: Waiting 1 seconds before next connection
> attempt.
> 14/05/29 15:44:49 INFO metastore: Connected to metastore.
> Traceback (most recent call last):
>   File "<stdin>", line 1, in <module>
>   File "/opt/spark-1.0.0-rc3/python/pyspark/sql.py", line 189, in hql
>     return self.hiveql(hqlQuery)
>   File "/opt/spark-1.0.0-rc3/python/pyspark/sql.py", line 183, in hiveql
>     return SchemaRDD(self._ssql_ctx.hiveql(hqlQuery), self)
>   File
> "/opt/spark-1.0.0-rc3/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py",
> line 537, in __call__
>   File
> "/opt/spark-1.0.0-rc3/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py", line
> 300, in get_return_value
> py4j.protocol.Py4JJavaError: An error occurred while calling o14.hiveql.
> : java.lang.RuntimeException: Unsupported dataType: timestamp
>
> *The table I loaded:*
> DROP TABLE IF EXISTS aol;
> CREATE EXTERNAL TABLE aol (
>         userid STRING,
>         query STRING,
>         query_time TIMESTAMP,
>         item_rank INT,
>         click_url STRING)
> ROW FORMAT DELIMITED
> FIELDS TERMINATED BY '\t'
> LOCATION '/tmp/data/aol';
>
> *The pyspark commands:*
> from pyspark.sql import HiveContext
> hctx= HiveContext(sc)
> results = hctx.hql("SELECT COUNT(*) FROM aol").collect()
>
>
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Timestamp-support-in-v1-0-tp6850.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a113a5a9a8b0f9404fa8c29c2--

From dev-return-7870-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 16:14:34 2014
Return-Path: <dev-return-7870-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 587BC1015A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 16:14:34 +0000 (UTC)
Received: (qmail 89039 invoked by uid 500); 29 May 2014 16:14:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88983 invoked by uid 500); 29 May 2014 16:14:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88975 invoked by uid 99); 29 May 2014 16:14:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 16:14:34 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.52 as permitted sender)
Received: from [209.85.219.52] (HELO mail-oa0-f52.google.com) (209.85.219.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 16:14:30 +0000
Received: by mail-oa0-f52.google.com with SMTP id eb12so559081oac.39
        for <dev@spark.apache.org>; Thu, 29 May 2014 09:14:09 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=ytQ83mAUQV+uyYc+DlSdzGpQGY0u8V08Lxq5FJAp1AQ=;
        b=ClK8/1bnNlaomx9LdVIge+OQAQo5yyZEqW/SC/MqF22dYhHooirJ1c7UnL3QfXYRVE
         t6kJRwX8gqve0wHRESPNc4wl6iAVOf1jHWI/KFuKf0rc7X9pe/oSURZ3lWLS+Sog48+/
         c0FeMKJrex0kQdddjNwDis7UwGUNYPoZkYHSENW8tpK5BTtl+QRdVeaKfd3QPHR/Trv7
         0alqJ+mpa6Zca25orL1AxXekUmN1GskvEmHe8GEa+AODRJphBwLrlyjZ5edGw/WWVWTv
         U9yMNutgAg9KtVX5oHOm3F0P7ZXC6dYiwOFnRiMnv0r0Wp9zSg49U6c0SMCHMUTDzTQN
         mepw==
MIME-Version: 1.0
X-Received: by 10.182.29.195 with SMTP id m3mr9601583obh.33.1401380049636;
 Thu, 29 May 2014 09:14:09 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Thu, 29 May 2014 09:14:09 -0700 (PDT)
In-Reply-To: <5386B9C8.6060608@oracle.com>
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
	<BA089183-FAF4-4B65-8118-9DDB7E744AFB@webtrends.com>
	<CAJgQjQ9z3V4L7HDD-Ksck1JyuAbgt0xcqoeTO27RGq1DKWJtQA@mail.gmail.com>
	<CALEZFQyQu=mgxgF-wRC6kpOd0Yq+rVHXvCcLxEkmQh9xVtLErA@mail.gmail.com>
	<CAOTBr2m1-wuwahrZUS8A0izqOHQJWCr9KywPWXi00-9fBAiPbQ@mail.gmail.com>
	<5386B9C8.6060608@oracle.com>
Date: Thu, 29 May 2014 09:14:09 -0700
Message-ID: <CABPQxssaQTUMaE4OoEy=B7WDjpVkiS4hB-vuvQFGYdCXdfky-A@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

+1

I spun up a few EC2 clusters and ran my normal audit checks. Tests
passing, sigs, CHANGES and NOTICE look good

Thanks TD for helping cut this RC!

On Wed, May 28, 2014 at 9:38 PM, Kevin Markey <kevin.markey@oracle.com> wrote:
> +1
>
> Built -Pyarn -Phadoop-2.3 -Dhadoop.version=2.3.0
> Ran current version of one of my applications on 1-node pseudocluster
> (sorry, unable to test on full cluster).
> yarn-cluster mode
> Ran regression tests.
>
> Thanks
> Kevin
>
>
> On 05/28/2014 09:55 PM, Krishna Sankar wrote:
>>
>> +1
>> Pulled & built on MacOS X, EC2 Amazon Linux
>> Ran test programs on OS X, 5 node c3.4xlarge cluster
>> Cheers
>> <k/>
>>
>>
>> On Wed, May 28, 2014 at 7:36 PM, Andy Konwinski
>> <andykonwinski@gmail.com>wrote:
>>
>>> +1
>>> On May 28, 2014 7:05 PM, "Xiangrui Meng" <mengxr@gmail.com> wrote:
>>>
>>>> +1
>>>>
>>>> Tested apps with standalone client mode and yarn cluster and client
>>>
>>> modes.
>>>>
>>>> Xiangrui
>>>>
>>>> On Wed, May 28, 2014 at 1:07 PM, Sean McNamara
>>>> <Sean.McNamara@webtrends.com> wrote:
>>>>>
>>>>> Pulled down, compiled, and tested examples on OS X and ubuntu.
>>>>> Deployed app we are building on spark and poured data through it.
>>>>>
>>>>> +1
>>>>>
>>>>> Sean
>>>>>
>>>>>
>>>>> On May 26, 2014, at 8:39 AM, Tathagata Das <
>>>
>>> tathagata.das1565@gmail.com>
>>>>
>>>> wrote:
>>>>>>
>>>>>> Please vote on releasing the following candidate as Apache Spark
>>>>
>>>> version 1.0.0!
>>>>>>
>>>>>> This has a few important bug fixes on top of rc10:
>>>>>> SPARK-1900 and SPARK-1918: https://github.com/apache/spark/pull/853
>>>>>> SPARK-1870: https://github.com/apache/spark/pull/848
>>>>>> SPARK-1897: https://github.com/apache/spark/pull/849
>>>>>>
>>>>>> The tag to be voted on is v1.0.0-rc11 (commit c69d97cd):
>>>>>>
>>>
>>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=c69d97cdb42f809cb71113a1db4194c21372242a
>>>>>>
>>>>>> The release files, including signatures, digests, etc. can be found
>>>
>>> at:
>>>>>>
>>>>>> http://people.apache.org/~tdas/spark-1.0.0-rc11/
>>>>>>
>>>>>> Release artifacts are signed with the following key:
>>>>>> https://people.apache.org/keys/committer/tdas.asc
>>>>>>
>>>>>> The staging repository for this release can be found at:
>>>>>>
>>> https://repository.apache.org/content/repositories/orgapachespark-1019/
>>>>>>
>>>>>> The documentation corresponding to this release can be found at:
>>>>>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/
>>>>>>
>>>>>> Please vote on releasing this package as Apache Spark 1.0.0!
>>>>>>
>>>>>> The vote is open until Thursday, May 29, at 16:00 UTC and passes if
>>>>>> a majority of at least 3 +1 PMC votes are cast.
>>>>>>
>>>>>> [ ] +1 Release this package as Apache Spark 1.0.0
>>>>>> [ ] -1 Do not release this package because ...
>>>>>>
>>>>>> To learn more about Apache Spark, please see
>>>>>> http://spark.apache.org/
>>>>>>
>>>>>> == API Changes ==
>>>>>> We welcome users to compile Spark applications against 1.0. There are
>>>>>> a few API changes in this release. Here are links to the associated
>>>>>> upgrade guides - user facing changes have been kept as small as
>>>>>> possible.
>>>>>>
>>>>>> Changes to ML vector specification:
>>>>>>
>>>
>>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/mllib-guide.html#from-09-to-10
>>>>>>
>>>>>> Changes to the Java API:
>>>>>>
>>>
>>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>>>>>>
>>>>>> Changes to the streaming API:
>>>>>>
>>>
>>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>>>>>>
>>>>>> Changes to the GraphX API:
>>>>>>
>>>
>>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>>>>>>
>>>>>> Other changes:
>>>>>> coGroup and related functions now return Iterable[T] instead of Seq[T]
>>>>>> ==> Call toSeq on the result to restore the old behavior
>>>>>>
>>>>>> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
>>>>>> ==> Call toSeq on the result to restore old behavior
>
>

From dev-return-7871-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 16:55:06 2014
Return-Path: <dev-return-7871-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0BD3A1036D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 16:55:06 +0000 (UTC)
Received: (qmail 66768 invoked by uid 500); 29 May 2014 16:55:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66696 invoked by uid 500); 29 May 2014 16:55:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66684 invoked by uid 99); 29 May 2014 16:55:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 16:55:05 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 16:55:02 +0000
Received: by mail-qg0-f50.google.com with SMTP id z60so1823407qgd.9
        for <dev@spark.apache.org>; Thu, 29 May 2014 09:54:38 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=5C1pSTVT1rpgHO1HvfZaWYW3jLl0CCiJTfKse99vdmM=;
        b=VGyIQ/A6mTpcbs0+1gh0CQ9qXbRWvEY168lSbM4QMnIpbFJuYNgobx/S6BRxAjG/7c
         Te2Wfuc6Qjda90GIm+DAbXbgvx68GYECju5TptT1u6A22pvVy5wE//px5Qyk5RHRml1X
         dMosLA+59FpBWASmIRjKXBlZ+vnb0zXJQhxksM/CKZvgnZJq9V0iF8ZQMRR0zqz0/J+a
         VQml6I3cdZ6WpRz9KohlwB5FYTK32IIQtbKd6aIJFVlzDI45m/Y/RBekrxMiiaGeJnZD
         qB7O2Zbk4BzNRUMwahyIgsR1CQaylf8GE9YDohQ/2RYyErwwfGdZtn7qm4xr6izFO2WO
         DmtA==
X-Gm-Message-State: ALoCoQnsQ/D+TtTvwaY4GkHwQrRHk2ihP+W6Qrax3l2jPzr9Tr3c9dT+qNdewqoPn0ezfBPY1YF2
X-Received: by 10.224.20.72 with SMTP id e8mr11899257qab.86.1401382478658;
 Thu, 29 May 2014 09:54:38 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.209.73 with HTTP; Thu, 29 May 2014 09:54:18 -0700 (PDT)
In-Reply-To: <CA+-p3AGYXHGT2wJaohx8qCsUUEDN1WAf4jUAFaAVJqFY8bmXwQ@mail.gmail.com>
References: <1401378895078-6850.post@n3.nabble.com> <CA+-p3AGYXHGT2wJaohx8qCsUUEDN1WAf4jUAFaAVJqFY8bmXwQ@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Thu, 29 May 2014 09:54:18 -0700
Message-ID: <CAAswR-7_5OPBKqO5BS_mUdH6eNGq5gzv1Qg27T=mqdbdCFy7Gg@mail.gmail.com>
Subject: Re: Timestamp support in v1.0
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1bfead614e104fa8cc86e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1bfead614e104fa8cc86e
Content-Type: text/plain; charset=UTF-8

Thanks for reporting this!

https://issues.apache.org/jira/browse/SPARK-1964
https://github.com/apache/spark/pull/913

If you could test out that PR and see if it fixes your problems I'd really
appreciate it!

Michael


On Thu, May 29, 2014 at 9:09 AM, Andrew Ash <andrew@andrewash.com> wrote:

> I can confirm that the commit is included in the 1.0.0 release candidates
> (it was committed before branch-1.0 split off from master), but I can't
> confirm that it works in PySpark.  Generally the Python and Java interfaces
> lag a little behind the Scala interface to Spark, but we're working to keep
> that diff much smaller going forward.
>
> Can you try the same thing in Scala?
>
>
> On Thu, May 29, 2014 at 8:54 AM, dataginjaninja <
> rickett.stephanie@gmail.com
> > wrote:
>
> > Can anyone verify which rc  [SPARK-1360] Add Timestamp Support for SQL
> #275
> > <https://github.com/apache/spark/pull/275>   is included in? I am
> running
> > rc3, but receiving errors with TIMESTAMP as a datatype in my Hive tables
> > when trying to use them in pyspark.
> >
> > *The error I get:
> > *
> > 14/05/29 15:44:47 INFO ParseDriver: Parsing command: SELECT COUNT(*) FROM
> > aol
> > 14/05/29 15:44:48 INFO ParseDriver: Parse Completed
> > 14/05/29 15:44:48 INFO metastore: Trying to connect to metastore with URI
> > thrift:
> > 14/05/29 15:44:48 INFO metastore: Waiting 1 seconds before next
> connection
> > attempt.
> > 14/05/29 15:44:49 INFO metastore: Connected to metastore.
> > Traceback (most recent call last):
> >   File "<stdin>", line 1, in <module>
> >   File "/opt/spark-1.0.0-rc3/python/pyspark/sql.py", line 189, in hql
> >     return self.hiveql(hqlQuery)
> >   File "/opt/spark-1.0.0-rc3/python/pyspark/sql.py", line 183, in hiveql
> >     return SchemaRDD(self._ssql_ctx.hiveql(hqlQuery), self)
> >   File
> >
> "/opt/spark-1.0.0-rc3/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py",
> > line 537, in __call__
> >   File
> > "/opt/spark-1.0.0-rc3/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py",
> line
> > 300, in get_return_value
> > py4j.protocol.Py4JJavaError: An error occurred while calling o14.hiveql.
> > : java.lang.RuntimeException: Unsupported dataType: timestamp
> >
> > *The table I loaded:*
> > DROP TABLE IF EXISTS aol;
> > CREATE EXTERNAL TABLE aol (
> >         userid STRING,
> >         query STRING,
> >         query_time TIMESTAMP,
> >         item_rank INT,
> >         click_url STRING)
> > ROW FORMAT DELIMITED
> > FIELDS TERMINATED BY '\t'
> > LOCATION '/tmp/data/aol';
> >
> > *The pyspark commands:*
> > from pyspark.sql import HiveContext
> > hctx= HiveContext(sc)
> > results = hctx.hql("SELECT COUNT(*) FROM aol").collect()
> >
> >
> >
> >
> >
> >
> > --
> > View this message in context:
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Timestamp-support-in-v1-0-tp6850.html
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> >
>

--001a11c1bfead614e104fa8cc86e--

From dev-return-7872-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 16:55:06 2014
Return-Path: <dev-return-7872-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9DACB1036E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 16:55:06 +0000 (UTC)
Received: (qmail 66955 invoked by uid 500); 29 May 2014 16:55:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66707 invoked by uid 500); 29 May 2014 16:55:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66697 invoked by uid 99); 29 May 2014 16:55:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 16:55:05 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 16:55:02 +0000
Received: by mail-qg0-f42.google.com with SMTP id q107so1844468qgd.1
        for <dev@spark.incubator.apache.org>; Thu, 29 May 2014 09:54:38 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=5C1pSTVT1rpgHO1HvfZaWYW3jLl0CCiJTfKse99vdmM=;
        b=MtuZLtAqHMAWQt/GFpk6JYfCmEeGTPbHiM5Lps3TRDqwJAfo/Pb/egbTM5KN+oPUHO
         UPbWzYwHynsRqzzjfPaxR7WyPiduS8cROFlObGm1LVEX73wuwJ/pO/8BRz9U4ZrMYazk
         R9Cl5V7tK1UPD/oWOlCFnQK+CpZ5w31aVwNzltljYJWe54p6moh4gsGY6UtuOQwLCKS2
         zoNWC9pa5WmAYyCR5zDK3Fxlx59/2msNfGGaT6e+l1sFezf0T2Rqo7Cf6i0eqsGmB6Zb
         0UNyT8/+ziRvsszHg1THGLEIZnpuqkaXnyWNWYqofty8d1husRIJ/i/qPYxD5ajzBJDa
         o9Nw==
X-Gm-Message-State: ALoCoQkdTcwUxhV8PL/vkgqEsNo2A2UPgXCi1nzvplo5AhxMe/mx3q2YAGRVCFvh8Y7Cn8pwTCxh
X-Received: by 10.224.20.72 with SMTP id e8mr11899257qab.86.1401382478658;
 Thu, 29 May 2014 09:54:38 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.209.73 with HTTP; Thu, 29 May 2014 09:54:18 -0700 (PDT)
In-Reply-To: <CA+-p3AGYXHGT2wJaohx8qCsUUEDN1WAf4jUAFaAVJqFY8bmXwQ@mail.gmail.com>
References: <1401378895078-6850.post@n3.nabble.com> <CA+-p3AGYXHGT2wJaohx8qCsUUEDN1WAf4jUAFaAVJqFY8bmXwQ@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Thu, 29 May 2014 09:54:18 -0700
Message-ID: <CAAswR-7_5OPBKqO5BS_mUdH6eNGq5gzv1Qg27T=mqdbdCFy7Gg@mail.gmail.com>
Subject: Re: Timestamp support in v1.0
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1bfead614e104fa8cc86e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1bfead614e104fa8cc86e
Content-Type: text/plain; charset=UTF-8

Thanks for reporting this!

https://issues.apache.org/jira/browse/SPARK-1964
https://github.com/apache/spark/pull/913

If you could test out that PR and see if it fixes your problems I'd really
appreciate it!

Michael


On Thu, May 29, 2014 at 9:09 AM, Andrew Ash <andrew@andrewash.com> wrote:

> I can confirm that the commit is included in the 1.0.0 release candidates
> (it was committed before branch-1.0 split off from master), but I can't
> confirm that it works in PySpark.  Generally the Python and Java interfaces
> lag a little behind the Scala interface to Spark, but we're working to keep
> that diff much smaller going forward.
>
> Can you try the same thing in Scala?
>
>
> On Thu, May 29, 2014 at 8:54 AM, dataginjaninja <
> rickett.stephanie@gmail.com
> > wrote:
>
> > Can anyone verify which rc  [SPARK-1360] Add Timestamp Support for SQL
> #275
> > <https://github.com/apache/spark/pull/275>   is included in? I am
> running
> > rc3, but receiving errors with TIMESTAMP as a datatype in my Hive tables
> > when trying to use them in pyspark.
> >
> > *The error I get:
> > *
> > 14/05/29 15:44:47 INFO ParseDriver: Parsing command: SELECT COUNT(*) FROM
> > aol
> > 14/05/29 15:44:48 INFO ParseDriver: Parse Completed
> > 14/05/29 15:44:48 INFO metastore: Trying to connect to metastore with URI
> > thrift:
> > 14/05/29 15:44:48 INFO metastore: Waiting 1 seconds before next
> connection
> > attempt.
> > 14/05/29 15:44:49 INFO metastore: Connected to metastore.
> > Traceback (most recent call last):
> >   File "<stdin>", line 1, in <module>
> >   File "/opt/spark-1.0.0-rc3/python/pyspark/sql.py", line 189, in hql
> >     return self.hiveql(hqlQuery)
> >   File "/opt/spark-1.0.0-rc3/python/pyspark/sql.py", line 183, in hiveql
> >     return SchemaRDD(self._ssql_ctx.hiveql(hqlQuery), self)
> >   File
> >
> "/opt/spark-1.0.0-rc3/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py",
> > line 537, in __call__
> >   File
> > "/opt/spark-1.0.0-rc3/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py",
> line
> > 300, in get_return_value
> > py4j.protocol.Py4JJavaError: An error occurred while calling o14.hiveql.
> > : java.lang.RuntimeException: Unsupported dataType: timestamp
> >
> > *The table I loaded:*
> > DROP TABLE IF EXISTS aol;
> > CREATE EXTERNAL TABLE aol (
> >         userid STRING,
> >         query STRING,
> >         query_time TIMESTAMP,
> >         item_rank INT,
> >         click_url STRING)
> > ROW FORMAT DELIMITED
> > FIELDS TERMINATED BY '\t'
> > LOCATION '/tmp/data/aol';
> >
> > *The pyspark commands:*
> > from pyspark.sql import HiveContext
> > hctx= HiveContext(sc)
> > results = hctx.hql("SELECT COUNT(*) FROM aol").collect()
> >
> >
> >
> >
> >
> >
> > --
> > View this message in context:
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Timestamp-support-in-v1-0-tp6850.html
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> >
>

--001a11c1bfead614e104fa8cc86e--

From dev-return-7873-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 16:55:52 2014
Return-Path: <dev-return-7873-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5DFA81037E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 16:55:52 +0000 (UTC)
Received: (qmail 68686 invoked by uid 500); 29 May 2014 16:55:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68628 invoked by uid 500); 29 May 2014 16:55:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68620 invoked by uid 99); 29 May 2014 16:55:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 16:55:46 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of rickett.stephanie@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 16:55:42 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <rickett.stephanie@gmail.com>)
	id 1Wq3bu-0007Kw-4I
	for dev@spark.incubator.apache.org; Thu, 29 May 2014 09:55:22 -0700
Date: Thu, 29 May 2014 09:55:22 -0700 (PDT)
From: dataginjaninja <rickett.stephanie@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1401382522124-6853.post@n3.nabble.com>
In-Reply-To: <CA+-p3AGYXHGT2wJaohx8qCsUUEDN1WAf4jUAFaAVJqFY8bmXwQ@mail.gmail.com>
References: <1401378895078-6850.post@n3.nabble.com> <CA+-p3AGYXHGT2wJaohx8qCsUUEDN1WAf4jUAFaAVJqFY8bmXwQ@mail.gmail.com>
Subject: Re: Timestamp support in v1.0
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Yes, I get the same error:

scala> val hc = new org.apache.spark.sql.hive.HiveContext(sc)
14/05/29 16:53:40 INFO deprecation: mapred.input.dir.recursive is
deprecated. Instead, use mapreduce.input.fileinputformat.input.dir.recursive
14/05/29 16:53:40 INFO deprecation: mapred.max.split.size is deprecated.
Instead, use mapreduce.input.fileinputformat.split.maxsize
14/05/29 16:53:40 INFO deprecation: mapred.min.split.size is deprecated.
Instead, use mapreduce.input.fileinputformat.split.minsize
14/05/29 16:53:40 INFO deprecation: mapred.min.split.size.per.rack is
deprecated. Instead, use
mapreduce.input.fileinputformat.split.minsize.per.rack
14/05/29 16:53:40 INFO deprecation: mapred.min.split.size.per.node is
deprecated. Instead, use
mapreduce.input.fileinputformat.split.minsize.per.node
14/05/29 16:53:40 INFO deprecation: mapred.reduce.tasks is deprecated.
Instead, use mapreduce.job.reduces
14/05/29 16:53:40 INFO deprecation:
mapred.reduce.tasks.speculative.execution is deprecated. Instead, use
mapreduce.reduce.speculative
14/05/29 16:53:41 WARN HiveConf: DEPRECATED: Configuration property
hive.metastore.local no longer has any effect. Make sure to provide a valid
value for hive.metastore.uris if you are connecting to a remote metastore.
14/05/29 16:53:42 WARN HiveConf: DEPRECATED: Configuration property
hive.metastore.local no longer has any effect. Make sure to provide a valid
value for hive.metastore.uris if you are connecting to a remote metastore.
hc: org.apache.spark.sql.hive.HiveContext =
org.apache.spark.sql.hive.HiveContext@36482814

scala> val results = hc.hql("SELECT COUNT(*) FROM aol").collect() 
14/05/29 16:53:46 INFO ParseDriver: Parsing command: SELECT COUNT(*) FROM
aol
14/05/29 16:53:46 INFO ParseDriver: Parse Completed
14/05/29 16:53:47 INFO metastore: Trying to connect to metastore with URI th
14/05/29 16:53:47 INFO metastore: Waiting 1 seconds before next connection
attempt.
14/05/29 16:53:48 INFO metastore: Connected to metastore.
java.lang.RuntimeException: Unsupported dataType: timestamp
	at scala.sys.package$.error(package.scala:27)




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Timestamp-support-in-v1-0-tp6850p6853.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7874-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 17:02:46 2014
Return-Path: <dev-return-7874-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9211510412
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 17:02:46 +0000 (UTC)
Received: (qmail 80292 invoked by uid 500); 29 May 2014 17:02:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80236 invoked by uid 500); 29 May 2014 17:02:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80222 invoked by uid 99); 29 May 2014 17:02:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 17:02:41 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of rickett.stephanie@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 17:02:37 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <rickett.stephanie@gmail.com>)
	id 1Wq3iX-000817-Fc
	for dev@spark.incubator.apache.org; Thu, 29 May 2014 10:02:13 -0700
Date: Thu, 29 May 2014 10:02:13 -0700 (PDT)
From: dataginjaninja <rickett.stephanie@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1401382933476-6855.post@n3.nabble.com>
In-Reply-To: <CAAswR-7_5OPBKqO5BS_mUdH6eNGq5gzv1Qg27T=mqdbdCFy7Gg@mail.gmail.com>
References: <1401378895078-6850.post@n3.nabble.com> <CA+-p3AGYXHGT2wJaohx8qCsUUEDN1WAf4jUAFaAVJqFY8bmXwQ@mail.gmail.com> <CAAswR-7_5OPBKqO5BS_mUdH6eNGq5gzv1Qg27T=mqdbdCFy7Gg@mail.gmail.com>
Subject: Re: Timestamp support in v1.0
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Michael,

Will I have to rebuild after adding the change? Thanks



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Timestamp-support-in-v1-0-tp6850p6855.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7875-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 17:09:33 2014
Return-Path: <dev-return-7875-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3E2821047A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 17:09:33 +0000 (UTC)
Received: (qmail 99775 invoked by uid 500); 29 May 2014 17:09:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99721 invoked by uid 500); 29 May 2014 17:09:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99713 invoked by uid 99); 29 May 2014 17:09:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 17:09:32 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.45] (HELO mail-qg0-f45.google.com) (209.85.192.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 17:09:28 +0000
Received: by mail-qg0-f45.google.com with SMTP id z60so1862759qgd.32
        for <dev@spark.apache.org>; Thu, 29 May 2014 10:09:07 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=iUAOz6YChSIqVacARxepk2w94d+igDVdF5Rlw7B+bos=;
        b=kUBA6LFnuSudcdY2kUNZbJZF+//5gmvUAuXQkgedCMQloO6d2WCmFXuL+Es4or+qfu
         yTQI5ZBijyCD0uejh8GoWkK6/Pdzo0WV629FDAriYjJVQ7TzkTcrW2Y7yTv5jhlQDjkh
         wC878ahwLilcsccp7MR/er4/a7JUKfkq+RBNaTkc8p1ub85rlOsNicB2x3vMo16pHu7m
         5TbHNHwsf6c9evStJCLhi6tyV3lZqf1vq3E6BCxoug5z6ajKMSmlIsDiZqqd4+UFOr3J
         wAsJpcB+Iq6KzGNBDfKuSa9YB/mskh7jQuc3c94L5he2+ZSzwhF5vW4zZuQJuRzbAzjc
         mVJQ==
X-Gm-Message-State: ALoCoQlS6mug14pRgtV5dVpHU1RWZsOfKXESLoZ8m6MLNxRtiFrNuFLB7MQzJoFzCOs8FaPpjKhe
X-Received: by 10.224.169.6 with SMTP id w6mr12273957qay.102.1401383347357;
 Thu, 29 May 2014 10:09:07 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.209.73 with HTTP; Thu, 29 May 2014 10:08:46 -0700 (PDT)
In-Reply-To: <1401382933476-6855.post@n3.nabble.com>
References: <1401378895078-6850.post@n3.nabble.com> <CA+-p3AGYXHGT2wJaohx8qCsUUEDN1WAf4jUAFaAVJqFY8bmXwQ@mail.gmail.com>
 <CAAswR-7_5OPBKqO5BS_mUdH6eNGq5gzv1Qg27T=mqdbdCFy7Gg@mail.gmail.com> <1401382933476-6855.post@n3.nabble.com>
From: Michael Armbrust <michael@databricks.com>
Date: Thu, 29 May 2014 10:08:46 -0700
Message-ID: <CAAswR-4_ZmxRatf9bHd1sg+=38LUwu=ocwPuX3R2EjrNsRsGKA@mail.gmail.com>
Subject: Re: Timestamp support in v1.0
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0160c2309d42b604fa8cfcc7
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160c2309d42b604fa8cfcc7
Content-Type: text/plain; charset=UTF-8

Yes, you'll need to download the code from that PR and reassemble Spark
(sbt/sbt assembly).


On Thu, May 29, 2014 at 10:02 AM, dataginjaninja <
rickett.stephanie@gmail.com> wrote:

> Michael,
>
> Will I have to rebuild after adding the change? Thanks
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Timestamp-support-in-v1-0-tp6850p6855.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--089e0160c2309d42b604fa8cfcc7--

From dev-return-7876-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 17:17:28 2014
Return-Path: <dev-return-7876-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EDCBB104EC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 17:17:28 +0000 (UTC)
Received: (qmail 20051 invoked by uid 500); 29 May 2014 17:17:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19990 invoked by uid 500); 29 May 2014 17:17:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19982 invoked by uid 99); 29 May 2014 17:17:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 17:17:28 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of rickett.stephanie@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 17:17:23 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <rickett.stephanie@gmail.com>)
	id 1Wq3wt-0000xR-IO
	for dev@spark.incubator.apache.org; Thu, 29 May 2014 10:17:03 -0700
Date: Thu, 29 May 2014 10:17:03 -0700 (PDT)
From: dataginjaninja <rickett.stephanie@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1401383823560-6857.post@n3.nabble.com>
In-Reply-To: <CAAswR-4_ZmxRatf9bHd1sg+=38LUwu=ocwPuX3R2EjrNsRsGKA@mail.gmail.com>
References: <1401378895078-6850.post@n3.nabble.com> <CA+-p3AGYXHGT2wJaohx8qCsUUEDN1WAf4jUAFaAVJqFY8bmXwQ@mail.gmail.com> <CAAswR-7_5OPBKqO5BS_mUdH6eNGq5gzv1Qg27T=mqdbdCFy7Gg@mail.gmail.com> <1401382933476-6855.post@n3.nabble.com> <CAAswR-4_ZmxRatf9bHd1sg+=38LUwu=ocwPuX3R2EjrNsRsGKA@mail.gmail.com>
Subject: Re: Timestamp support in v1.0
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Darn, I was hoping just to sneak it in that file. I am not the only person
working on the cluster; if I rebuild it that means I have to redeploy
everything to all the nodes as well.  So I cannot do that ... today. If
someone else doesn't beat me to it. I can rebuild at another time. 



-----
Cheers,

Stephanie
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Timestamp-support-in-v1-0-tp6850p6857.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7877-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 18:12:18 2014
Return-Path: <dev-return-7877-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B83EE108B9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 18:12:18 +0000 (UTC)
Received: (qmail 85305 invoked by uid 500); 29 May 2014 18:12:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85250 invoked by uid 500); 29 May 2014 18:12:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85242 invoked by uid 99); 29 May 2014 18:12:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 18:12:18 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.46] (HELO mail-qg0-f46.google.com) (209.85.192.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 18:12:13 +0000
Received: by mail-qg0-f46.google.com with SMTP id q108so2095442qgd.33
        for <dev@spark.incubator.apache.org>; Thu, 29 May 2014 11:11:52 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=ENzimIdSngP09DABAxUu+ihgl82psUKOG848vTMJgi0=;
        b=Xdto+FHvc2ePGxjF/lyf6KyXLEaBBOk7I/Qv3wdg6D0mzY07jffSn9pbH/UAx93PYh
         YFPpLRU+gSZLybEWujbFqURte+PSkbEIUr+SveBqZcOFT2JLY/VZTzvJNKfcH5qXZ8U8
         TlR/9VziMZjAu7fM9MRW6kVQLeGQadFDZlID9lKVkAnZH/Lm37QMiv5e3GwVxAkO8wQh
         85aPjtN6KAj+Cb2dnfLDBHRfviLAYTuxT+ZplRTSqQzfqFkH7qkCixIZN2Ybky4EDldt
         v7X4UH6MQIXRarAOz92UMl1D4Du5UYrEB2xhuiy0r7Pn6/vxxhHhrI5SIPHvAAbMTxs0
         VxUw==
X-Gm-Message-State: ALoCoQn4ftkDWQQK5z5L3U6IIIu/H3V/ZuL4b/YgHL82NL/xlofURnhUqxktRpkO+ljjDibgdXTj
X-Received: by 10.229.89.65 with SMTP id d1mr13049690qcm.14.1401387112724;
 Thu, 29 May 2014 11:11:52 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.209.73 with HTTP; Thu, 29 May 2014 11:11:32 -0700 (PDT)
In-Reply-To: <1401383823560-6857.post@n3.nabble.com>
References: <1401378895078-6850.post@n3.nabble.com> <CA+-p3AGYXHGT2wJaohx8qCsUUEDN1WAf4jUAFaAVJqFY8bmXwQ@mail.gmail.com>
 <CAAswR-7_5OPBKqO5BS_mUdH6eNGq5gzv1Qg27T=mqdbdCFy7Gg@mail.gmail.com>
 <1401382933476-6855.post@n3.nabble.com> <CAAswR-4_ZmxRatf9bHd1sg+=38LUwu=ocwPuX3R2EjrNsRsGKA@mail.gmail.com>
 <1401383823560-6857.post@n3.nabble.com>
From: Michael Armbrust <michael@databricks.com>
Date: Thu, 29 May 2014 11:11:32 -0700
Message-ID: <CAAswR-5gfAc3pz9Pj3P3E8gFigotkZkKfLvJjhhKe-WMUK-6UQ@mail.gmail.com>
Subject: Re: Timestamp support in v1.0
To: dev@spark.apache.org
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a1133cbc80c2a3904fa8ddd2f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133cbc80c2a3904fa8ddd2f
Content-Type: text/plain; charset=UTF-8

You should be able to get away with only doing it locally.  This bug is
happening during analysis which only occurs on the driver.


On Thu, May 29, 2014 at 10:17 AM, dataginjaninja <
rickett.stephanie@gmail.com> wrote:

> Darn, I was hoping just to sneak it in that file. I am not the only person
> working on the cluster; if I rebuild it that means I have to redeploy
> everything to all the nodes as well.  So I cannot do that ... today. If
> someone else doesn't beat me to it. I can rebuild at another time.
>
>
>
> -----
> Cheers,
>
> Stephanie
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Timestamp-support-in-v1-0-tp6850p6857.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a1133cbc80c2a3904fa8ddd2f--

From dev-return-7878-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 18:12:20 2014
Return-Path: <dev-return-7878-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 35376108BA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 18:12:20 +0000 (UTC)
Received: (qmail 86075 invoked by uid 500); 29 May 2014 18:12:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86008 invoked by uid 500); 29 May 2014 18:12:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86000 invoked by uid 99); 29 May 2014 18:12:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 18:12:19 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.49] (HELO mail-qg0-f49.google.com) (209.85.192.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 18:12:16 +0000
Received: by mail-qg0-f49.google.com with SMTP id a108so2076682qge.22
        for <dev@spark.apache.org>; Thu, 29 May 2014 11:11:52 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=ENzimIdSngP09DABAxUu+ihgl82psUKOG848vTMJgi0=;
        b=G8dGbWB1iFbHKDxqq7G31ajLfU9IRevmeHJm4kthcJ9nSY7pFjtbOhzn9nLDeIq9oD
         eZTefFhOcZS2OkrS46EdcRn/08EMjEb5cdQmRoJhdfskipIWXtxgkFo/kmrsQdOLSs80
         /iNVoSFUuZY7CZ+poyiuEswgZe3sBqaGs9s9Lige5R3zg/OHEshVw/YDcnbT9ZJ8mpIA
         WiYPzJ+8Y/TajQohnntbGwzFBBHhskWfHhVl4uTztXxrMyKGCnBYaN9Ufin76ZYPbAfk
         rpiyc8sGSuSeS+2WBVSvobmbQ81xt1Mkp91CEBW+QiymdExCiU7ZnI7pr4SNbu54uHbH
         swJQ==
X-Gm-Message-State: ALoCoQn6UOH3kjZmZX2TkhDR2M1NwuJzaTOfd+cY1R0/iLlINksFX+7cr2F7K6i2Ivrv6/VMpjO4
X-Received: by 10.229.89.65 with SMTP id d1mr13049690qcm.14.1401387112724;
 Thu, 29 May 2014 11:11:52 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.209.73 with HTTP; Thu, 29 May 2014 11:11:32 -0700 (PDT)
In-Reply-To: <1401383823560-6857.post@n3.nabble.com>
References: <1401378895078-6850.post@n3.nabble.com> <CA+-p3AGYXHGT2wJaohx8qCsUUEDN1WAf4jUAFaAVJqFY8bmXwQ@mail.gmail.com>
 <CAAswR-7_5OPBKqO5BS_mUdH6eNGq5gzv1Qg27T=mqdbdCFy7Gg@mail.gmail.com>
 <1401382933476-6855.post@n3.nabble.com> <CAAswR-4_ZmxRatf9bHd1sg+=38LUwu=ocwPuX3R2EjrNsRsGKA@mail.gmail.com>
 <1401383823560-6857.post@n3.nabble.com>
From: Michael Armbrust <michael@databricks.com>
Date: Thu, 29 May 2014 11:11:32 -0700
Message-ID: <CAAswR-5gfAc3pz9Pj3P3E8gFigotkZkKfLvJjhhKe-WMUK-6UQ@mail.gmail.com>
Subject: Re: Timestamp support in v1.0
To: dev@spark.apache.org
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a1133cbc80c2a3904fa8ddd2f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133cbc80c2a3904fa8ddd2f
Content-Type: text/plain; charset=UTF-8

You should be able to get away with only doing it locally.  This bug is
happening during analysis which only occurs on the driver.


On Thu, May 29, 2014 at 10:17 AM, dataginjaninja <
rickett.stephanie@gmail.com> wrote:

> Darn, I was hoping just to sneak it in that file. I am not the only person
> working on the cluster; if I rebuild it that means I have to redeploy
> everything to all the nodes as well.  So I cannot do that ... today. If
> someone else doesn't beat me to it. I can rebuild at another time.
>
>
>
> -----
> Cheers,
>
> Stephanie
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Timestamp-support-in-v1-0-tp6850p6857.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a1133cbc80c2a3904fa8ddd2f--

From dev-return-7879-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 20:23:59 2014
Return-Path: <dev-return-7879-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EC1221001C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 20:23:58 +0000 (UTC)
Received: (qmail 82798 invoked by uid 500); 29 May 2014 20:23:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82736 invoked by uid 500); 29 May 2014 20:23:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82728 invoked by uid 99); 29 May 2014 20:23:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 20:23:58 +0000
X-ASF-Spam-Status: No, hits=-0.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.128.176 as permitted sender)
Received: from [209.85.128.176] (HELO mail-ve0-f176.google.com) (209.85.128.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 20:23:54 +0000
Received: by mail-ve0-f176.google.com with SMTP id jz11so1032634veb.7
        for <dev@spark.apache.org>; Thu, 29 May 2014 13:23:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=IMqe63Qo75AAL14PnVDbE941bwPIe08eC/Kq/2zZ8tI=;
        b=kgXVQrLwky7KgxvaGWh7fMd+UwsSeewnaVfDrNNQbAgfzj0COhvdPQYoYHg/vxcer/
         aBXMNscwQZoJVdbQUH1H5MJzdmo85dwEQxK5tz20m28sM668nGrHSs6+81RDmGNCXHV1
         wbvkCBCJtr+3s7Sf29/8GNasAgBYkn3zTcBmCbXpCJhSfip31WBdHsWud8vXcobQd7Em
         G9PRJwuKW/k2T6gOwxgJ7ai1rmKwf7J6EEyMeiJrOfXfRRE7WRWUCe5D9Dd27B7MgF9J
         vZywuFUU8YjNmRdlVr3jkrq70CXB4l/v6HnpOX6a22qoO020mzteS08rQ06VqEGcIBhn
         gT+A==
X-Received: by 10.52.35.69 with SMTP id f5mr3021568vdj.83.1401395014046; Thu,
 29 May 2014 13:23:34 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.118.9 with HTTP; Thu, 29 May 2014 13:23:03 -0700 (PDT)
In-Reply-To: <CABPQxssaQTUMaE4OoEy=B7WDjpVkiS4hB-vuvQFGYdCXdfky-A@mail.gmail.com>
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
 <BA089183-FAF4-4B65-8118-9DDB7E744AFB@webtrends.com> <CAJgQjQ9z3V4L7HDD-Ksck1JyuAbgt0xcqoeTO27RGq1DKWJtQA@mail.gmail.com>
 <CALEZFQyQu=mgxgF-wRC6kpOd0Yq+rVHXvCcLxEkmQh9xVtLErA@mail.gmail.com>
 <CAOTBr2m1-wuwahrZUS8A0izqOHQJWCr9KywPWXi00-9fBAiPbQ@mail.gmail.com>
 <5386B9C8.6060608@oracle.com> <CABPQxssaQTUMaE4OoEy=B7WDjpVkiS4hB-vuvQFGYdCXdfky-A@mail.gmail.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Thu, 29 May 2014 13:23:03 -0700
Message-ID: <CAMwrk0m-SPuDvabsp5h=BP3_s0VTV4GopV7bcg5CExnnRpXBdw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Let me put in my +1 as well!

This voting is now closed, and it successfully passes with 13 "+1"
votes and one "0" vote.
Thanks to everyone who tested the RC and voted. Here are the totals:

+1: (13 votes)
Matei Zaharia*
Mark Hamstra*
Holden Karau
Nick Pentreath*
Will Benton
Henry Saputra
Sean McNamara*
Xiangrui Meng*
Andy Konwinski*
Krishna Sankar
Kevin Markey
Patrick Wendell*
Tathagata Das*

0: (1 vote)
Ankur Dave*

-1: (0 vote)

* = binding

Please hold off announcing Spark 1.0.0 until Apache Software
Foundation makes the press release tomorrow. Thank you very much for
your cooperation.

TD

On Thu, May 29, 2014 at 9:14 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> +1
>
> I spun up a few EC2 clusters and ran my normal audit checks. Tests
> passing, sigs, CHANGES and NOTICE look good
>
> Thanks TD for helping cut this RC!
>
> On Wed, May 28, 2014 at 9:38 PM, Kevin Markey <kevin.markey@oracle.com> wrote:
>> +1
>>
>> Built -Pyarn -Phadoop-2.3 -Dhadoop.version=2.3.0
>> Ran current version of one of my applications on 1-node pseudocluster
>> (sorry, unable to test on full cluster).
>> yarn-cluster mode
>> Ran regression tests.
>>
>> Thanks
>> Kevin
>>
>>
>> On 05/28/2014 09:55 PM, Krishna Sankar wrote:
>>>
>>> +1
>>> Pulled & built on MacOS X, EC2 Amazon Linux
>>> Ran test programs on OS X, 5 node c3.4xlarge cluster
>>> Cheers
>>> <k/>
>>>
>>>
>>> On Wed, May 28, 2014 at 7:36 PM, Andy Konwinski
>>> <andykonwinski@gmail.com>wrote:
>>>
>>>> +1
>>>> On May 28, 2014 7:05 PM, "Xiangrui Meng" <mengxr@gmail.com> wrote:
>>>>
>>>>> +1
>>>>>
>>>>> Tested apps with standalone client mode and yarn cluster and client
>>>>
>>>> modes.
>>>>>
>>>>> Xiangrui
>>>>>
>>>>> On Wed, May 28, 2014 at 1:07 PM, Sean McNamara
>>>>> <Sean.McNamara@webtrends.com> wrote:
>>>>>>
>>>>>> Pulled down, compiled, and tested examples on OS X and ubuntu.
>>>>>> Deployed app we are building on spark and poured data through it.
>>>>>>
>>>>>> +1
>>>>>>
>>>>>> Sean
>>>>>>
>>>>>>
>>>>>> On May 26, 2014, at 8:39 AM, Tathagata Das <
>>>>
>>>> tathagata.das1565@gmail.com>
>>>>>
>>>>> wrote:
>>>>>>>
>>>>>>> Please vote on releasing the following candidate as Apache Spark
>>>>>
>>>>> version 1.0.0!
>>>>>>>
>>>>>>> This has a few important bug fixes on top of rc10:
>>>>>>> SPARK-1900 and SPARK-1918: https://github.com/apache/spark/pull/853
>>>>>>> SPARK-1870: https://github.com/apache/spark/pull/848
>>>>>>> SPARK-1897: https://github.com/apache/spark/pull/849
>>>>>>>
>>>>>>> The tag to be voted on is v1.0.0-rc11 (commit c69d97cd):
>>>>>>>
>>>>
>>>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=c69d97cdb42f809cb71113a1db4194c21372242a
>>>>>>>
>>>>>>> The release files, including signatures, digests, etc. can be found
>>>>
>>>> at:
>>>>>>>
>>>>>>> http://people.apache.org/~tdas/spark-1.0.0-rc11/
>>>>>>>
>>>>>>> Release artifacts are signed with the following key:
>>>>>>> https://people.apache.org/keys/committer/tdas.asc
>>>>>>>
>>>>>>> The staging repository for this release can be found at:
>>>>>>>
>>>> https://repository.apache.org/content/repositories/orgapachespark-1019/
>>>>>>>
>>>>>>> The documentation corresponding to this release can be found at:
>>>>>>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/
>>>>>>>
>>>>>>> Please vote on releasing this package as Apache Spark 1.0.0!
>>>>>>>
>>>>>>> The vote is open until Thursday, May 29, at 16:00 UTC and passes if
>>>>>>> a majority of at least 3 +1 PMC votes are cast.
>>>>>>>
>>>>>>> [ ] +1 Release this package as Apache Spark 1.0.0
>>>>>>> [ ] -1 Do not release this package because ...
>>>>>>>
>>>>>>> To learn more about Apache Spark, please see
>>>>>>> http://spark.apache.org/
>>>>>>>
>>>>>>> == API Changes ==
>>>>>>> We welcome users to compile Spark applications against 1.0. There are
>>>>>>> a few API changes in this release. Here are links to the associated
>>>>>>> upgrade guides - user facing changes have been kept as small as
>>>>>>> possible.
>>>>>>>
>>>>>>> Changes to ML vector specification:
>>>>>>>
>>>>
>>>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/mllib-guide.html#from-09-to-10
>>>>>>>
>>>>>>> Changes to the Java API:
>>>>>>>
>>>>
>>>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>>>>>>>
>>>>>>> Changes to the streaming API:
>>>>>>>
>>>>
>>>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>>>>>>>
>>>>>>> Changes to the GraphX API:
>>>>>>>
>>>>
>>>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>>>>>>>
>>>>>>> Other changes:
>>>>>>> coGroup and related functions now return Iterable[T] instead of Seq[T]
>>>>>>> ==> Call toSeq on the result to restore the old behavior
>>>>>>>
>>>>>>> SparkContext.jarOfClass returns Option[String] instead of Seq[String]
>>>>>>> ==> Call toSeq on the result to restore old behavior
>>
>>

From dev-return-7880-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 20:24:10 2014
Return-Path: <dev-return-7880-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6A2741001E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 20:24:10 +0000 (UTC)
Received: (qmail 83600 invoked by uid 500); 29 May 2014 20:24:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83538 invoked by uid 500); 29 May 2014 20:24:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83527 invoked by uid 99); 29 May 2014 20:24:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 20:24:09 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.51 as permitted sender)
Received: from [209.85.219.51] (HELO mail-oa0-f51.google.com) (209.85.219.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 20:24:05 +0000
Received: by mail-oa0-f51.google.com with SMTP id n16so907353oag.38
        for <dev@spark.apache.org>; Thu, 29 May 2014 13:23:45 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=48DuytKFnqexCyAIG9u2VmE019CsHzNYH/vdfD8BmoM=;
        b=w5zdMrmcazEHtW1YMzghccnhZkY4sgnjxPCK8cJNGz73um8HkWzCuTV5bUYy5eqNhe
         0SPZ78JsxOFdo1LvtkG346FQbexG7QctpbYvUzqodS+c4U33VFjSAaSWGolNG/r7+uYY
         MzbYyqzpdZ2eOGF8a8L7p4WMb8ZYZU97ZvgT57ngHb4bwVOpDkulLyBPlU5lqgeu4p8t
         xK4F949/WdBI/Mmd/9iOICXAclC/UKVg2g+ZLaq5Hz+xmv5qGON96xL6Y5AnKtpXTUBi
         HiQo6ZmuPYrMIdBSK/Z2Je27v28ALFndsw98BYF7nxae9drR3kuh5fYR1WF9SbIn63mL
         XFPw==
MIME-Version: 1.0
X-Received: by 10.182.199.5 with SMTP id jg5mr8134121obc.75.1401395024877;
 Thu, 29 May 2014 13:23:44 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Thu, 29 May 2014 13:23:44 -0700 (PDT)
In-Reply-To: <CAJiQeYKAbeAp7ZQK7bbxGF3YhfqeOOnt5-p7QO8n3g97f=YCKg@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
	<CA+-p3AEL6ct-bJwkBSXm+wTLBHEbBoGnnGOJGAjWHvQogDs-4g@mail.gmail.com>
	<CALRHqP-jLhJM3_5qCTWzrbU+GpmVsd2DFtvMXsH4-PGuT6X8ww@mail.gmail.com>
	<75094F23-AACD-4427-AE5B-A25AC02189A8@gmail.com>
	<CAJiQeY+Sy4EUKhM8ztsSfxaNoOG_73b=s7W_u5YRBPk-JegUDA@mail.gmail.com>
	<CAAsvFPm9OeOd7v2wsevLpKdqaCjfQuAA21r8itVu4-c2vft2OA@mail.gmail.com>
	<CAJiQeYKWeKoZuY9MjQTxzHn5d5XwvHwo0e8sEuEWL6CFW4p6CA@mail.gmail.com>
	<CAJiQeYKAbeAp7ZQK7bbxGF3YhfqeOOnt5-p7QO8n3g97f=YCKg@mail.gmail.com>
Date: Thu, 29 May 2014 13:23:44 -0700
Message-ID: <CABPQxst3M2r5aLur41JB+bjK9U2s9V0GSbrF45xTsQ9rS2d7gA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

[tl;dr stable API's are important - sorry, this is slightly meandering]

Hey - just wanted to chime in on this as I was travelling. Sean, you
bring up great points here about the velocity and stability of Spark.
Many projects have fairly customized semantics around what versions
actually mean (HBase is a good, if somewhat hard-to-comprehend,
example).

What the 1.X label means to Spark is that we are willing to guarantee
stability for Spark's core API. This is something that actually, Spark
has been doing for a while already (we've made few or no breaking
changes to the Spark core API for several years) and we want to codify
this for application developers. In this regard Spark has made a bunch
of changes to enforce the integrity of our API's:

- We went through and clearly annotated internal, or experimental
API's. This was a huge project-wide effort and included Scaladoc and
several other components to make it clear to users.
- We implemented automated byte-code verification of all proposed pull
requests that they don't break public API's. Pull requests after 1.0
will fail if they break API's that are not explicitly declared private
or experimental.

I can't possibly emphasize enough the importance of API stability.
What we want to avoid is the Hadoop approach. Candidly, Hadoop does a
poor job on this. There really isn't a well defined stable API for any
of the Hadoop components, for a few reasons:

1. Hadoop projects don't do any rigorous checking that new patches
don't break API's. Of course, the results in regular API breaks and a
poor understanding of what is a public API.
2. In several cases it's not possible to do basic things in Hadoop
without using deprecated or private API's.
3. There is significant vendor fragmentation of API's.

The main focus of the Hadoop vendors is making consistent cuts of the
core projects work together (HDFS/Pig/Hive/etc) - so API breaks are
sometimes considered "fixed" as long as the other projects work around
them (see [1]). We also regularly need to do archaeology (see [2]) and
directly interact with Hadoop committers to understand what API's are
stable and in which versions.

One goal of Spark is to deal with the pain of inter-operating with
Hadoop so that application writers don't to. We'd like to retain the
property that if you build an application against the (well defined,
stable) Spark API's right now, you'll be able to run it across many
Hadoop vendors and versions for the entire Spark 1.X release cycle.

Writing apps against Hadoop can be very difficult... consider how much
more engineering effort we spent maintaining YARN support than Mesos
support. There are many factors, but one is that Mesos has a single,
narrow, stable API. We've never had to make a change in Mesos due to
an API change, for several years. YARN on the other hand, there are at
least 3 YARN API's that currently exist, which are all binary
incompatible. We'd like to offer apps the ability to build against
Spark's API and just let us deal with it.

As more vendors packaging Spark, I'd like to see us put tools in the
upstream Spark repo that do validation for vendor packages of Spark,
so that we don't end up with fragmentation. Of course, vendors can
enhance the API and are encouraged to, but we need a kernel of API's
that vendors must maintain (think POSIX) to be considered compliant
with Apache Spark. I believe some other projects like OpenStack have
done this to avoid fragmentation.

- Patrick

[1] https://issues.apache.org/jira/browse/MAPREDUCE-5830
[2] http://2.bp.blogspot.com/-GO6HF0OAFHw/UOfNEH-4sEI/AAAAAAAAAD0/dEWFFYTRgYw/s1600/output-file.png

On Sun, May 18, 2014 at 2:13 AM, Mridul Muralidharan <mridul@gmail.com> wrote:
> So I think I need to clarify a few things here - particularly since
> this mail went to the wrong mailing list and a much wider audience
> than I intended it for :-)
>
>
> Most of the issues I mentioned are internal implementation detail of
> spark core : which means, we can enhance them in future without
> disruption to our userbase (ability to support large number of
> input/output partitions. Note: this is of order of 100k input and
> output partitions with uniform spread of keys - very rarely seen
> outside of some crazy jobs).
>
> Some of the issues I mentioned would reqiure DeveloperApi changes -
> which are not user exposed : they would impact developer use of these
> api's - which are mostly internally provided by spark. (Like fixing
> blocks > 2G would require change to Serializer api)
>
> A smaller faction might require interface changes - note, I am
> referring specifically to configuration changes (removing/deprecating
> some) and possibly newer options to submit/env, etc - I dont envision
> any programming api change itself.
> The only api change we did was from Seq -> Iterable - which is
> actually to address some of the issues I mentioned (join/cogroup).
>
> Remaining are bugs which need to be addressed or the feature
> removed/enhanced like shuffle consolidation.
>
> There might be semantic extension of some things like OFF_HEAP storage
> level to address other computation models - but that would not have an
> impact on end user - since other options would be pluggable with
> default set to Tachyon so that there is no user expectation change.
>
>
> So will the interface possibly change ? Sure though we will try to
> keep it backwardly compatible (as we did with 1.0).
> Will the api change - other than backward compatible enhancements, probably not.
>
>
> Regards,
> Mridul
>
>
> On Sun, May 18, 2014 at 12:11 PM, Mridul Muralidharan <mridul@gmail.com> wrote:
>>
>> On 18-May-2014 5:05 am, "Mark Hamstra" <mark@clearstorydata.com> wrote:
>>>
>>> I don't understand.  We never said that interfaces wouldn't change from
>>> 0.9
>>
>> Agreed.
>>
>>> to 1.0.  What we are committing to is stability going forward from the
>>> 1.0.0 baseline.  Nobody is disputing that backward-incompatible behavior
>>> or
>>> interface changes would be an issue post-1.0.0.  The question is whether
>>
>> The point is, how confident are we that these are the right set of interface
>> definitions.
>> We think it is, but we could also have gone through a 0.10 to vet the
>> proposed 1.0 changes to stabilize them.
>>
>> To give examples for which we don't have solutions currently (which we are
>> facing internally here btw, so not academic exercise) :
>>
>> - Current spark shuffle model breaks very badly as number of partitions
>> increases (input and output).
>>
>> - As number of nodes increase, the overhead per node keeps going up. Spark
>> currently is more geared towards large memory machines; when the RAM per
>> node is modest (8 to 16 gig) but large number of them are available, it does
>> not do too well.
>>
>> - Current block abstraction breaks as data per block goes beyond 2 gig.
>>
>> - Cogroup/join when value per key or number of keys (or both) is high breaks
>> currently.
>>
>> - Shuffle consolidation is so badly broken it is not funny.
>>
>> - Currently there is no way of effectively leveraging accelerator
>> cards/coprocessors/gpus from spark - to do so, I suspect we will need to
>> redefine OFF_HEAP.
>>
>> - Effectively leveraging ssd is still an open question IMO when you have mix
>> of both available.
>>
>> We have resolved some of these and looking at the rest. These are not unique
>> to our internal usage profile, I have seen most of these asked elsewhere
>> too.
>>
>> Thankfully some of the 1.0 changes actually are geared towards helping to
>> alleviate some of the above (Iterable change for ex), most of the rest are
>> internal impl detail of spark core which helps a lot - but there are cases
>> where this is not so.
>>
>> Unfortunately I don't know yet if the unresolved/uninvestigated issues will
>> require more changes or not.
>>
>> Given this I am very skeptical of expecting current spark interfaces to be
>> sufficient for next 1 year (forget 3)
>>
>> I understand this is an argument which can be made to never release 1.0 :-)
>> Which is why I was ok with a 1.0 instead of 0.10 release in spite of my
>> preference.
>>
>> This is a good problem to have IMO ... People are using spark extensively
>> and in circumstances that we did not envision : necessitating changes even
>> to spark core.
>>
>> But the claim that 1.0 interfaces are stable is not something I buy - they
>> are not, we will need to break them soon and cost of maintaining backward
>> compatibility will be high.
>>
>> We just need to make an informed decision to live with that cost, not hand
>> wave it away.
>>
>> Regards
>> Mridul
>>
>>> there is anything apparent now that is expected to require such disruptive
>>> changes if we were to commit to the current release candidate as our
>>> guaranteed 1.0.0 baseline.
>>>
>>>
>>> On Sat, May 17, 2014 at 2:05 PM, Mridul Muralidharan
>>> <mridul@gmail.com>wrote:
>>>
>>> > I would make the case for interface stability not just api stability.
>>> > Particularly given that we have significantly changed some of our
>>> > interfaces, I want to ensure developers/users are not seeing red flags.
>>> >
>>> > Bugs and code stability can be addressed in minor releases if found, but
>>> > behavioral change and/or interface changes would be a much more invasive
>>> > issue for our users.
>>> >
>>> > Regards
>>> > Mridul
>>> > On 18-May-2014 2:19 am, "Matei Zaharia" <matei.zaharia@gmail.com> wrote:
>>> >
>>> > > As others have said, the 1.0 milestone is about API stability, not
>>> > > about
>>> > > saying "we've eliminated all bugs". The sooner you declare 1.0, the
>>> > sooner
>>> > > users can confidently build on Spark, knowing that the application
>>> > > they
>>> > > build today will still run on Spark 1.9.9 three years from now. This
>>> > > is
>>> > > something that I've seen done badly (and experienced the effects
>>> > > thereof)
>>> > > in other big data projects, such as MapReduce and even YARN. The
>>> > > result
>>> > is
>>> > > that you annoy users, you end up with a fragmented userbase where
>>> > everyone
>>> > > is building against a different version, and you drastically slow down
>>> > > development.
>>> > >
>>> > > With a project as fast-growing as fast-growing as Spark in particular,
>>> > > there will be new bugs discovered and reported continuously,
>>> > > especially
>>> > in
>>> > > the non-core components. Look at the graph of # of contributors in
>>> > > time
>>> > to
>>> > > Spark: https://www.ohloh.net/p/apache-spark (bottom-most graph;
>>> > "commits"
>>> > > changed when we started merging each patch as a single commit). This
>>> > > is
>>> > not
>>> > > slowing down, and we need to have the culture now that we treat API
>>> > > stability and release numbers at the level expected for a 1.0 project
>>> > > instead of having people come in and randomly change the API.
>>> > >
>>> > > I'll also note that the issues marked "blocker" were marked so by
>>> > > their
>>> > > reporters, since the reporter can set the priority. I don't consider
>>> > stuff
>>> > > like parallelize() not partitioning ranges in the same way as other
>>> > > collections a blocker -- it's a bug, it would be good to fix it, but it
>>> > only
>>> > > affects a small number of use cases. Of course if we find a real
>>> > > blocker
>>> > > (in particular a regression from a previous version, or a feature
>>> > > that's
>>> > > just completely broken), we will delay the release for that, but at
>>> > > some
>>> > > point you have to say "okay, this fix will go into the next
>>> > > maintenance
>>> > > release". Maybe we need to write a clear policy for what the issue
>>> > > priorities mean.
>>> > >
>>> > > Finally, I believe it's much better to have a culture where you can
>>> > > make
>>> > > releases on a regular schedule, and have the option to make a
>>> > > maintenance
>>> > > release in 3-4 days if you find new bugs, than one where you pile up
>>> > stuff
>>> > > into each release. This is what much large project than us, like
>>> > > Linux,
>>> > do,
>>> > > and it's the only way to avoid indefinite stalling with a large
>>> > contributor
>>> > > base. In the worst case, if you find a new bug that warrants immediate
>>> > > release, it goes into 1.0.1 a week after 1.0.0 (we can vote on 1.0.1
>>> > > in
>>> > > three days with just your bug fix in it). And if you find an API that
>>> > you'd
>>> > > like to improve, just add a new one and maybe deprecate the old one --
>>> > > at
>>> > > some point we have to respect our users and let them know that code
>>> > > they
>>> > > write today will still run tomorrow.
>>> > >
>>> > > Matei
>>> > >
>>> > > On May 17, 2014, at 10:32 AM, Kan Zhang <kzhang@apache.org> wrote:
>>> > >
>>> > > > +1 on the running commentary here, non-binding of course :-)
>>> > > >
>>> > > >
>>> > > > On Sat, May 17, 2014 at 8:44 AM, Andrew Ash <andrew@andrewash.com>
>>> > > wrote:
>>> > > >
>>> > > >> +1 on the next release feeling more like a 0.10 than a 1.0
>>> > > >> On May 17, 2014 4:38 AM, "Mridul Muralidharan" <mridul@gmail.com>
>>> > > wrote:
>>> > > >>
>>> > > >>> I had echoed similar sentiments a while back when there was a
>>> > > discussion
>>> > > >>> around 0.10 vs 1.0 ... I would have preferred 0.10 to stabilize
>>> > > >>> the
>>> > api
>>> > > >>> changes, add missing functionality, go through a hardening release
>>> > > before
>>> > > >>> 1.0
>>> > > >>>
>>> > > >>> But the community preferred a 1.0 :-)
>>> > > >>>
>>> > > >>> Regards,
>>> > > >>> Mridul
>>> > > >>>
>>> > > >>> On 17-May-2014 3:19 pm, "Sean Owen" <sowen@cloudera.com> wrote:
>>> > > >>>>
>>> > > >>>> On this note, non-binding commentary:
>>> > > >>>>
>>> > > >>>> Releases happen in local minima of change, usually created by
>>> > > >>>> internally enforced code freeze. Spark is incredibly busy now due
>>> > > >>>> to
>>> > > >>>> external factors -- recently a TLP, recently discovered by a
>>> > > >>>> large
>>> > new
>>> > > >>>> audience, ease of contribution enabled by Github. It's getting
>>> > > >>>> like
>>> > > >>>> the first year of mainstream battle-testing in a month. It's been
>>> > very
>>> > > >>>> hard to freeze anything! I see a number of non-trivial issues
>>> > > >>>> being
>>> > > >>>> reported, and I don't think it has been possible to triage all of
>>> > > >>>> them, even.
>>> > > >>>>
>>> > > >>>> Given the high rate of change, my instinct would have been to
>>> > release
>>> > > >>>> 0.10.0 now. But won't it always be very busy? I do think the rate
>>> > > >>>> of
>>> > > >>>> significant issues will slow down.
>>> > > >>>>
>>> > > >>>> Version ain't nothing but a number, but if it has any meaning
>>> > > >>>> it's
>>> > the
>>> > > >>>> semantic versioning meaning. 1.0 imposes extra handicaps around
>>> > > >>>> striving to maintain backwards-compatibility. That may end up
>>> > > >>>> being
>>> > > >>>> bent to fit in important changes that are going to be required in
>>> > this
>>> > > >>>> continuing period of change. Hadoop does this all the time
>>> > > >>>> unfortunately and gets away with it, I suppose -- minor version
>>> > > >>>> releases are really major. (On the other extreme, HBase is at
>>> > > >>>> 0.98
>>> > and
>>> > > >>>> quite production-ready.)
>>> > > >>>>
>>> > > >>>> Just consider this a second vote for focus on fixes and 1.0.x
>>> > > >>>> rather
>>> > > >>>> than new features and 1.x. I think there are a few steps that
>>> > > >>>> could
>>> > > >>>> streamline triage of this flood of contributions, and make all of
>>> > this
>>> > > >>>> easier, but that's for another thread.
>>> > > >>>>
>>> > > >>>>
>>> > > >>>> On Fri, May 16, 2014 at 8:50 PM, Mark Hamstra <
>>> > > mark@clearstorydata.com
>>> > > >>>
>>> > > >>> wrote:
>>> > > >>>>> +1, but just barely.  We've got quite a number of outstanding
>>> > > >>>>> bugs
>>> > > >>>>> identified, and many of them have fixes in progress.  I'd hate
>>> > > >>>>> to
>>> > see
>>> > > >>> those
>>> > > >>>>> efforts get lost in a post-1.0.0 flood of new features targeted
>>> > > >>>>> at
>>> > > >>> 1.1.0 --
>>> > > >>>>> in other words, I'd like to see 1.0.1 retain a high priority
>>> > relative
>>> > > >>> to
>>> > > >>>>> 1.1.0.
>>> > > >>>>>
>>> > > >>>>> Looking through the unresolved JIRAs, it doesn't look like any
>>> > > >>>>> of
>>> > the
>>> > > >>>>> identified bugs are show-stoppers or strictly regressions
>>> > (although I
>>> > > >>> will
>>> > > >>>>> note that one that I have in progress, SPARK-1749, is a bug that
>>> > > >>>>> we
>>> > > >>>>> introduced with recent work -- it's not strictly a regression
>>> > because
>>> > > >>> we
>>> > > >>>>> had equally bad but different behavior when the DAGScheduler
>>> > > >> exceptions
>>> > > >>>>> weren't previously being handled at all vs. being slightly
>>> > > >> mis-handled
>>> > > >>>>> now), so I'm not currently seeing a reason not to release.
>>> > > >>>
>>> > > >>
>>> > >
>>> > >
>>> >

From dev-return-7881-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 20:25:14 2014
Return-Path: <dev-return-7881-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3C41E1002B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 20:25:14 +0000 (UTC)
Received: (qmail 86255 invoked by uid 500); 29 May 2014 20:25:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86189 invoked by uid 500); 29 May 2014 20:25:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86181 invoked by uid 99); 29 May 2014 20:25:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 20:25:13 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.220.176 as permitted sender)
Received: from [209.85.220.176] (HELO mail-vc0-f176.google.com) (209.85.220.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 20:25:06 +0000
Received: by mail-vc0-f176.google.com with SMTP id la4so1012593vcb.35
        for <dev@spark.apache.org>; Thu, 29 May 2014 13:24:45 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=KkLzy1otnobu3yTXIsapVGT8eTpOOiCwXHEe47BN1mM=;
        b=JVWKzwrXMqC6JFE+SPDpka2Fy7PTdm9chw4VBKEKQrz5fdQKC5AvPVBrVYyguhwytv
         sapKW9MQFhzoKBttVXWJ1wnpQqD7EiDXIgAvH89+PteZGUzaqYEySrAW9Qs2M4QQpLgn
         0r5LkXpsJY2HMvl1mzAlzUmFAo8+6JDnAmMgzITaNqKAPLVMxxejgOs9paZVy0xEwVxF
         NP2uysVjju42zi4/mn/E+cMjL5NAvFKm64slgEgt/B4V3HN8iih4ItCNRSIo+VUk4ZgS
         /YdfBo8hTCfnJeoTQn784n809MSr1omLK7Bbxq7eMwVmGA8c9B0wY4RlA81/ENF/KKpH
         p9ag==
X-Received: by 10.220.250.203 with SMTP id mp11mr9439833vcb.2.1401395085751;
 Thu, 29 May 2014 13:24:45 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.118.9 with HTTP; Thu, 29 May 2014 13:24:15 -0700 (PDT)
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Thu, 29 May 2014 13:24:15 -0700
Message-ID: <CAMwrk0kn5RmjdG3m9qjCWHcM1sZLdfz+t=tZGc9M3C7x5vED6A@mail.gmail.com>
Subject: [RESULT][VOTE] Release Apache Spark 1.0.0 (RC11)
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e013d050246bb1e04fa8fb851
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013d050246bb1e04fa8fb851
Content-Type: text/plain; charset=UTF-8

Hello everyone,

The vote on Spark 1.0.0 RC11 passes with13 "+1" votes, one "0" vote and no
"-1" vote.

Thanks to everyone who tested the RC and voted. Here are the totals:

+1: (13 votes)
Matei Zaharia*
Mark Hamstra*
Holden Karau
Nick Pentreath*
Will Benton
Henry Saputra
Sean McNamara*
Xiangrui Meng*
Andy Konwinski*
Krishna Sankar
Kevin Markey
Patrick Wendell*
Tathagata Das*

0: (1 vote)
Ankur Dave*

-1: (0 vote)

Please hold off announcing Spark 1.0.0 until Apache Software Foundation
makes the press release tomorrow. Thank you very much for your cooperation.

TD

--089e013d050246bb1e04fa8fb851--

From dev-return-7882-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 20:34:16 2014
Return-Path: <dev-return-7882-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 68CE9100BE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 20:34:16 +0000 (UTC)
Received: (qmail 11376 invoked by uid 500); 29 May 2014 20:34:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11313 invoked by uid 500); 29 May 2014 20:34:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11305 invoked by uid 99); 29 May 2014 20:34:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 20:34:15 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.54 as permitted sender)
Received: from [209.85.219.54] (HELO mail-oa0-f54.google.com) (209.85.219.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 20:34:10 +0000
Received: by mail-oa0-f54.google.com with SMTP id j17so933546oag.27
        for <dev@spark.apache.org>; Thu, 29 May 2014 13:33:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=1oYD/9awT9/5a/QQm8NepOmGPEXRCpvmD7KTjKCF8FI=;
        b=Jp95T1CG2OrulVMFBLN0z2Fa1RPn1842uM24OepVYZ0rw2NAQX57omqdy56nRIhTKh
         JtEMAPysOeWsAyMUL4FXPkzJaPYKxMUZst1dheyGrvtKyYKxrVA0rQOzfLHYIYz4hT9f
         crG9qRBNpteD1UYWaIKurkr/SfhLRwu/yEcrWtbjCjAU40BmFyHu9ENipKhQ6PAp67fA
         ZgPyklqgP81UEmqnM20OEBZOpnFJWVguTqnwRsWMeuH+IhvypoAvLj0gBdW1sla7fOPe
         oBBvD3f5H+bMiuFU15Um9wlmaigk08LIL1M5y/ZYm+5nogu3aPc4l2TtjCl1n9VJ67EK
         yRZQ==
MIME-Version: 1.0
X-Received: by 10.60.146.167 with SMTP id td7mr11719811oeb.6.1401395629893;
 Thu, 29 May 2014 13:33:49 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Thu, 29 May 2014 13:33:49 -0700 (PDT)
In-Reply-To: <CAMwrk0kn5RmjdG3m9qjCWHcM1sZLdfz+t=tZGc9M3C7x5vED6A@mail.gmail.com>
References: <CAMwrk0kn5RmjdG3m9qjCWHcM1sZLdfz+t=tZGc9M3C7x5vED6A@mail.gmail.com>
Date: Thu, 29 May 2014 13:33:49 -0700
Message-ID: <CABPQxsvGLBMz9k2srAL8_odmULOxLMtuoaMKWEnuEvOv6T8g2A@mail.gmail.com>
Subject: Re: [RESULT][VOTE] Release Apache Spark 1.0.0 (RC11)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Congrats everyone! This is a huge accomplishment!

On Thu, May 29, 2014 at 1:24 PM, Tathagata Das
<tathagata.das1565@gmail.com> wrote:
> Hello everyone,
>
> The vote on Spark 1.0.0 RC11 passes with13 "+1" votes, one "0" vote and no
> "-1" vote.
>
> Thanks to everyone who tested the RC and voted. Here are the totals:
>
> +1: (13 votes)
> Matei Zaharia*
> Mark Hamstra*
> Holden Karau
> Nick Pentreath*
> Will Benton
> Henry Saputra
> Sean McNamara*
> Xiangrui Meng*
> Andy Konwinski*
> Krishna Sankar
> Kevin Markey
> Patrick Wendell*
> Tathagata Das*
>
> 0: (1 vote)
> Ankur Dave*
>
> -1: (0 vote)
>
> Please hold off announcing Spark 1.0.0 until Apache Software Foundation
> makes the press release tomorrow. Thank you very much for your cooperation.
>
> TD

From dev-return-7883-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 21:08:52 2014
Return-Path: <dev-return-7883-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 99323102E9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 21:08:52 +0000 (UTC)
Received: (qmail 1250 invoked by uid 500); 29 May 2014 21:08:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1188 invoked by uid 500); 29 May 2014 21:08:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1180 invoked by uid 99); 29 May 2014 21:08:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 21:08:52 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.44 as permitted sender)
Received: from [209.85.160.44] (HELO mail-pb0-f44.google.com) (209.85.160.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 21:08:45 +0000
Received: by mail-pb0-f44.google.com with SMTP id rq2so931087pbb.3
        for <dev@spark.apache.org>; Thu, 29 May 2014 14:08:25 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=NCD1w2LgcwKH215lq8xSCob5RUF+z7uHrk41Tc5HGUE=;
        b=hcs2flHg+YI/bIg7lGhSFpz+AmGWpVxtLO9txSjKxuBQhuYb3ZQT4ANF41EAosHEUM
         qz/ej3Q7zgIif3H21uaYrHmLTpOATMNNVvL01A933FEpskj+PQyaYDxCuo8nYpo1SCii
         II08gs1JrPmyS2QrqxzROXq9GmoB2KcUsNRLHWhuy1rd2TYno4GifjafUVT/0t6n2L+o
         k2zeHQUfzqSe9VGGHp+eeULgB7cn//04X7np6+KtsR/VxgkU9OnSskDN2v23Q+oK/Qng
         nB5KPls7LKwCPVNjY8qmdbwou/BF3PnGX+EmBdIwbrb49Rpi1Q7WazSbMF+UTrm5WpJ9
         Mnug==
X-Received: by 10.69.31.65 with SMTP id kk1mr10103223pbd.126.1401397705330;
        Thu, 29 May 2014 14:08:25 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id ie9sm8089440pad.29.2014.05.29.14.08.21
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 29 May 2014 14:08:22 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
Subject: Re: [RESULT][VOTE] Release Apache Spark 1.0.0 (RC11)
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CABPQxsvGLBMz9k2srAL8_odmULOxLMtuoaMKWEnuEvOv6T8g2A@mail.gmail.com>
Date: Thu, 29 May 2014 14:08:20 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <FCE848F1-364B-4B4E-A0ED-8085FB310252@gmail.com>
References: <CAMwrk0kn5RmjdG3m9qjCWHcM1sZLdfz+t=tZGc9M3C7x5vED6A@mail.gmail.com> <CABPQxsvGLBMz9k2srAL8_odmULOxLMtuoaMKWEnuEvOv6T8g2A@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.2)
X-Virus-Checked: Checked by ClamAV on apache.org

Yup, congrats all. The most impressive thing is the number of =
contributors to this release =97 with over 100 contributors, it=92s =
becoming hard to even write the credits. Look forward to the Apache =
press release tomorrow.

Matei

On May 29, 2014, at 1:33 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Congrats everyone! This is a huge accomplishment!
>=20
> On Thu, May 29, 2014 at 1:24 PM, Tathagata Das
> <tathagata.das1565@gmail.com> wrote:
>> Hello everyone,
>>=20
>> The vote on Spark 1.0.0 RC11 passes with13 "+1" votes, one "0" vote =
and no
>> "-1" vote.
>>=20
>> Thanks to everyone who tested the RC and voted. Here are the totals:
>>=20
>> +1: (13 votes)
>> Matei Zaharia*
>> Mark Hamstra*
>> Holden Karau
>> Nick Pentreath*
>> Will Benton
>> Henry Saputra
>> Sean McNamara*
>> Xiangrui Meng*
>> Andy Konwinski*
>> Krishna Sankar
>> Kevin Markey
>> Patrick Wendell*
>> Tathagata Das*
>>=20
>> 0: (1 vote)
>> Ankur Dave*
>>=20
>> -1: (0 vote)
>>=20
>> Please hold off announcing Spark 1.0.0 until Apache Software =
Foundation
>> makes the press release tomorrow. Thank you very much for your =
cooperation.
>>=20
>> TD


From dev-return-7884-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu May 29 21:58:02 2014
Return-Path: <dev-return-7884-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A69A01056E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 29 May 2014 21:58:02 +0000 (UTC)
Received: (qmail 1240 invoked by uid 500); 29 May 2014 21:58:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1186 invoked by uid 500); 29 May 2014 21:58:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1178 invoked by uid 99); 29 May 2014 21:58:02 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 21:58:02 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of henry.saputra@gmail.com designates 74.125.82.169 as permitted sender)
Received: from [74.125.82.169] (HELO mail-we0-f169.google.com) (74.125.82.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 29 May 2014 21:57:56 +0000
Received: by mail-we0-f169.google.com with SMTP id u56so1115075wes.14
        for <dev@spark.apache.org>; Thu, 29 May 2014 14:57:35 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type:content-transfer-encoding;
        bh=yKE5Lgn1Bk8rRS/JGl13LOMNIxI61lc0EfvWPtJTQ3k=;
        b=w6jX31xCTiAv2f3NQVwuUtb9ayW+2kZpJyJCkzthZhFPI56p8WLPVfM4s4FIPZk11l
         UBX+AS0ER2zOZc+ynZ2tCeXNnhcOPzlImXwgNCERdbzbP8yyS+Yw13/KIaKMPtZQNLZs
         1vYveqiKXvGJQdNx3oRBzKwO968zvrlB4ZbcCwT3pvAVWaV0yK4DxkqQUkeMBqlt5xyR
         Fk8tBlUGZ+xWHZBebsj7ESNX3SNPFcicUdse31W2zAXxpVfxiXXCN+SKltL33DzS3HGp
         CoAj9/N6gmGyn+n2VjTTkdbZfh6xCHG3vTET+Y5inctsMN/1xD41fIlVzYwuHiCsN0Y+
         4g9A==
MIME-Version: 1.0
X-Received: by 10.194.133.1 with SMTP id oy1mr14794984wjb.87.1401400655333;
 Thu, 29 May 2014 14:57:35 -0700 (PDT)
Received: by 10.216.165.71 with HTTP; Thu, 29 May 2014 14:57:35 -0700 (PDT)
In-Reply-To: <FCE848F1-364B-4B4E-A0ED-8085FB310252@gmail.com>
References: <CAMwrk0kn5RmjdG3m9qjCWHcM1sZLdfz+t=tZGc9M3C7x5vED6A@mail.gmail.com>
	<CABPQxsvGLBMz9k2srAL8_odmULOxLMtuoaMKWEnuEvOv6T8g2A@mail.gmail.com>
	<FCE848F1-364B-4B4E-A0ED-8085FB310252@gmail.com>
Date: Thu, 29 May 2014 14:57:35 -0700
Message-ID: <CALuGr6bAj-EXOp_Az6xgS674mYSMzA-cMPk81ND+hdWr0sg_iA@mail.gmail.com>
Subject: Re: [RESULT][VOTE] Release Apache Spark 1.0.0 (RC11)
From: Henry Saputra <henry.saputra@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Congrats guys! Another milestone for Apache Spark indeed =3D)

- Henry

On Thu, May 29, 2014 at 2:08 PM, Matei Zaharia <matei.zaharia@gmail.com> wr=
ote:
> Yup, congrats all. The most impressive thing is the number of contributor=
s to this release =E2=80=94 with over 100 contributors, it=E2=80=99s becomi=
ng hard to even write the credits. Look forward to the Apache press release=
 tomorrow.
>
> Matei
>
> On May 29, 2014, at 1:33 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>
>> Congrats everyone! This is a huge accomplishment!
>>
>> On Thu, May 29, 2014 at 1:24 PM, Tathagata Das
>> <tathagata.das1565@gmail.com> wrote:
>>> Hello everyone,
>>>
>>> The vote on Spark 1.0.0 RC11 passes with13 "+1" votes, one "0" vote and=
 no
>>> "-1" vote.
>>>
>>> Thanks to everyone who tested the RC and voted. Here are the totals:
>>>
>>> +1: (13 votes)
>>> Matei Zaharia*
>>> Mark Hamstra*
>>> Holden Karau
>>> Nick Pentreath*
>>> Will Benton
>>> Henry Saputra
>>> Sean McNamara*
>>> Xiangrui Meng*
>>> Andy Konwinski*
>>> Krishna Sankar
>>> Kevin Markey
>>> Patrick Wendell*
>>> Tathagata Das*
>>>
>>> 0: (1 vote)
>>> Ankur Dave*
>>>
>>> -1: (0 vote)
>>>
>>> Please hold off announcing Spark 1.0.0 until Apache Software Foundation
>>> makes the press release tomorrow. Thank you very much for your cooperat=
ion.
>>>
>>> TD
>

From dev-return-7885-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 00:39:59 2014
Return-Path: <dev-return-7885-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C288A10CB4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 00:39:59 +0000 (UTC)
Received: (qmail 6308 invoked by uid 500); 30 May 2014 00:39:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6241 invoked by uid 500); 30 May 2014 00:39:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6233 invoked by uid 99); 30 May 2014 00:39:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 00:39:54 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of usman@platfora.com designates 74.125.82.45 as permitted sender)
Received: from [74.125.82.45] (HELO mail-wg0-f45.google.com) (74.125.82.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 00:39:49 +0000
Received: by mail-wg0-f45.google.com with SMTP id m15so1185898wgh.4
        for <dev@spark.apache.org>; Thu, 29 May 2014 17:39:27 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=94WuuaHj+VFVfyaA+ESc4EKaPKCB7zG5iUEeREeln1M=;
        b=bx0fkeupy0HuWNX9sTfr1nRNdnBBJivpRaV7qSeMMSDiab1rNE0TdeF7DFgU+YnNmM
         iVSe2bdVnQX5YrzmnWoI6MwZoioY/9L/2XpKbabA91ZzoXXLe6Yp+MnFVE4VMQdvMC95
         SUOu5aoPcTOLmbhaURo7Y0wsAvtrER0MQt8MeB8XPWVO0sin3OF/bQLrbJD6RxRnOuup
         gjjX4MtSKHXosNz2ABb3TtT/tSYvAopDGkTUgO9h2swnmnqypVxrS83nO7IKia+Bx1bq
         Mi9KQvY46xqwWgA0INvS0WdDLwh+JIKQ9EmRY0AAh9QYyNxN0iBpLC2xBMxxWM3ugCnr
         6twQ==
X-Gm-Message-State: ALoCoQkstRpY51Eajv9zQU9AzlJI90yxNRrf2GWdG8ckezROR/iEl70yt0cTDCk12kt32RYUbl8G
MIME-Version: 1.0
X-Received: by 10.180.93.163 with SMTP id cv3mr1176701wib.3.1401410367725;
 Thu, 29 May 2014 17:39:27 -0700 (PDT)
Received: by 10.216.241.130 with HTTP; Thu, 29 May 2014 17:39:27 -0700 (PDT)
In-Reply-To: <CALuGr6bAj-EXOp_Az6xgS674mYSMzA-cMPk81ND+hdWr0sg_iA@mail.gmail.com>
References: <CAMwrk0kn5RmjdG3m9qjCWHcM1sZLdfz+t=tZGc9M3C7x5vED6A@mail.gmail.com>
	<CABPQxsvGLBMz9k2srAL8_odmULOxLMtuoaMKWEnuEvOv6T8g2A@mail.gmail.com>
	<FCE848F1-364B-4B4E-A0ED-8085FB310252@gmail.com>
	<CALuGr6bAj-EXOp_Az6xgS674mYSMzA-cMPk81ND+hdWr0sg_iA@mail.gmail.com>
Date: Thu, 29 May 2014 17:39:27 -0700
Message-ID: <CAG_e43yt9k2AviuK_W06gMYokKJPBEc+Z_1piCQYg-h5eUxtng@mail.gmail.com>
Subject: Re: [RESULT][VOTE] Release Apache Spark 1.0.0 (RC11)
From: Usman Ghani <usman@platfora.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d04389539273a6204fa9347b3
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04389539273a6204fa9347b3
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Congrats everyone. Really pumped about this.


On Thu, May 29, 2014 at 2:57 PM, Henry Saputra <henry.saputra@gmail.com>
wrote:

> Congrats guys! Another milestone for Apache Spark indeed =3D)
>
> - Henry
>
> On Thu, May 29, 2014 at 2:08 PM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
> > Yup, congrats all. The most impressive thing is the number of
> contributors to this release =E2=80=94 with over 100 contributors, it=E2=
=80=99s becoming
> hard to even write the credits. Look forward to the Apache press release
> tomorrow.
> >
> > Matei
> >
> > On May 29, 2014, at 1:33 PM, Patrick Wendell <pwendell@gmail.com> wrote=
:
> >
> >> Congrats everyone! This is a huge accomplishment!
> >>
> >> On Thu, May 29, 2014 at 1:24 PM, Tathagata Das
> >> <tathagata.das1565@gmail.com> wrote:
> >>> Hello everyone,
> >>>
> >>> The vote on Spark 1.0.0 RC11 passes with13 "+1" votes, one "0" vote
> and no
> >>> "-1" vote.
> >>>
> >>> Thanks to everyone who tested the RC and voted. Here are the totals:
> >>>
> >>> +1: (13 votes)
> >>> Matei Zaharia*
> >>> Mark Hamstra*
> >>> Holden Karau
> >>> Nick Pentreath*
> >>> Will Benton
> >>> Henry Saputra
> >>> Sean McNamara*
> >>> Xiangrui Meng*
> >>> Andy Konwinski*
> >>> Krishna Sankar
> >>> Kevin Markey
> >>> Patrick Wendell*
> >>> Tathagata Das*
> >>>
> >>> 0: (1 vote)
> >>> Ankur Dave*
> >>>
> >>> -1: (0 vote)
> >>>
> >>> Please hold off announcing Spark 1.0.0 until Apache Software Foundati=
on
> >>> makes the press release tomorrow. Thank you very much for your
> cooperation.
> >>>
> >>> TD
> >
>

--f46d04389539273a6204fa9347b3--

From dev-return-7886-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 01:09:19 2014
Return-Path: <dev-return-7886-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 17A6A10D8A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 01:09:19 +0000 (UTC)
Received: (qmail 47347 invoked by uid 500); 30 May 2014 01:09:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47290 invoked by uid 500); 30 May 2014 01:09:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47282 invoked by uid 99); 30 May 2014 01:09:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 01:09:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of andykonwinski@gmail.com designates 209.85.215.47 as permitted sender)
Received: from [209.85.215.47] (HELO mail-la0-f47.google.com) (209.85.215.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 01:09:14 +0000
Received: by mail-la0-f47.google.com with SMTP id pn19so626553lab.20
        for <dev@spark.apache.org>; Thu, 29 May 2014 18:08:52 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=vIiF0Jv/xVjwpX3NxPb2tHZzsNYmxwpF4O5yjpuYiK8=;
        b=hjqbV09m1FBj9EKSgqIAehyEsImGthA9BPUt9SrVqTcT8FWycMz/n1Bp02AzTxscLp
         h5vAxq2CFCY8ZaBLEmM5QfZ0GkarTxzuwAMsXC85EZoXg33E41BtMqQ2afAQEW2czuMr
         P4BI422VD+gbH4c7J1GWXMgptxiOtfx9lFRZ9otjoNafF4gtrixA8F/cnmSOctqM9ZSo
         JTTo5UJzMdp+M46bQN1rBkjyBTr9AW9aGcN/xWOrUmSiL+jCPiKxsnOrv9C765elS/4L
         iTkfd8Scf1+/82yBYxODSklEDfxzWiUM++DboDRGDoJQoQgucdgxIpBXYrr3biKP9gEo
         0byw==
MIME-Version: 1.0
X-Received: by 10.152.203.168 with SMTP id kr8mr9193510lac.17.1401412132867;
 Thu, 29 May 2014 18:08:52 -0700 (PDT)
Received: by 10.112.147.4 with HTTP; Thu, 29 May 2014 18:08:52 -0700 (PDT)
Received: by 10.112.147.4 with HTTP; Thu, 29 May 2014 18:08:52 -0700 (PDT)
In-Reply-To: <CAG_e43yt9k2AviuK_W06gMYokKJPBEc+Z_1piCQYg-h5eUxtng@mail.gmail.com>
References: <CAMwrk0kn5RmjdG3m9qjCWHcM1sZLdfz+t=tZGc9M3C7x5vED6A@mail.gmail.com>
	<CABPQxsvGLBMz9k2srAL8_odmULOxLMtuoaMKWEnuEvOv6T8g2A@mail.gmail.com>
	<FCE848F1-364B-4B4E-A0ED-8085FB310252@gmail.com>
	<CALuGr6bAj-EXOp_Az6xgS674mYSMzA-cMPk81ND+hdWr0sg_iA@mail.gmail.com>
	<CAG_e43yt9k2AviuK_W06gMYokKJPBEc+Z_1piCQYg-h5eUxtng@mail.gmail.com>
Date: Thu, 29 May 2014 18:08:52 -0700
Message-ID: <CALEZFQy34=_krmapFJD7S2jQKt6-9S94B03jx4kuyrOGiTLmqA@mail.gmail.com>
Subject: Re: [RESULT][VOTE] Release Apache Spark 1.0.0 (RC11)
From: Andy Konwinski <andykonwinski@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113457dc5d176a04fa93b0f8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113457dc5d176a04fa93b0f8
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Yes great work all. Special thanks to Patrick (and TD) for excellent
leadership!
On May 29, 2014 5:39 PM, "Usman Ghani" <usman@platfora.com> wrote:

> Congrats everyone. Really pumped about this.
>
>
> On Thu, May 29, 2014 at 2:57 PM, Henry Saputra <henry.saputra@gmail.com>
> wrote:
>
> > Congrats guys! Another milestone for Apache Spark indeed =3D)
> >
> > - Henry
> >
> > On Thu, May 29, 2014 at 2:08 PM, Matei Zaharia <matei.zaharia@gmail.com=
>
> > wrote:
> > > Yup, congrats all. The most impressive thing is the number of
> > contributors to this release =E2=80=94 with over 100 contributors, it=
=E2=80=99s becoming
> > hard to even write the credits. Look forward to the Apache press releas=
e
> > tomorrow.
> > >
> > > Matei
> > >
> > > On May 29, 2014, at 1:33 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > >
> > >> Congrats everyone! This is a huge accomplishment!
> > >>
> > >> On Thu, May 29, 2014 at 1:24 PM, Tathagata Das
> > >> <tathagata.das1565@gmail.com> wrote:
> > >>> Hello everyone,
> > >>>
> > >>> The vote on Spark 1.0.0 RC11 passes with13 "+1" votes, one "0" vote
> > and no
> > >>> "-1" vote.
> > >>>
> > >>> Thanks to everyone who tested the RC and voted. Here are the totals=
:
> > >>>
> > >>> +1: (13 votes)
> > >>> Matei Zaharia*
> > >>> Mark Hamstra*
> > >>> Holden Karau
> > >>> Nick Pentreath*
> > >>> Will Benton
> > >>> Henry Saputra
> > >>> Sean McNamara*
> > >>> Xiangrui Meng*
> > >>> Andy Konwinski*
> > >>> Krishna Sankar
> > >>> Kevin Markey
> > >>> Patrick Wendell*
> > >>> Tathagata Das*
> > >>>
> > >>> 0: (1 vote)
> > >>> Ankur Dave*
> > >>>
> > >>> -1: (0 vote)
> > >>>
> > >>> Please hold off announcing Spark 1.0.0 until Apache Software
> Foundation
> > >>> makes the press release tomorrow. Thank you very much for your
> > cooperation.
> > >>>
> > >>> TD
> > >
> >
>

--001a113457dc5d176a04fa93b0f8--

From dev-return-7887-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 02:06:42 2014
Return-Path: <dev-return-7887-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DB29F10ED1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 02:06:41 +0000 (UTC)
Received: (qmail 9973 invoked by uid 500); 30 May 2014 02:06:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9916 invoked by uid 500); 30 May 2014 02:06:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9908 invoked by uid 99); 30 May 2014 02:06:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 02:06:36 +0000
X-ASF-Spam-Status: No, hits=0.0 required=10.0
	tests=
X-Spam-Check-By: apache.org
Received-SPF: unknown (athena.apache.org: error in processing during lookup of taeyun.kim@innowireless.co.kr)
Received: from [59.12.193.79] (HELO mail.innowireless.co.kr) (59.12.193.79)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 02:06:30 +0000
Received: from INNOC358 (218.154.28.162) by mail.innowireless.co.kr
 (59.12.193.79) with Microsoft SMTP Server id 8.2.255.0; Fri, 30 May 2014
 10:58:54 +0900
From: innowireless TaeYun Kim <taeyun.kim@innowireless.co.kr>
To: <dev@spark.apache.org>
Subject: Suggestion or question: Adding rdd.cancelCache() method
Date: Fri, 30 May 2014 11:06:15 +0900
Message-ID: <000c01cf7bab$bcb7f430$3627dc90$@innowireless.co.kr>
MIME-Version: 1.0
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: 7bit
X-Mailer: Microsoft Outlook 14.0
Thread-Index: Ac97qwZvkHwlX6eCR0Ky2mecLqQgig==
Content-Language: ko
X-Virus-Checked: Checked by ClamAV on apache.org

What I understand is that rdd.cache() is really
rdd.cache_this_rdd_when_it_actually_materializes().
So, somewhat esoteric problem may occur.

The example is as follows: 

void method1()
{
    JavaRDD<...> rdd =
        sc.textFile(...)
        .map(...);

    rdd.cache();
        // since the following methods can call the action methods multiple
times,
        // cache the rdd to prevent rebuilding.

    method2(rdd);  // may or may not call the action methods on rdd
    method3(rdd);  // may or may not call the action methods on rdd

    // #HERE#, the action methods could have been called or not.

    rdd.saveAsTextFile(...);
        // if none of the above methods called the action methods,
        // rdd will materialize here and cached.
    // but we don't need the cache anymore. Caching was unnecessary.
    rdd.unpersist();
}

If there were rdd.cancelCache() method and we could call it at #HERE#,
unnecessary caching could be avoided.
What cancelCache() would do is to cancel the pending request for caching, if
caching is not done yet.
It is different from unpersist(), since unpersist() undoes the caching that
has been actually done.

Will rdd.cancelCache() be really needed, or I'm misunderstanding the caching
mechanism?




From dev-return-7888-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 03:25:54 2014
Return-Path: <dev-return-7888-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E5E0B1008C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 03:25:53 +0000 (UTC)
Received: (qmail 88537 invoked by uid 500); 30 May 2014 03:25:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88465 invoked by uid 500); 30 May 2014 03:25:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88457 invoked by uid 99); 30 May 2014 03:25:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 03:25:53 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of liqingyang1985@gmail.com designates 74.125.82.43 as permitted sender)
Received: from [74.125.82.43] (HELO mail-wg0-f43.google.com) (74.125.82.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 03:25:47 +0000
Received: by mail-wg0-f43.google.com with SMTP id l18so1297117wgh.26
        for <dev@spark.apache.org>; Thu, 29 May 2014 20:25:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=WRy25AHSfVNaxc/3wEbFvkRwRusadLUK4/AlTpGT46M=;
        b=oi2ed98w3e6vXXl4EJQWa0d61+9XIw+nyHj2CKJLe6la0View7zTdSqrVwYm7Bk+TS
         9vWNHI7JSEmOAeQGm1Y9HS7EfpV1EjYLHG90KJ8sYtemp6hHZYAkHrUNVTjlkSdPmEVR
         ECv1ZYUhr2Z0Ln7+z0zjAjAp/+wahjIZwQIcmQPQyJTGgb7soS7CN0Yg5tqVtm7Why6w
         mh1i7PepLX3xqvBd+25mgQop5nhedCobTQfw/lJA7Z/frCVXg18HESqZYdzPgFnZ+KD2
         xDqr7t17wSnOBgVyqLIKZCwEoLC+VbYg5cTpW3VYw6QVwjzJoq5ASBhvE2ylus5S/Rph
         nsBA==
MIME-Version: 1.0
X-Received: by 10.194.81.98 with SMTP id z2mr17436528wjx.12.1401420326602;
 Thu, 29 May 2014 20:25:26 -0700 (PDT)
Received: by 10.194.22.2 with HTTP; Thu, 29 May 2014 20:25:26 -0700 (PDT)
Date: Fri, 30 May 2014 11:25:26 +0800
Message-ID: <CABDsqqadogLB=PDjvBgjCLnFXsfNsvKhuUGg75wFb3SJ0JZi0g@mail.gmail.com>
Subject: how spark partition data when creating table like " create table xxx
 as select * from xxx"
From: qingyang li <liqingyang1985@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bd6ba70bf8e1f04fa959899
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd6ba70bf8e1f04fa959899
Content-Type: text/plain; charset=UTF-8

hi,  spark-developers, i am using shark/spark,  and i am puzzled by such
question, and  can not find any info from the web, so i ask you.
1.  how spark partition data in memory when creating table when using
"create table a tblproperties("shark.cache"="memory") as select * from
table b " ,  in another words, how many rdds will be created ? how spark
decide the number of rdds ?

2.  how spark partition data on tachyon when creating table when using
"create table a tblproperties("shark.cache"="tachyon") as select * from
table b ".  in another words, how many files will be created ? how spark
decide the number of files?
i found this settings about tachyon "tachyon.user.default.block.size.byte"
,  what it means?  could i set it to control each file size ?

thanks for any guiding  .

--047d7bd6ba70bf8e1f04fa959899--

From dev-return-7889-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 07:47:38 2014
Return-Path: <dev-return-7889-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7934F105F2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 07:47:38 +0000 (UTC)
Received: (qmail 65776 invoked by uid 500); 30 May 2014 07:47:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65712 invoked by uid 500); 30 May 2014 07:47:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65704 invoked by uid 99); 30 May 2014 07:47:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 07:47:38 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.44] (HELO mail-qg0-f44.google.com) (209.85.192.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 07:47:35 +0000
Received: by mail-qg0-f44.google.com with SMTP id i50so4218840qgf.17
        for <dev@spark.apache.org>; Fri, 30 May 2014 00:47:11 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=GK/S3Efwg8WyAPiFfq/hUdIDEqiFHaYDeg24oYjqjRw=;
        b=dKoy7f2nVWL1ftSbw05nC7lHNDdXVLvpjaWq/LdkfQBQtJekwRXQzIXHG41Dr88Xuc
         YQCns4GQ0kHHwddSAyeiY2+WMmZ3TTOFF2p4Cu/R9Z7imEBcBOj92WfnXpUjLSw8DyW+
         uw1w4fxN1r0gk8QPaN+4bt4F7ZYxt1jsFX1aOGl0I+M+LphiO3YaErMiEaeCM7PFmMt3
         OW4qXXpsTRQUp6kVUlwIM1w3Q21yPFsx3GRSwy636cpIWv+e/ok9Db8Qf/tjfzhozAtW
         bDoK/VUffMbbbpsh7sI9kE/zKv7LMkxAQYT9JNjZqTZFCn1/MaELtsjkjbersIPm822k
         KD3w==
X-Gm-Message-State: ALoCoQniFrblhO7F2H3dY5fn/XcDiVK3wVFMbE8CDCu3JL8vY2TOOxYag+NiQpYuZIP6qSJDO3kk
X-Received: by 10.229.27.198 with SMTP id j6mr18413160qcc.12.1401436030902;
 Fri, 30 May 2014 00:47:10 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.49.231 with HTTP; Fri, 30 May 2014 00:46:50 -0700 (PDT)
In-Reply-To: <49229E870391FC49BBBED818C268753D704BAE59@SZXEMA501-MBX.china.huawei.com>
References: <49229E870391FC49BBBED818C268753D704BAE59@SZXEMA501-MBX.china.huawei.com>
From: Reynold Xin <rxin@databricks.com>
Date: Fri, 30 May 2014 00:46:50 -0700
Message-ID: <CAPh_B=ZGMvX35XrYh+CRCesp4Ur3eFv3CamWtw4umRT=RLiT+g@mail.gmail.com>
Subject: Re: Please change instruction about "Launching Applications Inside
 the Cluster"
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1133ba66cc5a2f04fa994024
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133ba66cc5a2f04fa994024
Content-Type: text/plain; charset=UTF-8

Can you take a look at the latest Spark 1.0 docs and see if they are fixed?

https://github.com/apache/spark/tree/master/docs

Thanks.


On Thu, May 29, 2014 at 5:29 AM, Lizhengbing (bing, BIPA) <
zhengbing.li@huawei.com> wrote:

> The instruction address is in
> http://spark.apache.org/docs/0.9.0/spark-standalone.html#launching-applications-inside-the-cluster
> or
> http://spark.apache.org/docs/0.9.1/spark-standalone.html#launching-applications-inside-the-cluster
>
> Origin instruction is:
> "./bin/spark-class org.apache.spark.deploy.Client launch
>    [client-options] \
>    <cluster-url> <application-jar-url> <main-class> \
>    [application-options] "
>
> If I follow this instruction, I will not run my program deployed in a
> spark standalone cluster properly.
>
> Based on source code, This instruction should be changed to
> "./bin/spark-class org.apache.spark.deploy.Client [client-options] launch \
>    <cluster-url> <application-jar-url> <main-class> \
>    [application-options] "
>
> That is to say: [client-options] must be put ahead of launch
>

--001a1133ba66cc5a2f04fa994024--

From dev-return-7890-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 07:49:21 2014
Return-Path: <dev-return-7890-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 75DBB10606
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 07:49:21 +0000 (UTC)
Received: (qmail 69688 invoked by uid 500); 30 May 2014 07:49:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69631 invoked by uid 500); 30 May 2014 07:49:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69623 invoked by uid 99); 30 May 2014 07:49:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 07:49:21 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.192.42 as permitted sender)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 07:49:18 +0000
Received: by mail-qg0-f42.google.com with SMTP id q107so4309319qgd.15
        for <dev@spark.apache.org>; Fri, 30 May 2014 00:48:54 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=dlM+b8DnLbWBEBIew9xvHSkU7fBXdLwhzOltKPHyrZo=;
        b=apW69kOySmkK+MDRO5HqjKHiOPHjpTL9rAWkDH+bF/aXKIwyHNMnjvSVCR1/w4dqR1
         Wj261/esV+7x78A+DeLwEETHg5DRntvBzeAxjLd0oVwfG+PWkfJJW7VaaGwr0AOoDJrI
         KW3QEmdcctifGLFhRJVHR52H+fRB4LbAlXF5n0bpfk3eZ7m7MnDFGwf7HFEVVUrPKaIE
         09KGnWegUS08ZPkmMD43uovrTjrEnhBc2k24XyKoXLVEsXEDL5C/mnDc6dizH2ovmXqh
         oTs33yToLsE3qt8h/8t3fluUPCVFVkiosBd3XaggGgnAD8jAswuJhBkzasgNdSM304uY
         uwPA==
X-Gm-Message-State: ALoCoQlzks3xz9yyQXT+/nxSRKlPQuiaEJiP2jd7IrTRQvpAAyx+2b/e4RslNlHSiLEy5EwmetN8
MIME-Version: 1.0
X-Received: by 10.140.40.81 with SMTP id w75mr17401379qgw.112.1401436134711;
 Fri, 30 May 2014 00:48:54 -0700 (PDT)
Received: by 10.140.106.202 with HTTP; Fri, 30 May 2014 00:48:54 -0700 (PDT)
In-Reply-To: <CAPh_B=ZGMvX35XrYh+CRCesp4Ur3eFv3CamWtw4umRT=RLiT+g@mail.gmail.com>
References: <49229E870391FC49BBBED818C268753D704BAE59@SZXEMA501-MBX.china.huawei.com>
	<CAPh_B=ZGMvX35XrYh+CRCesp4Ur3eFv3CamWtw4umRT=RLiT+g@mail.gmail.com>
Date: Fri, 30 May 2014 00:48:54 -0700
Message-ID: <CACBYxK+CNDVV0HFB2k5fgX25QYhd6KM1GqyqZvgNaYa7Uv0CCw@mail.gmail.com>
Subject: Re: Please change instruction about "Launching Applications Inside
 the Cluster"
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c15154fc3dca04fa994612
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c15154fc3dca04fa994612
Content-Type: text/plain; charset=UTF-8

They should be - in the sense that the docs now recommend using
spark-submit and thus include entirely different invocations.


On Fri, May 30, 2014 at 12:46 AM, Reynold Xin <rxin@databricks.com> wrote:

> Can you take a look at the latest Spark 1.0 docs and see if they are fixed?
>
> https://github.com/apache/spark/tree/master/docs
>
> Thanks.
>
>
> On Thu, May 29, 2014 at 5:29 AM, Lizhengbing (bing, BIPA) <
> zhengbing.li@huawei.com> wrote:
>
> > The instruction address is in
> >
> http://spark.apache.org/docs/0.9.0/spark-standalone.html#launching-applications-inside-the-cluster
> > or
> >
> http://spark.apache.org/docs/0.9.1/spark-standalone.html#launching-applications-inside-the-cluster
> >
> > Origin instruction is:
> > "./bin/spark-class org.apache.spark.deploy.Client launch
> >    [client-options] \
> >    <cluster-url> <application-jar-url> <main-class> \
> >    [application-options] "
> >
> > If I follow this instruction, I will not run my program deployed in a
> > spark standalone cluster properly.
> >
> > Based on source code, This instruction should be changed to
> > "./bin/spark-class org.apache.spark.deploy.Client [client-options]
> launch \
> >    <cluster-url> <application-jar-url> <main-class> \
> >    [application-options] "
> >
> > That is to say: [client-options] must be put ahead of launch
> >
>

--001a11c15154fc3dca04fa994612--

From dev-return-7891-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 08:12:57 2014
Return-Path: <dev-return-7891-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A0AB510690
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 08:12:57 +0000 (UTC)
Received: (qmail 95247 invoked by uid 500); 30 May 2014 08:12:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95196 invoked by uid 500); 30 May 2014 08:12:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95188 invoked by uid 99); 30 May 2014 08:12:57 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 08:12:57 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of aniket.bhatnagar@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 08:12:52 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <aniket.bhatnagar@gmail.com>)
	id 1WqHvU-0006S4-2P
	for dev@spark.incubator.apache.org; Fri, 30 May 2014 01:12:32 -0700
Date: Fri, 30 May 2014 01:12:32 -0700 (PDT)
From: Aniket <aniket.bhatnagar@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1401437552047-6871.post@n3.nabble.com>
Subject: Why does spark REPL not embed scala REPL?
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

My apologies in advance if this is a dev mailing list topic. I am working on
a small project to provide web interface to spark REPL. The interface will
allow people to use spark REPL and perform exploratory analysis on the data.
I already have a play application running that provides web interface to
standard scala REPL and I am just looking to extend it to optionally include
support for spark REPL. My initial idea was to include spark dependencies in
the project, create a new instance of SparkContext and bind it to a variable
(lets say 'sc') using imain.bind("sc", sparkContext). While theoretically
this may work, I am trying to understand why spark REPL takes a different
path by creating it's own SparkILoop, SparkIMain, etc. Can anyone help me
understand why there was a need to provide custom versions of IMain, ILoop,
etc instead of embedding the standard scala REPL and binding SparkContext
instance?

Here is my analysis so far:
1. ExecutorClassLoader - I understand this is need to load classes from
HDFS. Perhaps this could have been plugged into the standard scala REPL
using settings.embeddedDefaults(classLoaderInstance). Also, it's not clear
what ConstructorCleaner does.

2. SparkCommandLine & SparkRunnerSettings - Allow for providing an extra -i
file argument to the REPL. The standard sourcepath wouldn't have sufficed?

3. SparkExprTyper - The only difference between standard ExprTyper and
SparkExprTyper is that repldbg is replaced with logDebug. Not sure if this
was intentional/needed.

4. SparkILoop - Has a few deviations from standard ILoop class but this
could have been managed by extending or wrapping ILoop class or using
settings. Not sure what triggered the need to copy the source code and make
edits.

5. SparkILoopInit - Changes the welcome message and binds spark context in
the interpreter. Welcome message could have been changed by extending
ILoopInit.

6. SparkIMain - Contains quiet a few changes around class
loading/logging/etc but I found it very hard to figure out if extension of
IMain was an option and what exactly didn't work/will not work with IMain.

Rest of the classes seem very similar to their standard counterparts. I have
a feeling the spark REPL can be refactored to embed standard scala REPL. I
know refactoring would not help Spark project as such but would help people
embed the spark REPL much in the same way it's done with standard scala
REPL. Thoughts? 



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Why-does-spark-REPL-not-embed-scala-REPL-tp6871.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7892-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 10:13:09 2014
Return-Path: <dev-return-7892-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A744C109A3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 10:13:09 +0000 (UTC)
Received: (qmail 31170 invoked by uid 500); 30 May 2014 10:13:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31076 invoked by uid 500); 30 May 2014 10:13:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31058 invoked by uid 99); 30 May 2014 10:13:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 10:13:07 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.50 as permitted sender)
Received: from [209.85.219.50] (HELO mail-oa0-f50.google.com) (209.85.219.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 10:13:03 +0000
Received: by mail-oa0-f50.google.com with SMTP id i7so1641343oag.9
        for <multiple recipients>; Fri, 30 May 2014 03:12:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=47V+dqqGwaz1poKsvfx5YEBMMXJr47/Ki7G1ICYhdtw=;
        b=KIo+kznJvoib9+/pDdlRZXav6ZPkKolYisaouj/H9e70tF6f/m0ZRY6xp21ltsGbgr
         HhXy2OcKtjSG5Ay0dIhOCLfvVxZ8pTlO4EbeneU50gbXyQdiSjWx+Slyl3r9OQPc82Pj
         KA2X0gz+i9xesQQzQp/LoXNNMkoGtIhjZVgsXmNXRaUdem30mnWEfl/Po4fU8v0d409M
         EwaP2Ca+gk1L1VBs4kbvEVEQxDc1mGuloUXrVe1McBTle8ftzV2OScsnem3G7L0X1CHH
         x3tCTeIjrWVpwRt3APxIUOtdsLjKyeZwB/DIE7QswwY+SArwd+O/aM3xVtgAFxQAJaWS
         oGBg==
MIME-Version: 1.0
X-Received: by 10.60.179.80 with SMTP id de16mr15560629oec.69.1401444759524;
 Fri, 30 May 2014 03:12:39 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Fri, 30 May 2014 03:12:39 -0700 (PDT)
Date: Fri, 30 May 2014 03:12:39 -0700
Message-ID: <CABPQxstP6EP4oCndBpeUfHv1rpu=Pp7cknjPD_QTf4dak6yGhA@mail.gmail.com>
Subject: Announcing Spark 1.0.0
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>, user@spark.apache.org
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I'm thrilled to announce the availability of Spark 1.0.0! Spark 1.0.0
is a milestone release as the first in the 1.0 line of releases,
providing API stability for Spark's core interfaces.

Spark 1.0.0 is Spark's largest release ever, with contributions from
117 developers. I'd like to thank everyone involved in this release -
it was truly a community effort with fixes, features, and
optimizations contributed from dozens of organizations.

This release expands Spark's standard libraries, introducing a new SQL
package (SparkSQL) which lets users integrate SQL queries into
existing Spark workflows. MLlib, Spark's machine learning library, is
expanded with sparse vector support and several new algorithms. The
GraphX and Streaming libraries also introduce new features and
optimizations. Spark's core engine adds support for secured YARN
clusters, a unified tool for submitting Spark applications, and
several performance and stability improvements. Finally, Spark adds
support for Java 8 lambda syntax and improves coverage of the Java and
Python API's.

Those features only scratch the surface - check out the release notes here:
http://spark.apache.org/releases/spark-release-1-0-0.html

Note that since release artifacts were posted recently, certain
mirrors may not have working downloads for a few hours.

- Patrick

From dev-return-7893-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 10:19:03 2014
Return-Path: <dev-return-7893-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6E25E109BF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 10:19:03 +0000 (UTC)
Received: (qmail 43371 invoked by uid 500); 30 May 2014 10:19:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43320 invoked by uid 500); 30 May 2014 10:19:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43312 invoked by uid 99); 30 May 2014 10:19:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 10:19:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ctn@adatao.com designates 209.85.213.179 as permitted sender)
Received: from [209.85.213.179] (HELO mail-ig0-f179.google.com) (209.85.213.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 10:18:58 +0000
Received: by mail-ig0-f179.google.com with SMTP id hn18so618798igb.6
        for <dev@spark.apache.org>; Fri, 30 May 2014 03:18:36 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=adatao.com; s=google;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=Q92wTENGYP79YOU3FVMsdUPhaeyMVxcpPRHss9rZ7Ig=;
        b=jaCFoEuS0eSSwjvHi1W7HBWEDav1tnDKJ/G13cJgVEm2m2EEQlE/55b38MQfugkJE2
         xd+pGqyc9zDvkiKOxFgIhLM8fG3lD2IsiDypCFGp5Z4CvooULGsvfMekPckFMPIS0hHM
         W5NYqtWkQSTyXyHEJ3yJy8dT9TJXDhEwqXc9s=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=Q92wTENGYP79YOU3FVMsdUPhaeyMVxcpPRHss9rZ7Ig=;
        b=bzjG36WQ/WH7HwInHiBcJtrrOtzdeW3AXcj8mf7PyFtRTEgYzACcSJBWqLWO+BWCGB
         A8i454k1avWKoGDU+vNAgU77/TM1fENNR5wOkGoaN3EbCU1e3L7ER6NkoIbsUFih5KrO
         SElGO91m4pgnhFi6VsckHfCLDB5d9PEhepCcv4N2o7RzS7AGU4pUcUBeC7en6+hxluMK
         b+uq5+qNM+WS829xhiFzL//ykaH0+JQf8AM/MHU84iU1wKWc1DXmekSG+WHkFjBXjifB
         qAfmcZg28xcz1QxbzPQM3g2xOSsFCwPgVjz2PZKGVKoGwKcEYiMbhDrO3MxXoCcaR4ob
         g9Pg==
X-Gm-Message-State: ALoCoQlXzkaiSez3LNnjzO5fw0CchTxeOK/Mn2rahORGCZcxC6fEpCipgeu7EgfuKBaZmBIj7qB7
X-Received: by 10.42.137.70 with SMTP id x6mr14040650ict.79.1401445115820;
 Fri, 30 May 2014 03:18:35 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.64.7.80 with HTTP; Fri, 30 May 2014 03:18:15 -0700 (PDT)
X-Originating-IP: [76.21.63.199]
In-Reply-To: <CABPQxstP6EP4oCndBpeUfHv1rpu=Pp7cknjPD_QTf4dak6yGhA@mail.gmail.com>
References: <CABPQxstP6EP4oCndBpeUfHv1rpu=Pp7cknjPD_QTf4dak6yGhA@mail.gmail.com>
From: Christopher Nguyen <ctn@adatao.com>
Date: Fri, 30 May 2014 03:18:15 -0700
Message-ID: <CAGh_TuMODEp_J-JB4fYb72Jg76QLvXVw=EUy7voGbLPO=pJsQA@mail.gmail.com>
Subject: Re: Announcing Spark 1.0.0
To: user@spark.apache.org
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=90e6ba1f00744d03c504fa9b5e86
X-Virus-Checked: Checked by ClamAV on apache.org

--90e6ba1f00744d03c504fa9b5e86
Content-Type: text/plain; charset=UTF-8

Awesome work, Pat et al.!

--
Christopher T. Nguyen
Co-founder & CEO, Adatao <http://adatao.com>
linkedin.com/in/ctnguyen



On Fri, May 30, 2014 at 3:12 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> I'm thrilled to announce the availability of Spark 1.0.0! Spark 1.0.0
> is a milestone release as the first in the 1.0 line of releases,
> providing API stability for Spark's core interfaces.
>
> Spark 1.0.0 is Spark's largest release ever, with contributions from
> 117 developers. I'd like to thank everyone involved in this release -
> it was truly a community effort with fixes, features, and
> optimizations contributed from dozens of organizations.
>
> This release expands Spark's standard libraries, introducing a new SQL
> package (SparkSQL) which lets users integrate SQL queries into
> existing Spark workflows. MLlib, Spark's machine learning library, is
> expanded with sparse vector support and several new algorithms. The
> GraphX and Streaming libraries also introduce new features and
> optimizations. Spark's core engine adds support for secured YARN
> clusters, a unified tool for submitting Spark applications, and
> several performance and stability improvements. Finally, Spark adds
> support for Java 8 lambda syntax and improves coverage of the Java and
> Python API's.
>
> Those features only scratch the surface - check out the release notes here:
> http://spark.apache.org/releases/spark-release-1-0-0.html
>
> Note that since release artifacts were posted recently, certain
> mirrors may not have working downloads for a few hours.
>
> - Patrick
>

--90e6ba1f00744d03c504fa9b5e86--

From dev-return-7894-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 11:16:26 2014
Return-Path: <dev-return-7894-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C3D3C10B26
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 11:16:26 +0000 (UTC)
Received: (qmail 38574 invoked by uid 500); 30 May 2014 11:16:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38513 invoked by uid 500); 30 May 2014 11:16:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38505 invoked by uid 99); 30 May 2014 11:16:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 11:16:26 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of kanzhangemail@gmail.com designates 209.85.223.172 as permitted sender)
Received: from [209.85.223.172] (HELO mail-ie0-f172.google.com) (209.85.223.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 11:16:21 +0000
Received: by mail-ie0-f172.google.com with SMTP id tp5so1568697ieb.17
        for <dev@spark.apache.org>; Fri, 30 May 2014 04:16:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:reply-to:sender:in-reply-to:references:date:message-id
         :subject:from:to:content-type;
        bh=OlNHGaaUg6GcOkOm+wO+ONmV3sJ9nbf5raGAtsfq0KU=;
        b=DPfOHt12Vx6yNfyU0SMv7NX1RxzLKaliAo6Yes4Wyo73A8Vgsn62isGWRau1W0tq0V
         mzXQc0805rq9q6+LkX/nyNF3n59/5BX3NUWkeKLAP4//NqlB7H5BMzRrpi0MJxnEASUJ
         ygZtMUCzl9Db6F4cbODWj7/8Tc7eVk+C87jxyz5lQlH2xzFddrVOpT3UnCtKKdn00e2V
         UQEqOn1mv/ZFpffxHvfTq3kw/LNiVWCuvzWmKW9gzO9KZPfY1DTfSNlH1EbXfzXoCtn0
         8EnrXhNWBxg6UtX0VFKgcG7dRyz1swu0b0w8F2TNQSBeapy0QD7slkD9O7lwi5ydGFi6
         JHaA==
MIME-Version: 1.0
X-Received: by 10.50.92.42 with SMTP id cj10mr4702445igb.34.1401448560961;
 Fri, 30 May 2014 04:16:00 -0700 (PDT)
Reply-To: kzhang@apache.org
Sender: kanzhangemail@gmail.com
Received: by 10.64.22.230 with HTTP; Fri, 30 May 2014 04:16:00 -0700 (PDT)
In-Reply-To: <1401437552047-6871.post@n3.nabble.com>
References: <1401437552047-6871.post@n3.nabble.com>
Date: Fri, 30 May 2014 04:16:00 -0700
X-Google-Sender-Auth: 10O_WLEo63dIA43qSCibiBAPSg8
Message-ID: <CALRHqP_3faQX26y0pAmpauAYY4Uo18gRmn0DPBE1aNK1mkMAeQ@mail.gmail.com>
Subject: Re: Why does spark REPL not embed scala REPL?
From: Kan Zhang <kzhang@apache.org>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b10d0b5a5acd604fa9c2b6f
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b10d0b5a5acd604fa9c2b6f
Content-Type: text/plain; charset=UTF-8

One reason is standard Scala REPL uses object based wrappers and their
static initializers will be run on remote worker nodes, which may fail due
to differences between driver and worker nodes. See discussion here
https://groups.google.com/d/msg/scala-internals/h27CFLoJXjE/JoobM6NiUMQJ


On Fri, May 30, 2014 at 1:12 AM, Aniket <aniket.bhatnagar@gmail.com> wrote:

> My apologies in advance if this is a dev mailing list topic. I am working
> on
> a small project to provide web interface to spark REPL. The interface will
> allow people to use spark REPL and perform exploratory analysis on the
> data.
> I already have a play application running that provides web interface to
> standard scala REPL and I am just looking to extend it to optionally
> include
> support for spark REPL. My initial idea was to include spark dependencies
> in
> the project, create a new instance of SparkContext and bind it to a
> variable
> (lets say 'sc') using imain.bind("sc", sparkContext). While theoretically
> this may work, I am trying to understand why spark REPL takes a different
> path by creating it's own SparkILoop, SparkIMain, etc. Can anyone help me
> understand why there was a need to provide custom versions of IMain, ILoop,
> etc instead of embedding the standard scala REPL and binding SparkContext
> instance?
>
> Here is my analysis so far:
> 1. ExecutorClassLoader - I understand this is need to load classes from
> HDFS. Perhaps this could have been plugged into the standard scala REPL
> using settings.embeddedDefaults(classLoaderInstance). Also, it's not clear
> what ConstructorCleaner does.
>
> 2. SparkCommandLine & SparkRunnerSettings - Allow for providing an extra -i
> file argument to the REPL. The standard sourcepath wouldn't have sufficed?
>
> 3. SparkExprTyper - The only difference between standard ExprTyper and
> SparkExprTyper is that repldbg is replaced with logDebug. Not sure if this
> was intentional/needed.
>
> 4. SparkILoop - Has a few deviations from standard ILoop class but this
> could have been managed by extending or wrapping ILoop class or using
> settings. Not sure what triggered the need to copy the source code and make
> edits.
>
> 5. SparkILoopInit - Changes the welcome message and binds spark context in
> the interpreter. Welcome message could have been changed by extending
> ILoopInit.
>
> 6. SparkIMain - Contains quiet a few changes around class
> loading/logging/etc but I found it very hard to figure out if extension of
> IMain was an option and what exactly didn't work/will not work with IMain.
>
> Rest of the classes seem very similar to their standard counterparts. I
> have
> a feeling the spark REPL can be refactored to embed standard scala REPL. I
> know refactoring would not help Spark project as such but would help people
> embed the spark REPL much in the same way it's done with standard scala
> REPL. Thoughts?
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Why-does-spark-REPL-not-embed-scala-REPL-tp6871.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--047d7b10d0b5a5acd604fa9c2b6f--

From dev-return-7895-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 11:58:13 2014
Return-Path: <dev-return-7895-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C751E10DBD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 11:58:13 +0000 (UTC)
Received: (qmail 39961 invoked by uid 500); 30 May 2014 11:58:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39896 invoked by uid 500); 30 May 2014 11:58:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39887 invoked by uid 99); 30 May 2014 11:58:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 11:58:13 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of Rahul.Singhal@guavus.com designates 204.232.241.167 as permitted sender)
Received: from [204.232.241.167] (HELO mx1.guavus.com) (204.232.241.167)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 11:58:06 +0000
Received: from MX3.guavus.com ([192.168.11.2]) by mx1.guavus.com
 ([204.232.241.167]) with mapi id 14.03.0174.001; Fri, 30 May 2014 04:57:45
 -0700
From: Rahul Singhal <Rahul.Singhal@guavus.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: Announcing Spark 1.0.0
Thread-Topic: Announcing Spark 1.0.0
Thread-Index: AQHPe+/BgHxWifzNFEiImYfDFtBkB5tZ1mcA
Date: Fri, 30 May 2014 11:57:45 +0000
Message-ID: <CFAE6F75.1FF96%rahul.singhal@guavus.com>
References: <CABPQxstP6EP4oCndBpeUfHv1rpu=Pp7cknjPD_QTf4dak6yGhA@mail.gmail.com>
In-Reply-To: <CABPQxstP6EP4oCndBpeUfHv1rpu=Pp7cknjPD_QTf4dak6yGhA@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [61.12.3.121]
Content-Type: text/plain; charset="us-ascii"
Content-ID: <436B8342E3A1334B96C05A47C666BD87@guavus.com>
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Is it intentional/ok that the tag v1.0.0 is behind tag v1.0.0-rc11?


Thanks,
Rahul Singhal





On 30/05/14 3:43 PM, "Patrick Wendell" <pwendell@gmail.com> wrote:

>I'm thrilled to announce the availability of Spark 1.0.0! Spark 1.0.0
>is a milestone release as the first in the 1.0 line of releases,
>providing API stability for Spark's core interfaces.
>
>Spark 1.0.0 is Spark's largest release ever, with contributions from
>117 developers. I'd like to thank everyone involved in this release -
>it was truly a community effort with fixes, features, and
>optimizations contributed from dozens of organizations.
>
>This release expands Spark's standard libraries, introducing a new SQL
>package (SparkSQL) which lets users integrate SQL queries into
>existing Spark workflows. MLlib, Spark's machine learning library, is
>expanded with sparse vector support and several new algorithms. The
>GraphX and Streaming libraries also introduce new features and
>optimizations. Spark's core engine adds support for secured YARN
>clusters, a unified tool for submitting Spark applications, and
>several performance and stability improvements. Finally, Spark adds
>support for Java 8 lambda syntax and improves coverage of the Java and
>Python API's.
>
>Those features only scratch the surface - check out the release notes
>here:
>http://spark.apache.org/releases/spark-release-1-0-0.html
>
>Note that since release artifacts were posted recently, certain
>mirrors may not have working downloads for a few hours.
>
>- Patrick


From dev-return-7896-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 13:10:19 2014
Return-Path: <dev-return-7896-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E182510173
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 13:10:18 +0000 (UTC)
Received: (qmail 87535 invoked by uid 500); 30 May 2014 13:10:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87472 invoked by uid 500); 30 May 2014 13:10:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87464 invoked by uid 99); 30 May 2014 13:10:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 13:10:18 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.128.176 as permitted sender)
Received: from [209.85.128.176] (HELO mail-ve0-f176.google.com) (209.85.128.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 13:10:14 +0000
Received: by mail-ve0-f176.google.com with SMTP id jz11so2043613veb.7
        for <dev@spark.apache.org>; Fri, 30 May 2014 06:09:50 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=4yP0vaktxfir7F3X0sTonuaZ5oWVNpUP67wSR6DvCAs=;
        b=E9GSjXQFnOhiobEBmYI2j+AknP5iK9NxUqLc1h4b4LJAcO9SRb8+USKsQViB/xIBkL
         DT78y9Qt0WXkKhz1SYy0p43ysumAbR4qesCzAtU0TPVVrzIxXXBRjwibHBx9oxk9rzsA
         PrAHxQswCxbpY5+XB+JopVfSVk7fuqKEy+wjMoBHrwgd/7S+r+bg8UISuVTHkkOlNMzN
         SZZD++z3whwnPaxQnwPlV23U/d7LdtbboOKjR2Jc+6KT2oFmw9YVozc4vdvJ4jQZ15wZ
         ytnbyITxU6UwPixruffknBL2FTnIKxJup9pKeEp0Mt+TaDz/QRzBMeohCLwJQHEpaJiL
         Um8w==
X-Gm-Message-State: ALoCoQl5ZrK1co8NvLlj2R+LTTt5o3viD1xDki3/zLd5yLoI9TaqyuhJNscrFgyiNjqj1tlT/Jvt
X-Received: by 10.220.196.82 with SMTP id ef18mr386905vcb.78.1401455390232;
 Fri, 30 May 2014 06:09:50 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.121.6 with HTTP; Fri, 30 May 2014 06:09:30 -0700 (PDT)
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 30 May 2014 14:09:30 +0100
Message-ID: <CAMAsSdKp8B03cdW3TkNSCyaHTnCtzznkaqRSYjqHQzH_yzPSzQ@mail.gmail.com>
Subject: Streaming example stops outputting (Java, Kafka at least)
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Guys I'm struggling to debug some strange behavior in a simple
Streaming + Java + Kafka example -- in fact, a simplified version of
JavaKafkaWordcount, that is just calling print() on a sequence of
messages.

Data is flowing, but it only appears to work for a few periods --
sometimes 0 -- before ceasing to call any actions. Sorry for lots of
log posting but it may illustrate to someone who knows this better
what is happening:



Key action in the logs seems to be as follows -- it works a few times:

...
2014-05-30 13:53:50 INFO  ReceiverTracker:58 - Stream 0 received 0 blocks
2014-05-30 13:53:50 INFO  JobScheduler:58 - Added jobs for time 1401454430000 ms
-------------------------------------------
Time: 1401454430000 ms
-------------------------------------------

2014-05-30 13:53:50 INFO  JobScheduler:58 - Starting job streaming job
1401454430000 ms.0 from job set of time 1401454430000 ms
2014-05-30 13:53:50 INFO  JobScheduler:58 - Finished job streaming job
1401454430000 ms.0 from job set of time 1401454430000 ms
2014-05-30 13:53:50 INFO  JobScheduler:58 - Total delay: 0.004 s for
time 1401454430000 ms (execution: 0.000 s)
2014-05-30 13:53:50 INFO  MappedRDD:58 - Removing RDD 2 from persistence list
2014-05-30 13:53:50 INFO  BlockManager:58 - Removing RDD 2
2014-05-30 13:53:50 INFO  BlockRDD:58 - Removing RDD 1 from persistence list
2014-05-30 13:53:50 INFO  BlockManager:58 - Removing RDD 1
2014-05-30 13:53:50 INFO  KafkaInputDStream:58 - Removing blocks of
RDD BlockRDD[1] at BlockRDD at ReceiverInputDStream.scala:69 of time
1401454430000 ms
2014-05-30 13:54:00 INFO  ReceiverTracker:58 - Stream 0 received 0 blocks
2014-05-30 13:54:00 INFO  JobScheduler:58 - Added jobs for time 1401454440000 ms
...


Then works with some additional, different output in the logs -- here
you see output is flowing too:

...
2014-05-30 13:54:20 INFO  ReceiverTracker:58 - Stream 0 received 2 blocks
2014-05-30 13:54:20 INFO  JobScheduler:58 - Added jobs for time 1401454460000 ms
2014-05-30 13:54:20 INFO  JobScheduler:58 - Starting job streaming job
1401454460000 ms.0 from job set of time 1401454460000 ms
2014-05-30 13:54:20 INFO  SparkContext:58 - Starting job: take at
DStream.scala:593
2014-05-30 13:54:20 INFO  DAGScheduler:58 - Got job 1 (take at
DStream.scala:593) with 1 output partitions (allowLocal=true)
2014-05-30 13:54:20 INFO  DAGScheduler:58 - Final stage: Stage 1(take
at DStream.scala:593)
2014-05-30 13:54:20 INFO  DAGScheduler:58 - Parents of final stage: List()
2014-05-30 13:54:20 INFO  DAGScheduler:58 - Missing parents: List()
2014-05-30 13:54:20 INFO  DAGScheduler:58 - Computing the requested
partition locally
2014-05-30 13:54:20 INFO  BlockManager:58 - Found block
input-0-1401454458400 locally
2014-05-30 13:54:20 INFO  SparkContext:58 - Job finished: take at
DStream.scala:593, took 0.007007 s
2014-05-30 13:54:20 INFO  SparkContext:58 - Starting job: take at
DStream.scala:593
2014-05-30 13:54:20 INFO  DAGScheduler:58 - Got job 2 (take at
DStream.scala:593) with 1 output partitions (allowLocal=true)
2014-05-30 13:54:20 INFO  DAGScheduler:58 - Final stage: Stage 2(take
at DStream.scala:593)
2014-05-30 13:54:20 INFO  DAGScheduler:58 - Parents of final stage: List()
2014-05-30 13:54:20 INFO  DAGScheduler:58 - Missing parents: List()
2014-05-30 13:54:20 INFO  DAGScheduler:58 - Computing the requested
partition locally
2014-05-30 13:54:20 INFO  BlockManager:58 - Found block
input-0-1401454459400 locally
2014-05-30 13:54:20 INFO  SparkContext:58 - Job finished: take at
DStream.scala:593, took 0.002217 s
-------------------------------------------
Time: 1401454460000 ms
-------------------------------------------
99,true,-0.11342268416043325
17,false,1.6732879882133793
...


Then keeps repeating the following with no more evidence that the
print() action is being called:

...
2014-05-30 13:54:20 INFO  JobScheduler:58 - Finished job streaming job
1401454460000 ms.0 from job set of time 1401454460000 ms
2014-05-30 13:54:20 INFO  MappedRDD:58 - Removing RDD 8 from persistence list
2014-05-30 13:54:20 INFO  JobScheduler:58 - Total delay: 0.019 s for
time 1401454460000 ms (execution: 0.015 s)
2014-05-30 13:54:20 INFO  BlockManager:58 - Removing RDD 8
2014-05-30 13:54:20 INFO  BlockRDD:58 - Removing RDD 7 from persistence list
2014-05-30 13:54:20 INFO  BlockManager:58 - Removing RDD 7
2014-05-30 13:54:20 INFO  KafkaInputDStream:58 - Removing blocks of
RDD BlockRDD[7] at BlockRDD at ReceiverInputDStream.scala:69 of time
1401454460000 ms
2014-05-30 13:54:20 INFO  MemoryStore:58 - ensureFreeSpace(100) called
with curMem=201, maxMem=2290719129
2014-05-30 13:54:20 INFO  MemoryStore:58 - Block input-0-1401454460400
stored as bytes to memory (size 100.0 B, free 2.1 GB)
2014-05-30 13:54:20 INFO  BlockManagerInfo:58 - Added
input-0-1401454460400 in memory on 192.168.1.10:60886 (size: 100.0 B,
free: 2.1 GB)
2014-05-30 13:54:20 INFO  BlockManagerMaster:58 - Updated info of
block input-0-1401454460400
2014-05-30 13:54:20 WARN  BlockManager:70 - Block
input-0-1401454460400 already exists on this machine; not re-adding it
2014-05-30 13:54:20 INFO  BlockGenerator:58 - Pushed block input-0-1401454460400
2014-05-30 13:54:21 INFO  MemoryStore:58 - ensureFreeSpace(100) called
with curMem=301, maxMem=2290719129
2014-05-30 13:54:21 INFO  MemoryStore:58 - Block input-0-1401454461400
stored as bytes to memory (size 100.0 B, free 2.1 GB)
2014-05-30 13:54:21 INFO  BlockManagerInfo:58 - Added
input-0-1401454461400 in memory on 192.168.1.10:60886 (size: 100.0 B,
free: 2.1 GB)
2014-05-30 13:54:21 INFO  BlockManagerMaster:58 - Updated info of
block input-0-1401454461400
2014-05-30 13:54:21 WARN  BlockManager:70 - Block
input-0-1401454461400 already exists on this machine; not re-adding it
2014-05-30 13:54:21 INFO  BlockGenerator:58 - Pushed block input-0-1401454461400
2014-05-30 13:54:22 INFO  MemoryStore:58 - ensureFreeSpace(99) called
with curMem=401, maxMem=2290719129
2014-05-30 13:54:22 INFO  MemoryStore:58 - Block input-0-1401454462400
stored as bytes to memory (size 99.0 B, free 2.1 GB)
2014-05-30 13:54:22 INFO  BlockManagerInfo:58 - Added
input-0-1401454462400 in memory on 192.168.1.10:60886 (size: 99.0 B,
free: 2.1 GB)
...


Occasionally it says:

...
2014-05-30 13:54:30 INFO  ReceiverTracker:58 - Stream 0 received 10 blocks
2014-05-30 13:54:30 INFO  JobScheduler:58 - Added jobs for time 1401454470000 ms
2014-05-30 13:54:30 INFO  JobScheduler:58 - Starting job streaming job
1401454470000 ms.0 from job set of time 1401454470000 ms
2014-05-30 13:54:30 INFO  SparkContext:58 - Starting job: take at
DStream.scala:593
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Got job 3 (take at
DStream.scala:593) with 1 output partitions (allowLocal=true)
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Final stage: Stage 3(take
at DStream.scala:593)
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Parents of final stage: List()
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Missing parents: List()
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Computing the requested
partition locally
2014-05-30 13:54:30 INFO  BlockManager:58 - Found block
input-0-1401454460400 locally
2014-05-30 13:54:30 INFO  SparkContext:58 - Job finished: take at
DStream.scala:593, took 0.003993 s
2014-05-30 13:54:30 INFO  SparkContext:58 - Starting job: take at
DStream.scala:593
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Got job 4 (take at
DStream.scala:593) with 9 output partitions (allowLocal=true)
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Final stage: Stage 4(take
at DStream.scala:593)
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Parents of final stage: List()
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Missing parents: List()
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Submitting Stage 4
(MappedRDD[12] at map at MappedDStream.scala:35), which has no missing
parents
2014-05-30 13:54:30 INFO  DAGScheduler:58 - Submitting 9 missing tasks
from Stage 4 (MappedRDD[12] at map at MappedDStream.scala:35)
2014-05-30 13:54:30 INFO  TaskSchedulerImpl:58 - Adding task set 4.0
with 9 tasks
...


Output is definitely continuing to be written to Kafka; you can even
see that it seems to be acknolwedging that the stream is seeing more
data.

The same happens with operations like saving to file. It looks like
the action is no longer scheduled.

Does that ring any bells? I'm at a loss!

From dev-return-7897-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 13:34:28 2014
Return-Path: <dev-return-7897-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D46CA102DA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 13:34:28 +0000 (UTC)
Received: (qmail 36860 invoked by uid 500); 30 May 2014 13:34:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36802 invoked by uid 500); 30 May 2014 13:34:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36794 invoked by uid 99); 30 May 2014 13:34:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 13:34:28 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.192.41 as permitted sender)
Received: from [209.85.192.41] (HELO mail-qg0-f41.google.com) (209.85.192.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 13:34:22 +0000
Received: by mail-qg0-f41.google.com with SMTP id j5so5335053qga.0
        for <dev@spark.apache.org>; Fri, 30 May 2014 06:33:58 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=1w51MyLjAsXMatNE9QbMWrPZallf2OhT8eDEcLD84uU=;
        b=zgJeYZB2j+dn1yQzGc7bzflk6N6Zuzbwg2PuASgtgeZA4zL1XaZgYCF4Tw5wj2FtGu
         9NKefPhul5CsjrHvZEeuanGjlyXKqe3+7ONswWD87tsaCrD6TJ1EbGmLYwSy4gzT+Kxd
         bDoCgcyWhbpaKyyQlHG6jBJATNgj9cbnxr10kdOu9r0pFAjCPf3VelmqBmEl/byeJ9PO
         jU8nNvMFgkSclu+AqSb8Grys3IfZ2s7vKhg+j/lIjdfvNIv8/R/53U/tuvOERBWKYSFk
         GWbCWXjAdd43rZrzB7lwwkN8WnVTmtPDObaffJoLnOL5kqw+4inCO0m+4IDPB6jKEKjN
         5zyA==
X-Received: by 10.224.71.145 with SMTP id h17mr21589630qaj.74.1401456838828;
        Fri, 30 May 2014 06:33:58 -0700 (PDT)
Received: from [192.168.2.11] (MTRLPQ02-1177746539.sdsl.bell.ca. [70.50.252.107])
        by mx.google.com with ESMTPSA id u6sm6246896qah.28.2014.05.30.06.33.58
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Fri, 30 May 2014 06:33:58 -0700 (PDT)
Date: Fri, 30 May 2014 09:42:27 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Message-ID: <FE2E914E3E2F4BBBA69E856154E92CC7@gmail.com>
In-Reply-To: <CAMAsSdKp8B03cdW3TkNSCyaHTnCtzznkaqRSYjqHQzH_yzPSzQ@mail.gmail.com>
References: <CAMAsSdKp8B03cdW3TkNSCyaHTnCtzznkaqRSYjqHQzH_yzPSzQ@mail.gmail.com>
Subject: Re: Streaming example stops outputting (Java, Kafka at least)
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53888ac3_392edbe4_9a4"
X-Virus-Checked: Checked by ClamAV on apache.org

--53888ac3_392edbe4_9a4
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

Hi, Sean  =20

I was in the same problem

but when I changed MASTER=3D=E2=80=9Clocal=E2=80=9D to MASTER=3D=E2=80=9C=
local=5B2=5D=E2=80=9D

everything back to the normal

Hasn=E2=80=99t get a chance to ask here

Best, =20

-- =20
Nan Zhu


On =46riday, May 30, 2014 at 9:09 AM, Sean Owen wrote:

> Guys I'm struggling to debug some strange behavior in a simple
> Streaming + Java + Kafka example -- in fact, a simplified version of
> JavaKafkaWordcount, that is just calling print() on a sequence of
> messages.
> =20
> Data is flowing, but it only appears to work for a few periods --
> sometimes 0 -- before ceasing to call any actions. Sorry for lots of
> log posting but it may illustrate to someone who knows this better
> what is happening:
> =20
> =20
> =20
> Key action in the logs seems to be as follows -- it works a few times:
> =20
> ...
> 2014-05-30 13:53:50 IN=46O ReceiverTracker:58 - Stream 0 received 0 blo=
cks
> 2014-05-30 13:53:50 IN=46O JobScheduler:58 - Added jobs for time 140145=
4430000 ms
> -------------------------------------------
> Time: 1401454430000 ms
> -------------------------------------------
> =20
> 2014-05-30 13:53:50 IN=46O JobScheduler:58 - Starting job streaming job=

> 1401454430000 ms.0 from job set of time 1401454430000 ms
> 2014-05-30 13:53:50 IN=46O JobScheduler:58 - =46inished job streaming j=
ob
> 1401454430000 ms.0 from job set of time 1401454430000 ms
> 2014-05-30 13:53:50 IN=46O JobScheduler:58 - Total delay: 0.004 s for
> time 1401454430000 ms (execution: 0.000 s)
> 2014-05-30 13:53:50 IN=46O MappedRDD:58 - Removing RDD 2 from persisten=
ce list
> 2014-05-30 13:53:50 IN=46O BlockManager:58 - Removing RDD 2
> 2014-05-30 13:53:50 IN=46O BlockRDD:58 - Removing RDD 1 from persistenc=
e list
> 2014-05-30 13:53:50 IN=46O BlockManager:58 - Removing RDD 1
> 2014-05-30 13:53:50 IN=46O KafkaInputDStream:58 - Removing blocks of
> RDD BlockRDD=5B1=5D at BlockRDD at ReceiverInputDStream.scala:69 of tim=
e
> 1401454430000 ms
> 2014-05-30 13:54:00 IN=46O ReceiverTracker:58 - Stream 0 received 0 blo=
cks
> 2014-05-30 13:54:00 IN=46O JobScheduler:58 - Added jobs for time 140145=
4440000 ms
> ...
> =20
> =20
> Then works with some additional, different output in the logs -- here
> you see output is flowing too:
> =20
> ...
> 2014-05-30 13:54:20 IN=46O ReceiverTracker:58 - Stream 0 received 2 blo=
cks
> 2014-05-30 13:54:20 IN=46O JobScheduler:58 - Added jobs for time 140145=
4460000 ms
> 2014-05-30 13:54:20 IN=46O JobScheduler:58 - Starting job streaming job=

> 1401454460000 ms.0 from job set of time 1401454460000 ms
> 2014-05-30 13:54:20 IN=46O SparkContext:58 - Starting job: take at
> DStream.scala:593
> 2014-05-30 13:54:20 IN=46O DAGScheduler:58 - Got job 1 (take at
> DStream.scala:593) with 1 output partitions (allowLocal=3Dtrue)
> 2014-05-30 13:54:20 IN=46O DAGScheduler:58 - =46inal stage: Stage 1(tak=
e
> at DStream.scala:593)
> 2014-05-30 13:54:20 IN=46O DAGScheduler:58 - Parents of final stage: Li=
st()
> 2014-05-30 13:54:20 IN=46O DAGScheduler:58 - Missing parents: List()
> 2014-05-30 13:54:20 IN=46O DAGScheduler:58 - Computing the requested
> partition locally
> 2014-05-30 13:54:20 IN=46O BlockManager:58 - =46ound block
> input-0-1401454458400 locally
> 2014-05-30 13:54:20 IN=46O SparkContext:58 - Job finished: take at
> DStream.scala:593, took 0.007007 s
> 2014-05-30 13:54:20 IN=46O SparkContext:58 - Starting job: take at
> DStream.scala:593
> 2014-05-30 13:54:20 IN=46O DAGScheduler:58 - Got job 2 (take at
> DStream.scala:593) with 1 output partitions (allowLocal=3Dtrue)
> 2014-05-30 13:54:20 IN=46O DAGScheduler:58 - =46inal stage: Stage 2(tak=
e
> at DStream.scala:593)
> 2014-05-30 13:54:20 IN=46O DAGScheduler:58 - Parents of final stage: Li=
st()
> 2014-05-30 13:54:20 IN=46O DAGScheduler:58 - Missing parents: List()
> 2014-05-30 13:54:20 IN=46O DAGScheduler:58 - Computing the requested
> partition locally
> 2014-05-30 13:54:20 IN=46O BlockManager:58 - =46ound block
> input-0-1401454459400 locally
> 2014-05-30 13:54:20 IN=46O SparkContext:58 - Job finished: take at
> DStream.scala:593, took 0.002217 s
> -------------------------------------------
> Time: 1401454460000 ms
> -------------------------------------------
> 99,true,-0.11342268416043325
> 17,false,1.6732879882133793
> ...
> =20
> =20
> Then keeps repeating the following with no more evidence that the
> print() action is being called:
> =20
> ...
> 2014-05-30 13:54:20 IN=46O JobScheduler:58 - =46inished job streaming j=
ob
> 1401454460000 ms.0 from job set of time 1401454460000 ms
> 2014-05-30 13:54:20 IN=46O MappedRDD:58 - Removing RDD 8 from persisten=
ce list
> 2014-05-30 13:54:20 IN=46O JobScheduler:58 - Total delay: 0.019 s for
> time 1401454460000 ms (execution: 0.015 s)
> 2014-05-30 13:54:20 IN=46O BlockManager:58 - Removing RDD 8
> 2014-05-30 13:54:20 IN=46O BlockRDD:58 - Removing RDD 7 from persistenc=
e list
> 2014-05-30 13:54:20 IN=46O BlockManager:58 - Removing RDD 7
> 2014-05-30 13:54:20 IN=46O KafkaInputDStream:58 - Removing blocks of
> RDD BlockRDD=5B7=5D at BlockRDD at ReceiverInputDStream.scala:69 of tim=
e
> 1401454460000 ms
> 2014-05-30 13:54:20 IN=46O MemoryStore:58 - ensure=46reeSpace(100) call=
ed
> with curMem=3D201, maxMem=3D2290719129
> 2014-05-30 13:54:20 IN=46O MemoryStore:58 - Block input-0-1401454460400=

> stored as bytes to memory (size 100.0 B, free 2.1 GB)
> 2014-05-30 13:54:20 IN=46O BlockManagerInfo:58 - Added
> input-0-1401454460400 in memory on 192.168.1.10:60886 (size: 100.0 B,
> free: 2.1 GB)
> 2014-05-30 13:54:20 IN=46O BlockManagerMaster:58 - Updated info of
> block input-0-1401454460400
> 2014-05-30 13:54:20 WARN BlockManager:70 - Block
> input-0-1401454460400 already exists on this machine; not re-adding it
> 2014-05-30 13:54:20 IN=46O BlockGenerator:58 - Pushed block input-0-140=
1454460400
> 2014-05-30 13:54:21 IN=46O MemoryStore:58 - ensure=46reeSpace(100) call=
ed
> with curMem=3D301, maxMem=3D2290719129
> 2014-05-30 13:54:21 IN=46O MemoryStore:58 - Block input-0-1401454461400=

> stored as bytes to memory (size 100.0 B, free 2.1 GB)
> 2014-05-30 13:54:21 IN=46O BlockManagerInfo:58 - Added
> input-0-1401454461400 in memory on 192.168.1.10:60886 (size: 100.0 B,
> free: 2.1 GB)
> 2014-05-30 13:54:21 IN=46O BlockManagerMaster:58 - Updated info of
> block input-0-1401454461400
> 2014-05-30 13:54:21 WARN BlockManager:70 - Block
> input-0-1401454461400 already exists on this machine; not re-adding it
> 2014-05-30 13:54:21 IN=46O BlockGenerator:58 - Pushed block input-0-140=
1454461400
> 2014-05-30 13:54:22 IN=46O MemoryStore:58 - ensure=46reeSpace(99) calle=
d
> with curMem=3D401, maxMem=3D2290719129
> 2014-05-30 13:54:22 IN=46O MemoryStore:58 - Block input-0-1401454462400=

> stored as bytes to memory (size 99.0 B, free 2.1 GB)
> 2014-05-30 13:54:22 IN=46O BlockManagerInfo:58 - Added
> input-0-1401454462400 in memory on 192.168.1.10:60886 (size: 99.0 B,
> free: 2.1 GB)
> ...
> =20
> =20
> Occasionally it says:
> =20
> ...
> 2014-05-30 13:54:30 IN=46O ReceiverTracker:58 - Stream 0 received 10 bl=
ocks
> 2014-05-30 13:54:30 IN=46O JobScheduler:58 - Added jobs for time 140145=
4470000 ms
> 2014-05-30 13:54:30 IN=46O JobScheduler:58 - Starting job streaming job=

> 1401454470000 ms.0 from job set of time 1401454470000 ms
> 2014-05-30 13:54:30 IN=46O SparkContext:58 - Starting job: take at
> DStream.scala:593
> 2014-05-30 13:54:30 IN=46O DAGScheduler:58 - Got job 3 (take at
> DStream.scala:593) with 1 output partitions (allowLocal=3Dtrue)
> 2014-05-30 13:54:30 IN=46O DAGScheduler:58 - =46inal stage: Stage 3(tak=
e
> at DStream.scala:593)
> 2014-05-30 13:54:30 IN=46O DAGScheduler:58 - Parents of final stage: Li=
st()
> 2014-05-30 13:54:30 IN=46O DAGScheduler:58 - Missing parents: List()
> 2014-05-30 13:54:30 IN=46O DAGScheduler:58 - Computing the requested
> partition locally
> 2014-05-30 13:54:30 IN=46O BlockManager:58 - =46ound block
> input-0-1401454460400 locally
> 2014-05-30 13:54:30 IN=46O SparkContext:58 - Job finished: take at
> DStream.scala:593, took 0.003993 s
> 2014-05-30 13:54:30 IN=46O SparkContext:58 - Starting job: take at
> DStream.scala:593
> 2014-05-30 13:54:30 IN=46O DAGScheduler:58 - Got job 4 (take at
> DStream.scala:593) with 9 output partitions (allowLocal=3Dtrue)
> 2014-05-30 13:54:30 IN=46O DAGScheduler:58 - =46inal stage: Stage 4(tak=
e
> at DStream.scala:593)
> 2014-05-30 13:54:30 IN=46O DAGScheduler:58 - Parents of final stage: Li=
st()
> 2014-05-30 13:54:30 IN=46O DAGScheduler:58 - Missing parents: List()
> 2014-05-30 13:54:30 IN=46O DAGScheduler:58 - Submitting Stage 4
> (MappedRDD=5B12=5D at map at MappedDStream.scala:35), which has no miss=
ing
> parents
> 2014-05-30 13:54:30 IN=46O DAGScheduler:58 - Submitting 9 missing tasks=

> from Stage 4 (MappedRDD=5B12=5D at map at MappedDStream.scala:35)
> 2014-05-30 13:54:30 IN=46O TaskSchedulerImpl:58 - Adding task set 4.0
> with 9 tasks
> ...
> =20
> =20
> Output is definitely continuing to be written to Kafka; you can even
> see that it seems to be acknolwedging that the stream is seeing more
> data.
> =20
> The same happens with operations like saving to file. It looks like
> the action is no longer scheduled.
> =20
> Does that ring any bells=3F I'm at a loss=21 =20


--53888ac3_392edbe4_9a4--


From dev-return-7898-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 13:39:17 2014
Return-Path: <dev-return-7898-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D738B102F1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 13:39:17 +0000 (UTC)
Received: (qmail 44225 invoked by uid 500); 30 May 2014 13:39:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44169 invoked by uid 500); 30 May 2014 13:39:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44156 invoked by uid 99); 30 May 2014 13:39:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 13:39:17 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.220.174 as permitted sender)
Received: from [209.85.220.174] (HELO mail-vc0-f174.google.com) (209.85.220.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 13:39:14 +0000
Received: by mail-vc0-f174.google.com with SMTP id ik5so1881230vcb.19
        for <dev@spark.apache.org>; Fri, 30 May 2014 06:38:50 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type:content-transfer-encoding;
        bh=jw6RnNmaO0WGQtIq4EqyhNq70FZtKs32J5dC7oTCiYY=;
        b=jmU4YsauHEMLrxNk4BOa0loATODN07qp/yoGxO4gBDuEFPgg64JtZfna1dxJ0/2gg8
         DR+nPgbvyoAgAngqfMZHdOMpY3Irkf9w/NsbviY5tTtyeLSxu0tapbswF6vylDRndERB
         bo0TTaTXLJDZJqpcCMdnyzzz8kScNbVbZy1W8NPAqeGV6tP6x7anQKXFZZVJz/FyIBZm
         HQPq5a1Aa0WIhwoljwHHKPtT0kZFwjD9gipFd11iTPBpEId8EM/+vpPbqtAAVeE8zCg/
         EVnIBxiC81SbyloCrFYnyexjHTbEyBYemCShNhQXs02bkPMzIfkS8RBCT+o+Cea4PbD9
         USzw==
X-Gm-Message-State: ALoCoQlXCx9HOWiDX/vVRqCW6xIcLnxZZmeoQiuQSP0MMTVyR5tvYFOVrYHnI79pGk7UPnO99a7D
X-Received: by 10.52.230.34 with SMTP id sv2mr1714300vdc.57.1401457130132;
 Fri, 30 May 2014 06:38:50 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.121.6 with HTTP; Fri, 30 May 2014 06:38:30 -0700 (PDT)
In-Reply-To: <FE2E914E3E2F4BBBA69E856154E92CC7@gmail.com>
References: <CAMAsSdKp8B03cdW3TkNSCyaHTnCtzznkaqRSYjqHQzH_yzPSzQ@mail.gmail.com>
 <FE2E914E3E2F4BBBA69E856154E92CC7@gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 30 May 2014 14:38:30 +0100
Message-ID: <CAMAsSdJhWG8RjE+mH0K1BfQOLwOmm-EuXgtSmJax+CNnRwQYdg@mail.gmail.com>
Subject: Re: Streaming example stops outputting (Java, Kafka at least)
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks Nan, that does appear to fix it. I was using "local". Can
anyone say whether that's to be expected or whether it could be a bug
somewhere?

On Fri, May 30, 2014 at 2:42 PM, Nan Zhu <zhunanmcgill@gmail.com> wrote:
> Hi, Sean
>
> I was in the same problem
>
> but when I changed MASTER=3D=E2=80=9Clocal=E2=80=9D to MASTER=3D=E2=80=9C=
local[2]=E2=80=9D
>
> everything back to the normal
>
> Hasn=E2=80=99t get a chance to ask here
>
> Best,
>
> --
> Nan Zhu
>

From dev-return-7899-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 16:20:39 2014
Return-Path: <dev-return-7899-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8F21E10C13
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 16:20:39 +0000 (UTC)
Received: (qmail 83731 invoked by uid 500); 30 May 2014 16:20:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83641 invoked by uid 500); 30 May 2014 16:20:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83623 invoked by uid 99); 30 May 2014 16:20:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 16:20:38 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of upentech@gmail.com designates 209.85.220.174 as permitted sender)
Received: from [209.85.220.174] (HELO mail-vc0-f174.google.com) (209.85.220.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 16:20:32 +0000
Received: by mail-vc0-f174.google.com with SMTP id ik5so2148729vcb.19
        for <multiple recipients>; Fri, 30 May 2014 09:20:11 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:cc:content-type;
        bh=Lj0uLvSMCjmdlOIBRY+jsTJO1WUIsAl1B5vOWYwhPtE=;
        b=oOWcxcAF3Uw5ahmiTBtXW5apAEAunrpsm849KcbowqnkfjeDdqoTWVOPlT6tZlxV4O
         1y4cSmVArfqSIZFn7ehUQwfYBTyQaldtX4XPdSoKz6wzmzpIbHJCUzkwKopTcKwvC3Op
         jIHsHVHZ2cmfXjFO0jGa/F1xWXIi8pLwni7YXxaECfz8kFbTYbeReo1aZkPbPPoJWA9U
         Jh+LVWWHwYN9nMLIKtaliifZSkwHtz9L0Mdd/j3Ndm0na/szTVZ+dkpRYGRfjnbt99Uy
         BwVX9UoBbZiic+dEPu/tbcmGi4YHyeESo9HARKT62MhjRPMFygs9AuUtHf0cnK3/3P+Q
         zxDw==
MIME-Version: 1.0
X-Received: by 10.220.249.6 with SMTP id mi6mr14860201vcb.33.1401466811837;
 Fri, 30 May 2014 09:20:11 -0700 (PDT)
Received: by 10.220.141.134 with HTTP; Fri, 30 May 2014 09:20:11 -0700 (PDT)
Date: Fri, 30 May 2014 12:20:11 -0400
Message-ID: <CANTbs9rHSTPTrkC--6RzGpsyPA8k-8UJFf3vuMY72aAANTG9FQ@mail.gmail.com>
Subject: Spark 1.0.0 - Java 8
From: Upender Nimbekar <upentech@gmail.com>
To: user@spark.apache.org
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e013a062c7bf2dd04faa06b7f
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013a062c7bf2dd04faa06b7f
Content-Type: text/plain; charset=UTF-8

Great News ! I've been awaiting this release to start doing some coding
with Spark using Java 8. Can I run Spark 1.0 examples on a virtual host
with 16 GB ram and fair descent amount of hard disk ? Or do I reaaly need
to use a cluster of machines.
Second, are there any good exmaples of using MLIB on Spark. Please shoot me
in the right direction.

Thanks
Upender

On Fri, May 30, 2014 at 6:12 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> I'm thrilled to announce the availability of Spark 1.0.0! Spark 1.0.0
> is a milestone release as the first in the 1.0 line of releases,
> providing API stability for Spark's core interfaces.
>
> Spark 1.0.0 is Spark's largest release ever, with contributions from
> 117 developers. I'd like to thank everyone involved in this release -
> it was truly a community effort with fixes, features, and
> optimizations contributed from dozens of organizations.
>
> This release expands Spark's standard libraries, introducing a new SQL
> package (SparkSQL) which lets users integrate SQL queries into
> existing Spark workflows. MLlib, Spark's machine learning library, is
> expanded with sparse vector support and several new algorithms. The
> GraphX and Streaming libraries also introduce new features and
> optimizations. Spark's core engine adds support for secured YARN
> clusters, a unified tool for submitting Spark applications, and
> several performance and stability improvements. Finally, Spark adds
> support for Java 8 lambda syntax and improves coverage of the Java and
> Python API's.
>
> Those features only scratch the surface - check out the release notes here:
> http://spark.apache.org/releases/spark-release-1-0-0.html
>
> Note that since release artifacts were posted recently, certain
> mirrors may not have working downloads for a few hours.
>
> - Patrick
>

--089e013a062c7bf2dd04faa06b7f--

From dev-return-7900-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 16:21:59 2014
Return-Path: <dev-return-7900-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1C1C410C1F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 16:21:59 +0000 (UTC)
Received: (qmail 87351 invoked by uid 500); 30 May 2014 16:21:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87292 invoked by uid 500); 30 May 2014 16:21:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87284 invoked by uid 99); 30 May 2014 16:21:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 16:21:58 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.48] (HELO mail-qg0-f48.google.com) (209.85.192.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 16:21:51 +0000
Received: by mail-qg0-f48.google.com with SMTP id i50so5991940qgf.35
        for <dev@spark.apache.org>; Fri, 30 May 2014 09:21:27 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=rKldESDuygUo/aRHSkxbCkvXrJMmwmxGwZYFCEXBKSE=;
        b=DvBOIg4j4tE0j9LQD6IcSQXCpDGYJI8Q0IInvOOTrupV+JRLti3UhU+xVFg6yND1vK
         k57P1ZfctWlLxVV6hFENZllCDTi9OOyh5p6rauzumI0QkszmL9JpdZy+YUl/30nlH07U
         GSiv2fLAasKULlhUPZlBEIbHYIfY5s8YpWreZHq05Jgtv+pSFz9uu3M75VTjSBNJN8pY
         QJvnSJ4NIQ4/Axi7zKZEN3NaR3sbkgqj1p+orLhH10c04XrkZpR4tnozC1NkrO8Zl21G
         85ToqS0GBoL0hewzfO2XLLZ0QADw/Y+KoqYSNJw+aWDqUKbddkmFYs1Dq+tsgdtmuogB
         rNPg==
X-Gm-Message-State: ALoCoQlnAUcEY8Q9Zaocg6tmwuQWT2kNRTJle4Na/UsEkmv0xpJywGBDsFmBw0SaejT8quGbu/Ce
X-Received: by 10.224.163.8 with SMTP id y8mr23091028qax.46.1401466887211;
        Fri, 30 May 2014 09:21:27 -0700 (PDT)
Received: from mail-qg0-f51.google.com (mail-qg0-f51.google.com [209.85.192.51])
        by mx.google.com with ESMTPSA id l61sm2726992qge.11.2014.05.30.09.21.26
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 30 May 2014 09:21:26 -0700 (PDT)
Received: by mail-qg0-f51.google.com with SMTP id q107so5893960qgd.38
        for <dev@spark.apache.org>; Fri, 30 May 2014 09:21:25 -0700 (PDT)
X-Received: by 10.140.21.239 with SMTP id 102mr21663997qgl.31.1401466885737;
 Fri, 30 May 2014 09:21:25 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.41.99 with HTTP; Fri, 30 May 2014 09:21:04 -0700 (PDT)
From: Andrew Ash <andrew@andrewash.com>
Date: Fri, 30 May 2014 09:21:04 -0700
Message-ID: <CA+-p3AFisXZQbKZFr-3jSTcJ28t1zDQWFmTukft9QZT-zFJiCA@mail.gmail.com>
Subject: bin/spark-shell --jars option
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c1190ee396f904faa06f8e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1190ee396f904faa06f8e
Content-Type: text/plain; charset=UTF-8

Hi Spark users,

In past Spark releases I always had to add jars to multiple places when
using the spark-shell, and I'm looking to cut down on those.  The --jars
option looks like it does what I want, but it doesn't work.  I did a quick
experiment on latest branch-1.0 and found this:

*# 0) jar not added anywhere*
./bin/spark-shell --master spark://aash-mbp.local:7077
spark> import org.joda.time.DateTime
[fails -- expected because the .jar isn't anywhere]

*# 1) just --jars*
./bin/spark-shell --master spark://aash-mbp.local:7077 --jars
/tmp/joda-time-2.3.jar
spark> import org.joda.time.DateTime
[fails -- but might work on non-standalone clusters?]

*# 2) using --jars and sc.addJar()*
./bin/spark-shell --master spark://aash-mbp.local:7077 --jars
/tmp/joda-time-2.3.jar
spark> sc.addJar("/tmp/joda-time-2.3.jar")
spark> import org.joda.time.DateTime
[fails -- shouldn't sc.addJar() make imports possible?]

*# 3) just --driver-class-path*
./bin/spark-shell --master spark://aash-mbp.local:7077 --driver-class-path
/tmp/joda-time-2.3.jar
spark> import org.joda.time.DateTime
spark> new DateTime()
res0: org.joda.time.DateTime = 2014-05-29T11:10:56.745-07:00
spark> sc.parallelize(1 to 10).map(k => new DateTime()).collect
[fails -- expected because jar wasn't ever sent to executors, only driver]

*# 4) using --driver-class-path and sc.addJar()*
./bin/spark-shell --master spark://aash-mbp.local:7077 --driver-class-path
/tmp/joda-time-2.3.jar
spark> import org.joda.time.DateTime
spark> sc.addJar("/tmp/joda-time-2.3.jar")
spark> new DateTime()
res0: org.joda.time.DateTime = 2014-05-29T11:10:56.745-07:00
spark> sc.parallelize(1 to 10).map(k => new DateTime()).collect
[success!]


Looking at the documentation for --jars, it looks like --jars doesn't work
with standalone in cluster deployment mode.  Here are the relevant doc
entries:

  --jars JARS                 A comma-separated list of local jars to
include on the
                              driver classpath and that SparkContext.addJar
will work
                              with. Doesn't work on standalone with
'cluster' deploy mode.

  --driver-class-path         Extra class path entries to pass to the
driver. Note that
                              jars added with --jars are automatically
included in the
                              classpath.


For the --jars comment about not working with standalone, is this something
that can be fixed to make the "1) just --jars" path above work?  Or is
there some larger architecture reason that --jars can't work with
standalone mode?

Appreciate it!
Andrew

--001a11c1190ee396f904faa06f8e--

From dev-return-7902-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 16:30:35 2014
Return-Path: <dev-return-7902-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6928010C97
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 16:30:35 +0000 (UTC)
Received: (qmail 22277 invoked by uid 500); 30 May 2014 16:30:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22229 invoked by uid 500); 30 May 2014 16:30:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22221 invoked by uid 99); 30 May 2014 16:30:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 16:30:34 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ilikerps@gmail.com designates 209.85.192.50 as permitted sender)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 16:30:29 +0000
Received: by mail-qg0-f50.google.com with SMTP id z60so6074163qgd.9
        for <dev@spark.apache.org>; Fri, 30 May 2014 09:30:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=+7j8i1cUNiGyIVADjs6FHtphWp0Yfmwx90P9S7qz6ao=;
        b=bx9prkxHBYNBfu+aR786o0r3EimyZMewZuooU3j0RpD1sue4a94BCAaDOgjg8MGPls
         jCBhZIY7GvIDuYZ6neTZjx05TtNvi82UjMaIJ+DHpggs9M7V4dOr3Jl4J4USTvG2bbQr
         AQ+YwXH6OKUjzLr9d4DczmbwhiZathyW95cTUJISqTLNNxTrkCrB/rDCMuLMxFPfY/Tm
         clRKu7Vk70grRiDHHF5B01sv1SnD4N6YmLvs3diUiJzK3uwiI8DeUVT+F7Fp0sjR2wiL
         Hd7ypWtXApGxK0OVH4KJtorKt2STfcptdLEpHOjCH3UhKzOVTt9PMY+42tfR+sWyFbi2
         SAxQ==
X-Received: by 10.140.93.132 with SMTP id d4mr21737530qge.1.1401467408602;
 Fri, 30 May 2014 09:30:08 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.31.35 with HTTP; Fri, 30 May 2014 09:29:48 -0700 (PDT)
In-Reply-To: <CALRHqP_3faQX26y0pAmpauAYY4Uo18gRmn0DPBE1aNK1mkMAeQ@mail.gmail.com>
References: <1401437552047-6871.post@n3.nabble.com> <CALRHqP_3faQX26y0pAmpauAYY4Uo18gRmn0DPBE1aNK1mkMAeQ@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Fri, 30 May 2014 09:29:48 -0700
Message-ID: <CANGvG8r-3a5LxzGF8yzWGvvUm7GJSAxf_3RrTuhuEG9os+RVPw@mail.gmail.com>
Subject: Re: Why does spark REPL not embed scala REPL?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113b9cde0e0f2c04faa08fee
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113b9cde0e0f2c04faa08fee
Content-Type: text/plain; charset=UTF-8

There's some discussion here as well on just using the Scala REPL for 2.11:
http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-on-Scala-2-11-td6506.html#a6523

Matei's response mentions the features we needed to change from the Scala
REPL (class-based wrappers and where to output the generated classes),
which were added as options to the 2.11 REPL, so we may be able to trim
down a bunch when 2.11 becomes standard.


On Fri, May 30, 2014 at 4:16 AM, Kan Zhang <kzhang@apache.org> wrote:

> One reason is standard Scala REPL uses object based wrappers and their
> static initializers will be run on remote worker nodes, which may fail due
> to differences between driver and worker nodes. See discussion here
> https://groups.google.com/d/msg/scala-internals/h27CFLoJXjE/JoobM6NiUMQJ
>
>
> On Fri, May 30, 2014 at 1:12 AM, Aniket <aniket.bhatnagar@gmail.com>
> wrote:
>
> > My apologies in advance if this is a dev mailing list topic. I am working
> > on
> > a small project to provide web interface to spark REPL. The interface
> will
> > allow people to use spark REPL and perform exploratory analysis on the
> > data.
> > I already have a play application running that provides web interface to
> > standard scala REPL and I am just looking to extend it to optionally
> > include
> > support for spark REPL. My initial idea was to include spark dependencies
> > in
> > the project, create a new instance of SparkContext and bind it to a
> > variable
> > (lets say 'sc') using imain.bind("sc", sparkContext). While theoretically
> > this may work, I am trying to understand why spark REPL takes a different
> > path by creating it's own SparkILoop, SparkIMain, etc. Can anyone help me
> > understand why there was a need to provide custom versions of IMain,
> ILoop,
> > etc instead of embedding the standard scala REPL and binding SparkContext
> > instance?
> >
> > Here is my analysis so far:
> > 1. ExecutorClassLoader - I understand this is need to load classes from
> > HDFS. Perhaps this could have been plugged into the standard scala REPL
> > using settings.embeddedDefaults(classLoaderInstance). Also, it's not
> clear
> > what ConstructorCleaner does.
> >
> > 2. SparkCommandLine & SparkRunnerSettings - Allow for providing an extra
> -i
> > file argument to the REPL. The standard sourcepath wouldn't have
> sufficed?
> >
> > 3. SparkExprTyper - The only difference between standard ExprTyper and
> > SparkExprTyper is that repldbg is replaced with logDebug. Not sure if
> this
> > was intentional/needed.
> >
> > 4. SparkILoop - Has a few deviations from standard ILoop class but this
> > could have been managed by extending or wrapping ILoop class or using
> > settings. Not sure what triggered the need to copy the source code and
> make
> > edits.
> >
> > 5. SparkILoopInit - Changes the welcome message and binds spark context
> in
> > the interpreter. Welcome message could have been changed by extending
> > ILoopInit.
> >
> > 6. SparkIMain - Contains quiet a few changes around class
> > loading/logging/etc but I found it very hard to figure out if extension
> of
> > IMain was an option and what exactly didn't work/will not work with
> IMain.
> >
> > Rest of the classes seem very similar to their standard counterparts. I
> > have
> > a feeling the spark REPL can be refactored to embed standard scala REPL.
> I
> > know refactoring would not help Spark project as such but would help
> people
> > embed the spark REPL much in the same way it's done with standard scala
> > REPL. Thoughts?
> >
> >
> >
> > --
> > View this message in context:
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Why-does-spark-REPL-not-embed-scala-REPL-tp6871.html
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> >
>

--001a113b9cde0e0f2c04faa08fee--

From dev-return-7901-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 16:32:09 2014
Return-Path: <dev-return-7901-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0A93F10CA9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 16:32:09 +0000 (UTC)
Received: (qmail 96980 invoked by uid 500); 30 May 2014 16:24:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96930 invoked by uid 500); 30 May 2014 16:24:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96899 invoked by uid 99); 30 May 2014 16:24:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 16:24:48 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of suren.hiraman@sociocast.com designates 209.85.220.173 as permitted sender)
Received: from [209.85.220.173] (HELO mail-vc0-f173.google.com) (209.85.220.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 16:24:45 +0000
Received: by mail-vc0-f173.google.com with SMTP id il7so2351135vcb.18
        for <dev@spark.apache.org>; Fri, 30 May 2014 09:24:22 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=xgchvw43FjaaPj61rQTJ+NJIrsAFdnGtW9Xw4+Bc1jQ=;
        b=KUBe0Koyt29h4W79dmRroYjE+9LC25Zm/GyDQP2V1htxkxKx5HIq0ewuxxxwK4dK9v
         yS5DHOvxdk7oDZpRZ4Nv8dNRmX0Z0RrTFSc/UYSSkmZP2QPhXHjGaVoxSaLQ3TARqFD4
         KIMc6PfffgeJmXees7gkH0cBjDDMrYwyvF3e6koQmtvuhMgvFnLRxN7ph8PUHXbUBFj3
         WxsQ0CPQSheJKk75iPOxybnJVmmp7dyOzYmoZa6kLGEmq5gD6l9REKT8fbZ0yqD8Vw3g
         qEXQE5rTWkaVTBZoap6i5NVfso95kAP8hezLqvkscO99GlurE07sEKhgEj8UoVdsVtou
         PdMA==
X-Gm-Message-State: ALoCoQkUy11RCgHsmJT85Be1ARqQzv7nyyM2ThVDuX1P2MTxDVFYfIrwqqrUM79S3GjH2PIluii1
MIME-Version: 1.0
X-Received: by 10.220.116.136 with SMTP id m8mr1800927vcq.77.1401467061982;
 Fri, 30 May 2014 09:24:21 -0700 (PDT)
Received: by 10.58.31.231 with HTTP; Fri, 30 May 2014 09:24:21 -0700 (PDT)
In-Reply-To: <CANTbs9rHSTPTrkC--6RzGpsyPA8k-8UJFf3vuMY72aAANTG9FQ@mail.gmail.com>
References: <CANTbs9rHSTPTrkC--6RzGpsyPA8k-8UJFf3vuMY72aAANTG9FQ@mail.gmail.com>
Date: Fri, 30 May 2014 12:24:21 -0400
Message-ID: <CALWDz_vtz55iMY_kACCDTkdE0vS=e5DX9FvLfb-AU9BzxnMMGg@mail.gmail.com>
Subject: Re: Spark 1.0.0 - Java 8
From: Surendranauth Hiraman <suren.hiraman@velos.io>
To: user@spark.apache.org
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b3439c264eb8b04faa07a0f
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3439c264eb8b04faa07a0f
Content-Type: text/plain; charset=UTF-8

With respect to virtual hosts, my team uses Vagrant/Virtualbox. We have 3
CentOS VMs with 4 GB RAM each - 2 worker nodes and a master node.

Everything works fine, though if you are using MapR, you have to make sure
they are all on the same subnet.

-Suren



On Fri, May 30, 2014 at 12:20 PM, Upender Nimbekar <upentech@gmail.com>
wrote:

> Great News ! I've been awaiting this release to start doing some coding
> with Spark using Java 8. Can I run Spark 1.0 examples on a virtual host
> with 16 GB ram and fair descent amount of hard disk ? Or do I reaaly need
> to use a cluster of machines.
> Second, are there any good exmaples of using MLIB on Spark. Please shoot
> me in the right direction.
>
> Thanks
> Upender
>
> On Fri, May 30, 2014 at 6:12 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
>> I'm thrilled to announce the availability of Spark 1.0.0! Spark 1.0.0
>> is a milestone release as the first in the 1.0 line of releases,
>> providing API stability for Spark's core interfaces.
>>
>> Spark 1.0.0 is Spark's largest release ever, with contributions from
>> 117 developers. I'd like to thank everyone involved in this release -
>> it was truly a community effort with fixes, features, and
>> optimizations contributed from dozens of organizations.
>>
>> This release expands Spark's standard libraries, introducing a new SQL
>> package (SparkSQL) which lets users integrate SQL queries into
>> existing Spark workflows. MLlib, Spark's machine learning library, is
>> expanded with sparse vector support and several new algorithms. The
>> GraphX and Streaming libraries also introduce new features and
>> optimizations. Spark's core engine adds support for secured YARN
>> clusters, a unified tool for submitting Spark applications, and
>> several performance and stability improvements. Finally, Spark adds
>> support for Java 8 lambda syntax and improves coverage of the Java and
>> Python API's.
>>
>> Those features only scratch the surface - check out the release notes
>> here:
>> http://spark.apache.org/releases/spark-release-1-0-0.html
>>
>> Note that since release artifacts were posted recently, certain
>> mirrors may not have working downloads for a few hours.
>>
>> - Patrick
>>
>
>


-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io

--047d7b3439c264eb8b04faa07a0f--

From dev-return-7903-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 17:54:00 2014
Return-Path: <dev-return-7903-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3672A10087
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 17:54:00 +0000 (UTC)
Received: (qmail 57436 invoked by uid 500); 30 May 2014 17:53:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57371 invoked by uid 500); 30 May 2014 17:53:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57363 invoked by uid 99); 30 May 2014 17:53:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 17:53:59 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.52 as permitted sender)
Received: from [209.85.219.52] (HELO mail-oa0-f52.google.com) (209.85.219.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 17:53:55 +0000
Received: by mail-oa0-f52.google.com with SMTP id eb12so2187753oac.39
        for <dev@spark.apache.org>; Fri, 30 May 2014 10:53:35 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=0o3nnFPAFli+LZg71DDv3vxdAitwwmaRDZCXQ3kF/cs=;
        b=nshPDRz89p8b8hae6yq0jaG2gRE22OZKX1K+iwxgrzLgK9NS6b66xmbnUUHw5IkWio
         RJJ75mnzf5XmSzMXot1xyLa1pgZro51XHZQgwfV7ZIMje1X+fbZB9Unfdeq4CtIOtgsg
         RcfRO4hrZgv7kSnFkjbQTiiQ9N8IiLxSrKLDLbfdd/0D2lavaJfWjMujMuwQxQ6Gev+F
         uTpZkw02KDKP0Nm+9Xu63/NOqBFhZBbU1xwBo7/eaMm3hXFlagDs8vmOhjIJ7w4PxeTD
         WZzGusvFJnmFFuhIXbGVlxBtVyra81PQEMcnafoHyk6CSqIwEiDCOsDYP7UkcDHEvOK0
         +sbQ==
MIME-Version: 1.0
X-Received: by 10.182.144.161 with SMTP id sn1mr19310967obb.82.1401472414962;
 Fri, 30 May 2014 10:53:34 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Fri, 30 May 2014 10:53:34 -0700 (PDT)
In-Reply-To: <CAMAsSdJhWG8RjE+mH0K1BfQOLwOmm-EuXgtSmJax+CNnRwQYdg@mail.gmail.com>
References: <CAMAsSdKp8B03cdW3TkNSCyaHTnCtzznkaqRSYjqHQzH_yzPSzQ@mail.gmail.com>
	<FE2E914E3E2F4BBBA69E856154E92CC7@gmail.com>
	<CAMAsSdJhWG8RjE+mH0K1BfQOLwOmm-EuXgtSmJax+CNnRwQYdg@mail.gmail.com>
Date: Fri, 30 May 2014 10:53:34 -0700
Message-ID: <CABPQxsv3u=foKdnZL8hPg8pWCUBWTJXJUZhXH57A2FO2tWbSmA@mail.gmail.com>
Subject: Re: Streaming example stops outputting (Java, Kafka at least)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah - Spark streaming needs at least two threads to run. I actually
thought we warned the user if they only use one (@tdas?) but the
warning might not be working correctly - or I'm misremembering.

On Fri, May 30, 2014 at 6:38 AM, Sean Owen <sowen@cloudera.com> wrote:
> Thanks Nan, that does appear to fix it. I was using "local". Can
> anyone say whether that's to be expected or whether it could be a bug
> somewhere?
>
> On Fri, May 30, 2014 at 2:42 PM, Nan Zhu <zhunanmcgill@gmail.com> wrote:
>> Hi, Sean
>>
>> I was in the same problem
>>
>> but when I changed MASTER="local" to MASTER="local[2]"
>>
>> everything back to the normal
>>
>> Hasn't get a chance to ask here
>>
>> Best,
>>
>> --
>> Nan Zhu
>>

From dev-return-7904-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 18:27:34 2014
Return-Path: <dev-return-7904-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6A3FD10250
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 18:27:34 +0000 (UTC)
Received: (qmail 48621 invoked by uid 500); 30 May 2014 18:27:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48564 invoked by uid 500); 30 May 2014 18:27:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48556 invoked by uid 99); 30 May 2014 18:27:34 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 18:27:34 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.192.47 as permitted sender)
Received: from [209.85.192.47] (HELO mail-qg0-f47.google.com) (209.85.192.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 18:27:31 +0000
Received: by mail-qg0-f47.google.com with SMTP id j107so6463549qga.34
        for <dev@spark.apache.org>; Fri, 30 May 2014 11:27:07 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=a+dz9T2AVeRUQefVBND7Bir7m67iF1OuKnU89oQofc4=;
        b=Rs68g4WipjQb27mG38sJda4gYco5Lqt7OP1fEMfQz2Zjp+iH14xk0hEyPVof25QraT
         tHXK0ozqVGJaIvLl7Jld0uIacU9RNk1BnNTPGbOMFufgRTGfU5W7TsJsPXvWhyn67qk0
         FxbGRzu3Noc0dhcWzyJ9tFSm3otub6u0RShuJIRFQ0FxLojC0vj9E6I4cM81xdZlhp35
         fcbLz2c94ea68h2imu9vGYOaMTf02RG6xVInA2yZbRxF4+ndhBr2OdXjGYdr7bZkZoe1
         KjNrHYbyh9W0Uw6fosg6tts9XRy061VyfHiC9ZQYim5XoaABEq/Q55tD0IHYE5Bn72j5
         hugA==
X-Received: by 10.224.161.83 with SMTP id q19mr24125490qax.56.1401474427026;
        Fri, 30 May 2014 11:27:07 -0700 (PDT)
Received: from [192.168.2.11] (MTRLPQ02-1177746539.sdsl.bell.ca. [70.50.252.107])
        by mx.google.com with ESMTPSA id 39sm2920031qgs.28.2014.05.30.11.27.06
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Fri, 30 May 2014 11:27:06 -0700 (PDT)
Date: Fri, 30 May 2014 14:35:35 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Message-ID: <35CBC40D368E4C1981BA133F2DD03A62@gmail.com>
In-Reply-To: <CABPQxsv3u=foKdnZL8hPg8pWCUBWTJXJUZhXH57A2FO2tWbSmA@mail.gmail.com>
References: <CAMAsSdKp8B03cdW3TkNSCyaHTnCtzznkaqRSYjqHQzH_yzPSzQ@mail.gmail.com>
 <FE2E914E3E2F4BBBA69E856154E92CC7@gmail.com>
 <CAMAsSdJhWG8RjE+mH0K1BfQOLwOmm-EuXgtSmJax+CNnRwQYdg@mail.gmail.com>
 <CABPQxsv3u=foKdnZL8hPg8pWCUBWTJXJUZhXH57A2FO2tWbSmA@mail.gmail.com>
Subject: Re: Streaming example stops outputting (Java, Kafka at least)
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="5388cf77_4ad3afd2_9a4"
X-Virus-Checked: Checked by ClamAV on apache.org

--5388cf77_4ad3afd2_9a4
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

If local[2] is expected, then the streaming doc is actually misleading? 

as the given example is 

import org.apache.spark.api.java.function._
import org.apache.spark.streaming._
import org.apache.spark.streaming.api._
// Create a StreamingContext with a local master
val ssc = new StreamingContext("local", "NetworkWordCount", Seconds(1))

http://spark.apache.org/docs/latest/streaming-programming-guide.html

I created a JIRA and a PR 

https://github.com/apache/spark/pull/924 

-- 
Nan Zhu


On Friday, May 30, 2014 at 1:53 PM, Patrick Wendell wrote:

> Yeah - Spark streaming needs at least two threads to run. I actually
> thought we warned the user if they only use one (@tdas?) but the
> warning might not be working correctly - or I'm misremembering.
> 
> On Fri, May 30, 2014 at 6:38 AM, Sean Owen <sowen@cloudera.com (mailto:sowen@cloudera.com)> wrote:
> > Thanks Nan, that does appear to fix it. I was using "local". Can
> > anyone say whether that's to be expected or whether it could be a bug
> > somewhere?
> > 
> > On Fri, May 30, 2014 at 2:42 PM, Nan Zhu <zhunanmcgill@gmail.com (mailto:zhunanmcgill@gmail.com)> wrote:
> > > Hi, Sean
> > > 
> > > I was in the same problem
> > > 
> > > but when I changed MASTER="local" to MASTER="local[2]"
> > > 
> > > everything back to the normal
> > > 
> > > Hasn't get a chance to ask here
> > > 
> > > Best,
> > > 
> > > --
> > > Nan Zhu
> > > 
> > 
> > 
> 
> 
> 



--5388cf77_4ad3afd2_9a4--


From dev-return-7905-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 19:05:44 2014
Return-Path: <dev-return-7905-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 90F6E104D8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 19:05:44 +0000 (UTC)
Received: (qmail 53980 invoked by uid 500); 30 May 2014 19:05:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53927 invoked by uid 500); 30 May 2014 19:05:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53919 invoked by uid 99); 30 May 2014 19:05:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 19:05:44 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rarecactus@gmail.com designates 209.85.212.180 as permitted sender)
Received: from [209.85.212.180] (HELO mail-wi0-f180.google.com) (209.85.212.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 19:05:39 +0000
Received: by mail-wi0-f180.google.com with SMTP id hi2so1681396wib.1
        for <dev@spark.apache.org>; Fri, 30 May 2014 12:05:14 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=tim6aHUoj8Wan7Km3MTIQPUP78xYTK7RVc3SrRUrzWE=;
        b=AYloLTk5PjWq2mLSv8+mOE4KhTVros/yUamGE3qf3vMw/RaYUJ5a7abwDEULl84fYA
         Q3NzsdEWx8OilPpAf2eO0SlmwJKs51/oLdt1QsO+XoGVhJX3J40LOFZMqLFDZEAC5xkR
         PNgqqQIEqVDkaBA2/Oo2rGDuyRPpd/5DrjTBlKIyaq5LDaf7eVwGBVxQf0v9WQDS82n3
         CIr//inJBtKRVGm+uxUs/MCKChVZJ/zTHxAPRpphfVdyWW6mFoZvBSNzhKwS3uFJOYmo
         DqRHO9rfXn6mOHkvLl5Aqo91aODbnAivUUlQv2Dt+pSMk1/Chmw0le0ToeaCL/0XzatL
         JcOg==
MIME-Version: 1.0
X-Received: by 10.180.85.163 with SMTP id i3mr9864417wiz.14.1401476713180;
 Fri, 30 May 2014 12:05:13 -0700 (PDT)
Sender: rarecactus@gmail.com
Received: by 10.194.242.35 with HTTP; Fri, 30 May 2014 12:05:13 -0700 (PDT)
In-Reply-To: <CABPQxst3M2r5aLur41JB+bjK9U2s9V0GSbrF45xTsQ9rS2d7gA@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
	<CA+-p3AEL6ct-bJwkBSXm+wTLBHEbBoGnnGOJGAjWHvQogDs-4g@mail.gmail.com>
	<CALRHqP-jLhJM3_5qCTWzrbU+GpmVsd2DFtvMXsH4-PGuT6X8ww@mail.gmail.com>
	<75094F23-AACD-4427-AE5B-A25AC02189A8@gmail.com>
	<CAJiQeY+Sy4EUKhM8ztsSfxaNoOG_73b=s7W_u5YRBPk-JegUDA@mail.gmail.com>
	<CAAsvFPm9OeOd7v2wsevLpKdqaCjfQuAA21r8itVu4-c2vft2OA@mail.gmail.com>
	<CAJiQeYKWeKoZuY9MjQTxzHn5d5XwvHwo0e8sEuEWL6CFW4p6CA@mail.gmail.com>
	<CAJiQeYKAbeAp7ZQK7bbxGF3YhfqeOOnt5-p7QO8n3g97f=YCKg@mail.gmail.com>
	<CABPQxst3M2r5aLur41JB+bjK9U2s9V0GSbrF45xTsQ9rS2d7gA@mail.gmail.com>
Date: Fri, 30 May 2014 12:05:13 -0700
X-Google-Sender-Auth: ogROo_e1bInfLWWrR1OGVTKl8v4
Message-ID: <CA+qbEUMAV=uZUCjAmgLMOjX3ccPcY=F6ade7TDtz5Lm92XJguA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Colin McCabe <cmccabe@alumni.cmu.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d0444eb09a6751704faa2b952
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0444eb09a6751704faa2b952
Content-Type: text/plain; charset=UTF-8

First of all, I think it's great that you're thinking about this.  API
stability is super important and it would be good to see Spark get on top
of this.

I want to clarify a bit about Hadoop.  The problem that Hadoop faces is
that the Java package system isn't very flexible.  If you have a method in,
say, the org.apache.hadoop.hdfs.shortcircuit package that should only ever
be used by the org.apache.hadoop.hdfs.client package, there is no way to
express that.  You have to make the method public.  You can hide things by
making them package-private, but that only works if your entire project is
a single giant package, and that is not the road Hadoop devs wanted to go
down.

So a lot of internal stuff ended up being public.  Once things are public,
of course, they can be called by anyone.  To get around this limitation,
Hadoop came up with a pretty rigorous compatibility policy, discussed here:
https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/InterfaceClassification.html
The basic idea is that we'd put "interface annotations" on every public
class.  The "Private" annotation meant that it was only supposed to be used
in the project itself.  "Limited-Private" was kind of the project and maybe
one or two closely related projects.  And "Public" was supposed to be the
public API.  At a finer granularity, for specific public methods, you could
add the "VisibileForTesting" annotation to indicate that they were only
visible to make a unit test possible.

This sounds great in theory.  But in practice, users often ignore the
annotation and just do whatever they want.  This is not because they're
mustache-twirling villains, but because they have legitimate (to them)
reasons.  For example, HBase would often find that they could get better
performance by hooking into supposedly private HDFS APIs.  Of course, they
could always ask HDFS to add public versions of those APIs.  But that takes
time, and could be contentious.  In the best case, they'd have to wait for
another Hadoop release to happen before HBase could benefit.  From their
perspective, supporting the feature on more Hadoop releases was better than
supporting it on fewer, even if the latter was the "correct" way of doing
things.  Then of course there were the cases where there were simple
oversights... there either was no interface annotation or the user of the
downstream project forgot to check it.

Ideally, we'd later add a @stable API and transition everyone to it.  But
that's much easier said than done.  A lot of projects just don't want to
change, because it would mean giving up compatibility with older releases
without the "blessed" API.  Basically, it's a tragedy of the commons.  It
would be much better for everyone if we all used public stable APIs and
never used private or unstable ones.  But each individual project feels
that it can get advantages by cheating and using (or continuing to use) the
private / unstable APIs.  Candidly, Spark is one of those projects that
continues to use deprecated and private Hadoop APIs-- mostly for
compatibility reasons, as I understand.

I think that the lesson learned here is that the compiler needs to be in
charge of preventing people from using APIs, not an annotation.
 Public/private annotations "Just Don't Work."  I don't know if Scala
provides any mechanisms to do this beyond what Java provides.  Even if not,
there are probably classloader and CLASSPATH tricks that could be used to
hide internals.  I also think that it makes sense to put a lot of thought
into APIs up front, because changing them later can be very painful.  On a
related note, there were definitely cases where Hadoop changed an API, and
the pain outweighed the gain.

There are other dimensions to compatibility... for example, Hadoop
currently leaks its CLASSPATH, so that you can't easily write a MapReduce
job without using the same versions of Guava (just to pick one random
example) that it does.  In practice, this led to a pathological fear of
updating dependencies, since we didn't want to break users who needed
specific version of their deps.  Does Spark also expose its CLASSPATH in
this way to executors?  I was under the impression that it did.

At some point we will also have to confront the Scala version issue.  Will
there be flag days where Spark jobs need to be upgraded to a new,
incompatible version of Scala to run on the latest Spark?  There are pros
and cons, but I think users will mostly see the cons.

On Thu, May 29, 2014 at 1:23 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> 1. Hadoop projects don't do any rigorous checking that new patches
> don't break API's. Of course, the results in regular API breaks and a
> poor understanding of what is a public API.
>

I agree with this.  We should test these compatibility scenarios, and we
don't.  It would be awesome to do this in an automated way for Spark.


> 2. In several cases it's not possible to do basic things in Hadoop
> without using deprecated or private API's.
>

Disagree.  The problem is that we have stable APIs, but users don't want to
use them (they prefer the ancient API Doug Cutting wrote in 2008, because
it works on some old version of Hadoop).  It's hard to argue against this
kind of reasoning, since (to reiterate) it's rational from the point of
view of the individual.  This is the problem with deprecation in general--
once you've let an API out into the wild, it's very difficult to get it
back into its cage.

3. There is significant vendor fragmentation of API's.
>

The big difference in the last few years was that some people were creating
distributions based on Hadoop 1.x and others were creating distributions
based on 2.x.  But nobody added vendor specific APIs (or at least I haven't
heard of any).  (I can't speak for MapR... since they are proprietary, I
have not seen the code.)  Now that Hadoop 1.x is starting to die a natural
death, any differences between 2.x and 1.x are becoming less important.
 Sadly, Yahoo continues to use and develop 0.23, for now at least... But I
think their efforts are mostly directed at backporting.  They have not
added divergent APIs, to my knowledge.

best,
Colin


The main focus of the Hadoop vendors is making consistent cuts of the
> core projects work together (HDFS/Pig/Hive/etc) - so API breaks are
> sometimes considered "fixed" as long as the other projects work around
> them (see [1]). We also regularly need to do archaeology (see [2]) and
> directly interact with Hadoop committers to understand what API's are
> stable and in which versions.
>
> One goal of Spark is to deal with the pain of inter-operating with
> Hadoop so that application writers don't to. We'd like to retain the
> property that if you build an application against the (well defined,
> stable) Spark API's right now, you'll be able to run it across many
> Hadoop vendors and versions for the entire Spark 1.X release cycle.
>
> Writing apps against Hadoop can be very difficult... consider how much
> more engineering effort we spent maintaining YARN support than Mesos
> support. There are many factors, but one is that Mesos has a single,
> narrow, stable API. We've never had to make a change in Mesos due to
> an API change, for several years. YARN on the other hand, there are at
> least 3 YARN API's that currently exist, which are all binary
> incompatible. We'd like to offer apps the ability to build against
> Spark's API and just let us deal with it.
>
> As more vendors packaging Spark, I'd like to see us put tools in the
> upstream Spark repo that do validation for vendor packages of Spark,
> so that we don't end up with fragmentation. Of course, vendors can
> enhance the API and are encouraged to, but we need a kernel of API's
> that vendors must maintain (think POSIX) to be considered compliant
> with Apache Spark. I believe some other projects like OpenStack have
> done this to avoid fragmentation.
>
> - Patrick
>
> [1] https://issues.apache.org/jira/browse/MAPREDUCE-5830
> [2]
> http://2.bp.blogspot.com/-GO6HF0OAFHw/UOfNEH-4sEI/AAAAAAAAAD0/dEWFFYTRgYw/s1600/output-file.png
>
> On Sun, May 18, 2014 at 2:13 AM, Mridul Muralidharan <mridul@gmail.com>
> wrote:
> > So I think I need to clarify a few things here - particularly since
> > this mail went to the wrong mailing list and a much wider audience
> > than I intended it for :-)
> >
> >
> > Most of the issues I mentioned are internal implementation detail of
> > spark core : which means, we can enhance them in future without
> > disruption to our userbase (ability to support large number of
> > input/output partitions. Note: this is of order of 100k input and
> > output partitions with uniform spread of keys - very rarely seen
> > outside of some crazy jobs).
> >
> > Some of the issues I mentioned would reqiure DeveloperApi changes -
> > which are not user exposed : they would impact developer use of these
> > api's - which are mostly internally provided by spark. (Like fixing
> > blocks > 2G would require change to Serializer api)
> >
> > A smaller faction might require interface changes - note, I am
> > referring specifically to configuration changes (removing/deprecating
> > some) and possibly newer options to submit/env, etc - I dont envision
> > any programming api change itself.
> > The only api change we did was from Seq -> Iterable - which is
> > actually to address some of the issues I mentioned (join/cogroup).
> >
> > Remaining are bugs which need to be addressed or the feature
> > removed/enhanced like shuffle consolidation.
> >
> > There might be semantic extension of some things like OFF_HEAP storage
> > level to address other computation models - but that would not have an
> > impact on end user - since other options would be pluggable with
> > default set to Tachyon so that there is no user expectation change.
> >
> >
> > So will the interface possibly change ? Sure though we will try to
> > keep it backwardly compatible (as we did with 1.0).
> > Will the api change - other than backward compatible enhancements,
> probably not.
> >
> >
> > Regards,
> > Mridul
> >
> >
> > On Sun, May 18, 2014 at 12:11 PM, Mridul Muralidharan <mridul@gmail.com>
> wrote:
> >>
> >> On 18-May-2014 5:05 am, "Mark Hamstra" <mark@clearstorydata.com> wrote:
> >>>
> >>> I don't understand.  We never said that interfaces wouldn't change from
> >>> 0.9
> >>
> >> Agreed.
> >>
> >>> to 1.0.  What we are committing to is stability going forward from the
> >>> 1.0.0 baseline.  Nobody is disputing that backward-incompatible
> behavior
> >>> or
> >>> interface changes would be an issue post-1.0.0.  The question is
> whether
> >>
> >> The point is, how confident are we that these are the right set of
> interface
> >> definitions.
> >> We think it is, but we could also have gone through a 0.10 to vet the
> >> proposed 1.0 changes to stabilize them.
> >>
> >> To give examples for which we don't have solutions currently (which we
> are
> >> facing internally here btw, so not academic exercise) :
> >>
> >> - Current spark shuffle model breaks very badly as number of partitions
> >> increases (input and output).
> >>
> >> - As number of nodes increase, the overhead per node keeps going up.
> Spark
> >> currently is more geared towards large memory machines; when the RAM per
> >> node is modest (8 to 16 gig) but large number of them are available, it
> does
> >> not do too well.
> >>
> >> - Current block abstraction breaks as data per block goes beyond 2 gig.
> >>
> >> - Cogroup/join when value per key or number of keys (or both) is high
> breaks
> >> currently.
> >>
> >> - Shuffle consolidation is so badly broken it is not funny.
> >>
> >> - Currently there is no way of effectively leveraging accelerator
> >> cards/coprocessors/gpus from spark - to do so, I suspect we will need to
> >> redefine OFF_HEAP.
> >>
> >> - Effectively leveraging ssd is still an open question IMO when you
> have mix
> >> of both available.
> >>
> >> We have resolved some of these and looking at the rest. These are not
> unique
> >> to our internal usage profile, I have seen most of these asked elsewhere
> >> too.
> >>
> >> Thankfully some of the 1.0 changes actually are geared towards helping
> to
> >> alleviate some of the above (Iterable change for ex), most of the rest
> are
> >> internal impl detail of spark core which helps a lot - but there are
> cases
> >> where this is not so.
> >>
> >> Unfortunately I don't know yet if the unresolved/uninvestigated issues
> will
> >> require more changes or not.
> >>
> >> Given this I am very skeptical of expecting current spark interfaces to
> be
> >> sufficient for next 1 year (forget 3)
> >>
> >> I understand this is an argument which can be made to never release 1.0
> :-)
> >> Which is why I was ok with a 1.0 instead of 0.10 release in spite of my
> >> preference.
> >>
> >> This is a good problem to have IMO ... People are using spark
> extensively
> >> and in circumstances that we did not envision : necessitating changes
> even
> >> to spark core.
> >>
> >> But the claim that 1.0 interfaces are stable is not something I buy -
> they
> >> are not, we will need to break them soon and cost of maintaining
> backward
> >> compatibility will be high.
> >>
> >> We just need to make an informed decision to live with that cost, not
> hand
> >> wave it away.
> >>
> >> Regards
> >> Mridul
> >>
> >>> there is anything apparent now that is expected to require such
> disruptive
> >>> changes if we were to commit to the current release candidate as our
> >>> guaranteed 1.0.0 baseline.
> >>>
> >>>
> >>> On Sat, May 17, 2014 at 2:05 PM, Mridul Muralidharan
> >>> <mridul@gmail.com>wrote:
> >>>
> >>> > I would make the case for interface stability not just api stability.
> >>> > Particularly given that we have significantly changed some of our
> >>> > interfaces, I want to ensure developers/users are not seeing red
> flags.
> >>> >
> >>> > Bugs and code stability can be addressed in minor releases if found,
> but
> >>> > behavioral change and/or interface changes would be a much more
> invasive
> >>> > issue for our users.
> >>> >
> >>> > Regards
> >>> > Mridul
> >>> > On 18-May-2014 2:19 am, "Matei Zaharia" <matei.zaharia@gmail.com>
> wrote:
> >>> >
> >>> > > As others have said, the 1.0 milestone is about API stability, not
> >>> > > about
> >>> > > saying "we've eliminated all bugs". The sooner you declare 1.0, the
> >>> > sooner
> >>> > > users can confidently build on Spark, knowing that the application
> >>> > > they
> >>> > > build today will still run on Spark 1.9.9 three years from now.
> This
> >>> > > is
> >>> > > something that I've seen done badly (and experienced the effects
> >>> > > thereof)
> >>> > > in other big data projects, such as MapReduce and even YARN. The
> >>> > > result
> >>> > is
> >>> > > that you annoy users, you end up with a fragmented userbase where
> >>> > everyone
> >>> > > is building against a different version, and you drastically slow
> down
> >>> > > development.
> >>> > >
> >>> > > With a project as fast-growing as fast-growing as Spark in
> particular,
> >>> > > there will be new bugs discovered and reported continuously,
> >>> > > especially
> >>> > in
> >>> > > the non-core components. Look at the graph of # of contributors in
> >>> > > time
> >>> > to
> >>> > > Spark: https://www.ohloh.net/p/apache-spark (bottom-most graph;
> >>> > "commits"
> >>> > > changed when we started merging each patch as a single commit).
> This
> >>> > > is
> >>> > not
> >>> > > slowing down, and we need to have the culture now that we treat API
> >>> > > stability and release numbers at the level expected for a 1.0
> project
> >>> > > instead of having people come in and randomly change the API.
> >>> > >
> >>> > > I'll also note that the issues marked "blocker" were marked so by
> >>> > > their
> >>> > > reporters, since the reporter can set the priority. I don't
> consider
> >>> > stuff
> >>> > > like parallelize() not partitioning ranges in the same way as other
> >>> > > collections a blocker -- it's a bug, it would be good to fix it,
> but it
> >>> > only
> >>> > > affects a small number of use cases. Of course if we find a real
> >>> > > blocker
> >>> > > (in particular a regression from a previous version, or a feature
> >>> > > that's
> >>> > > just completely broken), we will delay the release for that, but at
> >>> > > some
> >>> > > point you have to say "okay, this fix will go into the next
> >>> > > maintenance
> >>> > > release". Maybe we need to write a clear policy for what the issue
> >>> > > priorities mean.
> >>> > >
> >>> > > Finally, I believe it's much better to have a culture where you can
> >>> > > make
> >>> > > releases on a regular schedule, and have the option to make a
> >>> > > maintenance
> >>> > > release in 3-4 days if you find new bugs, than one where you pile
> up
> >>> > stuff
> >>> > > into each release. This is what much large project than us, like
> >>> > > Linux,
> >>> > do,
> >>> > > and it's the only way to avoid indefinite stalling with a large
> >>> > contributor
> >>> > > base. In the worst case, if you find a new bug that warrants
> immediate
> >>> > > release, it goes into 1.0.1 a week after 1.0.0 (we can vote on
> 1.0.1
> >>> > > in
> >>> > > three days with just your bug fix in it). And if you find an API
> that
> >>> > you'd
> >>> > > like to improve, just add a new one and maybe deprecate the old
> one --
> >>> > > at
> >>> > > some point we have to respect our users and let them know that code
> >>> > > they
> >>> > > write today will still run tomorrow.
> >>> > >
> >>> > > Matei
> >>> > >
> >>> > > On May 17, 2014, at 10:32 AM, Kan Zhang <kzhang@apache.org> wrote:
> >>> > >
> >>> > > > +1 on the running commentary here, non-binding of course :-)
> >>> > > >
> >>> > > >
> >>> > > > On Sat, May 17, 2014 at 8:44 AM, Andrew Ash <
> andrew@andrewash.com>
> >>> > > wrote:
> >>> > > >
> >>> > > >> +1 on the next release feeling more like a 0.10 than a 1.0
> >>> > > >> On May 17, 2014 4:38 AM, "Mridul Muralidharan" <
> mridul@gmail.com>
> >>> > > wrote:
> >>> > > >>
> >>> > > >>> I had echoed similar sentiments a while back when there was a
> >>> > > discussion
> >>> > > >>> around 0.10 vs 1.0 ... I would have preferred 0.10 to stabilize
> >>> > > >>> the
> >>> > api
> >>> > > >>> changes, add missing functionality, go through a hardening
> release
> >>> > > before
> >>> > > >>> 1.0
> >>> > > >>>
> >>> > > >>> But the community preferred a 1.0 :-)
> >>> > > >>>
> >>> > > >>> Regards,
> >>> > > >>> Mridul
> >>> > > >>>
> >>> > > >>> On 17-May-2014 3:19 pm, "Sean Owen" <sowen@cloudera.com>
> wrote:
> >>> > > >>>>
> >>> > > >>>> On this note, non-binding commentary:
> >>> > > >>>>
> >>> > > >>>> Releases happen in local minima of change, usually created by
> >>> > > >>>> internally enforced code freeze. Spark is incredibly busy now
> due
> >>> > > >>>> to
> >>> > > >>>> external factors -- recently a TLP, recently discovered by a
> >>> > > >>>> large
> >>> > new
> >>> > > >>>> audience, ease of contribution enabled by Github. It's getting
> >>> > > >>>> like
> >>> > > >>>> the first year of mainstream battle-testing in a month. It's
> been
> >>> > very
> >>> > > >>>> hard to freeze anything! I see a number of non-trivial issues
> >>> > > >>>> being
> >>> > > >>>> reported, and I don't think it has been possible to triage
> all of
> >>> > > >>>> them, even.
> >>> > > >>>>
> >>> > > >>>> Given the high rate of change, my instinct would have been to
> >>> > release
> >>> > > >>>> 0.10.0 now. But won't it always be very busy? I do think the
> rate
> >>> > > >>>> of
> >>> > > >>>> significant issues will slow down.
> >>> > > >>>>
> >>> > > >>>> Version ain't nothing but a number, but if it has any meaning
> >>> > > >>>> it's
> >>> > the
> >>> > > >>>> semantic versioning meaning. 1.0 imposes extra handicaps
> around
> >>> > > >>>> striving to maintain backwards-compatibility. That may end up
> >>> > > >>>> being
> >>> > > >>>> bent to fit in important changes that are going to be
> required in
> >>> > this
> >>> > > >>>> continuing period of change. Hadoop does this all the time
> >>> > > >>>> unfortunately and gets away with it, I suppose -- minor
> version
> >>> > > >>>> releases are really major. (On the other extreme, HBase is at
> >>> > > >>>> 0.98
> >>> > and
> >>> > > >>>> quite production-ready.)
> >>> > > >>>>
> >>> > > >>>> Just consider this a second vote for focus on fixes and 1.0.x
> >>> > > >>>> rather
> >>> > > >>>> than new features and 1.x. I think there are a few steps that
> >>> > > >>>> could
> >>> > > >>>> streamline triage of this flood of contributions, and make
> all of
> >>> > this
> >>> > > >>>> easier, but that's for another thread.
> >>> > > >>>>
> >>> > > >>>>
> >>> > > >>>> On Fri, May 16, 2014 at 8:50 PM, Mark Hamstra <
> >>> > > mark@clearstorydata.com
> >>> > > >>>
> >>> > > >>> wrote:
> >>> > > >>>>> +1, but just barely.  We've got quite a number of outstanding
> >>> > > >>>>> bugs
> >>> > > >>>>> identified, and many of them have fixes in progress.  I'd
> hate
> >>> > > >>>>> to
> >>> > see
> >>> > > >>> those
> >>> > > >>>>> efforts get lost in a post-1.0.0 flood of new features
> targeted
> >>> > > >>>>> at
> >>> > > >>> 1.1.0 --
> >>> > > >>>>> in other words, I'd like to see 1.0.1 retain a high priority
> >>> > relative
> >>> > > >>> to
> >>> > > >>>>> 1.1.0.
> >>> > > >>>>>
> >>> > > >>>>> Looking through the unresolved JIRAs, it doesn't look like
> any
> >>> > > >>>>> of
> >>> > the
> >>> > > >>>>> identified bugs are show-stoppers or strictly regressions
> >>> > (although I
> >>> > > >>> will
> >>> > > >>>>> note that one that I have in progress, SPARK-1749, is a bug
> that
> >>> > > >>>>> we
> >>> > > >>>>> introduced with recent work -- it's not strictly a regression
> >>> > because
> >>> > > >>> we
> >>> > > >>>>> had equally bad but different behavior when the DAGScheduler
> >>> > > >> exceptions
> >>> > > >>>>> weren't previously being handled at all vs. being slightly
> >>> > > >> mis-handled
> >>> > > >>>>> now), so I'm not currently seeing a reason not to release.
> >>> > > >>>
> >>> > > >>
> >>> > >
> >>> > >
> >>> >
>

--f46d0444eb09a6751704faa2b952--

From dev-return-7906-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 19:30:56 2014
Return-Path: <dev-return-7906-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4F9A810636
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 19:30:56 +0000 (UTC)
Received: (qmail 21094 invoked by uid 500); 30 May 2014 19:30:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21032 invoked by uid 500); 30 May 2014 19:30:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21024 invoked by uid 99); 30 May 2014 19:30:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 19:30:55 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of vanzin@cloudera.com designates 209.85.192.41 as permitted sender)
Received: from [209.85.192.41] (HELO mail-qg0-f41.google.com) (209.85.192.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 19:30:50 +0000
Received: by mail-qg0-f41.google.com with SMTP id j5so6773247qga.0
        for <dev@spark.apache.org>; Fri, 30 May 2014 12:30:30 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=Cn7t+C95QQqmRcVhkrwwCcVFHrF2YXlMZSqXMzNRGTE=;
        b=fF8JX2yYwvLNaDYopxH2xdZPhaOQXB3mNBAJ9v6X4b6IwrWrTt/Q6YOKWqLSp0yEQa
         Y2eYPPR1NlqDixzcaLW4tac5HhVmft05c/cAn8ZPXIt+Vn1aNBFQwHubI8aR3tq1ll+U
         i4nDbrvc0T1EImMNZRk1Pi5GYPYr7qmLRrG1865aRNfIxhXbNHT869Lzp8BWQOYsAocs
         ZIThRQrqUC04Zj7LCNUxpeyg8tbTiyr0UI3jc2PCkW+55VAzfNhGEa527u46MEB0nNXM
         CCFGjvatPQIbvhV4km0pyeZLLVtXrjfarAY6gbeS7+GEu+fAPUFgraC+fvIzTYReg3GM
         JMiQ==
X-Gm-Message-State: ALoCoQkzbHKKDnhJwQ27bTqZcbDJHQyN0weGS9NkgmU913r+ZDuQHfsOyq6q4szHCP4DvUFQxK84
MIME-Version: 1.0
X-Received: by 10.140.27.23 with SMTP id 23mr22968793qgw.94.1401478230083;
 Fri, 30 May 2014 12:30:30 -0700 (PDT)
Received: by 10.229.55.6 with HTTP; Fri, 30 May 2014 12:30:29 -0700 (PDT)
In-Reply-To: <CA+qbEUMAV=uZUCjAmgLMOjX3ccPcY=F6ade7TDtz5Lm92XJguA@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
	<CA+-p3AEL6ct-bJwkBSXm+wTLBHEbBoGnnGOJGAjWHvQogDs-4g@mail.gmail.com>
	<CALRHqP-jLhJM3_5qCTWzrbU+GpmVsd2DFtvMXsH4-PGuT6X8ww@mail.gmail.com>
	<75094F23-AACD-4427-AE5B-A25AC02189A8@gmail.com>
	<CAJiQeY+Sy4EUKhM8ztsSfxaNoOG_73b=s7W_u5YRBPk-JegUDA@mail.gmail.com>
	<CAAsvFPm9OeOd7v2wsevLpKdqaCjfQuAA21r8itVu4-c2vft2OA@mail.gmail.com>
	<CAJiQeYKWeKoZuY9MjQTxzHn5d5XwvHwo0e8sEuEWL6CFW4p6CA@mail.gmail.com>
	<CAJiQeYKAbeAp7ZQK7bbxGF3YhfqeOOnt5-p7QO8n3g97f=YCKg@mail.gmail.com>
	<CABPQxst3M2r5aLur41JB+bjK9U2s9V0GSbrF45xTsQ9rS2d7gA@mail.gmail.com>
	<CA+qbEUMAV=uZUCjAmgLMOjX3ccPcY=F6ade7TDtz5Lm92XJguA@mail.gmail.com>
Date: Fri, 30 May 2014 12:30:29 -0700
Message-ID: <CAAOnQ7vZGCQVYxZGS1+g5_2ZJkKxucVg2F-AaaeMYFruEq6VWQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Marcelo Vanzin <vanzin@cloudera.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Fri, May 30, 2014 at 12:05 PM, Colin McCabe <cmccabe@alumni.cmu.edu> wrote:
> I don't know if Scala provides any mechanisms to do this beyond what Java provides.

In fact it does. You can say something like "private[foo]" and the
annotated element will be visible for all classes under "foo" (where
"foo" is any package in the hierarchy leading up to the class). That's
used a lot in Spark.

I haven't fully looked at how the @DeveloperApi is used, but I agree
with you - annotations are not a good way to do this. The Scala
feature above would be much better, but it might still leak things at
the Java bytecode level (don't know how Scala implements it under the
cover, but I assume it's not by declaring the element as a Java
"private").

Another thing is that in Scala the default visibility is public, which
makes it very easy to inadvertently add things to the API. I'd like to
see more care in making things have the proper visibility - I
generally declare things private first, and relax that as needed.
Using @VisibleForTesting would be great too, when the Scala
private[foo] approach doesn't work.

> Does Spark also expose its CLASSPATH in
> this way to executors?  I was under the impression that it did.

If you're using the Spark assemblies, yes, there is a lot of things
that your app gets exposed to. For example, you can see Guava and
Jetty (and many other things) there. This is something that has always
bugged me, but I don't really have a good suggestion of how to fix it;
shading goes a certain way, but it also breaks codes that uses
reflection (e.g. Class.forName()-style class loading).

What is worse is that Spark doesn't even agree with the Hadoop code it
depends on; e.g., Spark uses Guava 14.x while Hadoop is still in Guava
11.x. So when you run your Scala app, what gets loaded?

> At some point we will also have to confront the Scala version issue.  Will
> there be flag days where Spark jobs need to be upgraded to a new,
> incompatible version of Scala to run on the latest Spark?

Yes, this could be an issue - I'm not sure Scala has a policy towards
this, but updates (at least minor, e.g. 2.9 -> 2.10) tend to break
binary compatibility.

Scala also makes some API updates tricky - e.g., adding a new named
argument to a Scala method is not a binary compatible change (while,
e.g., adding a new keyword argument in a python method is just fine).
The use of implicits and other Scala features make this even more
opaque...

Anyway, not really any solutions in this message, just a few comments
I wanted to throw out there. :-)

-- 
Marcelo

From dev-return-7907-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 21:12:08 2014
Return-Path: <dev-return-7907-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 98F9910AB1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 21:12:08 +0000 (UTC)
Received: (qmail 50196 invoked by uid 500); 30 May 2014 21:12:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50141 invoked by uid 500); 30 May 2014 21:12:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50133 invoked by uid 99); 30 May 2014 21:12:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 21:12:03 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.45 as permitted sender)
Received: from [209.85.219.45] (HELO mail-oa0-f45.google.com) (209.85.219.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 21:12:00 +0000
Received: by mail-oa0-f45.google.com with SMTP id l6so2473230oag.32
        for <dev@spark.apache.org>; Fri, 30 May 2014 14:11:36 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=AO2v7/6T7vBQWRo28M5QUbhPsdDur5KGoHctH6msVfI=;
        b=vTrqYgjiaRa2Fm8UdyRh07MeyArysx20lFKVjxi1VFywN2putTO/+d0/avPOZOamv4
         PPGOko+Ntnn67vMDX4VI91hsWOfynS1g3clgeqKiMNe6wuWwe+C4Xho+Lj6bSsCmGGE+
         feXn0PBsrQUyxZfPPb+HIHnXeUsyjv60Afk3xBjWQcYyeL1sX7aWQIKlIxiF55SBGUIW
         FauEEGvsFPe1ac/P2SHOpbb6BS6hbgr+q8JRjhOpbBc5FxKqHuQmlGwwpc7fI8aHoWzd
         5qZR/6Eu5cxTRinv30o6MhM9x9h6QcM2FI1ZXBNtnLCNI4q39akoDmgra80sJVpPFKzN
         8s2w==
MIME-Version: 1.0
X-Received: by 10.182.3.36 with SMTP id 4mr20782881obz.56.1401484296462; Fri,
 30 May 2014 14:11:36 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Fri, 30 May 2014 14:11:36 -0700 (PDT)
In-Reply-To: <CAAOnQ7vZGCQVYxZGS1+g5_2ZJkKxucVg2F-AaaeMYFruEq6VWQ@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
	<CA+-p3AEL6ct-bJwkBSXm+wTLBHEbBoGnnGOJGAjWHvQogDs-4g@mail.gmail.com>
	<CALRHqP-jLhJM3_5qCTWzrbU+GpmVsd2DFtvMXsH4-PGuT6X8ww@mail.gmail.com>
	<75094F23-AACD-4427-AE5B-A25AC02189A8@gmail.com>
	<CAJiQeY+Sy4EUKhM8ztsSfxaNoOG_73b=s7W_u5YRBPk-JegUDA@mail.gmail.com>
	<CAAsvFPm9OeOd7v2wsevLpKdqaCjfQuAA21r8itVu4-c2vft2OA@mail.gmail.com>
	<CAJiQeYKWeKoZuY9MjQTxzHn5d5XwvHwo0e8sEuEWL6CFW4p6CA@mail.gmail.com>
	<CAJiQeYKAbeAp7ZQK7bbxGF3YhfqeOOnt5-p7QO8n3g97f=YCKg@mail.gmail.com>
	<CABPQxst3M2r5aLur41JB+bjK9U2s9V0GSbrF45xTsQ9rS2d7gA@mail.gmail.com>
	<CA+qbEUMAV=uZUCjAmgLMOjX3ccPcY=F6ade7TDtz5Lm92XJguA@mail.gmail.com>
	<CAAOnQ7vZGCQVYxZGS1+g5_2ZJkKxucVg2F-AaaeMYFruEq6VWQ@mail.gmail.com>
Date: Fri, 30 May 2014 14:11:36 -0700
Message-ID: <CABPQxsshMrxNrwwhQNiN4aRzSgLK_S558Y7_2cgLANVsz0ez_w@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey guys, thanks for the insights. Also, I realize Hadoop has gotten
way better about this with 2.2+ and I think it's great progress.

We have well defined API levels in Spark and also automated checking
of API violations for new pull requests. When doing code reviews we
always enforce the narrowest possible visibility:

1. private
2. private[spark]
3. @Experimental or @DeveloperApi
4. public

Our automated checks exclude 1-3. Anything that breaks 4 will trigger
a build failure.

The Scala compiler prevents anyone external from using 1 or 2. We do
have "bytecode public but annotated" (3) API's that we might change.
We spent a lot of time looking into whether these can offer compiler
warnings, but we haven't found a way to do this and do not see a
better alternative at this point.

Regarding Scala compatibility, Scala 2.11+ is "source code
compatible", meaning we'll be able to cross-compile Spark for
different versions of Scala. We've already been in touch with Typesafe
about this and they've offered to integrate Spark into their
compatibility test suite. They've also committed to patching 2.11 with
a minor release if bugs are found.

Anyways, my point is we've actually thought a lot about this already.

The CLASSPATH thing is different than API stability, but indeed also a
form of compatibility. This is something where I'd also like to see
Spark have better isolation of user classes from Spark's own
execution...

- Patrick



On Fri, May 30, 2014 at 12:30 PM, Marcelo Vanzin <vanzin@cloudera.com> wrote:
> On Fri, May 30, 2014 at 12:05 PM, Colin McCabe <cmccabe@alumni.cmu.edu> wrote:
>> I don't know if Scala provides any mechanisms to do this beyond what Java provides.
>
> In fact it does. You can say something like "private[foo]" and the
> annotated element will be visible for all classes under "foo" (where
> "foo" is any package in the hierarchy leading up to the class). That's
> used a lot in Spark.
>
> I haven't fully looked at how the @DeveloperApi is used, but I agree
> with you - annotations are not a good way to do this. The Scala
> feature above would be much better, but it might still leak things at
> the Java bytecode level (don't know how Scala implements it under the
> cover, but I assume it's not by declaring the element as a Java
> "private").
>
> Another thing is that in Scala the default visibility is public, which
> makes it very easy to inadvertently add things to the API. I'd like to
> see more care in making things have the proper visibility - I
> generally declare things private first, and relax that as needed.
> Using @VisibleForTesting would be great too, when the Scala
> private[foo] approach doesn't work.
>
>> Does Spark also expose its CLASSPATH in
>> this way to executors?  I was under the impression that it did.
>
> If you're using the Spark assemblies, yes, there is a lot of things
> that your app gets exposed to. For example, you can see Guava and
> Jetty (and many other things) there. This is something that has always
> bugged me, but I don't really have a good suggestion of how to fix it;
> shading goes a certain way, but it also breaks codes that uses
> reflection (e.g. Class.forName()-style class loading).
>
> What is worse is that Spark doesn't even agree with the Hadoop code it
> depends on; e.g., Spark uses Guava 14.x while Hadoop is still in Guava
> 11.x. So when you run your Scala app, what gets loaded?
>
>> At some point we will also have to confront the Scala version issue.  Will
>> there be flag days where Spark jobs need to be upgraded to a new,
>> incompatible version of Scala to run on the latest Spark?
>
> Yes, this could be an issue - I'm not sure Scala has a policy towards
> this, but updates (at least minor, e.g. 2.9 -> 2.10) tend to break
> binary compatibility.
>
> Scala also makes some API updates tricky - e.g., adding a new named
> argument to a Scala method is not a binary compatible change (while,
> e.g., adding a new keyword argument in a python method is just fine).
> The use of implicits and other Scala features make this even more
> opaque...
>
> Anyway, not really any solutions in this message, just a few comments
> I wanted to throw out there. :-)
>
> --
> Marcelo

From dev-return-7908-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 21:36:30 2014
Return-Path: <dev-return-7908-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9DD1010B81
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 21:36:30 +0000 (UTC)
Received: (qmail 92915 invoked by uid 500); 30 May 2014 21:36:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92861 invoked by uid 500); 30 May 2014 21:36:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92853 invoked by uid 99); 30 May 2014 21:36:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 21:36:30 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vanzin@cloudera.com designates 209.85.220.175 as permitted sender)
Received: from [209.85.220.175] (HELO mail-vc0-f175.google.com) (209.85.220.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 21:36:26 +0000
Received: by mail-vc0-f175.google.com with SMTP id id10so2761563vcb.20
        for <dev@spark.apache.org>; Fri, 30 May 2014 14:36:02 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=rEm3k9T5TykNvtnSOxmBLkMliMkzBcfwIq/j1ib2XKQ=;
        b=nHSIwbt2AMJZnJpUzUrnpaJktztlZ9gVGL0AHaMtq6Le7DVmK2jAwlRS8IHmISp0RP
         H6ectYrFiYqWlSVz1TC4Wgy5S/M5z++L73DRBWQnHe74w7XyZ2J96fOxxXiOVEw5qLk8
         9j+YkvpO6FSY8SvPODBs3I00CYet3iLj4xc7LAPp0hemLpHBwaBV597APWjZp8boyp0L
         EorMlRW8+b64wCDSaA2ncGZAncYsooedoVd8zvLGnoDUUYZ3U4SHWgz7HOGB+iSR61E6
         MJ8UpM5g29AOlj327Zx59uMhap/tZkAnvXxu34Yi1Y5y3MBNST6YFdJyAuD5j70zmz9z
         UvRQ==
X-Gm-Message-State: ALoCoQnZ+Ratk3Ed71m6Dbrz5jx2A2CAMFVvD7c9qT0DjYCdfODsOw/pWzm9enXUJl37E1VjDS94
MIME-Version: 1.0
X-Received: by 10.58.160.134 with SMTP id xk6mr3973091veb.64.1401485762906;
 Fri, 30 May 2014 14:36:02 -0700 (PDT)
Received: by 10.58.209.135 with HTTP; Fri, 30 May 2014 14:36:02 -0700 (PDT)
In-Reply-To: <CABPQxsshMrxNrwwhQNiN4aRzSgLK_S558Y7_2cgLANVsz0ez_w@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
	<CA+-p3AEL6ct-bJwkBSXm+wTLBHEbBoGnnGOJGAjWHvQogDs-4g@mail.gmail.com>
	<CALRHqP-jLhJM3_5qCTWzrbU+GpmVsd2DFtvMXsH4-PGuT6X8ww@mail.gmail.com>
	<75094F23-AACD-4427-AE5B-A25AC02189A8@gmail.com>
	<CAJiQeY+Sy4EUKhM8ztsSfxaNoOG_73b=s7W_u5YRBPk-JegUDA@mail.gmail.com>
	<CAAsvFPm9OeOd7v2wsevLpKdqaCjfQuAA21r8itVu4-c2vft2OA@mail.gmail.com>
	<CAJiQeYKWeKoZuY9MjQTxzHn5d5XwvHwo0e8sEuEWL6CFW4p6CA@mail.gmail.com>
	<CAJiQeYKAbeAp7ZQK7bbxGF3YhfqeOOnt5-p7QO8n3g97f=YCKg@mail.gmail.com>
	<CABPQxst3M2r5aLur41JB+bjK9U2s9V0GSbrF45xTsQ9rS2d7gA@mail.gmail.com>
	<CA+qbEUMAV=uZUCjAmgLMOjX3ccPcY=F6ade7TDtz5Lm92XJguA@mail.gmail.com>
	<CAAOnQ7vZGCQVYxZGS1+g5_2ZJkKxucVg2F-AaaeMYFruEq6VWQ@mail.gmail.com>
	<CABPQxsshMrxNrwwhQNiN4aRzSgLK_S558Y7_2cgLANVsz0ez_w@mail.gmail.com>
Date: Fri, 30 May 2014 14:36:02 -0700
Message-ID: <CAAOnQ7v2XJ0zstZJ+QeB0LUbKXjn-qLdsrtvp+1TeaRcK=Sdiw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Marcelo Vanzin <vanzin@cloudera.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Patrick,

On Fri, May 30, 2014 at 2:11 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> 2. private[spark]
> 3. @Experimental or @DeveloperApi

I understand @Experimental, but when would you use @DeveloperApi
instead of private[spark]? Seems to me that, for the API user, they
both mean very similar, if not exactly the same, thing. And the second
is actually more user-friendly since the compiler will yell at you.

Who's the "Developer" that the annotation refers to?

-- 
Marcelo

From dev-return-7909-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri May 30 21:57:16 2014
Return-Path: <dev-return-7909-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 760B110C19
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 30 May 2014 21:57:16 +0000 (UTC)
Received: (qmail 38534 invoked by uid 500); 30 May 2014 21:57:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38490 invoked by uid 500); 30 May 2014 21:57:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38482 invoked by uid 99); 30 May 2014 21:57:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 21:57:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rarecactus@gmail.com designates 74.125.82.169 as permitted sender)
Received: from [74.125.82.169] (HELO mail-we0-f169.google.com) (74.125.82.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 30 May 2014 21:57:12 +0000
Received: by mail-we0-f169.google.com with SMTP id u56so2663445wes.28
        for <dev@spark.apache.org>; Fri, 30 May 2014 14:56:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=hAF0542toAHMn/qNodMBNrquh8inYwnN3GRYoG9okDs=;
        b=PYllk4/oFR3FlXR0OHuwV4c0mAZYBcIfXIGZmUyFIstowt/LznhgMNpLy8Ltt8fJtC
         cN+GoO3Dw+ARu7AdFxhwX5e2MiHpe90iy9qomrcIi+OBWa9DsGsSyxN4syAMyidNQPrY
         8ZgYFxGrk3JhMzxH8E0MV9o3iuHN4LB7Yg1i7Ysn3Q+Eo1q1fQFhxL6OxGe1j7QAWeTs
         MkUTVGpzRerEfJzRSm/lpLh3+PN0YVckCrCG4hryLrMa79WlSrRpAt9uxDVWnmFE5UG8
         AJ3mJfz+h5O3Sc+LbbGhBvEVacy1QcSb0N6+90HjJLzACRM1tjCfv/XxuoD0yBPQNH7r
         Uc7A==
MIME-Version: 1.0
X-Received: by 10.180.85.163 with SMTP id i3mr612271wiz.14.1401487009410; Fri,
 30 May 2014 14:56:49 -0700 (PDT)
Sender: rarecactus@gmail.com
Received: by 10.194.242.35 with HTTP; Fri, 30 May 2014 14:56:49 -0700 (PDT)
In-Reply-To: <CABPQxsshMrxNrwwhQNiN4aRzSgLK_S558Y7_2cgLANVsz0ez_w@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
	<CA+-p3AEL6ct-bJwkBSXm+wTLBHEbBoGnnGOJGAjWHvQogDs-4g@mail.gmail.com>
	<CALRHqP-jLhJM3_5qCTWzrbU+GpmVsd2DFtvMXsH4-PGuT6X8ww@mail.gmail.com>
	<75094F23-AACD-4427-AE5B-A25AC02189A8@gmail.com>
	<CAJiQeY+Sy4EUKhM8ztsSfxaNoOG_73b=s7W_u5YRBPk-JegUDA@mail.gmail.com>
	<CAAsvFPm9OeOd7v2wsevLpKdqaCjfQuAA21r8itVu4-c2vft2OA@mail.gmail.com>
	<CAJiQeYKWeKoZuY9MjQTxzHn5d5XwvHwo0e8sEuEWL6CFW4p6CA@mail.gmail.com>
	<CAJiQeYKAbeAp7ZQK7bbxGF3YhfqeOOnt5-p7QO8n3g97f=YCKg@mail.gmail.com>
	<CABPQxst3M2r5aLur41JB+bjK9U2s9V0GSbrF45xTsQ9rS2d7gA@mail.gmail.com>
	<CA+qbEUMAV=uZUCjAmgLMOjX3ccPcY=F6ade7TDtz5Lm92XJguA@mail.gmail.com>
	<CAAOnQ7vZGCQVYxZGS1+g5_2ZJkKxucVg2F-AaaeMYFruEq6VWQ@mail.gmail.com>
	<CABPQxsshMrxNrwwhQNiN4aRzSgLK_S558Y7_2cgLANVsz0ez_w@mail.gmail.com>
Date: Fri, 30 May 2014 14:56:49 -0700
X-Google-Sender-Auth: KPF0-nmcpnZCn5rqhrNKXOLGpqA
Message-ID: <CA+qbEUNUj_Z-ssG0p8UL9yvyBQeZCdjifB2_M3PJE-E=QHLFtw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Colin McCabe <cmccabe@alumni.cmu.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d0444eb095a757a04faa51fdd
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0444eb095a757a04faa51fdd
Content-Type: text/plain; charset=UTF-8

On Fri, May 30, 2014 at 2:11 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Hey guys, thanks for the insights. Also, I realize Hadoop has gotten
> way better about this with 2.2+ and I think it's great progress.
>
> We have well defined API levels in Spark and also automated checking
> of API violations for new pull requests. When doing code reviews we
> always enforce the narrowest possible visibility:
>
> 1. private
> 2. private[spark]
> 3. @Experimental or @DeveloperApi
> 4. public
>
> Our automated checks exclude 1-3. Anything that breaks 4 will trigger
> a build failure.
>
>
That's really excellent.  Great job.

I like the private[spark] visibility level-- sounds like this is another
way Scala has greatly improved on Java.

The Scala compiler prevents anyone external from using 1 or 2. We do
> have "bytecode public but annotated" (3) API's that we might change.
> We spent a lot of time looking into whether these can offer compiler
> warnings, but we haven't found a way to do this and do not see a
> better alternative at this point.
>

It would be nice if the production build could strip this stuff out.
 Otherwise, it feels a lot like a @private, @unstable Hadoop API... and we
know how those turned out.


> Regarding Scala compatibility, Scala 2.11+ is "source code
> compatible", meaning we'll be able to cross-compile Spark for
> different versions of Scala. We've already been in touch with Typesafe
> about this and they've offered to integrate Spark into their
> compatibility test suite. They've also committed to patching 2.11 with
> a minor release if bugs are found.
>

Thanks, I hadn't heard about this plan.  Hopefully we can get everyone on
2.11 ASAP.


> Anyways, my point is we've actually thought a lot about this already.
>
> The CLASSPATH thing is different than API stability, but indeed also a
> form of compatibility. This is something where I'd also like to see
> Spark have better isolation of user classes from Spark's own
> execution...
>
>
I think the best thing to do is just "shade" all the dependencies.  Then
they will be in a different namespace, and clients can have their own
versions of whatever dependencies they like without conflicting.  As
Marcelo mentioned, there might be a few edge cases where this breaks
reflection, but I don't think that's an issue for most libraries.  So at
worst case we could end up needing apps to follow us in lockstep for Kryo
or maybe Akka, but not the whole kit and caboodle like with Hadoop.

best,
Colin


- Patrick
>
>
>
> On Fri, May 30, 2014 at 12:30 PM, Marcelo Vanzin <vanzin@cloudera.com>
> wrote:
> > On Fri, May 30, 2014 at 12:05 PM, Colin McCabe <cmccabe@alumni.cmu.edu>
> wrote:
> >> I don't know if Scala provides any mechanisms to do this beyond what
> Java provides.
> >
> > In fact it does. You can say something like "private[foo]" and the
> > annotated element will be visible for all classes under "foo" (where
> > "foo" is any package in the hierarchy leading up to the class). That's
> > used a lot in Spark.
> >
> > I haven't fully looked at how the @DeveloperApi is used, but I agree
> > with you - annotations are not a good way to do this. The Scala
> > feature above would be much better, but it might still leak things at
> > the Java bytecode level (don't know how Scala implements it under the
> > cover, but I assume it's not by declaring the element as a Java
> > "private").
> >
> > Another thing is that in Scala the default visibility is public, which
> > makes it very easy to inadvertently add things to the API. I'd like to
> > see more care in making things have the proper visibility - I
> > generally declare things private first, and relax that as needed.
> > Using @VisibleForTesting would be great too, when the Scala
> > private[foo] approach doesn't work.
> >
> >> Does Spark also expose its CLASSPATH in
> >> this way to executors?  I was under the impression that it did.
> >
> > If you're using the Spark assemblies, yes, there is a lot of things
> > that your app gets exposed to. For example, you can see Guava and
> > Jetty (and many other things) there. This is something that has always
> > bugged me, but I don't really have a good suggestion of how to fix it;
> > shading goes a certain way, but it also breaks codes that uses
> > reflection (e.g. Class.forName()-style class loading).
> >
> > What is worse is that Spark doesn't even agree with the Hadoop code it
> > depends on; e.g., Spark uses Guava 14.x while Hadoop is still in Guava
> > 11.x. So when you run your Scala app, what gets loaded?
> >
> >> At some point we will also have to confront the Scala version issue.
>  Will
> >> there be flag days where Spark jobs need to be upgraded to a new,
> >> incompatible version of Scala to run on the latest Spark?
> >
> > Yes, this could be an issue - I'm not sure Scala has a policy towards
> > this, but updates (at least minor, e.g. 2.9 -> 2.10) tend to break
> > binary compatibility.
> >
> > Scala also makes some API updates tricky - e.g., adding a new named
> > argument to a Scala method is not a binary compatible change (while,
> > e.g., adding a new keyword argument in a python method is just fine).
> > The use of implicits and other Scala features make this even more
> > opaque...
> >
> > Anyway, not really any solutions in this message, just a few comments
> > I wanted to throw out there. :-)
> >
> > --
> > Marcelo
>

--f46d0444eb095a757a04faa51fdd--

From dev-return-7910-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 31 05:55:12 2014
Return-Path: <dev-return-7910-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A664110716
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 31 May 2014 05:55:12 +0000 (UTC)
Received: (qmail 4183 invoked by uid 500); 31 May 2014 05:55:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4119 invoked by uid 500); 31 May 2014 05:55:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4111 invoked by uid 99); 31 May 2014 05:55:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 May 2014 05:55:12 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.54 as permitted sender)
Received: from [209.85.219.54] (HELO mail-oa0-f54.google.com) (209.85.219.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 May 2014 05:55:09 +0000
Received: by mail-oa0-f54.google.com with SMTP id j17so2761629oag.13
        for <dev@spark.apache.org>; Fri, 30 May 2014 22:54:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=B+3AmsPFBTG7y/wMRgrKbqEOskaDRMhShTHnmMLdSyY=;
        b=K/jeWs8j6hoAm7SxKQtyGRdaAuUurYBICOr3/U7f0eY2wIpHLdEE9D1jAgRtI1FUin
         PyOrkr7fLuD/8kpSY2J0CphKqo2I5mr1ZFOPmQT2rg/ew5wqFEI3JJ/FDHLCdXvpLv0i
         RorpLq9lBzE/1XCN4rNYiKkjaKo/2dJmdS/VcmzYCnl8EcQfP1OswvIGIkB/1P4SXEvp
         cQDeEPxO9DF+8PsFdRKUMCvxg8oTGF4nwco+oiySIZIzLASVH9F0kUSzmMCVBb1MkmpN
         WVMb/Vc4gnBsuu09CLGs37ocKx4ILd4TV4bQGhhub77f7ECj+AsY5FDb637VzX/EUn1/
         XOyQ==
MIME-Version: 1.0
X-Received: by 10.60.44.243 with SMTP id h19mr22714060oem.46.1401515684448;
 Fri, 30 May 2014 22:54:44 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Fri, 30 May 2014 22:54:44 -0700 (PDT)
In-Reply-To: <CA+qbEUNUj_Z-ssG0p8UL9yvyBQeZCdjifB2_M3PJE-E=QHLFtw@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
	<CA+-p3AEL6ct-bJwkBSXm+wTLBHEbBoGnnGOJGAjWHvQogDs-4g@mail.gmail.com>
	<CALRHqP-jLhJM3_5qCTWzrbU+GpmVsd2DFtvMXsH4-PGuT6X8ww@mail.gmail.com>
	<75094F23-AACD-4427-AE5B-A25AC02189A8@gmail.com>
	<CAJiQeY+Sy4EUKhM8ztsSfxaNoOG_73b=s7W_u5YRBPk-JegUDA@mail.gmail.com>
	<CAAsvFPm9OeOd7v2wsevLpKdqaCjfQuAA21r8itVu4-c2vft2OA@mail.gmail.com>
	<CAJiQeYKWeKoZuY9MjQTxzHn5d5XwvHwo0e8sEuEWL6CFW4p6CA@mail.gmail.com>
	<CAJiQeYKAbeAp7ZQK7bbxGF3YhfqeOOnt5-p7QO8n3g97f=YCKg@mail.gmail.com>
	<CABPQxst3M2r5aLur41JB+bjK9U2s9V0GSbrF45xTsQ9rS2d7gA@mail.gmail.com>
	<CA+qbEUMAV=uZUCjAmgLMOjX3ccPcY=F6ade7TDtz5Lm92XJguA@mail.gmail.com>
	<CAAOnQ7vZGCQVYxZGS1+g5_2ZJkKxucVg2F-AaaeMYFruEq6VWQ@mail.gmail.com>
	<CABPQxsshMrxNrwwhQNiN4aRzSgLK_S558Y7_2cgLANVsz0ez_w@mail.gmail.com>
	<CA+qbEUNUj_Z-ssG0p8UL9yvyBQeZCdjifB2_M3PJE-E=QHLFtw@mail.gmail.com>
Date: Fri, 30 May 2014 22:54:44 -0700
Message-ID: <CABPQxsuwLYpca1aX__Ceq6tCcpkh_be38iZ6WmTa+=59+aTV+Q@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Spark is a bit different than Hadoop MapReduce, so maybe that's a
source of some confusion. Spark is often used as a substrate for
building different types of analytics applications, so @DeveloperAPI
are internal API's that we'd like to expose to application writers,
but that might be more volatile. This is like the internal API's in
the linux kernel, they aren't stable, but of course we try to minimize
changes to them. If people want to write lower-level modules against
them, that's fine with us, but they know the interfaces might change.

This has worked pretty well over the years, even with many different
companies writing against those API's.

@Experimental are user-facing features we are trying out. Hopefully
that one is more clear.

In terms of making a big jar that shades all of our dependencies - I'm
curious how that would actually work in practice. It would be good to
explore. There are a few potential challenges I see:

1. If any of our dependencies encode class name information in IPC
messages, this would break. E.g. can you definitely shade the Hadoop
client, protobuf, hbase client, etc and have them send messages over
the wire? This could break things if class names are ever encoded in a
wire format.
2. Many libraries like logging subsystems, configuration systems, etc
rely on static state and initialization. I'm not totally sure how e.g.
slf4j initializes itself if you have both a shaded and non-shaded copy
of slf4j present.
3. This would mean the spark-core jar would be really massive because
it would inline all of our deps. We've actually been thinking of
avoiding the current assembly jar approach because, due to scala
specialized classes, our assemblies now have more than 65,000 class
files in them leading to all kinds of bad issues. We'd have to stick
with a big uber assembly-like jar if we decide to shade stuff.
4. I'm not totally sure how this would work when people want to e.g.
build Spark with different Hadoop versions. Would we publish different
shaded uber-jars for every Hadoop version? Would the Hadoop dep just
not be shaded... if so what about all it's dependencies.

Anyways just some things to consider... simplifying our classpath is
definitely an avenue worth exploring!




On Fri, May 30, 2014 at 2:56 PM, Colin McCabe <cmccabe@alumni.cmu.edu> wrote:
> On Fri, May 30, 2014 at 2:11 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>
>> Hey guys, thanks for the insights. Also, I realize Hadoop has gotten
>> way better about this with 2.2+ and I think it's great progress.
>>
>> We have well defined API levels in Spark and also automated checking
>> of API violations for new pull requests. When doing code reviews we
>> always enforce the narrowest possible visibility:
>>
>> 1. private
>> 2. private[spark]
>> 3. @Experimental or @DeveloperApi
>> 4. public
>>
>> Our automated checks exclude 1-3. Anything that breaks 4 will trigger
>> a build failure.
>>
>>
> That's really excellent.  Great job.
>
> I like the private[spark] visibility level-- sounds like this is another
> way Scala has greatly improved on Java.
>
> The Scala compiler prevents anyone external from using 1 or 2. We do
>> have "bytecode public but annotated" (3) API's that we might change.
>> We spent a lot of time looking into whether these can offer compiler
>> warnings, but we haven't found a way to do this and do not see a
>> better alternative at this point.
>>
>
> It would be nice if the production build could strip this stuff out.
>  Otherwise, it feels a lot like a @private, @unstable Hadoop API... and we
> know how those turned out.
>
>
>> Regarding Scala compatibility, Scala 2.11+ is "source code
>> compatible", meaning we'll be able to cross-compile Spark for
>> different versions of Scala. We've already been in touch with Typesafe
>> about this and they've offered to integrate Spark into their
>> compatibility test suite. They've also committed to patching 2.11 with
>> a minor release if bugs are found.
>>
>
> Thanks, I hadn't heard about this plan.  Hopefully we can get everyone on
> 2.11 ASAP.
>
>
>> Anyways, my point is we've actually thought a lot about this already.
>>
>> The CLASSPATH thing is different than API stability, but indeed also a
>> form of compatibility. This is something where I'd also like to see
>> Spark have better isolation of user classes from Spark's own
>> execution...
>>
>>
> I think the best thing to do is just "shade" all the dependencies.  Then
> they will be in a different namespace, and clients can have their own
> versions of whatever dependencies they like without conflicting.  As
> Marcelo mentioned, there might be a few edge cases where this breaks
> reflection, but I don't think that's an issue for most libraries.  So at
> worst case we could end up needing apps to follow us in lockstep for Kryo
> or maybe Akka, but not the whole kit and caboodle like with Hadoop.
>
> best,
> Colin
>
>
> - Patrick
>>
>>
>>
>> On Fri, May 30, 2014 at 12:30 PM, Marcelo Vanzin <vanzin@cloudera.com>
>> wrote:
>> > On Fri, May 30, 2014 at 12:05 PM, Colin McCabe <cmccabe@alumni.cmu.edu>
>> wrote:
>> >> I don't know if Scala provides any mechanisms to do this beyond what
>> Java provides.
>> >
>> > In fact it does. You can say something like "private[foo]" and the
>> > annotated element will be visible for all classes under "foo" (where
>> > "foo" is any package in the hierarchy leading up to the class). That's
>> > used a lot in Spark.
>> >
>> > I haven't fully looked at how the @DeveloperApi is used, but I agree
>> > with you - annotations are not a good way to do this. The Scala
>> > feature above would be much better, but it might still leak things at
>> > the Java bytecode level (don't know how Scala implements it under the
>> > cover, but I assume it's not by declaring the element as a Java
>> > "private").
>> >
>> > Another thing is that in Scala the default visibility is public, which
>> > makes it very easy to inadvertently add things to the API. I'd like to
>> > see more care in making things have the proper visibility - I
>> > generally declare things private first, and relax that as needed.
>> > Using @VisibleForTesting would be great too, when the Scala
>> > private[foo] approach doesn't work.
>> >
>> >> Does Spark also expose its CLASSPATH in
>> >> this way to executors?  I was under the impression that it did.
>> >
>> > If you're using the Spark assemblies, yes, there is a lot of things
>> > that your app gets exposed to. For example, you can see Guava and
>> > Jetty (and many other things) there. This is something that has always
>> > bugged me, but I don't really have a good suggestion of how to fix it;
>> > shading goes a certain way, but it also breaks codes that uses
>> > reflection (e.g. Class.forName()-style class loading).
>> >
>> > What is worse is that Spark doesn't even agree with the Hadoop code it
>> > depends on; e.g., Spark uses Guava 14.x while Hadoop is still in Guava
>> > 11.x. So when you run your Scala app, what gets loaded?
>> >
>> >> At some point we will also have to confront the Scala version issue.
>>  Will
>> >> there be flag days where Spark jobs need to be upgraded to a new,
>> >> incompatible version of Scala to run on the latest Spark?
>> >
>> > Yes, this could be an issue - I'm not sure Scala has a policy towards
>> > this, but updates (at least minor, e.g. 2.9 -> 2.10) tend to break
>> > binary compatibility.
>> >
>> > Scala also makes some API updates tricky - e.g., adding a new named
>> > argument to a Scala method is not a binary compatible change (while,
>> > e.g., adding a new keyword argument in a python method is just fine).
>> > The use of implicits and other Scala features make this even more
>> > opaque...
>> >
>> > Anyway, not really any solutions in this message, just a few comments
>> > I wanted to throw out there. :-)
>> >
>> > --
>> > Marcelo
>>

From dev-return-7911-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 31 07:17:47 2014
Return-Path: <dev-return-7911-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6E51210870
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 31 May 2014 07:17:47 +0000 (UTC)
Received: (qmail 72753 invoked by uid 500); 31 May 2014 07:17:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72689 invoked by uid 500); 31 May 2014 07:17:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72681 invoked by uid 99); 31 May 2014 07:17:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 May 2014 07:17:46 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mayur.rustagi@gmail.com designates 74.125.82.41 as permitted sender)
Received: from [74.125.82.41] (HELO mail-wg0-f41.google.com) (74.125.82.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 May 2014 07:17:43 +0000
Received: by mail-wg0-f41.google.com with SMTP id z12so2953136wgg.24
        for <dev@spark.apache.org>; Sat, 31 May 2014 00:17:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=L+hgT2LMo7H7tt09R4Gxno7v/bekigO8wYhaNjRV3U8=;
        b=vmCKxZ4ul1D2FSvpPkMirNtgWp5WL2gP2+N8tVXttHZQ/wBfMfk9UidvUMxjY0/rZb
         kStnf5J5CvCbWZjeS28HChe4uLqfHLkIvqAg9bi3yk07eE2gxLLRfzznbsrycMxMDPwt
         JLpDyYfx0PT04b0rSzH7sPp/qqIRtjGq5fk31dtJs+uGjYCB6kmi3E9gdw31mXoPHlra
         saczwCzvMwbqsOf/B/N8OIa73rtm3rdX7ffOmXTYGNnSwXuRhFxvmDLY7pOXnTlDNlGP
         Di2SKTikJRbVPB/+y0xmFPuKHxDc0cccPXEfQ3CRoLalR2zuym+2Ev87/bX2HKlEph3R
         7xuA==
X-Received: by 10.180.96.225 with SMTP id dv1mr3573670wib.37.1401520641934;
 Sat, 31 May 2014 00:17:21 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.195.17.165 with HTTP; Sat, 31 May 2014 00:17:01 -0700 (PDT)
In-Reply-To: <CABPQxsuBqZuZ5WHqf5+Wv8UXB-1Xf=1MmH8mxkCehD3YbMfMAQ@mail.gmail.com>
References: <CAMseSVMwoqAZiYkCVWkq6aYM3o5rkAhqOr8d=9YoWCtnkh-JTw@mail.gmail.com>
 <CABPQxsuBqZuZ5WHqf5+Wv8UXB-1Xf=1MmH8mxkCehD3YbMfMAQ@mail.gmail.com>
From: Mayur Rustagi <mayur.rustagi@gmail.com>
Date: Sat, 31 May 2014 12:47:01 +0530
Message-ID: <CAAqHKj5T9nUvffhNgyqZKPeB4UajduG-B2tp4Za89gVkaq9+hw@mail.gmail.com>
Subject: Fwd: Monitoring / Instrumenting jobs in 1.0
To: dev <dev@spark.apache.org>, Praveen R <praveen@sigmoidanalytics.com>
Content-Type: multipart/alternative; boundary=f46d04428a6c020ca504faacf487
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04428a6c020ca504faacf487
Content-Type: text/plain; charset=UTF-8

We have a json feed of spark application interface that we use for easier
instrumentation & monitoring. Has that been considered/found relevant?
Already sent as a pull request to 0.9.0, would that work or should we
update it to 1.0.0?


Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>



---------- Forwarded message ----------
From: Patrick Wendell <pwendell@gmail.com>
Date: Sat, May 31, 2014 at 9:09 AM
Subject: Re: Monitoring / Instrumenting jobs in 1.0
To: user@spark.apache.org


The main change here was refactoring the SparkListener interface which
is where we expose internal state about a Spark job to other
applications. We've cleaned up these API's a bunch and also added a
way to log all data as JSON for post-hoc analysis:

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala

- Patrick

On Fri, May 30, 2014 at 7:09 AM, Daniel Siegmann
<daniel.siegmann@velos.io> wrote:
> The Spark 1.0.0 release notes state "Internal instrumentation has been
added
> to allow applications to monitor and instrument Spark jobs." Can anyone
> point me to the docs for this?
>
> --
> Daniel Siegmann, Software Developer
> Velos
> Accelerating Machine Learning
>
> 440 NINTH AVENUE, 11TH FLOOR, NEW YORK, NY 10001
> E: daniel.siegmann@velos.io W: www.velos.io

--f46d04428a6c020ca504faacf487--

From dev-return-7912-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 31 10:23:43 2014
Return-Path: <dev-return-7912-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A9B6010AD2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 31 May 2014 10:23:43 +0000 (UTC)
Received: (qmail 86332 invoked by uid 500); 31 May 2014 10:23:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86232 invoked by uid 500); 31 May 2014 10:23:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86216 invoked by uid 99); 31 May 2014 10:23:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 May 2014 10:23:42 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of prabsmails@gmail.com designates 209.85.128.178 as permitted sender)
Received: from [209.85.128.178] (HELO mail-ve0-f178.google.com) (209.85.128.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 May 2014 10:23:38 +0000
Received: by mail-ve0-f178.google.com with SMTP id sa20so3274945veb.37
        for <multiple recipients>; Sat, 31 May 2014 03:23:14 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=JHfehvGytZvpqEcumppy2mcdr2ovq9/GH3PkDHEr0r0=;
        b=mfkodfGswL+ce6T2dnEO/bsC5xkWwCTbWsXausMptAhr3D1w733KTwfa7crDN5WdwX
         z5ulf7C1CgDzJi7R71b0YDnma3YeYsEDsgMi37ITxzaarCAOgRivgduZExzgpOeiEwSA
         6VMi/o2wTPZPzHA2VK2JvikSxLuK4coD6GinsrjZModXCxfDW22r2D2rwGmgxKSI60x7
         L/6qfdh7ZV9bKy71Fz2WHqqJ/k/DJJ1NtJLNhbJ/AJGOlXGgiUJ85ipKcI6dmeHln3+T
         gC4piU8ft5M5TJiQaHVy4Bog+tcWYOIL8ZdGsUbomalKWB+c7XBojAFpbwbNnEYRLG7S
         p7Vw==
X-Received: by 10.221.4.66 with SMTP id ob2mr19669455vcb.28.1401531794491;
 Sat, 31 May 2014 03:23:14 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.195.209 with HTTP; Sat, 31 May 2014 03:22:34 -0700 (PDT)
From: prabeesh k <prabsmails@gmail.com>
Date: Sat, 31 May 2014 15:52:34 +0530
Message-ID: <CAPdPcW0BQym1jp-unWuagVwzGnD9xYY8X8fSGOJieg5suv3upg@mail.gmail.com>
Subject: Unable to execute saveAsTextFile on multi node mesos
To: user@spark.apache.org, user@mesos.apache.org, dev <dev@spark.apache.org>, 
	dev <dev@mesos.apache.org>
Content-Type: multipart/alternative; boundary=089e0122a924c0900004faaf8cdf
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0122a924c0900004faaf8cdf
Content-Type: text/plain; charset=UTF-8

Hi,

scenario : Read data from HDFS and apply hive query  on it and the result
is written back to HDFS.

 Scheme creation, Querying  and saveAsTextFile are working fine with
following mode

   - local mode
   - mesos cluster with single node
   - spark cluster with multi node

Schema creation and querying are working fine with mesos multi node cluster.
But  while trying to write back to HDFS using saveAsTextFile, the following
error occurs

* 14/05/30 10:16:35 INFO DAGScheduler: The failed fetch was from Stage 4
(mapPartitionsWithIndex at Operator.scala:333); marking it for resubmission*
*14/05/30 10:16:35 INFO DAGScheduler: Executor lost:
201405291518-3644595722-5050-17933-1 (epoch 148)*

Let me know your thoughts regarding this.

Regards,
prabeesh

--089e0122a924c0900004faaf8cdf--

From dev-return-7913-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 31 17:16:00 2014
Return-Path: <dev-return-7913-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7E22A1135F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 31 May 2014 17:16:00 +0000 (UTC)
Received: (qmail 32845 invoked by uid 500); 31 May 2014 17:16:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32799 invoked by uid 500); 31 May 2014 17:15:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32784 invoked by uid 99); 31 May 2014 17:15:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 May 2014 17:15:59 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.44 as permitted sender)
Received: from [209.85.219.44] (HELO mail-oa0-f44.google.com) (209.85.219.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 May 2014 17:15:54 +0000
Received: by mail-oa0-f44.google.com with SMTP id o6so3119214oag.17
        for <dev@spark.apache.org>; Sat, 31 May 2014 10:15:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=1TcrDmzSkKDz/q2LgaW+8YEK7Osop4e94bU6KoYN5l0=;
        b=QBVv0pdpwVBmPQxJR75VmV6XSTajgGN0ABom6gV1cnqXZtoU/b3QRsDObcyAJ+2g7w
         C2IE+11+vMbX92Vy1MF/7Pe3tQpMHuMBgYFt1fmz7En2sOQjIRPBzREvTWaWNonOFtIj
         KqR61vbUWo9sadqNa08SHGcsfPt72EqM7KR1aLnIU9maH2/xaN0WZfv5acsweV0aIqpE
         XWfRmMCFBaEP8e3+EAiJ1VxtmxEPLXFso1eZuyOJOP5X8QRTJ6iGY49R+P7K//q/V6Xh
         8C1x+FoVIjZooB2xAlbrYypvY0wmAS9oTXPUa3SDr3PrwV/31mOjTl/77QAyICoacHlj
         1muw==
MIME-Version: 1.0
X-Received: by 10.60.179.80 with SMTP id de16mr26182143oec.69.1401556533883;
 Sat, 31 May 2014 10:15:33 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Sat, 31 May 2014 10:15:33 -0700 (PDT)
In-Reply-To: <CAPdPcW0BQym1jp-unWuagVwzGnD9xYY8X8fSGOJieg5suv3upg@mail.gmail.com>
References: <CAPdPcW0BQym1jp-unWuagVwzGnD9xYY8X8fSGOJieg5suv3upg@mail.gmail.com>
Date: Sat, 31 May 2014 10:15:33 -0700
Message-ID: <CABPQxsvo2ay5DNzRu-n3drvP-2o=KmhpitmbKMcgx_2KZBJxWQ@mail.gmail.com>
Subject: Re: Unable to execute saveAsTextFile on multi node mesos
From: Patrick Wendell <pwendell@gmail.com>
To: user@spark.apache.org
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Can you look at the logs from the executor or in the UI? They should
give an exception with the reason for the task failure. Also in the
future, for this type of e-mail please only e-mail the "user@" list
and not both lists.

- Patrick

On Sat, May 31, 2014 at 3:22 AM, prabeesh k <prabsmails@gmail.com> wrote:
> Hi,
>
> scenario : Read data from HDFS and apply hive query  on it and the result is
> written back to HDFS.
>
>  Scheme creation, Querying  and saveAsTextFile are working fine with
> following mode
>
> local mode
> mesos cluster with single node
> spark cluster with multi node
>
> Schema creation and querying are working fine with mesos multi node cluster.
> But  while trying to write back to HDFS using saveAsTextFile, the following
> error occurs
>
>  14/05/30 10:16:35 INFO DAGScheduler: The failed fetch was from Stage 4
> (mapPartitionsWithIndex at Operator.scala:333); marking it for resubmission
> 14/05/30 10:16:35 INFO DAGScheduler: Executor lost:
> 201405291518-3644595722-5050-17933-1 (epoch 148)
>
> Let me know your thoughts regarding this.
>
> Regards,
> prabeesh

From dev-return-7914-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 31 17:45:46 2014
Return-Path: <dev-return-7914-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2CB8B113B8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 31 May 2014 17:45:46 +0000 (UTC)
Received: (qmail 69623 invoked by uid 500); 31 May 2014 17:45:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69564 invoked by uid 500); 31 May 2014 17:45:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69556 invoked by uid 99); 31 May 2014 17:45:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 May 2014 17:45:45 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.43 as permitted sender)
Received: from [209.85.219.43] (HELO mail-oa0-f43.google.com) (209.85.219.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 May 2014 17:45:41 +0000
Received: by mail-oa0-f43.google.com with SMTP id l6so3154232oag.2
        for <dev@spark.apache.org>; Sat, 31 May 2014 10:45:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=5bUxA+dwrhvqS8MRExsm04SEOTC78cYYToTXBV03o0c=;
        b=C1UVkW3zCyV2q2qm2fwFZxJzZ9N8/pZdPYXJ0fXIz+mDoXz/O0Hm5jR2KzTGpmmOTK
         3r7u69ntVtwrVviNadv/m7EQCI93ju7DPftaRfoHOKXFa0h57vhERROkW+oxUlmvQaiU
         PyepCzKjD/Ld6pBB8wBrPBG2XOq1OPmGC4CtuysEbZ5aCLCo4kAVGRGQbLV3ZRk0Pe+F
         ygW0aXVUvQGyDun2mDg4CjfXKbt3+hAodCZ+vEad/B0dOCTkl+4fm1aOSjCB27u0lQWY
         n0ci/VuOCHdc+tBFYrcflcCpteXjIUkjJTO9Ns1fOH++cuuVPawIP4X4GbareQzqZh3c
         Sglw==
MIME-Version: 1.0
X-Received: by 10.60.146.167 with SMTP id td7mr26698158oeb.6.1401558321155;
 Sat, 31 May 2014 10:45:21 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Sat, 31 May 2014 10:45:21 -0700 (PDT)
In-Reply-To: <CABPQxsuwLYpca1aX__Ceq6tCcpkh_be38iZ6WmTa+=59+aTV+Q@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
	<CA+-p3AEL6ct-bJwkBSXm+wTLBHEbBoGnnGOJGAjWHvQogDs-4g@mail.gmail.com>
	<CALRHqP-jLhJM3_5qCTWzrbU+GpmVsd2DFtvMXsH4-PGuT6X8ww@mail.gmail.com>
	<75094F23-AACD-4427-AE5B-A25AC02189A8@gmail.com>
	<CAJiQeY+Sy4EUKhM8ztsSfxaNoOG_73b=s7W_u5YRBPk-JegUDA@mail.gmail.com>
	<CAAsvFPm9OeOd7v2wsevLpKdqaCjfQuAA21r8itVu4-c2vft2OA@mail.gmail.com>
	<CAJiQeYKWeKoZuY9MjQTxzHn5d5XwvHwo0e8sEuEWL6CFW4p6CA@mail.gmail.com>
	<CAJiQeYKAbeAp7ZQK7bbxGF3YhfqeOOnt5-p7QO8n3g97f=YCKg@mail.gmail.com>
	<CABPQxst3M2r5aLur41JB+bjK9U2s9V0GSbrF45xTsQ9rS2d7gA@mail.gmail.com>
	<CA+qbEUMAV=uZUCjAmgLMOjX3ccPcY=F6ade7TDtz5Lm92XJguA@mail.gmail.com>
	<CAAOnQ7vZGCQVYxZGS1+g5_2ZJkKxucVg2F-AaaeMYFruEq6VWQ@mail.gmail.com>
	<CABPQxsshMrxNrwwhQNiN4aRzSgLK_S558Y7_2cgLANVsz0ez_w@mail.gmail.com>
	<CA+qbEUNUj_Z-ssG0p8UL9yvyBQeZCdjifB2_M3PJE-E=QHLFtw@mail.gmail.com>
	<CABPQxsuwLYpca1aX__Ceq6tCcpkh_be38iZ6WmTa+=59+aTV+Q@mail.gmail.com>
Date: Sat, 31 May 2014 10:45:21 -0700
Message-ID: <CABPQxsv8+LOzPk4FzhY_tVFLZ8gBvwUNERQoAM+WXeWBM6KNtw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

One other consideration popped into my head:

5. Shading our dependencies could mess up our external API's if we
ever return types that are outside of the spark package because we'd
then be returned shaded types that users have to deal with. E.g. where
before we returned an o.a.flume.AvroFlumeEvent we'd have to return a
some.namespace.AvroFlumeEvent. Then users downstream would have to
deal with converting our types into the correct namespace if they want
to inter-operate with other libraries. We generally try to avoid ever
returning types from other libraries, but it would be good to audit
our API's and see if we ever do this.

On Fri, May 30, 2014 at 10:54 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Spark is a bit different than Hadoop MapReduce, so maybe that's a
> source of some confusion. Spark is often used as a substrate for
> building different types of analytics applications, so @DeveloperAPI
> are internal API's that we'd like to expose to application writers,
> but that might be more volatile. This is like the internal API's in
> the linux kernel, they aren't stable, but of course we try to minimize
> changes to them. If people want to write lower-level modules against
> them, that's fine with us, but they know the interfaces might change.
>
> This has worked pretty well over the years, even with many different
> companies writing against those API's.
>
> @Experimental are user-facing features we are trying out. Hopefully
> that one is more clear.
>
> In terms of making a big jar that shades all of our dependencies - I'm
> curious how that would actually work in practice. It would be good to
> explore. There are a few potential challenges I see:
>
> 1. If any of our dependencies encode class name information in IPC
> messages, this would break. E.g. can you definitely shade the Hadoop
> client, protobuf, hbase client, etc and have them send messages over
> the wire? This could break things if class names are ever encoded in a
> wire format.
> 2. Many libraries like logging subsystems, configuration systems, etc
> rely on static state and initialization. I'm not totally sure how e.g.
> slf4j initializes itself if you have both a shaded and non-shaded copy
> of slf4j present.
> 3. This would mean the spark-core jar would be really massive because
> it would inline all of our deps. We've actually been thinking of
> avoiding the current assembly jar approach because, due to scala
> specialized classes, our assemblies now have more than 65,000 class
> files in them leading to all kinds of bad issues. We'd have to stick
> with a big uber assembly-like jar if we decide to shade stuff.
> 4. I'm not totally sure how this would work when people want to e.g.
> build Spark with different Hadoop versions. Would we publish different
> shaded uber-jars for every Hadoop version? Would the Hadoop dep just
> not be shaded... if so what about all it's dependencies.
>
> Anyways just some things to consider... simplifying our classpath is
> definitely an avenue worth exploring!
>
>
>
>
> On Fri, May 30, 2014 at 2:56 PM, Colin McCabe <cmccabe@alumni.cmu.edu> wrote:
>> On Fri, May 30, 2014 at 2:11 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>
>>> Hey guys, thanks for the insights. Also, I realize Hadoop has gotten
>>> way better about this with 2.2+ and I think it's great progress.
>>>
>>> We have well defined API levels in Spark and also automated checking
>>> of API violations for new pull requests. When doing code reviews we
>>> always enforce the narrowest possible visibility:
>>>
>>> 1. private
>>> 2. private[spark]
>>> 3. @Experimental or @DeveloperApi
>>> 4. public
>>>
>>> Our automated checks exclude 1-3. Anything that breaks 4 will trigger
>>> a build failure.
>>>
>>>
>> That's really excellent.  Great job.
>>
>> I like the private[spark] visibility level-- sounds like this is another
>> way Scala has greatly improved on Java.
>>
>> The Scala compiler prevents anyone external from using 1 or 2. We do
>>> have "bytecode public but annotated" (3) API's that we might change.
>>> We spent a lot of time looking into whether these can offer compiler
>>> warnings, but we haven't found a way to do this and do not see a
>>> better alternative at this point.
>>>
>>
>> It would be nice if the production build could strip this stuff out.
>>  Otherwise, it feels a lot like a @private, @unstable Hadoop API... and we
>> know how those turned out.
>>
>>
>>> Regarding Scala compatibility, Scala 2.11+ is "source code
>>> compatible", meaning we'll be able to cross-compile Spark for
>>> different versions of Scala. We've already been in touch with Typesafe
>>> about this and they've offered to integrate Spark into their
>>> compatibility test suite. They've also committed to patching 2.11 with
>>> a minor release if bugs are found.
>>>
>>
>> Thanks, I hadn't heard about this plan.  Hopefully we can get everyone on
>> 2.11 ASAP.
>>
>>
>>> Anyways, my point is we've actually thought a lot about this already.
>>>
>>> The CLASSPATH thing is different than API stability, but indeed also a
>>> form of compatibility. This is something where I'd also like to see
>>> Spark have better isolation of user classes from Spark's own
>>> execution...
>>>
>>>
>> I think the best thing to do is just "shade" all the dependencies.  Then
>> they will be in a different namespace, and clients can have their own
>> versions of whatever dependencies they like without conflicting.  As
>> Marcelo mentioned, there might be a few edge cases where this breaks
>> reflection, but I don't think that's an issue for most libraries.  So at
>> worst case we could end up needing apps to follow us in lockstep for Kryo
>> or maybe Akka, but not the whole kit and caboodle like with Hadoop.
>>
>> best,
>> Colin
>>
>>
>> - Patrick
>>>
>>>
>>>
>>> On Fri, May 30, 2014 at 12:30 PM, Marcelo Vanzin <vanzin@cloudera.com>
>>> wrote:
>>> > On Fri, May 30, 2014 at 12:05 PM, Colin McCabe <cmccabe@alumni.cmu.edu>
>>> wrote:
>>> >> I don't know if Scala provides any mechanisms to do this beyond what
>>> Java provides.
>>> >
>>> > In fact it does. You can say something like "private[foo]" and the
>>> > annotated element will be visible for all classes under "foo" (where
>>> > "foo" is any package in the hierarchy leading up to the class). That's
>>> > used a lot in Spark.
>>> >
>>> > I haven't fully looked at how the @DeveloperApi is used, but I agree
>>> > with you - annotations are not a good way to do this. The Scala
>>> > feature above would be much better, but it might still leak things at
>>> > the Java bytecode level (don't know how Scala implements it under the
>>> > cover, but I assume it's not by declaring the element as a Java
>>> > "private").
>>> >
>>> > Another thing is that in Scala the default visibility is public, which
>>> > makes it very easy to inadvertently add things to the API. I'd like to
>>> > see more care in making things have the proper visibility - I
>>> > generally declare things private first, and relax that as needed.
>>> > Using @VisibleForTesting would be great too, when the Scala
>>> > private[foo] approach doesn't work.
>>> >
>>> >> Does Spark also expose its CLASSPATH in
>>> >> this way to executors?  I was under the impression that it did.
>>> >
>>> > If you're using the Spark assemblies, yes, there is a lot of things
>>> > that your app gets exposed to. For example, you can see Guava and
>>> > Jetty (and many other things) there. This is something that has always
>>> > bugged me, but I don't really have a good suggestion of how to fix it;
>>> > shading goes a certain way, but it also breaks codes that uses
>>> > reflection (e.g. Class.forName()-style class loading).
>>> >
>>> > What is worse is that Spark doesn't even agree with the Hadoop code it
>>> > depends on; e.g., Spark uses Guava 14.x while Hadoop is still in Guava
>>> > 11.x. So when you run your Scala app, what gets loaded?
>>> >
>>> >> At some point we will also have to confront the Scala version issue.
>>>  Will
>>> >> there be flag days where Spark jobs need to be upgraded to a new,
>>> >> incompatible version of Scala to run on the latest Spark?
>>> >
>>> > Yes, this could be an issue - I'm not sure Scala has a policy towards
>>> > this, but updates (at least minor, e.g. 2.9 -> 2.10) tend to break
>>> > binary compatibility.
>>> >
>>> > Scala also makes some API updates tricky - e.g., adding a new named
>>> > argument to a Scala method is not a binary compatible change (while,
>>> > e.g., adding a new keyword argument in a python method is just fine).
>>> > The use of implicits and other Scala features make this even more
>>> > opaque...
>>> >
>>> > Anyway, not really any solutions in this message, just a few comments
>>> > I wanted to throw out there. :-)
>>> >
>>> > --
>>> > Marcelo
>>>

From dev-return-7915-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat May 31 21:10:10 2014
Return-Path: <dev-return-7915-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5A630117E9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 31 May 2014 21:10:10 +0000 (UTC)
Received: (qmail 87872 invoked by uid 500); 31 May 2014 21:10:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87813 invoked by uid 500); 31 May 2014 21:10:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87805 invoked by uid 99); 31 May 2014 21:10:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 May 2014 21:10:09 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rarecactus@gmail.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 31 May 2014 21:10:05 +0000
Received: by mail-wi0-f175.google.com with SMTP id f8so2692458wiw.14
        for <dev@spark.apache.org>; Sat, 31 May 2014 14:09:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=TbApJSgZdTVoY0ZcdXzXerPfpxYkI/JpnfjUiu/udGg=;
        b=TYEU6lSChHTfjSAO6d3+8L3+qmeCqR4ozLe456YNjdTxAqwXdj+QnwTW6tix4T9kxr
         CtBNUQIa2runci3iRCch7CI1R9SfFROh9PkfGwKtkJQ0e4LmrvF8j6Zm5zVeZ1CbDVW+
         xKOfCRIgo+AOOdigYqWsyR/ZJwrRaKHo0nR7Ng4WCM2rW5o4EgTDpx2dxY/yFbq8KusQ
         9Y7lQTor67zyfpwWHgELkXIpkgD9wqnvqQXWZ+JxqlQH86UrjJvVL2oXANcsCbFh12wt
         oKVxRsJ+N0QBJ1hfFzCbpmuuLNFCjsS5ZCE5mALnJsNbCeX68K6dOJxri4k5z/AXB873
         lY1w==
MIME-Version: 1.0
X-Received: by 10.180.85.163 with SMTP id i3mr9520486wiz.14.1401570584215;
 Sat, 31 May 2014 14:09:44 -0700 (PDT)
Sender: rarecactus@gmail.com
Received: by 10.194.242.35 with HTTP; Sat, 31 May 2014 14:09:44 -0700 (PDT)
In-Reply-To: <CABPQxsv8+LOzPk4FzhY_tVFLZ8gBvwUNERQoAM+WXeWBM6KNtw@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
	<CA+-p3AEL6ct-bJwkBSXm+wTLBHEbBoGnnGOJGAjWHvQogDs-4g@mail.gmail.com>
	<CALRHqP-jLhJM3_5qCTWzrbU+GpmVsd2DFtvMXsH4-PGuT6X8ww@mail.gmail.com>
	<75094F23-AACD-4427-AE5B-A25AC02189A8@gmail.com>
	<CAJiQeY+Sy4EUKhM8ztsSfxaNoOG_73b=s7W_u5YRBPk-JegUDA@mail.gmail.com>
	<CAAsvFPm9OeOd7v2wsevLpKdqaCjfQuAA21r8itVu4-c2vft2OA@mail.gmail.com>
	<CAJiQeYKWeKoZuY9MjQTxzHn5d5XwvHwo0e8sEuEWL6CFW4p6CA@mail.gmail.com>
	<CAJiQeYKAbeAp7ZQK7bbxGF3YhfqeOOnt5-p7QO8n3g97f=YCKg@mail.gmail.com>
	<CABPQxst3M2r5aLur41JB+bjK9U2s9V0GSbrF45xTsQ9rS2d7gA@mail.gmail.com>
	<CA+qbEUMAV=uZUCjAmgLMOjX3ccPcY=F6ade7TDtz5Lm92XJguA@mail.gmail.com>
	<CAAOnQ7vZGCQVYxZGS1+g5_2ZJkKxucVg2F-AaaeMYFruEq6VWQ@mail.gmail.com>
	<CABPQxsshMrxNrwwhQNiN4aRzSgLK_S558Y7_2cgLANVsz0ez_w@mail.gmail.com>
	<CA+qbEUNUj_Z-ssG0p8UL9yvyBQeZCdjifB2_M3PJE-E=QHLFtw@mail.gmail.com>
	<CABPQxsuwLYpca1aX__Ceq6tCcpkh_be38iZ6WmTa+=59+aTV+Q@mail.gmail.com>
	<CABPQxsv8+LOzPk4FzhY_tVFLZ8gBvwUNERQoAM+WXeWBM6KNtw@mail.gmail.com>
Date: Sat, 31 May 2014 14:09:44 -0700
X-Google-Sender-Auth: qGH2TrUdi5XZsLw5lOggUYi2vcs
Message-ID: <CA+qbEUNb7k6k5y_jSKWgc6Stw+-R71CToTw=cfVAFRxfy2PXxg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Colin McCabe <cmccabe@alumni.cmu.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d0444eb09ccc96f04fab8947b
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0444eb09ccc96f04fab8947b
Content-Type: text/plain; charset=UTF-8

On Sat, May 31, 2014 at 10:45 AM, Patrick Wendell <pwendell@gmail.com>
wrote:

> One other consideration popped into my head:
>
> 5. Shading our dependencies could mess up our external API's if we
> ever return types that are outside of the spark package because we'd
> then be returned shaded types that users have to deal with. E.g. where
> before we returned an o.a.flume.AvroFlumeEvent we'd have to return a
> some.namespace.AvroFlumeEvent. Then users downstream would have to
> deal with converting our types into the correct namespace if they want
> to inter-operate with other libraries. We generally try to avoid ever
> returning types from other libraries, but it would be good to audit
> our API's and see if we ever do this.


That's a good point.  It seems to me that if Spark is returning a type in
the public API, that type is part of the public API (for better or worse).
 So this is a case where you wouldn't want to shade that type.  But it
would be nice to avoid doing this, for exactly the reasons you state...

On Fri, May 30, 2014 at 10:54 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > Spark is a bit different than Hadoop MapReduce, so maybe that's a
> > source of some confusion. Spark is often used as a substrate for
> > building different types of analytics applications, so @DeveloperAPI
> > are internal API's that we'd like to expose to application writers,
> > but that might be more volatile. This is like the internal API's in
> > the linux kernel, they aren't stable, but of course we try to minimize
> > changes to them. If people want to write lower-level modules against
> > them, that's fine with us, but they know the interfaces might change.
>

MapReduce is used as a substrate in a lot of cases, too.  Hive has
traditionally created MR jobs to do what it needs to do.  Similarly, Oozie
can create MR jobs.  It seems that what @DeveloperAPI is pretty similar to
@LimitedPrivate in Hadoop.  If I understand correctly, your hope is that
frameworks will use @DeveloperAPI, but individual application developers
will steer clear.  That is a good plan, as long as you can ensure that the
framework developers are willing to lock their versions to a certain Spark
version.  Otherwise they will make the same arguments we've heard before,
that they don't want to transition off of a deprecated @DeveloperAPI
because they want to keep support for Spark 1.0.0 (or whatever).  We hear
these arguments in Hadoop all the time...  now that spark as a 1.0 release
they will carry more weight.  Remember, Hadoop APIs started nice and simple
too :)

>
> > This has worked pretty well over the years, even with many different
> > companies writing against those API's.
> >
> > @Experimental are user-facing features we are trying out. Hopefully
> > that one is more clear.
> >
> > In terms of making a big jar that shades all of our dependencies - I'm
> > curious how that would actually work in practice. It would be good to
> > explore. There are a few potential challenges I see:
> >
> > 1. If any of our dependencies encode class name information in IPC
> > messages, this would break. E.g. can you definitely shade the Hadoop
> > client, protobuf, hbase client, etc and have them send messages over
> > the wire? This could break things if class names are ever encoded in a
> > wire format.
>

Google protobuffers assume a fixed schema.  That is to say, they do not
include metadata identifying the types of what is placed in them.  The
types are determined by convention.  It is possible to change the java
package in which the protobuf classes reside with no harmful effects.  (See
HDFS-4909 for an example of this).  The RPC itself does include a java
class name for the interface we're talking to, though.  The code for
handling this is all under our control, though, so if we had to make any
minor modifications to make shading work, we could.

> 2. Many libraries like logging subsystems, configuration systems, etc
> > rely on static state and initialization. I'm not totally sure how e.g.
> > slf4j initializes itself if you have both a shaded and non-shaded copy
> > of slf4j present.
>

I guess the worst case scenario would be that the shaded version of slf4j
creates a log file, but then the app's unshaded version overwrites that log
file.  I don't see how the two versions could "cooperate" since they aren't
sharing static state.  The only solutions I can see are leaving slf4j
unshaded, or setting up separate log files for the spark-core versus the
application.  I haven't thought this through completely, but my gut feeling
is that if you're sharing a log file, you probably want to share the
logging code too.


> > 3. This would mean the spark-core jar would be really massive because
> > it would inline all of our deps. We've actually been thinking of
> > avoiding the current assembly jar approach because, due to scala
> > specialized classes, our assemblies now have more than 65,000 class
> > files in them leading to all kinds of bad issues. We'd have to stick
> > with a big uber assembly-like jar if we decide to shade stuff.
> > 4. I'm not totally sure how this would work when people want to e.g.
> > build Spark with different Hadoop versions. Would we publish different
> > shaded uber-jars for every Hadoop version? Would the Hadoop dep just
> > not be shaded... if so what about all it's dependencies.
>

I wonder if it would be possible to put Hadoop and its dependencies "in a
box," (as it were) by using a separate classloader for them.  That might
solve this without requiring an uber-jar.  It would be nice to not have to
transfer all that stuff each time you start a job... in a perfect world,
the stuff that had not changed would not need to be transferred (thinking
out loud here)

best,
Colin


>
> > Anyways just some things to consider... simplifying our classpath is
> > definitely an avenue worth exploring!
> >
> >
> >
> >
> > On Fri, May 30, 2014 at 2:56 PM, Colin McCabe <cmccabe@alumni.cmu.edu>
> wrote:
> >> On Fri, May 30, 2014 at 2:11 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >>
> >>> Hey guys, thanks for the insights. Also, I realize Hadoop has gotten
> >>> way better about this with 2.2+ and I think it's great progress.
> >>>
> >>> We have well defined API levels in Spark and also automated checking
> >>> of API violations for new pull requests. When doing code reviews we
> >>> always enforce the narrowest possible visibility:
> >>>
> >>> 1. private
> >>> 2. private[spark]
> >>> 3. @Experimental or @DeveloperApi
> >>> 4. public
> >>>
> >>> Our automated checks exclude 1-3. Anything that breaks 4 will trigger
> >>> a build failure.
> >>>
> >>>
> >> That's really excellent.  Great job.
> >>
> >> I like the private[spark] visibility level-- sounds like this is another
> >> way Scala has greatly improved on Java.
> >>
> >> The Scala compiler prevents anyone external from using 1 or 2. We do
> >>> have "bytecode public but annotated" (3) API's that we might change.
> >>> We spent a lot of time looking into whether these can offer compiler
> >>> warnings, but we haven't found a way to do this and do not see a
> >>> better alternative at this point.
> >>>
> >>
> >> It would be nice if the production build could strip this stuff out.
> >>  Otherwise, it feels a lot like a @private, @unstable Hadoop API... and
> we
> >> know how those turned out.
> >>
> >>
> >>> Regarding Scala compatibility, Scala 2.11+ is "source code
> >>> compatible", meaning we'll be able to cross-compile Spark for
> >>> different versions of Scala. We've already been in touch with Typesafe
> >>> about this and they've offered to integrate Spark into their
> >>> compatibility test suite. They've also committed to patching 2.11 with
> >>> a minor release if bugs are found.
> >>>
> >>
> >> Thanks, I hadn't heard about this plan.  Hopefully we can get everyone
> on
> >> 2.11 ASAP.
> >>
> >>
> >>> Anyways, my point is we've actually thought a lot about this already.
> >>>
> >>> The CLASSPATH thing is different than API stability, but indeed also a
> >>> form of compatibility. This is something where I'd also like to see
> >>> Spark have better isolation of user classes from Spark's own
> >>> execution...
> >>>
> >>>
> >> I think the best thing to do is just "shade" all the dependencies.  Then
> >> they will be in a different namespace, and clients can have their own
> >> versions of whatever dependencies they like without conflicting.  As
> >> Marcelo mentioned, there might be a few edge cases where this breaks
> >> reflection, but I don't think that's an issue for most libraries.  So at
> >> worst case we could end up needing apps to follow us in lockstep for
> Kryo
> >> or maybe Akka, but not the whole kit and caboodle like with Hadoop.
> >>
> >> best,
> >> Colin
> >>
> >>
> >> - Patrick
> >>>
> >>>
> >>>
> >>> On Fri, May 30, 2014 at 12:30 PM, Marcelo Vanzin <vanzin@cloudera.com>
> >>> wrote:
> >>> > On Fri, May 30, 2014 at 12:05 PM, Colin McCabe <
> cmccabe@alumni.cmu.edu>
> >>> wrote:
> >>> >> I don't know if Scala provides any mechanisms to do this beyond what
> >>> Java provides.
> >>> >
> >>> > In fact it does. You can say something like "private[foo]" and the
> >>> > annotated element will be visible for all classes under "foo" (where
> >>> > "foo" is any package in the hierarchy leading up to the class).
> That's
> >>> > used a lot in Spark.
> >>> >
> >>> > I haven't fully looked at how the @DeveloperApi is used, but I agree
> >>> > with you - annotations are not a good way to do this. The Scala
> >>> > feature above would be much better, but it might still leak things at
> >>> > the Java bytecode level (don't know how Scala implements it under the
> >>> > cover, but I assume it's not by declaring the element as a Java
> >>> > "private").
> >>> >
> >>> > Another thing is that in Scala the default visibility is public,
> which
> >>> > makes it very easy to inadvertently add things to the API. I'd like
> to
> >>> > see more care in making things have the proper visibility - I
> >>> > generally declare things private first, and relax that as needed.
> >>> > Using @VisibleForTesting would be great too, when the Scala
> >>> > private[foo] approach doesn't work.
> >>> >
> >>> >> Does Spark also expose its CLASSPATH in
> >>> >> this way to executors?  I was under the impression that it did.
> >>> >
> >>> > If you're using the Spark assemblies, yes, there is a lot of things
> >>> > that your app gets exposed to. For example, you can see Guava and
> >>> > Jetty (and many other things) there. This is something that has
> always
> >>> > bugged me, but I don't really have a good suggestion of how to fix
> it;
> >>> > shading goes a certain way, but it also breaks codes that uses
> >>> > reflection (e.g. Class.forName()-style class loading).
> >>> >
> >>> > What is worse is that Spark doesn't even agree with the Hadoop code
> it
> >>> > depends on; e.g., Spark uses Guava 14.x while Hadoop is still in
> Guava
> >>> > 11.x. So when you run your Scala app, what gets loaded?
> >>> >
> >>> >> At some point we will also have to confront the Scala version issue.
> >>>  Will
> >>> >> there be flag days where Spark jobs need to be upgraded to a new,
> >>> >> incompatible version of Scala to run on the latest Spark?
> >>> >
> >>> > Yes, this could be an issue - I'm not sure Scala has a policy towards
> >>> > this, but updates (at least minor, e.g. 2.9 -> 2.10) tend to break
> >>> > binary compatibility.
> >>> >
> >>> > Scala also makes some API updates tricky - e.g., adding a new named
> >>> > argument to a Scala method is not a binary compatible change (while,
> >>> > e.g., adding a new keyword argument in a python method is just fine).
> >>> > The use of implicits and other Scala features make this even more
> >>> > opaque...
> >>> >
> >>> > Anyway, not really any solutions in this message, just a few comments
> >>> > I wanted to throw out there. :-)
> >>> >
> >>> > --
> >>> > Marcelo
> >>>
>

--f46d0444eb09ccc96f04fab8947b--

