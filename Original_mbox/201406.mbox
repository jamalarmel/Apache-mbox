From dev-return-7916-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun  1 02:07:24 2014
Return-Path: <dev-return-7916-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9BC4111BA8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  1 Jun 2014 02:07:24 +0000 (UTC)
Received: (qmail 75399 invoked by uid 500); 1 Jun 2014 02:07:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75338 invoked by uid 500); 1 Jun 2014 02:07:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75330 invoked by uid 99); 1 Jun 2014 02:07:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Jun 2014 02:07:24 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.214.178] (HELO mail-ob0-f178.google.com) (209.85.214.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Jun 2014 02:07:18 +0000
Received: by mail-ob0-f178.google.com with SMTP id va2so3317223obc.9
        for <dev@spark.apache.org>; Sat, 31 May 2014 19:06:57 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=VaXOpJwScRPpCtttxVpWIonNJ6MfDtdqmpf+tIcPuZM=;
        b=V4l5REbSOUwtcop+3MP27Cje8uVFpQ6SOhREV70mVC8QupWvulwdOgMoZG4bwkVtLl
         uTlMCgvp1nXu4HruE4ucwFM9YmvU97osParFWPsHUMcSdzO3wm2rDF6wyO4+GDLoXvmH
         63NGuCBl5BJrVcNVg0tUo6pi++ReUvcXuEZAcTKCZlNyJX4/EIktGv7h4tNuMd4/8BGT
         TRlfSdalLoP3BmySl8TnzjKizIba7Y+iozuCMT8mM/GLhYqZxzkMyt/w+zQjmP4cnYbO
         lbGkbyVIEcqDPR9L3NnQoU5wVpzljBvuFcnnyGtWkY1u6bQhZnP3V+qiep3x26ClHXvn
         yHcw==
X-Gm-Message-State: ALoCoQkw/3PwjhaSQgi7eIm9rOGXQYkQSWdw6Z1WaV2p8En7N2cJFRJsfSPcWTuWm7eE6TrNhEDv
MIME-Version: 1.0
X-Received: by 10.60.37.99 with SMTP id x3mr28469646oej.65.1401588417629; Sat,
 31 May 2014 19:06:57 -0700 (PDT)
Received: by 10.182.184.40 with HTTP; Sat, 31 May 2014 19:06:57 -0700 (PDT)
Date: Sat, 31 May 2014 19:06:57 -0700
Message-ID: <CAJ3iqPT2p_-FEsU_3HfM7Vq8yjMSodDSLe23J-HxsfCfVNTxtA@mail.gmail.com>
Subject: SCALA_HOME or SCALA_LIBRARY_PATH not set during build
From: Soren Macbeth <soren@yieldbot.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01176279c12ac804fabcbbed
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01176279c12ac804fabcbbed
Content-Type: text/plain; charset=UTF-8

Hello,

Following the instructions for building spark 1.0.0, I encountered the
following error:

[ERROR] Failed to execute goal
org.apache.maven.plugins:maven-antrun-plugin:1.7:run (default) on project
spark-core_2.10: An Ant BuildException has occured: Please set the
SCALA_HOME (or SCALA_LIBRARY_PATH if scala is on the path) environment
variables and retry.
[ERROR] around Ant part ...<fail message="Please set the SCALA_HOME (or
SCALA_LIBRARY_PATH if scala is on the path) environment variables and
retry.">... @ 6:126 in
/Users/soren/src/spark-1.0.0/core/target/antrun/build-main.xml

No where in the documentation does it mention that having scala installed
and either of these env vars set nor what version should be installed.
Setting these env vars wasn't required for 0.9.1 with sbt.

I was able to get past it by downloading the scala 2.10.4 binary package to
a temp dir and setting SCALA_HOME to that dir.

Ideally, it would be nice to not have to require people to have a
standalone scala installation but at a minimum this requirement should be
documented in the build instructions no?

-Soren

--089e01176279c12ac804fabcbbed--

From dev-return-7917-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun  1 03:34:58 2014
Return-Path: <dev-return-7917-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9391711C79
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  1 Jun 2014 03:34:58 +0000 (UTC)
Received: (qmail 19091 invoked by uid 500); 1 Jun 2014 03:34:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18987 invoked by uid 500); 1 Jun 2014 03:34:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18977 invoked by uid 99); 1 Jun 2014 03:34:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Jun 2014 03:34:57 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rarecactus@gmail.com designates 74.125.82.51 as permitted sender)
Received: from [74.125.82.51] (HELO mail-wg0-f51.google.com) (74.125.82.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Jun 2014 03:34:53 +0000
Received: by mail-wg0-f51.google.com with SMTP id x13so3544772wgg.34
        for <dev@spark.apache.org>; Sat, 31 May 2014 20:34:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=2rzWowN7Mx/6U124jTpziimu0nFHmMABKmYcJCyMZaU=;
        b=dXW2gpRgCLgdqp2w2IbmOk9d2RZVK0N0A0zeHB6vKSvWqZCu14ucJ8ZrGqgLjk1rvC
         0I5D/TwxcqlmdVWGC9ruxJ82/jB18sCmuT/f8HGlapEn+B1Eplu9khiRv1Y3YkUMJmik
         YtvBrJmF+agXW1PbWfZM9gxckGpwV2S0qAvstisYdasQKa6+hMKRd0MH4t9+H7EiyNZu
         FFKknGWjrB9f1NFA8MzE1kSI9oZFb0/A7Em7+sbpgw9EQREPaJA43GgMBTF7D9r9MWfB
         vEyVrxOvQR8JhQGQdt5mxbmpkbomJ/O4xbPrMV7M9+MK6OolBVNfhcGELV4PqhkZJ6nd
         BEkA==
MIME-Version: 1.0
X-Received: by 10.181.8.67 with SMTP id di3mr11191458wid.8.1401593670333; Sat,
 31 May 2014 20:34:30 -0700 (PDT)
Sender: rarecactus@gmail.com
Received: by 10.194.242.35 with HTTP; Sat, 31 May 2014 20:34:30 -0700 (PDT)
In-Reply-To: <CAJ3iqPT2p_-FEsU_3HfM7Vq8yjMSodDSLe23J-HxsfCfVNTxtA@mail.gmail.com>
References: <CAJ3iqPT2p_-FEsU_3HfM7Vq8yjMSodDSLe23J-HxsfCfVNTxtA@mail.gmail.com>
Date: Sat, 31 May 2014 20:34:30 -0700
X-Google-Sender-Auth: H6q2R4aU3iuq31d6f5fdPotyfJY
Message-ID: <CA+qbEUN_a+z1Qt-_qT210e6uy11c5jWgN6ogqrqu86N685OzAw@mail.gmail.com>
Subject: Re: SCALA_HOME or SCALA_LIBRARY_PATH not set during build
From: Colin McCabe <cmccabe@alumni.cmu.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134d244d6fd4f04fabdf4c5
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134d244d6fd4f04fabdf4c5
Content-Type: text/plain; charset=UTF-8

Spark currently supports two build systems, sbt and maven.  sbt will
download the correct version of scala, but with Maven you need to supply it
yourself and set SCALA_HOME.

It sounds like the instructions need to be updated-- perhaps create a JIRA?

best,
Colin


On Sat, May 31, 2014 at 7:06 PM, Soren Macbeth <soren@yieldbot.com> wrote:

> Hello,
>
> Following the instructions for building spark 1.0.0, I encountered the
> following error:
>
> [ERROR] Failed to execute goal
> org.apache.maven.plugins:maven-antrun-plugin:1.7:run (default) on project
> spark-core_2.10: An Ant BuildException has occured: Please set the
> SCALA_HOME (or SCALA_LIBRARY_PATH if scala is on the path) environment
> variables and retry.
> [ERROR] around Ant part ...<fail message="Please set the SCALA_HOME (or
> SCALA_LIBRARY_PATH if scala is on the path) environment variables and
> retry.">... @ 6:126 in
> /Users/soren/src/spark-1.0.0/core/target/antrun/build-main.xml
>
> No where in the documentation does it mention that having scala installed
> and either of these env vars set nor what version should be installed.
> Setting these env vars wasn't required for 0.9.1 with sbt.
>
> I was able to get past it by downloading the scala 2.10.4 binary package to
> a temp dir and setting SCALA_HOME to that dir.
>
> Ideally, it would be nice to not have to require people to have a
> standalone scala installation but at a minimum this requirement should be
> documented in the build instructions no?
>
> -Soren
>

--001a1134d244d6fd4f04fabdf4c5--

From dev-return-7918-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun  1 18:14:24 2014
Return-Path: <dev-return-7918-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 17874107FF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  1 Jun 2014 18:14:24 +0000 (UTC)
Received: (qmail 50747 invoked by uid 500); 1 Jun 2014 18:14:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50683 invoked by uid 500); 1 Jun 2014 18:14:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50675 invoked by uid 99); 1 Jun 2014 18:14:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Jun 2014 18:14:23 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.181 as permitted sender)
Received: from [209.85.214.181] (HELO mail-ob0-f181.google.com) (209.85.214.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Jun 2014 18:14:21 +0000
Received: by mail-ob0-f181.google.com with SMTP id wm4so3750446obc.12
        for <dev@spark.apache.org>; Sun, 01 Jun 2014 11:13:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=c58wsoE+0p3sWEbziP7tHkMHoEghXjEdjno5A/dMP1w=;
        b=WPX9ySTWM2A5xYAGSAEsW4uFb2H/njR1WqaV64rpf0r85k9ZVbchpxyqkwt+hikEIs
         RCXEYbS+DwwsyULkT7gNS/eE79V/bJ+loY1wo6JTdZeX/6P3Tb5PYLLt5hiHMWoETkCL
         xvuiYk+95lmDo4t170cQCMXNfeV8QSdsn+DiF5wruaHxwtbIOxk+CkH5f1TZ0xTK4AOl
         LI/4cCaHqKYJsIlhsr3y/arfg6tuZfz82RYGFXo7KHey4Bn0L7IsES6zSKrsuxPQleMA
         J3zzkeh+YxTzSTsKdSuZ140+N6hEgLufONpcn8mxWKhA9poz1E7NDBFR/U6ZZyAReGFh
         wBgQ==
MIME-Version: 1.0
X-Received: by 10.182.144.161 with SMTP id sn1mr4163650obb.82.1401646437197;
 Sun, 01 Jun 2014 11:13:57 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Sun, 1 Jun 2014 11:13:57 -0700 (PDT)
In-Reply-To: <CA+qbEUN_a+z1Qt-_qT210e6uy11c5jWgN6ogqrqu86N685OzAw@mail.gmail.com>
References: <CAJ3iqPT2p_-FEsU_3HfM7Vq8yjMSodDSLe23J-HxsfCfVNTxtA@mail.gmail.com>
	<CA+qbEUN_a+z1Qt-_qT210e6uy11c5jWgN6ogqrqu86N685OzAw@mail.gmail.com>
Date: Sun, 1 Jun 2014 11:13:57 -0700
Message-ID: <CABPQxstwoe4ty6qQ-QNYcNrM14UFRVFUdF0zzmXq+S2KM9TnXA@mail.gmail.com>
Subject: Re: SCALA_HOME or SCALA_LIBRARY_PATH not set during build
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

This is a false error message actually - the Maven build no longer
requires SCALA_HOME but the message/check was still there. This was
fixed recently in master:

https://github.com/apache/spark/commit/d8c005d5371f81a2a06c5d27c7021e1ae43d7193

I can back port that fix into branch-1.0 so it will be in 1.0.1 as
well. For other people running into this, you can export SCALA_HOME to
any value and it will work.

- Patrick

On Sat, May 31, 2014 at 8:34 PM, Colin McCabe <cmccabe@alumni.cmu.edu> wrote:
> Spark currently supports two build systems, sbt and maven.  sbt will
> download the correct version of scala, but with Maven you need to supply it
> yourself and set SCALA_HOME.
>
> It sounds like the instructions need to be updated-- perhaps create a JIRA?
>
> best,
> Colin
>
>
> On Sat, May 31, 2014 at 7:06 PM, Soren Macbeth <soren@yieldbot.com> wrote:
>
>> Hello,
>>
>> Following the instructions for building spark 1.0.0, I encountered the
>> following error:
>>
>> [ERROR] Failed to execute goal
>> org.apache.maven.plugins:maven-antrun-plugin:1.7:run (default) on project
>> spark-core_2.10: An Ant BuildException has occured: Please set the
>> SCALA_HOME (or SCALA_LIBRARY_PATH if scala is on the path) environment
>> variables and retry.
>> [ERROR] around Ant part ...<fail message="Please set the SCALA_HOME (or
>> SCALA_LIBRARY_PATH if scala is on the path) environment variables and
>> retry.">... @ 6:126 in
>> /Users/soren/src/spark-1.0.0/core/target/antrun/build-main.xml
>>
>> No where in the documentation does it mention that having scala installed
>> and either of these env vars set nor what version should be installed.
>> Setting these env vars wasn't required for 0.9.1 with sbt.
>>
>> I was able to get past it by downloading the scala 2.10.4 binary package to
>> a temp dir and setting SCALA_HOME to that dir.
>>
>> Ideally, it would be nice to not have to require people to have a
>> standalone scala installation but at a minimum this requirement should be
>> documented in the build instructions no?
>>
>> -Soren
>>

From dev-return-7919-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun  1 18:21:42 2014
Return-Path: <dev-return-7919-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 42A3B10817
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  1 Jun 2014 18:21:42 +0000 (UTC)
Received: (qmail 57578 invoked by uid 500); 1 Jun 2014 18:21:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57524 invoked by uid 500); 1 Jun 2014 18:21:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57513 invoked by uid 99); 1 Jun 2014 18:21:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Jun 2014 18:21:41 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.174 as permitted sender)
Received: from [209.85.214.174] (HELO mail-ob0-f174.google.com) (209.85.214.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Jun 2014 18:21:39 +0000
Received: by mail-ob0-f174.google.com with SMTP id uz6so3703052obc.33
        for <dev@spark.apache.org>; Sun, 01 Jun 2014 11:21:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=h6HLMtBhLsDW3zkytUEgL2pGk+2RFV2z4xlbIF2AUSc=;
        b=FXUFArghTSRJbQrLt3+txyXOApXo/2DZL2k/3uAc467ND72GYfqbJ2E0zEgq5eq9Mg
         07O1PL9+d7DvpzINClrSmldTNEknDi3OPkEfMpjRMkF1Ak+y5/+risxb+zQuB34QkdHI
         p021zHHbUooWYDZ/QfvWg7HSCaahnUpjlSejVDp36TDW/PlVSP4AIH/0nY5fyAZYdrO8
         G9zE+3DT4KXOzVpHi7el2mPF9uenpgqfCSV/0BdsC8OqTG5SwomUKpFO7ZltNugRkCkT
         FKSwA8aDahXmLZAaQevqi2eW5/wy4vPWi6PTkBbDfkyepJKYPvfmTexGyJEQp7Jsmh5Z
         5mgQ==
MIME-Version: 1.0
X-Received: by 10.182.60.42 with SMTP id e10mr2663395obr.33.1401646875187;
 Sun, 01 Jun 2014 11:21:15 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Sun, 1 Jun 2014 11:21:15 -0700 (PDT)
In-Reply-To: <CABPQxstwoe4ty6qQ-QNYcNrM14UFRVFUdF0zzmXq+S2KM9TnXA@mail.gmail.com>
References: <CAJ3iqPT2p_-FEsU_3HfM7Vq8yjMSodDSLe23J-HxsfCfVNTxtA@mail.gmail.com>
	<CA+qbEUN_a+z1Qt-_qT210e6uy11c5jWgN6ogqrqu86N685OzAw@mail.gmail.com>
	<CABPQxstwoe4ty6qQ-QNYcNrM14UFRVFUdF0zzmXq+S2KM9TnXA@mail.gmail.com>
Date: Sun, 1 Jun 2014 11:21:15 -0700
Message-ID: <CABPQxsvZT=t0ywPpwUnxsdmSJuk2OO156vkVTszxbP8PJMieUg@mail.gmail.com>
Subject: Re: SCALA_HOME or SCALA_LIBRARY_PATH not set during build
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I went ahead and created a JIRA for this and back ported the
improvement into branch-1.0. This wasn't a regression per-se because
the behavior existed in all previous versions, but it's annoying
behavior so best to fix it.

https://issues.apache.org/jira/browse/SPARK-1984

- Patrick

On Sun, Jun 1, 2014 at 11:13 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> This is a false error message actually - the Maven build no longer
> requires SCALA_HOME but the message/check was still there. This was
> fixed recently in master:
>
> https://github.com/apache/spark/commit/d8c005d5371f81a2a06c5d27c7021e1ae43d7193
>
> I can back port that fix into branch-1.0 so it will be in 1.0.1 as
> well. For other people running into this, you can export SCALA_HOME to
> any value and it will work.
>
> - Patrick
>
> On Sat, May 31, 2014 at 8:34 PM, Colin McCabe <cmccabe@alumni.cmu.edu> wrote:
>> Spark currently supports two build systems, sbt and maven.  sbt will
>> download the correct version of scala, but with Maven you need to supply it
>> yourself and set SCALA_HOME.
>>
>> It sounds like the instructions need to be updated-- perhaps create a JIRA?
>>
>> best,
>> Colin
>>
>>
>> On Sat, May 31, 2014 at 7:06 PM, Soren Macbeth <soren@yieldbot.com> wrote:
>>
>>> Hello,
>>>
>>> Following the instructions for building spark 1.0.0, I encountered the
>>> following error:
>>>
>>> [ERROR] Failed to execute goal
>>> org.apache.maven.plugins:maven-antrun-plugin:1.7:run (default) on project
>>> spark-core_2.10: An Ant BuildException has occured: Please set the
>>> SCALA_HOME (or SCALA_LIBRARY_PATH if scala is on the path) environment
>>> variables and retry.
>>> [ERROR] around Ant part ...<fail message="Please set the SCALA_HOME (or
>>> SCALA_LIBRARY_PATH if scala is on the path) environment variables and
>>> retry.">... @ 6:126 in
>>> /Users/soren/src/spark-1.0.0/core/target/antrun/build-main.xml
>>>
>>> No where in the documentation does it mention that having scala installed
>>> and either of these env vars set nor what version should be installed.
>>> Setting these env vars wasn't required for 0.9.1 with sbt.
>>>
>>> I was able to get past it by downloading the scala 2.10.4 binary package to
>>> a temp dir and setting SCALA_HOME to that dir.
>>>
>>> Ideally, it would be nice to not have to require people to have a
>>> standalone scala installation but at a minimum this requirement should be
>>> documented in the build instructions no?
>>>
>>> -Soren
>>>

From dev-return-7920-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun  1 18:23:30 2014
Return-Path: <dev-return-7920-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AA43B10828
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  1 Jun 2014 18:23:30 +0000 (UTC)
Received: (qmail 59927 invoked by uid 500); 1 Jun 2014 18:23:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59868 invoked by uid 500); 1 Jun 2014 18:23:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59860 invoked by uid 99); 1 Jun 2014 18:23:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Jun 2014 18:23:30 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.214.180] (HELO mail-ob0-f180.google.com) (209.85.214.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Jun 2014 18:23:27 +0000
Received: by mail-ob0-f180.google.com with SMTP id va2so3650065obc.25
        for <dev@spark.apache.org>; Sun, 01 Jun 2014 11:23:01 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=+lLy7tlP3tr4m9km5nv7dfLBrzTpT5rE+0P4vNSEPWc=;
        b=S1PTXWOVgpNNrkV8WU/UOINAkENvEorBr2073SuCZBeKOFoskNX+zC6M6X3DJgthUX
         5SaAHkN+4PcNHORdvZUGY5rwd8+bGWnsvvBjputfD/mrPH5f9nOIyZrApyJBBeHn/yQZ
         ZntqlBgFagWrDbJ0+yotiMb0Gqlw7XQ7524hFgXkciZtZgRlioJc9uA+SINnxRzq7XNE
         YP/1f0ESoFtB44yQRIWuV3AK3PHRVtDYh02KMqhcu2cmgdxlw/6OyhLID87YqDSl53Kr
         g9UabXgHrsuo4OmbEOTZZ9fuyq4BSjRxLBPGqFYUmg2VVVuXS5Yu0Xto0soKE4xweR73
         VE6g==
X-Gm-Message-State: ALoCoQksB9O5tCb4TjDY34WVaULIHF/gVicoHvDncqFG39/R6+KTkgMjUr6rmIAXS6x/XI7TsiW4
MIME-Version: 1.0
X-Received: by 10.182.121.170 with SMTP id ll10mr33761600obb.58.1401646981693;
 Sun, 01 Jun 2014 11:23:01 -0700 (PDT)
Received: by 10.182.184.40 with HTTP; Sun, 1 Jun 2014 11:23:01 -0700 (PDT)
In-Reply-To: <CABPQxsvZT=t0ywPpwUnxsdmSJuk2OO156vkVTszxbP8PJMieUg@mail.gmail.com>
References: <CAJ3iqPT2p_-FEsU_3HfM7Vq8yjMSodDSLe23J-HxsfCfVNTxtA@mail.gmail.com>
	<CA+qbEUN_a+z1Qt-_qT210e6uy11c5jWgN6ogqrqu86N685OzAw@mail.gmail.com>
	<CABPQxstwoe4ty6qQ-QNYcNrM14UFRVFUdF0zzmXq+S2KM9TnXA@mail.gmail.com>
	<CABPQxsvZT=t0ywPpwUnxsdmSJuk2OO156vkVTszxbP8PJMieUg@mail.gmail.com>
Date: Sun, 1 Jun 2014 11:23:01 -0700
Message-ID: <CAJ3iqPSGKpFjLA0Oji3zKHNTyx0Bkt4-Bp0L25nO24nbyBy7qw@mail.gmail.com>
Subject: Re: SCALA_HOME or SCALA_LIBRARY_PATH not set during build
From: Soren Macbeth <soren@yieldbot.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01536a5a71d84c04faca5e43
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01536a5a71d84c04faca5e43
Content-Type: text/plain; charset=UTF-8

Cheers, I didn't think it was needed, but just wanted to point it out.


On Sun, Jun 1, 2014 at 11:21 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> I went ahead and created a JIRA for this and back ported the
> improvement into branch-1.0. This wasn't a regression per-se because
> the behavior existed in all previous versions, but it's annoying
> behavior so best to fix it.
>
> https://issues.apache.org/jira/browse/SPARK-1984
>
> - Patrick
>
> On Sun, Jun 1, 2014 at 11:13 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > This is a false error message actually - the Maven build no longer
> > requires SCALA_HOME but the message/check was still there. This was
> > fixed recently in master:
> >
> >
> https://github.com/apache/spark/commit/d8c005d5371f81a2a06c5d27c7021e1ae43d7193
> >
> > I can back port that fix into branch-1.0 so it will be in 1.0.1 as
> > well. For other people running into this, you can export SCALA_HOME to
> > any value and it will work.
> >
> > - Patrick
> >
> > On Sat, May 31, 2014 at 8:34 PM, Colin McCabe <cmccabe@alumni.cmu.edu>
> wrote:
> >> Spark currently supports two build systems, sbt and maven.  sbt will
> >> download the correct version of scala, but with Maven you need to
> supply it
> >> yourself and set SCALA_HOME.
> >>
> >> It sounds like the instructions need to be updated-- perhaps create a
> JIRA?
> >>
> >> best,
> >> Colin
> >>
> >>
> >> On Sat, May 31, 2014 at 7:06 PM, Soren Macbeth <soren@yieldbot.com>
> wrote:
> >>
> >>> Hello,
> >>>
> >>> Following the instructions for building spark 1.0.0, I encountered the
> >>> following error:
> >>>
> >>> [ERROR] Failed to execute goal
> >>> org.apache.maven.plugins:maven-antrun-plugin:1.7:run (default) on
> project
> >>> spark-core_2.10: An Ant BuildException has occured: Please set the
> >>> SCALA_HOME (or SCALA_LIBRARY_PATH if scala is on the path) environment
> >>> variables and retry.
> >>> [ERROR] around Ant part ...<fail message="Please set the SCALA_HOME (or
> >>> SCALA_LIBRARY_PATH if scala is on the path) environment variables and
> >>> retry.">... @ 6:126 in
> >>> /Users/soren/src/spark-1.0.0/core/target/antrun/build-main.xml
> >>>
> >>> No where in the documentation does it mention that having scala
> installed
> >>> and either of these env vars set nor what version should be installed.
> >>> Setting these env vars wasn't required for 0.9.1 with sbt.
> >>>
> >>> I was able to get past it by downloading the scala 2.10.4 binary
> package to
> >>> a temp dir and setting SCALA_HOME to that dir.
> >>>
> >>> Ideally, it would be nice to not have to require people to have a
> >>> standalone scala installation but at a minimum this requirement should
> be
> >>> documented in the build instructions no?
> >>>
> >>> -Soren
> >>>
>

--089e01536a5a71d84c04faca5e43--

From dev-return-7921-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun  1 22:41:07 2014
Return-Path: <dev-return-7921-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A7E4B10C43
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  1 Jun 2014 22:41:07 +0000 (UTC)
Received: (qmail 43373 invoked by uid 500); 1 Jun 2014 22:41:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43317 invoked by uid 500); 1 Jun 2014 22:41:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43309 invoked by uid 99); 1 Jun 2014 22:41:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Jun 2014 22:41:07 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.214.170] (HELO mail-ob0-f170.google.com) (209.85.214.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Jun 2014 22:41:04 +0000
Received: by mail-ob0-f170.google.com with SMTP id uy5so3861670obc.1
        for <dev@spark.apache.org>; Sun, 01 Jun 2014 15:40:39 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=UdqqD/M61cv07R+yzWr76kj8SheAqjveUpMh5vJ8pMk=;
        b=ATWnAHHLMyCDM4K2X0pa3iNYyxHDwhtHcf4Mn6s3DI8KmRO1i3HxOOOuf+FZ+B6ajb
         ZI5oNVPUenSvBFIwHoGP2QKJcNlfYaM+KA+5mGCXsCVHhfZCy1+6gyz0BUJYR38rNyom
         TBqdpYltRc6drM9w2BrGseUA1/pf3Rl0Cb7msgS9WYM9h/8DbeB0ChHjH2b22nezWbx/
         tvM9RiwZbQd2CNlNcpr4uos7GdEaQlBfAFSqJDfBQCsBVZSYtVs7KfWPwW/kPIPV/9RX
         vTX6QGrxRuxya0T8vr5oW2abOAoIyyQlKJFxUpdKfzoRhtxQ/cu4h4FGPix6PsuqFwWh
         CXzQ==
X-Gm-Message-State: ALoCoQmmKoACTfEyiYRgdoxR+Z7vLAxTy3V9ifs9ZAm2fSazYWGrjMxYJJBVIIK43EslCqaczLex
MIME-Version: 1.0
X-Received: by 10.182.163.45 with SMTP id yf13mr34240462obb.66.1401662439225;
 Sun, 01 Jun 2014 15:40:39 -0700 (PDT)
Received: by 10.182.184.40 with HTTP; Sun, 1 Jun 2014 15:40:39 -0700 (PDT)
Date: Sun, 1 Jun 2014 15:40:39 -0700
Message-ID: <CAJ3iqPQ74sJqwtSi48v0QNjk8wsYthyJpcqR5-zFA97s9tXwaA@mail.gmail.com>
Subject: ClassTag in Serializer in 1.0.0 makes non-scala callers sad panda
From: Soren Macbeth <soren@yieldbot.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=e89a8f503b5cc90cb404facdf78d
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f503b5cc90cb404facdf78d
Content-Type: text/plain; charset=UTF-8

https://github.com/apache/spark/blob/v1.0.0/core/src/main/scala/org/apache/spark/serializer/Serializer.scala#L64-L66

These changes to the SerializerInstance make it really gross to call
serialize and deserialize from non-scala languages. I'm not sure what the
purpose of a ClassTag is, but if we could get some other arities that don't
require classtags that would help a ton.

--e89a8f503b5cc90cb404facdf78d--

From dev-return-7922-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun  1 23:25:02 2014
Return-Path: <dev-return-7922-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DF18710D96
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  1 Jun 2014 23:25:01 +0000 (UTC)
Received: (qmail 78757 invoked by uid 500); 1 Jun 2014 23:25:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78694 invoked by uid 500); 1 Jun 2014 23:25:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78685 invoked by uid 99); 1 Jun 2014 23:25:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Jun 2014 23:25:01 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.45 as permitted sender)
Received: from [209.85.220.45] (HELO mail-pa0-f45.google.com) (209.85.220.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Jun 2014 23:24:58 +0000
Received: by mail-pa0-f45.google.com with SMTP id ey11so3567417pad.4
        for <dev@spark.apache.org>; Sun, 01 Jun 2014 16:24:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=0y9C+3DZ+DyMYYyMudLeBEJydDoPTjeMaj2hKmILj3M=;
        b=KGK6vYxmA4uVr6KwpdsVcuWAY8D2+ZHfsCHZ3NsU2CoCsFwbp+1Gr8xEjX3JoYqy91
         +4nrFiR/0zD6A3dc+epxwRTv+f4I8KxT+EkaH2RQAt3G4RxUoYnkgCen+8K7d6ckahSc
         QiRUWVObW3QtIwkuRVnMNgx+V0aJeitqB69PUyl4F62hOk9TcZasmepDed8OwiHpEn+F
         U8YvtY2NVW+dmwmX/Ii9iaW/jZy53V27uafoNhRdm3l8eLo4iAzCAd2O1uaovt2S91Zc
         AH7lZm2FgY0ZP7SEIbmsWMLrAQtj9vYOTQKepsMKeJ6APMx6UXHmLoINspiMcEeOUf0Q
         Jwog==
X-Received: by 10.67.14.231 with SMTP id fj7mr35918999pad.115.1401665074224;
        Sun, 01 Jun 2014 16:24:34 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id iz2sm17136262pbb.95.2014.06.01.16.24.33
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 01 Jun 2014 16:24:33 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
Subject: Re: ClassTag in Serializer in 1.0.0 makes non-scala callers sad panda
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAJ3iqPQ74sJqwtSi48v0QNjk8wsYthyJpcqR5-zFA97s9tXwaA@mail.gmail.com>
Date: Sun, 1 Jun 2014 16:24:31 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <8CD217E6-50EB-40E3-8830-BDDA82431620@gmail.com>
References: <CAJ3iqPQ74sJqwtSi48v0QNjk8wsYthyJpcqR5-zFA97s9tXwaA@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.2)
X-Virus-Checked: Checked by ClamAV on apache.org

Why do you need to call Serializer from your own program? It=92s an =
internal developer API so ideally it would only be called to extend =
Spark. Are you looking to implement a custom Serializer?

Matei

On Jun 1, 2014, at 3:40 PM, Soren Macbeth <soren@yieldbot.com> wrote:

> =
https://github.com/apache/spark/blob/v1.0.0/core/src/main/scala/org/apache=
/spark/serializer/Serializer.scala#L64-L66
>=20
> These changes to the SerializerInstance make it really gross to call
> serialize and deserialize from non-scala languages. I'm not sure what =
the
> purpose of a ClassTag is, but if we could get some other arities that =
don't
> require classtags that would help a ton.


From dev-return-7923-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun  1 23:26:07 2014
Return-Path: <dev-return-7923-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F04D510D9B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  1 Jun 2014 23:26:06 +0000 (UTC)
Received: (qmail 81111 invoked by uid 500); 1 Jun 2014 23:26:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81052 invoked by uid 500); 1 Jun 2014 23:26:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81044 invoked by uid 99); 1 Jun 2014 23:26:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Jun 2014 23:26:06 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.42 as permitted sender)
Received: from [209.85.160.42] (HELO mail-pb0-f42.google.com) (209.85.160.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Jun 2014 23:26:02 +0000
Received: by mail-pb0-f42.google.com with SMTP id md12so3606505pbc.1
        for <dev@spark.apache.org>; Sun, 01 Jun 2014 16:25:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=oE1ulIlccpOhoaegJ81Zji+YZ2X3SCAvYxXaeXg73wk=;
        b=O83Ems/Ub66c5cj9R5zRaRR5vCcRbaI1Emi12bjgWzvFM6ytEaYbk2JNzWncqDuqas
         HJbMhpWy++ylef3iEEV0FktiP22LccHksYNyCwUXWfrYYZCD0pHu4Pl7MyGrsOg79HkY
         UT8yFJsNV0O1wP4YrD5FGzE2LtzEAMtCbh8iG8U1n9OH/Zf5NYkmKzZCn/ARHm0M2h3H
         I5yax8HoEsCy5BZr5innMEhn4hM2WflLt9ic6x/DZLt6eM9f4ynUotKW9MY2HFtxSajz
         fNRK2h1bDpDd2srZfOtXN6jq4rxQcjKNNrGB7Ob6+XhNJaHFoG/Mtlu4cSGviFBO9ApR
         dvMQ==
X-Received: by 10.68.222.196 with SMTP id qo4mr36169607pbc.14.1401665141765;
        Sun, 01 Jun 2014 16:25:41 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id x5sm17200213pbw.26.2014.06.01.16.25.39
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 01 Jun 2014 16:25:39 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
Subject: Re: ClassTag in Serializer in 1.0.0 makes non-scala callers sad panda
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <8CD217E6-50EB-40E3-8830-BDDA82431620@gmail.com>
Date: Sun, 1 Jun 2014 16:25:37 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <01DE19A2-D369-4116-9654-56F05FCDCC39@gmail.com>
References: <CAJ3iqPQ74sJqwtSi48v0QNjk8wsYthyJpcqR5-zFA97s9tXwaA@mail.gmail.com> <8CD217E6-50EB-40E3-8830-BDDA82431620@gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.2)
X-Virus-Checked: Checked by ClamAV on apache.org

BTW passing a ClassTag tells the Serializer what the type of object =
being serialized is when you compile your program, which will allow for =
more efficient serializers (especially on streams).

Matei

On Jun 1, 2014, at 4:24 PM, Matei Zaharia <matei.zaharia@gmail.com> =
wrote:

> Why do you need to call Serializer from your own program? It=92s an =
internal developer API so ideally it would only be called to extend =
Spark. Are you looking to implement a custom Serializer?
>=20
> Matei
>=20
> On Jun 1, 2014, at 3:40 PM, Soren Macbeth <soren@yieldbot.com> wrote:
>=20
>> =
https://github.com/apache/spark/blob/v1.0.0/core/src/main/scala/org/apache=
/spark/serializer/Serializer.scala#L64-L66
>>=20
>> These changes to the SerializerInstance make it really gross to call
>> serialize and deserialize from non-scala languages. I'm not sure what =
the
>> purpose of a ClassTag is, but if we could get some other arities that =
don't
>> require classtags that would help a ton.
>=20


From dev-return-7924-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun  1 23:33:36 2014
Return-Path: <dev-return-7924-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2499910DC2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  1 Jun 2014 23:33:35 +0000 (UTC)
Received: (qmail 91791 invoked by uid 500); 1 Jun 2014 23:33:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91732 invoked by uid 500); 1 Jun 2014 23:33:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91722 invoked by uid 99); 1 Jun 2014 23:33:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Jun 2014 23:33:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.214.181] (HELO mail-ob0-f181.google.com) (209.85.214.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Jun 2014 23:33:32 +0000
Received: by mail-ob0-f181.google.com with SMTP id wm4so3866461obc.40
        for <dev@spark.apache.org>; Sun, 01 Jun 2014 16:33:08 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=7PfQNoSxS6Go+dJnzjQuyb0jSs65CDCbyTnMNokWC8I=;
        b=gXXRe+ClBD07Yxh75iJoAUNXX0IOFsVAjF22YPFAH7v2/Ea6XghJNcE6V06JtdJ4Fb
         FX9ocK/5+kjA01etkmwZb0D9tQVb9oxu2Od2qz3lNvgF5473L0nli4RP77cPSs5sz4qW
         SBQYIos9kVZxhSyiLc/akPRTT5CctsOiyPftJQsWN4hLeNezmM4TVhusbqHtQZuSkp9f
         t4xTI2Un1jd7Q04euC59dZ0YA2A353P918hR8FAJqHnnX1A7N9udEbmYRhJ19ctIg8m/
         rikrOpgdgx5cPkUs1jY8Sy4VZTMkct8tMBMnmHmsaWvCdkbcu5Sal169vHgBIXhIfw/3
         8fjg==
X-Gm-Message-State: ALoCoQnxhw/IYIR5wAcOjVw5JPtB1XLm3C2bUfYhkhzMYU0BdKaXaKErv4kBjINbY0DPirUerVTe
MIME-Version: 1.0
X-Received: by 10.182.102.34 with SMTP id fl2mr19789299obb.57.1401665588544;
 Sun, 01 Jun 2014 16:33:08 -0700 (PDT)
Received: by 10.182.184.40 with HTTP; Sun, 1 Jun 2014 16:33:08 -0700 (PDT)
In-Reply-To: <01DE19A2-D369-4116-9654-56F05FCDCC39@gmail.com>
References: <CAJ3iqPQ74sJqwtSi48v0QNjk8wsYthyJpcqR5-zFA97s9tXwaA@mail.gmail.com>
	<8CD217E6-50EB-40E3-8830-BDDA82431620@gmail.com>
	<01DE19A2-D369-4116-9654-56F05FCDCC39@gmail.com>
Date: Sun, 1 Jun 2014 16:33:08 -0700
Message-ID: <CAJ3iqPRNW6wuFoGUVOpyLrArjHTMYGLG9jrOwpE5uC_DR-Y-vA@mail.gmail.com>
Subject: Re: ClassTag in Serializer in 1.0.0 makes non-scala callers sad panda
From: Soren Macbeth <soren@yieldbot.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e013d0d567fe00104faceb30f
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013d0d567fe00104faceb30f
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I'm writing a Clojure DSL for Spark. I use kryo to serialize my clojure
functions and for efficiency I hook into Spark's kryo serializer. In order
to do that I get a SerializerInstance from SparkEnv and call the serialize
and deserialize methods. I was able to workaround it by making ClassTag
object in clojure, but it's less than ideal.


On Sun, Jun 1, 2014 at 4:25 PM, Matei Zaharia <matei.zaharia@gmail.com>
wrote:

> BTW passing a ClassTag tells the Serializer what the type of object being
> serialized is when you compile your program, which will allow for more
> efficient serializers (especially on streams).
>
> Matei
>
> On Jun 1, 2014, at 4:24 PM, Matei Zaharia <matei.zaharia@gmail.com> wrote=
:
>
> > Why do you need to call Serializer from your own program? It=E2=80=99s =
an
> internal developer API so ideally it would only be called to extend Spark=
.
> Are you looking to implement a custom Serializer?
> >
> > Matei
> >
> > On Jun 1, 2014, at 3:40 PM, Soren Macbeth <soren@yieldbot.com> wrote:
> >
> >>
> https://github.com/apache/spark/blob/v1.0.0/core/src/main/scala/org/apach=
e/spark/serializer/Serializer.scala#L64-L66
> >>
> >> These changes to the SerializerInstance make it really gross to call
> >> serialize and deserialize from non-scala languages. I'm not sure what
> the
> >> purpose of a ClassTag is, but if we could get some other arities that
> don't
> >> require classtags that would help a ton.
> >
>
>

--089e013d0d567fe00104faceb30f--

From dev-return-7925-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun  2 00:10:48 2014
Return-Path: <dev-return-7925-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CBF5710EA4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  2 Jun 2014 00:10:48 +0000 (UTC)
Received: (qmail 60528 invoked by uid 500); 2 Jun 2014 00:10:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60383 invoked by uid 500); 2 Jun 2014 00:10:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60263 invoked by uid 99); 2 Jun 2014 00:10:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Jun 2014 00:10:48 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.176 as permitted sender)
Received: from [209.85.192.176] (HELO mail-pd0-f176.google.com) (209.85.192.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Jun 2014 00:10:44 +0000
Received: by mail-pd0-f176.google.com with SMTP id p10so2808622pdj.35
        for <dev@spark.apache.org>; Sun, 01 Jun 2014 17:10:20 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=bPaIBbDLMuE8i2dfDFw6aJLba6D+Veh1eJco/gBkzK0=;
        b=YJu2KZqWI+aqGRJhaz9QU7UpncBKRRvYwhZQUwwoVbBpurTXyCfCh0ro0d5HRldvga
         cIl4Ib/iqe6QATyp0z6Vva/OfdPcFWiTWlVYjMBa9jUhXKYBUH84iF0yDYx8FQbNyzca
         G1zAuWKscXdVFNbpEuVrAEkIRRuIcAsCFOzu3/scOVPWy4c1/Xz+lXSBkp/S0ChVKyaF
         XIysNfhplwzaRoGiJduY6wYhQDdPFoaKB7whPrrtAlPQx7Y/Hz20j+h1Wu9+55ZMCKXm
         pLpkFqUO7HwQqj3oYwI6nD5HLvY6jrXI+89/OCmRnUDMhwYhroDyUN2YxzQoLh20jFja
         Ghzg==
X-Received: by 10.68.202.167 with SMTP id kj7mr36683170pbc.160.1401667820191;
        Sun, 01 Jun 2014 17:10:20 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id pz10sm17286192pbb.33.2014.06.01.17.10.17
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 01 Jun 2014 17:10:18 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
Subject: Re: ClassTag in Serializer in 1.0.0 makes non-scala callers sad panda
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAJ3iqPRNW6wuFoGUVOpyLrArjHTMYGLG9jrOwpE5uC_DR-Y-vA@mail.gmail.com>
Date: Sun, 1 Jun 2014 17:10:16 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <BE0E1B2C-04C7-45F0-847F-1DB92EB1A6C3@gmail.com>
References: <CAJ3iqPQ74sJqwtSi48v0QNjk8wsYthyJpcqR5-zFA97s9tXwaA@mail.gmail.com> <8CD217E6-50EB-40E3-8830-BDDA82431620@gmail.com> <01DE19A2-D369-4116-9654-56F05FCDCC39@gmail.com> <CAJ3iqPRNW6wuFoGUVOpyLrArjHTMYGLG9jrOwpE5uC_DR-Y-vA@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.2)
X-Virus-Checked: Checked by ClamAV on apache.org

Ah, got it. In general it will always be safe to pass the ClassTag for =
java.lang.Object here =97 this is what our Java API does to say that =
type info is not known. So you can always pass that. Look at the Java =
code for how to get this ClassTag.

Matei

On Jun 1, 2014, at 4:33 PM, Soren Macbeth <soren@yieldbot.com> wrote:

> I'm writing a Clojure DSL for Spark. I use kryo to serialize my =
clojure
> functions and for efficiency I hook into Spark's kryo serializer. In =
order
> to do that I get a SerializerInstance from SparkEnv and call the =
serialize
> and deserialize methods. I was able to workaround it by making =
ClassTag
> object in clojure, but it's less than ideal.
>=20
>=20
> On Sun, Jun 1, 2014 at 4:25 PM, Matei Zaharia =
<matei.zaharia@gmail.com>
> wrote:
>=20
>> BTW passing a ClassTag tells the Serializer what the type of object =
being
>> serialized is when you compile your program, which will allow for =
more
>> efficient serializers (especially on streams).
>>=20
>> Matei
>>=20
>> On Jun 1, 2014, at 4:24 PM, Matei Zaharia <matei.zaharia@gmail.com> =
wrote:
>>=20
>>> Why do you need to call Serializer from your own program? It=92s an
>> internal developer API so ideally it would only be called to extend =
Spark.
>> Are you looking to implement a custom Serializer?
>>>=20
>>> Matei
>>>=20
>>> On Jun 1, 2014, at 3:40 PM, Soren Macbeth <soren@yieldbot.com> =
wrote:
>>>=20
>>>>=20
>> =
https://github.com/apache/spark/blob/v1.0.0/core/src/main/scala/org/apache=
/spark/serializer/Serializer.scala#L64-L66
>>>>=20
>>>> These changes to the SerializerInstance make it really gross to =
call
>>>> serialize and deserialize from non-scala languages. I'm not sure =
what
>> the
>>>> purpose of a ClassTag is, but if we could get some other arities =
that
>> don't
>>>> require classtags that would help a ton.
>>>=20
>>=20
>>=20


From dev-return-7926-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun  2 00:42:44 2014
Return-Path: <dev-return-7926-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6D5FE10F2C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  2 Jun 2014 00:42:44 +0000 (UTC)
Received: (qmail 75549 invoked by uid 500); 2 Jun 2014 00:42:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75497 invoked by uid 500); 2 Jun 2014 00:42:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75489 invoked by uid 99); 2 Jun 2014 00:42:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Jun 2014 00:42:44 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.214.173] (HELO mail-ob0-f173.google.com) (209.85.214.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Jun 2014 00:42:41 +0000
Received: by mail-ob0-f173.google.com with SMTP id wm4so3909865obc.18
        for <dev@spark.apache.org>; Sun, 01 Jun 2014 17:42:16 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=ehMlHA2wiuB/agbxo+G7gY6a3xJRG29bBo/phbKjQvA=;
        b=DakU5u45VOM9vIbNOg5lZAflvc8zINQVFWsBppHZfrAal3CCBCow7om/GN3WvFWpC8
         24J1VMMB9FqULYIvXQ4Hv4KX7ikp84K+GeAA/+JHPB7+tPXPC1PnrBK0bONRKTdNZ5Mh
         oNciqQyKNEHDDhYWV9FeI8/u1dhfSUg+r8EtNyKaeEk3cP87NHMZwoivC+a6m1zJy5p6
         2ngSHpRUHWRUg7o2forgsJ6AKXkv2o3sn0NXMRAooX6P8ZTXNrg5Kq1tfwYGY6RhmHgW
         b+Z9/Y1pg+ibRtwXc5WFpH1YnHFGzEwl/fmrV1aLUXj+71b92dcxthZh0E6vYHKHMywb
         VD3w==
X-Gm-Message-State: ALoCoQl9OCAIkVGwGZY+38Kf0O0yo+sdupH5rhyILTwEVda+tNa+pBWkzSCtX7Z0RNM18KKIBgOU
MIME-Version: 1.0
X-Received: by 10.182.76.38 with SMTP id h6mr34983087obw.8.1401669736575; Sun,
 01 Jun 2014 17:42:16 -0700 (PDT)
Received: by 10.182.184.40 with HTTP; Sun, 1 Jun 2014 17:42:16 -0700 (PDT)
In-Reply-To: <BE0E1B2C-04C7-45F0-847F-1DB92EB1A6C3@gmail.com>
References: <CAJ3iqPQ74sJqwtSi48v0QNjk8wsYthyJpcqR5-zFA97s9tXwaA@mail.gmail.com>
	<8CD217E6-50EB-40E3-8830-BDDA82431620@gmail.com>
	<01DE19A2-D369-4116-9654-56F05FCDCC39@gmail.com>
	<CAJ3iqPRNW6wuFoGUVOpyLrArjHTMYGLG9jrOwpE5uC_DR-Y-vA@mail.gmail.com>
	<BE0E1B2C-04C7-45F0-847F-1DB92EB1A6C3@gmail.com>
Date: Sun, 1 Jun 2014 17:42:16 -0700
Message-ID: <CAJ3iqPSNWddVp4Ci2V-J-myFtzNFJXV3divVJJQLfswSCoeqDQ@mail.gmail.com>
Subject: Re: ClassTag in Serializer in 1.0.0 makes non-scala callers sad panda
From: Soren Macbeth <soren@yieldbot.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b4503cabdca5a04facfaa3d
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b4503cabdca5a04facfaa3d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Yep, that's what I'm doing.

(def OBJECT-CLASS-TAG (.apply ClassTag$/MODULE$ java.lang.Object))

ps - I'm planning to open source this Clojure DSL soon as well


On Sun, Jun 1, 2014 at 5:10 PM, Matei Zaharia <matei.zaharia@gmail.com>
wrote:

> Ah, got it. In general it will always be safe to pass the ClassTag for
> java.lang.Object here =E2=80=94 this is what our Java API does to say tha=
t type
> info is not known. So you can always pass that. Look at the Java code for
> how to get this ClassTag.
>
> Matei
>
> On Jun 1, 2014, at 4:33 PM, Soren Macbeth <soren@yieldbot.com> wrote:
>
> > I'm writing a Clojure DSL for Spark. I use kryo to serialize my clojure
> > functions and for efficiency I hook into Spark's kryo serializer. In
> order
> > to do that I get a SerializerInstance from SparkEnv and call the
> serialize
> > and deserialize methods. I was able to workaround it by making ClassTag
> > object in clojure, but it's less than ideal.
> >
> >
> > On Sun, Jun 1, 2014 at 4:25 PM, Matei Zaharia <matei.zaharia@gmail.com>
> > wrote:
> >
> >> BTW passing a ClassTag tells the Serializer what the type of object
> being
> >> serialized is when you compile your program, which will allow for more
> >> efficient serializers (especially on streams).
> >>
> >> Matei
> >>
> >> On Jun 1, 2014, at 4:24 PM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
> >>
> >>> Why do you need to call Serializer from your own program? It=E2=80=99=
s an
> >> internal developer API so ideally it would only be called to extend
> Spark.
> >> Are you looking to implement a custom Serializer?
> >>>
> >>> Matei
> >>>
> >>> On Jun 1, 2014, at 3:40 PM, Soren Macbeth <soren@yieldbot.com> wrote:
> >>>
> >>>>
> >>
> https://github.com/apache/spark/blob/v1.0.0/core/src/main/scala/org/apach=
e/spark/serializer/Serializer.scala#L64-L66
> >>>>
> >>>> These changes to the SerializerInstance make it really gross to call
> >>>> serialize and deserialize from non-scala languages. I'm not sure wha=
t
> >> the
> >>>> purpose of a ClassTag is, but if we could get some other arities tha=
t
> >> don't
> >>>> require classtags that would help a ton.
> >>>
> >>
> >>
>
>

--047d7b4503cabdca5a04facfaa3d--

From dev-return-7927-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun  2 00:50:52 2014
Return-Path: <dev-return-7927-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EBC5210F4B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  2 Jun 2014 00:50:51 +0000 (UTC)
Received: (qmail 79951 invoked by uid 500); 2 Jun 2014 00:50:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79892 invoked by uid 500); 2 Jun 2014 00:50:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79884 invoked by uid 99); 2 Jun 2014 00:50:51 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Jun 2014 00:50:51 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.54 as permitted sender)
Received: from [209.85.220.54] (HELO mail-pa0-f54.google.com) (209.85.220.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Jun 2014 00:50:46 +0000
Received: by mail-pa0-f54.google.com with SMTP id lf10so3399388pab.27
        for <dev@spark.apache.org>; Sun, 01 Jun 2014 17:50:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=C2maFIQhfzOD/ml4U4aaAbdnBoPqMkFndM6+gJKIV/E=;
        b=FVfzM4TZvKgobWl6OfokvI2Mhcj0G0/vd0yYFW5gMeoV2kPcGUP2Ox3B+BE/gA6icn
         Kl89cDukJdmcwgEBhIhswv174MCypPLfbesLg0fKCOKDPPW0etBj0Zv61yeaJomQe3lK
         jAezDpfvJ6yMRLUZNcGix+CEEy5HZB+nLnGtVbNdgnH6vRTwNBkvMdkxw8dscCLmHPD/
         9hZ5wVXRI+zZTGBm+i5PzIa3pBTcUFylqVkl2EW3Qoptphl41Nkh4T1ZLtslJrW2yGRo
         uYOj4JFtl+K/ppNIx3/LC9gZi4csZ/JMduCYhHQCUJOYsx2fZafu+aDj1WSUPQI6tU8C
         jL6A==
X-Received: by 10.66.193.104 with SMTP id hn8mr36048339pac.99.1401670226404;
        Sun, 01 Jun 2014 17:50:26 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id ln2sm55521435pab.35.2014.06.01.17.50.23
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 01 Jun 2014 17:50:23 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
Subject: Re: ClassTag in Serializer in 1.0.0 makes non-scala callers sad panda
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAJ3iqPSNWddVp4Ci2V-J-myFtzNFJXV3divVJJQLfswSCoeqDQ@mail.gmail.com>
Date: Sun, 1 Jun 2014 17:50:23 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <5123AA62-8E91-4ABB-9627-E4AC6E5F5184@gmail.com>
References: <CAJ3iqPQ74sJqwtSi48v0QNjk8wsYthyJpcqR5-zFA97s9tXwaA@mail.gmail.com> <8CD217E6-50EB-40E3-8830-BDDA82431620@gmail.com> <01DE19A2-D369-4116-9654-56F05FCDCC39@gmail.com> <CAJ3iqPRNW6wuFoGUVOpyLrArjHTMYGLG9jrOwpE5uC_DR-Y-vA@mail.gmail.com> <BE0E1B2C-04C7-45F0-847F-1DB92EB1A6C3@gmail.com> <CAJ3iqPSNWddVp4Ci2V-J-myFtzNFJXV3divVJJQLfswSCoeqDQ@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.2)
X-Virus-Checked: Checked by ClamAV on apache.org

Very cool, looking forward to it!

Matei

On Jun 1, 2014, at 5:42 PM, Soren Macbeth <soren@yieldbot.com> wrote:

> Yep, that's what I'm doing.
>=20
> (def OBJECT-CLASS-TAG (.apply ClassTag$/MODULE$ java.lang.Object))
>=20
> ps - I'm planning to open source this Clojure DSL soon as well
>=20
>=20
> On Sun, Jun 1, 2014 at 5:10 PM, Matei Zaharia =
<matei.zaharia@gmail.com>
> wrote:
>=20
>> Ah, got it. In general it will always be safe to pass the ClassTag =
for
>> java.lang.Object here =97 this is what our Java API does to say that =
type
>> info is not known. So you can always pass that. Look at the Java code =
for
>> how to get this ClassTag.
>>=20
>> Matei
>>=20
>> On Jun 1, 2014, at 4:33 PM, Soren Macbeth <soren@yieldbot.com> wrote:
>>=20
>>> I'm writing a Clojure DSL for Spark. I use kryo to serialize my =
clojure
>>> functions and for efficiency I hook into Spark's kryo serializer. In
>> order
>>> to do that I get a SerializerInstance from SparkEnv and call the
>> serialize
>>> and deserialize methods. I was able to workaround it by making =
ClassTag
>>> object in clojure, but it's less than ideal.
>>>=20
>>>=20
>>> On Sun, Jun 1, 2014 at 4:25 PM, Matei Zaharia =
<matei.zaharia@gmail.com>
>>> wrote:
>>>=20
>>>> BTW passing a ClassTag tells the Serializer what the type of object
>> being
>>>> serialized is when you compile your program, which will allow for =
more
>>>> efficient serializers (especially on streams).
>>>>=20
>>>> Matei
>>>>=20
>>>> On Jun 1, 2014, at 4:24 PM, Matei Zaharia <matei.zaharia@gmail.com>
>> wrote:
>>>>=20
>>>>> Why do you need to call Serializer from your own program? It=92s =
an
>>>> internal developer API so ideally it would only be called to extend
>> Spark.
>>>> Are you looking to implement a custom Serializer?
>>>>>=20
>>>>> Matei
>>>>>=20
>>>>> On Jun 1, 2014, at 3:40 PM, Soren Macbeth <soren@yieldbot.com> =
wrote:
>>>>>=20
>>>>>>=20
>>>>=20
>> =
https://github.com/apache/spark/blob/v1.0.0/core/src/main/scala/org/apache=
/spark/serializer/Serializer.scala#L64-L66
>>>>>>=20
>>>>>> These changes to the SerializerInstance make it really gross to =
call
>>>>>> serialize and deserialize from non-scala languages. I'm not sure =
what
>>>> the
>>>>>> purpose of a ClassTag is, but if we could get some other arities =
that
>>>> don't
>>>>>> require classtags that would help a ton.
>>>>>=20
>>>>=20
>>>>=20
>>=20
>>=20


From dev-return-7928-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun  2 14:44:12 2014
Return-Path: <dev-return-7928-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 557A711269
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  2 Jun 2014 14:44:12 +0000 (UTC)
Received: (qmail 7266 invoked by uid 500); 2 Jun 2014 14:44:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7203 invoked by uid 500); 2 Jun 2014 14:44:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7193 invoked by uid 99); 2 Jun 2014 14:44:12 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Jun 2014 14:44:12 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Jun 2014 14:44:08 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <madhu@madhu.com>)
	id 1WrTSj-000110-2w
	for dev@spark.incubator.apache.org; Mon, 02 Jun 2014 07:43:45 -0700
Date: Mon, 2 Jun 2014 07:43:45 -0700 (PDT)
From: Madhu <madhu@madhu.com>
To: dev@spark.incubator.apache.org
Message-ID: <1401720225042-6908.post@n3.nabble.com>
Subject: Eclipse Scala IDE/Scala test and Wiki
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I was able to set up Spark in Eclipse using the Spark IDE plugin.
I also got unit tests running with Scala Test, which makes development quick
and easy.

I wanted to document the setup steps in this wiki page:

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-IDESetup

I can't seem to edit that page.
Confluence usually has a an "Edit" button in the upper right, but it does
not appear for me, even though I am logged in.

Am I missing something?



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Eclipse-Scala-IDE-Scala-test-and-Wiki-tp6908.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7929-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun  2 17:05:35 2014
Return-Path: <dev-return-7929-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 64E68118B9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  2 Jun 2014 17:05:35 +0000 (UTC)
Received: (qmail 35836 invoked by uid 500); 2 Jun 2014 17:05:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35778 invoked by uid 500); 2 Jun 2014 17:05:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35768 invoked by uid 99); 2 Jun 2014 17:05:34 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Jun 2014 17:05:34 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vanzin@cloudera.com designates 209.85.216.46 as permitted sender)
Received: from [209.85.216.46] (HELO mail-qa0-f46.google.com) (209.85.216.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Jun 2014 17:05:31 +0000
Received: by mail-qa0-f46.google.com with SMTP id w8so3050716qac.19
        for <dev@spark.apache.org>; Mon, 02 Jun 2014 10:05:07 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=p873sGDsVlUsb8iz9dUbsaLQ5QGYRwCd08TchyMn87A=;
        b=jC/jPt/1YSiee7Avus3n+4eXFZgP/vAK0CQEJZSPtaD/6KPnVOIPolN+L1pg0VpRaZ
         7aimPv2dmc8kN7tLzZqDrgfgMFEHM/XjWQmaSng/Ekeqb2qwwCpQ9uJv8jRAG/C0OJT/
         YWgMDLsc4XimlCWzqyfwk2H4/B9Jjb6Qy386ayINk5iJQlI8NOvf5I67dvkreMm6y0o6
         3EttdQlvyG9Y83HOIfTAUKk8xY8MmAO/sHF16OP53nqrlXOxWhFkHjlZqyy4RylCiS6Y
         ga510X57f/AJ6RnBVyAdfRGojs/MtGqdQy19SFdsqZ2xL4TmdVbZr4UzulzzEpVnKyzb
         tqDQ==
X-Gm-Message-State: ALoCoQnoGDR4W2nNsYwqz/N4YT4dmwjZoJC0Wd9E04IDAdSR7Q/EksrSQPmMCj4qwwA4m6r4Ak99
MIME-Version: 1.0
X-Received: by 10.224.36.141 with SMTP id t13mr48509028qad.75.1401728706932;
 Mon, 02 Jun 2014 10:05:06 -0700 (PDT)
Received: by 10.229.55.6 with HTTP; Mon, 2 Jun 2014 10:05:06 -0700 (PDT)
In-Reply-To: <CABPQxsuwLYpca1aX__Ceq6tCcpkh_be38iZ6WmTa+=59+aTV+Q@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
	<CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
	<CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
	<CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
	<CA+-p3AEL6ct-bJwkBSXm+wTLBHEbBoGnnGOJGAjWHvQogDs-4g@mail.gmail.com>
	<CALRHqP-jLhJM3_5qCTWzrbU+GpmVsd2DFtvMXsH4-PGuT6X8ww@mail.gmail.com>
	<75094F23-AACD-4427-AE5B-A25AC02189A8@gmail.com>
	<CAJiQeY+Sy4EUKhM8ztsSfxaNoOG_73b=s7W_u5YRBPk-JegUDA@mail.gmail.com>
	<CAAsvFPm9OeOd7v2wsevLpKdqaCjfQuAA21r8itVu4-c2vft2OA@mail.gmail.com>
	<CAJiQeYKWeKoZuY9MjQTxzHn5d5XwvHwo0e8sEuEWL6CFW4p6CA@mail.gmail.com>
	<CAJiQeYKAbeAp7ZQK7bbxGF3YhfqeOOnt5-p7QO8n3g97f=YCKg@mail.gmail.com>
	<CABPQxst3M2r5aLur41JB+bjK9U2s9V0GSbrF45xTsQ9rS2d7gA@mail.gmail.com>
	<CA+qbEUMAV=uZUCjAmgLMOjX3ccPcY=F6ade7TDtz5Lm92XJguA@mail.gmail.com>
	<CAAOnQ7vZGCQVYxZGS1+g5_2ZJkKxucVg2F-AaaeMYFruEq6VWQ@mail.gmail.com>
	<CABPQxsshMrxNrwwhQNiN4aRzSgLK_S558Y7_2cgLANVsz0ez_w@mail.gmail.com>
	<CA+qbEUNUj_Z-ssG0p8UL9yvyBQeZCdjifB2_M3PJE-E=QHLFtw@mail.gmail.com>
	<CABPQxsuwLYpca1aX__Ceq6tCcpkh_be38iZ6WmTa+=59+aTV+Q@mail.gmail.com>
Date: Mon, 2 Jun 2014 10:05:06 -0700
Message-ID: <CAAOnQ7sq-27Bd236ypYiLRdObOi2Eaz5vE52Z=-GFJPdAvZLTA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
From: Marcelo Vanzin <vanzin@cloudera.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Patrick,

Thanks for all the explanations, that makes sense. @DeveloperApi
worries me a little bit especially because of the things Colin
mentions - it's sort of hard to make people move off of APIs, or
support different versions of the same API. But maybe if expectations
(or lack thereof) are set up front, there will be less issues.

You mentioned something in your shading argument that kinda reminded
me of something. Spark currently depends on slf4j implementations and
log4j with "compile" scope. I'd argue that's the wrong approach if
we're talking about Spark being used embedded inside applications;
Spark should only depend on the slf4j API package, and let the
application provide the underlying implementation.

The assembly jars could include an implementation (since I assume
those are currently targeted at cluster deployment and not embedding).

That way there is less sources of conflict at runtime (i.e. the
"multiple implementation jars" messages you can see when running some
Spark programs).

On Fri, May 30, 2014 at 10:54 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> 2. Many libraries like logging subsystems, configuration systems, etc
> rely on static state and initialization. I'm not totally sure how e.g.
> slf4j initializes itself if you have both a shaded and non-shaded copy
> of slf4j present.

-- 
Marcelo

From dev-return-7930-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun  2 17:51:14 2014
Return-Path: <dev-return-7930-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5CA1A11AF5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  2 Jun 2014 17:51:14 +0000 (UTC)
Received: (qmail 79206 invoked by uid 500); 2 Jun 2014 17:51:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79147 invoked by uid 500); 2 Jun 2014 17:51:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79137 invoked by uid 99); 2 Jun 2014 17:51:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Jun 2014 17:51:13 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.128.173 as permitted sender)
Received: from [209.85.128.173] (HELO mail-ve0-f173.google.com) (209.85.128.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Jun 2014 17:51:11 +0000
Received: by mail-ve0-f173.google.com with SMTP id pa12so5607714veb.18
        for <dev@spark.apache.org>; Mon, 02 Jun 2014 10:50:47 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=aq2aA4XDzNnLtdR8F9upqjLJLovK4b414NPVhcMkM7k=;
        b=bbgcuM/h2pWb4i3B+EFtu6ZHGmuOagaGjAt3hWhs1Lio4mvM1tKbMSSx+rFy42O0Uu
         95/RM6UKphpzRv68GuMqwymH/1j+0AewEd0cjy3LJwwHGj5ueLxaCYHquCO9bYq5AT7m
         jViEYPButR2XjRXxNBmZS1voIwCexDcNwh68T56v7mjyctNrlYstrOW057s4hFC/z+X/
         Y4fOImzvigX/YvaNGxszwuZwoveI4mhOLBiIDeuGkcOnjyNoTTLC0xJk56UcSRS8GL+w
         vXvNGU1mPmDnVekUMPBLLbHjeCpQsERlmnpXdzEbTMZfJ7EaxyO3O2HKV+M86lqDKW2x
         Gqkg==
X-Gm-Message-State: ALoCoQnDORh4jHpn1e3QBl8grCl2vsSbwNGsTrjEZuUG3B/lfjmjQAe0sIOk0+UFtbcmOVjj2uxM
X-Received: by 10.58.196.231 with SMTP id ip7mr2427977vec.47.1401731447114;
 Mon, 02 Jun 2014 10:50:47 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.121.6 with HTTP; Mon, 2 Jun 2014 10:50:26 -0700 (PDT)
In-Reply-To: <CAAOnQ7sq-27Bd236ypYiLRdObOi2Eaz5vE52Z=-GFJPdAvZLTA@mail.gmail.com>
References: <CABPQxsuS1LuiXp3FfkcuidQ=J_dhp_kk7Bam-WD2tgOjyTonVw@mail.gmail.com>
 <CAAsvFPkj8k8c=EMcdNByoeZMwSN_7MZ05vko665an=XkrFzkmg@mail.gmail.com>
 <CAMAsSdJ9XMHE2owAk5rW4SxNtNXDo2gqXJAVvm9GygFW=hF0pQ@mail.gmail.com>
 <CAJiQeYJUmpdgUuTyxb_UkVGAim0pJqa7uR+Fu__Pz-1Xo5CvRQ@mail.gmail.com>
 <CA+-p3AEL6ct-bJwkBSXm+wTLBHEbBoGnnGOJGAjWHvQogDs-4g@mail.gmail.com>
 <CALRHqP-jLhJM3_5qCTWzrbU+GpmVsd2DFtvMXsH4-PGuT6X8ww@mail.gmail.com>
 <75094F23-AACD-4427-AE5B-A25AC02189A8@gmail.com> <CAJiQeY+Sy4EUKhM8ztsSfxaNoOG_73b=s7W_u5YRBPk-JegUDA@mail.gmail.com>
 <CAAsvFPm9OeOd7v2wsevLpKdqaCjfQuAA21r8itVu4-c2vft2OA@mail.gmail.com>
 <CAJiQeYKWeKoZuY9MjQTxzHn5d5XwvHwo0e8sEuEWL6CFW4p6CA@mail.gmail.com>
 <CAJiQeYKAbeAp7ZQK7bbxGF3YhfqeOOnt5-p7QO8n3g97f=YCKg@mail.gmail.com>
 <CABPQxst3M2r5aLur41JB+bjK9U2s9V0GSbrF45xTsQ9rS2d7gA@mail.gmail.com>
 <CA+qbEUMAV=uZUCjAmgLMOjX3ccPcY=F6ade7TDtz5Lm92XJguA@mail.gmail.com>
 <CAAOnQ7vZGCQVYxZGS1+g5_2ZJkKxucVg2F-AaaeMYFruEq6VWQ@mail.gmail.com>
 <CABPQxsshMrxNrwwhQNiN4aRzSgLK_S558Y7_2cgLANVsz0ez_w@mail.gmail.com>
 <CA+qbEUNUj_Z-ssG0p8UL9yvyBQeZCdjifB2_M3PJE-E=QHLFtw@mail.gmail.com>
 <CABPQxsuwLYpca1aX__Ceq6tCcpkh_be38iZ6WmTa+=59+aTV+Q@mail.gmail.com> <CAAOnQ7sq-27Bd236ypYiLRdObOi2Eaz5vE52Z=-GFJPdAvZLTA@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 2 Jun 2014 18:50:26 +0100
Message-ID: <CAMAsSdJ5u6zHwsBgs16XHv9otS_wkiQMg8S4_9-OzXor+HYmCg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (rc5)
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Mon, Jun 2, 2014 at 6:05 PM, Marcelo Vanzin <vanzin@cloudera.com> wrote:
> You mentioned something in your shading argument that kinda reminded
> me of something. Spark currently depends on slf4j implementations and
> log4j with "compile" scope. I'd argue that's the wrong approach if
> we're talking about Spark being used embedded inside applications;
> Spark should only depend on the slf4j API package, and let the
> application provide the underlying implementation.

Good idea in general; in practice, the drawback is that you can't do
things like set log levels if you only depend on the SLF4J API. There
are a few cases where that's nice to control, and that's only possible
if you bind to a particular logger as well.

You typically bundle a SLF4J binding anyway, to give a default, or
else the end-user has to know to also bind some SLF4J logger to get
output. Of course it does make for a bit more surgery if you want to
override the binding this way.

Shading can bring a whole new level of confusion; I myself would only
use it where essential as a workaround. Same with trying to make more
elaborate custom classloading schemes -- never in my darkest
nightmares have I imagine the failure modes that probably pop up when
that goes wrong. I think the library collisions will get better over
time as only later versions of Hadoop are in scope, for example,
and/or one build system is in play. I like tackling complexity along
those lines first.

From dev-return-7931-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun  3 00:31:20 2014
Return-Path: <dev-return-7931-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1BA0210E19
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Jun 2014 00:31:20 +0000 (UTC)
Received: (qmail 82549 invoked by uid 500); 3 Jun 2014 00:31:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82490 invoked by uid 500); 3 Jun 2014 00:31:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82479 invoked by uid 99); 3 Jun 2014 00:31:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 00:31:19 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 74.125.82.46 as permitted sender)
Received: from [74.125.82.46] (HELO mail-wg0-f46.google.com) (74.125.82.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 00:31:16 +0000
Received: by mail-wg0-f46.google.com with SMTP id n12so5840581wgh.17
        for <dev@spark.apache.org>; Mon, 02 Jun 2014 17:30:52 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=PysmNBXrGriaV20nD08XwDb9kQuUjMOOfJavJNi9Oj8=;
        b=EIqsqRpgY1GFKNUsUDb52aPV8fuVtow9O5Gk0/Hn9IAvhP6WEmvOqXDCWaIFByqPgE
         zpMvKv9Dl06Ge0xuvuJi43cyqxY3gJVS7TMWioDMNNyzVZj7bjcXG3LOxibqSD94+4xw
         I3/BE5SbPhEyIEVzBMfiXiYneJ/hTrxd9qi+hmaLlqSA2PKkxdLJcn/ldMMuGnmSXVob
         kxDOSeFmB72/dzF8IeQzIYuDOhHmw6XFG8+cAG3EuAsgqoEZldn4QtF8tq2D+xmX74ZE
         qeI6VHeLAjFUHcuI6JAkksSP8QCbpF7RaUecAg35nA0xtoGsDcB4GqxhvcI2jhCfl+6a
         tjkw==
MIME-Version: 1.0
X-Received: by 10.194.80.161 with SMTP id s1mr8363624wjx.47.1401755452713;
 Mon, 02 Jun 2014 17:30:52 -0700 (PDT)
Received: by 10.194.169.227 with HTTP; Mon, 2 Jun 2014 17:30:52 -0700 (PDT)
Date: Mon, 2 Jun 2014 17:30:52 -0700
Message-ID: <CAJgQjQ-Nrv+BcFocM78XPnuWvAG9ENFdCABjGx-ZJG2t7ZA+VQ@mail.gmail.com>
Subject: Which version does the binary compatibility test against by default?
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Is there a way to specify the target version? -Xiangrui

From dev-return-7932-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun  3 01:28:36 2014
Return-Path: <dev-return-7932-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B67B311016
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Jun 2014 01:28:36 +0000 (UTC)
Received: (qmail 64850 invoked by uid 500); 3 Jun 2014 01:28:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64793 invoked by uid 500); 3 Jun 2014 01:28:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64783 invoked by uid 99); 3 Jun 2014 01:28:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 01:28:36 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.47] (HELO mail-qa0-f47.google.com) (209.85.216.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 01:28:33 +0000
Received: by mail-qa0-f47.google.com with SMTP id s7so3847790qap.6
        for <dev@spark.apache.org>; Mon, 02 Jun 2014 18:28:09 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=0q5pvEx6ZPSkUnZbXyjT69WVl55zwUPCIPYsJ4SUBY8=;
        b=kbvDOaT/AQBC7CJyasHJ5DRg61nO21iOkdnRczsJawZ+dU7cZ2dPEs0xNIemHAhx6y
         rNwjITTl5ZBV3UG6zq+TzYAwJEVwRYd5D/kMDz7Hn7MGCRWxSWG1x+h5um6NIsDicmDp
         xJqlCgxsLYd5PF3oU3nCojeDhcFrxTFFwjbA2qM64U17fgJQ8xquXD6nrfQP9eFlqSRS
         pix+9ter5uEZ8dRGCvjViYZKZK7DzhIhZmLRUDnJd1XmlAUghfXRkgawTtCVglXN4ibc
         7hBthj2gwIYhVxIIEgTutZS/bywgHDVYOOy0WMBnvgPaEd2fnbTsmhiBLXZU+2f5Mzil
         jY1Q==
X-Gm-Message-State: ALoCoQmU0t1VjMvIlpkNt+exZxydX5qcRViLKJnEWMjxF49OeEAt57SafQjCg3ig/blMBAy4BR0T
X-Received: by 10.224.49.67 with SMTP id u3mr55196996qaf.63.1401758889540;
 Mon, 02 Jun 2014 18:28:09 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.49.231 with HTTP; Mon, 2 Jun 2014 18:27:49 -0700 (PDT)
In-Reply-To: <1401720225042-6908.post@n3.nabble.com>
References: <1401720225042-6908.post@n3.nabble.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 2 Jun 2014 18:27:49 -0700
Message-ID: <CAPh_B=YSkX27Q6LqYJMhVHWoZ9CaFXeZr-VUDYHy8WDoX8S-5A@mail.gmail.com>
Subject: Re: Eclipse Scala IDE/Scala test and Wiki
To: dev@spark.apache.org
Cc: Matei Zaharia <matei@databricks.com>
Content-Type: multipart/alternative; boundary=001a11c2ef80ac40c004fae46c89
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ef80ac40c004fae46c89
Content-Type: text/plain; charset=UTF-8

I tried but didn't find where I could add you. You probably need Matei to
help out with this.



On Mon, Jun 2, 2014 at 7:43 AM, Madhu <madhu@madhu.com> wrote:

> I was able to set up Spark in Eclipse using the Spark IDE plugin.
> I also got unit tests running with Scala Test, which makes development
> quick
> and easy.
>
> I wanted to document the setup steps in this wiki page:
>
>
> https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-IDESetup
>
> I can't seem to edit that page.
> Confluence usually has a an "Edit" button in the upper right, but it does
> not appear for me, even though I am logged in.
>
> Am I missing something?
>
>
>
> -----
> --
> Madhu
> https://www.linkedin.com/in/msiddalingaiah
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Eclipse-Scala-IDE-Scala-test-and-Wiki-tp6908.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c2ef80ac40c004fae46c89--

From dev-return-7933-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun  3 01:46:01 2014
Return-Path: <dev-return-7933-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6DA2311064
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Jun 2014 01:46:01 +0000 (UTC)
Received: (qmail 92945 invoked by uid 500); 3 Jun 2014 01:46:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92888 invoked by uid 500); 3 Jun 2014 01:46:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 77528 invoked by uid 99); 3 Jun 2014 01:33:22 -0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:content-type:mime-version:subject:from
         :in-reply-to:date:cc:message-id:references:to;
        bh=W1mP8tHViT+nR0SB7yhMUd/cGZkZ/xixIR7v6bNhZPA=;
        b=AIqugfAJZ9XMXPFG++rsUMWiUJbO7v/CqXSwFXiqMmXZvdBrICC2CefWvTX/wydjQY
         OofR/FQOf8zAPusBWIVj3I8t9YG2DgVczFEfEJJrF/uauGvMdk4wB+UbIM2Ihi+j2DfR
         q0JV2C0DsuaLz+z3MkY/W/FhNQdX881lagU3IqCYJeIkjQ1C+ZixMprnzrsOxM5sKzyD
         14pCCWUnGUdko/6m6+N6PA5qVp/tt5GaeSNwvK50XTRUNI49fLbjlFGMoCkutvntX+1n
         Z8V2jApKdJLTteFHSlr4CGR7b7AYjNwfjqFkXUJ2bVdkzPgP6/wLgWE548qKhdffxbK7
         5dqQ==
X-Gm-Message-State: ALoCoQm9Xo/rd1wsDQLX9wrE6WUVjze6ZGWh+pknFjvcJGZYaEjjNcRjYY1c6/4fooePH1wOsn2S
X-Received: by 10.68.135.42 with SMTP id pp10mr45533504pbb.58.1401759175559;
        Mon, 02 Jun 2014 18:32:55 -0700 (PDT)
Content-Type: multipart/alternative; boundary="Apple-Mail=_DC9FD232-66B2-468D-8732-43D50501AD44"
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
Subject: Re: Eclipse Scala IDE/Scala test and Wiki
From: Matei Zaharia <matei@databricks.com>
In-Reply-To: <CAPh_B=YSkX27Q6LqYJMhVHWoZ9CaFXeZr-VUDYHy8WDoX8S-5A@mail.gmail.com>
Date: Mon, 2 Jun 2014 18:32:52 -0700
Cc: dev@spark.apache.org
Message-Id: <EBB6278C-D731-480F-9256-FD1869CC81AF@databricks.com>
References: <1401720225042-6908.post@n3.nabble.com> <CAPh_B=YSkX27Q6LqYJMhVHWoZ9CaFXeZr-VUDYHy8WDoX8S-5A@mail.gmail.com>
To: madhu@madhu.com
X-Mailer: Apple Mail (2.1878.2)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_DC9FD232-66B2-468D-8732-43D50501AD44
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=us-ascii

Madhu, can you send me your Wiki username? (Sending it just to me is =
fine.) I can add you to the list to edit it.

Matei

On Jun 2, 2014, at 6:27 PM, Reynold Xin <rxin@databricks.com> wrote:

> I tried but didn't find where I could add you. You probably need Matei =
to help out with this.
>=20
>=20
>=20
> On Mon, Jun 2, 2014 at 7:43 AM, Madhu <madhu@madhu.com> wrote:
> I was able to set up Spark in Eclipse using the Spark IDE plugin.
> I also got unit tests running with Scala Test, which makes development =
quick
> and easy.
>=20
> I wanted to document the setup steps in this wiki page:
>=20
> =
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#Co=
ntributingtoSpark-IDESetup
>=20
> I can't seem to edit that page.
> Confluence usually has a an "Edit" button in the upper right, but it =
does
> not appear for me, even though I am logged in.
>=20
> Am I missing something?
>=20
>=20
>=20
> -----
> --
> Madhu
> https://www.linkedin.com/in/msiddalingaiah
> --
> View this message in context: =
http://apache-spark-developers-list.1001551.n3.nabble.com/Eclipse-Scala-ID=
E-Scala-test-and-Wiki-tp6908.html
> Sent from the Apache Spark Developers List mailing list archive at =
Nabble.com.
>=20


--Apple-Mail=_DC9FD232-66B2-468D-8732-43D50501AD44--

From dev-return-7934-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun  3 02:44:31 2014
Return-Path: <dev-return-7934-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0F52A11188
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Jun 2014 02:44:31 +0000 (UTC)
Received: (qmail 65878 invoked by uid 500); 3 Jun 2014 02:44:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65822 invoked by uid 500); 3 Jun 2014 02:44:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65811 invoked by uid 99); 3 Jun 2014 02:44:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 02:44:30 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.54 as permitted sender)
Received: from [209.85.219.54] (HELO mail-oa0-f54.google.com) (209.85.219.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 02:44:28 +0000
Received: by mail-oa0-f54.google.com with SMTP id j17so5644251oag.27
        for <dev@spark.apache.org>; Mon, 02 Jun 2014 19:44:04 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=op9Camh/i60rD9aiAQzvOhgYOFguqYYrTmq6D7qWtvY=;
        b=KsFyssaLvyrpu98qf1aq3wKXU7CakiTcd6jKtG+eRRE/CEWWTcw0JmstXTt73qJSKq
         PhxMB/R0HDbvdEQOfdZzXCCyGnSPHa+zL2Oy8ywNaMhlJ/JLV8AKKY6j/cLw2KjXwS6V
         5ZN/XkqER1brZv4bP2a//M4AFC6QOhd/WijJ+Tw2EiWgmZnRnnY0t4B4B2/erdqKl2bb
         WRd+BKPww288twZ2pTydxfzA+knRdUUinLaG6EoHhKKYZzRFoi0KjtsOJjWVqTqr0wNF
         iuMOIDllN8HrHIMNiFrmsK5AzWA7+DkYfMXo1InPdQkyFdaNzGQusu3ToauzDLKLTSgw
         AQRA==
MIME-Version: 1.0
X-Received: by 10.182.163.45 with SMTP id yf13mr8475865obb.66.1401763444169;
 Mon, 02 Jun 2014 19:44:04 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Mon, 2 Jun 2014 19:44:04 -0700 (PDT)
In-Reply-To: <CAJgQjQ-Nrv+BcFocM78XPnuWvAG9ENFdCABjGx-ZJG2t7ZA+VQ@mail.gmail.com>
References: <CAJgQjQ-Nrv+BcFocM78XPnuWvAG9ENFdCABjGx-ZJG2t7ZA+VQ@mail.gmail.com>
Date: Mon, 2 Jun 2014 19:44:04 -0700
Message-ID: <CABPQxstse=hzBmJpwu29A04ByiKUgzTOjZ2aN=BE7h_t2u2Cdw@mail.gmail.com>
Subject: Re: Which version does the binary compatibility test against by default?
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah - check out sparkPreviousArtifact in the build:
https://github.com/apache/spark/blob/master/project/SparkBuild.scala#L325

- Patrick

On Mon, Jun 2, 2014 at 5:30 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
> Is there a way to specify the target version? -Xiangrui

From dev-return-7935-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun  3 05:38:33 2014
Return-Path: <dev-return-7935-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C8FCE1154D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Jun 2014 05:38:33 +0000 (UTC)
Received: (qmail 84640 invoked by uid 500); 3 Jun 2014 05:38:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84583 invoked by uid 500); 3 Jun 2014 05:38:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84572 invoked by uid 99); 3 Jun 2014 05:38:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 05:38:33 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of nitinpanj@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 05:38:30 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <nitinpanj@gmail.com>)
	id 1WrhQE-0007lQ-BR
	for dev@spark.incubator.apache.org; Mon, 02 Jun 2014 22:38:06 -0700
Date: Mon, 2 Jun 2014 22:38:06 -0700 (PDT)
From: npanj <nitinpanj@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1401773886315-6915.post@n3.nabble.com>
Subject: Spark 1.1-snapshot: java.io.FileNotFoundException from
 ShuffleMapTask
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Quite often I notice that shuffle file is missing thus FileNotFoundException
is throws.
Any idea why shuffle file will be missing ? Am I running low in memory?
(I am using latest code from master branch on yarn-hadoop-2.2)

--
java.io.FileNotFoundException:
/var/storage/sda3/nm-local/usercache/npanj/appcache/application_1401394632504_0131/spark-local-20140603050956-6728/20/shuffle_0_2_97
(No such file or directory)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(FileOutputStream.java:221)
	at
org.apache.spark.storage.DiskBlockObjectWriter.open(BlockObjectWriter.scala:116)
	at
org.apache.spark.storage.DiskBlockObjectWriter.write(BlockObjectWriter.scala:177)
	at
org.apache.spark.scheduler.ShuffleMapTask$$anonfun$runTask$1.apply(ShuffleMapTask.scala:161)
	at
org.apache.spark.scheduler.ShuffleMapTask$$anonfun$runTask$1.apply(ShuffleMapTask.scala:158)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
	at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
	at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
--



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-1-1-snapshot-java-io-FileNotFoundException-from-ShuffleMapTask-tp6915.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7936-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun  3 06:22:06 2014
Return-Path: <dev-return-7936-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D8EC211650
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Jun 2014 06:22:06 +0000 (UTC)
Received: (qmail 75498 invoked by uid 500); 3 Jun 2014 06:22:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75432 invoked by uid 500); 3 Jun 2014 06:22:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75411 invoked by uid 99); 3 Jun 2014 06:22:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 06:22:06 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.54 as permitted sender)
Received: from [209.85.219.54] (HELO mail-oa0-f54.google.com) (209.85.219.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 06:22:00 +0000
Received: by mail-oa0-f54.google.com with SMTP id j17so5807637oag.27
        for <dev@spark.apache.org>; Mon, 02 Jun 2014 23:21:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=p+gI7BGK3E6IiqtlPZCn5I2ONqSghEQdcJNRzoYX6jg=;
        b=cDGBf0Xj5Ox0uTTyCuO6HnDUdOEPg9Y61C6+fXKXUFoed354kxcClKXjndJwU/yjaM
         PqzH2ZBW8Q6xS/9ARt96R3Q1A+d2crAY6uRyZ3DVyUEAixpLq6hkshD3Eu8/vJbUymFx
         X81YCB07zcfVWKff3moCCvWpo6jVhCXq4FBef6pQFzGH+/LIhzpyk9JpdJcZjiSGXedJ
         j0bXulLuk9btYZZusIfTgq3uAL57HzUaSMSdsXFHxz5emm2gKr0DEZTpdyQA+tYMK3CI
         nr7f7bSskElm9xhWh/1wcrY5WuRapbC19X2gHUSRUIo+h9UbBMIminwHdWMz94+PHf/k
         GKuQ==
MIME-Version: 1.0
X-Received: by 10.182.163.45 with SMTP id yf13mr9443792obb.66.1401776500383;
 Mon, 02 Jun 2014 23:21:40 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Mon, 2 Jun 2014 23:21:40 -0700 (PDT)
Date: Mon, 2 Jun 2014 23:21:40 -0700
Message-ID: <CABPQxsto5abq4+GbyaW6MhoWx5rZh6XvGY=a6r19Py-pReiOmw@mail.gmail.com>
Subject: Spark 1.1 Window and 1.0 Wrap-up
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey All,

I wanted to announce the the Spark 1.1 release window:
June 1 - Merge window opens
July 25 - Cut-off for new pull requests
August 1 - Merge window closes (code freeze), QA period starts
August 15+ - RC's and voting

This is consistent with the "3 month" release cycle we are targeting.
I'd really encourage people submitting larger features to do so during
the month of June, as features submitted closer to the window closing
could end up getting pushed into the next release.

I wanted to reflect a bit as well on the 1.0 release. First, thanks to
everyone who was involved in this release. It was the largest release
ever and it's something we should all be proud of.

In the 1.0 release, we cleaned up and consolidated several parts of the
Spark code base. In particular, we  consolidated the previously
fragmented process of submitting Spark jobs across a wide variety of
environments {YARN/Mesos/Standalone, Windows/Unix, Python/Java/Scala}.
We also brought the three language API's into much closer alignment.
These were difficult (but critical) tasks towards having a stable
deployment environment on which higher level libraries can build.

These cross-cutting changes also had associated test burden resulting
in an extended QA period. The 1.1, 1.2, 1.3, family of releases are
intended to be smaller releases, and I'd like to deliver them with
very predictable timing to the community. This will mean being fairly
strict about freezes and investing in QA infrastructure to allow us to
get through voting more quickly.

With 1.0 shipped, now is a great time to catch up on code reviews and look at
outstanding patches. Despite the large queue, we've actually been
consistently merging/closing about 80% of proposed PR's, which
is definitely good (for instance, we have 170 outstanding out of 950
proposed), but there remain a lot of people waiting on reviews, and
it's something everyone can help with!

Thanks again to everyone involved. Looking forward to more great releases!

- Patrick

From dev-return-7937-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun  3 06:36:32 2014
Return-Path: <dev-return-7937-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2B3C911688
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Jun 2014 06:36:32 +0000 (UTC)
Received: (qmail 94708 invoked by uid 500); 3 Jun 2014 06:36:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94646 invoked by uid 500); 3 Jun 2014 06:36:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94636 invoked by uid 99); 3 Jun 2014 06:36:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 06:36:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.128.178] (HELO mail-ve0-f178.google.com) (209.85.128.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 06:36:24 +0000
Received: by mail-ve0-f178.google.com with SMTP id sa20so6533491veb.9
        for <dev@spark.apache.org>; Mon, 02 Jun 2014 23:36:03 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=35PLecyW2bLeMaIl6FW3tI3NnMNf+hGBrp1vGgIRTsk=;
        b=WcAl1XnoyzHBksmJm+ncRLRfBUtW3CHGvy37EobY8dBG2rb7cy+Mc3X2EhUOlOlIuo
         Cy5WWeCacShxdi5yCmho61g7BpAQSSPBS7cKE6nzpHQSqF26Dz2PMIrJzqhVVA40cI4i
         YEfexXpn7nZHX8bkEZXgkUzi0B6QbTPCz4YQTQH4p0z+FajKouoG31VsG19AjaLB2kga
         X6RHraBevlQdXbFs6PNB8AeLQosNGIle4sWkGgdOhw1J9kkD9qZL0BahmKxETTqMDFjL
         Gp3QIaxQanfP7dHALwrrwJgcgT0bfaLCAHwbXgDrxfjurjzzt2UpWIhN4twnbs0/osAH
         zXsA==
X-Gm-Message-State: ALoCoQnmPPzmrQCovy+01h9D6ekBO2iquh9h6KN1WLa4bsE2r80WoTlm+eW5fY0igHjzcyl3l9R2
X-Received: by 10.58.161.101 with SMTP id xr5mr6314054veb.36.1401777363492;
        Mon, 02 Jun 2014 23:36:03 -0700 (PDT)
Received: from mail-vc0-f169.google.com (mail-vc0-f169.google.com [209.85.220.169])
        by mx.google.com with ESMTPSA id s16sm5077075vdi.5.2014.06.02.23.36.02
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 02 Jun 2014 23:36:02 -0700 (PDT)
Received: by mail-vc0-f169.google.com with SMTP id il7so2625510vcb.28
        for <dev@spark.apache.org>; Mon, 02 Jun 2014 23:36:01 -0700 (PDT)
X-Received: by 10.220.167.2 with SMTP id o2mr35100084vcy.8.1401777361847; Mon,
 02 Jun 2014 23:36:01 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Mon, 2 Jun 2014 23:35:41 -0700 (PDT)
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 2 Jun 2014 23:35:41 -0700
Message-ID: <CA+-p3AG8D_fWwEgU4cqNpY3_Sy3-3UHC47UHdbTJsk8YJ-fKng@mail.gmail.com>
Subject: Scala Language NPE
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e011618ceb50f3f04fae8b9da
X-Virus-Checked: Checked by ClamAV on apache.org

--089e011618ceb50f3f04fae8b9da
Content-Type: text/plain; charset=UTF-8

// observed in Spark 1.0

Scala devs,

I was observing an unusual NPE in my code recently, and came up with the
below minimal test case:

class Super extends Serializable {
    lazy val superVal: String = null
}

class Sub extends Super {
    lazy val subVal: String = {
        try {
            "literal"
        } catch {
            case _:Throwable => return superVal
        }
    }
}

Save this to a file, open the Spark shell, and load with ":l
/tmp/test.scala"

I got the below really unusual exception.  It goes away though when
removing the try/catch inside subVal and just returning either a straight
literal or superVal.


Is this a bug in Spark, or Scala, or my code, or what?  I think it might be
related to the Spark Repl doing magic but I'm unsure what.

Cheers!
Andrew




scala> :l /tmp/test.scala
Loading /tmp/test.scala...
defined class Super

     while compiling: <console>
        during phase: mixin
     library version: version 2.10.4
    compiler version: version 2.10.4
  reconstructed args:

  last tree to typer: Apply(constructor $read)
              symbol: constructor $read in class $read (flags: <method>
<triedcooking>)
   symbol definition: def <init>(): $line9.$read
                 tpe: $line9.$read
       symbol owners: constructor $read -> class $read -> package $line9
      context owners: class iwC$Sub -> package $line9

== Enclosing template or block ==

Template( // val <local Sub>: <notype>, tree.tpe=$line9.iwC$Sub
  "$line5.$read$$iwC$$iwC$$iwC$$iwC$Super" // parents
  ValDef(
    private
    "_"
    <tpt>
    <empty>
  )
  // 6 statements
  ValDef( // lazy private[this] var subVal: String
    private <mutable> <local> lazy <triedcooking>
    "subVal "
    <tpt> // tree.tpe=String
    <empty>
  )
  DefDef( // lazy val subVal(): String
    <method> <stable> <accessor> lazy
    "subVal"
    []
    List(Nil)
    <tpt> // tree.tpe=String
    Block( // tree.tpe=String
      ValDef( // val nonLocalReturnKey1: Object
        <synthetic> <triedcooking>
        "nonLocalReturnKey1"
        <tpt> // tree.tpe=Object
        Apply( // def <init>(): Object in class Object, tree.tpe=Object
          new Object."<init>" // def <init>(): Object in class Object,
tree.tpe=()Object
          Nil
        )
      )
      Try( // tree.tpe=String
        Block( // tree.tpe=String
          Assign( // tree.tpe=Unit
            $read$$iwC$$iwC$$iwC$$iwC$Sub.this."subVal " // lazy
private[this] var subVal: String, tree.tpe=String
            Block( // tree.tpe=String
              {}
              Apply( // final private[this] def
liftedTree1$1(nonLocalReturnKey1$1: Object): String, tree.tpe=String
                $read$$iwC$$iwC$$iwC$$iwC$Sub.this."liftedTree1$1" // final
private[this] def liftedTree1$1(nonLocalReturnKey1$1: Object): String,
tree.tpe=(nonLocalReturnKey1$1: Object)String
                "nonLocalReturnKey1" // val nonLocalReturnKey1: Object,
tree.tpe=Object
              )
            )
          )
          $read$$iwC$$iwC$$iwC$$iwC$Sub.this."subVal " // lazy
private[this] var subVal: String, tree.tpe=String
        )
        CaseDef( // tree.tpe=String
          Bind( // val ex: runtime.NonLocalReturnControl,
tree.tpe=runtime.NonLocalReturnControl
            "ex"
            Typed( // tree.tpe=runtime.NonLocalReturnControl
              "_" // tree.tpe=runtime.NonLocalReturnControl
              <tpt> // tree.tpe=runtime.NonLocalReturnControl
            )
          )
          If( // tree.tpe=String
            Apply( // final def eq(x$1: Object): Boolean in class Object,
tree.tpe=Boolean
              ex.key()."eq" // final def eq(x$1: Object): Boolean in class
Object, tree.tpe=(x$1: Object)Boolean
              "nonLocalReturnKey1" // val nonLocalReturnKey1: Object,
tree.tpe=Object
            )
            Apply( // final def $asInstanceOf[T0 >: ? <: ?](): T0 in class
Object, tree.tpe=String
              TypeApply( // final def $asInstanceOf[T0 >: ? <: ?](): T0 in
class Object, tree.tpe=()String
                ex.value()."$asInstanceOf" // final def $asInstanceOf[T0 >:
? <: ?](): T0 in class Object, tree.tpe=[T0 >: ? <: ?]()T0
                <tpt> // tree.tpe=String
              )
              Nil
            )
            Throw("ex")tree.tpe=Nothing
          )
        )
      )
    )
  )
  ValDef( // protected val $outer: $line9.iwC
    protected <synthetic> <paramaccessor> <triedcooking>
    "$outer "
    <tpt> // tree.tpe=$line9.iwC
    <empty>
  )
  DefDef( // val $outer(): $line9.iwC
    <method> <synthetic> <stable> <expandedname>
    "$line9$$read$$iwC$$iwC$$iwC$$iwC$Sub$$$outer"
    []
    List(Nil)
    <tpt> // tree.tpe=$line9.iwC
    $read$$iwC$$iwC$$iwC$$iwC$Sub.this."$outer " // protected val $outer:
$line9.iwC, tree.tpe=$line9.iwC
  )
  DefDef( // final private[this] def liftedTree1$1(nonLocalReturnKey1$1:
Object): String
    <method> private final <local> <lifted> <triedcooking>
    "liftedTree1"
    []
    // 1 parameter list
    ValDef( // nonLocalReturnKey1$1: Object
      <param> <synthetic>
      "nonLocalReturnKey1$1"
      <tpt> // tree.tpe=Object
      <empty>
    )
    <tpt> // tree.tpe=String
    Try( // tree.tpe=String
      "literal"
      CaseDef( // tree.tpe=Nothing
        Typed( // tree.tpe=Throwable
          "_" // tree.tpe=Throwable
          <tpt> // tree.tpe=Throwable
        )
        Throw( // tree.tpe=Nothing
          Apply( // def <init>(key: Object,value: Object):
scala.runtime.NonLocalReturnControl in class NonLocalReturnControl,
tree.tpe=scala.runtime.NonLocalReturnControl
            new runtime.NonLocalReturnControl."<init>" // def <init>(key:
Object,value: Object): scala.runtime.NonLocalReturnControl in class
NonLocalReturnControl, tree.tpe=(key: Object, value:
Object)scala.runtime.NonLocalReturnControl
            // 2 arguments
            "nonLocalReturnKey1$1" // nonLocalReturnKey1$1: Object,
tree.tpe=Object
            Apply( // lazy val superVal(): String, tree.tpe=String
              $read$$iwC$$iwC$$iwC$$iwC$Sub.this."superVal" // lazy val
superVal(): String, tree.tpe=()String
              Nil
            )
          )
        )
      )
    )
  )
  DefDef( // def <init>(arg$outer: $line9.iwC): $line9.iwC$Sub
    <method>
    "<init>"
    []
    // 1 parameter list
    ValDef( // $outer: $line9.iwC
      <param> <triedcooking>
      "$outer"
      <tpt> // tree.tpe=$line9.iwC
      <empty>
    )
    <tpt> // tree.tpe=$line9.iwC$Sub
    Block( // tree.tpe=Unit
      // 2 statements
      If( // tree.tpe=Unit
        Apply( // final def eq(x$1: Object): Boolean in class Object,
tree.tpe=Boolean
          "$outer"."eq" // final def eq(x$1: Object): Boolean in class
Object, tree.tpe=(x$1: Object)Boolean
          null
        )
        Throw( // tree.tpe=Nothing
          Apply( // def <init>(): NullPointerException in class
NullPointerException, tree.tpe=NullPointerException
            new NullPointerException."<init>" // def <init>():
NullPointerException in class NullPointerException,
tree.tpe=()NullPointerException
            Nil
          )
        )
        Assign( // tree.tpe=Unit
          $read$$iwC$$iwC$$iwC$$iwC$Sub.this."$outer " // protected val
$outer: $line9.iwC, tree.tpe=$line9.iwC
          "$outer" // $outer: $line9.iwC, tree.tpe=$line9.iwC
        )
      )
      Apply( // def <init>(arg$outer: $line5.iwC): $line5.iwC$Super,
tree.tpe=$line5.iwC$Super
        $read$$iwC$$iwC$$iwC$$iwC$Sub.super."<init>" // def
<init>(arg$outer: $line5.iwC): $line5.iwC$Super, tree.tpe=(arg$outer:
$line5.iwC)$line5.iwC$Super
        Apply( // val $iw(): $line5.iwC, tree.tpe=$line5.iwC

$outer.$line9$$read$$iwC$$iwC$$iwC$$iwC$$$outer().$VAL1().$iw().$iw().$iw()."$iw"
// val $iw(): $line5.iwC, tree.tpe=()$line5.iwC
          Nil
        )
      )
      ()
    )
  )
)

== Expanded type of tree ==

TypeRef(TypeSymbol(class $read extends Serializable))

unhandled exception while transforming <console>
error: uncaught exception during compilation: java.lang.NullPointerException
java.lang.NullPointerException
at scala.reflect.internal.Trees$class.Select(Trees.scala:1066)
 at scala.reflect.internal.SymbolTable.Select(SymbolTable.scala:13)
at
scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$scala$tools$nsc$transform$Mixin$MixinTransformer$$dd$1$2.apply(Mixin.scala:908)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$scala$tools$nsc$transform$Mixin$MixinTransformer$$dd$1$2.apply(Mixin.scala:904)
 at scala.reflect.internal.Trees$class.deriveDefDef(Trees.scala:1598)
at scala.reflect.internal.SymbolTable.deriveDefDef(SymbolTable.scala:13)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer.scala$tools$nsc$transform$Mixin$MixinTransformer$$dd$1(Mixin.scala:904)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$addCheckedGetters$1$1.apply(Mixin.scala:945)
at
scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$addCheckedGetters$1$1.apply(Mixin.scala:945)
 at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
 at scala.collection.immutable.List.foreach(List.scala:318)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
 at scala.collection.AbstractTraversable.map(Traversable.scala:105)
at
scala.tools.nsc.transform.Mixin$MixinTransformer.addCheckedGetters$1(Mixin.scala:945)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer.addNewDefs(Mixin.scala:1013)
at
scala.tools.nsc.transform.Mixin$MixinTransformer.scala$tools$nsc$transform$Mixin$MixinTransformer$$postTransform(Mixin.scala:1147)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$transform$1.apply(Mixin.scala:1261)
at
scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$transform$1.apply(Mixin.scala:1261)
 at scala.reflect.internal.SymbolTable.atPhase(SymbolTable.scala:207)
at scala.reflect.internal.SymbolTable.afterPhase(SymbolTable.scala:216)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:1261)
at
scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:471)
 at scala.reflect.api.Trees$Transformer.transformTemplate(Trees.scala:2904)
at
scala.reflect.internal.Trees$$anonfun$itransform$4.apply(Trees.scala:1280)
 at
scala.reflect.internal.Trees$$anonfun$itransform$4.apply(Trees.scala:1279)
at scala.reflect.api.Trees$Transformer.atOwner(Trees.scala:2936)
 at scala.reflect.internal.Trees$class.itransform(Trees.scala:1278)
at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:13)
 at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:13)
at scala.reflect.api.Trees$Transformer.transform(Trees.scala:2897)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:1258)
at
scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:471)
 at
scala.reflect.api.Trees$Transformer$$anonfun$transformStats$1.apply(Trees.scala:2927)
at
scala.reflect.api.Trees$Transformer$$anonfun$transformStats$1.apply(Trees.scala:2925)
 at scala.collection.immutable.List.loop$1(List.scala:170)
at scala.collection.immutable.List.mapConserve(List.scala:186)
 at scala.reflect.api.Trees$Transformer.transformStats(Trees.scala:2925)
at
scala.reflect.internal.Trees$$anonfun$itransform$7.apply(Trees.scala:1298)
 at
scala.reflect.internal.Trees$$anonfun$itransform$7.apply(Trees.scala:1298)
at scala.reflect.api.Trees$Transformer.atOwner(Trees.scala:2936)
 at scala.reflect.internal.Trees$class.itransform(Trees.scala:1297)
at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:13)
 at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:13)
at scala.reflect.api.Trees$Transformer.transform(Trees.scala:2897)
 at
scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:1258)
at
scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:471)
 at scala.tools.nsc.ast.Trees$Transformer.transformUnit(Trees.scala:227)
at scala.tools.nsc.transform.Transform$Phase.apply(Transform.scala:30)
 at scala.tools.nsc.Global$GlobalPhase.applyPhase(Global.scala:464)
at scala.tools.nsc.Global$GlobalPhase$$anonfun$run$1.apply(Global.scala:431)
 at
scala.tools.nsc.Global$GlobalPhase$$anonfun$run$1.apply(Global.scala:431)
at scala.collection.Iterator$class.foreach(Iterator.scala:727)
 at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
at scala.tools.nsc.Global$GlobalPhase.run(Global.scala:431)
 at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
 at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
at
org.apache.spark.repl.SparkIMain.compileSourcesKeepingRun(SparkIMain.scala:468)
 at
org.apache.spark.repl.SparkIMain$ReadEvalPrint.compileAndSaveRun(SparkIMain.scala:859)
at
org.apache.spark.repl.SparkIMain$ReadEvalPrint.compile(SparkIMain.scala:815)
 at
org.apache.spark.repl.SparkIMain$Request.compile$lzycompute(SparkIMain.scala:1009)
at org.apache.spark.repl.SparkIMain$Request.compile(SparkIMain.scala:1004)
 at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:644)
at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:609)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:796)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
at
org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
 at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:753)
at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:601)
 at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:608)
at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:611)
 at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$2.apply(SparkILoop.scala:621)
 at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$2.apply(SparkILoop.scala:618)
 at
scala.reflect.io.Streamable$Chars$class.applyReader(Streamable.scala:104)
at scala.reflect.io.File.applyReader(File.scala:82)
 at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(SparkILoop.scala:618)
 at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1$$anonfun$apply$mcV$sp$1.apply(SparkILoop.scala:618)
 at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1$$anonfun$apply$mcV$sp$1.apply(SparkILoop.scala:618)
at org.apache.spark.repl.SparkILoop.savingReplayStack(SparkILoop.scala:150)
 at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1.apply$mcV$sp(SparkILoop.scala:617)
at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1.apply(SparkILoop.scala:617)
 at
org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1.apply(SparkILoop.scala:617)
at org.apache.spark.repl.SparkILoop.savingReader(SparkILoop.scala:155)
 at org.apache.spark.repl.SparkILoop.interpretAllFrom(SparkILoop.scala:616)
at
org.apache.spark.repl.SparkILoop$$anonfun$loadCommand$1.apply(SparkILoop.scala:681)
 at
org.apache.spark.repl.SparkILoop$$anonfun$loadCommand$1.apply(SparkILoop.scala:680)
at org.apache.spark.repl.SparkILoop.withFile(SparkILoop.scala:674)
 at org.apache.spark.repl.SparkILoop.loadCommand(SparkILoop.scala:680)
at
org.apache.spark.repl.SparkILoop$$anonfun$standardCommands$7.apply(SparkILoop.scala:294)
 at
org.apache.spark.repl.SparkILoop$$anonfun$standardCommands$7.apply(SparkILoop.scala:294)
at
scala.tools.nsc.interpreter.LoopCommands$LineCmd.apply(LoopCommands.scala:81)
 at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:748)
at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:601)
 at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:608)
at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:611)
 at
org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:936)
at
org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:884)
 at
org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:884)
at
scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
 at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:884)
at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:983)
 at org.apache.spark.repl.Main$.main(Main.scala:31)
at org.apache.spark.repl.Main.main(Main.scala)
 at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
 at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
 at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:256)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:54)
 at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)


Abandoning crashed session.

scala>

--089e011618ceb50f3f04fae8b9da--

From dev-return-7938-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun  3 06:53:24 2014
Return-Path: <dev-return-7938-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 24C87116CA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Jun 2014 06:53:24 +0000 (UTC)
Received: (qmail 24220 invoked by uid 500); 3 Jun 2014 06:53:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24169 invoked by uid 500); 3 Jun 2014 06:53:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24159 invoked by uid 99); 3 Jun 2014 06:53:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 06:53:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.181] (HELO mail-vc0-f181.google.com) (209.85.220.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 06:53:18 +0000
Received: by mail-vc0-f181.google.com with SMTP id hq11so4289496vcb.40
        for <dev@spark.apache.org>; Mon, 02 Jun 2014 23:52:53 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=BA65GSbEzNnjTbZLsGb1FUcC9o5X5Yn+w0t0jlgkD0Q=;
        b=k0sBIB5tTpW33cJ3jaLoP6AZQmnOVDIuupN12Y6B50HVCIcoz+ADBUvTIPnL+/Ptma
         XoWGv2/WGeBv7vFECjo3Gcg0e8JBodTrqj6lxJZzzUvfV19T/Sdp/QGsS3eVxtffmFbr
         5vRECTtc+IKeda3pZDl66pf9/Ir3heJBhIF/Pe1S199kAzP+eD8Ij8zib+wstZvkC/ry
         IP0QJVJ72B2s1EJ8nvGY0PSVCuRHsQ4euP3J4gVeNVEJYLUnQYcK/7DopVtyXDdSAN1H
         55/XNaIjOhe4H6pfb82x3UmtW2HN6YSUZBr3Vzq9vp1w12R5mHjNBL38SnsJJxwObDmK
         CpSA==
X-Gm-Message-State: ALoCoQmOuR+vkZXLeYfqVHICD9ALCV1vtShGYEk90a3ife69BSwP4+f54v4XL3jNPvXAx9QoNLOV
X-Received: by 10.220.123.201 with SMTP id q9mr30234vcr.77.1401778373355;
        Mon, 02 Jun 2014 23:52:53 -0700 (PDT)
Received: from mail-vc0-f175.google.com (mail-vc0-f175.google.com [209.85.220.175])
        by mx.google.com with ESMTPSA id ij2sm26920679vdb.2.2014.06.02.23.52.52
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 02 Jun 2014 23:52:52 -0700 (PDT)
Received: by mail-vc0-f175.google.com with SMTP id id10so6336292vcb.34
        for <dev@spark.apache.org>; Mon, 02 Jun 2014 23:52:51 -0700 (PDT)
X-Received: by 10.58.74.164 with SMTP id u4mr19544vev.81.1401778371618; Mon,
 02 Jun 2014 23:52:51 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Mon, 2 Jun 2014 23:52:31 -0700 (PDT)
In-Reply-To: <CA+-p3AG8D_fWwEgU4cqNpY3_Sy3-3UHC47UHdbTJsk8YJ-fKng@mail.gmail.com>
References: <CA+-p3AG8D_fWwEgU4cqNpY3_Sy3-3UHC47UHdbTJsk8YJ-fKng@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 2 Jun 2014 23:52:31 -0700
Message-ID: <CA+-p3AHGZE_HmaBK7cPLpXZxZ1M2zxwTbf1nFhGuybB-Fa8ZJw@mail.gmail.com>
Subject: Re: Scala Language NPE
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b6dc1ece4ef2704fae8f551
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6dc1ece4ef2704fae8f551
Content-Type: text/plain; charset=UTF-8

Ah nevermind, the fix is to get rid of "return" from my method.  There's
probably a bug somewhere related to the repl taking bad input more cleanly,
but this isn't the end of the world once you figure out what the issue is.

Thanks for the time,
Andrew


On Mon, Jun 2, 2014 at 11:35 PM, Andrew Ash <andrew@andrewash.com> wrote:

> // observed in Spark 1.0
>
> Scala devs,
>
> I was observing an unusual NPE in my code recently, and came up with the
> below minimal test case:
>
> class Super extends Serializable {
>     lazy val superVal: String = null
> }
>
> class Sub extends Super {
>     lazy val subVal: String = {
>         try {
>             "literal"
>         } catch {
>             case _:Throwable => return superVal
>         }
>     }
> }
>
> Save this to a file, open the Spark shell, and load with ":l
> /tmp/test.scala"
>
> I got the below really unusual exception.  It goes away though when
> removing the try/catch inside subVal and just returning either a straight
> literal or superVal.
>
>
> Is this a bug in Spark, or Scala, or my code, or what?  I think it might
> be related to the Spark Repl doing magic but I'm unsure what.
>
> Cheers!
> Andrew
>
>
>
>
> scala> :l /tmp/test.scala
> Loading /tmp/test.scala...
> defined class Super
>
>      while compiling: <console>
>         during phase: mixin
>      library version: version 2.10.4
>     compiler version: version 2.10.4
>   reconstructed args:
>
>   last tree to typer: Apply(constructor $read)
>               symbol: constructor $read in class $read (flags: <method>
> <triedcooking>)
>    symbol definition: def <init>(): $line9.$read
>                  tpe: $line9.$read
>        symbol owners: constructor $read -> class $read -> package $line9
>        context owners: class iwC$Sub -> package $line9
>
> == Enclosing template or block ==
>
> Template( // val <local Sub>: <notype>, tree.tpe=$line9.iwC$Sub
>   "$line5.$read$$iwC$$iwC$$iwC$$iwC$Super" // parents
>   ValDef(
>     private
>     "_"
>     <tpt>
>     <empty>
>   )
>   // 6 statements
>   ValDef( // lazy private[this] var subVal: String
>     private <mutable> <local> lazy <triedcooking>
>     "subVal "
>     <tpt> // tree.tpe=String
>     <empty>
>   )
>   DefDef( // lazy val subVal(): String
>      <method> <stable> <accessor> lazy
>     "subVal"
>     []
>     List(Nil)
>     <tpt> // tree.tpe=String
>     Block( // tree.tpe=String
>       ValDef( // val nonLocalReturnKey1: Object
>         <synthetic> <triedcooking>
>         "nonLocalReturnKey1"
>         <tpt> // tree.tpe=Object
>         Apply( // def <init>(): Object in class Object, tree.tpe=Object
>           new Object."<init>" // def <init>(): Object in class Object,
> tree.tpe=()Object
>           Nil
>         )
>       )
>       Try( // tree.tpe=String
>         Block( // tree.tpe=String
>           Assign( // tree.tpe=Unit
>             $read$$iwC$$iwC$$iwC$$iwC$Sub.this."subVal " // lazy
> private[this] var subVal: String, tree.tpe=String
>             Block( // tree.tpe=String
>               {}
>               Apply( // final private[this] def
> liftedTree1$1(nonLocalReturnKey1$1: Object): String, tree.tpe=String
>                 $read$$iwC$$iwC$$iwC$$iwC$Sub.this."liftedTree1$1" //
> final private[this] def liftedTree1$1(nonLocalReturnKey1$1: Object):
> String, tree.tpe=(nonLocalReturnKey1$1: Object)String
>                 "nonLocalReturnKey1" // val nonLocalReturnKey1: Object,
> tree.tpe=Object
>               )
>              )
>           )
>           $read$$iwC$$iwC$$iwC$$iwC$Sub.this."subVal " // lazy
> private[this] var subVal: String, tree.tpe=String
>         )
>         CaseDef( // tree.tpe=String
>           Bind( // val ex: runtime.NonLocalReturnControl,
> tree.tpe=runtime.NonLocalReturnControl
>             "ex"
>             Typed( // tree.tpe=runtime.NonLocalReturnControl
>               "_" // tree.tpe=runtime.NonLocalReturnControl
>               <tpt> // tree.tpe=runtime.NonLocalReturnControl
>             )
>           )
>           If( // tree.tpe=String
>             Apply( // final def eq(x$1: Object): Boolean in class Object,
> tree.tpe=Boolean
>               ex.key()."eq" // final def eq(x$1: Object): Boolean in class
> Object, tree.tpe=(x$1: Object)Boolean
>               "nonLocalReturnKey1" // val nonLocalReturnKey1: Object,
> tree.tpe=Object
>             )
>             Apply( // final def $asInstanceOf[T0 >: ? <: ?](): T0 in class
> Object, tree.tpe=String
>               TypeApply( // final def $asInstanceOf[T0 >: ? <: ?](): T0 in
> class Object, tree.tpe=()String
>                 ex.value()."$asInstanceOf" // final def $asInstanceOf[T0
> >: ? <: ?](): T0 in class Object, tree.tpe=[T0 >: ? <: ?]()T0
>                 <tpt> // tree.tpe=String
>               )
>               Nil
>             )
>             Throw("ex")tree.tpe=Nothing
>           )
>         )
>       )
>     )
>   )
>   ValDef( // protected val $outer: $line9.iwC
>     protected <synthetic> <paramaccessor> <triedcooking>
>     "$outer "
>     <tpt> // tree.tpe=$line9.iwC
>     <empty>
>   )
>   DefDef( // val $outer(): $line9.iwC
>     <method> <synthetic> <stable> <expandedname>
>     "$line9$$read$$iwC$$iwC$$iwC$$iwC$Sub$$$outer"
>     []
>     List(Nil)
>     <tpt> // tree.tpe=$line9.iwC
>     $read$$iwC$$iwC$$iwC$$iwC$Sub.this."$outer " // protected val $outer:
> $line9.iwC, tree.tpe=$line9.iwC
>   )
>   DefDef( // final private[this] def liftedTree1$1(nonLocalReturnKey1$1:
> Object): String
>     <method> private final <local> <lifted> <triedcooking>
>     "liftedTree1"
>     []
>     // 1 parameter list
>     ValDef( // nonLocalReturnKey1$1: Object
>       <param> <synthetic>
>       "nonLocalReturnKey1$1"
>       <tpt> // tree.tpe=Object
>       <empty>
>     )
>     <tpt> // tree.tpe=String
>     Try( // tree.tpe=String
>       "literal"
>       CaseDef( // tree.tpe=Nothing
>         Typed( // tree.tpe=Throwable
>           "_" // tree.tpe=Throwable
>           <tpt> // tree.tpe=Throwable
>         )
>         Throw( // tree.tpe=Nothing
>           Apply( // def <init>(key: Object,value: Object):
> scala.runtime.NonLocalReturnControl in class NonLocalReturnControl,
> tree.tpe=scala.runtime.NonLocalReturnControl
>             new runtime.NonLocalReturnControl."<init>" // def <init>(key:
> Object,value: Object): scala.runtime.NonLocalReturnControl in class
> NonLocalReturnControl, tree.tpe=(key: Object, value:
> Object)scala.runtime.NonLocalReturnControl
>             // 2 arguments
>             "nonLocalReturnKey1$1" // nonLocalReturnKey1$1: Object,
> tree.tpe=Object
>             Apply( // lazy val superVal(): String, tree.tpe=String
>               $read$$iwC$$iwC$$iwC$$iwC$Sub.this."superVal" // lazy val
> superVal(): String, tree.tpe=()String
>               Nil
>             )
>           )
>         )
>       )
>     )
>   )
>   DefDef( // def <init>(arg$outer: $line9.iwC): $line9.iwC$Sub
>     <method>
>     "<init>"
>     []
>     // 1 parameter list
>     ValDef( // $outer: $line9.iwC
>       <param> <triedcooking>
>       "$outer"
>       <tpt> // tree.tpe=$line9.iwC
>       <empty>
>     )
>     <tpt> // tree.tpe=$line9.iwC$Sub
>     Block( // tree.tpe=Unit
>       // 2 statements
>       If( // tree.tpe=Unit
>         Apply( // final def eq(x$1: Object): Boolean in class Object,
> tree.tpe=Boolean
>            "$outer"."eq" // final def eq(x$1: Object): Boolean in class
> Object, tree.tpe=(x$1: Object)Boolean
>           null
>         )
>         Throw( // tree.tpe=Nothing
>           Apply( // def <init>(): NullPointerException in class
> NullPointerException, tree.tpe=NullPointerException
>             new NullPointerException."<init>" // def <init>():
> NullPointerException in class NullPointerException,
> tree.tpe=()NullPointerException
>             Nil
>           )
>         )
>         Assign( // tree.tpe=Unit
>           $read$$iwC$$iwC$$iwC$$iwC$Sub.this."$outer " // protected val
> $outer: $line9.iwC, tree.tpe=$line9.iwC
>           "$outer" // $outer: $line9.iwC, tree.tpe=$line9.iwC
>         )
>       )
>       Apply( // def <init>(arg$outer: $line5.iwC): $line5.iwC$Super,
> tree.tpe=$line5.iwC$Super
>         $read$$iwC$$iwC$$iwC$$iwC$Sub.super."<init>" // def
> <init>(arg$outer: $line5.iwC): $line5.iwC$Super, tree.tpe=(arg$outer:
> $line5.iwC)$line5.iwC$Super
>         Apply( // val $iw(): $line5.iwC, tree.tpe=$line5.iwC
>
> $outer.$line9$$read$$iwC$$iwC$$iwC$$iwC$$$outer().$VAL1().$iw().$iw().$iw()."$iw"
> // val $iw(): $line5.iwC, tree.tpe=()$line5.iwC
>           Nil
>         )
>       )
>       ()
>     )
>   )
> )
>
> == Expanded type of tree ==
>
> TypeRef(TypeSymbol(class $read extends Serializable))
>
> unhandled exception while transforming <console>
> error: uncaught exception during compilation:
> java.lang.NullPointerException
> java.lang.NullPointerException
> at scala.reflect.internal.Trees$class.Select(Trees.scala:1066)
>  at scala.reflect.internal.SymbolTable.Select(SymbolTable.scala:13)
> at
> scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$scala$tools$nsc$transform$Mixin$MixinTransformer$$dd$1$2.apply(Mixin.scala:908)
>  at
> scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$scala$tools$nsc$transform$Mixin$MixinTransformer$$dd$1$2.apply(Mixin.scala:904)
>  at scala.reflect.internal.Trees$class.deriveDefDef(Trees.scala:1598)
> at scala.reflect.internal.SymbolTable.deriveDefDef(SymbolTable.scala:13)
>  at
> scala.tools.nsc.transform.Mixin$MixinTransformer.scala$tools$nsc$transform$Mixin$MixinTransformer$$dd$1(Mixin.scala:904)
>  at
> scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$addCheckedGetters$1$1.apply(Mixin.scala:945)
> at
> scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$addCheckedGetters$1$1.apply(Mixin.scala:945)
>  at
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
> at
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
>  at scala.collection.immutable.List.foreach(List.scala:318)
> at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
>  at scala.collection.AbstractTraversable.map(Traversable.scala:105)
> at
> scala.tools.nsc.transform.Mixin$MixinTransformer.addCheckedGetters$1(Mixin.scala:945)
>  at
> scala.tools.nsc.transform.Mixin$MixinTransformer.addNewDefs(Mixin.scala:1013)
> at
> scala.tools.nsc.transform.Mixin$MixinTransformer.scala$tools$nsc$transform$Mixin$MixinTransformer$$postTransform(Mixin.scala:1147)
>  at
> scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$transform$1.apply(Mixin.scala:1261)
> at
> scala.tools.nsc.transform.Mixin$MixinTransformer$$anonfun$transform$1.apply(Mixin.scala:1261)
>  at scala.reflect.internal.SymbolTable.atPhase(SymbolTable.scala:207)
> at scala.reflect.internal.SymbolTable.afterPhase(SymbolTable.scala:216)
>  at
> scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:1261)
> at
> scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:471)
>  at
> scala.reflect.api.Trees$Transformer.transformTemplate(Trees.scala:2904)
> at
> scala.reflect.internal.Trees$$anonfun$itransform$4.apply(Trees.scala:1280)
>  at
> scala.reflect.internal.Trees$$anonfun$itransform$4.apply(Trees.scala:1279)
> at scala.reflect.api.Trees$Transformer.atOwner(Trees.scala:2936)
>  at scala.reflect.internal.Trees$class.itransform(Trees.scala:1278)
> at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:13)
>  at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:13)
> at scala.reflect.api.Trees$Transformer.transform(Trees.scala:2897)
>  at
> scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:1258)
> at
> scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:471)
>  at
> scala.reflect.api.Trees$Transformer$$anonfun$transformStats$1.apply(Trees.scala:2927)
> at
> scala.reflect.api.Trees$Transformer$$anonfun$transformStats$1.apply(Trees.scala:2925)
>  at scala.collection.immutable.List.loop$1(List.scala:170)
> at scala.collection.immutable.List.mapConserve(List.scala:186)
>  at scala.reflect.api.Trees$Transformer.transformStats(Trees.scala:2925)
> at
> scala.reflect.internal.Trees$$anonfun$itransform$7.apply(Trees.scala:1298)
>  at
> scala.reflect.internal.Trees$$anonfun$itransform$7.apply(Trees.scala:1298)
> at scala.reflect.api.Trees$Transformer.atOwner(Trees.scala:2936)
>  at scala.reflect.internal.Trees$class.itransform(Trees.scala:1297)
> at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:13)
>  at scala.reflect.internal.SymbolTable.itransform(SymbolTable.scala:13)
> at scala.reflect.api.Trees$Transformer.transform(Trees.scala:2897)
>  at
> scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:1258)
> at
> scala.tools.nsc.transform.Mixin$MixinTransformer.transform(Mixin.scala:471)
>  at scala.tools.nsc.ast.Trees$Transformer.transformUnit(Trees.scala:227)
> at scala.tools.nsc.transform.Transform$Phase.apply(Transform.scala:30)
>  at scala.tools.nsc.Global$GlobalPhase.applyPhase(Global.scala:464)
> at
> scala.tools.nsc.Global$GlobalPhase$$anonfun$run$1.apply(Global.scala:431)
>  at
> scala.tools.nsc.Global$GlobalPhase$$anonfun$run$1.apply(Global.scala:431)
> at scala.collection.Iterator$class.foreach(Iterator.scala:727)
>  at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
> at scala.tools.nsc.Global$GlobalPhase.run(Global.scala:431)
>  at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
> at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
>  at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
> at
> org.apache.spark.repl.SparkIMain.compileSourcesKeepingRun(SparkIMain.scala:468)
>  at
> org.apache.spark.repl.SparkIMain$ReadEvalPrint.compileAndSaveRun(SparkIMain.scala:859)
> at
> org.apache.spark.repl.SparkIMain$ReadEvalPrint.compile(SparkIMain.scala:815)
>  at
> org.apache.spark.repl.SparkIMain$Request.compile$lzycompute(SparkIMain.scala:1009)
> at org.apache.spark.repl.SparkIMain$Request.compile(SparkIMain.scala:1004)
>  at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:644)
> at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:609)
>  at
> org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:796)
> at
> org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
>  at
> org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
> at
> org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
>  at
> org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
> at
> org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
>  at
> org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
> at
> org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
>  at
> org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
> at
> org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
>  at
> org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
> at
> org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
>  at
> org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
> at
> org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
>  at
> org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
> at
> org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
>  at
> org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:814)
> at
> org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
>  at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:753)
> at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:601)
>  at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:608)
> at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:611)
>  at
> org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$2.apply(SparkILoop.scala:621)
>  at
> org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1$$anonfun$apply$mcV$sp$1$$anonfun$apply$mcV$sp$2.apply(SparkILoop.scala:618)
>  at
> scala.reflect.io.Streamable$Chars$class.applyReader(Streamable.scala:104)
> at scala.reflect.io.File.applyReader(File.scala:82)
>  at
> org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(SparkILoop.scala:618)
>  at
> org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1$$anonfun$apply$mcV$sp$1.apply(SparkILoop.scala:618)
>  at
> org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1$$anonfun$apply$mcV$sp$1.apply(SparkILoop.scala:618)
> at org.apache.spark.repl.SparkILoop.savingReplayStack(SparkILoop.scala:150)
>  at
> org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1.apply$mcV$sp(SparkILoop.scala:617)
> at
> org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1.apply(SparkILoop.scala:617)
>  at
> org.apache.spark.repl.SparkILoop$$anonfun$interpretAllFrom$1.apply(SparkILoop.scala:617)
> at org.apache.spark.repl.SparkILoop.savingReader(SparkILoop.scala:155)
>  at
> org.apache.spark.repl.SparkILoop.interpretAllFrom(SparkILoop.scala:616)
> at
> org.apache.spark.repl.SparkILoop$$anonfun$loadCommand$1.apply(SparkILoop.scala:681)
>  at
> org.apache.spark.repl.SparkILoop$$anonfun$loadCommand$1.apply(SparkILoop.scala:680)
> at org.apache.spark.repl.SparkILoop.withFile(SparkILoop.scala:674)
>  at org.apache.spark.repl.SparkILoop.loadCommand(SparkILoop.scala:680)
> at
> org.apache.spark.repl.SparkILoop$$anonfun$standardCommands$7.apply(SparkILoop.scala:294)
>  at
> org.apache.spark.repl.SparkILoop$$anonfun$standardCommands$7.apply(SparkILoop.scala:294)
> at
> scala.tools.nsc.interpreter.LoopCommands$LineCmd.apply(LoopCommands.scala:81)
>  at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:748)
> at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:601)
>  at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:608)
> at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:611)
>  at
> org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:936)
> at
> org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:884)
>  at
> org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:884)
> at
> scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
>  at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:884)
> at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:983)
>  at org.apache.spark.repl.Main$.main(Main.scala:31)
> at org.apache.spark.repl.Main.main(Main.scala)
>  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
>  at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
> at java.lang.reflect.Method.invoke(Method.java:606)
>  at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:256)
> at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:54)
>  at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
>
>
> Abandoning crashed session.
>
> scala>
>

--047d7b6dc1ece4ef2704fae8f551--

From dev-return-7939-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun  3 16:20:46 2014
Return-Path: <dev-return-7939-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5410C10A04
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Jun 2014 16:20:46 +0000 (UTC)
Received: (qmail 18537 invoked by uid 500); 3 Jun 2014 16:20:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18484 invoked by uid 500); 3 Jun 2014 16:20:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18474 invoked by uid 99); 3 Jun 2014 16:20:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 16:20:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: unknown (athena.apache.org: error in processing during lookup of jdonahue@adobe.com)
Received: from [207.46.163.211] (HELO na01-bl2-obe.outbound.protection.outlook.com) (207.46.163.211)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 16:20:39 +0000
Received: from BLUPR02MB405.namprd02.prod.outlook.com (10.141.80.13) by
 BLUPR02MB405.namprd02.prod.outlook.com (10.141.80.13) with Microsoft SMTP
 Server (TLS) id 15.0.954.9; Tue, 3 Jun 2014 16:20:04 +0000
Received: from BLUPR02MB405.namprd02.prod.outlook.com ([10.141.80.13]) by
 BLUPR02MB405.namprd02.prod.outlook.com ([10.141.80.13]) with mapi id
 15.00.0954.000; Tue, 3 Jun 2014 16:20:04 +0000
From: Jim Donahue <jdonahue@adobe.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: sbt run with spark.ContextCleaner ERROR
Thread-Topic: sbt run with spark.ContextCleaner ERROR
Thread-Index: AQHPf0es3COhyiwmVkeqfWRPht1tLQ==
Date: Tue, 3 Jun 2014 16:20:03 +0000
Message-ID: <CFB343BC.146CC%jdonahue@adobe.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
user-agent: Microsoft-MacOutlook/14.4.2.140509
x-originating-ip: [71.198.217.9]
x-microsoft-antispam: BL:0;ACTION:Default;RISK:Low;SCL:0;SPMLVL:NotSpam;PCL:0;RULEID:
x-forefront-prvs: 02318D10FB
x-forefront-antispam-report: SFV:NSPM;SFS:(428001)(189002)(199002)(479174003)(4396001)(80022001)(83322001)(19580395003)(19580405001)(79102001)(92566001)(92726001)(77096999)(86362001)(54356999)(31966008)(81542001)(81342001)(74502001)(20776003)(66066001)(74662001)(83072002)(64706001)(15975445006)(101416001)(46102001)(76482001)(99396002)(99286001)(87936001)(21056001)(85852003)(2656002)(15202345003)(77982001)(50986999)(36756003)(16236675002);DIR:OUT;SFP:;SCL:1;SRVR:BLUPR02MB405;H:BLUPR02MB405.namprd02.prod.outlook.com;FPR:;MLV:sfv;PTR:InfoNoRecords;MX:1;A:1;LANG:en;
received-spf: None (: adobe.com does not designate permitted sender hosts)
authentication-results: spf=none (sender IP is )
 smtp.mailfrom=jdonahue@adobe.com; 
Content-Type: multipart/alternative;
	boundary="_000_CFB343BC146CCjdonahueadobecom_"
MIME-Version: 1.0
X-OriginatorOrg: adobe.com
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_CFB343BC146CCjdonahueadobecom_
Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

This was posted before, but I couldn't find an answer (http://mail-archives=
.apache.org/mod_mbox/incubator-spark-user/201405.mbox/%3C1399196458177-5304=
.post@n3.nabble.com%3E)


i use sbt to run my spark application, after the app completes, error
occurs:

14/05/04 17:32:28 INFO network.ConnectionManager: Selector thread was
interrupted!
14/05/04 17:32:28 ERROR spark.ContextCleaner: Error in cleaning thread
java.lang.InterruptedException
        at java.lang.Object.wait(Native Method)
        at java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:135)
        at
org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleani=
ng(ContextCleaner.scala:116)
        at org.apache.spark.ContextCleaner$$anon$3.run(ContextCleaner.scala=
:64)



Now I'm seeing it.

--_000_CFB343BC146CCjdonahueadobecom_--

From dev-return-7940-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun  3 17:27:05 2014
Return-Path: <dev-return-7940-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 97C6410CBC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Jun 2014 17:27:05 +0000 (UTC)
Received: (qmail 3268 invoked by uid 500); 3 Jun 2014 17:27:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3211 invoked by uid 500); 3 Jun 2014 17:27:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 3200 invoked by uid 99); 3 Jun 2014 17:27:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 17:27:05 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rarecactus@gmail.com designates 74.125.82.47 as permitted sender)
Received: from [74.125.82.47] (HELO mail-wg0-f47.google.com) (74.125.82.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 17:27:02 +0000
Received: by mail-wg0-f47.google.com with SMTP id x12so6997313wgg.18
        for <dev@spark.apache.org>; Tue, 03 Jun 2014 10:26:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=9drtJqV8QkMm0Dj9C7hnuOpQubVcjhhImOi0VmFKqrM=;
        b=De2XYk4RH3IqXEYDejU2N5TVexeOZMfFnPGLMtWM4r8NUs/cYrVN0nnU/hHY8c9eDm
         rizs2OoSoUvvPiddJW5iflE5Tk2Qmj77IXUcee13Sn9J3/P7fCq6SIs5f4aI36dBmj5s
         GwP5EipKcwuId6K3CTgq5xfBx2yPX+6m/8ODewmy5ZIrWpjc2p00PwqeCGBoUGcaGeIv
         ZvPLAyS2AJx8rQjxXnlR/44cr8eJhHUjPcIVkAosdMmeO5RtMfx40w5EpGEwLLNwEW8X
         ImCJ2HIAJiL7lv2KXchvSxL221QoW70bpBkhykjHo9F5byyeusToHkCTAxcHBgkQA1zg
         6gow==
MIME-Version: 1.0
X-Received: by 10.180.85.163 with SMTP id i3mr35318600wiz.14.1401816399264;
 Tue, 03 Jun 2014 10:26:39 -0700 (PDT)
Sender: rarecactus@gmail.com
Received: by 10.194.242.35 with HTTP; Tue, 3 Jun 2014 10:26:39 -0700 (PDT)
In-Reply-To: <CABPQxsvZT=t0ywPpwUnxsdmSJuk2OO156vkVTszxbP8PJMieUg@mail.gmail.com>
References: <CAJ3iqPT2p_-FEsU_3HfM7Vq8yjMSodDSLe23J-HxsfCfVNTxtA@mail.gmail.com>
	<CA+qbEUN_a+z1Qt-_qT210e6uy11c5jWgN6ogqrqu86N685OzAw@mail.gmail.com>
	<CABPQxstwoe4ty6qQ-QNYcNrM14UFRVFUdF0zzmXq+S2KM9TnXA@mail.gmail.com>
	<CABPQxsvZT=t0ywPpwUnxsdmSJuk2OO156vkVTszxbP8PJMieUg@mail.gmail.com>
Date: Tue, 3 Jun 2014 10:26:39 -0700
X-Google-Sender-Auth: GL6dau7WACZFbCpnzXx7ed44GiQ
Message-ID: <CA+qbEUPWJJVqy5=fwM77Ca4QAmzod=Xor3z6MeAciYx7F-esLw@mail.gmail.com>
Subject: Re: SCALA_HOME or SCALA_LIBRARY_PATH not set during build
From: Colin McCabe <cmccabe@alumni.cmu.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d0444eb0984c0d704faf1d0b0
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0444eb0984c0d704faf1d0b0
Content-Type: text/plain; charset=UTF-8

Cool.  Nice to not have to set this any more.

best,
Colin


On Sun, Jun 1, 2014 at 11:21 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> I went ahead and created a JIRA for this and back ported the
> improvement into branch-1.0. This wasn't a regression per-se because
> the behavior existed in all previous versions, but it's annoying
> behavior so best to fix it.
>
> https://issues.apache.org/jira/browse/SPARK-1984
>
> - Patrick
>
> On Sun, Jun 1, 2014 at 11:13 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > This is a false error message actually - the Maven build no longer
> > requires SCALA_HOME but the message/check was still there. This was
> > fixed recently in master:
> >
> >
> https://github.com/apache/spark/commit/d8c005d5371f81a2a06c5d27c7021e1ae43d7193
> >
> > I can back port that fix into branch-1.0 so it will be in 1.0.1 as
> > well. For other people running into this, you can export SCALA_HOME to
> > any value and it will work.
> >
> > - Patrick
> >
> > On Sat, May 31, 2014 at 8:34 PM, Colin McCabe <cmccabe@alumni.cmu.edu>
> wrote:
> >> Spark currently supports two build systems, sbt and maven.  sbt will
> >> download the correct version of scala, but with Maven you need to
> supply it
> >> yourself and set SCALA_HOME.
> >>
> >> It sounds like the instructions need to be updated-- perhaps create a
> JIRA?
> >>
> >> best,
> >> Colin
> >>
> >>
> >> On Sat, May 31, 2014 at 7:06 PM, Soren Macbeth <soren@yieldbot.com>
> wrote:
> >>
> >>> Hello,
> >>>
> >>> Following the instructions for building spark 1.0.0, I encountered the
> >>> following error:
> >>>
> >>> [ERROR] Failed to execute goal
> >>> org.apache.maven.plugins:maven-antrun-plugin:1.7:run (default) on
> project
> >>> spark-core_2.10: An Ant BuildException has occured: Please set the
> >>> SCALA_HOME (or SCALA_LIBRARY_PATH if scala is on the path) environment
> >>> variables and retry.
> >>> [ERROR] around Ant part ...<fail message="Please set the SCALA_HOME (or
> >>> SCALA_LIBRARY_PATH if scala is on the path) environment variables and
> >>> retry.">... @ 6:126 in
> >>> /Users/soren/src/spark-1.0.0/core/target/antrun/build-main.xml
> >>>
> >>> No where in the documentation does it mention that having scala
> installed
> >>> and either of these env vars set nor what version should be installed.
> >>> Setting these env vars wasn't required for 0.9.1 with sbt.
> >>>
> >>> I was able to get past it by downloading the scala 2.10.4 binary
> package to
> >>> a temp dir and setting SCALA_HOME to that dir.
> >>>
> >>> Ideally, it would be nice to not have to require people to have a
> >>> standalone scala installation but at a minimum this requirement should
> be
> >>> documented in the build instructions no?
> >>>
> >>> -Soren
> >>>
>

--f46d0444eb0984c0d704faf1d0b0--

From dev-return-7941-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun  3 17:59:52 2014
Return-Path: <dev-return-7941-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 98B2510E2C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Jun 2014 17:59:52 +0000 (UTC)
Received: (qmail 64235 invoked by uid 500); 3 Jun 2014 17:59:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64172 invoked by uid 500); 3 Jun 2014 17:59:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64158 invoked by uid 99); 3 Jun 2014 17:59:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 17:59:52 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of henry.saputra@gmail.com designates 74.125.82.177 as permitted sender)
Received: from [74.125.82.177] (HELO mail-we0-f177.google.com) (74.125.82.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 17:59:47 +0000
Received: by mail-we0-f177.google.com with SMTP id x48so7269409wes.36
        for <dev@spark.apache.org>; Tue, 03 Jun 2014 10:59:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=FbJ4d8iKX8ePJVP4Qlsm6LFDUybpeLSKdBAtobSqtqw=;
        b=OMPIHsl7heCNg/8RGri42uSmsRbFZqXham/ILbp49yI10D///djScLiXscRackQy/R
         g7HnurvM28jAv03+9qe2Ryfv18vz0kbPhFUtVj/zL8pj2XodlwZeL/SJGyN5kJtjycV/
         pqReUAJEg/suOqlDfclW5R/yKTcIUKO+LE1E1oHsS/7QE6vqXlecIzepf2YGboV8GJvd
         0X4FrtJ2vgxjB2VcuFPQZb9sVxuYXRX86XS36bcY4v1LUEWCO5Ylsl6l4A8/kTvyv2td
         St/dJEf4yIA9OB1tfNm79kuQcIxBnW2CssEuPPPOzp1m2w+PorKJUaeJ5jTY07mWqZ6F
         ghYg==
MIME-Version: 1.0
X-Received: by 10.194.133.1 with SMTP id oy1mr18461934wjb.87.1401818366151;
 Tue, 03 Jun 2014 10:59:26 -0700 (PDT)
Received: by 10.216.165.71 with HTTP; Tue, 3 Jun 2014 10:59:26 -0700 (PDT)
Date: Tue, 3 Jun 2014 10:59:26 -0700
Message-ID: <CALuGr6a0HUeQvVeMhF87kPK5-5ofZA5QvvhXZmJjm3DRt7nrQw@mail.gmail.com>
Subject: Removing spark-debugger.md file from master?
From: Henry Saputra <henry.saputra@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi All,

Seemed like the spark-debugger.md is no longer accurate (see
http://spark.apache.org/docs/latest/spark-debugger.html) and since it
was originally written Spark has evolved that makes the doc obsolete.

There are already work pending for new replay debugging (I could not
find the PR links for it) so I

With version control we could always reinstate the old doc if needed,
but as of today the doc is no longer reflect the current state of
Spark's RDD.

If no objection I could send PR to remove the md file in master.

Thoughts?

- Henry

From dev-return-7942-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun  3 18:12:31 2014
Return-Path: <dev-return-7942-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0A7B410ECF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Jun 2014 18:12:31 +0000 (UTC)
Received: (qmail 944 invoked by uid 500); 3 Jun 2014 18:12:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 885 invoked by uid 500); 3 Jun 2014 18:12:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 874 invoked by uid 99); 3 Jun 2014 18:12:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 18:12:30 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ankurdave@gmail.com designates 209.85.128.182 as permitted sender)
Received: from [209.85.128.182] (HELO mail-ve0-f182.google.com) (209.85.128.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 18:12:27 +0000
Received: by mail-ve0-f182.google.com with SMTP id sa20so7457368veb.13
        for <dev@spark.apache.org>; Tue, 03 Jun 2014 11:12:03 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=emL3tiZavXkNBSHEsuGOrUEP2k6CgfPckQMj7FzdBGQ=;
        b=otk3EwaR8vxfVwAfRvsawJZJ6lj/pcEjGsFCDC48oR2XU/2Lc7RTOSyrXvYGQ9haKV
         8hTWb4Obs9gRbKwpkN2uaqHUFjCSKUayBAXRehD9GiVyMFeViAQJrg/kwVWjzsU3Kl8Y
         7GTY4Fno4wcuDMaj/hYJk2O9v37iCLCgi5axj7l5wKmfBlNEymHxLYaKtnLlRJ/WEKhW
         6c1Ge+9JaU1yZ2isMsNXz9KqKzSNVnDCN62CznTxhCxFLpYmtKFDoDGvUDaqVQpJt/NV
         4WMH2eBYjUZDDKcTKk+m6eVIdoB7hkMGX9SOI7N+koKMyFM/ptjeR4/NnV0BsW1gZ1W3
         dRyg==
X-Received: by 10.58.126.4 with SMTP id mu4mr39491564veb.0.1401819123642; Tue,
 03 Jun 2014 11:12:03 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.99.36 with HTTP; Tue, 3 Jun 2014 11:11:43 -0700 (PDT)
In-Reply-To: <CALuGr6a0HUeQvVeMhF87kPK5-5ofZA5QvvhXZmJjm3DRt7nrQw@mail.gmail.com>
References: <CALuGr6a0HUeQvVeMhF87kPK5-5ofZA5QvvhXZmJjm3DRt7nrQw@mail.gmail.com>
From: Ankur Dave <ankurdave@gmail.com>
Date: Tue, 3 Jun 2014 11:11:43 -0700
Message-ID: <CAK1A71wJypXEF3FA1dgv4D11m=faFXRH=aVmZP0MgW0XD7NpWQ@mail.gmail.com>
Subject: Re: Removing spark-debugger.md file from master?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b6700afe776f804faf27218
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6700afe776f804faf27218
Content-Type: text/plain; charset=UTF-8

I agree, let's go ahead and remove it.

Ankur <http://www.ankurdave.com/>

--047d7b6700afe776f804faf27218--

From dev-return-7943-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun  3 18:25:53 2014
Return-Path: <dev-return-7943-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4BA1B10FB2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Jun 2014 18:25:53 +0000 (UTC)
Received: (qmail 38870 invoked by uid 500); 3 Jun 2014 18:25:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38813 invoked by uid 500); 3 Jun 2014 18:25:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38802 invoked by uid 99); 3 Jun 2014 18:25:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 18:25:52 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of henry.saputra@gmail.com designates 74.125.82.174 as permitted sender)
Received: from [74.125.82.174] (HELO mail-we0-f174.google.com) (74.125.82.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 18:25:48 +0000
Received: by mail-we0-f174.google.com with SMTP id k48so7256813wev.19
        for <dev@spark.apache.org>; Tue, 03 Jun 2014 11:25:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=pMUA6O301NsPDPdnna9hW0niKd0+PP5p1PNdS1O4PnU=;
        b=xRroGY7t5Up/tZmQRCg8LXIM/zkTlN7W2ws7/2CnfvJtOYwnX/8+BtO70x1qMhhPG+
         bL5j9gS1E4gthcFMpeorVWT1UmnXq2gRgyBoj2BRqGeUatu3+Je8KkFcNuolFVYJacnz
         GstOdWl6OyQvFsY3PCKRKjx0XSKsBpm8aueLAXTXdcQxJyJnxOsLQ94nRPWMlWZ2KykJ
         r+ekJJYBqKm+2lD7dIRTu/2/2sFqXtP6ECh7Fi2hDxFltFIlmF9FTd/5/2p0Z7iAMKup
         zxDVtOdvzXOUQfTjgV0jXpPnMXnVZLFTNy8Pidg0EBrgweWTnDTFYZRQMOh1L34ktGbl
         yrOg==
MIME-Version: 1.0
X-Received: by 10.194.133.1 with SMTP id oy1mr18692009wjb.87.1401819927542;
 Tue, 03 Jun 2014 11:25:27 -0700 (PDT)
Received: by 10.216.165.71 with HTTP; Tue, 3 Jun 2014 11:25:27 -0700 (PDT)
In-Reply-To: <CAK1A71wJypXEF3FA1dgv4D11m=faFXRH=aVmZP0MgW0XD7NpWQ@mail.gmail.com>
References: <CALuGr6a0HUeQvVeMhF87kPK5-5ofZA5QvvhXZmJjm3DRt7nrQw@mail.gmail.com>
	<CAK1A71wJypXEF3FA1dgv4D11m=faFXRH=aVmZP0MgW0XD7NpWQ@mail.gmail.com>
Date: Tue, 3 Jun 2014 11:25:27 -0700
Message-ID: <CALuGr6by6s9EVQ2JZiu3yS_1eFOJYe=hCvOWn-9NdqZ9TAbm3w@mail.gmail.com>
Subject: Re: Removing spark-debugger.md file from master?
From: Henry Saputra <henry.saputra@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Cool, thanks Ankur, sounds good. PR is coming.

- Henry

On Tue, Jun 3, 2014 at 11:11 AM, Ankur Dave <ankurdave@gmail.com> wrote:
> I agree, let's go ahead and remove it.
>
> Ankur <http://www.ankurdave.com/>

From dev-return-7944-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun  3 18:33:37 2014
Return-Path: <dev-return-7944-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A19DD11003
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Jun 2014 18:33:37 +0000 (UTC)
Received: (qmail 51465 invoked by uid 500); 3 Jun 2014 18:33:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51411 invoked by uid 500); 3 Jun 2014 18:33:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51400 invoked by uid 99); 3 Jun 2014 18:33:37 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 18:33:37 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of henry.saputra@gmail.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 18:33:33 +0000
Received: by mail-wi0-f175.google.com with SMTP id f8so7125835wiw.2
        for <dev@spark.apache.org>; Tue, 03 Jun 2014 11:33:10 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=7MFmLL1zZagmTwBf1IyGpS3RO4UqGqSrgOImBcMRjlo=;
        b=R0ESHFmwf6dn9GOrjSSQ0gVpl0xxVlWUOs7a0bMNmvVqnOuFnddAUfvl4PEcMrBCUe
         cWwktrtyXcQcVS/UrmteDzinMkIcbDZQjZSbWVj2DaKb0dKZdRF9X0EZfTxSAUXocjtw
         dwbuUxBdMKwdgdPuN8qUQcN7HPHyAWdLlDz6BvANREdpuJIheI0fnaZnOuqmjqb+J3k9
         oOup/b55wHp2lNU1lxLE7OkSLKnnQ+Z5jcTw4HEBDH5lW6TcRXSWH2swW2cwqIz92Hag
         a8mwZk5JmsHd8bqFhnSHiDKGEvSLj6YAeUuGfPxFSgWt7iPFSsJftGdKOytK6+5OSp2y
         /eBw==
MIME-Version: 1.0
X-Received: by 10.194.6.166 with SMTP id c6mr62853051wja.64.1401820390030;
 Tue, 03 Jun 2014 11:33:10 -0700 (PDT)
Received: by 10.216.165.71 with HTTP; Tue, 3 Jun 2014 11:33:09 -0700 (PDT)
Date: Tue, 3 Jun 2014 11:33:09 -0700
Message-ID: <CALuGr6ZxiNiY0M7FqYy8R6FAPwraLTWd3NZ1SMaQC9C2dq=Jrw@mail.gmail.com>
Subject: Add my JIRA username (hsaputra) to Spark's contributor's list
From: Henry Saputra <henry.saputra@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

Could someone with right karma kindly add my username (hsaputra) to
Spark's contributor list?

I was added before but somehow now I can no longer assign ticket to
myself nor update tickets I am working on.


Thanks,

- Henry

From dev-return-7945-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun  3 18:37:17 2014
Return-Path: <dev-return-7945-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5C9FA11032
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Jun 2014 18:37:17 +0000 (UTC)
Received: (qmail 63856 invoked by uid 500); 3 Jun 2014 18:37:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63799 invoked by uid 500); 3 Jun 2014 18:37:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63782 invoked by uid 99); 3 Jun 2014 18:37:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 18:37:17 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.41 as permitted sender)
Received: from [209.85.160.41] (HELO mail-pb0-f41.google.com) (209.85.160.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 18:37:12 +0000
Received: by mail-pb0-f41.google.com with SMTP id uo5so5882453pbc.0
        for <dev@spark.apache.org>; Tue, 03 Jun 2014 11:36:48 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=XTKk0jSMoSQxcSywe1BHE7Jyc/lEuuMd3X5Eq5zLV88=;
        b=TurVHXyegRF6JIbTA8zUEKer/aYf9A5tuSxoqaQ2OZONvKyXEzlCRvAiidxi0cjFUT
         SUIk3kCu8oMKO06yVfDgIsqCo+s2CFMbUAUjxWoyH+vbzxgAUxvwLV1xpJiOmuQ4nWQ8
         b4IQqEohdJIX0or1OEnE23f578nZSZMBI1L74bi3YcNx4Na/ZGZ+n2kWXgFP3jlr4+zb
         ClayGF2FJWLpvr5FpGOahCtZk1HwqNA6cUrO5g+y1U+DcUSzqkiqHektk0ujaZ9VocbO
         /0Zs7XEzycpcnBoVsUJLMR0RKMrsg/Qr6piEuzuE08TzD2Rc5kcwYgNK16je8rOZaxq/
         hvZA==
X-Received: by 10.68.249.2 with SMTP id yq2mr53221409pbc.70.1401820608047;
        Tue, 03 Jun 2014 11:36:48 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id ak1sm64081pbc.58.2014.06.03.11.36.44
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 03 Jun 2014 11:36:45 -0700 (PDT)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
Subject: Re: Add my JIRA username (hsaputra) to Spark's contributor's list
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CALuGr6ZxiNiY0M7FqYy8R6FAPwraLTWd3NZ1SMaQC9C2dq=Jrw@mail.gmail.com>
Date: Tue, 3 Jun 2014 11:36:42 -0700
Content-Transfer-Encoding: 7bit
Message-Id: <AF0583D4-1C80-40CB-BCCE-D9403ECA4534@gmail.com>
References: <CALuGr6ZxiNiY0M7FqYy8R6FAPwraLTWd3NZ1SMaQC9C2dq=Jrw@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.2)
X-Virus-Checked: Checked by ClamAV on apache.org

Done. Looks like this was lost in the JIRA import.

Matei

On Jun 3, 2014, at 11:33 AM, Henry Saputra <henry.saputra@gmail.com> wrote:

> Hi,
> 
> Could someone with right karma kindly add my username (hsaputra) to
> Spark's contributor list?
> 
> I was added before but somehow now I can no longer assign ticket to
> myself nor update tickets I am working on.
> 
> 
> Thanks,
> 
> - Henry


From dev-return-7946-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun  3 18:39:55 2014
Return-Path: <dev-return-7946-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C48111104E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Jun 2014 18:39:55 +0000 (UTC)
Received: (qmail 69906 invoked by uid 500); 3 Jun 2014 18:39:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69852 invoked by uid 500); 3 Jun 2014 18:39:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69841 invoked by uid 99); 3 Jun 2014 18:39:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 18:39:55 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of henry.saputra@gmail.com designates 209.85.212.176 as permitted sender)
Received: from [209.85.212.176] (HELO mail-wi0-f176.google.com) (209.85.212.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 18:39:50 +0000
Received: by mail-wi0-f176.google.com with SMTP id n15so6989545wiw.9
        for <dev@spark.apache.org>; Tue, 03 Jun 2014 11:39:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=+Due8/YuEos4EEDVHPZkVMFPzlsYQ8pu8EXTbgN354Y=;
        b=Ap642GrpBI2AjRmrmpo27SSE/XEJilWFtWotCkPZaUji0gfmri76w/MLUJyL5i26kE
         Unjeqs51yCB5GSgexEtU6JwRGa1qSRIUYGLBOKQRjg29fCfev93dEIqjxcz+B14HZseT
         CDyrZdznIjuUPL4exP9gRyFAvJewwJb2TdYXzVohWbwcw+QWtxkbT6PNrIffaeXCWGKk
         JnGGBBqeZPi06XKVxT44/+Bxxr3DX3y0HObkv5vzgaoQp/fXIOysgnufzJLY86AImk0w
         70W6je/YUV0CCsF1fLgSgnH5MCLEwalt/cbJuhGpwN2xLIZDE0GVJ7f8oMphax6gG2R3
         +jHw==
MIME-Version: 1.0
X-Received: by 10.194.6.166 with SMTP id c6mr62908527wja.64.1401820769042;
 Tue, 03 Jun 2014 11:39:29 -0700 (PDT)
Received: by 10.216.165.71 with HTTP; Tue, 3 Jun 2014 11:39:29 -0700 (PDT)
In-Reply-To: <AF0583D4-1C80-40CB-BCCE-D9403ECA4534@gmail.com>
References: <CALuGr6ZxiNiY0M7FqYy8R6FAPwraLTWd3NZ1SMaQC9C2dq=Jrw@mail.gmail.com>
	<AF0583D4-1C80-40CB-BCCE-D9403ECA4534@gmail.com>
Date: Tue, 3 Jun 2014 11:39:29 -0700
Message-ID: <CALuGr6ZJA-SiabGuB-e1HSFHjm4n+XHGVAw49pVegoxRFiXnNw@mail.gmail.com>
Subject: Re: Add my JIRA username (hsaputra) to Spark's contributor's list
From: Henry Saputra <henry.saputra@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks Matei!

- Henry

On Tue, Jun 3, 2014 at 11:36 AM, Matei Zaharia <matei.zaharia@gmail.com> wrote:
> Done. Looks like this was lost in the JIRA import.
>
> Matei
>
> On Jun 3, 2014, at 11:33 AM, Henry Saputra <henry.saputra@gmail.com> wrote:
>
>> Hi,
>>
>> Could someone with right karma kindly add my username (hsaputra) to
>> Spark's contributor list?
>>
>> I was added before but somehow now I can no longer assign ticket to
>> myself nor update tickets I am working on.
>>
>>
>> Thanks,
>>
>> - Henry
>

From dev-return-7947-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun  3 19:00:34 2014
Return-Path: <dev-return-7947-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8FC0A1113D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Jun 2014 19:00:34 +0000 (UTC)
Received: (qmail 20914 invoked by uid 500); 3 Jun 2014 19:00:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20855 invoked by uid 500); 3 Jun 2014 19:00:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20844 invoked by uid 99); 3 Jun 2014 19:00:34 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 19:00:34 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of kanzhangemail@gmail.com designates 209.85.213.174 as permitted sender)
Received: from [209.85.213.174] (HELO mail-ig0-f174.google.com) (209.85.213.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 19:00:30 +0000
Received: by mail-ig0-f174.google.com with SMTP id h3so5335449igd.13
        for <dev@spark.apache.org>; Tue, 03 Jun 2014 12:00:06 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:reply-to:sender:in-reply-to:references:date:message-id
         :subject:from:to:content-type;
        bh=2bltHb3Y38pagZchORIV6lnFQvVaqMD7YNRHxBlDF7A=;
        b=G2ZiyHgFdQpzppTBicxUxJzO92eb/wkX6TDxPLLXCHBHTcuHhdbfOLLEnO2uji0uGT
         xIaAQcZGVqdFpusi9riB5PrAOqi9m0h5jcZRpeXUkFjtg8QnvEekbxQCRb6jPgCX6d3j
         LfRuGq0CW1PqK8fcKNNYpGWv0+MSHt69iikN47+5KJVdtjG6F+TgrrDwGCC4X8zN3Tu/
         yA+1EnHUd8nLLQV9LD1/aazTWO1I1A21/CXhV0PV1QYp+dWQYydLNynXFmV99z+UpMOp
         UhjDbryUhGZtBuqqqgzFdkBmlwS4Ub+NmMPuhNxLmcyKOQKIJxjZ/ihAWuykduOBmKdH
         65ww==
MIME-Version: 1.0
X-Received: by 10.42.4.201 with SMTP id 9mr45481814ict.57.1401822005078; Tue,
 03 Jun 2014 12:00:05 -0700 (PDT)
Reply-To: kzhang@apache.org
Sender: kanzhangemail@gmail.com
Received: by 10.64.22.230 with HTTP; Tue, 3 Jun 2014 12:00:05 -0700 (PDT)
In-Reply-To: <CALuGr6ZJA-SiabGuB-e1HSFHjm4n+XHGVAw49pVegoxRFiXnNw@mail.gmail.com>
References: <CALuGr6ZxiNiY0M7FqYy8R6FAPwraLTWd3NZ1SMaQC9C2dq=Jrw@mail.gmail.com>
	<AF0583D4-1C80-40CB-BCCE-D9403ECA4534@gmail.com>
	<CALuGr6ZJA-SiabGuB-e1HSFHjm4n+XHGVAw49pVegoxRFiXnNw@mail.gmail.com>
Date: Tue, 3 Jun 2014 12:00:05 -0700
X-Google-Sender-Auth: sImz77_6_OnF4bC_WYjQk9AUIqQ
Message-ID: <CALRHqP-OnYoSmMUg-u11=H=4nXqD0XLOgkQVXQm2KatgchLQBA@mail.gmail.com>
Subject: Re: Add my JIRA username (hsaputra) to Spark's contributor's list
From: Kan Zhang <kzhang@apache.org>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11346bd2a88d7e04faf31ed6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11346bd2a88d7e04faf31ed6
Content-Type: text/plain; charset=UTF-8

Same here please, username (kzhang). Thanks!


On Tue, Jun 3, 2014 at 11:39 AM, Henry Saputra <henry.saputra@gmail.com>
wrote:

> Thanks Matei!
>
> - Henry
>
> On Tue, Jun 3, 2014 at 11:36 AM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
> > Done. Looks like this was lost in the JIRA import.
> >
> > Matei
> >
> > On Jun 3, 2014, at 11:33 AM, Henry Saputra <henry.saputra@gmail.com>
> wrote:
> >
> >> Hi,
> >>
> >> Could someone with right karma kindly add my username (hsaputra) to
> >> Spark's contributor list?
> >>
> >> I was added before but somehow now I can no longer assign ticket to
> >> myself nor update tickets I am working on.
> >>
> >>
> >> Thanks,
> >>
> >> - Henry
> >
>

--001a11346bd2a88d7e04faf31ed6--

From dev-return-7948-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun  3 19:00:49 2014
Return-Path: <dev-return-7948-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CA0E91113F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Jun 2014 19:00:49 +0000 (UTC)
Received: (qmail 21865 invoked by uid 500); 3 Jun 2014 19:00:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21806 invoked by uid 500); 3 Jun 2014 19:00:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21795 invoked by uid 99); 3 Jun 2014 19:00:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 19:00:49 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.216.50 as permitted sender)
Received: from [209.85.216.50] (HELO mail-qa0-f50.google.com) (209.85.216.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 19:00:42 +0000
Received: by mail-qa0-f50.google.com with SMTP id j15so5532320qaq.37
        for <dev@spark.apache.org>; Tue, 03 Jun 2014 12:00:18 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=wwbO4HjtyBd57y+Pk3U6be9WHHIDs05EgGCYskRLeM0=;
        b=qTqnp6fikh1dNAULaRBZcumrnWCwt+78LuGpo1EB4CfJcCSHZZqva97KpvWyAoJ4ex
         ACZ+QYfeom2fo5LKP3+ljyrxQz3/32qhLi9OuUcYmhaKcs392wdVDAvFkRWhTMKHRWN3
         acvQEoEEYuXTrvsmeaHtD08pVBR+omyXBWo2w+TIkD4rQkfalTtg2xBrhvDRWuJ6RYN+
         8BPBHzosk12kWqrjfaAoDeyOY+3vDAIhhvFj+WrR8HROaFqLvOg2OoceIh5jNzS+agrM
         GQhaB/LxE+E1ItqFaE21RjyMCAZDXr/zTs8Wnfmehg9RALSABgEZaclZ5XBz99Ji2yVS
         GpqQ==
X-Received: by 10.140.84.83 with SMTP id k77mr59013019qgd.70.1401822018386;
        Tue, 03 Jun 2014 12:00:18 -0700 (PDT)
Received: from [192.168.2.11] (MTRLPQ02-1177746539.sdsl.bell.ca. [70.50.252.107])
        by mx.google.com with ESMTPSA id m2sm161812qac.3.2014.06.03.12.00.17
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Tue, 03 Jun 2014 12:00:18 -0700 (PDT)
Date: Tue, 3 Jun 2014 15:09:00 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Message-ID: <6C907553712E414C801D3D048DF7F315@gmail.com>
In-Reply-To: <CALuGr6ZJA-SiabGuB-e1HSFHjm4n+XHGVAw49pVegoxRFiXnNw@mail.gmail.com>
References: <CALuGr6ZxiNiY0M7FqYy8R6FAPwraLTWd3NZ1SMaQC9C2dq=Jrw@mail.gmail.com>
 <AF0583D4-1C80-40CB-BCCE-D9403ECA4534@gmail.com>
 <CALuGr6ZJA-SiabGuB-e1HSFHjm4n+XHGVAw49pVegoxRFiXnNw@mail.gmail.com>
Subject: Re: Add my JIRA username (hsaputra) to Spark's contributor's
 list
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="538e1d4c_440badfc_1cb"
X-Virus-Checked: Checked by ClamAV on apache.org

--538e1d4c_440badfc_1cb
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

I think I lost that permission too=3F =20

Patrick once helped to recover the permission, but I lost that permission=
 again=3F

username is CodingCat, or Nan Zhu (I=E2=80=99m not sure which one you use=
 when doing this)=3F

Best, =20

-- =20
Nan Zhu


On Tuesday, June 3, 2014 at 2:39 PM, Henry Saputra wrote:

> Thanks Matei=21
> =20
> - Henry
> =20
> On Tue, Jun 3, 2014 at 11:36 AM, Matei Zaharia <matei.zaharia=40gmail.c=
om (mailto:matei.zaharia=40gmail.com)> wrote:
> > Done. Looks like this was lost in the JIRA import.
> > =20
> > Matei
> > =20
> > On Jun 3, 2014, at 11:33 AM, Henry Saputra <henry.saputra=40gmail.com=
 (mailto:henry.saputra=40gmail.com)> wrote:
> > =20
> > > Hi,
> > > =20
> > > Could someone with right karma kindly add my username (hsaputra) to=

> > > Spark's contributor list=3F
> > > =20
> > > I was added before but somehow now I can no longer assign ticket to=

> > > myself nor update tickets I am working on.
> > > =20
> > > =20
> > > Thanks,
> > > =20
> > > - Henry =20


--538e1d4c_440badfc_1cb--


From dev-return-7949-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun  3 21:08:28 2014
Return-Path: <dev-return-7949-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 92C75116A3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Jun 2014 21:08:28 +0000 (UTC)
Received: (qmail 24722 invoked by uid 500); 3 Jun 2014 21:08:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24666 invoked by uid 500); 3 Jun 2014 21:08:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24655 invoked by uid 99); 3 Jun 2014 21:08:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 21:08:28 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of doris.s.xin@gmail.com designates 209.85.219.44 as permitted sender)
Received: from [209.85.219.44] (HELO mail-oa0-f44.google.com) (209.85.219.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 21:08:22 +0000
Received: by mail-oa0-f44.google.com with SMTP id o6so6873618oag.3
        for <dev@spark.apache.org>; Tue, 03 Jun 2014 14:08:02 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=sbBMXV4XzfaKajJc9MlUnmiIOhqZY+71znvKolQ1/2E=;
        b=qHf+iZ6wqELfBnhKSIO5Zn1iyUQ/IJxeiV3cF15FC7heMQ+jOEfQE1iAUC7xXqS9LV
         Qfdrefhl/GHC2Qx3L3i4IFJSganlXKl7Rw7cZx1f4MHmuvhlZ3ZofLCaCNhWER27+T1U
         ozzWaT/acfmsMzoqN/GW0i/OmcDuT0bDhIHe+8MrOpkfzX2tSAnPisB7+1D+rzn2SwJS
         DW0RvUlXbdRCzcA3wSFBi5Wa1p1shteMBVgIxR/Mmuw2uLcI2IULtEEDwZ+4F8X8Is8i
         JfC1S/b1CUncCtUhUonuYF/lOwsMywDIPZGPM0RN+x4WJmlnThCBwDFx8gvT33sQw05O
         rEEg==
MIME-Version: 1.0
X-Received: by 10.60.156.38 with SMTP id wb6mr51806255oeb.41.1401829682242;
 Tue, 03 Jun 2014 14:08:02 -0700 (PDT)
Received: by 10.76.71.10 with HTTP; Tue, 3 Jun 2014 14:08:02 -0700 (PDT)
Date: Tue, 3 Jun 2014 14:08:02 -0700
Message-ID: <CAL90uaoXdbi9bNtxDM6M=GVGhXQRqtC-BdmkmZU82HqHaYFfLg@mail.gmail.com>
Subject: collectAsMap doesn't return a multiMap?
From: Doris Xin <doris.s.xin@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hey guys,

Just wanted to check real quick if collectAsMap was by design not to
return a multimap (so multiple values to the same key can overwrite
the same entry). It seems like it's only used in some unit tests in
the codebase. I added a warning in the comment saying not to expect a
multimap. Let me know if this is actually a bug that needs to be
fixed.

Thanks,
Doris

From dev-return-7950-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun  3 21:46:11 2014
Return-Path: <dev-return-7950-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9971111835
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Jun 2014 21:46:11 +0000 (UTC)
Received: (qmail 4599 invoked by uid 500); 3 Jun 2014 21:46:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4539 invoked by uid 500); 3 Jun 2014 21:46:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4529 invoked by uid 99); 3 Jun 2014 21:46:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 21:46:11 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Jun 2014 21:46:07 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <madhu@madhu.com>)
	id 1WrwWf-0000oU-N4
	for dev@spark.incubator.apache.org; Tue, 03 Jun 2014 14:45:45 -0700
Date: Tue, 3 Jun 2014 14:45:45 -0700 (PDT)
From: Madhu <madhu@madhu.com>
To: dev@spark.incubator.apache.org
Message-ID: <1401831945704-6930.post@n3.nabble.com>
In-Reply-To: <EBB6278C-D731-480F-9256-FD1869CC81AF@databricks.com>
References: <1401720225042-6908.post@n3.nabble.com> <CAPh_B=YSkX27Q6LqYJMhVHWoZ9CaFXeZr-VUDYHy8WDoX8S-5A@mail.gmail.com> <EBB6278C-D731-480F-9256-FD1869CC81AF@databricks.com>
Subject: Re: Eclipse Scala IDE/Scala test and Wiki
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I was able to edit the page and add Eclipse setup steps.

Thanks Matei and Reynold!



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Eclipse-Scala-IDE-Scala-test-and-Wiki-tp6908p6930.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7951-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun  4 00:30:28 2014
Return-Path: <dev-return-7951-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B249411E33
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Jun 2014 00:30:28 +0000 (UTC)
Received: (qmail 98964 invoked by uid 500); 4 Jun 2014 00:30:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98906 invoked by uid 500); 4 Jun 2014 00:30:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98894 invoked by uid 99); 4 Jun 2014 00:30:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Jun 2014 00:30:28 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.53 as permitted sender)
Received: from [209.85.160.53] (HELO mail-pb0-f53.google.com) (209.85.160.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Jun 2014 00:30:21 +0000
Received: by mail-pb0-f53.google.com with SMTP id md12so6171864pbc.12
        for <dev@spark.apache.org>; Tue, 03 Jun 2014 17:30:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=o2ycyGLirykPNMz2IavcgHG00f4c6Tv7Lw6rn/ieLo0=;
        b=ttDtWF9t6URoyoJtro6T9sbF7b/SgMdorX7OE0Bo2ddo/H7qbUn/fS7APgEYNki5so
         HErFvEFZTFr2dHsKRhyVKLj1P5lfxdOQEC7wJggZdErXcpUz8y9gQoLXt0fyoLYOViKA
         C1QBHdvXfVLSsCDD4CG5AnfB9zoKNFXzQx123SZcR0f+putNfjoRRTbv4wkoLL8FoHOi
         xC4Le/0O9M4wKkBd+Ftq4wOhV92v4sHcfBbJl6FPJiNv1dUKphRa5FQUjBBJ5kCJNbth
         5CHGN7EHCWNOQWIDjz68teXm68qR2lPpRMr8I3oAFOa7RbAdtJ0+jKPUm3lu12Em9qs6
         6CBw==
X-Received: by 10.69.19.139 with SMTP id gu11mr56542689pbd.36.1401841801394;
        Tue, 03 Jun 2014 17:30:01 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id qf10sm2575042pbc.23.2014.06.03.17.29.57
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 03 Jun 2014 17:29:58 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
Subject: Re: collectAsMap doesn't return a multiMap?
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAL90uaoXdbi9bNtxDM6M=GVGhXQRqtC-BdmkmZU82HqHaYFfLg@mail.gmail.com>
Date: Tue, 3 Jun 2014 17:29:57 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <85D860EB-B8F3-4F41-BBB1-4933229EBE85@gmail.com>
References: <CAL90uaoXdbi9bNtxDM6M=GVGhXQRqtC-BdmkmZU82HqHaYFfLg@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.2)
X-Virus-Checked: Checked by ClamAV on apache.org

Yup, it=92s meant to be just a Map. You should probably use collect() =
and build a multimap instead if you=92d like that.

Matei

On Jun 3, 2014, at 2:08 PM, Doris Xin <doris.s.xin@gmail.com> wrote:

> Hey guys,
>=20
> Just wanted to check real quick if collectAsMap was by design not to
> return a multimap (so multiple values to the same key can overwrite
> the same entry). It seems like it's only used in some unit tests in
> the codebase. I added a warning in the comment saying not to expect a
> multimap. Let me know if this is actually a bug that needs to be
> fixed.
>=20
> Thanks,
> Doris


From dev-return-7952-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun  4 03:34:00 2014
Return-Path: <dev-return-7952-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9DF08102D4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Jun 2014 03:34:00 +0000 (UTC)
Received: (qmail 15385 invoked by uid 500); 4 Jun 2014 03:34:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15326 invoked by uid 500); 4 Jun 2014 03:34:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15316 invoked by uid 99); 4 Jun 2014 03:34:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Jun 2014 03:34:00 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ueshin@happy-camper.st designates 209.85.220.170 as permitted sender)
Received: from [209.85.220.170] (HELO mail-vc0-f170.google.com) (209.85.220.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Jun 2014 03:33:56 +0000
Received: by mail-vc0-f170.google.com with SMTP id lc6so726201vcb.15
        for <dev@spark.apache.org>; Tue, 03 Jun 2014 20:33:31 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=KEgfHZw29HQ9sp6GDSDbO/ahZPbktvEWfxRgXeCib8I=;
        b=ZY1JMtQmEzlxnv/yYAnMHed7AVy5g7GK+wtoZFnDNwsZw300nVPsc5HA1oR2xqMgwW
         D2CZbC/VKYtKzsN3RtbHYXOUr6uZYF24VoR0QZPmiH5w6qL3GhM7589tPuhxDi/L9rE8
         lX9q4R6JCHhAraW1KLPoMTL+ZqtnrS+OUnRBnDmdfaI2Ytcq4SwFnYRys8fNm1ODgIcU
         yVVm1oTewAAeTijRIBIf50UPrKk34W50X7NRi0MOhkQVZ94SFCHuBrqciT9F+5DEPwY3
         m05ew5NyX4GMJeMCfOKlBLg51No71ulZbONWyjgWiabbO8rkhVzKST71CMGpMNwakk0W
         yIXQ==
X-Gm-Message-State: ALoCoQkipjDWJN41om2LVjHz3SvVfu2e5+1EyM6g+tDyMCKua2i6+kOr+IJDFXzK+ks6Fmc72JBz
MIME-Version: 1.0
X-Received: by 10.52.163.161 with SMTP id yj1mr11737314vdb.35.1401852811156;
 Tue, 03 Jun 2014 20:33:31 -0700 (PDT)
Received: by 10.221.27.131 with HTTP; Tue, 3 Jun 2014 20:33:31 -0700 (PDT)
Date: Wed, 4 Jun 2014 12:33:31 +0900
Message-ID: <CADkZp9uro-SXp=ekGPPDB4y-Fg0dhh9TA-6NuV3C9xEB8NKvBg@mail.gmail.com>
Subject: What is the correct Spark version of master/branch-1.0?
From: Takuya UESHIN <ueshin@happy-camper.st>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all,

I'm wondering what is the correct Spark version of each HEAD of master
and branch-1.0.

current master HEAD (e8d93ee5284cb6a1d4551effe91ee8d233323329):
- pom.xml: 1.0.0-SNAPSHOT
- SparkBuild.scala: 1.1.0-SNAPSHOT

It should be 1.1.0-SNAPSHOT?


current branch-1.0 HEAD (d96794132e37cf57f8dd945b9d11f8adcfc30490):
- pom.xml: 1.0.1-SNAPSHOT
- SparkBuild.scala: 1.0.0

It should be 1.0.1-SNAPSHOT?


Thanks.

-- 
Takuya UESHIN
Tokyo, Japan

http://twitter.com/ueshin

From dev-return-7953-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun  4 07:16:32 2014
Return-Path: <dev-return-7953-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 32C9010885
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Jun 2014 07:16:32 +0000 (UTC)
Received: (qmail 70940 invoked by uid 500); 4 Jun 2014 07:16:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70854 invoked by uid 500); 4 Jun 2014 07:16:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70834 invoked by uid 99); 4 Jun 2014 07:16:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Jun 2014 07:16:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of shihaoliang@huawei.com designates 119.145.14.65 as permitted sender)
Received: from [119.145.14.65] (HELO szxga02-in.huawei.com) (119.145.14.65)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Jun 2014 07:16:26 +0000
Received: from 172.24.2.119 (EHLO szxeml210-edg.china.huawei.com) ([172.24.2.119])
	by szxrg02-dlp.huawei.com (MOS 4.3.7-GA FastPath queued)
	with ESMTP id BUQ64659;
	Wed, 04 Jun 2014 15:16:00 +0800 (CST)
Received: from SZXEML462-HUB.china.huawei.com (10.82.67.205) by
 szxeml210-edg.china.huawei.com (172.24.2.183) with Microsoft SMTP Server
 (TLS) id 14.3.158.1; Wed, 4 Jun 2014 15:15:53 +0800
Received: from nkgeml409-hub.china.huawei.com (10.98.56.40) by
 szxeml462-hub.china.huawei.com (10.82.67.205) with Microsoft SMTP Server
 (TLS) id 14.3.158.1; Wed, 4 Jun 2014 15:15:54 +0800
Received: from NKGEML511-MBX.china.huawei.com ([169.254.5.36]) by
 nkgeml409-hub.china.huawei.com ([10.98.56.40]) with mapi id 14.03.0158.001;
 Wed, 4 Jun 2014 15:15:49 +0800
From: "Shihaoliang (Shihaoliang)" <shihaoliang@huawei.com>
To: "dev@mesos.apache.org" <dev@mesos.apache.org>,
        "dev@spark.apache.org"
	<dev@spark.apache.org>
Subject: enable Spark on Mesos security delegation token transfer
Thread-Topic: enable Spark on Mesos security delegation token transfer
Thread-Index: Ac9/xLG6qVEiBj0JT8WHZPdvew+Uog==
Date: Wed, 4 Jun 2014 07:15:48 +0000
Message-ID: <FE502ABB7E0C0F48B64DA686C8210F19561C56DF@nkgeml511-mbx.china.huawei.com>
Accept-Language: zh-CN, en-US
Content-Language: zh-CN
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [10.177.16.59]
Content-Type: multipart/alternative;
	boundary="_000_FE502ABB7E0C0F48B64DA686C8210F19561C56DFnkgeml511mbxchi_"
MIME-Version: 1.0
X-CFilter-Loop: Reflected
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_FE502ABB7E0C0F48B64DA686C8210F19561C56DFnkgeml511mbxchi_
Content-Type: text/plain; charset="gb2312"
Content-Transfer-Encoding: base64
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--_000_FE502ABB7E0C0F48B64DA686C8210F19561C56DFnkgeml511mbxchi_--

From dev-return-7954-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun  4 17:47:41 2014
Return-Path: <dev-return-7954-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3FFD111E41
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Jun 2014 17:47:41 +0000 (UTC)
Received: (qmail 34176 invoked by uid 500); 4 Jun 2014 17:47:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34097 invoked by uid 500); 4 Jun 2014 17:47:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34086 invoked by uid 99); 4 Jun 2014 17:47:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Jun 2014 17:47:40 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tgraves_cs@yahoo.com designates 216.109.114.223 as permitted sender)
Received: from [216.109.114.223] (HELO nm43-vm4.bullet.mail.bf1.yahoo.com) (216.109.114.223)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Jun 2014 17:47:35 +0000
Received: from [66.196.81.171] by nm43.bullet.mail.bf1.yahoo.com with NNFMP; 04 Jun 2014 17:47:14 -0000
Received: from [98.139.212.221] by tm17.bullet.mail.bf1.yahoo.com with NNFMP; 04 Jun 2014 17:47:14 -0000
Received: from [127.0.0.1] by omp1030.mail.bf1.yahoo.com with NNFMP; 04 Jun 2014 17:47:14 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 249644.54612.bm@omp1030.mail.bf1.yahoo.com
Received: (qmail 46464 invoked by uid 60001); 4 Jun 2014 17:47:14 -0000
X-YMail-OSG: zFmjQWUVM1mukrTJrd6jMNMTWFniUExzHsAQaHnuuxHGRJ6
 XSuD_W4chKXupLprbl7yez7PycokNHkO7S5XHxZYfXl.W0RFo7MDVQ87_gE4
 X7HXmv6zcRkpeuoxuuftIVXtp7YBpdcrGm5JX7EtSMIlijfx7m2k3oc5tEAE
 T9IlkYWPR8y3HstEv9C8bJyO2cOXrdEEGl4bFlZV4xMRoSKrA9tuiLPZV_lp
 6qQJ7gBRxfllbPE0AQ9QqQSdluIEKo2jKgKbeF5BtS1SjCLKFigMYXnBkOyc
 yz2mUooIm0dk9X7ZTIso7YM0dJy9GEgMjlT5ZyPd7nwjnR_3kG4kpuCfxWrB
 4qzu7_gRtdqcheUwfIMPi3Vevh7S_ERch07_trIFZWxGm6Vv_dnrjLgW9vEb
 G0RvXlQoEIZwd2B8hluwjkIGXHJT3PSlZtFDEzjUZyBK8.Hz0MOWwI56PMP9
 nc59v_MxOUcLopUAa7nPc1nSq8xqom9kJFDhov4m2AqU8..PoDpGmXBe15NQ
 GrF7X8NT0dBQsRSPY88_KJauTWMfWNi75s9bzQa0pJY89K3srNhtIKdDbdOp
 tXYcVsbfm3plO7B0JGQaCPbD0jSbycT6u3u0_OvsQ9EMBBDYqcGFUkGVkXqe
 00hCRQddmesn4SSaSYDH.BHAUzrvKYMhmA4bGbX.Nu8BX..6n63nQxoyHnAg
 guak-
Received: from [204.11.79.50] by web140106.mail.bf1.yahoo.com via HTTP; Wed, 04 Jun 2014 10:47:14 PDT
X-Rocket-MIMEInfo: 002.001,VGVzdGluZy4uLiBSZXNlbmRpbmcgYXMgaXQgYXBwZWFycyBteSBtZXNzYWdlIGRpZG4ndCBnbyB0aHJvdWdoIGxhc3Qgd2Vlay4KClRvbQoKCk9uIFdlZG5lc2RheSwgTWF5IDI4LCAyMDE0IDQ6MTIgUE0sIFRvbSBHcmF2ZXMgPHRncmF2ZXNfY3NAeWFob28uY29tPiB3cm90ZToKIAoKCisxLiBUZXN0ZWQgc3Bhcmsgb24geWFybiAoY2x1c3RlciBtb2RlLCBjbGllbnQgbW9kZSwgcHlzcGFyaywgc3Bhcmstc2hlbGwpIG9uIGhhZG9vcCAwLjIzIGFuZCAyLjQuwqAKClRvbQoKCk9uIFdlZG5lc2RheSwgTWF5IDIBMAEBAQE-
X-Mailer: YahooMailWebService/0.8.188.663
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com> <BA089183-FAF4-4B65-8118-9DDB7E744AFB@webtrends.com>  <1401311576.69780.YahooMailNeo@web140105.mail.bf1.yahoo.com>
Message-ID: <1401904034.3697.YahooMailNeo@web140106.mail.bf1.yahoo.com>
Date: Wed, 4 Jun 2014 10:47:14 -0700 (PDT)
From: Tom Graves <tgraves_cs@yahoo.com.INVALID>
Reply-To: Tom Graves <tgraves_cs@yahoo.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
To: "dev@spark.apache.org" <dev@spark.apache.org>
In-Reply-To: <1401311576.69780.YahooMailNeo@web140105.mail.bf1.yahoo.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="-156808750-1167641094-1401904034=:3697"
X-Virus-Checked: Checked by ClamAV on apache.org

---156808750-1167641094-1401904034=:3697
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

Testing... Resending as it appears my message didn't go through last week.=
=0A=0ATom=0A=0A=0AOn Wednesday, May 28, 2014 4:12 PM, Tom Graves <tgraves_c=
s@yahoo.com> wrote:=0A =0A=0A=0A+1. Tested spark on yarn (cluster mode, cli=
ent mode, pyspark, spark-shell) on hadoop 0.23 and 2.4.=A0=0A=0ATom=0A=0A=
=0AOn Wednesday, May 28, 2014 3:07 PM, Sean McNamara <Sean.McNamara@Webtren=
ds.com> wrote:=0A =0A=0A=0APulled down, compiled, and tested examples on OS=
 X and ubuntu.=0ADeployed app we are building on spark and poured data thro=
ugh it.=0A=0A+1=0A=0ASean=0A=0A=0A=0AOn May 26, 2014, at 8:39 AM, Tathagata=
 Das <tathagata.das1565@gmail.com> wrote:=0A=0A> Please vote on releasing t=
he following candidate as Apache Spark version 1.0.0!=0A> =0A> This has a f=
ew important bug fixes on top of rc10:=0A> SPARK-1900 and SPARK-1918: https=
://github.com/apache/spark/pull/853=0A> SPARK-1870: https://github.com/apac=
he/spark/pull/848=0A> SPARK-1897: https://github.com/apache/spark/pull/849=
=0A> =0A> The tag to be voted on is v1.0.0-rc11 (commit c69d97cd):=0A> http=
s://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3Dc69d97cdb4=
2f809cb71113a1db4194c21372242a=0A> =0A> The release files, including signat=
ures, digests, etc. can be found at:=0A> http://people.apache.org/~tdas/spa=
rk-1.0.0-rc11/=0A> =0A> Release=0A artifacts are signed with the following =
key:=0A> https://people.apache.org/keys/committer/tdas.asc=0A> =0A> The sta=
ging repository for this release can be found at:=0A> https://repository.ap=
ache.org/content/repositories/orgapachespark-1019/=0A> =0A> The documentati=
on corresponding to this release can be found at:=0A> http://people.apache.=
org/~tdas/spark-1.0.0-rc11-docs/=0A> =0A> Please vote on releasing this pac=
kage as Apache Spark 1.0.0!=0A> =0A> The vote is open until=0A Thursday, Ma=
y 29, at 16:00 UTC and passes if=0A> a majority of at least 3 +1 PMC votes =
are cast.=0A> =0A> [ ] +1 Release this package as Apache Spark 1.0.0=0A> [ =
] -1 Do not release this package because ...=0A> =0A> To learn more about A=
pache Spark, please see=0A> http://spark.apache.org/=0A> =0A> =3D=3D API Ch=
anges =3D=3D=0A> We welcome users to compile Spark applications against 1.0=
. There are=0A> a few API changes in this release. Here are links to the as=
sociated=0A> upgrade guides - user facing changes have been kept as small a=
s=0A> possible.=0A> =0A> Changes to ML vector specification:=0A> http://peo=
ple.apache.org/~tdas/spark-1.0.0-rc11-docs/mllib-guide.html#from-09-to-10=
=0A> =0A> Changes to the Java API:=0A> http://people.apache.org/~tdas/spark=
-1.0.0-rc11-docs/java-programming-guide.html#upgrading-from-pre-10-versions=
-of-spark=0A> =0A> Changes to the streaming API:=0A> http://people.apache.o=
rg/~tdas/spark-1.0.0-rc11-docs/streaming-programming-guide.html#migration-g=
uide-from-091-or-below-to-1x=0A> =0A> Changes to the GraphX API:=0A> http:/=
/people.apache.org/~tdas/spark-1.0.0-rc11-docs/graphx-programming-guide.htm=
l#upgrade-guide-from-spark-091=0A> =0A> Other changes:=0A> coGroup and rela=
ted functions now return Iterable[T] instead of Seq[T]=0A> =3D=3D> Call toS=
eq on the result to restore the old behavior=0A> =0A> SparkContext.jarOfCla=
ss returns Option[String] instead of=0A Seq[String]=0A> =3D=3D> Call toSeq =
on the result to restore old behavior
---156808750-1167641094-1401904034=:3697--

From dev-return-7955-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun  4 17:50:00 2014
Return-Path: <dev-return-7955-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C629D11E62
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Jun 2014 17:50:00 +0000 (UTC)
Received: (qmail 43463 invoked by uid 500); 4 Jun 2014 17:50:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43404 invoked by uid 500); 4 Jun 2014 17:50:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43393 invoked by uid 99); 4 Jun 2014 17:50:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Jun 2014 17:50:00 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.175 as permitted sender)
Received: from [209.85.214.175] (HELO mail-ob0-f175.google.com) (209.85.214.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Jun 2014 17:49:57 +0000
Received: by mail-ob0-f175.google.com with SMTP id wo20so8083678obc.6
        for <dev@spark.apache.org>; Wed, 04 Jun 2014 10:49:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=/5NUBWc9Z2yN4l81dmhs3QU5NacmAIviCWXy/N3tXUE=;
        b=za4pGAIJcf1X9p7L3do7sjRvo/zGKroh1CJsX8Q59JQFoLFuBDPBXm3lFgk/mS205w
         e2vBcANWfv021UKZqLfuUMHCtSjGQKidb4nRhb2bG/3LQzKh6PuBw4ixehcE3mybD0B8
         1oe5/tvyZpJjF983k4+9hRZ3UomQccgChVeyeGXVsQqos2mxFAYqwm4DqTRzK8X8dtx/
         F6fmcbvwG24epXX/KL+v7vp3xe528IHxXh2orvbEvzaWzd3bCNnDVpSVHV9uTaxfrx7z
         OdiaSPROMqk4T95rA8ruswOvRwCIQowG6nmhmOIZENhPIVZTqNnxJWJtvKDhQhwAxbH0
         TlXQ==
MIME-Version: 1.0
X-Received: by 10.60.146.167 with SMTP id td7mr59545706oeb.6.1401904173890;
 Wed, 04 Jun 2014 10:49:33 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Wed, 4 Jun 2014 10:49:33 -0700 (PDT)
In-Reply-To: <1401904034.3697.YahooMailNeo@web140106.mail.bf1.yahoo.com>
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
	<BA089183-FAF4-4B65-8118-9DDB7E744AFB@webtrends.com>
	<1401311576.69780.YahooMailNeo@web140105.mail.bf1.yahoo.com>
	<1401904034.3697.YahooMailNeo@web140106.mail.bf1.yahoo.com>
Date: Wed, 4 Jun 2014 10:49:33 -0700
Message-ID: <CABPQxsuXG9iLkqJcq8rzR3bEckXNZdRZubq4Gi=rPBVhUgUCjg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>, Tom Graves <tgraves_cs@yahoo.com>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Received!

On Wed, Jun 4, 2014 at 10:47 AM, Tom Graves
<tgraves_cs@yahoo.com.invalid> wrote:
> Testing... Resending as it appears my message didn't go through last week.
>
> Tom
>
>
> On Wednesday, May 28, 2014 4:12 PM, Tom Graves <tgraves_cs@yahoo.com> wrote:
>
>
>
> +1. Tested spark on yarn (cluster mode, client mode, pyspark, spark-shell) on hadoop 0.23 and 2.4.
>
> Tom
>
>
> On Wednesday, May 28, 2014 3:07 PM, Sean McNamara <Sean.McNamara@Webtrends.com> wrote:
>
>
>
> Pulled down, compiled, and tested examples on OS X and ubuntu.
> Deployed app we are building on spark and poured data through it.
>
> +1
>
> Sean
>
>
>
> On May 26, 2014, at 8:39 AM, Tathagata Das <tathagata.das1565@gmail.com> wrote:
>
>> Please vote on releasing the following candidate as Apache Spark version 1.0.0!
>>
>> This has a few important bug fixes on top of rc10:
>> SPARK-1900 and SPARK-1918: https://github.com/apache/spark/pull/853
>> SPARK-1870: https://github.com/apache/spark/pull/848
>> SPARK-1897: https://github.com/apache/spark/pull/849
>>
>> The tag to be voted on is v1.0.0-rc11 (commit c69d97cd):
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=c69d97cdb42f809cb71113a1db4194c21372242a
>>
>> The release files, including signatures, digests, etc. can be found at:
>> http://people.apache.org/~tdas/spark-1.0.0-rc11/
>>
>> Release
>  artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/tdas.asc
>>
>> The staging repository for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1019/
>>
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/
>>
>> Please vote on releasing this package as Apache Spark 1.0.0!
>>
>> The vote is open until
>  Thursday, May 29, at 16:00 UTC and passes if
>> a majority of at least 3 +1 PMC votes are cast.
>>
>> [ ] +1 Release this package as Apache Spark 1.0.0
>> [ ] -1 Do not release this package because ...
>>
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>>
>> == API Changes ==
>> We welcome users to compile Spark applications against 1.0. There are
>> a few API changes in this release. Here are links to the associated
>> upgrade guides - user facing changes have been kept as small as
>> possible.
>>
>> Changes to ML vector specification:
>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/mllib-guide.html#from-09-to-10
>>
>> Changes to the Java API:
>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>>
>> Changes to the streaming API:
>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>>
>> Changes to the GraphX API:
>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>>
>> Other changes:
>> coGroup and related functions now return Iterable[T] instead of Seq[T]
>> ==> Call toSeq on the result to restore the old behavior
>>
>> SparkContext.jarOfClass returns Option[String] instead of
>  Seq[String]
>> ==> Call toSeq on the result to restore old behavior

From dev-return-7956-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun  4 18:17:25 2014
Return-Path: <dev-return-7956-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A16DC11FC7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Jun 2014 18:17:25 +0000 (UTC)
Received: (qmail 98502 invoked by uid 500); 4 Jun 2014 18:17:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98447 invoked by uid 500); 4 Jun 2014 18:17:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98436 invoked by uid 99); 4 Jun 2014 18:17:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Jun 2014 18:17:20 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.182 as permitted sender)
Received: from [209.85.214.182] (HELO mail-ob0-f182.google.com) (209.85.214.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Jun 2014 18:17:17 +0000
Received: by mail-ob0-f182.google.com with SMTP id wn1so8115631obc.41
        for <dev@spark.apache.org>; Wed, 04 Jun 2014 11:16:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=0FizfLsC7l44akkvDc5Lp3f2iz2w8grw8A5TWuuJmrM=;
        b=p4w2+15p1FtejRzoWgFptbmcSQi3a5/71YVwCLkevltIZCROuxX3OH5Y53q1e3XxO7
         HHttwgHKtLNcrmc4QjYdlc76lpaZ7ivFLm/PSOY6/g2e4/jGlT/3aYtvbL0yCrbTJZAV
         7l+Dw7gebBLcEU92jP2/TMTbBen2OSKgHcVHDMeaHzMEM/22BdpouFg2/+pTWEcqcDxo
         iJdM5MpNCv/0J3s/a9LJAPSODhAfN5RcZs5us3b+WW1J35JN0ShcdYwgkCHtuWinN4H+
         W5+Z6TKCE15k6OoqhLmlIMmKAWil2GWpVvCfg2kdxDKvR1J9Eas7/H7capCfblTvj6+M
         3Iig==
MIME-Version: 1.0
X-Received: by 10.182.163.45 with SMTP id yf13mr25772672obb.66.1401905813565;
 Wed, 04 Jun 2014 11:16:53 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Wed, 4 Jun 2014 11:16:53 -0700 (PDT)
In-Reply-To: <CADkZp9uro-SXp=ekGPPDB4y-Fg0dhh9TA-6NuV3C9xEB8NKvBg@mail.gmail.com>
References: <CADkZp9uro-SXp=ekGPPDB4y-Fg0dhh9TA-6NuV3C9xEB8NKvBg@mail.gmail.com>
Date: Wed, 4 Jun 2014 11:16:53 -0700
Message-ID: <CABPQxsuoyXT0S3h8UZK1ZS=OxXS45hAQoj4pYC8Q7=KyEskBbw@mail.gmail.com>
Subject: Re: What is the correct Spark version of master/branch-1.0?
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

It should be 1.1-SNAPSHOT. Feel free to submit a PR to clean up any
inconsistencies.

On Tue, Jun 3, 2014 at 8:33 PM, Takuya UESHIN <ueshin@happy-camper.st> wrote:
> Hi all,
>
> I'm wondering what is the correct Spark version of each HEAD of master
> and branch-1.0.
>
> current master HEAD (e8d93ee5284cb6a1d4551effe91ee8d233323329):
> - pom.xml: 1.0.0-SNAPSHOT
> - SparkBuild.scala: 1.1.0-SNAPSHOT
>
> It should be 1.1.0-SNAPSHOT?
>
>
> current branch-1.0 HEAD (d96794132e37cf57f8dd945b9d11f8adcfc30490):
> - pom.xml: 1.0.1-SNAPSHOT
> - SparkBuild.scala: 1.0.0
>
> It should be 1.0.1-SNAPSHOT?
>
>
> Thanks.
>
> --
> Takuya UESHIN
> Tokyo, Japan
>
> http://twitter.com/ueshin

From dev-return-7957-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun  4 19:19:45 2014
Return-Path: <dev-return-7957-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 512CA1036B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Jun 2014 19:19:45 +0000 (UTC)
Received: (qmail 53053 invoked by uid 500); 4 Jun 2014 19:19:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52990 invoked by uid 500); 4 Jun 2014 19:19:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52979 invoked by uid 99); 4 Jun 2014 19:19:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Jun 2014 19:19:44 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.49 as permitted sender)
Received: from [209.85.216.49] (HELO mail-qa0-f49.google.com) (209.85.216.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Jun 2014 19:19:42 +0000
Received: by mail-qa0-f49.google.com with SMTP id cm18so7494246qab.8
        for <dev@spark.apache.org>; Wed, 04 Jun 2014 12:19:18 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=/BldBJmAaYGQwLKEcwqRvhKhuNET/JZs9KRD1KM6+zs=;
        b=oLfTO59f+rdmZB5ADIU3/JFZlCS0f8b/GTZGSC7QZfOV2HtdqyWk9w954lgD28iNzX
         2sKfP38XJXvE/cDZbxXdqHLvIokzfdJ12LmH0DN/CuZ5R+FoHGj5CJ6PeoYQyE9U1/jP
         hNA9TroJHd2HV2pNiHFCJ3nNDWdbThpW0oOI5Mus0WHWwKrHPbJ+JfiPI9PeFyGjquSJ
         F0W0vP2bMqjT4d+rV4YIqudonq7CDfi5dW70jflyD+zh0U37BtWm7d312UyzUd1J6V3a
         O3bh5oy8XzXALn01v5n0ZvRg32tsE8y+WQ0klTu/Qc3U6IlePQun9GSKx09tUQyWYRe3
         YZAg==
MIME-Version: 1.0
X-Received: by 10.140.26.179 with SMTP id 48mr72395803qgv.51.1401909558326;
 Wed, 04 Jun 2014 12:19:18 -0700 (PDT)
Received: by 10.140.82.164 with HTTP; Wed, 4 Jun 2014 12:19:18 -0700 (PDT)
In-Reply-To: <CABPQxsuXG9iLkqJcq8rzR3bEckXNZdRZubq4Gi=rPBVhUgUCjg@mail.gmail.com>
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
	<BA089183-FAF4-4B65-8118-9DDB7E744AFB@webtrends.com>
	<1401311576.69780.YahooMailNeo@web140105.mail.bf1.yahoo.com>
	<1401904034.3697.YahooMailNeo@web140106.mail.bf1.yahoo.com>
	<CABPQxsuXG9iLkqJcq8rzR3bEckXNZdRZubq4Gi=rPBVhUgUCjg@mail.gmail.com>
Date: Wed, 4 Jun 2014 12:19:18 -0700
Message-ID: <CA+B-+fz-4tri39AOMf=SKG+uMcxZoc-we+-OYVwwKdwGqFDRFw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Cc: Tom Graves <tgraves_cs@yahoo.com>
Content-Type: multipart/alternative; boundary=001a11c035f43b80e804fb0781c3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c035f43b80e804fb0781c3
Content-Type: text/plain; charset=UTF-8

Hi Patrick,

We maintain internal Spark mirror in sync with Spark github master...

What's the way to get the 1.0.0 stable release from github to deploy on our
production cluster ? Is there a tag for 1.0.0 that I should use to deploy ?

Thanks.
Deb



On Wed, Jun 4, 2014 at 10:49 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> Received!
>
> On Wed, Jun 4, 2014 at 10:47 AM, Tom Graves
> <tgraves_cs@yahoo.com.invalid> wrote:
> > Testing... Resending as it appears my message didn't go through last
> week.
> >
> > Tom
> >
> >
> > On Wednesday, May 28, 2014 4:12 PM, Tom Graves <tgraves_cs@yahoo.com>
> wrote:
> >
> >
> >
> > +1. Tested spark on yarn (cluster mode, client mode, pyspark,
> spark-shell) on hadoop 0.23 and 2.4.
> >
> > Tom
> >
> >
> > On Wednesday, May 28, 2014 3:07 PM, Sean McNamara
> <Sean.McNamara@Webtrends.com> wrote:
> >
> >
> >
> > Pulled down, compiled, and tested examples on OS X and ubuntu.
> > Deployed app we are building on spark and poured data through it.
> >
> > +1
> >
> > Sean
> >
> >
> >
> > On May 26, 2014, at 8:39 AM, Tathagata Das <tathagata.das1565@gmail.com>
> wrote:
> >
> >> Please vote on releasing the following candidate as Apache Spark
> version 1.0.0!
> >>
> >> This has a few important bug fixes on top of rc10:
> >> SPARK-1900 and SPARK-1918: https://github.com/apache/spark/pull/853
> >> SPARK-1870: https://github.com/apache/spark/pull/848
> >> SPARK-1897: https://github.com/apache/spark/pull/849
> >>
> >> The tag to be voted on is v1.0.0-rc11 (commit c69d97cd):
> >>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=c69d97cdb42f809cb71113a1db4194c21372242a
> >>
> >> The release files, including signatures, digests, etc. can be found at:
> >> http://people.apache.org/~tdas/spark-1.0.0-rc11/
> >>
> >> Release
> >  artifacts are signed with the following key:
> >> https://people.apache.org/keys/committer/tdas.asc
> >>
> >> The staging repository for this release can be found at:
> >> https://repository.apache.org/content/repositories/orgapachespark-1019/
> >>
> >> The documentation corresponding to this release can be found at:
> >> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/
> >>
> >> Please vote on releasing this package as Apache Spark 1.0.0!
> >>
> >> The vote is open until
> >  Thursday, May 29, at 16:00 UTC and passes if
> >> a majority of at least 3 +1 PMC votes are cast.
> >>
> >> [ ] +1 Release this package as Apache Spark 1.0.0
> >> [ ] -1 Do not release this package because ...
> >>
> >> To learn more about Apache Spark, please see
> >> http://spark.apache.org/
> >>
> >> == API Changes ==
> >> We welcome users to compile Spark applications against 1.0. There are
> >> a few API changes in this release. Here are links to the associated
> >> upgrade guides - user facing changes have been kept as small as
> >> possible.
> >>
> >> Changes to ML vector specification:
> >>
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/mllib-guide.html#from-09-to-10
> >>
> >> Changes to the Java API:
> >>
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
> >>
> >> Changes to the streaming API:
> >>
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
> >>
> >> Changes to the GraphX API:
> >>
> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
> >>
> >> Other changes:
> >> coGroup and related functions now return Iterable[T] instead of Seq[T]
> >> ==> Call toSeq on the result to restore the old behavior
> >>
> >> SparkContext.jarOfClass returns Option[String] instead of
> >  Seq[String]
> >> ==> Call toSeq on the result to restore old behavior
>

--001a11c035f43b80e804fb0781c3--

From dev-return-7958-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun  4 20:01:13 2014
Return-Path: <dev-return-7958-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D083D1076E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Jun 2014 20:01:13 +0000 (UTC)
Received: (qmail 49564 invoked by uid 500); 4 Jun 2014 20:01:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49506 invoked by uid 500); 4 Jun 2014 20:01:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49495 invoked by uid 99); 4 Jun 2014 20:01:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Jun 2014 20:01:12 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.46 as permitted sender)
Received: from [209.85.219.46] (HELO mail-oa0-f46.google.com) (209.85.219.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Jun 2014 20:01:10 +0000
Received: by mail-oa0-f46.google.com with SMTP id g18so8476550oah.33
        for <dev@spark.apache.org>; Wed, 04 Jun 2014 13:00:46 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Ap1cZES96XhaFGHTfceviauWH669NqVmXs+ReTiXk+0=;
        b=liAxoRlPCtqyHXsmfxLn2ieN8Za3yhcmjE6hoDRf6UH2H9a/6WgOep8NL5rY6sSwjo
         BUH2LP32yGGqVVSowXKlXhEEJZ+no8zbguRwDmlkTfjMn1V69H6C99PIUURPlxl/viBS
         3tUr5ut4d9Evg+OnXLv/cvuWqRDOlLxqHhVSWr8nldM3bBhl43tMKkTy0LHtsLQTBDHT
         d6C2BLxI3zb4+yUFcYOSoZcVK1kOep2AfCXTnGeLFpkU5Ws6IF59yU0k4Npr/sSGtg93
         taIVQ1ZwbLgU/LgmFeyao164PXgCkcZ4438rVoclt2uKMvj8WTu/+hzYUHqz0GFiQUQN
         Y8LQ==
MIME-Version: 1.0
X-Received: by 10.182.163.45 with SMTP id yf13mr26719709obb.66.1401912046077;
 Wed, 04 Jun 2014 13:00:46 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Wed, 4 Jun 2014 13:00:45 -0700 (PDT)
In-Reply-To: <CA+B-+fz-4tri39AOMf=SKG+uMcxZoc-we+-OYVwwKdwGqFDRFw@mail.gmail.com>
References: <CAMwrk0nbNfaPpAqv7Poqm7o_b55hwmoBATPoGKdTtdzwAWfsBA@mail.gmail.com>
	<BA089183-FAF4-4B65-8118-9DDB7E744AFB@webtrends.com>
	<1401311576.69780.YahooMailNeo@web140105.mail.bf1.yahoo.com>
	<1401904034.3697.YahooMailNeo@web140106.mail.bf1.yahoo.com>
	<CABPQxsuXG9iLkqJcq8rzR3bEckXNZdRZubq4Gi=rPBVhUgUCjg@mail.gmail.com>
	<CA+B-+fz-4tri39AOMf=SKG+uMcxZoc-we+-OYVwwKdwGqFDRFw@mail.gmail.com>
Date: Wed, 4 Jun 2014 13:00:45 -0700
Message-ID: <CABPQxsta7H3g-R8436wt7eYhEcO5vh9wnpJ9_HEZTTCZ+qBgBA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.0 (RC11)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: Tom Graves <tgraves_cs@yahoo.com>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey There,

The best way is to use the v1.0.0 tag:
https://github.com/apache/spark/releases/tag/v1.0.0

- Patrick

On Wed, Jun 4, 2014 at 12:19 PM, Debasish Das <debasish.das83@gmail.com> wrote:
> Hi Patrick,
>
> We maintain internal Spark mirror in sync with Spark github master...
>
> What's the way to get the 1.0.0 stable release from github to deploy on our
> production cluster ? Is there a tag for 1.0.0 that I should use to deploy ?
>
> Thanks.
> Deb
>
>
>
> On Wed, Jun 4, 2014 at 10:49 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>
>> Received!
>>
>> On Wed, Jun 4, 2014 at 10:47 AM, Tom Graves
>> <tgraves_cs@yahoo.com.invalid> wrote:
>> > Testing... Resending as it appears my message didn't go through last
>> week.
>> >
>> > Tom
>> >
>> >
>> > On Wednesday, May 28, 2014 4:12 PM, Tom Graves <tgraves_cs@yahoo.com>
>> wrote:
>> >
>> >
>> >
>> > +1. Tested spark on yarn (cluster mode, client mode, pyspark,
>> spark-shell) on hadoop 0.23 and 2.4.
>> >
>> > Tom
>> >
>> >
>> > On Wednesday, May 28, 2014 3:07 PM, Sean McNamara
>> <Sean.McNamara@Webtrends.com> wrote:
>> >
>> >
>> >
>> > Pulled down, compiled, and tested examples on OS X and ubuntu.
>> > Deployed app we are building on spark and poured data through it.
>> >
>> > +1
>> >
>> > Sean
>> >
>> >
>> >
>> > On May 26, 2014, at 8:39 AM, Tathagata Das <tathagata.das1565@gmail.com>
>> wrote:
>> >
>> >> Please vote on releasing the following candidate as Apache Spark
>> version 1.0.0!
>> >>
>> >> This has a few important bug fixes on top of rc10:
>> >> SPARK-1900 and SPARK-1918: https://github.com/apache/spark/pull/853
>> >> SPARK-1870: https://github.com/apache/spark/pull/848
>> >> SPARK-1897: https://github.com/apache/spark/pull/849
>> >>
>> >> The tag to be voted on is v1.0.0-rc11 (commit c69d97cd):
>> >>
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=c69d97cdb42f809cb71113a1db4194c21372242a
>> >>
>> >> The release files, including signatures, digests, etc. can be found at:
>> >> http://people.apache.org/~tdas/spark-1.0.0-rc11/
>> >>
>> >> Release
>> >  artifacts are signed with the following key:
>> >> https://people.apache.org/keys/committer/tdas.asc
>> >>
>> >> The staging repository for this release can be found at:
>> >> https://repository.apache.org/content/repositories/orgapachespark-1019/
>> >>
>> >> The documentation corresponding to this release can be found at:
>> >> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/
>> >>
>> >> Please vote on releasing this package as Apache Spark 1.0.0!
>> >>
>> >> The vote is open until
>> >  Thursday, May 29, at 16:00 UTC and passes if
>> >> a majority of at least 3 +1 PMC votes are cast.
>> >>
>> >> [ ] +1 Release this package as Apache Spark 1.0.0
>> >> [ ] -1 Do not release this package because ...
>> >>
>> >> To learn more about Apache Spark, please see
>> >> http://spark.apache.org/
>> >>
>> >> == API Changes ==
>> >> We welcome users to compile Spark applications against 1.0. There are
>> >> a few API changes in this release. Here are links to the associated
>> >> upgrade guides - user facing changes have been kept as small as
>> >> possible.
>> >>
>> >> Changes to ML vector specification:
>> >>
>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/mllib-guide.html#from-09-to-10
>> >>
>> >> Changes to the Java API:
>> >>
>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/java-programming-guide.html#upgrading-from-pre-10-versions-of-spark
>> >>
>> >> Changes to the streaming API:
>> >>
>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/streaming-programming-guide.html#migration-guide-from-091-or-below-to-1x
>> >>
>> >> Changes to the GraphX API:
>> >>
>> http://people.apache.org/~tdas/spark-1.0.0-rc11-docs/graphx-programming-guide.html#upgrade-guide-from-spark-091
>> >>
>> >> Other changes:
>> >> coGroup and related functions now return Iterable[T] instead of Seq[T]
>> >> ==> Call toSeq on the result to restore the old behavior
>> >>
>> >> SparkContext.jarOfClass returns Option[String] instead of
>> >  Seq[String]
>> >> ==> Call toSeq on the result to restore old behavior
>>

From dev-return-7959-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun  5 02:18:17 2014
Return-Path: <dev-return-7959-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1EB05113AC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Jun 2014 02:18:17 +0000 (UTC)
Received: (qmail 28571 invoked by uid 500); 5 Jun 2014 02:18:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28512 invoked by uid 500); 5 Jun 2014 02:18:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28501 invoked by uid 99); 5 Jun 2014 02:18:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 02:18:16 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=FROM_EXCESS_BASE64,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of witgo@qq.com designates 103.7.28.238 as permitted sender)
Received: from [103.7.28.238] (HELO smtpbg64.qq.com) (103.7.28.238)
    by apache.org (qpsmtpd/0.29) with SMTP; Thu, 05 Jun 2014 02:18:11 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=qq.com; s=s201307;
	t=1401934668; bh=QcVkUosWILZBLkGwuwHSJBm3a2VEWHCU4Ovm0FV1yRU=;
	h=X-QQ-FEAT:X-QQ-SSF:X-HAS-ATTACH:X-QQ-BUSINESS-ORIGIN:
	 X-Originating-IP:In-Reply-To:References:X-QQ-STYLE:X-QQ-mid:From:To:Subject:Mime-Version:Content-Type:Content-Transfer-Encoding:Date:
	 X-Priority:Message-ID:X-QQ-MIME:X-Mailer:X-QQ-Mailer:
	 X-QQ-ReplyHash:X-QQ-SENDSIZE;
	b=dwNAIVO39dJ6wJWY2fyV9XSYqDzGER6A3vWVusZmj/D+JUzwCTdovowLpmxp/L+U6
	 Do2vUgQYPF/cYjpUGO5GiYAFIWBo0zcnoM+EGKg44m/LvFinB3A6iIlQiaPH2CMHIx
	 OZ3tfwOXJEp4N1VaSlGEXegAunp6E6OkQFFFgvs8=
X-QQ-FEAT: T6G+Kbm3XEp5pgGqTXYFkDEEwD1oWR1wZeOUDDTyOFiIaXa4CzKfHs4icEDaJ
	FwXDE//RNWI/YGFi7pn3G7lM8mleUpXbD+tAvItg18UZI8hUjfFVzGzLGNVkrj2TwdLS5tJ
	ACtRlu133RrjFWKAC3jSvN4PpL64C79OnQXuuqA=
X-QQ-SSF: 000000000000001000000000000000M
X-HAS-ATTACH: no
X-QQ-BUSINESS-ORIGIN: 2
X-Originating-IP: 118.186.13.68
In-Reply-To: <CALRHqP-OnYoSmMUg-u11=H=4nXqD0XLOgkQVXQm2KatgchLQBA@mail.gmail.com>
References: <CALuGr6ZxiNiY0M7FqYy8R6FAPwraLTWd3NZ1SMaQC9C2dq=Jrw@mail.gmail.com>
	<AF0583D4-1C80-40CB-BCCE-D9403ECA4534@gmail.com>
	<CALuGr6ZJA-SiabGuB-e1HSFHjm4n+XHGVAw49pVegoxRFiXnNw@mail.gmail.com>
	<CALRHqP-OnYoSmMUg-u11=H=4nXqD0XLOgkQVXQm2KatgchLQBA@mail.gmail.com>
X-QQ-STYLE: 
X-QQ-mid: webmail421t1401934666t4040521
From: "=?ISO-8859-1?B?d2l0Z28=?=" <witgo@qq.com>
To: "=?ISO-8859-1?B?ZGV2?=" <dev@spark.apache.org>
Subject: Re: Add my JIRA username (hsaputra) to Spark's contributor's list
Mime-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_538FD34A_08F7BD78_7211F4E3"
Content-Transfer-Encoding: 8Bit
Date: Thu, 5 Jun 2014 10:17:46 +0800
X-Priority: 3
Message-ID: <tencent_58080867505A60AE22DB8852@qq.com>
X-QQ-MIME: TCMime 1.0 by Tencent
X-Mailer: QQMail 2.x
X-QQ-Mailer: QQMail 2.x
X-QQ-ReplyHash: 2451179944
X-QQ-SENDSIZE: 520
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_538FD34A_08F7BD78_7211F4E3
Content-Type: text/plain;
	charset="ISO-8859-1"
Content-Transfer-Encoding: base64

VWgsd3JpdGUgbXkgbmFtZSB3cm9uZywgcmlnaHQgc2hvdWxkIGJlIEd1b3FpYW5nIExpIHJh
dGhlciB0aGFuIEd1b3F1aWFuZyBMaQ0KDQoNCg0KDQotLS0tLS0tLS0tLS0tLS0tLS0gT3Jp
Z2luYWwgLS0tLS0tLS0tLS0tLS0tLS0tDQpGcm9tOiAgIkthbiBaaGFuZyI7PGt6aGFuZ0Bh
cGFjaGUub3JnPjsNCkRhdGU6ICBXZWQsIEp1biA0LCAyMDE0IDAzOjAwIEFNDQpUbzogICJk
ZXYiPGRldkBzcGFyay5hcGFjaGUub3JnPjsgDQoNClN1YmplY3Q6ICBSZTogQWRkIG15IEpJ
UkEgdXNlcm5hbWUgKGhzYXB1dHJhKSB0byBTcGFyaydzIGNvbnRyaWJ1dG9yJ3MgbGlzdA0K
DQoNCg0KU2FtZSBoZXJlIHBsZWFzZSwgdXNlcm5hbWUgKGt6aGFuZykuIFRoYW5rcyENCg0K
DQpPbiBUdWUsIEp1biAzLCAyMDE0IGF0IDExOjM5IEFNLCBIZW5yeSBTYXB1dHJhIDxoZW5y
eS5zYXB1dHJhQGdtYWlsLmNvbT4NCndyb3RlOg0KDQo+IFRoYW5rcyBNYXRlaSENCj4NCj4g
LSBIZW5yeQ0KPg0KPiBPbiBUdWUsIEp1biAzLCAyMDE0IGF0IDExOjM2IEFNLCBNYXRlaSBa
YWhhcmlhIDxtYXRlaS56YWhhcmlhQGdtYWlsLmNvbT4NCj4gd3JvdGU6DQo+ID4gRG9uZS4g
TG9va3MgbGlrZSB0aGlzIHdhcyBsb3N0IGluIHRoZSBKSVJBIGltcG9ydC4NCj4gPg0KPiA+
IE1hdGVpDQo+ID4NCj4gPiBPbiBKdW4gMywgMjAxNCwgYXQgMTE6MzMgQU0sIEhlbnJ5IFNh
cHV0cmEgPGhlbnJ5LnNhcHV0cmFAZ21haWwuY29tPg0KPiB3cm90ZToNCj4gPg0KPiA+PiBI
aSwNCj4gPj4NCj4gPj4gQ291bGQgc29tZW9uZSB3aXRoIHJpZ2h0IGthcm1hIGtpbmRseSBh
ZGQgbXkgdXNlcm5hbWUgKGhzYXB1dHJhKSB0bw0KPiA+PiBTcGFyaydzIGNvbnRyaWJ1dG9y
IGxpc3Q/DQo+ID4+DQo+ID4+IEkgd2FzIGFkZGVkIGJlZm9yZSBidXQgc29tZWhvdyBub3cg
SSBjYW4gbm8gbG9uZ2VyIGFzc2lnbiB0aWNrZXQgdG8NCj4gPj4gbXlzZWxmIG5vciB1cGRh
dGUgdGlja2V0cyBJIGFtIHdvcmtpbmcgb24uDQo+ID4+DQo+ID4+DQo+ID4+IFRoYW5rcywN
Cj4gPj4NCj4gPj4gLSBIZW5yeQ0KPiA+DQo+

------=_NextPart_538FD34A_08F7BD78_7211F4E3--


From dev-return-7960-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun  5 03:29:45 2014
Return-Path: <dev-return-7960-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 73EFA114E8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Jun 2014 03:29:45 +0000 (UTC)
Received: (qmail 6639 invoked by uid 500); 5 Jun 2014 03:29:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6564 invoked by uid 500); 5 Jun 2014 03:29:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6554 invoked by uid 99); 5 Jun 2014 03:29:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 03:29:45 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of Rahul.Singhal@guavus.com designates 204.232.241.167 as permitted sender)
Received: from [204.232.241.167] (HELO mx1.guavus.com) (204.232.241.167)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 03:29:41 +0000
Received: from MX3.guavus.com ([192.168.11.2]) by mx1.guavus.com
 ([204.232.241.167]) with mapi id 14.03.0174.001; Wed, 4 Jun 2014 20:29:16
 -0700
From: Rahul Singhal <Rahul.Singhal@guavus.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: Announcing Spark 1.0.0
Thread-Topic: Announcing Spark 1.0.0
Thread-Index: AQHPe+/BgHxWifzNFEiImYfDFtBkB5tZ1mcAgAjf7IA=
Date: Thu, 5 Jun 2014 03:29:16 +0000
Message-ID: <CFB5E167.20400%rahul.singhal@guavus.com>
References: <CABPQxstP6EP4oCndBpeUfHv1rpu=Pp7cknjPD_QTf4dak6yGhA@mail.gmail.com>
 <CFAE6F75.1FF96%rahul.singhal@guavus.com>
In-Reply-To: <CFAE6F75.1FF96%rahul.singhal@guavus.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [61.12.3.120]
Content-Type: text/plain; charset="us-ascii"
Content-ID: <D54FF014DB374D4981EFBA0ECF94431C@guavus.com>
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Could someone please clarify my confusion or is this not an issue that we
should be concerned about?

Thanks,
Rahul Singhal





On 30/05/14 5:28 PM, "Rahul Singhal" <Rahul.Singhal@guavus.com> wrote:

>Is it intentional/ok that the tag v1.0.0 is behind tag v1.0.0-rc11?
>
>
>Thanks,
>Rahul Singhal
>
>
>
>
>
>On 30/05/14 3:43 PM, "Patrick Wendell" <pwendell@gmail.com> wrote:
>
>>I'm thrilled to announce the availability of Spark 1.0.0! Spark 1.0.0
>>is a milestone release as the first in the 1.0 line of releases,
>>providing API stability for Spark's core interfaces.
>>
>>Spark 1.0.0 is Spark's largest release ever, with contributions from
>>117 developers. I'd like to thank everyone involved in this release -
>>it was truly a community effort with fixes, features, and
>>optimizations contributed from dozens of organizations.
>>
>>This release expands Spark's standard libraries, introducing a new SQL
>>package (SparkSQL) which lets users integrate SQL queries into
>>existing Spark workflows. MLlib, Spark's machine learning library, is
>>expanded with sparse vector support and several new algorithms. The
>>GraphX and Streaming libraries also introduce new features and
>>optimizations. Spark's core engine adds support for secured YARN
>>clusters, a unified tool for submitting Spark applications, and
>>several performance and stability improvements. Finally, Spark adds
>>support for Java 8 lambda syntax and improves coverage of the Java and
>>Python API's.
>>
>>Those features only scratch the surface - check out the release notes
>>here:
>>http://spark.apache.org/releases/spark-release-1-0-0.html
>>
>>Note that since release artifacts were posted recently, certain
>>mirrors may not have working downloads for a few hours.
>>
>>- Patrick
>


From dev-return-7961-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun  5 04:40:15 2014
Return-Path: <dev-return-7961-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0C465115D4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Jun 2014 04:40:15 +0000 (UTC)
Received: (qmail 62466 invoked by uid 500); 5 Jun 2014 04:40:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62411 invoked by uid 500); 5 Jun 2014 04:40:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62398 invoked by uid 99); 5 Jun 2014 04:40:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 04:40:14 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.47 as permitted sender)
Received: from [209.85.219.47] (HELO mail-oa0-f47.google.com) (209.85.219.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 04:40:11 +0000
Received: by mail-oa0-f47.google.com with SMTP id i7so525715oag.6
        for <dev@spark.apache.org>; Wed, 04 Jun 2014 21:39:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=EbbaB1tNsOEJt2Jfw6XiSJ9MkJXhIuNdnztEMF8X8xI=;
        b=mqp0/A94qmhWmfDN1O/zS82KDD2cZX7Xh7/+kjtCyhjDk8O/uu/TtHKIpWR+lW/DX5
         nAUeq9DZ/BNOsm1GJfS409YMN7fRf4w6TL61hShznV53U0tpjqAqZDS1WaSf3ENcho1p
         pXDYp87urC8iTdpdXYwoBBarUdWxsVZd2QxAVDdlwatVEif7KpRrsr1MBb4cgIURj5RV
         IxfiF+Zp1H4fpN0GKzHm7Wm+K2nCEALcU5F8CzZYh6aBtzFcpEwHxELPgyV3kGMfLiWq
         AIdRBrWUK6B3pr0BKH2vUNkvwwuKSR73oK06zciQnNWUOcBjQ8njed2mVUSgXAjw2HT2
         QcNA==
MIME-Version: 1.0
X-Received: by 10.182.60.42 with SMTP id e10mr33081314obr.33.1401943187345;
 Wed, 04 Jun 2014 21:39:47 -0700 (PDT)
Received: by 10.182.136.106 with HTTP; Wed, 4 Jun 2014 21:39:47 -0700 (PDT)
In-Reply-To: <CFB5E167.20400%rahul.singhal@guavus.com>
References: <CABPQxstP6EP4oCndBpeUfHv1rpu=Pp7cknjPD_QTf4dak6yGhA@mail.gmail.com>
	<CFAE6F75.1FF96%rahul.singhal@guavus.com>
	<CFB5E167.20400%rahul.singhal@guavus.com>
Date: Wed, 4 Jun 2014 21:39:47 -0700
Message-ID: <CABPQxsvCAQVdyd3DnoFgb0oQKCbYAC3qjxZFwGEUTAQLuLWOwQ@mail.gmail.com>
Subject: Re: Announcing Spark 1.0.0
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Rahul,

The v1.0.0 tag is correct. When we release Spark we create multiple
candidates. One of the candidates is promoted to the full release. So
rc11 is also the same as the official v1.0.0 release.

- Patrick

On Wed, Jun 4, 2014 at 8:29 PM, Rahul Singhal <Rahul.Singhal@guavus.com> wrote:
> Could someone please clarify my confusion or is this not an issue that we
> should be concerned about?
>
> Thanks,
> Rahul Singhal
>
>
>
>
>
> On 30/05/14 5:28 PM, "Rahul Singhal" <Rahul.Singhal@guavus.com> wrote:
>
>>Is it intentional/ok that the tag v1.0.0 is behind tag v1.0.0-rc11?
>>
>>
>>Thanks,
>>Rahul Singhal
>>
>>
>>
>>
>>
>>On 30/05/14 3:43 PM, "Patrick Wendell" <pwendell@gmail.com> wrote:
>>
>>>I'm thrilled to announce the availability of Spark 1.0.0! Spark 1.0.0
>>>is a milestone release as the first in the 1.0 line of releases,
>>>providing API stability for Spark's core interfaces.
>>>
>>>Spark 1.0.0 is Spark's largest release ever, with contributions from
>>>117 developers. I'd like to thank everyone involved in this release -
>>>it was truly a community effort with fixes, features, and
>>>optimizations contributed from dozens of organizations.
>>>
>>>This release expands Spark's standard libraries, introducing a new SQL
>>>package (SparkSQL) which lets users integrate SQL queries into
>>>existing Spark workflows. MLlib, Spark's machine learning library, is
>>>expanded with sparse vector support and several new algorithms. The
>>>GraphX and Streaming libraries also introduce new features and
>>>optimizations. Spark's core engine adds support for secured YARN
>>>clusters, a unified tool for submitting Spark applications, and
>>>several performance and stability improvements. Finally, Spark adds
>>>support for Java 8 lambda syntax and improves coverage of the Java and
>>>Python API's.
>>>
>>>Those features only scratch the surface - check out the release notes
>>>here:
>>>http://spark.apache.org/releases/spark-release-1-0-0.html
>>>
>>>Note that since release artifacts were posted recently, certain
>>>mirrors may not have working downloads for a few hours.
>>>
>>>- Patrick
>>
>

From dev-return-7962-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun  5 05:49:13 2014
Return-Path: <dev-return-7962-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 10892116F9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Jun 2014 05:49:13 +0000 (UTC)
Received: (qmail 40760 invoked by uid 500); 5 Jun 2014 05:49:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40704 invoked by uid 500); 5 Jun 2014 05:49:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40694 invoked by uid 99); 5 Jun 2014 05:49:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 05:49:12 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ueshin@happy-camper.st designates 209.85.128.176 as permitted sender)
Received: from [209.85.128.176] (HELO mail-ve0-f176.google.com) (209.85.128.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 05:49:10 +0000
Received: by mail-ve0-f176.google.com with SMTP id jz11so608927veb.21
        for <dev@spark.apache.org>; Wed, 04 Jun 2014 22:48:46 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=EeWzK6sCiw0rj9UoPhQc94et+YB8whM/3ukAfN4GDE0=;
        b=JzZJf2zXav2pEXXVGv00FeCfuLQh/yX5dtuKyhGLxrTQlok2f8voL77YNsauvf1F2r
         YLp75kUOAs2o9oXx3DlFmeNCxajokJsEWUXFz97hjQ0b2besFBPZGZwVD+zsleehbKOO
         tIyms+rYu81n6kqbm5jSn9q52fo27lhmVaLKur6jFDboh1PT1esUFydmXy/f+9mBh6G8
         N4/PlF28SeQW99ad9sCtAtcSpk7Y7D1h7z2U5Bu6/fhljbB+6IYfR55J5W47fUAve/kF
         1WJ1Lb/ohKbz9Z7L77ESmtJYxg1IgmipWlJcY49XMW3RM6BdVISK62qfy9jjnUZuYdrQ
         6Y1Q==
X-Gm-Message-State: ALoCoQkZ6cNmQyzPMFYI3QL958cuUJgs0a9RtUR1GFbMIeNsk8LJMoYScwfgNQR87OZoxkudHchE
MIME-Version: 1.0
X-Received: by 10.58.66.195 with SMTP id h3mr29102vet.57.1401947325988; Wed,
 04 Jun 2014 22:48:45 -0700 (PDT)
Received: by 10.221.27.131 with HTTP; Wed, 4 Jun 2014 22:48:45 -0700 (PDT)
In-Reply-To: <CABPQxsuoyXT0S3h8UZK1ZS=OxXS45hAQoj4pYC8Q7=KyEskBbw@mail.gmail.com>
References: <CADkZp9uro-SXp=ekGPPDB4y-Fg0dhh9TA-6NuV3C9xEB8NKvBg@mail.gmail.com>
	<CABPQxsuoyXT0S3h8UZK1ZS=OxXS45hAQoj4pYC8Q7=KyEskBbw@mail.gmail.com>
Date: Thu, 5 Jun 2014 14:48:45 +0900
Message-ID: <CADkZp9tPViXfBkJMJPi8tKdtiVzfoZWFfqmEd0f_VZDEW-42Bg@mail.gmail.com>
Subject: Re: What is the correct Spark version of master/branch-1.0?
From: Takuya UESHIN <ueshin@happy-camper.st>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Thank you for your reply.

I've sent pull requests.


Thanks.


2014-06-05 3:16 GMT+09:00 Patrick Wendell <pwendell@gmail.com>:
> It should be 1.1-SNAPSHOT. Feel free to submit a PR to clean up any
> inconsistencies.
>
> On Tue, Jun 3, 2014 at 8:33 PM, Takuya UESHIN <ueshin@happy-camper.st> wrote:
>> Hi all,
>>
>> I'm wondering what is the correct Spark version of each HEAD of master
>> and branch-1.0.
>>
>> current master HEAD (e8d93ee5284cb6a1d4551effe91ee8d233323329):
>> - pom.xml: 1.0.0-SNAPSHOT
>> - SparkBuild.scala: 1.1.0-SNAPSHOT
>>
>> It should be 1.1.0-SNAPSHOT?
>>
>>
>> current branch-1.0 HEAD (d96794132e37cf57f8dd945b9d11f8adcfc30490):
>> - pom.xml: 1.0.1-SNAPSHOT
>> - SparkBuild.scala: 1.0.0
>>
>> It should be 1.0.1-SNAPSHOT?
>>
>>
>> Thanks.
>>
>> --
>> Takuya UESHIN
>> Tokyo, Japan
>>
>> http://twitter.com/ueshin



-- 
Takuya UESHIN
Tokyo, Japan

http://twitter.com/ueshin

From dev-return-7963-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun  5 06:09:34 2014
Return-Path: <dev-return-7963-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 38E531175B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Jun 2014 06:09:34 +0000 (UTC)
Received: (qmail 70961 invoked by uid 500); 5 Jun 2014 06:09:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70908 invoked by uid 500); 5 Jun 2014 06:09:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70897 invoked by uid 99); 5 Jun 2014 06:09:33 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 06:09:33 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of qiuzhuang.lian@gmail.com designates 209.85.128.181 as permitted sender)
Received: from [209.85.128.181] (HELO mail-ve0-f181.google.com) (209.85.128.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 06:09:28 +0000
Received: by mail-ve0-f181.google.com with SMTP id pa12so619651veb.26
        for <dev@spark.apache.org>; Wed, 04 Jun 2014 23:09:07 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=j1IF3muSqcgXQQnxp6ympFnuc1Wx8mKvWNsebZklMak=;
        b=BO/nSAZAHrxkAsnmStV8dhKfLz470ULUf99dRWmEEhG+M6wkWo4HxMQL+LCoGAwrXo
         Z/L/ErfPiq+1bBGYnrKf6vUA3NQYBJf+M+fEJAdebxBgcmuTXv1pZlrWY8J7NGDboypl
         77+Lbiqx5kg9xAMsTJiwaMIU8mbt1eBypYMvlIYHy/LPZb8nhbSgognzr+Ue3A6XW6uo
         02QHI1CSjWtprIqyPKq6otX6o+L7vxrI8W4ugMGs95VN7+zWB3h8BV1E3mie7fPmaY2l
         8IaOND1IK+MbpbW13uUCkPTqWdz1Z/d1/JkBipFEVCj5iFOIujeMsdheHKNgkzoUaAwP
         uLmQ==
MIME-Version: 1.0
X-Received: by 10.220.106.7 with SMTP id v7mr292596vco.46.1401948547782; Wed,
 04 Jun 2014 23:09:07 -0700 (PDT)
Received: by 10.220.196.11 with HTTP; Wed, 4 Jun 2014 23:09:07 -0700 (PDT)
Date: Thu, 5 Jun 2014 14:09:07 +0800
Message-ID: <CABKvOWtfAfnHxtHpUDjjA7Daodzw9Y_6Zk7Nz88J=cEe3Pzqbw@mail.gmail.com>
Subject: How to add user local repository defined in localRepository in
 settings.xml into Spark SBT build
From: Qiuzhuang Lian <qiuzhuang.lian@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b3432362f1e2004fb109559
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3432362f1e2004fb109559
Content-Type: text/plain; charset=UTF-8

Hi,

I customized MVN_HOME/conf/settings.xml's localRepository tag To manage
maven local jars.

<localRepository>F:/Java/maven-build/.m2/repository</localRepository>

However when I build Spark with SBT, it seems that it still gets the
default .m2 repository under

Path.userHome + "/.m2/repository"

How should I let SBT pick up my customized localRepository instead?

Thanks,
Qiuzhuang

--047d7b3432362f1e2004fb109559--

From dev-return-7964-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun  5 17:30:08 2014
Return-Path: <dev-return-7964-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B2A59CE7C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Jun 2014 17:30:08 +0000 (UTC)
Received: (qmail 1756 invoked by uid 500); 5 Jun 2014 17:30:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1703 invoked by uid 500); 5 Jun 2014 17:30:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1689 invoked by uid 99); 5 Jun 2014 17:30:08 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 17:30:08 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.128.173] (HELO mail-ve0-f173.google.com) (209.85.128.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 17:30:04 +0000
Received: by mail-ve0-f173.google.com with SMTP id pa12so1618612veb.32
        for <dev@spark.apache.org>; Thu, 05 Jun 2014 10:29:40 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=RVnr6DYcDUOS3l5z1pZNM5VGr1qU5q33UQDCqwn6cnA=;
        b=AlwlSXG/2ZpC+FI654zNgxla+FdBYi3qHNUMBQ3dxN8ANWFDH9kqpPcZhVL4V8Emnz
         zaSCOGqJ9YFAyLn5GhbmvN0JLmigrixqqUL5GxBmXLkuyJ16DTmZGHb8Ut/IovLq2Z2h
         ky+3I5h9IoAMEF2FbhOr3IWccHIavWqNItItmQTgFi15I1pwZCZA547Mgew7UBxXSaMC
         rluNMUgeIgZees72QG3xDCNCbXQY9yACt4qJ9KUO/VJoRoFEV9Q67eNKf8Bdb8zKOiwy
         TJjfGXg0HRUJXSsOR4KxrsUDTJqGvDHyzC/mVRYV/cUna/ugdtB08Wz21/2Z709M5ZFB
         3uug==
X-Gm-Message-State: ALoCoQk2rwyI2UodnGu7BHvQ7haZejD+zFkQ+CkhQbIs94vjsyG0T//4SxxkHqXkb3VwHPii9G04
X-Received: by 10.220.133.197 with SMTP id g5mr52287774vct.20.1401989379912;
        Thu, 05 Jun 2014 10:29:39 -0700 (PDT)
Received: from mail-ve0-f172.google.com (mail-ve0-f172.google.com [209.85.128.172])
        by mx.google.com with ESMTPSA id ex14sm6678827veb.15.2014.06.05.10.29.38
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 05 Jun 2014 10:29:38 -0700 (PDT)
Received: by mail-ve0-f172.google.com with SMTP id oz11so1618258veb.17
        for <dev@spark.apache.org>; Thu, 05 Jun 2014 10:29:38 -0700 (PDT)
X-Received: by 10.220.88.18 with SMTP id y18mr1650716vcl.26.1401989378270;
 Thu, 05 Jun 2014 10:29:38 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Thu, 5 Jun 2014 10:29:17 -0700 (PDT)
From: Andrew Ash <andrew@andrewash.com>
Date: Thu, 5 Jun 2014 10:29:17 -0700
Message-ID: <CA+-p3AFdKJmQcXVgCKRtuTUaTjbDqja6QGzDLV9wmxMsfYOeJQ@mail.gmail.com>
Subject: Implementing rdd.scanLeft()
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b3a89d2def17304fb1a163f
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a89d2def17304fb1a163f
Content-Type: text/plain; charset=UTF-8

I have a use case that would greatly benefit from RDDs having a .scanLeft()
method.  Are the project developers interested in adding this to the public
API?


Looking through past message traffic, this has come up a few times.  The
recommendation from the list before has been to implement a parallel prefix
scan.

http://comments.gmane.org/gmane.comp.lang.scala.spark.user/1880
https://groups.google.com/forum/#!topic/spark-users/ts-FdB50ltY

The algorithm Reynold sketched in the first link leads to this working
implementation:

val vector = sc.parallelize(1 to 20, 3)

val sums = 0 +: vector.mapPartitionsWithIndex{ case(partition, iter) =>
Iterator(iter.sum) }.collect.scanLeft(0)(_+_).drop(1)

val prefixScan = vector.mapPartitionsWithIndex { case(partition, iter) =>
  val base = sums(partition)
  println(partition, base)
  iter.scanLeft(base)(_+_).drop(1)
}.collect


I'd love to have that replaced with this:

val vector = sc.parallelize(1 to 20, 3)
val cumSum: RDD[Int] = vector.scanLeft(0)(_+_)


Any thoughts on whether this contribution would be accepted?  What pitfalls
exist that I should be thinking about?

Thanks!
Andrew

--047d7b3a89d2def17304fb1a163f--

From dev-return-7965-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun  5 17:48:35 2014
Return-Path: <dev-return-7965-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B9C5BCF4D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Jun 2014 17:48:35 +0000 (UTC)
Received: (qmail 51757 invoked by uid 500); 5 Jun 2014 17:48:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51686 invoked by uid 500); 5 Jun 2014 17:48:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51675 invoked by uid 99); 5 Jun 2014 17:48:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 17:48:35 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.41] (HELO mail-qg0-f41.google.com) (209.85.192.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 17:48:32 +0000
Received: by mail-qg0-f41.google.com with SMTP id j5so2180900qga.14
        for <dev@spark.apache.org>; Thu, 05 Jun 2014 10:48:08 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=RVfg9uWjrbRwYAvvJkcHUVP6cmN9xuB9e1ElmRfDPes=;
        b=Oe0A8oaVzF8tUH60lviEZlq4QsfGchAPabqzl+WTn9ZKkckq8y6DVQwpnJYdS6wQRy
         y1VKaFJv92yP0Qv8583ZZjAUYQXBuNj84Mi0un4JgASIWaHOQga5QdGMVAzs5yax+cS+
         jjc6Q1ip/MMMGpsHmPFj3wBL3vBEIoOUaW5vpYySSj67YlNVa7g3Kq06LtYWBuwg6ehl
         IbIZLQM7ui4Atdkz4c2/jHAosW0iAmc4U4JWFxIbpaRstnLqlNXgn96VxvjYjQMNfTsp
         6hdU9tTSIbWDAbhH31EPwRzeEZmfhdzbFGlbHCOhrgcih4yjvOtuCbbak1pwFG/B9jVY
         Wk1g==
X-Gm-Message-State: ALoCoQnmYqA8go96UebpzWBkEP/AUsUjfT5I2EJiWYxUlpJg1jmjmi44EUOyeIekytIwWHY91DKv
X-Received: by 10.140.95.164 with SMTP id i33mr80196270qge.6.1401990488117;
 Thu, 05 Jun 2014 10:48:08 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.49.231 with HTTP; Thu, 5 Jun 2014 10:47:47 -0700 (PDT)
In-Reply-To: <CA+-p3AFdKJmQcXVgCKRtuTUaTjbDqja6QGzDLV9wmxMsfYOeJQ@mail.gmail.com>
References: <CA+-p3AFdKJmQcXVgCKRtuTUaTjbDqja6QGzDLV9wmxMsfYOeJQ@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 5 Jun 2014 10:47:47 -0700
Message-ID: <CAPh_B=YNNO87x7wyjb1BFXSo2MKp+LENJPMkgv_qYiouPfyWpQ@mail.gmail.com>
Subject: Re: Implementing rdd.scanLeft()
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c161e205f79404fb1a5908
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c161e205f79404fb1a5908
Content-Type: text/plain; charset=UTF-8

I think the main concern is this would require scanning the data twice, and
maybe the user should be aware of it ...


On Thu, Jun 5, 2014 at 10:29 AM, Andrew Ash <andrew@andrewash.com> wrote:

> I have a use case that would greatly benefit from RDDs having a .scanLeft()
> method.  Are the project developers interested in adding this to the public
> API?
>
>
> Looking through past message traffic, this has come up a few times.  The
> recommendation from the list before has been to implement a parallel prefix
> scan.
>
> http://comments.gmane.org/gmane.comp.lang.scala.spark.user/1880
> https://groups.google.com/forum/#!topic/spark-users/ts-FdB50ltY
>
> The algorithm Reynold sketched in the first link leads to this working
> implementation:
>
> val vector = sc.parallelize(1 to 20, 3)
>
> val sums = 0 +: vector.mapPartitionsWithIndex{ case(partition, iter) =>
> Iterator(iter.sum) }.collect.scanLeft(0)(_+_).drop(1)
>
> val prefixScan = vector.mapPartitionsWithIndex { case(partition, iter) =>
>   val base = sums(partition)
>   println(partition, base)
>   iter.scanLeft(base)(_+_).drop(1)
> }.collect
>
>
> I'd love to have that replaced with this:
>
> val vector = sc.parallelize(1 to 20, 3)
> val cumSum: RDD[Int] = vector.scanLeft(0)(_+_)
>
>
> Any thoughts on whether this contribution would be accepted?  What pitfalls
> exist that I should be thinking about?
>
> Thanks!
> Andrew
>

--001a11c161e205f79404fb1a5908--

From dev-return-7966-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun  5 19:25:28 2014
Return-Path: <dev-return-7966-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5F9D710565
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Jun 2014 19:25:28 +0000 (UTC)
Received: (qmail 76275 invoked by uid 500); 5 Jun 2014 19:25:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76219 invoked by uid 500); 5 Jun 2014 19:25:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76209 invoked by uid 99); 5 Jun 2014 19:25:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 19:25:27 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.128.180] (HELO mail-ve0-f180.google.com) (209.85.128.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 19:25:24 +0000
Received: by mail-ve0-f180.google.com with SMTP id db12so1810985veb.11
        for <dev@spark.apache.org>; Thu, 05 Jun 2014 12:25:00 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=QPGRW1ipdJPkxSS3LO6s10ME+fRPoF+NmqgPAMboUyc=;
        b=e3RrxUbha2qTbfTOl6gnJ/Bph/Hlm44bhQxvezFLAyP5LgRNnf7TH3IBljGJhV4f6q
         1cHGgYBR+Wf2CA3x7U/tQarE2coX0I+B8sR/Jp9k2yZ4GHJyaeAr2blGpmavO2VLljPi
         dTvPoAQ3jYSzRjzcItErmA+1k7QH6iW8ZtPVOsi6e1rEoy0X5gek8kmVRxUwZ3YBcYUp
         Yai9zrNlmNByrpbv/71h92NzvsjoCbyEymPMzCda/3QFxjk2ajYhgRwpg/9zwReKeH7r
         32czhxCuJPqFtsH4Kp5FUHo+kLluEgQpqXocmSJ7+8tDOEHuehGxHT0Q47HhL0aQl5yg
         /wXg==
X-Gm-Message-State: ALoCoQnJ3C6bR2e+q84/DayV4kQrmjbL9c5y+95YuIrWcIBkxSb1sc49VtHU0aS1M0VK1qxsyVQ3
X-Received: by 10.220.98.143 with SMTP id q15mr13742492vcn.38.1401996300293;
        Thu, 05 Jun 2014 12:25:00 -0700 (PDT)
Received: from mail-vc0-f174.google.com (mail-vc0-f174.google.com [209.85.220.174])
        by mx.google.com with ESMTPSA id bk6sm6961587vec.6.2014.06.05.12.24.58
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 05 Jun 2014 12:24:58 -0700 (PDT)
Received: by mail-vc0-f174.google.com with SMTP id ik5so1725749vcb.19
        for <dev@spark.apache.org>; Thu, 05 Jun 2014 12:24:58 -0700 (PDT)
X-Received: by 10.220.251.13 with SMTP id mq13mr4080991vcb.73.1401996298581;
 Thu, 05 Jun 2014 12:24:58 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Thu, 5 Jun 2014 12:24:38 -0700 (PDT)
In-Reply-To: <CAPh_B=YNNO87x7wyjb1BFXSo2MKp+LENJPMkgv_qYiouPfyWpQ@mail.gmail.com>
References: <CA+-p3AFdKJmQcXVgCKRtuTUaTjbDqja6QGzDLV9wmxMsfYOeJQ@mail.gmail.com>
 <CAPh_B=YNNO87x7wyjb1BFXSo2MKp+LENJPMkgv_qYiouPfyWpQ@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Thu, 5 Jun 2014 12:24:38 -0700
Message-ID: <CA+-p3AHP1SJWrtMX2dUG7yUfkYMkCsw=Aq3+5xsZ33jxZ890kQ@mail.gmail.com>
Subject: Re: Implementing rdd.scanLeft()
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e013cbf8a5a7e6004fb1bb393
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013cbf8a5a7e6004fb1bb393
Content-Type: text/plain; charset=UTF-8

I that something that documentation on the method can solve?


On Thu, Jun 5, 2014 at 10:47 AM, Reynold Xin <rxin@databricks.com> wrote:

> I think the main concern is this would require scanning the data twice, and
> maybe the user should be aware of it ...
>
>
> On Thu, Jun 5, 2014 at 10:29 AM, Andrew Ash <andrew@andrewash.com> wrote:
>
> > I have a use case that would greatly benefit from RDDs having a
> .scanLeft()
> > method.  Are the project developers interested in adding this to the
> public
> > API?
> >
> >
> > Looking through past message traffic, this has come up a few times.  The
> > recommendation from the list before has been to implement a parallel
> prefix
> > scan.
> >
> > http://comments.gmane.org/gmane.comp.lang.scala.spark.user/1880
> > https://groups.google.com/forum/#!topic/spark-users/ts-FdB50ltY
> >
> > The algorithm Reynold sketched in the first link leads to this working
> > implementation:
> >
> > val vector = sc.parallelize(1 to 20, 3)
> >
> > val sums = 0 +: vector.mapPartitionsWithIndex{ case(partition, iter) =>
> > Iterator(iter.sum) }.collect.scanLeft(0)(_+_).drop(1)
> >
> > val prefixScan = vector.mapPartitionsWithIndex { case(partition, iter) =>
> >   val base = sums(partition)
> >   println(partition, base)
> >   iter.scanLeft(base)(_+_).drop(1)
> > }.collect
> >
> >
> > I'd love to have that replaced with this:
> >
> > val vector = sc.parallelize(1 to 20, 3)
> > val cumSum: RDD[Int] = vector.scanLeft(0)(_+_)
> >
> >
> > Any thoughts on whether this contribution would be accepted?  What
> pitfalls
> > exist that I should be thinking about?
> >
> > Thanks!
> > Andrew
> >
>

--089e013cbf8a5a7e6004fb1bb393--

From dev-return-7967-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun  5 19:36:50 2014
Return-Path: <dev-return-7967-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0DF4D10606
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Jun 2014 19:36:50 +0000 (UTC)
Received: (qmail 17934 invoked by uid 500); 5 Jun 2014 19:36:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17873 invoked by uid 500); 5 Jun 2014 19:36:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17862 invoked by uid 99); 5 Jun 2014 19:36:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 19:36:49 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of meisam.fathi@gmail.com designates 209.85.213.47 as permitted sender)
Received: from [209.85.213.47] (HELO mail-yh0-f47.google.com) (209.85.213.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 19:36:45 +0000
Received: by mail-yh0-f47.google.com with SMTP id z6so1284483yhz.6
        for <dev@spark.apache.org>; Thu, 05 Jun 2014 12:36:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=X6DTy/1iXJgcaGUJdjSne/AOMVlGrxjmtz0oWpVP+oA=;
        b=OHmWQ3/mfjd5tH89FKKbVKaUHvLzz6TJzkxNAOfg39KKaOYrqIePS/u4v1j39W7WR0
         p2cyfK6p39wepU/dWjJRpk0Ka42LcGK2s1tlOsg1EAo/rheedJposU0YeArlrSR2Efkk
         q/pwc15T/PYlhX3ea4Bbf8X+8ifIbTxQLBd26O9DIU1vKf2qAqy44VjMKJU7rwO2BFX9
         BlEXEJlS+/2+h1Fk7KO1tdVHF8pinH9mF3ySjXAMsKSEZE1x+Cm8JVASEsOzCu07HXOf
         JsPyeuUfbKcTKV5l4wzuOAEN3ePy6vOA3WNprPzefiaCHVA2NxThA60Di4+V0fyF0u6k
         eCAA==
MIME-Version: 1.0
X-Received: by 10.236.149.107 with SMTP id w71mr20903527yhj.106.1401996981786;
 Thu, 05 Jun 2014 12:36:21 -0700 (PDT)
Received: by 10.170.149.67 with HTTP; Thu, 5 Jun 2014 12:36:21 -0700 (PDT)
Date: Thu, 5 Jun 2014 15:36:21 -0400
Message-ID: <CAByMnGu5rHP+rUXFGqHvXQ9=4_YFZggcQR5MQhrhn3bvLPvm1g@mail.gmail.com>
Subject: Building Spark against Scala 2.10.1 virtualized
From: Meisam Fathi <meisam.fathi@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi community,

How should I change sbt to compile spark core with a different version
of Scala? I see maven pom files define dependencies to scala 2.10.4. I
need to override/ignore the maven dependencies and use Scala
virtualized, which needs these lines in a build.sbt file:

scalaOrganization := "org.scala-lang.virtualized"

scalaVersion := "2.10.1"

libraryDependencies += "EPFL" %% "lms" % "0.3-SNAPSHOT"

scalacOptions += "-Yvirtualize"


Thanks,
Meisam

From dev-return-7968-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun  5 20:30:45 2014
Return-Path: <dev-return-7968-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E086010839
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Jun 2014 20:30:45 +0000 (UTC)
Received: (qmail 45665 invoked by uid 500); 5 Jun 2014 20:30:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45611 invoked by uid 500); 5 Jun 2014 20:30:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45601 invoked by uid 99); 5 Jun 2014 20:30:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 20:30:45 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of rickett.stephanie@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 20:30:40 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <rickett.stephanie@gmail.com>)
	id 1WseIl-0008Pg-Gb
	for dev@spark.incubator.apache.org; Thu, 05 Jun 2014 13:30:19 -0700
Date: Thu, 5 Jun 2014 13:30:19 -0700 (PDT)
From: dataginjaninja <rickett.stephanie@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1402000219505-6948.post@n3.nabble.com>
In-Reply-To: <CAAswR-5gfAc3pz9Pj3P3E8gFigotkZkKfLvJjhhKe-WMUK-6UQ@mail.gmail.com>
References: <1401378895078-6850.post@n3.nabble.com> <CA+-p3AGYXHGT2wJaohx8qCsUUEDN1WAf4jUAFaAVJqFY8bmXwQ@mail.gmail.com> <CAAswR-7_5OPBKqO5BS_mUdH6eNGq5gzv1Qg27T=mqdbdCFy7Gg@mail.gmail.com> <1401382933476-6855.post@n3.nabble.com> <CAAswR-4_ZmxRatf9bHd1sg+=38LUwu=ocwPuX3R2EjrNsRsGKA@mail.gmail.com> <1401383823560-6857.post@n3.nabble.com> <CAAswR-5gfAc3pz9Pj3P3E8gFigotkZkKfLvJjhhKe-WMUK-6UQ@mail.gmail.com>
Subject: Re: Timestamp support in v1.0
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I can confirm that the patch fixed my issue. :-)



-----
Cheers,

Stephanie
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Timestamp-support-in-v1-0-tp6850p6948.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7970-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun  5 20:35:53 2014
Return-Path: <dev-return-7970-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 12FE81085A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Jun 2014 20:35:53 +0000 (UTC)
Received: (qmail 54989 invoked by uid 500); 5 Jun 2014 20:35:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54853 invoked by uid 500); 5 Jun 2014 20:35:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54675 invoked by uid 99); 5 Jun 2014 20:35:51 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 20:35:51 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.47] (HELO mail-qa0-f47.google.com) (209.85.216.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 20:35:47 +0000
Received: by mail-qa0-f47.google.com with SMTP id s7so2174378qap.6
        for <dev@spark.apache.org>; Thu, 05 Jun 2014 13:35:26 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=FKo5GkpKD7u5dMj2Z1eOGfwXz8j1XS9Gk0FjkOrN3fM=;
        b=M3TiusofpyACl01dIivNmNRYDOywIUDh+JCMkg5xhwktidB2x2yurxCpruEcxpGbGu
         qPY2hWOxB2Dbi0u/y8UgIfE6atYkJV3IfvowHbB/3SCEDsG4PqoH3P84A7dQkgDNftyZ
         qu7KfvRJuqq2qqxi4fvU85AZC6JdfzduPWSrtR5xfl/+wotBmMB9BjcB2QzctSdj0mE3
         oQxgWbJUc1Llxcmvly5ltgcAqCMqUlyIBCSaKpCq3aBITAIpuLop0L+svqYhelEtkjqc
         cSS0itcaxTIWG0h2PrRcN+wC2Z0jFyjT5+w60S/CSOj0IyMd0qPySfVtYuiT1ayYYvJv
         BCyQ==
X-Gm-Message-State: ALoCoQmLj1RQ8FHmcsR76cMpwOUJGQgnfM3YuRR4oRMc53n3lwc9Z4++9GxVud+1fN4UHeg8HjuH
X-Received: by 10.224.19.196 with SMTP id c4mr108194qab.86.1402000526191; Thu,
 05 Jun 2014 13:35:26 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.209.73 with HTTP; Thu, 5 Jun 2014 13:35:06 -0700 (PDT)
In-Reply-To: <1402000219505-6948.post@n3.nabble.com>
References: <1401378895078-6850.post@n3.nabble.com> <CA+-p3AGYXHGT2wJaohx8qCsUUEDN1WAf4jUAFaAVJqFY8bmXwQ@mail.gmail.com>
 <CAAswR-7_5OPBKqO5BS_mUdH6eNGq5gzv1Qg27T=mqdbdCFy7Gg@mail.gmail.com>
 <1401382933476-6855.post@n3.nabble.com> <CAAswR-4_ZmxRatf9bHd1sg+=38LUwu=ocwPuX3R2EjrNsRsGKA@mail.gmail.com>
 <1401383823560-6857.post@n3.nabble.com> <CAAswR-5gfAc3pz9Pj3P3E8gFigotkZkKfLvJjhhKe-WMUK-6UQ@mail.gmail.com>
 <1402000219505-6948.post@n3.nabble.com>
From: Michael Armbrust <michael@databricks.com>
Date: Thu, 5 Jun 2014 13:35:06 -0700
Message-ID: <CAAswR-57T1k4vWzdVXejV7odXe67PJ9Wybrq1ZBbzjTzVYEusw@mail.gmail.com>
Subject: Re: Timestamp support in v1.0
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11c35be856c2cd04fb1caf8a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c35be856c2cd04fb1caf8a
Content-Type: text/plain; charset=UTF-8

Awesome, thanks for testing!


On Thu, Jun 5, 2014 at 1:30 PM, dataginjaninja <rickett.stephanie@gmail.com>
wrote:

> I can confirm that the patch fixed my issue. :-)
>
>
>
> -----
> Cheers,
>
> Stephanie
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Timestamp-support-in-v1-0-tp6850p6948.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c35be856c2cd04fb1caf8a--

From dev-return-7969-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun  5 20:35:57 2014
Return-Path: <dev-return-7969-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 19DBA1085C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Jun 2014 20:35:57 +0000 (UTC)
Received: (qmail 54740 invoked by uid 500); 5 Jun 2014 20:35:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54674 invoked by uid 500); 5 Jun 2014 20:35:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54664 invoked by uid 99); 5 Jun 2014 20:35:51 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 20:35:51 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.44] (HELO mail-qg0-f44.google.com) (209.85.192.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 20:35:47 +0000
Received: by mail-qg0-f44.google.com with SMTP id i50so2549202qgf.31
        for <dev@spark.incubator.apache.org>; Thu, 05 Jun 2014 13:35:26 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=FKo5GkpKD7u5dMj2Z1eOGfwXz8j1XS9Gk0FjkOrN3fM=;
        b=i1V68j/EtIL5XBKJ4fOV60hlUijGBow+6yZ89ILiTzm8kq5v6SFiWo2/cbVirNyvzZ
         muD9j9yIMqCMzmu9SHKCb10ZJD54qbYu+TmX9vHSyfgJanAVystRh64jZ7mg0BqzvqaX
         fLUpkQ23Ba9odOm+/xDprRrbicvOp4506j1veldV5Dx6kgcS98iQLBgpnZvYLnq2OsOP
         Edh0dQVzizx+UzawKq+zCFC0uPs/bd6hVojwjqfxx5trPrEtTr5S2ZgJFG2I67cSjzyq
         +eUlm5EYASP7wHWU9lCzAYmRU4ZlFwYaMGK+DnDs+DsNnSh51ep+G2G9MC69D4zY8k+2
         CLnw==
X-Gm-Message-State: ALoCoQnAu6a5o0dPO5ixZZRu0hEXIl2FTZ4owRh1Fzfn27uB6FKmPqa2v8S8G6qziHNwpj7clzO8
X-Received: by 10.224.19.196 with SMTP id c4mr108194qab.86.1402000526191; Thu,
 05 Jun 2014 13:35:26 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.209.73 with HTTP; Thu, 5 Jun 2014 13:35:06 -0700 (PDT)
In-Reply-To: <1402000219505-6948.post@n3.nabble.com>
References: <1401378895078-6850.post@n3.nabble.com> <CA+-p3AGYXHGT2wJaohx8qCsUUEDN1WAf4jUAFaAVJqFY8bmXwQ@mail.gmail.com>
 <CAAswR-7_5OPBKqO5BS_mUdH6eNGq5gzv1Qg27T=mqdbdCFy7Gg@mail.gmail.com>
 <1401382933476-6855.post@n3.nabble.com> <CAAswR-4_ZmxRatf9bHd1sg+=38LUwu=ocwPuX3R2EjrNsRsGKA@mail.gmail.com>
 <1401383823560-6857.post@n3.nabble.com> <CAAswR-5gfAc3pz9Pj3P3E8gFigotkZkKfLvJjhhKe-WMUK-6UQ@mail.gmail.com>
 <1402000219505-6948.post@n3.nabble.com>
From: Michael Armbrust <michael@databricks.com>
Date: Thu, 5 Jun 2014 13:35:06 -0700
Message-ID: <CAAswR-57T1k4vWzdVXejV7odXe67PJ9Wybrq1ZBbzjTzVYEusw@mail.gmail.com>
Subject: Re: Timestamp support in v1.0
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11c35be856c2cd04fb1caf8a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c35be856c2cd04fb1caf8a
Content-Type: text/plain; charset=UTF-8

Awesome, thanks for testing!


On Thu, Jun 5, 2014 at 1:30 PM, dataginjaninja <rickett.stephanie@gmail.com>
wrote:

> I can confirm that the patch fixed my issue. :-)
>
>
>
> -----
> Cheers,
>
> Stephanie
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Timestamp-support-in-v1-0-tp6850p6948.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c35be856c2cd04fb1caf8a--

From dev-return-7971-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun  5 20:52:29 2014
Return-Path: <dev-return-7971-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EDBBF109AD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Jun 2014 20:52:28 +0000 (UTC)
Received: (qmail 1991 invoked by uid 500); 5 Jun 2014 20:52:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1935 invoked by uid 500); 5 Jun 2014 20:52:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1924 invoked by uid 99); 5 Jun 2014 20:52:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 20:52:28 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.46 as permitted sender)
Received: from [209.85.160.46] (HELO mail-pb0-f46.google.com) (209.85.160.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 20:52:24 +0000
Received: by mail-pb0-f46.google.com with SMTP id rq2so1626629pbb.19
        for <dev@spark.apache.org>; Thu, 05 Jun 2014 13:51:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=r5ZWAs77z8tVF9Qy21fLNwZWtbDfVJtrmGouP2WHD7Q=;
        b=iUBeiNrzc5ZnCFB3yiiA+UuVn10VxxwUs9a+BHJ9H3CeLpRCWr45vMlHlQzTo6y0D2
         fjUhn+TCj+MBtI8usTQtBkM+PQkm9SOXB4xCLhRC/sOofU/DTXDXqaCB8GWFbGNvdfW3
         Lx2KXtN9o15qAgGY2j/M4XqDTEBGhlQxEHaWdf1HDDvapqaiA+5inT7IbOAWhcwWS85o
         MfZWvAOk8stVbBUDn4Nrf34twCoAv45nNHMbIBx/kTvusxy0AwZ9zDygkQ6Zm18AjfFN
         KYhWC4j/XgK33ada83hxVmr7k2wFtdcwmF7jbTNm3qu5lEMjFwQT3a2YkToj9ApJVTR5
         pDQA==
X-Received: by 10.68.189.232 with SMTP id gl8mr58581pbc.89.1402001519708;
        Thu, 05 Jun 2014 13:51:59 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id tf10sm26686511pbc.70.2014.06.05.13.51.57
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 05 Jun 2014 13:51:57 -0700 (PDT)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
Subject: Re: Building Spark against Scala 2.10.1 virtualized
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAByMnGu5rHP+rUXFGqHvXQ9=4_YFZggcQR5MQhrhn3bvLPvm1g@mail.gmail.com>
Date: Thu, 5 Jun 2014 13:51:56 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <258A3B11-C4E8-409B-A525-C8FF6387AED4@gmail.com>
References: <CAByMnGu5rHP+rUXFGqHvXQ9=4_YFZggcQR5MQhrhn3bvLPvm1g@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.2)
X-Virus-Checked: Checked by ClamAV on apache.org

You can modify project/SparkBuild.scala and build Spark with sbt instead =
of Maven.


On Jun 5, 2014, at 12:36 PM, Meisam Fathi <meisam.fathi@gmail.com> =
wrote:

> Hi community,
>=20
> How should I change sbt to compile spark core with a different version
> of Scala? I see maven pom files define dependencies to scala 2.10.4. I
> need to override/ignore the maven dependencies and use Scala
> virtualized, which needs these lines in a build.sbt file:
>=20
> scalaOrganization :=3D "org.scala-lang.virtualized"
>=20
> scalaVersion :=3D "2.10.1"
>=20
> libraryDependencies +=3D "EPFL" %% "lms" % "0.3-SNAPSHOT"
>=20
> scalacOptions +=3D "-Yvirtualize"
>=20
>=20
> Thanks,
> Meisam


From dev-return-7972-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun  5 21:46:06 2014
Return-Path: <dev-return-7972-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0E57D10B8E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Jun 2014 21:46:06 +0000 (UTC)
Received: (qmail 8582 invoked by uid 500); 5 Jun 2014 21:46:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8520 invoked by uid 500); 5 Jun 2014 21:46:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8510 invoked by uid 99); 5 Jun 2014 21:46:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 21:46:05 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.214.174] (HELO mail-ob0-f174.google.com) (209.85.214.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 21:46:00 +0000
Received: by mail-ob0-f174.google.com with SMTP id uz6so1768790obc.19
        for <dev@spark.apache.org>; Thu, 05 Jun 2014 14:45:38 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:date:from:to:message-id:subject:mime-version
         :content-type;
        bh=g/B51bvqDRQCfs9vskQZY/9qAIw3j1yssrLyhbxwVaE=;
        b=GnYYI4vQc6e3wov7TosZB6ZjDAqrOsJNb65I/4ddngCv5SBcsd6gy6PFxmKd+eV7sH
         4PNyoO0jz9LZCppo/BJkThKUONaYOKK0qqr895/haQpyk4gVMnzWh3Y5yxn+4UVbI3Wd
         SnExOa24jkXriTv4KUgMnj7Fsim7igWGGldatUdQBwYScFFXHXPiqEojQn12VlmwrZy1
         DmLhNJFMwu094WcNuHgQQtNJJEeDf0WMqO342FWQifh9zoi3zNJ0WgXq6tlzsP73F3N5
         /6pXoAwrEYmqxCHez/SbNpIE5LYa67mLLLWXqlLvPi/kNYwVrw4rEmPvSMIINwxLDwMt
         N2EQ==
X-Gm-Message-State: ALoCoQmPNdiLUloQvmjrIr7lJ/6nILMepfYfRzobsYD62KuzY6E54oyi2vH+qt6BEB15H9T19tPL
X-Received: by 10.60.73.39 with SMTP id i7mr548481oev.51.1402004738772;
        Thu, 05 Jun 2014 14:45:38 -0700 (PDT)
Received: from Tims-MacBook-Pro-2.local ([8.28.85.130])
        by mx.google.com with ESMTPSA id bh9sm13279875obb.7.2014.06.05.14.45.38
        for <dev@spark.apache.org>
        (version=SSLv3 cipher=RC4-SHA bits=128/128);
        Thu, 05 Jun 2014 14:45:38 -0700 (PDT)
Date: Thu, 5 Jun 2014 15:45:37 -0600
From: Tim Kellogg <tim@2lemetry.com>
To: dev@spark.apache.org
Message-ID: <etPan.5390e501.47398c89.fd@Tims-MacBook-Pro-2.local>
Subject: Cassandra Examples Don't Work
X-Mailer: Airmail Beta (242)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="5390e501_354fe9f9_fd"
X-Virus-Checked: Checked by ClamAV on apache.org

--5390e501_354fe9f9_fd
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

Hi,

I=E2=80=99ve tried running the CassandraTest example against several vers=
ions of Cassandra and I can=E2=80=99t get it to work. I=E2=80=99m wonderi=
ng if I=E2=80=99m doing something wrong, or if they simply don=E2=80=99t =
work. Please help=21

http://stackoverflow.com/q/24069039/503826

Much Thanks=21

Tim Kellogg
Sr. Software Engineer, Protocols
2lemetry
605-593-7099
=40kellogh
--5390e501_354fe9f9_fd--


From dev-return-7973-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun  5 21:51:59 2014
Return-Path: <dev-return-7973-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 43B5710BB4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Jun 2014 21:51:59 +0000 (UTC)
Received: (qmail 19656 invoked by uid 500); 5 Jun 2014 21:51:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19592 invoked by uid 500); 5 Jun 2014 21:51:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19582 invoked by uid 99); 5 Jun 2014 21:51:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 21:51:58 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of swatt@redhat.com designates 209.132.183.24 as permitted sender)
Received: from [209.132.183.24] (HELO mx3-phx2.redhat.com) (209.132.183.24)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 21:51:55 +0000
Received: from zmail14.collab.prod.int.phx2.redhat.com (zmail14.collab.prod.int.phx2.redhat.com [10.5.83.16])
	by mx3-phx2.redhat.com (8.13.8/8.13.8) with ESMTP id s55LpTBV018411
	for <dev@spark.apache.org>; Thu, 5 Jun 2014 17:51:29 -0400
Date: Thu, 5 Jun 2014 17:51:29 -0400 (EDT)
From: Stephen Watt <swatt@redhat.com>
To: dev@spark.apache.org
Message-ID: <1507854394.24034969.1402005089409.JavaMail.zimbra@redhat.com>
In-Reply-To: <378708004.23998387.1402004635131.JavaMail.zimbra@redhat.com>
Subject: Contributing Spark Infrastructure Configuration Docs
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.12]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - GC35 (Mac)/8.0.6_GA_5922)
Thread-Topic: Contributing Spark Infrastructure Configuration Docs
Thread-Index: 9zy5fW28QPwbpCr46vcwgtm8S9Ux8Q==
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Folks

My name is Steve Watt and I work in the CTO Office at Red Hat. I've recently spent quite a bit of time designing single rack and multi-rack infrastructures for Spark for our own hardware procurement at Red Hat and I thought the diagrams and server specs for both Dell and HP would be useful to the broader community as well. Even if folks don't want to go with my exact design, having the designs as a starting point should save  quite a bit of time. I think I can fold this quite easily into http://spark.apache.org/docs/latest/hardware-provisioning.html

However, before submitting a pull request for the modified page I thought I'd send this note up front and see if anyone wanted to provide some feedback first. If there is none, I'll submit the pull request early next week.

Regards
Steve Watt

From dev-return-7974-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun  5 22:13:14 2014
Return-Path: <dev-return-7974-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DCD0710CDA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Jun 2014 22:13:13 +0000 (UTC)
Received: (qmail 64816 invoked by uid 500); 5 Jun 2014 22:13:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64763 invoked by uid 500); 5 Jun 2014 22:13:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 45128 invoked by uid 99); 5 Jun 2014 22:05:34 -0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of kris@kalish.net does not designate 216.139.236.26 as permitted sender)
Date: Thu, 5 Jun 2014 15:05:10 -0700 (PDT)
From: kriskalish <kris@kalish.net>
To: dev@spark.incubator.apache.org
Message-ID: <1402005910366-6953.post@n3.nabble.com>
Subject: Cannot use pyspark to aggregate on remote EC2 cluster
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I'm in a situation where I have two compute nodes in Amazon EC2 and a third
node that is used to just execute queries. The third node is not part of the
cluster. It's also configured slightly differently. That is, the third node
runs Ubuntu 14.04 while the two cluster nodes run CentOS. 

I launch pyspark on the third node using the following command:
./bin/pyspark --master spark://<ip of cluster master node>:7077

I'm attempting to execute the following python snippet:

data = sc.textFile('s3n://<keyhere>@test/data/y=2014/m=06/d=05/h=02/')
data.count()

However, I get the following exception:

Py4JJavaError: An error occurred while calling o130.collectPartitions.
: org.apache.spark.api.python.PythonException: Traceback (most recent call
last):
  File "/root/spark/python/pyspark/worker.py", line 77, in main
    serializer.dump_stream(func(split_index, iterator), outfile)
  File "/root/spark/python/pyspark/serializers.py", line 191, in dump_stream
    self.serializer.dump_stream(self._batched(iterator), stream)
  File "/root/spark/python/pyspark/serializers.py", line 123, in dump_stream
    for obj in iterator:
  File "/root/spark/python/pyspark/serializers.py", line 180, in _batched
    for item in iterator:
  File "/root/spark/python/pyspark/rdd.py", line 856, in takeUpToNum
    yield next(iterator)
  File "<ipython-input-14-cf25bfc806aa>", line 1, in <lambda>
IndexError: list index out of range

	at org.apache.spark.api.python.PythonRDD$$anon$1.read(PythonRDD.scala:115)
	at
org.apache.spark.api.python.PythonRDD$$anon$1.<init>(PythonRDD.scala:145)
	at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:78)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at
org.apache.spark.scheduler.DAGScheduler.runLocallyWithinThread(DAGScheduler.scala:574)
	at
org.apache.spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:559)


I have been able to narrow the problem down to something related to py4j
because I can run the analogous code in the scala shell. To make matters
more confusing, if I ssh into the master node (CentOS), then execute
pyspark, the exact same code snippet works fine.

Additionally, if I execute data.collect() instead of data.count() on the
third node, then all of the data is written to the console as expected.


If anyone has some ideas on how to continue troubleshooting this problem,
the help would be appreciated. 

Thanks,
Kris





--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Cannot-use-pyspark-to-aggregate-on-remote-EC2-cluster-tp6953.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7978-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun  6 00:22:05 2014
Return-Path: <dev-return-7978-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0CFA8101CF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Jun 2014 00:22:05 +0000 (UTC)
Received: (qmail 38362 invoked by uid 500); 5 Jun 2014 23:55:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38292 invoked by uid 500); 5 Jun 2014 23:55:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38281 invoked by uid 99); 5 Jun 2014 23:55:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 23:55:25 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.212.176 as permitted sender)
Received: from [209.85.212.176] (HELO mail-wi0-f176.google.com) (209.85.212.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 23:55:22 +0000
Received: by mail-wi0-f176.google.com with SMTP id n15so31599wiw.15
        for <dev@spark.apache.org>; Thu, 05 Jun 2014 16:54:58 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=D+e49clxmcEiDtlumKMJUrT1JXXuTfK6/wEhbvaYNiQ=;
        b=Hfm3G1KjeTPakK90xwwRQscrNOXz0MCqLJ1/kH9JhMoF/JUXj8WtmBKaoieJNzPyM5
         mxSBgwN78nPJrcBB9qDoYm+XTcifJkPjqHIurUKEfw1mAKi8tzE6S9bBYux9D5d0LgxX
         G+h5dzlcnOwbPB6mjGrPeHGwx2tO9zdo6NhiBcZXe9pdGLXeuSyw1mTkPbWawhWBuMfZ
         T7OSZzkVD8GgqkF/H22B1v2nJW5J6wHVKMtS8xRwy7SuUKt2ByKbZQ2EWf64pwT9vjku
         oJ3MZzi86GjcvRHb42+2Frb54U0JzkzK7IncEGiSAQs7DKGeRD5LMECQ0RaGELrFx+bT
         uOFQ==
MIME-Version: 1.0
X-Received: by 10.180.106.1 with SMTP id gq1mr20653939wib.45.1402012498801;
 Thu, 05 Jun 2014 16:54:58 -0700 (PDT)
Received: by 10.194.169.227 with HTTP; Thu, 5 Jun 2014 16:54:58 -0700 (PDT)
In-Reply-To: <CA+B-+fw8EkK1ECg=GPACiJUCMRS6C4vVmQC9dwAfpxvcoPDZtA@mail.gmail.com>
References: <CA+B-+fw8EkK1ECg=GPACiJUCMRS6C4vVmQC9dwAfpxvcoPDZtA@mail.gmail.com>
Date: Thu, 5 Jun 2014 16:54:58 -0700
Message-ID: <CAJgQjQ9y_B1LATjt6c1-PmmytBzf2R1XyFVhMayVK2Hay5stMQ@mail.gmail.com>
Subject: Re: Constraint Solver for Spark
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Deb,

Why do you want to make those methods public? If you only need to
replace the solver for subproblems. You can try to make the solver
pluggable. Now it supports least squares and non-negative least
squares. You can define an interface for the subproblem solvers and
maintain the IPM solver at your own code base, if the only information
you need is Y^T Y and Y^T b.

Btw, just curious, what is the use case for quadratic constraints?

Best,
Xiangrui

On Thu, Jun 5, 2014 at 3:38 PM, Debasish Das <debasish.das83@gmail.com> wrote:
> Hi,
>
> We are adding a constrained ALS solver in Spark to solve matrix
> factorization use-cases which needs additional constraints (bounds,
> equality, inequality, quadratic constraints)
>
> We are using a native version of a primal dual SOCP solver due to its small
> memory footprint and sparse ccs matrix computation it uses...The solver
> depends on AMD and LDL packages from Timothy Davis for sparse ccs matrix
> algebra (released under lgpl)...
>
> Due to GPL dependencies, it won't be possible to release the code as Apache
> license for now...If we get good results on our use-cases, we will plan to
> write a version in breeze/modify joptimizer for sparse ccs operations...
>
> I derived ConstrainedALS from Spark mllib ALS and I am comparing the
> performance with default ALS and non-negative ALS as baseline. Plan is to
> release the code as GPL license for community review...I have kept the
> package structure as org.apache.spark.mllib.recommendation
>
> There are some private functions defined in ALS, which I would like to
> reuse....Is it possible to take the private out from the following
> functions:
>
> 1. makeLinkRDDs
> 2. makeInLinkBlock
> 3. makeOutLinkBlock
> 4. randomFactor
> 5. unblockFactors
>
> I don't want to copy any code.... I can ask for a PR to make these
> changes...
>
> Thanks.
> Deb

From dev-return-7975-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun  6 00:34:12 2014
Return-Path: <dev-return-7975-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0AE961035E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Jun 2014 00:34:12 +0000 (UTC)
Received: (qmail 11903 invoked by uid 500); 5 Jun 2014 22:44:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9412 invoked by uid 500); 5 Jun 2014 22:44:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9299 invoked by uid 99); 5 Jun 2014 22:44:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 22:44:49 +0000
X-ASF-Spam-Status: No, hits=2.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.45 as permitted sender)
Received: from [209.85.192.45] (HELO mail-qg0-f45.google.com) (209.85.192.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 22:39:18 +0000
Received: by mail-qg0-f45.google.com with SMTP id z60so2844920qgd.32
        for <dev@spark.apache.org>; Thu, 05 Jun 2014 15:38:58 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=xK/lcJHFO1uzQ586B39OOS5dWWai9Q57aPDXKB6UL/E=;
        b=CJhxDm5+pvcdec9N+CZdYy4z0Hj2BpcjO0YdbceJF4xo1mbh0peIR65DkEbS0BFwe+
         hjilWN+eyn9lHLfFmfxGkYRZMnkr5URvXioLVumluiwxpFNP4yJBw19gCkk9j3+z/sc3
         5U11imThuxwkT9vfEMxdpmkZ+lJ5WqIgqMAvqqoIWa1RMJRpivignjQi3vQJN4TKfHzJ
         LTsYNk9hJZfhkaJaMAuxK34LnoSI/3J0uWO2iDDS0aysWwPB0ocEpk2rtRG1i5Vhb6FZ
         9zEEXVnTC1snPoeHL9R7mJ5qMCKEd2+w9bVlpOGulyACzrubSLShZ6WsBwWS3BiY8GuK
         NKzg==
MIME-Version: 1.0
X-Received: by 10.224.3.202 with SMTP id 10mr1368706qao.24.1402007938359; Thu,
 05 Jun 2014 15:38:58 -0700 (PDT)
Received: by 10.140.82.164 with HTTP; Thu, 5 Jun 2014 15:38:58 -0700 (PDT)
Date: Thu, 5 Jun 2014 15:38:58 -0700
Message-ID: <CA+B-+fw8EkK1ECg=GPACiJUCMRS6C4vVmQC9dwAfpxvcoPDZtA@mail.gmail.com>
Subject: Constraint Solver for Spark
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2478623857804fb1e6939
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2478623857804fb1e6939
Content-Type: text/plain; charset=UTF-8

Hi,

We are adding a constrained ALS solver in Spark to solve matrix
factorization use-cases which needs additional constraints (bounds,
equality, inequality, quadratic constraints)

We are using a native version of a primal dual SOCP solver due to its small
memory footprint and sparse ccs matrix computation it uses...The solver
depends on AMD and LDL packages from Timothy Davis for sparse ccs matrix
algebra (released under lgpl)...

Due to GPL dependencies, it won't be possible to release the code as Apache
license for now...If we get good results on our use-cases, we will plan to
write a version in breeze/modify joptimizer for sparse ccs operations...

I derived ConstrainedALS from Spark mllib ALS and I am comparing the
performance with default ALS and non-negative ALS as baseline. Plan is to
release the code as GPL license for community review...I have kept the
package structure as org.apache.spark.mllib.recommendation

There are some private functions defined in ALS, which I would like to
reuse....Is it possible to take the private out from the following
functions:

1. makeLinkRDDs
2. makeInLinkBlock
3. makeOutLinkBlock
4. randomFactor
5. unblockFactors

I don't want to copy any code.... I can ask for a PR to make these
changes...

Thanks.
Deb

--001a11c2478623857804fb1e6939--

From dev-return-7976-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun  6 00:51:34 2014
Return-Path: <dev-return-7976-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6B8461057C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Jun 2014 00:51:34 +0000 (UTC)
Received: (qmail 67084 invoked by uid 500); 5 Jun 2014 23:04:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67021 invoked by uid 500); 5 Jun 2014 23:04:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67010 invoked by uid 99); 5 Jun 2014 23:04:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 23:04:53 +0000
X-ASF-Spam-Status: No, hits=2.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ksankar42@gmail.com designates 209.85.192.176 as permitted sender)
Received: from [209.85.192.176] (HELO mail-pd0-f176.google.com) (209.85.192.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 23:04:47 +0000
Received: by mail-pd0-f176.google.com with SMTP id p10so1712010pdj.7
        for <dev@spark.apache.org>; Thu, 05 Jun 2014 16:04:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=9wP/kQsRm/vPmTAOtvc2pzqoWxDgfcIB4x5HsKeeYys=;
        b=KLcuxFq85/Uj0leXIarpWIo+/0oiteqC5BcC8SLbhF1J51gc1hZlDWeNfbANSfmd3L
         2NatAVjrstw6/fxRDumi05lKKISbWHze5ETuo3QK7ReqEqRl0UCQhXRLx1WGNu69wyr8
         gUKwoBYKEYvIC93zf3DcfgnyoTwZf5Q9KYY/FvJO3CV8bl4paamBWQwV52ORzmyEuyyY
         A6Qi9baQuxps8dIp1xGLbDg9GvXsOH2eFjkBe090IpkN8kg3r4hh8aV7q3L7DaRxIRyS
         d7ZTjqo792W4QWVeQs61DGU8vNnh6dr0YaGVhqeEgb58jPUIItBtwJ8OxcUSvg+OHRmS
         dd2w==
MIME-Version: 1.0
X-Received: by 10.68.241.68 with SMTP id wg4mr1578541pbc.66.1402009467442;
 Thu, 05 Jun 2014 16:04:27 -0700 (PDT)
Received: by 10.70.22.69 with HTTP; Thu, 5 Jun 2014 16:04:27 -0700 (PDT)
In-Reply-To: <1507854394.24034969.1402005089409.JavaMail.zimbra@redhat.com>
References: <378708004.23998387.1402004635131.JavaMail.zimbra@redhat.com>
	<1507854394.24034969.1402005089409.JavaMail.zimbra@redhat.com>
Date: Thu, 5 Jun 2014 16:04:27 -0700
Message-ID: <CAOTBr2mkH8qSHAc7og_nd4wawzSreX3iBEgnAJ890GYdT_nEUQ@mail.gmail.com>
Subject: Re: Contributing Spark Infrastructure Configuration Docs
From: Krishna Sankar <ksankar42@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b33952d483b9304fb1ec4f4
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b33952d483b9304fb1ec4f4
Content-Type: text/plain; charset=UTF-8

Stephen,
    We are working thru Dell configurations; would be happy to review your
diagrams and offer feedback from our experience. Let me know the URLs.
Cheers
<k/>


On Thu, Jun 5, 2014 at 2:51 PM, Stephen Watt <swatt@redhat.com> wrote:

> Hi Folks
>
> My name is Steve Watt and I work in the CTO Office at Red Hat. I've
> recently spent quite a bit of time designing single rack and multi-rack
> infrastructures for Spark for our own hardware procurement at Red Hat and I
> thought the diagrams and server specs for both Dell and HP would be useful
> to the broader community as well. Even if folks don't want to go with my
> exact design, having the designs as a starting point should save  quite a
> bit of time. I think I can fold this quite easily into
> http://spark.apache.org/docs/latest/hardware-provisioning.html
>
> However, before submitting a pull request for the modified page I thought
> I'd send this note up front and see if anyone wanted to provide some
> feedback first. If there is none, I'll submit the pull request early next
> week.
>
> Regards
> Steve Watt
>

--047d7b33952d483b9304fb1ec4f4--

From dev-return-7977-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun  6 00:57:29 2014
Return-Path: <dev-return-7977-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8B19B105CE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Jun 2014 00:57:29 +0000 (UTC)
Received: (qmail 73559 invoked by uid 500); 5 Jun 2014 23:10:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73506 invoked by uid 500); 5 Jun 2014 23:10:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73496 invoked by uid 99); 5 Jun 2014 23:10:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 23:10:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.128.169] (HELO mail-ve0-f169.google.com) (209.85.128.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Jun 2014 23:10:44 +0000
Received: by mail-ve0-f169.google.com with SMTP id jx11so2131009veb.28
        for <dev@spark.apache.org>; Thu, 05 Jun 2014 16:10:20 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=g2tfXvs1BfrZuKvkUZqoMcJRiQG/q+a8rHm+tzsvy94=;
        b=PVMEOit599NOpVUKcK9KylO4trabJ/cig8j2xTcMV++svTajUOR76VZfTmtbsxfUOT
         v9UXx42fqQagovUFcNF7Q0e6uHMWgAyDjXywwgYEAbsIbSHa+xMnIosmAYDgExB6cL1R
         GEjNmIkb2ykbXVY3ZaIMaJWOoGid57ca+FptuzTw7xU8ZpzEGOQu/EYsobhasyyS/r0g
         YzBA/EGaPdqnU8DQLJ1CWoXHMN4PJxsbvt4YKmNGXOZeKwtyXWdVGSj18Flbe8uKvbwd
         jDYSUv3vdKYum8PAkzC7IR+bVJY5+pAxgID/rfIaUsSBjxiqkZRIy14vFJaH9/eFuydz
         FT4w==
X-Gm-Message-State: ALoCoQlQtnsXc/dpM8CaBFn3cTaponMxw8YkhyNfpPQJZ2fjhaTwbSaON17ml6WbEqS3FX0urZX1
X-Received: by 10.221.20.199 with SMTP id qp7mr1069580vcb.24.1402009819758;
        Thu, 05 Jun 2014 16:10:19 -0700 (PDT)
Received: from mail-vc0-f170.google.com (mail-vc0-f170.google.com [209.85.220.170])
        by mx.google.com with ESMTPSA id om10sm7467291vec.9.2014.06.05.16.10.18
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 05 Jun 2014 16:10:18 -0700 (PDT)
Received: by mail-vc0-f170.google.com with SMTP id lc6so2011991vcb.15
        for <dev@spark.apache.org>; Thu, 05 Jun 2014 16:10:18 -0700 (PDT)
X-Received: by 10.58.74.164 with SMTP id u4mr10289vev.81.1402009818295; Thu,
 05 Jun 2014 16:10:18 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Thu, 5 Jun 2014 16:09:58 -0700 (PDT)
In-Reply-To: <CAOTBr2mkH8qSHAc7og_nd4wawzSreX3iBEgnAJ890GYdT_nEUQ@mail.gmail.com>
References: <378708004.23998387.1402004635131.JavaMail.zimbra@redhat.com>
 <1507854394.24034969.1402005089409.JavaMail.zimbra@redhat.com> <CAOTBr2mkH8qSHAc7og_nd4wawzSreX3iBEgnAJ890GYdT_nEUQ@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Thu, 5 Jun 2014 16:09:58 -0700
Message-ID: <CA+-p3AGxF97sBSoX-70WxxKqg1fwYA7w80gox=3bawYTw0VxCg@mail.gmail.com>
Subject: Re: Contributing Spark Infrastructure Configuration Docs
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b6dc1ec310d1b04fb1ed95d
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6dc1ec310d1b04fb1ed95d
Content-Type: text/plain; charset=UTF-8

I would appreciate seeing the specs you came up with as well but don't need
to particularly quickly.  I'll wait until seeing the PR to comment on the
specifics, but have some questions about the thought process that went into
configuring the hardware.

Is the idea to see how you spec'd out memory/disk/processor for each
physical machine?  How did you optimize the ratios between the three for
Spark specifically?  Were you able to test this configuration against
others to optimize for price per performance?

Sorry for the barrage of questions -- I know talking hardware tends to
bring out the crazy in people.

Cheers,
Andrew


On Thu, Jun 5, 2014 at 4:04 PM, Krishna Sankar <ksankar42@gmail.com> wrote:

> Stephen,
>     We are working thru Dell configurations; would be happy to review your
> diagrams and offer feedback from our experience. Let me know the URLs.
> Cheers
> <k/>
>
>
> On Thu, Jun 5, 2014 at 2:51 PM, Stephen Watt <swatt@redhat.com> wrote:
>
> > Hi Folks
> >
> > My name is Steve Watt and I work in the CTO Office at Red Hat. I've
> > recently spent quite a bit of time designing single rack and multi-rack
> > infrastructures for Spark for our own hardware procurement at Red Hat
> and I
> > thought the diagrams and server specs for both Dell and HP would be
> useful
> > to the broader community as well. Even if folks don't want to go with my
> > exact design, having the designs as a starting point should save  quite a
> > bit of time. I think I can fold this quite easily into
> > http://spark.apache.org/docs/latest/hardware-provisioning.html
> >
> > However, before submitting a pull request for the modified page I thought
> > I'd send this note up front and see if anyone wanted to provide some
> > feedback first. If there is none, I'll submit the pull request early next
> > week.
> >
> > Regards
> > Steve Watt
> >
>

--047d7b6dc1ec310d1b04fb1ed95d--

From dev-return-7979-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun  6 02:21:07 2014
Return-Path: <dev-return-7979-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A8DEB107AB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Jun 2014 02:21:07 +0000 (UTC)
Received: (qmail 21017 invoked by uid 500); 6 Jun 2014 02:21:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20958 invoked by uid 500); 6 Jun 2014 02:21:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20947 invoked by uid 99); 6 Jun 2014 02:21:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Jun 2014 02:21:07 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.44 as permitted sender)
Received: from [209.85.216.44] (HELO mail-qa0-f44.google.com) (209.85.216.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Jun 2014 02:21:01 +0000
Received: by mail-qa0-f44.google.com with SMTP id j7so2719049qaq.3
        for <dev@spark.apache.org>; Thu, 05 Jun 2014 19:20:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=YkPk+F9bN0/N/XmsOv+oxNRsHil8L0drdI2N5gM9CsI=;
        b=DTgzhHEp66HWY0p+3Ef0d27Ux7pDpscexEJt6d8wplT2UiWwphQEVcKDc9ql8ylZuo
         YZpkHe0pnL1b3LohmKnt6WtwU37hwvhmwHJzb3mrWrhkLX+eV/1opscIuvPBCKRT3h8J
         bAm4930JvRobMuSGtrWS7R6n11SoYbO3Y9eX6L76xhlzmi+k/zcssIv80n+e2bSl4Dxh
         I8o29Fw6BEHPTa45iaXOiKeOUS9LLae8UI+UYPwuuMKuGpqrUj3o1zXZcLM6xl/+4w70
         subD/0dr967SKZW7SW4mn7UfTBFy+FUkQ4jZe5mt4W4K/ZCXHMbonMx6hbmeJfhd/ruH
         QqRA==
MIME-Version: 1.0
X-Received: by 10.224.92.75 with SMTP id q11mr2847209qam.15.1402021241111;
 Thu, 05 Jun 2014 19:20:41 -0700 (PDT)
Received: by 10.140.82.164 with HTTP; Thu, 5 Jun 2014 19:20:41 -0700 (PDT)
Received: by 10.140.82.164 with HTTP; Thu, 5 Jun 2014 19:20:41 -0700 (PDT)
In-Reply-To: <CAJgQjQ9y_B1LATjt6c1-PmmytBzf2R1XyFVhMayVK2Hay5stMQ@mail.gmail.com>
References: <CA+B-+fw8EkK1ECg=GPACiJUCMRS6C4vVmQC9dwAfpxvcoPDZtA@mail.gmail.com>
	<CAJgQjQ9y_B1LATjt6c1-PmmytBzf2R1XyFVhMayVK2Hay5stMQ@mail.gmail.com>
Date: Thu, 5 Jun 2014 19:20:41 -0700
Message-ID: <CA+B-+fxE2rMOf_uURn7OtF_HXpjgZFSRZWD4QS=ZDyyvT-HTAQ@mail.gmail.com>
Subject: Re: Constraint Solver for Spark
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01493f100b4f7004fb218221
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01493f100b4f7004fb218221
Content-Type: text/plain; charset=UTF-8

Hi Xiangrui,

For orthogonality properties in the factors we need a constraint solver
other than the usuals (l1, upper and lower bounds, l2 etc)

The interface of constraint solver is standard and I can add it in mllib
optimization....

But I am not sure how will I call the gpl licensed ipm solver from
mllib....assume the solver interface is as follows:

Qpsolver (densematrix h, array [double] f, int linearEquality, int
linearInequality, bool lb, bool ub)

And then I have functions to update equalities, inequalities, bounds etc
followed by the run which generates the solution....

For l1 constraints I have to use epigraph formulation which needs a
variable transformation before the solve....

I was thinking that for the problems that does not need constraints people
will use ALS.scala and ConstrainedALS.scala will have the constrained
formulations....

I can point you to the code once it is ready and then you can guide me how
to refactor it to mllib als ?

Thanks.
Deb
Hi Deb,

Why do you want to make those methods public? If you only need to
replace the solver for subproblems. You can try to make the solver
pluggable. Now it supports least squares and non-negative least
squares. You can define an interface for the subproblem solvers and
maintain the IPM solver at your own code base, if the only information
you need is Y^T Y and Y^T b.

Btw, just curious, what is the use case for quadratic constraints?

Best,
Xiangrui

On Thu, Jun 5, 2014 at 3:38 PM, Debasish Das <debasish.das83@gmail.com>
wrote:
> Hi,
>
> We are adding a constrained ALS solver in Spark to solve matrix
> factorization use-cases which needs additional constraints (bounds,
> equality, inequality, quadratic constraints)
>
> We are using a native version of a primal dual SOCP solver due to its
small
> memory footprint and sparse ccs matrix computation it uses...The solver
> depends on AMD and LDL packages from Timothy Davis for sparse ccs matrix
> algebra (released under lgpl)...
>
> Due to GPL dependencies, it won't be possible to release the code as
Apache
> license for now...If we get good results on our use-cases, we will plan to
> write a version in breeze/modify joptimizer for sparse ccs operations...
>
> I derived ConstrainedALS from Spark mllib ALS and I am comparing the
> performance with default ALS and non-negative ALS as baseline. Plan is to
> release the code as GPL license for community review...I have kept the
> package structure as org.apache.spark.mllib.recommendation
>
> There are some private functions defined in ALS, which I would like to
> reuse....Is it possible to take the private out from the following
> functions:
>
> 1. makeLinkRDDs
> 2. makeInLinkBlock
> 3. makeOutLinkBlock
> 4. randomFactor
> 5. unblockFactors
>
> I don't want to copy any code.... I can ask for a PR to make these
> changes...
>
> Thanks.
> Deb

--089e01493f100b4f7004fb218221--

From dev-return-7980-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun  6 06:31:52 2014
Return-Path: <dev-return-7980-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C016A10E6A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Jun 2014 06:31:52 +0000 (UTC)
Received: (qmail 69388 invoked by uid 500); 6 Jun 2014 06:31:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69325 invoked by uid 500); 6 Jun 2014 06:31:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69312 invoked by uid 99); 6 Jun 2014 06:31:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Jun 2014 06:31:52 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 74.125.82.47 as permitted sender)
Received: from [74.125.82.47] (HELO mail-wg0-f47.google.com) (74.125.82.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Jun 2014 06:31:46 +0000
Received: by mail-wg0-f47.google.com with SMTP id k14so1357281wgh.30
        for <dev@spark.apache.org>; Thu, 05 Jun 2014 23:31:25 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=K4U9c4Qnqm9h0sZcru0rDZqJwTwL3djVOieonOQQZMU=;
        b=f131M3rku8addxvA9601vaiz5dLTJ0rk+5gss/z8aYUQmUmRr7s9EpOQBMMebj0GwF
         InMwhE69mxbCvKEvWxtlWnnFP6PZ/d5PSez9ZN5ru9dkxUMuEP/+4owcnnPfimrSGvG1
         xjJI0f9dZNp9vwwWGKDG8SArzB/5utyoxuWm2SaHy43WE1MfVNX/NKL+WpYlEORJX0Q2
         s3Pkv3Cq/lxfl+GuBbxFdk1TudHptNqF2Eg1GamSTOfFrRY4/3ERHIUHS/3Y1gs0j/MU
         NnsytQfoqIDATUeAUFdH6e122bXgabE5qRFY1g2JaWrxLwXGpdyukqp488GlDFw+ijdn
         ML/w==
MIME-Version: 1.0
X-Received: by 10.194.80.161 with SMTP id s1mr3463019wjx.47.1402036285440;
 Thu, 05 Jun 2014 23:31:25 -0700 (PDT)
Received: by 10.194.169.227 with HTTP; Thu, 5 Jun 2014 23:31:25 -0700 (PDT)
In-Reply-To: <CA+B-+fxE2rMOf_uURn7OtF_HXpjgZFSRZWD4QS=ZDyyvT-HTAQ@mail.gmail.com>
References: <CA+B-+fw8EkK1ECg=GPACiJUCMRS6C4vVmQC9dwAfpxvcoPDZtA@mail.gmail.com>
	<CAJgQjQ9y_B1LATjt6c1-PmmytBzf2R1XyFVhMayVK2Hay5stMQ@mail.gmail.com>
	<CA+B-+fxE2rMOf_uURn7OtF_HXpjgZFSRZWD4QS=ZDyyvT-HTAQ@mail.gmail.com>
Date: Thu, 5 Jun 2014 23:31:25 -0700
Message-ID: <CAJgQjQ9UWjURVD2U0uTUUc2DHH5-_VXQ=uH8mFYZvjLgCLSr9w@mail.gmail.com>
Subject: Re: Constraint Solver for Spark
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I don't quite understand why putting linear constraints can promote
orthogonality. For the interfaces, if the subproblem is determined by
Y^T Y and Y^T b for each iteration, then the least squares solver, the
non-negative least squares solver, or your convex solver is simply a
function

(A, b) -> x.

You can define it as an interface, and make the solver pluggable by
adding a setter to ALS. If you want to use your lgpl solver, just
include it in the classpath. Creating two separate files still seems
unnecessary to me. Could you create a JIRA and we can move our
discussion there? Thanks!

Best,
Xiangrui

On Thu, Jun 5, 2014 at 7:20 PM, Debasish Das <debasish.das83@gmail.com> wrote:
> Hi Xiangrui,
>
> For orthogonality properties in the factors we need a constraint solver
> other than the usuals (l1, upper and lower bounds, l2 etc)
>
> The interface of constraint solver is standard and I can add it in mllib
> optimization....
>
> But I am not sure how will I call the gpl licensed ipm solver from
> mllib....assume the solver interface is as follows:
>
> Qpsolver (densematrix h, array [double] f, int linearEquality, int
> linearInequality, bool lb, bool ub)
>
> And then I have functions to update equalities, inequalities, bounds etc
> followed by the run which generates the solution....
>
> For l1 constraints I have to use epigraph formulation which needs a
> variable transformation before the solve....
>
> I was thinking that for the problems that does not need constraints people
> will use ALS.scala and ConstrainedALS.scala will have the constrained
> formulations....
>
> I can point you to the code once it is ready and then you can guide me how
> to refactor it to mllib als ?
>
> Thanks.
> Deb
> Hi Deb,
>
> Why do you want to make those methods public? If you only need to
> replace the solver for subproblems. You can try to make the solver
> pluggable. Now it supports least squares and non-negative least
> squares. You can define an interface for the subproblem solvers and
> maintain the IPM solver at your own code base, if the only information
> you need is Y^T Y and Y^T b.
>
> Btw, just curious, what is the use case for quadratic constraints?
>
> Best,
> Xiangrui
>
> On Thu, Jun 5, 2014 at 3:38 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>> Hi,
>>
>> We are adding a constrained ALS solver in Spark to solve matrix
>> factorization use-cases which needs additional constraints (bounds,
>> equality, inequality, quadratic constraints)
>>
>> We are using a native version of a primal dual SOCP solver due to its
> small
>> memory footprint and sparse ccs matrix computation it uses...The solver
>> depends on AMD and LDL packages from Timothy Davis for sparse ccs matrix
>> algebra (released under lgpl)...
>>
>> Due to GPL dependencies, it won't be possible to release the code as
> Apache
>> license for now...If we get good results on our use-cases, we will plan to
>> write a version in breeze/modify joptimizer for sparse ccs operations...
>>
>> I derived ConstrainedALS from Spark mllib ALS and I am comparing the
>> performance with default ALS and non-negative ALS as baseline. Plan is to
>> release the code as GPL license for community review...I have kept the
>> package structure as org.apache.spark.mllib.recommendation
>>
>> There are some private functions defined in ALS, which I would like to
>> reuse....Is it possible to take the private out from the following
>> functions:
>>
>> 1. makeLinkRDDs
>> 2. makeInLinkBlock
>> 3. makeOutLinkBlock
>> 4. randomFactor
>> 5. unblockFactors
>>
>> I don't want to copy any code.... I can ask for a PR to make these
>> changes...
>>
>> Thanks.
>> Deb

From dev-return-7981-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun  6 16:03:45 2014
Return-Path: <dev-return-7981-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9395011E26
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Jun 2014 16:03:45 +0000 (UTC)
Received: (qmail 91778 invoked by uid 500); 6 Jun 2014 16:03:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91716 invoked by uid 500); 6 Jun 2014 16:03:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91706 invoked by uid 99); 6 Jun 2014 16:03:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Jun 2014 16:03:45 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_HELO_PASS
X-Spam-Check-By: apache.org
Received-SPF: unknown (athena.apache.org: error in processing during lookup of jdonahue@adobe.com)
Received: from [207.46.163.186] (HELO na01-bn1-obe.outbound.protection.outlook.com) (207.46.163.186)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Jun 2014 16:03:38 +0000
Received: from BLUPR02MB405.namprd02.prod.outlook.com (10.141.80.13) by
 BLUPR02MB407.namprd02.prod.outlook.com (10.141.80.17) with Microsoft SMTP
 Server (TLS) id 15.0.954.9; Fri, 6 Jun 2014 16:03:16 +0000
Received: from BLUPR02MB405.namprd02.prod.outlook.com ([10.141.80.13]) by
 BLUPR02MB405.namprd02.prod.outlook.com ([10.141.80.13]) with mapi id
 15.00.0954.000; Fri, 6 Jun 2014 16:03:16 +0000
From: Jim Donahue <jdonahue@adobe.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Getting this sporadically running Spark Streaming
Thread-Topic: Getting this sporadically running Spark Streaming
Thread-Index: AQHPgaDTkjm3V+Id7k2VNEbPKaK7rQ==
Date: Fri, 6 Jun 2014 16:03:15 +0000
Message-ID: <CFB7344B.1496B%jdonahue@adobe.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
user-agent: Microsoft-MacOutlook/14.4.2.140509
x-originating-ip: [192.150.10.200]
x-microsoft-antispam: BL:0;ACTION:Default;RISK:Low;SCL:0;SPMLVL:NotSpam;PCL:0;RULEID:
x-forefront-prvs: 023495660C
x-forefront-antispam-report: SFV:NSPM;SFS:(6009001)(428001)(199002)(189002)(479174003)(36756003)(76482001)(77096999)(99396002)(86362001)(77982001)(87936001)(2656002)(50986999)(79102001)(80022001)(54356999)(85852003)(83072002)(20776003)(4396001)(83322001)(99286001)(64706001)(92566001)(19580395003)(92726001)(74502001)(46102001)(74662001)(31966008)(101416001)(83506001)(16236675004)(81542001)(66066001)(81342001)(21056001);DIR:OUT;SFP:;SCL:1;SRVR:BLUPR02MB407;H:BLUPR02MB405.namprd02.prod.outlook.com;FPR:;MLV:sfv;PTR:InfoNoRecords;A:1;MX:1;LANG:en;
received-spf: None (: adobe.com does not designate permitted sender hosts)
authentication-results: spf=none (sender IP is )
 smtp.mailfrom=jdonahue@adobe.com; 
Content-Type: multipart/alternative;
	boundary="_000_CFB7344B1496Bjdonahueadobecom_"
MIME-Version: 1.0
X-OriginatorOrg: adobe.com
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_CFB7344B1496Bjdonahueadobecom_
Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

I'm seeing this sporadically while running a Spark Streaming job on Spark 1=
.0.0.



Jim


14/06/06 08:53:56 ERROR LiveListenerBus: Listener JobProgressListener threw=
 an exception

java.util.NoSuchElementException: key not found: 1029063

at scala.collection.MapLike$class.default(MapLike.scala:228)

at scala.collection.AbstractMap.default(Map.scala:58)

at scala.collection.mutable.HashMap.apply(HashMap.scala:64)

at org.apache.spark.ui.jobs.JobProgressListener.onStageCompleted(JobProgres=
sListener.scala:78)

at org.apache.spark.scheduler.SparkListenerBus$$anonfun$postToAll$2.apply(S=
parkListenerBus.scala:48)

at org.apache.spark.scheduler.SparkListenerBus$$anonfun$postToAll$2.apply(S=
parkListenerBus.scala:48)

at org.apache.spark.scheduler.SparkListenerBus$$anonfun$foreachListener$1.a=
pply(SparkListenerBus.scala:81)

at org.apache.spark.scheduler.SparkListenerBus$$anonfun$foreachListener$1.a=
pply(SparkListenerBus.scala:79)

at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.sca=
la:59)

at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

at org.apache.spark.scheduler.SparkListenerBus$class.foreachListener(SparkL=
istenerBus.scala:79)

at org.apache.spark.scheduler.SparkListenerBus$class.postToAll(SparkListene=
rBus.scala:48)

at org.apache.spark.scheduler.LiveListenerBus.postToAll(LiveListenerBus.sca=
la:32)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonf=
un$apply$mcV$sp$1.apply(LiveListenerBus.scala:56)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonf=
un$apply$mcV$sp$1.apply(LiveListenerBus.scala:56)

at scala.Option.foreach(Option.scala:236)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$=
mcV$sp(LiveListenerBus.scala:56)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(=
LiveListenerBus.scala:47)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(=
LiveListenerBus.scala:47)

at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1160)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.s=
cala:46)

14/06/06 08:53:56 ERROR LiveListenerBus: Listener JobProgressListener threw=
 an exception

java.util.NoSuchElementException: key not found: 1029105

at scala.collection.MapLike$class.default(MapLike.scala:228)

at scala.collection.AbstractMap.default(Map.scala:58)

at scala.collection.mutable.HashMap.apply(HashMap.scala:64)

at org.apache.spark.ui.jobs.JobProgressListener.onStageCompleted(JobProgres=
sListener.scala:78)

at org.apache.spark.scheduler.SparkListenerBus$$anonfun$postToAll$2.apply(S=
parkListenerBus.scala:48)

at org.apache.spark.scheduler.SparkListenerBus$$anonfun$postToAll$2.apply(S=
parkListenerBus.scala:48)

at org.apache.spark.scheduler.SparkListenerBus$$anonfun$foreachListener$1.a=
pply(SparkListenerBus.scala:81)

at org.apache.spark.scheduler.SparkListenerBus$$anonfun$foreachListener$1.a=
pply(SparkListenerBus.scala:79)

at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.sca=
la:59)

at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)

at org.apache.spark.scheduler.SparkListenerBus$class.foreachListener(SparkL=
istenerBus.scala:79)

at org.apache.spark.scheduler.SparkListenerBus$class.postToAll(SparkListene=
rBus.scala:48)

at org.apache.spark.scheduler.LiveListenerBus.postToAll(LiveListenerBus.sca=
la:32)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonf=
un$apply$mcV$sp$1.apply(LiveListenerBus.scala:56)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1$$anonf=
un$apply$mcV$sp$1.apply(LiveListenerBus.scala:56)

at scala.Option.foreach(Option.scala:236)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$=
mcV$sp(LiveListenerBus.scala:56)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(=
LiveListenerBus.scala:47)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(=
LiveListenerBus.scala:47)

at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1160)

at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.s=
cala:46)


--_000_CFB7344B1496Bjdonahueadobecom_--

From dev-return-7982-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun  6 17:42:40 2014
Return-Path: <dev-return-7982-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2DB0F11280
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Jun 2014 17:42:40 +0000 (UTC)
Received: (qmail 42166 invoked by uid 500); 6 Jun 2014 17:42:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41977 invoked by uid 500); 6 Jun 2014 17:42:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41966 invoked by uid 99); 6 Jun 2014 17:42:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Jun 2014 17:42:39 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.52 as permitted sender)
Received: from [209.85.216.52] (HELO mail-qa0-f52.google.com) (209.85.216.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Jun 2014 17:42:35 +0000
Received: by mail-qa0-f52.google.com with SMTP id cm18so4358538qab.25
        for <dev@spark.apache.org>; Fri, 06 Jun 2014 10:42:11 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=L1NqxAFTQwV3y/uXqokxm0FPl9jBYEbp9jPaWjQWRhE=;
        b=xg2cMI2MhxVKcD8lx+G/n6JcYyIlLNSI6O93vggvbTfHIoxHATuOjd8iCxazTrGJhQ
         /QMZYYZuxDJ1ALySX+Kgui4yIa7hcu8CYK0kbUd0e7LZ24jsup+CqA3JRLT8Xsv1gMTr
         JVzDd6im7KE1v0DyYGLP4ulk0j/ZynXwSv34vs+7f17YJzNSEGQDCxeFqBb47GH9vcoT
         Bv/6wa+E9r4afzRgGHw/V6unb4gnkjn8azi+durSFP3BAM8H2fowfP5zzPi0fFDb4zYU
         HKW98h0/xnnxCBDtuqWN+63DwImWFfgMmv+1wwCISsUjKuvLqTnT0Hqj2Wk0ML3yITDi
         SGNg==
MIME-Version: 1.0
X-Received: by 10.140.48.1 with SMTP id n1mr10510360qga.107.1402076531231;
 Fri, 06 Jun 2014 10:42:11 -0700 (PDT)
Received: by 10.140.82.164 with HTTP; Fri, 6 Jun 2014 10:42:11 -0700 (PDT)
In-Reply-To: <CAJgQjQ9UWjURVD2U0uTUUc2DHH5-_VXQ=uH8mFYZvjLgCLSr9w@mail.gmail.com>
References: <CA+B-+fw8EkK1ECg=GPACiJUCMRS6C4vVmQC9dwAfpxvcoPDZtA@mail.gmail.com>
	<CAJgQjQ9y_B1LATjt6c1-PmmytBzf2R1XyFVhMayVK2Hay5stMQ@mail.gmail.com>
	<CA+B-+fxE2rMOf_uURn7OtF_HXpjgZFSRZWD4QS=ZDyyvT-HTAQ@mail.gmail.com>
	<CAJgQjQ9UWjURVD2U0uTUUc2DHH5-_VXQ=uH8mFYZvjLgCLSr9w@mail.gmail.com>
Date: Fri, 6 Jun 2014 10:42:11 -0700
Message-ID: <CA+B-+fx64WwiSK_ymuvEXMnw2sd+F5fCL3-mQiJbexqyJgzbww@mail.gmail.com>
Subject: Re: Constraint Solver for Spark
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11350eac9790d404fb2e618e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11350eac9790d404fb2e618e
Content-Type: text/plain; charset=UTF-8

Hi Xiangrui,

It's not the linear constraint, It is quadratic inequality with slack,
first order taylor approximation of off diagonal cross terms and a cyclic
coordinate descent, which we think will yield orthogonality....It's still
under works...

Also we want to put a L1 constraint as set of linear equations when solving
for ALS...

I will create the JIRA...as I see it, this will evolve to a generic
constraint solver for machine learning problems that has a QP
structure....ALS is one example....another example is kernel SVMs...

I did not know that lgpl solver can be added to the classpath....if it can
be then definitely we should add these in ALS.scala...

Thanks.
Deb



On Thu, Jun 5, 2014 at 11:31 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> I don't quite understand why putting linear constraints can promote
> orthogonality. For the interfaces, if the subproblem is determined by
> Y^T Y and Y^T b for each iteration, then the least squares solver, the
> non-negative least squares solver, or your convex solver is simply a
> function
>
> (A, b) -> x.
>
> You can define it as an interface, and make the solver pluggable by
> adding a setter to ALS. If you want to use your lgpl solver, just
> include it in the classpath. Creating two separate files still seems
> unnecessary to me. Could you create a JIRA and we can move our
> discussion there? Thanks!
>
> Best,
> Xiangrui
>
> On Thu, Jun 5, 2014 at 7:20 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
> > Hi Xiangrui,
> >
> > For orthogonality properties in the factors we need a constraint solver
> > other than the usuals (l1, upper and lower bounds, l2 etc)
> >
> > The interface of constraint solver is standard and I can add it in mllib
> > optimization....
> >
> > But I am not sure how will I call the gpl licensed ipm solver from
> > mllib....assume the solver interface is as follows:
> >
> > Qpsolver (densematrix h, array [double] f, int linearEquality, int
> > linearInequality, bool lb, bool ub)
> >
> > And then I have functions to update equalities, inequalities, bounds etc
> > followed by the run which generates the solution....
> >
> > For l1 constraints I have to use epigraph formulation which needs a
> > variable transformation before the solve....
> >
> > I was thinking that for the problems that does not need constraints
> people
> > will use ALS.scala and ConstrainedALS.scala will have the constrained
> > formulations....
> >
> > I can point you to the code once it is ready and then you can guide me
> how
> > to refactor it to mllib als ?
> >
> > Thanks.
> > Deb
> > Hi Deb,
> >
> > Why do you want to make those methods public? If you only need to
> > replace the solver for subproblems. You can try to make the solver
> > pluggable. Now it supports least squares and non-negative least
> > squares. You can define an interface for the subproblem solvers and
> > maintain the IPM solver at your own code base, if the only information
> > you need is Y^T Y and Y^T b.
> >
> > Btw, just curious, what is the use case for quadratic constraints?
> >
> > Best,
> > Xiangrui
> >
> > On Thu, Jun 5, 2014 at 3:38 PM, Debasish Das <debasish.das83@gmail.com>
> > wrote:
> >> Hi,
> >>
> >> We are adding a constrained ALS solver in Spark to solve matrix
> >> factorization use-cases which needs additional constraints (bounds,
> >> equality, inequality, quadratic constraints)
> >>
> >> We are using a native version of a primal dual SOCP solver due to its
> > small
> >> memory footprint and sparse ccs matrix computation it uses...The solver
> >> depends on AMD and LDL packages from Timothy Davis for sparse ccs matrix
> >> algebra (released under lgpl)...
> >>
> >> Due to GPL dependencies, it won't be possible to release the code as
> > Apache
> >> license for now...If we get good results on our use-cases, we will plan
> to
> >> write a version in breeze/modify joptimizer for sparse ccs operations...
> >>
> >> I derived ConstrainedALS from Spark mllib ALS and I am comparing the
> >> performance with default ALS and non-negative ALS as baseline. Plan is
> to
> >> release the code as GPL license for community review...I have kept the
> >> package structure as org.apache.spark.mllib.recommendation
> >>
> >> There are some private functions defined in ALS, which I would like to
> >> reuse....Is it possible to take the private out from the following
> >> functions:
> >>
> >> 1. makeLinkRDDs
> >> 2. makeInLinkBlock
> >> 3. makeOutLinkBlock
> >> 4. randomFactor
> >> 5. unblockFactors
> >>
> >> I don't want to copy any code.... I can ask for a PR to make these
> >> changes...
> >>
> >> Thanks.
> >> Deb
>

--001a11350eac9790d404fb2e618e--

From dev-return-7983-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun  6 18:27:29 2014
Return-Path: <dev-return-7983-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5E2C611465
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Jun 2014 18:27:29 +0000 (UTC)
Received: (qmail 77003 invoked by uid 500); 6 Jun 2014 18:27:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76946 invoked by uid 500); 6 Jun 2014 18:27:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76936 invoked by uid 99); 6 Jun 2014 18:27:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Jun 2014 18:27:28 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of Yan.Zhou.sc@huawei.com designates 206.16.17.72 as permitted sender)
Received: from [206.16.17.72] (HELO dfwrgout.huawei.com) (206.16.17.72)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Jun 2014 18:27:24 +0000
Received: from 172.18.9.243 (EHLO dfweml702-chm.china.huawei.com) ([172.18.9.243])
	by dfwrg02-dlp.huawei.com (MOS 4.3.7-GA FastPath queued)
	with ESMTP id AUR38889;
	Fri, 06 Jun 2014 13:27:00 -0500 (CDT)
Received: from SJCEML702-CHM.china.huawei.com (10.212.94.48) by
 dfweml702-chm.china.huawei.com (10.193.5.72) with Microsoft SMTP Server (TLS)
 id 14.3.158.1; Fri, 6 Jun 2014 11:26:59 -0700
Received: from SJCEML701-CHM.china.huawei.com ([169.254.3.133]) by
 SJCEML702-CHM.china.huawei.com ([169.254.4.7]) with mapi id 14.03.0158.001;
 Fri, 6 Jun 2014 11:26:56 -0700
From: "Yan Zhou.sc" <Yan.Zhou.sc@huawei.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Opiq for SParkSQL?
Thread-Topic: Opiq for SParkSQL?
Thread-Index: Ac+BtOUlguBRXQyQQTaXDaY2WTBLDQ==
Date: Fri, 6 Jun 2014 18:26:56 +0000
Message-ID: <C434A3773D08A842B26FED6A1BA2E6546D128C20@SJCEML701-CHM.china.huawei.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [10.193.36.71]
Content-Type: multipart/alternative;
	boundary="_000_C434A3773D08A842B26FED6A1BA2E6546D128C20SJCEML701CHMchi_"
MIME-Version: 1.0
X-CFilter-Loop: Reflected
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_C434A3773D08A842B26FED6A1BA2E6546D128C20SJCEML701CHMchi_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Can anybody share your thoughts/comments/interests of applicability of the =
"optiq"  framework to Spark, and SparkSQL in particular?

Thanks,

--_000_C434A3773D08A842B26FED6A1BA2E6546D128C20SJCEML701CHMchi_--

From dev-return-7984-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jun  7 20:27:34 2014
Return-Path: <dev-return-7984-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 38B8111E71
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  7 Jun 2014 20:27:34 +0000 (UTC)
Received: (qmail 26135 invoked by uid 500); 7 Jun 2014 20:27:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26074 invoked by uid 500); 7 Jun 2014 20:27:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26063 invoked by uid 99); 7 Jun 2014 20:27:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 07 Jun 2014 20:27:30 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ctn@adatao.com designates 209.85.223.173 as permitted sender)
Received: from [209.85.223.173] (HELO mail-ie0-f173.google.com) (209.85.223.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 07 Jun 2014 20:27:28 +0000
Received: by mail-ie0-f173.google.com with SMTP id y20so1412537ier.18
        for <dev@spark.apache.org>; Sat, 07 Jun 2014 13:27:03 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=adatao.com; s=google;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=DFsOgv+TedWwEM8shkMoOwBoW5sB7g3i5qg+KgSgMWE=;
        b=djbDCNdFjCzbpNIae/ybhGuPKwJoYDMjpq6dBE45AC18m75lOSVtDkD8iib1v1DHBP
         GnvFkp8pgb8de4AEZ2pu1Wa0RotU+DI1wvUK5pPsDte+kjJa0zZWd9AIYWdujoouuDUe
         NXuOvrcIEKGnp5px9XGjaCI0LTIzO8YHUqUrQ=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=DFsOgv+TedWwEM8shkMoOwBoW5sB7g3i5qg+KgSgMWE=;
        b=WeGsPjKiiAfuvQtf07asxl+RMQkU40X4MWNYK9Dn8uAzweH9yX7K2vm4HOG6dc3GT5
         OQT8BalQKblxfZCw+ZWAFZ08niqL0VdL8b83sl3GwSOHnldJtzFkeFi0gwlAdAhrVS/w
         DtTyWadAFux5+WEUslzjNtGjBvlTp+OxYBZDts3mDVRpiiY1t3jQqLyRy7Qk4UKNrOQW
         u8aZo2suwDrranwOJQnc64OT2roFJ+AJ4hVQ6tQGpH806NKrpq/oOFDSS/ZehV7vJ52p
         w5VvPCvh2MO+68NHoOz/u19KgTlfK54QYDRcZ8OyWmYU0KCPl9M3bC1GToe34e0ZZeak
         y2VA==
X-Gm-Message-State: ALoCoQm23Ce/tqrBtiOEYS9CL/KPzy7ksni0jo8MjFXdPmLLseNr8xHItQ3P7NRfnEXeLzVwdsjg
X-Received: by 10.50.27.4 with SMTP id p4mr18944956igg.8.1402172823337; Sat,
 07 Jun 2014 13:27:03 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.64.7.80 with HTTP; Sat, 7 Jun 2014 13:26:43 -0700 (PDT)
X-Originating-IP: [76.21.63.199]
In-Reply-To: <C434A3773D08A842B26FED6A1BA2E6546D128C20@SJCEML701-CHM.china.huawei.com>
References: <C434A3773D08A842B26FED6A1BA2E6546D128C20@SJCEML701-CHM.china.huawei.com>
From: Christopher Nguyen <ctn@adatao.com>
Date: Sat, 7 Jun 2014 13:26:43 -0700
Message-ID: <CAGh_TuOmB-Rc3rNM4+qqkDCtd+iUfFODBCjpj=2xQNsx9F5SqQ@mail.gmail.com>
Subject: Re: Opiq for SParkSQL?
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b1117770c933e04fb44cd48
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b1117770c933e04fb44cd48
Content-Type: text/plain; charset=UTF-8

Yan, it looks like Julian did anticipate exactly this possibility:

https://github.com/julianhyde/optiq/tree/master/spark

Optiq is a cool project vision in terms of hiding various engines behind
one consistent API.

That said, from just the Spark perspective, I don't see a huge value add to
layer Optiq above SparkSQL---until and unless Optiq provides a lot more
idioms and/or operational facilities than just making Spark RDDs look like
tables, which SparkSQL already does quite nicely and increasingly.
Warehousing, perhaps?

Here, I can't avoid a mention of DDF which aims to add more algorithmic and
data manipulation value in addition to the table abstraction (
https://spark-summit.org/2014/talk/distributed-dataframe-ddf-on-apache-spark-simplifying-big-data-for-the-rest-of-us
)
--
Christopher T. Nguyen
Co-founder & CEO, Adatao <http://adatao.com>
linkedin.com/in/ctnguyen



On Fri, Jun 6, 2014 at 11:26 AM, Yan Zhou.sc <Yan.Zhou.sc@huawei.com> wrote:

> Can anybody share your thoughts/comments/interests of applicability of the
> "optiq"  framework to Spark, and SparkSQL in particular?
>
> Thanks,
>

--047d7b1117770c933e04fb44cd48--

From dev-return-7985-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun  8 08:02:21 2014
Return-Path: <dev-return-7985-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D258811654
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  8 Jun 2014 08:02:21 +0000 (UTC)
Received: (qmail 53486 invoked by uid 500); 8 Jun 2014 08:02:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53417 invoked by uid 500); 8 Jun 2014 08:02:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53407 invoked by uid 99); 8 Jun 2014 08:02:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Jun 2014 08:02:21 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of gilv@il.ibm.com designates 195.75.94.110 as permitted sender)
Received: from [195.75.94.110] (HELO e06smtp14.uk.ibm.com) (195.75.94.110)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Jun 2014 08:02:14 +0000
Received: from /spool/local
	by e06smtp14.uk.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <gilv@il.ibm.com>;
	Sun, 8 Jun 2014 09:01:53 +0100
Received: from d06dlp01.portsmouth.uk.ibm.com (9.149.20.13)
	by e06smtp14.uk.ibm.com (192.168.101.144) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Sun, 8 Jun 2014 09:01:51 +0100
Received: from b06cxnps4076.portsmouth.uk.ibm.com (d06relay13.portsmouth.uk.ibm.com [9.149.109.198])
	by d06dlp01.portsmouth.uk.ibm.com (Postfix) with ESMTP id D175D17D8042
	for <dev@spark.apache.org>; Sun,  8 Jun 2014 09:03:10 +0100 (BST)
Received: from d06av02.portsmouth.uk.ibm.com (d06av02.portsmouth.uk.ibm.com [9.149.37.228])
	by b06cxnps4076.portsmouth.uk.ibm.com (8.13.8/8.13.8/NCO v10.0) with ESMTP id s5881opX27000900
	for <dev@spark.apache.org>; Sun, 8 Jun 2014 08:01:50 GMT
Received: from d06av02.portsmouth.uk.ibm.com (localhost [127.0.0.1])
	by d06av02.portsmouth.uk.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id s5881ops015901
	for <dev@spark.apache.org>; Sun, 8 Jun 2014 02:01:50 -0600
Received: from d06ml319.portsmouth.uk.ibm.com (d06ml319.portsmouth.uk.ibm.com [9.149.76.146])
	by d06av02.portsmouth.uk.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id s5881ouR015897
	for <dev@spark.apache.org>; Sun, 8 Jun 2014 02:01:50 -0600
To: dev@spark.apache.org
MIME-Version: 1.0
Subject: Apache Spark and Swift object store
X-KeepSent: 4A260C43:5AF736DE-C2257CF1:002B1BBC;
 type=4; name=$KeepSent
X-Mailer: Lotus Notes Release 8.5.3FP4 SHF39 May 13, 2013
From: Gil Vernik <GILV@il.ibm.com>
Message-ID: <OF4A260C43.5AF736DE-ONC2257CF1.002B1BBC-C2257CF1.002C1CC0@il.ibm.com>
Date: Sun, 8 Jun 2014 11:01:50 +0300
X-MIMETrack: Serialize by Router on D06ML319/06/M/IBM(Release 8.5.3FP5IF1HF3|November 07, 2013) at
 08/06/2014 11:01:50,
	Serialize complete at 08/06/2014 11:01:50
Content-Type: multipart/alternative; boundary="=_alternative 002C1BE4C2257CF1_="
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 14060808-1948-0000-0000-0000001C7D3D
X-Virus-Checked: Checked by ClamAV on apache.org

--=_alternative 002C1BE4C2257CF1_=
Content-Type: text/plain; charset="US-ASCII"

Hello everyone,

I would like to initiate discussion about integration Apache Spark and 
Openstack Swift. 
(https://issues.apache.org/jira/browse/SPARK-938 was created while ago)

I created a patch (https://github.com/apache/spark/pull/1010) that 
provides initial information how to connect Swift and Spark. Currently it 
uses Hadoop 2.3.0 and only stand alone mode of Spark. This patch is mainly 
used to provide community a way to experiment with this integration.
I have it fully working on my private cluster and it works very well, 
allowing me to make various analytics using Spark.

My next planned patches will include information how to configure Swift 
for other cluster deployment of Spark and also information how to 
integrate Spark and Swift with earlier versions of Hadoop. 
I am confident that the integration between Spark and Swift is very 
important future that will  benefit greatly for the exposure of Spark.

The integration between Spark and Swift is very similar to how Spark 
integrates with S3.

Will be great to hear comments / suggestions / remarks from the community!

All the best,
Gil Vernik.
--=_alternative 002C1BE4C2257CF1_=--


From dev-return-7986-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun  8 19:40:32 2014
Return-Path: <dev-return-7986-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BBE7211F3E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  8 Jun 2014 19:40:32 +0000 (UTC)
Received: (qmail 71358 invoked by uid 500); 8 Jun 2014 19:40:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71301 invoked by uid 500); 8 Jun 2014 19:40:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71290 invoked by uid 99); 8 Jun 2014 19:40:32 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Jun 2014 19:40:32 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.41 as permitted sender)
Received: from [209.85.219.41] (HELO mail-oa0-f41.google.com) (209.85.219.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Jun 2014 19:40:29 +0000
Received: by mail-oa0-f41.google.com with SMTP id m1so5099980oag.28
        for <dev@spark.apache.org>; Sun, 08 Jun 2014 12:40:04 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=d2YSr0tq97DroLFtrGb46PmI2FII36a5bgKE27BX9mU=;
        b=CmkD2lMhL565ALtRAQdcviOgrGfpV7X/onpTwHLUMQiZl0NqdXyJIbqSBgMVZPWQDr
         73c9amJ8aut+HkP8NW9HJGb0gXV53X7HXIhabkJsethEns1/Q2EoNuBOJPxR3oE6lIVY
         OVlXzsiNDg0RxeYfceZJvNZVaq+KWEeyOT2u0GsNei2Xjh5zt4p0SfDByVeNYHww5Mtn
         89UhvK7ytwoefSM1h4dHVgpTy3WUVf3bXgDKg8qS0ucfrm0tLzrRnY6LxhbfeOf9Rd0H
         QigEQ+apzS8KW54PF31QOTA3qF3+ujdeU0RzwO+f0v6T1Mgdjtez820Kt4DPnrrydzRw
         THKQ==
MIME-Version: 1.0
X-Received: by 10.182.232.135 with SMTP id to7mr4904583obc.73.1402256404068;
 Sun, 08 Jun 2014 12:40:04 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Sun, 8 Jun 2014 12:40:04 -0700 (PDT)
Date: Sun, 8 Jun 2014 12:40:04 -0700
Message-ID: <CABPQxstGPL85DAiXLgC9x7aPq42vvaGDmDngAwwMMbOtoVtXvA@mail.gmail.com>
Subject: MIMA Compatiblity Checks
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey All,

Some people may have noticed PR failures due to binary compatibility
checks. We've had these enabled in several of the sub-modules since
the 0.9.0 release but we've turned them on in Spark core post 1.0.0
which has much higher churn.

The checks are based on the "migration manager" tool from Typesafe.
One issue is that tool doesn't support package-private declarations of
classes or methods. Prashant Sharma has built instrumentation that
adds partial support for package-privacy (via a workaround) but since
there isn't really native support for this in MIMA we are still
finding cases in which we trigger false positives.

In the next week or two we'll make it a priority to handle more of
these false-positive cases. In the mean time users can add manual
excludes to:

project/MimaExcludes.scala

to avoid triggering warnings for certain issues.

This is definitely annoying - sorry about that. Unfortunately we are
the first open source Scala project to ever do this, so we are dealing
with uncharted territory.

Longer term I'd actually like to see us just write our own sbt-based
tool to do this in a better way (we've had trouble trying to extend
MIMA itself, it e.g. has copy-pasted code in it from an old version of
the scala compiler). If someone in the community is a Scala fan and
wants to take that on, I'm happy to give more details.

- Patrick

From dev-return-7987-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun  8 19:46:41 2014
Return-Path: <dev-return-7987-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EBB1D11F61
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  8 Jun 2014 19:46:40 +0000 (UTC)
Received: (qmail 81109 invoked by uid 500); 8 Jun 2014 19:46:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81050 invoked by uid 500); 8 Jun 2014 19:46:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81040 invoked by uid 99); 8 Jun 2014 19:46:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Jun 2014 19:46:40 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.214.179] (HELO mail-ob0-f179.google.com) (209.85.214.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Jun 2014 19:46:36 +0000
Received: by mail-ob0-f179.google.com with SMTP id uz6so210651obc.10
        for <dev@spark.apache.org>; Sun, 08 Jun 2014 12:46:15 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=JShPBWuErM1pgknUc2VDxzhzNZ35iblmmsgVLc/iwrM=;
        b=MTVNfSBEzLuSMWp2olrIgCRDxc9SRqRJ6l+nfkfCjEmWYLcRP3MaKOH/SNhWsBY5we
         Imt9AFBPxIM5JNyRidSVgaysLI+mGDBUGQJaTsIZD9h3qLaVQrMAqBuXAmci1/r7Y87Q
         vIsZhDKjD+Aw7WI2afiAmiCwTRyVOEDeWU1neQEvByIIypugo0UMTt9Pib1SApOIGyTZ
         IojZgYTDZX8jBXkbwkrVY+0MYm0v+8gNgg8RqbkA6wQGUN1Lmf7A3QDxB6GNJ60aOpgF
         hjGBrnbFPfUyA2uhYz9O3eMpbWWurd+j3DUFPdPoTV9RxGmFZwRYVIM/6QVMHGUb5EQK
         Ax7w==
X-Gm-Message-State: ALoCoQkpjwrb5dqHmjWgVmf3Wxfop9UrEB2ST3GpKnuxxXNUHEFjRfaVQOh8jpgABf/L5UnyN4DX
X-Received: by 10.182.236.68 with SMTP id us4mr20395404obc.39.1402256775129;
 Sun, 08 Jun 2014 12:46:15 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.76.152.163 with HTTP; Sun, 8 Jun 2014 12:45:54 -0700 (PDT)
In-Reply-To: <1402047909672-7122.post@n3.nabble.com>
References: <CAJbo4ndnxJLva8FZv6VzFbs=gXuV4ZNA8BijADXXGWJAxwtz+w@mail.gmail.com>
 <1402047909672-7122.post@n3.nabble.com>
From: Paul Brown <prb@mult.ifario.us>
Date: Sun, 8 Jun 2014 12:45:54 -0700
Message-ID: <CACArsZ-0xjDxhy5LPuLQvfnGVmrbhp03brTpxY8QZ2GiYRKjvw@mail.gmail.com>
Subject: Re: Strange problem with saveAsTextFile after upgrade Spark 0.9.0->1.0.0
To: dev@spark.apache.org
Cc: user@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c3128ef74c2004fb5858ef
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3128ef74c2004fb5858ef
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Moving over to the dev list, as this isn't a user-scope issue.

I just ran into this issue with the missing saveAsTestFile, and here's a
little additional information:

- Code ported from 0.9.1 up to 1.0.0; works with local[n] in both cases.
- Driver built as an uberjar via Maven.
- Deployed to smallish EC2 cluster in standalone mode (S3 storage) with
Spark 1.0.0-hadoop1 downloaded from Apache.

Given that it functions correctly in local mode but not in a standalone
cluster, this suggests to me that the issue is in a difference between the
Maven version and the hadoop1 version.

In the spirit of taking the computer at its word, we can just have a look
in the JAR files.  Here's what's in the Maven dep as of 1.0.0:

jar tvf
~/.m2/repository/org/apache/spark/spark-core_2.10/1.0.0/spark-core_2.10-1.0=
.0.jar
| grep 'rdd/RDD' | grep 'saveAs'
  1519 Mon May 26 13:57:58 PDT 2014
org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$1.class
  1560 Mon May 26 13:57:58 PDT 2014
org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$2.class


And here's what's in the hadoop1 distribution:

jar tvf spark-assembly-1.0.0-hadoop1.0.4.jar| grep 'rdd/RDD' | grep 'saveAs=
'


I.e., it's not there.  It is in the hadoop2 distribution:

jar tvf spark-assembly-1.0.0-hadoop2.2.0.jar| grep 'rdd/RDD' | grep 'saveAs=
'
  1519 Mon May 26 07:29:54 PDT 2014
org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$1.class
  1560 Mon May 26 07:29:54 PDT 2014
org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$2.class


So something's clearly broken with the way that the distribution assemblies
are created.

FWIW and IMHO, the "right" way to publish the hadoop1 and hadoop2 flavors
of Spark to Maven Central would be as *entirely different* artifacts
(spark-core-h1, spark-core-h2).

Logged as SPARK-2075 <https://issues.apache.org/jira/browse/SPARK-2075>.

Cheers.
-- Paul



=E2=80=94
prb@mult.ifario.us | Multifarious, Inc. | http://mult.ifario.us/


On Fri, Jun 6, 2014 at 2:45 AM, HenriV <henri.vanhove@vdab.be> wrote:

> I'm experiencing the same error while upgrading from 0.9.1 to 1.0.0.
> Im using google compute engine and cloud storage. but saveAsTextFile is
> returning errors while saving in the cloud or saving local. When i start =
a
> job in the cluster i do get an error but after this error it keeps on
> running fine untill the saveAsTextFile. ( I don't know if the two are
> connected)
>
> -----------Error at job startup-------
>  ERROR metrics.MetricsSystem: Sink class
> org.apache.spark.metrics.sink.MetricsServlet cannot be instantialized
> java.lang.reflect.InvocationTargetException
>         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
> Method)
>         at
>
> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAc=
cessorImpl.java:57)
>         at
>
> sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConst=
ructorAccessorImpl.java:45)
>         at java.lang.reflect.Constructor.newInstance(Constructor.java:526=
)
>         at
>
> org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply(Met=
ricsSystem.scala:136)
>         at
>
> org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply(Met=
ricsSystem.scala:130)
>         at
> scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:9=
8)
>         at
> scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:9=
8)
>         at
> scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226=
)
>         at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39=
)
>         at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
>         at
>
> org.apache.spark.metrics.MetricsSystem.registerSinks(MetricsSystem.scala:=
130)
>         at
> org.apache.spark.metrics.MetricsSystem.<init>(MetricsSystem.scala:84)
>         at
>
> org.apache.spark.metrics.MetricsSystem$.createMetricsSystem(MetricsSystem=
.scala:167)
>         at org.apache.spark.SparkEnv$.create(SparkEnv.scala:230)
>         at org.apache.spark.SparkContext.<init>(SparkContext.scala:202)
>         at Hello$.main(Hello.scala:101)
>         at Hello.main(Hello.scala)
>         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>         at
>
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
>         at
>
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
>         at java.lang.reflect.Method.invoke(Method.java:606)
>         at sbt.Run.invokeMain(Run.scala:72)
>         at sbt.Run.run0(Run.scala:65)
>         at sbt.Run.sbt$Run$$execute$1(Run.scala:54)
>         at sbt.Run$$anonfun$run$1.apply$mcV$sp(Run.scala:58)
>         at sbt.Run$$anonfun$run$1.apply(Run.scala:58)
>         at sbt.Run$$anonfun$run$1.apply(Run.scala:58)
>         at sbt.Logger$$anon$4.apply(Logger.scala:90)
>         at sbt.TrapExit$App.run(TrapExit.scala:244)
>         at java.lang.Thread.run(Thread.java:744)
> Caused by: java.lang.NoSuchMethodError:
> com.fasterxml.jackson.core.JsonFactory.requiresPropertyOrdering()Z
>         at
> com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:445)
>         at
> com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:366)
>         at
>
> org.apache.spark.metrics.sink.MetricsServlet.<init>(MetricsServlet.scala:=
45)
>         ... 31 more
>
> then it runs fine till i get to saveAsTextFile
>
> 14/06/06 09:05:12 INFO scheduler.TaskSetManager: Loss was due to
> java.lang.ClassNotFoundException:
> org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1 [duplicate 17]
> 14/06/06 09:05:12 INFO scheduler.DAGScheduler: Failed to run saveAsTextFi=
le
> at Hello.scala:123
> 14/06/06 09:05:12 INFO scheduler.TaskSchedulerImpl: Cancelling stage 0
> [error] (run-main-0) org.apache.spark.SparkException: Job aborted due to
> stage failure: Task 0.0:3 failed 4 times, most recent failure: Exception
> failure in TID 142 on host sparky-s1.c.quick-heaven-560.internal:
> java.lang.ClassNotFoundException:
> org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1
> [error]         java.net.URLClassLoader$1.run(URLClassLoader.java:366)
> [error]         java.net.URLClassLoader$1.run(URLClassLoader.java:355)
> [error]         java.security.AccessController.doPrivileged(Native Method=
)
> [error]         java.net.URLClassLoader.findClass(URLClassLoader.java:354=
)
> [error]         java.lang.ClassLoader.loadClass(ClassLoader.java:425)
> [error]         java.lang.ClassLoader.loadClass(ClassLoader.java:358)
> [error]         java.lang.Class.forName0(Native Method)
> [error]         java.lang.Class.forName(Class.java:270)
> [error]
>
> org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClas=
s(JavaSerializer.scala:60)
> [error]
> java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1612)
> [error]
> java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
> [error]
> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
> [error]
> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
> [error]
> java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
> [error]
> java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
> [error]
> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
> [error]
> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
> [error]
> java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
> [error]
>
> org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSeri=
alizer.scala:63)
> [error]
> org.apache.spark.scheduler.ResultTask$.deserializeInfo(ResultTask.scala:6=
1)
> [error]
> org.apache.spark.scheduler.ResultTask.readExternal(ResultTask.scala:141)
> [error]
> java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1837)
> [error]
> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
> [error]
> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
> [error]
> java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
> [error]
>
> org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSeri=
alizer.scala:63)
> [error]
>
> org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerial=
izer.scala:85)
> [error]
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:169)
> [error]
>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java=
:1145)
> [error]
>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.jav=
a:615)
> [error]         java.lang.Thread.run(Thread.java:744)
>
> Thanks for any help or guidance.
>
>
>
>
>
> --
> View this message in context:
> http://apache-spark-user-list.1001560.n3.nabble.com/Strange-problem-with-=
saveAsTextFile-after-upgrade-Spark-0-9-0-1-0-0-tp6832p7122.html
> Sent from the Apache Spark User List mailing list archive at Nabble.com.
>

--001a11c3128ef74c2004fb5858ef--

From dev-return-7988-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun  8 20:02:35 2014
Return-Path: <dev-return-7988-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B879111F97
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  8 Jun 2014 20:02:35 +0000 (UTC)
Received: (qmail 94303 invoked by uid 500); 8 Jun 2014 20:02:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94136 invoked by uid 500); 8 Jun 2014 20:02:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94117 invoked by uid 99); 8 Jun 2014 20:02:34 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Jun 2014 20:02:34 +0000
X-ASF-Spam-Status: No, hits=0.6 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.172 as permitted sender)
Received: from [209.85.214.172] (HELO mail-ob0-f172.google.com) (209.85.214.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Jun 2014 20:02:32 +0000
Received: by mail-ob0-f172.google.com with SMTP id uy5so3099012obc.17
        for <multiple recipients>; Sun, 08 Jun 2014 13:02:07 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=FhnpNG/OC1FNURNp+mL+O6geHbUhpVInA0GNQl4NFUY=;
        b=0gM/aJnNYZyEXQTFhySA0S4NPjGNdokpa2VKOQq/0AXbhCIO56eSMhmKBs4Zvas/oG
         MaKCwbin7t71uudd+cD3kRwlcp9wWfHa8WaXWwUs/4gIz8YG6BgoDOpVIbYdZFg2b3hv
         Hmh5vsCu5PfsnHdz0tuFITI3u+CStp7VSs8jj3dUgY1ak7GZkEQuyhh9NmEihaDlt4+b
         X/1SHDomes8mwiID6RskBg4zQeQGuYicqPCUUsYpB8l0mJcDJ+B+XaWABNWP8y2zWZhf
         zvWk6VYYi2KAejArQr4QfQy0riQqTVqA533dLZ9bESf+gD/IykASCMkjQ2glPYKUgTu8
         SrSA==
MIME-Version: 1.0
X-Received: by 10.182.138.99 with SMTP id qp3mr4991798obb.69.1402257727554;
 Sun, 08 Jun 2014 13:02:07 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Sun, 8 Jun 2014 13:02:07 -0700 (PDT)
In-Reply-To: <CACArsZ-0xjDxhy5LPuLQvfnGVmrbhp03brTpxY8QZ2GiYRKjvw@mail.gmail.com>
References: <CAJbo4ndnxJLva8FZv6VzFbs=gXuV4ZNA8BijADXXGWJAxwtz+w@mail.gmail.com>
	<1402047909672-7122.post@n3.nabble.com>
	<CACArsZ-0xjDxhy5LPuLQvfnGVmrbhp03brTpxY8QZ2GiYRKjvw@mail.gmail.com>
Date: Sun, 8 Jun 2014 13:02:07 -0700
Message-ID: <CABPQxsvTWCMdGiSkSS09xHR6bvoo4Cz007x7KQ2YuM9zO83vzg@mail.gmail.com>
Subject: Re: Strange problem with saveAsTextFile after upgrade Spark 0.9.0->1.0.0
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: user@spark.apache.org
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Paul,

Could you give the version of Java that you are building with and the
version of Java you are running with? Are they the same?

Just off the cuff, I wonder if this is related to:
https://issues.apache.org/jira/browse/SPARK-1520

If it is, it could appear that certain functions are not in the jar
because they go beyond the extended zip boundary `jar tvf` won't list
them.

- Patrick

On Sun, Jun 8, 2014 at 12:45 PM, Paul Brown <prb@mult.ifario.us> wrote:
> Moving over to the dev list, as this isn't a user-scope issue.
>
> I just ran into this issue with the missing saveAsTestFile, and here's a
> little additional information:
>
> - Code ported from 0.9.1 up to 1.0.0; works with local[n] in both cases.
> - Driver built as an uberjar via Maven.
> - Deployed to smallish EC2 cluster in standalone mode (S3 storage) with
> Spark 1.0.0-hadoop1 downloaded from Apache.
>
> Given that it functions correctly in local mode but not in a standalone
> cluster, this suggests to me that the issue is in a difference between the
> Maven version and the hadoop1 version.
>
> In the spirit of taking the computer at its word, we can just have a look
> in the JAR files.  Here's what's in the Maven dep as of 1.0.0:
>
> jar tvf
> ~/.m2/repository/org/apache/spark/spark-core_2.10/1.0.0/spark-core_2.10-1.0.0.jar
> | grep 'rdd/RDD' | grep 'saveAs'
>   1519 Mon May 26 13:57:58 PDT 2014
> org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$1.class
>   1560 Mon May 26 13:57:58 PDT 2014
> org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$2.class
>
>
> And here's what's in the hadoop1 distribution:
>
> jar tvf spark-assembly-1.0.0-hadoop1.0.4.jar| grep 'rdd/RDD' | grep 'saveAs'
>
>
> I.e., it's not there.  It is in the hadoop2 distribution:
>
> jar tvf spark-assembly-1.0.0-hadoop2.2.0.jar| grep 'rdd/RDD' | grep 'saveAs'
>   1519 Mon May 26 07:29:54 PDT 2014
> org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$1.class
>   1560 Mon May 26 07:29:54 PDT 2014
> org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$2.class
>
>
> So something's clearly broken with the way that the distribution assemblies
> are created.
>
> FWIW and IMHO, the "right" way to publish the hadoop1 and hadoop2 flavors
> of Spark to Maven Central would be as *entirely different* artifacts
> (spark-core-h1, spark-core-h2).
>
> Logged as SPARK-2075 <https://issues.apache.org/jira/browse/SPARK-2075>.
>
> Cheers.
> -- Paul
>
>
>
> --
> prb@mult.ifario.us | Multifarious, Inc. | http://mult.ifario.us/
>
>
> On Fri, Jun 6, 2014 at 2:45 AM, HenriV <henri.vanhove@vdab.be> wrote:
>
>> I'm experiencing the same error while upgrading from 0.9.1 to 1.0.0.
>> Im using google compute engine and cloud storage. but saveAsTextFile is
>> returning errors while saving in the cloud or saving local. When i start a
>> job in the cluster i do get an error but after this error it keeps on
>> running fine untill the saveAsTextFile. ( I don't know if the two are
>> connected)
>>
>> -----------Error at job startup-------
>>  ERROR metrics.MetricsSystem: Sink class
>> org.apache.spark.metrics.sink.MetricsServlet cannot be instantialized
>> java.lang.reflect.InvocationTargetException
>>         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
>> Method)
>>         at
>>
>> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
>>         at
>>
>> sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
>>         at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
>>         at
>>
>> org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply(MetricsSystem.scala:136)
>>         at
>>
>> org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply(MetricsSystem.scala:130)
>>         at
>> scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
>>         at
>> scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
>>         at
>> scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
>>         at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
>>         at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
>>         at
>>
>> org.apache.spark.metrics.MetricsSystem.registerSinks(MetricsSystem.scala:130)
>>         at
>> org.apache.spark.metrics.MetricsSystem.<init>(MetricsSystem.scala:84)
>>         at
>>
>> org.apache.spark.metrics.MetricsSystem$.createMetricsSystem(MetricsSystem.scala:167)
>>         at org.apache.spark.SparkEnv$.create(SparkEnv.scala:230)
>>         at org.apache.spark.SparkContext.<init>(SparkContext.scala:202)
>>         at Hello$.main(Hello.scala:101)
>>         at Hello.main(Hello.scala)
>>         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>>         at
>>
>> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
>>         at
>>
>> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
>>         at java.lang.reflect.Method.invoke(Method.java:606)
>>         at sbt.Run.invokeMain(Run.scala:72)
>>         at sbt.Run.run0(Run.scala:65)
>>         at sbt.Run.sbt$Run$$execute$1(Run.scala:54)
>>         at sbt.Run$$anonfun$run$1.apply$mcV$sp(Run.scala:58)
>>         at sbt.Run$$anonfun$run$1.apply(Run.scala:58)
>>         at sbt.Run$$anonfun$run$1.apply(Run.scala:58)
>>         at sbt.Logger$$anon$4.apply(Logger.scala:90)
>>         at sbt.TrapExit$App.run(TrapExit.scala:244)
>>         at java.lang.Thread.run(Thread.java:744)
>> Caused by: java.lang.NoSuchMethodError:
>> com.fasterxml.jackson.core.JsonFactory.requiresPropertyOrdering()Z
>>         at
>> com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:445)
>>         at
>> com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:366)
>>         at
>>
>> org.apache.spark.metrics.sink.MetricsServlet.<init>(MetricsServlet.scala:45)
>>         ... 31 more
>>
>> then it runs fine till i get to saveAsTextFile
>>
>> 14/06/06 09:05:12 INFO scheduler.TaskSetManager: Loss was due to
>> java.lang.ClassNotFoundException:
>> org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1 [duplicate 17]
>> 14/06/06 09:05:12 INFO scheduler.DAGScheduler: Failed to run saveAsTextFile
>> at Hello.scala:123
>> 14/06/06 09:05:12 INFO scheduler.TaskSchedulerImpl: Cancelling stage 0
>> [error] (run-main-0) org.apache.spark.SparkException: Job aborted due to
>> stage failure: Task 0.0:3 failed 4 times, most recent failure: Exception
>> failure in TID 142 on host sparky-s1.c.quick-heaven-560.internal:
>> java.lang.ClassNotFoundException:
>> org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1
>> [error]         java.net.URLClassLoader$1.run(URLClassLoader.java:366)
>> [error]         java.net.URLClassLoader$1.run(URLClassLoader.java:355)
>> [error]         java.security.AccessController.doPrivileged(Native Method)
>> [error]         java.net.URLClassLoader.findClass(URLClassLoader.java:354)
>> [error]         java.lang.ClassLoader.loadClass(ClassLoader.java:425)
>> [error]         java.lang.ClassLoader.loadClass(ClassLoader.java:358)
>> [error]         java.lang.Class.forName0(Native Method)
>> [error]         java.lang.Class.forName(Class.java:270)
>> [error]
>>
>> org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:60)
>> [error]
>> java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1612)
>> [error]
>> java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
>> [error]
>> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
>> [error]
>> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
>> [error]
>> java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
>> [error]
>> java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
>> [error]
>> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
>> [error]
>> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
>> [error]
>> java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
>> [error]
>>
>> org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)
>> [error]
>> org.apache.spark.scheduler.ResultTask$.deserializeInfo(ResultTask.scala:61)
>> [error]
>> org.apache.spark.scheduler.ResultTask.readExternal(ResultTask.scala:141)
>> [error]
>> java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1837)
>> [error]
>> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
>> [error]
>> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
>> [error]
>> java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
>> [error]
>>
>> org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)
>> [error]
>>
>> org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:85)
>> [error]
>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:169)
>> [error]
>>
>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>> [error]
>>
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>> [error]         java.lang.Thread.run(Thread.java:744)
>>
>> Thanks for any help or guidance.
>>
>>
>>
>>
>>
>> --
>> View this message in context:
>> http://apache-spark-user-list.1001560.n3.nabble.com/Strange-problem-with-saveAsTextFile-after-upgrade-Spark-0-9-0-1-0-0-tp6832p7122.html
>> Sent from the Apache Spark User List mailing list archive at Nabble.com.
>>

From dev-return-7989-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun  8 20:03:08 2014
Return-Path: <dev-return-7989-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BB44211F9C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  8 Jun 2014 20:03:08 +0000 (UTC)
Received: (qmail 96652 invoked by uid 500); 8 Jun 2014 20:03:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96476 invoked by uid 500); 8 Jun 2014 20:03:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96457 invoked by uid 99); 8 Jun 2014 20:03:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Jun 2014 20:03:07 +0000
X-ASF-Spam-Status: No, hits=0.6 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.176 as permitted sender)
Received: from [209.85.214.176] (HELO mail-ob0-f176.google.com) (209.85.214.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Jun 2014 20:03:04 +0000
Received: by mail-ob0-f176.google.com with SMTP id wo20so5019807obc.35
        for <multiple recipients>; Sun, 08 Jun 2014 13:02:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=css9k5u4enpxor2sCxziIzNju+vRNZmeoluA7Qu/Zus=;
        b=WhN+Lk/l9KMLhz8Zq3nSbOpSkvqlemFhL885ZJpSS5dO0kbEXx4Y4wvrG4M4f2nhLI
         CbSh1BpNVg8tvw0diy5MuwfZUIbeweNPuRcvy78UmjL+agiSfjv426jNK4kt3uaWejRW
         a8otNkXpQlwcCmVDOjlfkDzFMcBz/fP+VYbpxl1o7l2dILDhUXInj5f9Gd0BoOsE+NMp
         0gkeZAMGap8VO4Yc5UW7ZHO6gDyExYCQvJtTKKACwL0Zxj/p5xORaoIrTZlUybIGvmBi
         9+IbfwfPToMcB7Ax7ZsLLy+ryqpT68cSZipRwWB/IL2Cb+tkz8FeuodHoQGTJoiZd98B
         EkoQ==
MIME-Version: 1.0
X-Received: by 10.60.175.163 with SMTP id cb3mr21840687oec.29.1402257760477;
 Sun, 08 Jun 2014 13:02:40 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Sun, 8 Jun 2014 13:02:40 -0700 (PDT)
In-Reply-To: <CABPQxsvTWCMdGiSkSS09xHR6bvoo4Cz007x7KQ2YuM9zO83vzg@mail.gmail.com>
References: <CAJbo4ndnxJLva8FZv6VzFbs=gXuV4ZNA8BijADXXGWJAxwtz+w@mail.gmail.com>
	<1402047909672-7122.post@n3.nabble.com>
	<CACArsZ-0xjDxhy5LPuLQvfnGVmrbhp03brTpxY8QZ2GiYRKjvw@mail.gmail.com>
	<CABPQxsvTWCMdGiSkSS09xHR6bvoo4Cz007x7KQ2YuM9zO83vzg@mail.gmail.com>
Date: Sun, 8 Jun 2014 13:02:40 -0700
Message-ID: <CABPQxstGZiZvrmt+K0yYsTf2OBC9pUz8ahkvUdsEpNXUW0GQZg@mail.gmail.com>
Subject: Re: Strange problem with saveAsTextFile after upgrade Spark 0.9.0->1.0.0
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: user@spark.apache.org
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Also I should add - thanks for taking time to help narrow this down!

On Sun, Jun 8, 2014 at 1:02 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Paul,
>
> Could you give the version of Java that you are building with and the
> version of Java you are running with? Are they the same?
>
> Just off the cuff, I wonder if this is related to:
> https://issues.apache.org/jira/browse/SPARK-1520
>
> If it is, it could appear that certain functions are not in the jar
> because they go beyond the extended zip boundary `jar tvf` won't list
> them.
>
> - Patrick
>
> On Sun, Jun 8, 2014 at 12:45 PM, Paul Brown <prb@mult.ifario.us> wrote:
>> Moving over to the dev list, as this isn't a user-scope issue.
>>
>> I just ran into this issue with the missing saveAsTestFile, and here's a
>> little additional information:
>>
>> - Code ported from 0.9.1 up to 1.0.0; works with local[n] in both cases.
>> - Driver built as an uberjar via Maven.
>> - Deployed to smallish EC2 cluster in standalone mode (S3 storage) with
>> Spark 1.0.0-hadoop1 downloaded from Apache.
>>
>> Given that it functions correctly in local mode but not in a standalone
>> cluster, this suggests to me that the issue is in a difference between the
>> Maven version and the hadoop1 version.
>>
>> In the spirit of taking the computer at its word, we can just have a look
>> in the JAR files.  Here's what's in the Maven dep as of 1.0.0:
>>
>> jar tvf
>> ~/.m2/repository/org/apache/spark/spark-core_2.10/1.0.0/spark-core_2.10-1.0.0.jar
>> | grep 'rdd/RDD' | grep 'saveAs'
>>   1519 Mon May 26 13:57:58 PDT 2014
>> org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$1.class
>>   1560 Mon May 26 13:57:58 PDT 2014
>> org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$2.class
>>
>>
>> And here's what's in the hadoop1 distribution:
>>
>> jar tvf spark-assembly-1.0.0-hadoop1.0.4.jar| grep 'rdd/RDD' | grep 'saveAs'
>>
>>
>> I.e., it's not there.  It is in the hadoop2 distribution:
>>
>> jar tvf spark-assembly-1.0.0-hadoop2.2.0.jar| grep 'rdd/RDD' | grep 'saveAs'
>>   1519 Mon May 26 07:29:54 PDT 2014
>> org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$1.class
>>   1560 Mon May 26 07:29:54 PDT 2014
>> org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$2.class
>>
>>
>> So something's clearly broken with the way that the distribution assemblies
>> are created.
>>
>> FWIW and IMHO, the "right" way to publish the hadoop1 and hadoop2 flavors
>> of Spark to Maven Central would be as *entirely different* artifacts
>> (spark-core-h1, spark-core-h2).
>>
>> Logged as SPARK-2075 <https://issues.apache.org/jira/browse/SPARK-2075>.
>>
>> Cheers.
>> -- Paul
>>
>>
>>
>> --
>> prb@mult.ifario.us | Multifarious, Inc. | http://mult.ifario.us/
>>
>>
>> On Fri, Jun 6, 2014 at 2:45 AM, HenriV <henri.vanhove@vdab.be> wrote:
>>
>>> I'm experiencing the same error while upgrading from 0.9.1 to 1.0.0.
>>> Im using google compute engine and cloud storage. but saveAsTextFile is
>>> returning errors while saving in the cloud or saving local. When i start a
>>> job in the cluster i do get an error but after this error it keeps on
>>> running fine untill the saveAsTextFile. ( I don't know if the two are
>>> connected)
>>>
>>> -----------Error at job startup-------
>>>  ERROR metrics.MetricsSystem: Sink class
>>> org.apache.spark.metrics.sink.MetricsServlet cannot be instantialized
>>> java.lang.reflect.InvocationTargetException
>>>         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
>>> Method)
>>>         at
>>>
>>> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
>>>         at
>>>
>>> sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
>>>         at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
>>>         at
>>>
>>> org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply(MetricsSystem.scala:136)
>>>         at
>>>
>>> org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply(MetricsSystem.scala:130)
>>>         at
>>> scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
>>>         at
>>> scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
>>>         at
>>> scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)
>>>         at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
>>>         at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
>>>         at
>>>
>>> org.apache.spark.metrics.MetricsSystem.registerSinks(MetricsSystem.scala:130)
>>>         at
>>> org.apache.spark.metrics.MetricsSystem.<init>(MetricsSystem.scala:84)
>>>         at
>>>
>>> org.apache.spark.metrics.MetricsSystem$.createMetricsSystem(MetricsSystem.scala:167)
>>>         at org.apache.spark.SparkEnv$.create(SparkEnv.scala:230)
>>>         at org.apache.spark.SparkContext.<init>(SparkContext.scala:202)
>>>         at Hello$.main(Hello.scala:101)
>>>         at Hello.main(Hello.scala)
>>>         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>>>         at
>>>
>>> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
>>>         at
>>>
>>> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
>>>         at java.lang.reflect.Method.invoke(Method.java:606)
>>>         at sbt.Run.invokeMain(Run.scala:72)
>>>         at sbt.Run.run0(Run.scala:65)
>>>         at sbt.Run.sbt$Run$$execute$1(Run.scala:54)
>>>         at sbt.Run$$anonfun$run$1.apply$mcV$sp(Run.scala:58)
>>>         at sbt.Run$$anonfun$run$1.apply(Run.scala:58)
>>>         at sbt.Run$$anonfun$run$1.apply(Run.scala:58)
>>>         at sbt.Logger$$anon$4.apply(Logger.scala:90)
>>>         at sbt.TrapExit$App.run(TrapExit.scala:244)
>>>         at java.lang.Thread.run(Thread.java:744)
>>> Caused by: java.lang.NoSuchMethodError:
>>> com.fasterxml.jackson.core.JsonFactory.requiresPropertyOrdering()Z
>>>         at
>>> com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:445)
>>>         at
>>> com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:366)
>>>         at
>>>
>>> org.apache.spark.metrics.sink.MetricsServlet.<init>(MetricsServlet.scala:45)
>>>         ... 31 more
>>>
>>> then it runs fine till i get to saveAsTextFile
>>>
>>> 14/06/06 09:05:12 INFO scheduler.TaskSetManager: Loss was due to
>>> java.lang.ClassNotFoundException:
>>> org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1 [duplicate 17]
>>> 14/06/06 09:05:12 INFO scheduler.DAGScheduler: Failed to run saveAsTextFile
>>> at Hello.scala:123
>>> 14/06/06 09:05:12 INFO scheduler.TaskSchedulerImpl: Cancelling stage 0
>>> [error] (run-main-0) org.apache.spark.SparkException: Job aborted due to
>>> stage failure: Task 0.0:3 failed 4 times, most recent failure: Exception
>>> failure in TID 142 on host sparky-s1.c.quick-heaven-560.internal:
>>> java.lang.ClassNotFoundException:
>>> org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1
>>> [error]         java.net.URLClassLoader$1.run(URLClassLoader.java:366)
>>> [error]         java.net.URLClassLoader$1.run(URLClassLoader.java:355)
>>> [error]         java.security.AccessController.doPrivileged(Native Method)
>>> [error]         java.net.URLClassLoader.findClass(URLClassLoader.java:354)
>>> [error]         java.lang.ClassLoader.loadClass(ClassLoader.java:425)
>>> [error]         java.lang.ClassLoader.loadClass(ClassLoader.java:358)
>>> [error]         java.lang.Class.forName0(Native Method)
>>> [error]         java.lang.Class.forName(Class.java:270)
>>> [error]
>>>
>>> org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:60)
>>> [error]
>>> java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1612)
>>> [error]
>>> java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
>>> [error]
>>> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
>>> [error]
>>> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
>>> [error]
>>> java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
>>> [error]
>>> java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
>>> [error]
>>> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
>>> [error]
>>> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
>>> [error]
>>> java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
>>> [error]
>>>
>>> org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)
>>> [error]
>>> org.apache.spark.scheduler.ResultTask$.deserializeInfo(ResultTask.scala:61)
>>> [error]
>>> org.apache.spark.scheduler.ResultTask.readExternal(ResultTask.scala:141)
>>> [error]
>>> java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1837)
>>> [error]
>>> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
>>> [error]
>>> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
>>> [error]
>>> java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
>>> [error]
>>>
>>> org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)
>>> [error]
>>>
>>> org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:85)
>>> [error]
>>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:169)
>>> [error]
>>>
>>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>>> [error]
>>>
>>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>>> [error]         java.lang.Thread.run(Thread.java:744)
>>>
>>> Thanks for any help or guidance.
>>>
>>>
>>>
>>>
>>>
>>> --
>>> View this message in context:
>>> http://apache-spark-user-list.1001560.n3.nabble.com/Strange-problem-with-saveAsTextFile-after-upgrade-Spark-0-9-0-1-0-0-tp6832p7122.html
>>> Sent from the Apache Spark User List mailing list archive at Nabble.com.
>>>

From dev-return-7990-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun  8 20:13:10 2014
Return-Path: <dev-return-7990-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1574111FBF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  8 Jun 2014 20:13:10 +0000 (UTC)
Received: (qmail 4786 invoked by uid 500); 8 Jun 2014 20:13:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4687 invoked by uid 500); 8 Jun 2014 20:13:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4669 invoked by uid 99); 8 Jun 2014 20:13:08 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Jun 2014 20:13:08 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.128.178 as permitted sender)
Received: from [209.85.128.178] (HELO mail-ve0-f178.google.com) (209.85.128.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Jun 2014 20:13:05 +0000
Received: by mail-ve0-f178.google.com with SMTP id sa20so5734692veb.9
        for <dev@spark.apache.org>; Sun, 08 Jun 2014 13:12:41 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=+hRTPgcxFC2yhPijQfkE7sNSkZqZyqM64vpNIgEkVqg=;
        b=FaVEznufQX/LbAL5iFtiR2Fa2aJfhmDyLWN6waV6RhMiFVfQuY4jdhojm4KCF7Lbwc
         zzP/JxMD3PDT6NU+L4WCi8sjqfb99xqkEmigHgx/HqwjkiiM/mxcc6YnxFdyhwwSauSf
         s/UAX7QaCnBZ90y/TLkb6HFR8FXb7eUiWg5p/cxxSENxZD9PJWUVo5f4zdnqrLAHEqNK
         aQ08+rO4OIkmw4E/eJUuhE9321fBZX7HTltrD/FJnMiPEIhC/mmvGD7XHhIQXOBhAZwk
         DS396wAe+IUZ8uAMqEJoDOG7VsjwSa7+qMVKyUmxQPz+HwTlH/G5LhIFUyRIRtVTKsPa
         u9Eg==
X-Gm-Message-State: ALoCoQmvNkIQ2/MUwguRYxsDDFvaoNxodedx+lGm2Bw9jRyRK3VqEMxw3Uax8z0OmWDSpOHSibv7
X-Received: by 10.58.187.19 with SMTP id fo19mr5301294vec.45.1402258361650;
 Sun, 08 Jun 2014 13:12:41 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.121.6 with HTTP; Sun, 8 Jun 2014 13:12:20 -0700 (PDT)
In-Reply-To: <CABPQxsvTWCMdGiSkSS09xHR6bvoo4Cz007x7KQ2YuM9zO83vzg@mail.gmail.com>
References: <CAJbo4ndnxJLva8FZv6VzFbs=gXuV4ZNA8BijADXXGWJAxwtz+w@mail.gmail.com>
 <1402047909672-7122.post@n3.nabble.com> <CACArsZ-0xjDxhy5LPuLQvfnGVmrbhp03brTpxY8QZ2GiYRKjvw@mail.gmail.com>
 <CABPQxsvTWCMdGiSkSS09xHR6bvoo4Cz007x7KQ2YuM9zO83vzg@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Sun, 8 Jun 2014 16:12:20 -0400
Message-ID: <CAMAsSd+azmQwZ1A0=CLTTvRPMWp9Q4KFpQ8M2jP_g+gxYt+44A@mail.gmail.com>
Subject: Re: Strange problem with saveAsTextFile after upgrade Spark 0.9.0->1.0.0
To: user@spark.apache.org
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I suspect Patrick is right about the cause. The Maven artifact that
was released does contain this class (phew)

http://search.maven.org/#artifactdetails%7Corg.apache.spark%7Cspark-core_2.10%7C1.0.0%7Cjar

As to the hadoop1 / hadoop2 artifact question -- agree that is often
done. Here the working theory seems to be to depend on the one
artifact (whose API should be identical regardless of dependencies)
and then customize the hadoop-client dep. Here, there are not two
versions deployed to Maven at all.


On Sun, Jun 8, 2014 at 4:02 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Paul,
>
> Could you give the version of Java that you are building with and the
> version of Java you are running with? Are they the same?
>
> Just off the cuff, I wonder if this is related to:
> https://issues.apache.org/jira/browse/SPARK-1520
>
> If it is, it could appear that certain functions are not in the jar
> because they go beyond the extended zip boundary `jar tvf` won't list
> them.

From dev-return-7991-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun  8 22:12:11 2014
Return-Path: <dev-return-7991-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A042F114D3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  8 Jun 2014 22:12:11 +0000 (UTC)
Received: (qmail 59979 invoked by uid 500); 8 Jun 2014 22:12:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59796 invoked by uid 500); 8 Jun 2014 22:12:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59777 invoked by uid 99); 8 Jun 2014 22:12:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Jun 2014 22:12:09 +0000
X-ASF-Spam-Status: No, hits=0.6 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.170 as permitted sender)
Received: from [209.85.214.170] (HELO mail-ob0-f170.google.com) (209.85.214.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Jun 2014 22:12:07 +0000
Received: by mail-ob0-f170.google.com with SMTP id uz6so283692obc.29
        for <multiple recipients>; Sun, 08 Jun 2014 15:11:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type:content-transfer-encoding;
        bh=HDRXzhUXoI/issmBkeF6xyw8hQKZkOj23URPQd912e4=;
        b=y9c3Kc++86vbJ9ySRD1IGyfoRH6WJY1Vo6opM0SKcOdFxX7zA4Dz0VeZa2vgFeVn6C
         aFuuhqTfv8v8bw+qxEUh5H9fET4s6jqpiF13Gj2OsdA4rrxJUsrdFftPcFUiBiaH0+td
         yMADNXpQlTTR7CQv14vC9iFI/lyVEIhsP22vHg8mAl5tpt+IO3bQwXy23KgEaJ1+Enmc
         xohHFtz7eGvvS/8R7TC+6RzG1pReW0scBN5hBhiuVE8zKGs18qARBcMU4St1YUHu8yQE
         AKAhXYBL7qNlNbEGaCL/Y2kSbANfPI2OyQyC5FBKfIjylcsMFxbd/HeI772cxc1w8+Kb
         SpcA==
MIME-Version: 1.0
X-Received: by 10.60.131.210 with SMTP id oo18mr5515863oeb.70.1402265502736;
 Sun, 08 Jun 2014 15:11:42 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Sun, 8 Jun 2014 15:11:42 -0700 (PDT)
In-Reply-To: <CACArsZ_oNLfGix4eziktXt_Zz1dDaMSx9K35B-OkdWCVE=+K5A@mail.gmail.com>
References: <CAJbo4ndnxJLva8FZv6VzFbs=gXuV4ZNA8BijADXXGWJAxwtz+w@mail.gmail.com>
	<1402047909672-7122.post@n3.nabble.com>
	<CACArsZ-0xjDxhy5LPuLQvfnGVmrbhp03brTpxY8QZ2GiYRKjvw@mail.gmail.com>
	<CABPQxsvTWCMdGiSkSS09xHR6bvoo4Cz007x7KQ2YuM9zO83vzg@mail.gmail.com>
	<CACArsZ_oNLfGix4eziktXt_Zz1dDaMSx9K35B-OkdWCVE=+K5A@mail.gmail.com>
Date: Sun, 8 Jun 2014 15:11:42 -0700
Message-ID: <CABPQxsusnAyS-fnrj7MnmQtmu7wpvGZ9YBSm9dFa2DKYpu6iCg@mail.gmail.com>
Subject: Re: Strange problem with saveAsTextFile after upgrade Spark 0.9.0->1.0.0
From: Patrick Wendell <pwendell@gmail.com>
To: user@spark.apache.org, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Okay I think I've isolated this a bit more. Let's discuss over on the JIRA:

https://issues.apache.org/jira/browse/SPARK-2075

On Sun, Jun 8, 2014 at 1:16 PM, Paul Brown <prb@mult.ifario.us> wrote:
>
> Hi, Patrick --
>
> Java 7 on the development machines:
>
> =C2=BB java -version
> 1 =E2=86=B5
> java version "1.7.0_51"
> Java(TM) SE Runtime Environment (build 1.7.0_51-b13)
> Java HotSpot(TM) 64-Bit Server VM (build 24.51-b03, mixed mode)
>
>
> And on the deployed boxes:
>
> $ java -version
> java version "1.7.0_55"
> OpenJDK Runtime Environment (IcedTea 2.4.7) (7u55-2.4.7-1ubuntu1)
> OpenJDK 64-Bit Server VM (build 24.51-b03, mixed mode)
>
>
> Also, "unzip -l" in place of "jar tvf" gives the same results, so I don't
> think it's an issue with jar not reporting the files.  Also, the classes =
do
> get correctly packaged into the uberjar:
>
> unzip -l /target/[deleted]-driver.jar | grep 'rdd/RDD' | grep 'saveAs'
>      1519  06-08-14 12:05
> org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$1.class
>      1560  06-08-14 12:05
> org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$2.class
>
>
> Best.
> -- Paul
>
> =E2=80=94
> prb@mult.ifario.us | Multifarious, Inc. | http://mult.ifario.us/
>
>
> On Sun, Jun 8, 2014 at 1:02 PM, Patrick Wendell <pwendell@gmail.com> wrot=
e:
>>
>> Paul,
>>
>> Could you give the version of Java that you are building with and the
>> version of Java you are running with? Are they the same?
>>
>> Just off the cuff, I wonder if this is related to:
>> https://issues.apache.org/jira/browse/SPARK-1520
>>
>> If it is, it could appear that certain functions are not in the jar
>> because they go beyond the extended zip boundary `jar tvf` won't list
>> them.
>>
>> - Patrick
>>
>> On Sun, Jun 8, 2014 at 12:45 PM, Paul Brown <prb@mult.ifario.us> wrote:
>> > Moving over to the dev list, as this isn't a user-scope issue.
>> >
>> > I just ran into this issue with the missing saveAsTestFile, and here's=
 a
>> > little additional information:
>> >
>> > - Code ported from 0.9.1 up to 1.0.0; works with local[n] in both case=
s.
>> > - Driver built as an uberjar via Maven.
>> > - Deployed to smallish EC2 cluster in standalone mode (S3 storage) wit=
h
>> > Spark 1.0.0-hadoop1 downloaded from Apache.
>> >
>> > Given that it functions correctly in local mode but not in a standalon=
e
>> > cluster, this suggests to me that the issue is in a difference between
>> > the
>> > Maven version and the hadoop1 version.
>> >
>> > In the spirit of taking the computer at its word, we can just have a
>> > look
>> > in the JAR files.  Here's what's in the Maven dep as of 1.0.0:
>> >
>> > jar tvf
>> >
>> > ~/.m2/repository/org/apache/spark/spark-core_2.10/1.0.0/spark-core_2.1=
0-1.0.0.jar
>> > | grep 'rdd/RDD' | grep 'saveAs'
>> >   1519 Mon May 26 13:57:58 PDT 2014
>> > org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$1.class
>> >   1560 Mon May 26 13:57:58 PDT 2014
>> > org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$2.class
>> >
>> >
>> > And here's what's in the hadoop1 distribution:
>> >
>> > jar tvf spark-assembly-1.0.0-hadoop1.0.4.jar| grep 'rdd/RDD' | grep
>> > 'saveAs'
>> >
>> >
>> > I.e., it's not there.  It is in the hadoop2 distribution:
>> >
>> > jar tvf spark-assembly-1.0.0-hadoop2.2.0.jar| grep 'rdd/RDD' | grep
>> > 'saveAs'
>> >   1519 Mon May 26 07:29:54 PDT 2014
>> > org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$1.class
>> >   1560 Mon May 26 07:29:54 PDT 2014
>> > org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$2.class
>> >
>> >
>> > So something's clearly broken with the way that the distribution
>> > assemblies
>> > are created.
>> >
>> > FWIW and IMHO, the "right" way to publish the hadoop1 and hadoop2
>> > flavors
>> > of Spark to Maven Central would be as *entirely different* artifacts
>> > (spark-core-h1, spark-core-h2).
>> >
>> > Logged as SPARK-2075 <https://issues.apache.org/jira/browse/SPARK-2075=
>.
>> >
>> > Cheers.
>> > -- Paul
>> >
>> >
>> >
>> > --
>> > prb@mult.ifario.us | Multifarious, Inc. | http://mult.ifario.us/
>> >
>> >
>> > On Fri, Jun 6, 2014 at 2:45 AM, HenriV <henri.vanhove@vdab.be> wrote:
>> >
>> >> I'm experiencing the same error while upgrading from 0.9.1 to 1.0.0.
>> >> Im using google compute engine and cloud storage. but saveAsTextFile =
is
>> >> returning errors while saving in the cloud or saving local. When i
>> >> start a
>> >> job in the cluster i do get an error but after this error it keeps on
>> >> running fine untill the saveAsTextFile. ( I don't know if the two are
>> >> connected)
>> >>
>> >> -----------Error at job startup-------
>> >>  ERROR metrics.MetricsSystem: Sink class
>> >> org.apache.spark.metrics.sink.MetricsServlet cannot be instantialized
>> >> java.lang.reflect.InvocationTargetException
>> >>         at
>> >> sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
>> >> Method)
>> >>         at
>> >>
>> >>
>> >> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstruct=
orAccessorImpl.java:57)
>> >>         at
>> >>
>> >>
>> >> sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingC=
onstructorAccessorImpl.java:45)
>> >>         at
>> >> java.lang.reflect.Constructor.newInstance(Constructor.java:526)
>> >>         at
>> >>
>> >>
>> >> org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply=
(MetricsSystem.scala:136)
>> >>         at
>> >>
>> >>
>> >> org.apache.spark.metrics.MetricsSystem$$anonfun$registerSinks$1.apply=
(MetricsSystem.scala:130)
>> >>         at
>> >>
>> >> scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.sca=
la:98)
>> >>         at
>> >>
>> >> scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.sca=
la:98)
>> >>         at
>> >>
>> >> scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala=
:226)
>> >>         at
>> >> scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
>> >>         at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
>> >>         at
>> >>
>> >>
>> >> org.apache.spark.metrics.MetricsSystem.registerSinks(MetricsSystem.sc=
ala:130)
>> >>         at
>> >> org.apache.spark.metrics.MetricsSystem.<init>(MetricsSystem.scala:84)
>> >>         at
>> >>
>> >>
>> >> org.apache.spark.metrics.MetricsSystem$.createMetricsSystem(MetricsSy=
stem.scala:167)
>> >>         at org.apache.spark.SparkEnv$.create(SparkEnv.scala:230)
>> >>         at org.apache.spark.SparkContext.<init>(SparkContext.scala:20=
2)
>> >>         at Hello$.main(Hello.scala:101)
>> >>         at Hello.main(Hello.scala)
>> >>         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method=
)
>> >>         at
>> >>
>> >>
>> >> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.=
java:57)
>> >>         at
>> >>
>> >>
>> >> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcces=
sorImpl.java:43)
>> >>         at java.lang.reflect.Method.invoke(Method.java:606)
>> >>         at sbt.Run.invokeMain(Run.scala:72)
>> >>         at sbt.Run.run0(Run.scala:65)
>> >>         at sbt.Run.sbt$Run$$execute$1(Run.scala:54)
>> >>         at sbt.Run$$anonfun$run$1.apply$mcV$sp(Run.scala:58)
>> >>         at sbt.Run$$anonfun$run$1.apply(Run.scala:58)
>> >>         at sbt.Run$$anonfun$run$1.apply(Run.scala:58)
>> >>         at sbt.Logger$$anon$4.apply(Logger.scala:90)
>> >>         at sbt.TrapExit$App.run(TrapExit.scala:244)
>> >>         at java.lang.Thread.run(Thread.java:744)
>> >> Caused by: java.lang.NoSuchMethodError:
>> >> com.fasterxml.jackson.core.JsonFactory.requiresPropertyOrdering()Z
>> >>         at
>> >>
>> >> com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:=
445)
>> >>         at
>> >>
>> >> com.fasterxml.jackson.databind.ObjectMapper.<init>(ObjectMapper.java:=
366)
>> >>         at
>> >>
>> >>
>> >> org.apache.spark.metrics.sink.MetricsServlet.<init>(MetricsServlet.sc=
ala:45)
>> >>         ... 31 more
>> >>
>> >> then it runs fine till i get to saveAsTextFile
>> >>
>> >> 14/06/06 09:05:12 INFO scheduler.TaskSetManager: Loss was due to
>> >> java.lang.ClassNotFoundException:
>> >> org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1 [duplicate 17]
>> >> 14/06/06 09:05:12 INFO scheduler.DAGScheduler: Failed to run
>> >> saveAsTextFile
>> >> at Hello.scala:123
>> >> 14/06/06 09:05:12 INFO scheduler.TaskSchedulerImpl: Cancelling stage =
0
>> >> [error] (run-main-0) org.apache.spark.SparkException: Job aborted due
>> >> to
>> >> stage failure: Task 0.0:3 failed 4 times, most recent failure:
>> >> Exception
>> >> failure in TID 142 on host sparky-s1.c.quick-heaven-560.internal:
>> >> java.lang.ClassNotFoundException:
>> >> org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1
>> >> [error]         java.net.URLClassLoader$1.run(URLClassLoader.java:366=
)
>> >> [error]         java.net.URLClassLoader$1.run(URLClassLoader.java:355=
)
>> >> [error]         java.security.AccessController.doPrivileged(Native
>> >> Method)
>> >> [error]
>> >> java.net.URLClassLoader.findClass(URLClassLoader.java:354)
>> >> [error]         java.lang.ClassLoader.loadClass(ClassLoader.java:425)
>> >> [error]         java.lang.ClassLoader.loadClass(ClassLoader.java:358)
>> >> [error]         java.lang.Class.forName0(Native Method)
>> >> [error]         java.lang.Class.forName(Class.java:270)
>> >> [error]
>> >>
>> >>
>> >> org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolve=
Class(JavaSerializer.scala:60)
>> >> [error]
>> >> java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:161=
2)
>> >> [error]
>> >> java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
>> >> [error]
>> >>
>> >> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1=
771)
>> >> [error]
>> >> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
>> >> [error]
>> >>
>> >> java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:19=
90)
>> >> [error]
>> >> java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
>> >> [error]
>> >>
>> >> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1=
798)
>> >> [error]
>> >> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
>> >> [error]
>> >> java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
>> >> [error]
>> >>
>> >>
>> >> org.apache.spark.serializer.JavaDeserializationStream.readObject(Java=
Serializer.scala:63)
>> >> [error]
>> >>
>> >> org.apache.spark.scheduler.ResultTask$.deserializeInfo(ResultTask.sca=
la:61)
>> >> [error]
>> >>
>> >> org.apache.spark.scheduler.ResultTask.readExternal(ResultTask.scala:1=
41)
>> >> [error]
>> >> java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:183=
7)
>> >> [error]
>> >>
>> >> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1=
796)
>> >> [error]
>> >> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
>> >> [error]
>> >> java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
>> >> [error]
>> >>
>> >>
>> >> org.apache.spark.serializer.JavaDeserializationStream.readObject(Java=
Serializer.scala:63)
>> >> [error]
>> >>
>> >>
>> >> org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSe=
rializer.scala:85)
>> >> [error]
>> >> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:169)
>> >> [error]
>> >>
>> >>
>> >> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.=
java:1145)
>> >> [error]
>> >>
>> >>
>> >> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor=
.java:615)
>> >> [error]         java.lang.Thread.run(Thread.java:744)
>> >>
>> >> Thanks for any help or guidance.
>> >>
>> >>
>> >>
>> >>
>> >>
>> >> --
>> >> View this message in context:
>> >>
>> >> http://apache-spark-user-list.1001560.n3.nabble.com/Strange-problem-w=
ith-saveAsTextFile-after-upgrade-Spark-0-9-0-1-0-0-tp6832p7122.html
>> >> Sent from the Apache Spark User List mailing list archive at
>> >> Nabble.com.
>> >>
>
>

From dev-return-7992-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun  9 14:45:08 2014
Return-Path: <dev-return-7992-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 59E3711896
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Jun 2014 14:45:08 +0000 (UTC)
Received: (qmail 33005 invoked by uid 500); 9 Jun 2014 14:45:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32952 invoked by uid 500); 9 Jun 2014 14:45:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 69737 invoked by uid 99); 9 Jun 2014 06:44:40 -0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of salexln@gmail.com designates 209.85.223.172 as permitted sender)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:cc:content-type;
        bh=WFWjuR6H5kZzHr2B24jjoHOfN4S0H6QmaXmQ15YDrqU=;
        b=Bvy0E3eVk36CogiPLoGapkRQvEzrKAyGUWN9Gx4YaSrKgPAw+HupwgQNXYqeXxgaEH
         mWgyavi3jI3LIXNLv+800gdjihhHcDiRVNP9pWbcb+vPlqMmmdHiXvwdVUpcY8qpomh5
         8+pEfUxyJX64j3X43dG3RUXLueKk+vLPp3tLwn+iBB8Q/OM66mSKj9Q5IOqWgeuzI1i0
         OTZbFYz24gNmDr/BpRhtxuoIqkLxem1m0J3ZG02Di+EQ4YkZh0RsTgSIaGK2ByTJgpiy
         1Eya/H2me2N4MW0t+RGhr1+Gu1kUCbKcZRNp2eVlMuJ3wHQCwvbqM/KOQxnGEyCEZ7pj
         ATdw==
X-Received: by 10.50.118.102 with SMTP id kl6mr34581798igb.7.1402296255585;
 Sun, 08 Jun 2014 23:44:15 -0700 (PDT)
MIME-Version: 1.0
From: Alex Levin <salexln@gmail.com>
Date: Mon, 9 Jun 2014 09:43:35 +0300
Message-ID: <CAK2WwdP7akS1Nd0aTs7sOnzPR1+m35Ck6RA13-fh7Z0DRJ=V5Q@mail.gmail.com>
Subject: Contributing algorithms to MLlib
To: dev@spark.apache.org
Cc: Uzi Hadad <uzihadad@mta.ac.il>
Content-Type: multipart/alternative; boundary=089e011769cb2f234404fb618a28
X-Virus-Checked: Checked by ClamAV on apache.org

--089e011769cb2f234404fb618a28
Content-Type: text/plain; charset=UTF-8

Hi,



I'm a M.Sc. computer science student at Tel-Aviv College, Israel (
www.mta.ac.il) and as part of my final project that is dealing with Machine
Learning algorithms in distributed systems,

I would like to contribute couple of algorithms to MLlib.



My advisor, Dr. Uzi Hadad, and I thought of starting with an implementation
of* Fuzzy k - means* algorithm and continuing with *Hidden Markov Model*
algorithm.




Do you know if anyone is currently working on an implementation of these
algorithms for MLlib?







Regards,

Alex

--089e011769cb2f234404fb618a28--

From dev-return-7993-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun  9 15:48:37 2014
Return-Path: <dev-return-7993-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 563D111B20
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Jun 2014 15:48:37 +0000 (UTC)
Received: (qmail 87614 invoked by uid 500); 9 Jun 2014 15:48:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87547 invoked by uid 500); 9 Jun 2014 15:48:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87537 invoked by uid 99); 9 Jun 2014 15:48:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Jun 2014 15:48:36 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of rickett.stephanie@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Jun 2014 15:48:34 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <rickett.stephanie@gmail.com>)
	id 1Wu1nu-0002Uq-Gn
	for dev@spark.incubator.apache.org; Mon, 09 Jun 2014 08:48:10 -0700
Date: Mon, 9 Jun 2014 08:48:10 -0700 (PDT)
From: dataginjaninja <rickett.stephanie@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1402328890475-6972.post@n3.nabble.com>
Subject: implementing the VectorAccumulatorParam
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

The  programming-guide
<http://spark.apache.org/docs/latest/programming-guide.html>   has the
following:

However, when I try to use this I get an error:


Last thing, am I posting on the wrong list?



-----
Cheers,

Stephanie
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/implementing-the-VectorAccumulatorParam-tp6972.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7994-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun  9 15:58:13 2014
Return-Path: <dev-return-7994-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D571011B42
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Jun 2014 15:58:13 +0000 (UTC)
Received: (qmail 96291 invoked by uid 500); 9 Jun 2014 15:58:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96232 invoked by uid 500); 9 Jun 2014 15:58:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96222 invoked by uid 99); 9 Jun 2014 15:58:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Jun 2014 15:58:13 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of rickett.stephanie@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Jun 2014 15:58:08 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <rickett.stephanie@gmail.com>)
	id 1Wu1xE-0002xh-4n
	for dev@spark.incubator.apache.org; Mon, 09 Jun 2014 08:57:48 -0700
Date: Mon, 9 Jun 2014 08:57:48 -0700 (PDT)
From: dataginjaninja <rickett.stephanie@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1402329468137-6973.post@n3.nabble.com>
Subject: implementing the VectorAccumulatorParam
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

The  programming-guide
<http://spark.apache.org/docs/latest/programming-guide.html>   has the
following:
    
    object VectorAccumulatorParam extends AccumulatorParam[Vector] {
      def zero(initialValue: Vector): Vector = {
        Vector.zeros(initialValue.size)
      }
      def addInPlace(v1: Vector, v2: Vector): Vector = {
        v1 += v2
      }
    }


// Then, create an Accumulator of this type:
val vecAccum = sc.accumulator(new Vector(...))(VectorAccumulatorParam)

However, when I try to use this I get an error:

scala> import org.apache.spark.AccumulatorParam
import org.apache.spark.AccumulatorParam

scala> object VectorAccumulatorParam extends AccumulatorParam[Vector] {
     |   def zero(initialValue: Vector): Vector = {
     |     Vector.zeros(initialValue.size)
     |   }
     |   def addInPlace(v1: Vector, v2: Vector): Vector = {
     |     v1 += v2
     |   }
     | }
<console>:12: error: type Vector takes type parameters
       object VectorAccumulatorParam extends AccumulatorParam[Vector] {
                                                              ^


Last thing, am I posting on the wrong list?



-----
Cheers,

Stephanie
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/implementing-the-VectorAccumulatorParam-tp6973.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7995-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun  9 16:21:37 2014
Return-Path: <dev-return-7995-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2414011BFB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Jun 2014 16:21:37 +0000 (UTC)
Received: (qmail 56092 invoked by uid 500); 9 Jun 2014 16:21:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 56041 invoked by uid 500); 9 Jun 2014 16:21:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56031 invoked by uid 99); 9 Jun 2014 16:21:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Jun 2014 16:21:36 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.128.171 as permitted sender)
Received: from [209.85.128.171] (HELO mail-ve0-f171.google.com) (209.85.128.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Jun 2014 16:21:34 +0000
Received: by mail-ve0-f171.google.com with SMTP id jz11so2586345veb.2
        for <dev@spark.apache.org>; Mon, 09 Jun 2014 09:21:10 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=Fk8Y/1N4fTB+NDIxwTpAHTi9bYoVo8EdRU3aR2BlTVI=;
        b=IZVDqT0L0fd7j4QfN7tbgmw6AeuwD3tgShHs9Eb9qKqiezfD0EHNju9ne1W776cZmj
         r5SawmYANUd9sJjTuzcuqb9LK4TbE0RqEtnfwc3eWsagOrAua7F83mIP6eARH1STDKLW
         v+e803jdaY0uBM6K+spxhRdwe2noTn67bUJcdkRV/Zgu9FUjCo0rd/boTjWyEYvNVOH6
         X2y1oaGQRLgy9CaJOfnLXLB4tZ95QmCodgeVlIKSXFARWoAzipvKx+lYmlTw3wDfKXiN
         RQ9MlWYuSO6MRgSlsSUaQlzg+qFSHfBJrQ654jjPJfDM9C85n42TFHESmeB6JgMd69Nu
         J0fQ==
X-Gm-Message-State: ALoCoQl3KlRJO9LKK7Wels9PPpeqSegu8eCtYsrlXGqvUVZwiBckxvaNXn54lLsfNzzcPNuAzu0j
X-Received: by 10.58.119.167 with SMTP id kv7mr1083888veb.78.1402330869985;
 Mon, 09 Jun 2014 09:21:09 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.121.6 with HTTP; Mon, 9 Jun 2014 09:20:49 -0700 (PDT)
In-Reply-To: <1402329468137-6973.post@n3.nabble.com>
References: <1402329468137-6973.post@n3.nabble.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 9 Jun 2014 12:20:49 -0400
Message-ID: <CAMAsSdKXrkzgT51D4reLp+tfHp7d+14X1DRVStPAdsOyNm7BEg@mail.gmail.com>
Subject: Re: implementing the VectorAccumulatorParam
To: "dev@spark.apache.org" <dev@spark.apache.org>, user@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

(The user@ list might be a bit better but I can see why it might look
like a dev@ question.)

Did you import org.apache.spark.mllib.linalg.Vector ? I think you are
picking up Scala's Vector class instead.


On Mon, Jun 9, 2014 at 11:57 AM, dataginjaninja
<rickett.stephanie@gmail.com> wrote:
> The  programming-guide
> <http://spark.apache.org/docs/latest/programming-guide.html>   has the
> following:
>
>     object VectorAccumulatorParam extends AccumulatorParam[Vector] {
>       def zero(initialValue: Vector): Vector = {
>         Vector.zeros(initialValue.size)
>       }
>       def addInPlace(v1: Vector, v2: Vector): Vector = {
>         v1 += v2
>       }
>     }
>
>
> // Then, create an Accumulator of this type:
> val vecAccum = sc.accumulator(new Vector(...))(VectorAccumulatorParam)
>
> However, when I try to use this I get an error:
>
> scala> import org.apache.spark.AccumulatorParam
> import org.apache.spark.AccumulatorParam
>
> scala> object VectorAccumulatorParam extends AccumulatorParam[Vector] {
>      |   def zero(initialValue: Vector): Vector = {
>      |     Vector.zeros(initialValue.size)
>      |   }
>      |   def addInPlace(v1: Vector, v2: Vector): Vector = {
>      |     v1 += v2
>      |   }
>      | }
> <console>:12: error: type Vector takes type parameters
>        object VectorAccumulatorParam extends AccumulatorParam[Vector] {
>                                                               ^
>
>
> Last thing, am I posting on the wrong list?
>
>
>
> -----
> Cheers,
>
> Stephanie
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/implementing-the-VectorAccumulatorParam-tp6973.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7996-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun  9 16:23:19 2014
Return-Path: <dev-return-7996-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BDB2E11C01
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Jun 2014 16:23:19 +0000 (UTC)
Received: (qmail 58430 invoked by uid 500); 9 Jun 2014 16:23:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58370 invoked by uid 500); 9 Jun 2014 16:23:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58360 invoked by uid 99); 9 Jun 2014 16:23:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Jun 2014 16:23:19 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of rickett.stephanie@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Jun 2014 16:23:14 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <rickett.stephanie@gmail.com>)
	id 1Wu2LW-0005D9-Ae
	for dev@spark.incubator.apache.org; Mon, 09 Jun 2014 09:22:54 -0700
Date: Mon, 9 Jun 2014 09:22:54 -0700 (PDT)
From: dataginjaninja <rickett.stephanie@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1402330974320-6975.post@n3.nabble.com>
In-Reply-To: <CAMAsSdKXrkzgT51D4reLp+tfHp7d+14X1DRVStPAdsOyNm7BEg@mail.gmail.com>
References: <1402329468137-6973.post@n3.nabble.com> <CAMAsSdKXrkzgT51D4reLp+tfHp7d+14X1DRVStPAdsOyNm7BEg@mail.gmail.com>
Subject: Re: implementing the VectorAccumulatorParam
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

You are right. I was using the wrong vector class. Thanks.



-----
Cheers,

Stephanie
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/implementing-the-VectorAccumulatorParam-tp6973p6975.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7997-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun  9 16:25:59 2014
Return-Path: <dev-return-7997-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5421911C0C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Jun 2014 16:25:59 +0000 (UTC)
Received: (qmail 62250 invoked by uid 500); 9 Jun 2014 16:25:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62187 invoked by uid 500); 9 Jun 2014 16:25:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62177 invoked by uid 99); 9 Jun 2014 16:25:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Jun 2014 16:25:58 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of rickett.stephanie@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Jun 2014 16:25:56 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <rickett.stephanie@gmail.com>)
	id 1Wu2O4-0005PY-HE
	for dev@spark.incubator.apache.org; Mon, 09 Jun 2014 09:25:32 -0700
Date: Mon, 9 Jun 2014 09:25:32 -0700 (PDT)
From: dataginjaninja <rickett.stephanie@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1402331132522-6976.post@n3.nabble.com>
In-Reply-To: <CAMAsSdKXrkzgT51D4reLp+tfHp7d+14X1DRVStPAdsOyNm7BEg@mail.gmail.com>
References: <1402329468137-6973.post@n3.nabble.com> <CAMAsSdKXrkzgT51D4reLp+tfHp7d+14X1DRVStPAdsOyNm7BEg@mail.gmail.com>
Subject: Re: implementing the VectorAccumulatorParam
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

New error :-(

scala> object VectorAccumulatorParam extends AccumulatorParam[Vector] {
     |   def zero(initialValue: Vector): Vector = {
     |     Vector.zeros(initialValue.size)
     |   }
     |   def addInPlace(v1: Vector, v2: Vector): Vector = {
     |     v1 += v2
     |   }
     | }
<console>:12: error: not found: type AccumulatorParam
       object VectorAccumulatorParam extends AccumulatorParam[Vector] {
                                             ^
<console>:14: error: value zeros is not a member of object
scala.collection.immutable.Vector
           Vector.zeros(initialValue.size)
                  ^
<console>:17: error: value += is not a member of
org.apache.spark.mllib.linalg.Vector
           v1 += v2
              ^




-----
Cheers,

Stephanie
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/implementing-the-VectorAccumulatorParam-tp6973p6976.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7998-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun  9 16:27:50 2014
Return-Path: <dev-return-7998-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C034911C14
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Jun 2014 16:27:50 +0000 (UTC)
Received: (qmail 64510 invoked by uid 500); 9 Jun 2014 16:27:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64443 invoked by uid 500); 9 Jun 2014 16:27:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64433 invoked by uid 99); 9 Jun 2014 16:27:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Jun 2014 16:27:50 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of rickett.stephanie@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Jun 2014 16:27:45 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <rickett.stephanie@gmail.com>)
	id 1Wu2Pt-0005VO-GF
	for dev@spark.incubator.apache.org; Mon, 09 Jun 2014 09:27:25 -0700
Date: Mon, 9 Jun 2014 09:27:25 -0700 (PDT)
From: dataginjaninja <rickett.stephanie@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1402331245491-6978.post@n3.nabble.com>
In-Reply-To: <CAMAsSdKXrkzgT51D4reLp+tfHp7d+14X1DRVStPAdsOyNm7BEg@mail.gmail.com>
References: <1402329468137-6973.post@n3.nabble.com> <CAMAsSdKXrkzgT51D4reLp+tfHp7d+14X1DRVStPAdsOyNm7BEg@mail.gmail.com>
Subject: Re: implementing the VectorAccumulatorParam
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

New error :-(

scala> object VectorAccumulatorParam extends AccumulatorParam[Vector] {
     |   def zero(initialValue: Vector): Vector = {
     |     Vector.zeros(initialValue.size)
     |   }
     |   def addInPlace(v1: Vector, v2: Vector): Vector = {
     |     v1 += v2
     |   }
     | }
<console>:14: error: value zeros is not a member of object
scala.collection.immutable.Vector
           Vector.zeros(initialValue.size)
                  ^
<console>:17: error: value += is not a member of
org.apache.spark.mllib.linalg.Vector
           v1 += v2



-----
Cheers,

Stephanie
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/implementing-the-VectorAccumulatorParam-tp6973p6978.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-7999-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun  9 16:33:59 2014
Return-Path: <dev-return-7999-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E87E911C3B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Jun 2014 16:33:58 +0000 (UTC)
Received: (qmail 73781 invoked by uid 500); 9 Jun 2014 16:33:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73634 invoked by uid 500); 9 Jun 2014 16:33:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73461 invoked by uid 99); 9 Jun 2014 16:33:57 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Jun 2014 16:33:57 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.128.179 as permitted sender)
Received: from [209.85.128.179] (HELO mail-ve0-f179.google.com) (209.85.128.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Jun 2014 16:33:52 +0000
Received: by mail-ve0-f179.google.com with SMTP id oy12so6743727veb.38
        for <dev@spark.apache.org>; Mon, 09 Jun 2014 09:33:32 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=QeXAZSSJxJjLiA43dYDLub0IADefI6D0F1ZErOgzabc=;
        b=Rsegx1W4OWISpxIJK30ObSvfUE+Q0eG/2DcRl+MM3WgFAXcQwt0wTT2h9nY4fOw8sd
         A3LJ7BWqtecSk6+xdaw7SksdELEZz76mn2KYsOIsf6xiuBO4FDyHm/+mf3FzHmrFB9ee
         8TV2bzDrv00HAodkl3y24VmOVjbGwxkdBU0PoIQtD9KnE41dnY9t3B2W1YkIECEKpFq0
         1ZASsuCuEivV4BvHh2b0bad0jDbwfxjJvA1OWMxFfIaqNvBhKyiNXrah8pKTTvPI/p20
         vs1Cq9nkpmMtpcFuGQUOnmtwVYA1uBnmumdtRjSClFLHks41U8G7uopmwUCR4XbdV7EU
         p9ew==
X-Gm-Message-State: ALoCoQm3HdD0v132AjBqdO4/IwT19u1U0C1XkjoPC23TPwm1EMv+JUPIzRdD8UuQ7oXUSQciCj13
X-Received: by 10.58.74.201 with SMTP id w9mr2204541vev.56.1402331612221; Mon,
 09 Jun 2014 09:33:32 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.121.6 with HTTP; Mon, 9 Jun 2014 09:33:12 -0700 (PDT)
In-Reply-To: <1402331245491-6978.post@n3.nabble.com>
References: <1402329468137-6973.post@n3.nabble.com> <CAMAsSdKXrkzgT51D4reLp+tfHp7d+14X1DRVStPAdsOyNm7BEg@mail.gmail.com>
 <1402331245491-6978.post@n3.nabble.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 9 Jun 2014 12:33:12 -0400
Message-ID: <CAMAsSdL9Qv_FOiJSBAUsTGpiDbv5FbcqQ=GkHUtNhBMYE3dNsw@mail.gmail.com>
Subject: Re: implementing the VectorAccumulatorParam
To: user@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

(BCC dev@)

The example is out of date with respect to current Vector class. The
zeros() method is on "Vectors". There is not currently a += operation
for Vector anymore.

To be fair the example doesn't claim this illustrates use of the Spark
Vector class but it did work with the now-deprecated Vector.

Make sure you still have AccumulableParam imported.

You could make a PR to adjust the example to something that works with
the newer class once you have it working.

On Mon, Jun 9, 2014 at 12:27 PM, dataginjaninja
<rickett.stephanie@gmail.com> wrote:
> New error :-(
>
> scala> object VectorAccumulatorParam extends AccumulatorParam[Vector] {
>      |   def zero(initialValue: Vector): Vector = {
>      |     Vector.zeros(initialValue.size)
>      |   }
>      |   def addInPlace(v1: Vector, v2: Vector): Vector = {
>      |     v1 += v2
>      |   }
>      | }
> <console>:14: error: value zeros is not a member of object
> scala.collection.immutable.Vector
>            Vector.zeros(initialValue.size)
>                   ^
> <console>:17: error: value += is not a member of
> org.apache.spark.mllib.linalg.Vector
>            v1 += v2
>
>
>
> -----
> Cheers,
>
> Stephanie
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/implementing-the-VectorAccumulatorParam-tp6973p6978.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8000-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 10 05:18:08 2014
Return-Path: <dev-return-8000-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1010C113D9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 10 Jun 2014 05:18:08 +0000 (UTC)
Received: (qmail 54338 invoked by uid 500); 10 Jun 2014 05:18:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54269 invoked by uid 500); 10 Jun 2014 05:18:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54258 invoked by uid 99); 10 Jun 2014 05:18:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Jun 2014 05:18:05 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.170 as permitted sender)
Received: from [209.85.214.170] (HELO mail-ob0-f170.google.com) (209.85.214.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Jun 2014 05:18:00 +0000
Received: by mail-ob0-f170.google.com with SMTP id uz6so2178146obc.1
        for <dev@spark.apache.org>; Mon, 09 Jun 2014 22:17:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=gGZJSEnilNWx+b9AvnDrv5HL8PQRM0dbU+a6NWz35jI=;
        b=Al7yO80IBTgtkS4xzJUEwWk1Udi1Oj2sd0t6hQfLhrZUdsBV4KBHvp/sSOasv9918v
         W3xoBmdUe0sB3GehZlVaWxkEisGuVJtussEVt8+0NNXHChxjOQb6lfzP9vDVw90jzSIT
         tk5rigCJX6rwX2shqgd2DEpLhpZ6u3XJ3OlfEYFAH3fuis6nzNXWXo6yBYVsZW36m2kP
         dlUobg7kFBlG/fV627Aget1IktX3WRNlBEVfItb5oQOjbnrixztO7XKT8Z03lUXH0Apn
         1M1z9Ii01Hjkf/v3LjsyGgNYApD+UFzhrHNWXc3tpuvMNiAv9GQc6ofgalkC8LF+hbFr
         0Z3Q==
MIME-Version: 1.0
X-Received: by 10.60.48.36 with SMTP id i4mr264092oen.77.1402377460054; Mon,
 09 Jun 2014 22:17:40 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Mon, 9 Jun 2014 22:17:40 -0700 (PDT)
Date: Mon, 9 Jun 2014 22:17:40 -0700
Message-ID: <CABPQxsvBjZ6gaaRFqionDVWt3bHt7g5UdzyU5qLNTORV2UkpLw@mail.gmail.com>
Subject: Emergency maintenace on jenkins
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Just a heads up - due to an outage at UCB we've lost several of the
Jenkins slaves. I'm trying to spin up new slaves on EC2 in order to
compensate, but this might fail some ongoing builds.

The good news is if we do get it working with EC2 workers, then we
will have burst capability in the future - e.g. on release deadlines.
So it's not all bad!

- Patrick

From dev-return-8001-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 10 06:35:33 2014
Return-Path: <dev-return-8001-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0F9AA1157D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 10 Jun 2014 06:35:33 +0000 (UTC)
Received: (qmail 33967 invoked by uid 500); 10 Jun 2014 06:35:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33908 invoked by uid 500); 10 Jun 2014 06:35:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33897 invoked by uid 99); 10 Jun 2014 06:35:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Jun 2014 06:35:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of henry.saputra@gmail.com designates 74.125.82.47 as permitted sender)
Received: from [74.125.82.47] (HELO mail-wg0-f47.google.com) (74.125.82.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Jun 2014 06:35:26 +0000
Received: by mail-wg0-f47.google.com with SMTP id k14so5817767wgh.6
        for <dev@spark.apache.org>; Mon, 09 Jun 2014 23:35:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=0kP82mAdMjCpKyBCbtg0N2MMd3GO7u6Ltx0tG9dNiQ0=;
        b=PhkbjhT0lG8uWAx8i3NKq369fwXil6FHT1Bxo7boDI6/I/VfqC9T2d8Y790rEUfj7n
         5Sq8L1VYZu4eqwmRauEaeSiuPg92ADhF2IUjLikS8cTR1k6khRT3UC3+jpZoYbJfEb4O
         FxnCDOsEkxUQHWlS7TvSkqLs5SkbtJxSzsQb2YqgmdSJNoiFOoK9/S2oWkb+rm4/9nhc
         DaHVMGfToeSXvaA8bhaOk/WAQFNd1Ro8mcCpr6AobvLstPYR4lZB68R6krxsV3x25Exe
         kSTGSsSaSNnFt3J0Zhv5iwQ8a5x7XscbtT4K3D6zjDArAtpVImz5vWZTqoxQDZxcV9Pg
         /EwQ==
MIME-Version: 1.0
X-Received: by 10.194.6.2 with SMTP id w2mr37538971wjw.6.1402382105135; Mon,
 09 Jun 2014 23:35:05 -0700 (PDT)
Received: by 10.216.165.71 with HTTP; Mon, 9 Jun 2014 23:35:05 -0700 (PDT)
In-Reply-To: <CABPQxsvBjZ6gaaRFqionDVWt3bHt7g5UdzyU5qLNTORV2UkpLw@mail.gmail.com>
References: <CABPQxsvBjZ6gaaRFqionDVWt3bHt7g5UdzyU5qLNTORV2UkpLw@mail.gmail.com>
Date: Mon, 9 Jun 2014 23:35:05 -0700
Message-ID: <CALuGr6aqFVdUtNbQ=jy_Cn4NZWjv8N2o0yJD2H=8ojJn7B76uQ@mail.gmail.com>
Subject: Re: Emergency maintenace on jenkins
From: Henry Saputra <henry.saputra@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b5d2e203751a504fb758782
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b5d2e203751a504fb758782
Content-Type: text/plain; charset=UTF-8

Thanks for letting us know Patrick.

- Henry

On Monday, June 9, 2014, Patrick Wendell <pwendell@gmail.com> wrote:

> Just a heads up - due to an outage at UCB we've lost several of the
> Jenkins slaves. I'm trying to spin up new slaves on EC2 in order to
> compensate, but this might fail some ongoing builds.
>
> The good news is if we do get it working with EC2 workers, then we
> will have burst capability in the future - e.g. on release deadlines.
> So it's not all bad!
>
> - Patrick
>

--047d7b5d2e203751a504fb758782--

From dev-return-8002-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 10 08:16:36 2014
Return-Path: <dev-return-8002-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BF3AD117D5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 10 Jun 2014 08:16:36 +0000 (UTC)
Received: (qmail 17216 invoked by uid 500); 10 Jun 2014 08:16:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17128 invoked by uid 500); 10 Jun 2014 08:16:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17106 invoked by uid 99); 10 Jun 2014 08:16:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Jun 2014 08:16:27 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.54 as permitted sender)
Received: from [209.85.219.54] (HELO mail-oa0-f54.google.com) (209.85.219.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Jun 2014 08:16:24 +0000
Received: by mail-oa0-f54.google.com with SMTP id eb12so4574527oac.41
        for <dev@spark.apache.org>; Tue, 10 Jun 2014 01:16:00 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=xP+wponCTiifYlKu+2VaBYmpnpXmo6KM/YwpqKYHvjk=;
        b=T02tTCOOTbxkPcyHLBWYxs+6B9xrLpqym1gJ+ebyXEHRhFFnXIuwBotZpqmWzxJFFj
         cYfvtn1CEoCOPhRiqEwCaW1ff+ohzpridJggYHgMasw6Xi6qwfEI/Lvd45eC3kcYWsbw
         dFTfqiy8At5qm+ABCgwhNWXl3VI4o8dl1DAOx+NrMpdbUmH7zFWKIU33bqa8wQOElqbi
         1SD+vTxyIZ2f/RloauxKy8J6aAyBNcOFWlZ5RsQdRd4WrgZyZH7oTzKCss5/Um+T+egP
         ockNW7qBU6tf5L65lCCKxmUNwL59tb7DGN7NwRA0Yv6tzMAo70cdebEvkAazBOoWptn8
         FyxQ==
MIME-Version: 1.0
X-Received: by 10.60.48.36 with SMTP id i4mr1003702oen.77.1402388160032; Tue,
 10 Jun 2014 01:16:00 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Tue, 10 Jun 2014 01:15:59 -0700 (PDT)
In-Reply-To: <CABPQxsvBjZ6gaaRFqionDVWt3bHt7g5UdzyU5qLNTORV2UkpLw@mail.gmail.com>
References: <CABPQxsvBjZ6gaaRFqionDVWt3bHt7g5UdzyU5qLNTORV2UkpLw@mail.gmail.com>
Date: Tue, 10 Jun 2014 01:15:59 -0700
Message-ID: <CABPQxsuggDQKTb_89HyH+iaG0QR38LuKNQ-cixDifwhZr5FHcg@mail.gmail.com>
Subject: Re: Emergency maintenace on jenkins
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

No luck with this tonight - unfortunately our Python tests aren't
working well with Python 2.6 and some other issues made it hard to get
the EC2 worker up to speed. Hopefully we can have this up and running
tomororw.

- Patrick

On Mon, Jun 9, 2014 at 10:17 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Just a heads up - due to an outage at UCB we've lost several of the
> Jenkins slaves. I'm trying to spin up new slaves on EC2 in order to
> compensate, but this might fail some ongoing builds.
>
> The good news is if we do get it working with EC2 workers, then we
> will have burst capability in the future - e.g. on release deadlines.
> So it's not all bad!
>
> - Patrick

From dev-return-8003-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 10 16:59:55 2014
Return-Path: <dev-return-8003-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E48B1116BE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 10 Jun 2014 16:59:54 +0000 (UTC)
Received: (qmail 30896 invoked by uid 500); 10 Jun 2014 16:59:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30834 invoked by uid 500); 10 Jun 2014 16:59:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 28653 invoked by uid 99); 10 Jun 2014 07:37:27 -0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: unknown (nike.apache.org: error in processing during lookup of dhurwitz@ebay.com)
Date: Tue, 10 Jun 2014 00:36:59 -0700 (PDT)
From: DanielH <dhurwitz@ebay.com>
To: dev@spark.incubator.apache.org
Message-ID: <1402385819712-6982.post@n3.nabble.com>
In-Reply-To: <CAOEPXP5EZYot8feS_vbmPeAaRifShfg3RaPeOcHsmNHfd0FoKg@mail.gmail.com>
References: <CAFTXMEJaWEPLgPNoXZi0qFo4OP3o0VWs2zBfiKUFQaZKm15o_g@mail.gmail.com> <CAOEPXP5EZYot8feS_vbmPeAaRifShfg3RaPeOcHsmNHfd0FoKg@mail.gmail.com>
Subject: Re: debugger
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Josh,

I came across this post when looking for a debugger or RDD visualization
tool for Spark. I am using Spark 0.9.1 and upgrading soon to Spark 1.0. The
links you posted are dead. Can you please direct me to how I can debug my
existing Spark job.

Will I need to edit my existing job's code in addition to setting any
environment variables/parameters.

The problem: I am running Bagel on a very large graph and when the job gets
to the final step (saveAsTextFile) it will hang for up to many days until I
kill it. Oftentimes if I simply rerun the job, it will finish in an hour
which is the expected amount of time it should take.

Thanks!



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/debugger-tp284p6982.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8004-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 10 22:19:10 2014
Return-Path: <dev-return-8004-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5C0AC1153C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 10 Jun 2014 22:19:10 +0000 (UTC)
Received: (qmail 49290 invoked by uid 500); 10 Jun 2014 22:19:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49234 invoked by uid 500); 10 Jun 2014 22:19:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49222 invoked by uid 99); 10 Jun 2014 22:19:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Jun 2014 22:19:09 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.54 as permitted sender)
Received: from [209.85.219.54] (HELO mail-oa0-f54.google.com) (209.85.219.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Jun 2014 22:19:06 +0000
Received: by mail-oa0-f54.google.com with SMTP id eb12so5741459oac.13
        for <dev@spark.apache.org>; Tue, 10 Jun 2014 15:18:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=k1jyj59ORd931iHnaQKQUaQBwVaQanoMOYSyoc1teZU=;
        b=m8lccZPOw/dndVOKYp3862UXkfb9A0db+0jMu2wxJavUeDQF6NmYfJK8JtJYwa0EmB
         DbzJTkfyf/fRrcRGpXTnfrBzY9b2eJN+M3Mb/ZJhgOsQOBjpCr4d3pxGSBpsGMjZrrST
         cjjcoX9+1WbP5VEI5BwxjN4WhyBT4OYzvR+xgyj9EtAGcc/J/DxIZ5mM+Ar5En64UVAG
         mvpT1mTOZEpgcsks4mhxblJ2zhNgQLp8tGPdusqSQYS3jTP/V2hdM5VXUj75Gvd9bTNy
         7KsxGx8AawqjuKMy9NSqzAwpsH4Otf3p2PpXC8Bqi82GbtusxhreMmW8qFfsFMxAqF5v
         9GBQ==
MIME-Version: 1.0
X-Received: by 10.60.44.243 with SMTP id h19mr36017111oem.46.1402438721688;
 Tue, 10 Jun 2014 15:18:41 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Tue, 10 Jun 2014 15:18:41 -0700 (PDT)
In-Reply-To: <CABPQxsuggDQKTb_89HyH+iaG0QR38LuKNQ-cixDifwhZr5FHcg@mail.gmail.com>
References: <CABPQxsvBjZ6gaaRFqionDVWt3bHt7g5UdzyU5qLNTORV2UkpLw@mail.gmail.com>
	<CABPQxsuggDQKTb_89HyH+iaG0QR38LuKNQ-cixDifwhZr5FHcg@mail.gmail.com>
Date: Tue, 10 Jun 2014 15:18:41 -0700
Message-ID: <CABPQxsuhQuOv-kB+DRnD4OUcMmORa1poBtjabP6Qxy=xMXzwjw@mail.gmail.com>
Subject: Re: Emergency maintenace on jenkins
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey just to update people - as of around 1pm PT we were back up and
running with Jenkins slaves on EC2. Sorry about the disruption.

- Patrick

On Tue, Jun 10, 2014 at 1:15 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> No luck with this tonight - unfortunately our Python tests aren't
> working well with Python 2.6 and some other issues made it hard to get
> the EC2 worker up to speed. Hopefully we can have this up and running
> tomororw.
>
> - Patrick
>
> On Mon, Jun 9, 2014 at 10:17 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>> Just a heads up - due to an outage at UCB we've lost several of the
>> Jenkins slaves. I'm trying to spin up new slaves on EC2 in order to
>> compensate, but this might fail some ongoing builds.
>>
>> The good news is if we do get it working with EC2 workers, then we
>> will have burst capability in the future - e.g. on release deadlines.
>> So it's not all bad!
>>
>> - Patrick

From dev-return-8005-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 11 02:40:28 2014
Return-Path: <dev-return-8005-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5914C11CB4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Jun 2014 02:40:25 +0000 (UTC)
Received: (qmail 20371 invoked by uid 500); 11 Jun 2014 02:40:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20319 invoked by uid 500); 11 Jun 2014 02:40:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20306 invoked by uid 99); 11 Jun 2014 02:40:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 02:40:18 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of henry.yijieshen@gmail.com designates 209.85.160.45 as permitted sender)
Received: from [209.85.160.45] (HELO mail-pb0-f45.google.com) (209.85.160.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 02:40:13 +0000
Received: by mail-pb0-f45.google.com with SMTP id um1so6854937pbc.18
        for <dev@spark.apache.org>; Tue, 10 Jun 2014 19:39:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=from:content-type:content-transfer-encoding:subject:message-id:date
         :to:mime-version;
        bh=ADm0crjZdcEgMwqErotEJT+ATEWoq2am16ewedifPHQ=;
        b=evzqCzoHD/ZJwzykDiKyEpiVgZm3bY4h4yZbuxw/LUpeVe2NryKVR4m34ClC3ln/hC
         AxW46QGM0t2iGOXEH1/GlaF6o4jUg5uxeB9VpY6B5Xaz4tF+OxrTWkH1Wc8BgX9ruwTc
         2VDZHiJq8NuktM1B9ejTyF/8mCu51mV+tqGf0xEf607IB9M2JQGxHr7cmEl718/Ye8hC
         Q7+tScMF9U3+uduonu98mPYEFQO/5YWUXcIHvRpT+qXX6X2G19i++TgJ0tbrEIFOMzFM
         6JES8tccVzAs84gpaGRotYhMZGtLdfRqtxSC4+Nj2vxrPg1bIe92ZF5idCMRFqk1roZU
         7v5A==
X-Received: by 10.67.21.205 with SMTP id hm13mr9821041pad.112.1402454389246;
        Tue, 10 Jun 2014 19:39:49 -0700 (PDT)
Received: from [10.30.1.154] ([159.226.43.30])
        by mx.google.com with ESMTPSA id bx5sm14453506pbd.69.2014.06.10.19.39.47
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 10 Jun 2014 19:39:48 -0700 (PDT)
From: =?utf-8?B?55Sz5q+F5p2w?= <henry.yijieshen@gmail.com>
Content-Type: text/plain; charset=windows-1252
Content-Transfer-Encoding: quoted-printable
Subject: Run ScalaTest inside Intellij IDEA 
Message-Id: <811039BF-0610-4BAC-BFD7-41BBC981768C@gmail.com>
Date: Wed, 11 Jun 2014 10:39:45 +0800
To: dev@spark.apache.org
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

Hi All,

I want to run ScalaTest Suite in IDEA directly, but it seems didn=92t =
pass the make phase before test running.
The problems are as follows:

=
/Users/yijie/code/apache.spark.master/core/src/main/scala/org/apache/spark=
/executor/MesosExecutorBackend.scala
Error:(44, 35) type mismatch;
 found   : org.apache.mesos.protobuf.ByteString
 required: com.google.protobuf.ByteString
      .setData(ByteString.copyFrom(data))
                                  ^
=
/Users/yijie/code/apache.spark.master/core/src/main/scala/org/apache/spark=
/scheduler/cluster/mesos/MesosSchedulerBackend.scala
Error:(119, 35) type mismatch;
 found   : org.apache.mesos.protobuf.ByteString
 required: com.google.protobuf.ByteString
      .setData(ByteString.copyFrom(createExecArg()))
                                  ^
Error:(257, 35) type mismatch;
 found   : org.apache.mesos.protobuf.ByteString
 required: com.google.protobuf.ByteString
      .setData(ByteString.copyFrom(task.serializedTask))
                                  ^

Before I run test in IDEA, I build spark through =92sbt/sbt assembly=92,
import projects into IDEA after =92sbt/sbt gen-idea=92,=20
and able to run test in Terminal =92sbt/sbt test=92

Are there anything I leave out in order to run/debug testsuite inside =
IDEA?

Best regards,
Yijie=

From dev-return-8006-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 11 03:11:05 2014
Return-Path: <dev-return-8006-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9058711D77
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Jun 2014 03:11:05 +0000 (UTC)
Received: (qmail 49659 invoked by uid 500); 11 Jun 2014 03:11:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49599 invoked by uid 500); 11 Jun 2014 03:11:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49589 invoked by uid 99); 11 Jun 2014 03:11:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 03:11:04 +0000
X-ASF-Spam-Status: No, hits=0.0 required=10.0
	tests=
X-Spam-Check-By: apache.org
Received-SPF: unknown (athena.apache.org: error in processing during lookup of taeyun.kim@innowireless.co.kr)
Received: from [59.12.193.79] (HELO mail.innowireless.co.kr) (59.12.193.79)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 03:10:58 +0000
Received: from INNOC358 (218.154.28.162) by mail.innowireless.co.kr
 (59.12.193.79) with Microsoft SMTP Server id 8.2.255.0; Wed, 11 Jun 2014
 12:03:10 +0900
From: innowireless TaeYun Kim <taeyun.kim@innowireless.co.kr>
To: <dev@spark.apache.org>
Subject: Suggestion: rdd.compute()
Date: Wed, 11 Jun 2014 12:10:38 +0900
Message-ID: <001001cf8522$b8467040$28d350c0$@innowireless.co.kr>
MIME-Version: 1.0
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: 7bit
X-Mailer: Microsoft Outlook 14.0
Thread-Index: Ac+FIKriJJrRxEXbQpmTNJjEk8Ipuw==
Content-Language: ko
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

Regarding the following scenario, Would it be nice to have an action method
named like 'compute()' that does nothing but computing/materializing the
whole partitions of an RDD?
It can also be useful for the profiling.


-----Original Message-----
From: innowireless TaeYun Kim [mailto:taeyun.kim@innowireless.co.kr] 
Sent: Wednesday, June 11, 2014 11:40 AM
To: user@spark.apache.org
Subject: Question about RDD cache, unpersist, materialization

Hi,

What I (seems to) know about RDD persisting API is as follows:
- cache() and persist() is not an action. It only does a marking.
- unpersist() is also not an action. It only removes a marking. But if the
rdd is already in memory, it is unloaded.

And there seems no API to forcefully materialize the RDD without requiring a
data by an action method, for example first().

So, I am faced with the following scenario.

{
    JavaRDD<T> rddUnion = sc.parallelize(new ArrayList<T>());  // create
empty for merging
    for (int i = 0; i < 10; i++)
    {
        JavaRDD<T2> rdd = sc.textFile(inputFileNames[i]);
        rdd.cache();  // Since it will be used twice, cache.
        rdd.map(...).filter(...).saveAsTextFile(outputFileNames[i]);  //
Transform and save, rdd materializes
        rddUnion = rddUnion.union(rdd.map(...).filter(...));  // Do another
transform to T and merge by union
        rdd.unpersist();  // Now it seems not needed. (But needed actually)
    }
    // Here, rddUnion actually materializes, and needs all 10 rdds that
already unpersisted.
    // So, rebuilding all 10 rdds will occur.
    rddUnion.saveAsTextFile(mergedFileName);
}

If rddUnion can be materialized before the rdd.unpersist() line and
cache()d, the rdds in the loop will not be needed on
rddUnion.saveAsTextFile().

Now what is the best strategy?
- Do not unpersist all 10 rdds in the loop.
- Materialize rddUnion in the loop by calling 'light' action API, like
first().
- Give up and just rebuild/reload all 10 rdds when saving rddUnion.

Is there some misunderstanding?

Thanks.



From dev-return-8007-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 11 03:17:32 2014
Return-Path: <dev-return-8007-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 74F7911D9E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Jun 2014 03:17:32 +0000 (UTC)
Received: (qmail 61708 invoked by uid 500); 11 Jun 2014 03:17:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61649 invoked by uid 500); 11 Jun 2014 03:17:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61636 invoked by uid 99); 11 Jun 2014 03:17:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 03:17:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ankurdave@gmail.com designates 209.85.128.176 as permitted sender)
Received: from [209.85.128.176] (HELO mail-ve0-f176.google.com) (209.85.128.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 03:17:27 +0000
Received: by mail-ve0-f176.google.com with SMTP id db12so6674689veb.35
        for <dev@spark.apache.org>; Tue, 10 Jun 2014 20:17:06 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=YCWbFOQuBOHkrqEqwa9yotw4HNaij6rP3+ICeVhlg9U=;
        b=LAgf1toM8N3QlRrX+WbuFx5BLuJmR7g2AoI/7SiExAJUn5K9sIXn+RSk+iRXOGe0ZY
         SPGJzSt7Amy5OP0Zve+wz5wxTwSKj3SPR3bWLs2FgPOLhiOMoEmiSESsvIBLDxSlR2Ga
         EoK0HpJe7L5AA/OUmIjnasdSTTRMNbDLVHpJBcwRy0baj5S8oTowIfiiR4k8ed7HWEAP
         ZAFVN7g6al6orNJu/daXtBUllb2tgvIw8FTM5lawDMGk0Gz78mQEg1HgalswrS+eYKeI
         meRshkyv4ochee6f2hw6/Lo96x9BU0Hm68+yk6rdZBl1Qu27wUu4UOzLNV+dfkBRMZAk
         6qdQ==
X-Received: by 10.53.12.229 with SMTP id et5mr994770vdd.32.1402456626747; Tue,
 10 Jun 2014 20:17:06 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.95.166 with HTTP; Tue, 10 Jun 2014 20:16:46 -0700 (PDT)
In-Reply-To: <001001cf8522$b8467040$28d350c0$@innowireless.co.kr>
References: <001001cf8522$b8467040$28d350c0$@innowireless.co.kr>
From: Ankur Dave <ankurdave@gmail.com>
Date: Tue, 10 Jun 2014 20:16:46 -0700
Message-ID: <CAK1A71y5LcttZBapRBpvwpDKe0Bo_7FSQ=VKO9Mbao72wuP6Jw@mail.gmail.com>
Subject: Re: Suggestion: rdd.compute()
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1133fa5e0cddd004fb86e126
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133fa5e0cddd004fb86e126
Content-Type: text/plain; charset=UTF-8

You can achieve an equivalent effect by calling rdd.foreach(x => {}), which
is the lightest possible action that forces materialization of the whole
RDD.

Ankur <http://www.ankurdave.com/>

--001a1133fa5e0cddd004fb86e126--

From dev-return-8008-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 11 03:18:18 2014
Return-Path: <dev-return-8008-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6CC3D11DA3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Jun 2014 03:18:18 +0000 (UTC)
Received: (qmail 62636 invoked by uid 500); 11 Jun 2014 03:18:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62578 invoked by uid 500); 11 Jun 2014 03:18:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62567 invoked by uid 99); 11 Jun 2014 03:18:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 03:18:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of qiuzhuang.lian@gmail.com designates 209.85.128.171 as permitted sender)
Received: from [209.85.128.171] (HELO mail-ve0-f171.google.com) (209.85.128.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 03:18:12 +0000
Received: by mail-ve0-f171.google.com with SMTP id jz11so5627871veb.30
        for <dev@spark.apache.org>; Tue, 10 Jun 2014 20:17:51 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=wZs5435WZCvxo2X+ZV1+pbr9KV2hNUEQ5rk2tbnkIYY=;
        b=TtsRnUOd3vQK56vmA7x1nRl5qbfF72iyz+P6SZ2lZjn/ECfdEe1vf/OsS7k1sbSrJj
         3VPipLD6pREjGEYKWiBrRG+OE4033b5Mg0elqwciNQF2gXAHXv7cq82n5AvjcWoGFwvS
         WuPPJVKJzOt2dnPI5BtEedt0VR6xCrdWkGvhizhBQwg0JpbrME6zxUJXopAibXliKGzx
         jAbPAQzfpGWYFUwg2tbH/hMS/XyovIEuGt4akEN+80tSgmgxchzJ1QNHAp8i6BWh5LQG
         TshQtPRRcJGYV31g4JWs8rAbLgXKXaxjOblkRO+eS9Sqt52QND4Uh67MIuvSD3CcWG0M
         ax/g==
MIME-Version: 1.0
X-Received: by 10.58.85.65 with SMTP id f1mr36974146vez.20.1402456671668; Tue,
 10 Jun 2014 20:17:51 -0700 (PDT)
Received: by 10.220.196.11 with HTTP; Tue, 10 Jun 2014 20:17:51 -0700 (PDT)
In-Reply-To: <811039BF-0610-4BAC-BFD7-41BBC981768C@gmail.com>
References: <811039BF-0610-4BAC-BFD7-41BBC981768C@gmail.com>
Date: Wed, 11 Jun 2014 11:17:51 +0800
Message-ID: <CABKvOWstGq-QGNHZnmvRptYcg5idh7LACOk1z3jn5bR=9L_Qrw@mail.gmail.com>
Subject: Re: Run ScalaTest inside Intellij IDEA
From: Qiuzhuang Lian <qiuzhuang.lian@gmail.com>
To: dev@spark.apache.org, henry.yijieshen@gmail.com
Content-Type: multipart/alternative; boundary=047d7b86f126ba51ba04fb86e3e1
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b86f126ba51ba04fb86e3e1
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I also run into this problem when running examples in IDEA. The issue looks
that it uses depends on too many jars and that the classpath seems to have
length limit. So I import the assembly jar and put the head of the list
dependent path and it works.

Thanks,
Qiuzhuang


On Wed, Jun 11, 2014 at 10:39 AM, =E7=94=B3=E6=AF=85=E6=9D=B0 <henry.yijies=
hen@gmail.com> wrote:

> Hi All,
>
> I want to run ScalaTest Suite in IDEA directly, but it seems didn=E2=80=
=99t pass
> the make phase before test running.
> The problems are as follows:
>
>
> /Users/yijie/code/apache.spark.master/core/src/main/scala/org/apache/spar=
k/executor/MesosExecutorBackend.scala
> Error:(44, 35) type mismatch;
>  found   : org.apache.mesos.protobuf.ByteString
>  required: com.google.protobuf.ByteString
>       .setData(ByteString.copyFrom(data))
>                                   ^
>
> /Users/yijie/code/apache.spark.master/core/src/main/scala/org/apache/spar=
k/scheduler/cluster/mesos/MesosSchedulerBackend.scala
> Error:(119, 35) type mismatch;
>  found   : org.apache.mesos.protobuf.ByteString
>  required: com.google.protobuf.ByteString
>       .setData(ByteString.copyFrom(createExecArg()))
>                                   ^
> Error:(257, 35) type mismatch;
>  found   : org.apache.mesos.protobuf.ByteString
>  required: com.google.protobuf.ByteString
>       .setData(ByteString.copyFrom(task.serializedTask))
>                                   ^
>
> Before I run test in IDEA, I build spark through =E2=80=99sbt/sbt assembl=
y=E2=80=99,
> import projects into IDEA after =E2=80=99sbt/sbt gen-idea=E2=80=99,
> and able to run test in Terminal =E2=80=99sbt/sbt test=E2=80=99
>
> Are there anything I leave out in order to run/debug testsuite inside IDE=
A?
>
> Best regards,
> Yijie

--047d7b86f126ba51ba04fb86e3e1--

From dev-return-8009-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 11 03:56:40 2014
Return-Path: <dev-return-8009-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3ABC811E82
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Jun 2014 03:56:40 +0000 (UTC)
Received: (qmail 98221 invoked by uid 500); 11 Jun 2014 03:56:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98151 invoked by uid 500); 11 Jun 2014 03:56:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98140 invoked by uid 99); 11 Jun 2014 03:56:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 03:56:39 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.48 as permitted sender)
Received: from [209.85.216.48] (HELO mail-qa0-f48.google.com) (209.85.216.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 03:56:34 +0000
Received: by mail-qa0-f48.google.com with SMTP id x12so4463690qac.35
        for <dev@spark.apache.org>; Tue, 10 Jun 2014 20:56:13 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=hmazCYFVAne4+lKJ6FlzxGrT5RkMYQUlUPo0/S3CxOM=;
        b=ZAynl5ejLlwkg5o+cM7XS1zLRHhoOS6jtjj5eAg/TzALC3JRuvyJD1JWhTGhFKlobr
         1O0MCFGO+I9KVEcYDIrFT8q8rFPwFhHUMA2qmRwEyJz/jcNh58772E8GwWMsXP4zLD49
         VJBVY+Nd/5DmPzfJ7gsh0iTSB1woyv0WLEVd6MYQgqnqRuz9QOMj3zU1nKcGypicKtnF
         yIOpXWemX7AMEkzlhtDGtxdTlJnH5yQwWL8FuZ6eWZI5ZtXZGnZpxK8EemxOYvYbzCey
         IySeCoy/MEMAoH8zg9kzhwpsmpqQxFa0GweSCCHj24r/dkA7hcZ16pl1eAeG66LKgDTz
         D07A==
MIME-Version: 1.0
X-Received: by 10.224.3.202 with SMTP id 10mr49193179qao.24.1402458973445;
 Tue, 10 Jun 2014 20:56:13 -0700 (PDT)
Received: by 10.140.82.164 with HTTP; Tue, 10 Jun 2014 20:56:13 -0700 (PDT)
In-Reply-To: <CA+B-+fx64WwiSK_ymuvEXMnw2sd+F5fCL3-mQiJbexqyJgzbww@mail.gmail.com>
References: <CA+B-+fw8EkK1ECg=GPACiJUCMRS6C4vVmQC9dwAfpxvcoPDZtA@mail.gmail.com>
	<CAJgQjQ9y_B1LATjt6c1-PmmytBzf2R1XyFVhMayVK2Hay5stMQ@mail.gmail.com>
	<CA+B-+fxE2rMOf_uURn7OtF_HXpjgZFSRZWD4QS=ZDyyvT-HTAQ@mail.gmail.com>
	<CAJgQjQ9UWjURVD2U0uTUUc2DHH5-_VXQ=uH8mFYZvjLgCLSr9w@mail.gmail.com>
	<CA+B-+fx64WwiSK_ymuvEXMnw2sd+F5fCL3-mQiJbexqyJgzbww@mail.gmail.com>
Date: Tue, 10 Jun 2014 20:56:13 -0700
Message-ID: <CA+B-+fxircx6nrzHnhoMjDRzGFnY+R7xvjwU49GjkfT39vVs8A@mail.gmail.com>
Subject: Re: Constraint Solver for Spark
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c24786eca9b004fb876c55
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c24786eca9b004fb876c55
Content-Type: text/plain; charset=UTF-8

Hi,

I am bit confused wiht the code here:

// Solve the least-squares problem for each user and return the new feature
vectors

    Array.range(0, numUsers).map { index =>

      // Compute the full XtX matrix from the lower-triangular part we got
above

      fillFullMatrix(userXtX(index), fullXtX)

      // Add regularization

      var i = 0

      while (i < rank) {

        fullXtX.data(i * rank + i) += lambda

        i += 1

      }

      // Solve the resulting matrix, which is symmetric and
positive-definite

      algo match {

        case ALSAlgo.Implicit =>
Solve.solvePositive(fullXtX.addi(YtY.get.value),
userXy(index)).data

        case ALSAlgo.Explicit => Solve.solvePositive(fullXtX, userXy
(index)).data

      }

    }


On Fri, Jun 6, 2014 at 10:42 AM, Debasish Das <debasish.das83@gmail.com>
wrote:

> Hi Xiangrui,
>
> It's not the linear constraint, It is quadratic inequality with slack,
> first order taylor approximation of off diagonal cross terms and a cyclic
> coordinate descent, which we think will yield orthogonality....It's still
> under works...
>
> Also we want to put a L1 constraint as set of linear equations when
> solving for ALS...
>
> I will create the JIRA...as I see it, this will evolve to a generic
> constraint solver for machine learning problems that has a QP
> structure....ALS is one example....another example is kernel SVMs...
>
> I did not know that lgpl solver can be added to the classpath....if it can
> be then definitely we should add these in ALS.scala...
>
> Thanks.
> Deb
>
>
>
> On Thu, Jun 5, 2014 at 11:31 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
>> I don't quite understand why putting linear constraints can promote
>> orthogonality. For the interfaces, if the subproblem is determined by
>> Y^T Y and Y^T b for each iteration, then the least squares solver, the
>> non-negative least squares solver, or your convex solver is simply a
>> function
>>
>> (A, b) -> x.
>>
>> You can define it as an interface, and make the solver pluggable by
>> adding a setter to ALS. If you want to use your lgpl solver, just
>> include it in the classpath. Creating two separate files still seems
>> unnecessary to me. Could you create a JIRA and we can move our
>> discussion there? Thanks!
>>
>> Best,
>> Xiangrui
>>
>> On Thu, Jun 5, 2014 at 7:20 PM, Debasish Das <debasish.das83@gmail.com>
>> wrote:
>> > Hi Xiangrui,
>> >
>> > For orthogonality properties in the factors we need a constraint solver
>> > other than the usuals (l1, upper and lower bounds, l2 etc)
>> >
>> > The interface of constraint solver is standard and I can add it in mllib
>> > optimization....
>> >
>> > But I am not sure how will I call the gpl licensed ipm solver from
>> > mllib....assume the solver interface is as follows:
>> >
>> > Qpsolver (densematrix h, array [double] f, int linearEquality, int
>> > linearInequality, bool lb, bool ub)
>> >
>> > And then I have functions to update equalities, inequalities, bounds etc
>> > followed by the run which generates the solution....
>> >
>> > For l1 constraints I have to use epigraph formulation which needs a
>> > variable transformation before the solve....
>> >
>> > I was thinking that for the problems that does not need constraints
>> people
>> > will use ALS.scala and ConstrainedALS.scala will have the constrained
>> > formulations....
>> >
>> > I can point you to the code once it is ready and then you can guide me
>> how
>> > to refactor it to mllib als ?
>> >
>> > Thanks.
>> > Deb
>> > Hi Deb,
>> >
>> > Why do you want to make those methods public? If you only need to
>> > replace the solver for subproblems. You can try to make the solver
>> > pluggable. Now it supports least squares and non-negative least
>> > squares. You can define an interface for the subproblem solvers and
>> > maintain the IPM solver at your own code base, if the only information
>> > you need is Y^T Y and Y^T b.
>> >
>> > Btw, just curious, what is the use case for quadratic constraints?
>> >
>> > Best,
>> > Xiangrui
>> >
>> > On Thu, Jun 5, 2014 at 3:38 PM, Debasish Das <debasish.das83@gmail.com>
>> > wrote:
>> >> Hi,
>> >>
>> >> We are adding a constrained ALS solver in Spark to solve matrix
>> >> factorization use-cases which needs additional constraints (bounds,
>> >> equality, inequality, quadratic constraints)
>> >>
>> >> We are using a native version of a primal dual SOCP solver due to its
>> > small
>> >> memory footprint and sparse ccs matrix computation it uses...The solver
>> >> depends on AMD and LDL packages from Timothy Davis for sparse ccs
>> matrix
>> >> algebra (released under lgpl)...
>> >>
>> >> Due to GPL dependencies, it won't be possible to release the code as
>> > Apache
>> >> license for now...If we get good results on our use-cases, we will
>> plan to
>> >> write a version in breeze/modify joptimizer for sparse ccs
>> operations...
>> >>
>> >> I derived ConstrainedALS from Spark mllib ALS and I am comparing the
>> >> performance with default ALS and non-negative ALS as baseline. Plan is
>> to
>> >> release the code as GPL license for community review...I have kept the
>> >> package structure as org.apache.spark.mllib.recommendation
>> >>
>> >> There are some private functions defined in ALS, which I would like to
>> >> reuse....Is it possible to take the private out from the following
>> >> functions:
>> >>
>> >> 1. makeLinkRDDs
>> >> 2. makeInLinkBlock
>> >> 3. makeOutLinkBlock
>> >> 4. randomFactor
>> >> 5. unblockFactors
>> >>
>> >> I don't want to copy any code.... I can ask for a PR to make these
>> >> changes...
>> >>
>> >> Thanks.
>> >> Deb
>>
>
>

--001a11c24786eca9b004fb876c55--

From dev-return-8010-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 11 03:58:51 2014
Return-Path: <dev-return-8010-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 30E4C11E86
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Jun 2014 03:58:51 +0000 (UTC)
Received: (qmail 369 invoked by uid 500); 11 Jun 2014 03:58:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 308 invoked by uid 500); 11 Jun 2014 03:58:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 297 invoked by uid 99); 11 Jun 2014 03:58:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 03:58:50 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.42 as permitted sender)
Received: from [209.85.216.42] (HELO mail-qa0-f42.google.com) (209.85.216.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 03:58:45 +0000
Received: by mail-qa0-f42.google.com with SMTP id dc16so2359032qab.29
        for <dev@spark.apache.org>; Tue, 10 Jun 2014 20:58:24 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=hstj0UIblyjjuHiti3lty9wCCXCUgL7kYBLfN4/UtQA=;
        b=i6TfIuIEUUT2d+OEOB2tNrP7X510WJzyWBpOZS3t5uXnxig0Zg6AsyeVLIXZjQ299Z
         4kXm5rqPZbND1TAi52jjtA8vLGq7A/V2X82FIUhjvrvUUIlfYKCl4lAS0k//VJs9ierf
         Ult+LYK0+n+15IKUQlKgTTT+1jx78pTjkpa06Xl59PZ+iYdl1atR/q/xs4+jptJTj8DQ
         5kEBSCNT1kbOYp3RO6JuF71wId9IVdRAJaxPIvjOm37Hv+1gELuCEyoc5s44L2H0jZCl
         yoEBhlpPH1wJr+JtKCnqQwutke8A4f7PV+wqpoK16aKwuG/ByB5vtnR9rYuFLHeLl3dC
         VHyA==
MIME-Version: 1.0
X-Received: by 10.140.98.116 with SMTP id n107mr44682901qge.93.1402459104371;
 Tue, 10 Jun 2014 20:58:24 -0700 (PDT)
Received: by 10.140.82.164 with HTTP; Tue, 10 Jun 2014 20:58:24 -0700 (PDT)
In-Reply-To: <CA+B-+fxircx6nrzHnhoMjDRzGFnY+R7xvjwU49GjkfT39vVs8A@mail.gmail.com>
References: <CA+B-+fw8EkK1ECg=GPACiJUCMRS6C4vVmQC9dwAfpxvcoPDZtA@mail.gmail.com>
	<CAJgQjQ9y_B1LATjt6c1-PmmytBzf2R1XyFVhMayVK2Hay5stMQ@mail.gmail.com>
	<CA+B-+fxE2rMOf_uURn7OtF_HXpjgZFSRZWD4QS=ZDyyvT-HTAQ@mail.gmail.com>
	<CAJgQjQ9UWjURVD2U0uTUUc2DHH5-_VXQ=uH8mFYZvjLgCLSr9w@mail.gmail.com>
	<CA+B-+fx64WwiSK_ymuvEXMnw2sd+F5fCL3-mQiJbexqyJgzbww@mail.gmail.com>
	<CA+B-+fxircx6nrzHnhoMjDRzGFnY+R7xvjwU49GjkfT39vVs8A@mail.gmail.com>
Date: Tue, 10 Jun 2014 20:58:24 -0700
Message-ID: <CA+B-+fyO10rcr5bXUaxQOodotmy7njwJujzukzj-ZU8zT-oAuA@mail.gmail.com>
Subject: Re: Constraint Solver for Spark
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113a9238ba6ad504fb8774c8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a9238ba6ad504fb8774c8
Content-Type: text/plain; charset=UTF-8

Sorry last one went out by mistake:

Is not for users (0 to numUsers), fullXtX is same ? In the ALS formulation
this is W^TW or H^TH which should be same for all the users ? Why we are
reading userXtX(index) and adding it to fullXtX in the loop over all
numUsers ?

// Solve the least-squares problem for each user and return the new feature
vectors

    Array.range(0, numUsers).map { index =>

      // Compute the full XtX matrix from the lower-triangular part we got
above

      fillFullMatrix(userXtX(index), fullXtX)

      // Add regularization

      var i = 0

      while (i < rank) {

        fullXtX.data(i * rank + i) += lambda

        i += 1

      }

      // Solve the resulting matrix, which is symmetric and
positive-definite

      algo match {

        case ALSAlgo.Implicit =>
Solve.solvePositive(fullXtX.addi(YtY.get.value),
userXy(index)).data

        case ALSAlgo.Explicit => Solve.solvePositive(fullXtX, userXy
(index)).data

      }

    }


On Tue, Jun 10, 2014 at 8:56 PM, Debasish Das <debasish.das83@gmail.com>
wrote:

> Hi,
>
> I am bit confused wiht the code here:
>
> // Solve the least-squares problem for each user and return the new
> feature vectors
>
>     Array.range(0, numUsers).map { index =>
>
>       // Compute the full XtX matrix from the lower-triangular part we
> got above
>
>       fillFullMatrix(userXtX(index), fullXtX)
>
>       // Add regularization
>
>       var i = 0
>
>       while (i < rank) {
>
>         fullXtX.data(i * rank + i) += lambda
>
>         i += 1
>
>       }
>
>       // Solve the resulting matrix, which is symmetric and
> positive-definite
>
>       algo match {
>
>         case ALSAlgo.Implicit => Solve.solvePositive(fullXtX.addi(YtY.get.value),
> userXy(index)).data
>
>         case ALSAlgo.Explicit => Solve.solvePositive(fullXtX, userXy
> (index)).data
>
>       }
>
>     }
>
>
> On Fri, Jun 6, 2014 at 10:42 AM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>
>> Hi Xiangrui,
>>
>> It's not the linear constraint, It is quadratic inequality with slack,
>> first order taylor approximation of off diagonal cross terms and a cyclic
>> coordinate descent, which we think will yield orthogonality....It's still
>> under works...
>>
>> Also we want to put a L1 constraint as set of linear equations when
>> solving for ALS...
>>
>> I will create the JIRA...as I see it, this will evolve to a generic
>> constraint solver for machine learning problems that has a QP
>> structure....ALS is one example....another example is kernel SVMs...
>>
>> I did not know that lgpl solver can be added to the classpath....if it
>> can be then definitely we should add these in ALS.scala...
>>
>> Thanks.
>> Deb
>>
>>
>>
>> On Thu, Jun 5, 2014 at 11:31 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>
>>> I don't quite understand why putting linear constraints can promote
>>> orthogonality. For the interfaces, if the subproblem is determined by
>>> Y^T Y and Y^T b for each iteration, then the least squares solver, the
>>> non-negative least squares solver, or your convex solver is simply a
>>> function
>>>
>>> (A, b) -> x.
>>>
>>> You can define it as an interface, and make the solver pluggable by
>>> adding a setter to ALS. If you want to use your lgpl solver, just
>>> include it in the classpath. Creating two separate files still seems
>>> unnecessary to me. Could you create a JIRA and we can move our
>>> discussion there? Thanks!
>>>
>>> Best,
>>> Xiangrui
>>>
>>> On Thu, Jun 5, 2014 at 7:20 PM, Debasish Das <debasish.das83@gmail.com>
>>> wrote:
>>> > Hi Xiangrui,
>>> >
>>> > For orthogonality properties in the factors we need a constraint solver
>>> > other than the usuals (l1, upper and lower bounds, l2 etc)
>>> >
>>> > The interface of constraint solver is standard and I can add it in
>>> mllib
>>> > optimization....
>>> >
>>> > But I am not sure how will I call the gpl licensed ipm solver from
>>> > mllib....assume the solver interface is as follows:
>>> >
>>> > Qpsolver (densematrix h, array [double] f, int linearEquality, int
>>> > linearInequality, bool lb, bool ub)
>>> >
>>> > And then I have functions to update equalities, inequalities, bounds
>>> etc
>>> > followed by the run which generates the solution....
>>> >
>>> > For l1 constraints I have to use epigraph formulation which needs a
>>> > variable transformation before the solve....
>>> >
>>> > I was thinking that for the problems that does not need constraints
>>> people
>>> > will use ALS.scala and ConstrainedALS.scala will have the constrained
>>> > formulations....
>>> >
>>> > I can point you to the code once it is ready and then you can guide me
>>> how
>>> > to refactor it to mllib als ?
>>> >
>>> > Thanks.
>>> > Deb
>>> > Hi Deb,
>>> >
>>> > Why do you want to make those methods public? If you only need to
>>> > replace the solver for subproblems. You can try to make the solver
>>> > pluggable. Now it supports least squares and non-negative least
>>> > squares. You can define an interface for the subproblem solvers and
>>> > maintain the IPM solver at your own code base, if the only information
>>> > you need is Y^T Y and Y^T b.
>>> >
>>> > Btw, just curious, what is the use case for quadratic constraints?
>>> >
>>> > Best,
>>> > Xiangrui
>>> >
>>> > On Thu, Jun 5, 2014 at 3:38 PM, Debasish Das <debasish.das83@gmail.com
>>> >
>>> > wrote:
>>> >> Hi,
>>> >>
>>> >> We are adding a constrained ALS solver in Spark to solve matrix
>>> >> factorization use-cases which needs additional constraints (bounds,
>>> >> equality, inequality, quadratic constraints)
>>> >>
>>> >> We are using a native version of a primal dual SOCP solver due to its
>>> > small
>>> >> memory footprint and sparse ccs matrix computation it uses...The
>>> solver
>>> >> depends on AMD and LDL packages from Timothy Davis for sparse ccs
>>> matrix
>>> >> algebra (released under lgpl)...
>>> >>
>>> >> Due to GPL dependencies, it won't be possible to release the code as
>>> > Apache
>>> >> license for now...If we get good results on our use-cases, we will
>>> plan to
>>> >> write a version in breeze/modify joptimizer for sparse ccs
>>> operations...
>>> >>
>>> >> I derived ConstrainedALS from Spark mllib ALS and I am comparing the
>>> >> performance with default ALS and non-negative ALS as baseline. Plan
>>> is to
>>> >> release the code as GPL license for community review...I have kept the
>>> >> package structure as org.apache.spark.mllib.recommendation
>>> >>
>>> >> There are some private functions defined in ALS, which I would like to
>>> >> reuse....Is it possible to take the private out from the following
>>> >> functions:
>>> >>
>>> >> 1. makeLinkRDDs
>>> >> 2. makeInLinkBlock
>>> >> 3. makeOutLinkBlock
>>> >> 4. randomFactor
>>> >> 5. unblockFactors
>>> >>
>>> >> I don't want to copy any code.... I can ask for a PR to make these
>>> >> changes...
>>> >>
>>> >> Thanks.
>>> >> Deb
>>>
>>
>>
>

--001a113a9238ba6ad504fb8774c8--

From dev-return-8011-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 11 07:21:57 2014
Return-Path: <dev-return-8011-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D5AF91123A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Jun 2014 07:21:57 +0000 (UTC)
Received: (qmail 46388 invoked by uid 500); 11 Jun 2014 07:21:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46321 invoked by uid 500); 11 Jun 2014 07:21:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46310 invoked by uid 99); 11 Jun 2014 07:21:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 07:21:56 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 74.125.82.49 as permitted sender)
Received: from [74.125.82.49] (HELO mail-wg0-f49.google.com) (74.125.82.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 07:21:53 +0000
Received: by mail-wg0-f49.google.com with SMTP id y10so2611004wgg.20
        for <dev@spark.apache.org>; Wed, 11 Jun 2014 00:21:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=sPUYzMUhNUfcM1QO+SkExAkrZmXFP+6oFVS07WPNmdc=;
        b=chxt1Wpn30lBg+7Dm4hE1b4cl2/x/F7kq4yVerW5XO3Q3lX0CgKjQ3ZCVCGOl4VFuF
         /SnVupUUwwN0fBYQ/FeXIgo0niUI0nUqAOVf4uSxWtM3TJAWvbMK77JZ5sXT4UR43S2Z
         IvhnpwOpN3PmzHvpYj+u29oNeVqNsyfQJvtcQ/6Ntlj9MBXBNCgSZp4QZ8phzQSz64Zp
         JE+8kvilfkfIKe+9S1T8uLMpXpqEmEjVGHvYb82N1KTawF/hFGgk7dTszywo4krIj2JJ
         A8p04rApeS7wL5HanFtX3lvy4cLXLX990F0B7yN4UPH/jLDRGgTDlzOsTSFXBSckKAuw
         RM2w==
MIME-Version: 1.0
X-Received: by 10.180.106.1 with SMTP id gq1mr36531575wib.45.1402471289319;
 Wed, 11 Jun 2014 00:21:29 -0700 (PDT)
Received: by 10.194.169.234 with HTTP; Wed, 11 Jun 2014 00:21:29 -0700 (PDT)
In-Reply-To: <CA+B-+fyO10rcr5bXUaxQOodotmy7njwJujzukzj-ZU8zT-oAuA@mail.gmail.com>
References: <CA+B-+fw8EkK1ECg=GPACiJUCMRS6C4vVmQC9dwAfpxvcoPDZtA@mail.gmail.com>
	<CAJgQjQ9y_B1LATjt6c1-PmmytBzf2R1XyFVhMayVK2Hay5stMQ@mail.gmail.com>
	<CA+B-+fxE2rMOf_uURn7OtF_HXpjgZFSRZWD4QS=ZDyyvT-HTAQ@mail.gmail.com>
	<CAJgQjQ9UWjURVD2U0uTUUc2DHH5-_VXQ=uH8mFYZvjLgCLSr9w@mail.gmail.com>
	<CA+B-+fx64WwiSK_ymuvEXMnw2sd+F5fCL3-mQiJbexqyJgzbww@mail.gmail.com>
	<CA+B-+fxircx6nrzHnhoMjDRzGFnY+R7xvjwU49GjkfT39vVs8A@mail.gmail.com>
	<CA+B-+fyO10rcr5bXUaxQOodotmy7njwJujzukzj-ZU8zT-oAuA@mail.gmail.com>
Date: Wed, 11 Jun 2014 00:21:29 -0700
Message-ID: <CAJgQjQ-b+me5O_ZRqty_OniGFLPai0AFTu5mnZrhGYG9s-ekbg@mail.gmail.com>
Subject: Re: Constraint Solver for Spark
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

For explicit feedback, ALS uses only observed ratings for computation.
So XtXs are not the same. -Xiangrui

On Tue, Jun 10, 2014 at 8:58 PM, Debasish Das <debasish.das83@gmail.com> wrote:
> Sorry last one went out by mistake:
>
> Is not for users (0 to numUsers), fullXtX is same ? In the ALS formulation
> this is W^TW or H^TH which should be same for all the users ? Why we are
> reading userXtX(index) and adding it to fullXtX in the loop over all
> numUsers ?
>
> // Solve the least-squares problem for each user and return the new feature
> vectors
>
>     Array.range(0, numUsers).map { index =>
>
>       // Compute the full XtX matrix from the lower-triangular part we got
> above
>
>       fillFullMatrix(userXtX(index), fullXtX)
>
>       // Add regularization
>
>       var i = 0
>
>       while (i < rank) {
>
>         fullXtX.data(i * rank + i) += lambda
>
>         i += 1
>
>       }
>
>       // Solve the resulting matrix, which is symmetric and
> positive-definite
>
>       algo match {
>
>         case ALSAlgo.Implicit =>
> Solve.solvePositive(fullXtX.addi(YtY.get.value),
> userXy(index)).data
>
>         case ALSAlgo.Explicit => Solve.solvePositive(fullXtX, userXy
> (index)).data
>
>       }
>
>     }
>
>
> On Tue, Jun 10, 2014 at 8:56 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>
>> Hi,
>>
>> I am bit confused wiht the code here:
>>
>> // Solve the least-squares problem for each user and return the new
>> feature vectors
>>
>>     Array.range(0, numUsers).map { index =>
>>
>>       // Compute the full XtX matrix from the lower-triangular part we
>> got above
>>
>>       fillFullMatrix(userXtX(index), fullXtX)
>>
>>       // Add regularization
>>
>>       var i = 0
>>
>>       while (i < rank) {
>>
>>         fullXtX.data(i * rank + i) += lambda
>>
>>         i += 1
>>
>>       }
>>
>>       // Solve the resulting matrix, which is symmetric and
>> positive-definite
>>
>>       algo match {
>>
>>         case ALSAlgo.Implicit => Solve.solvePositive(fullXtX.addi(YtY.get.value),
>> userXy(index)).data
>>
>>         case ALSAlgo.Explicit => Solve.solvePositive(fullXtX, userXy
>> (index)).data
>>
>>       }
>>
>>     }
>>
>>
>> On Fri, Jun 6, 2014 at 10:42 AM, Debasish Das <debasish.das83@gmail.com>
>> wrote:
>>
>>> Hi Xiangrui,
>>>
>>> It's not the linear constraint, It is quadratic inequality with slack,
>>> first order taylor approximation of off diagonal cross terms and a cyclic
>>> coordinate descent, which we think will yield orthogonality....It's still
>>> under works...
>>>
>>> Also we want to put a L1 constraint as set of linear equations when
>>> solving for ALS...
>>>
>>> I will create the JIRA...as I see it, this will evolve to a generic
>>> constraint solver for machine learning problems that has a QP
>>> structure....ALS is one example....another example is kernel SVMs...
>>>
>>> I did not know that lgpl solver can be added to the classpath....if it
>>> can be then definitely we should add these in ALS.scala...
>>>
>>> Thanks.
>>> Deb
>>>
>>>
>>>
>>> On Thu, Jun 5, 2014 at 11:31 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>>
>>>> I don't quite understand why putting linear constraints can promote
>>>> orthogonality. For the interfaces, if the subproblem is determined by
>>>> Y^T Y and Y^T b for each iteration, then the least squares solver, the
>>>> non-negative least squares solver, or your convex solver is simply a
>>>> function
>>>>
>>>> (A, b) -> x.
>>>>
>>>> You can define it as an interface, and make the solver pluggable by
>>>> adding a setter to ALS. If you want to use your lgpl solver, just
>>>> include it in the classpath. Creating two separate files still seems
>>>> unnecessary to me. Could you create a JIRA and we can move our
>>>> discussion there? Thanks!
>>>>
>>>> Best,
>>>> Xiangrui
>>>>
>>>> On Thu, Jun 5, 2014 at 7:20 PM, Debasish Das <debasish.das83@gmail.com>
>>>> wrote:
>>>> > Hi Xiangrui,
>>>> >
>>>> > For orthogonality properties in the factors we need a constraint solver
>>>> > other than the usuals (l1, upper and lower bounds, l2 etc)
>>>> >
>>>> > The interface of constraint solver is standard and I can add it in
>>>> mllib
>>>> > optimization....
>>>> >
>>>> > But I am not sure how will I call the gpl licensed ipm solver from
>>>> > mllib....assume the solver interface is as follows:
>>>> >
>>>> > Qpsolver (densematrix h, array [double] f, int linearEquality, int
>>>> > linearInequality, bool lb, bool ub)
>>>> >
>>>> > And then I have functions to update equalities, inequalities, bounds
>>>> etc
>>>> > followed by the run which generates the solution....
>>>> >
>>>> > For l1 constraints I have to use epigraph formulation which needs a
>>>> > variable transformation before the solve....
>>>> >
>>>> > I was thinking that for the problems that does not need constraints
>>>> people
>>>> > will use ALS.scala and ConstrainedALS.scala will have the constrained
>>>> > formulations....
>>>> >
>>>> > I can point you to the code once it is ready and then you can guide me
>>>> how
>>>> > to refactor it to mllib als ?
>>>> >
>>>> > Thanks.
>>>> > Deb
>>>> > Hi Deb,
>>>> >
>>>> > Why do you want to make those methods public? If you only need to
>>>> > replace the solver for subproblems. You can try to make the solver
>>>> > pluggable. Now it supports least squares and non-negative least
>>>> > squares. You can define an interface for the subproblem solvers and
>>>> > maintain the IPM solver at your own code base, if the only information
>>>> > you need is Y^T Y and Y^T b.
>>>> >
>>>> > Btw, just curious, what is the use case for quadratic constraints?
>>>> >
>>>> > Best,
>>>> > Xiangrui
>>>> >
>>>> > On Thu, Jun 5, 2014 at 3:38 PM, Debasish Das <debasish.das83@gmail.com
>>>> >
>>>> > wrote:
>>>> >> Hi,
>>>> >>
>>>> >> We are adding a constrained ALS solver in Spark to solve matrix
>>>> >> factorization use-cases which needs additional constraints (bounds,
>>>> >> equality, inequality, quadratic constraints)
>>>> >>
>>>> >> We are using a native version of a primal dual SOCP solver due to its
>>>> > small
>>>> >> memory footprint and sparse ccs matrix computation it uses...The
>>>> solver
>>>> >> depends on AMD and LDL packages from Timothy Davis for sparse ccs
>>>> matrix
>>>> >> algebra (released under lgpl)...
>>>> >>
>>>> >> Due to GPL dependencies, it won't be possible to release the code as
>>>> > Apache
>>>> >> license for now...If we get good results on our use-cases, we will
>>>> plan to
>>>> >> write a version in breeze/modify joptimizer for sparse ccs
>>>> operations...
>>>> >>
>>>> >> I derived ConstrainedALS from Spark mllib ALS and I am comparing the
>>>> >> performance with default ALS and non-negative ALS as baseline. Plan
>>>> is to
>>>> >> release the code as GPL license for community review...I have kept the
>>>> >> package structure as org.apache.spark.mllib.recommendation
>>>> >>
>>>> >> There are some private functions defined in ALS, which I would like to
>>>> >> reuse....Is it possible to take the private out from the following
>>>> >> functions:
>>>> >>
>>>> >> 1. makeLinkRDDs
>>>> >> 2. makeInLinkBlock
>>>> >> 3. makeOutLinkBlock
>>>> >> 4. randomFactor
>>>> >> 5. unblockFactors
>>>> >>
>>>> >> I don't want to copy any code.... I can ask for a PR to make these
>>>> >> changes...
>>>> >>
>>>> >> Thanks.
>>>> >> Deb
>>>>
>>>
>>>
>>

From dev-return-8012-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 11 09:20:36 2014
Return-Path: <dev-return-8012-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B6640114C9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Jun 2014 09:20:36 +0000 (UTC)
Received: (qmail 51338 invoked by uid 500); 11 Jun 2014 09:20:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51286 invoked by uid 500); 11 Jun 2014 09:20:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51274 invoked by uid 99); 11 Jun 2014 09:20:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 09:20:36 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.43 as permitted sender)
Received: from [209.85.216.43] (HELO mail-qa0-f43.google.com) (209.85.216.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 09:20:30 +0000
Received: by mail-qa0-f43.google.com with SMTP id k15so1964925qaq.16
        for <dev@spark.apache.org>; Wed, 11 Jun 2014 02:20:10 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=n7LLnk1W/cG0O23rS4s+gWwCpKjhisn04wMCVyron2o=;
        b=iIwXZhvwb2Y/PPIS3VHkqDWSH0V3ygg8TcDb6wL1bb79WX1rO2N6DJQQZRclecuMB0
         sJsEP5jyEn3A9U0mzGW4ZFTJ8ulPlqpEI1A9WloB4CTnWp66BgfapBsIi4n9gRsVhRzV
         pKQ39NgqGjdXl09kQkTqtOYGqyRpuo4jdOjtF3Nf9uPWWp+sqrI2dyuTngrZ0Vwrv8Wd
         VSBjBicdJ3h6aY5J5gtJhDtVzUEpDMvEOaayzUHk1eHDJ3ceUQWOvG1P0a1rvdiEYVhO
         A6GQrdUsFSCTgJoxTc1+meThoJozTL6EMvPgPwwfxZSnPbYqD+jzn5TlTs4iWrM2j/fY
         ZF1g==
MIME-Version: 1.0
X-Received: by 10.140.105.119 with SMTP id b110mr47224185qgf.28.1402478409977;
 Wed, 11 Jun 2014 02:20:09 -0700 (PDT)
Received: by 10.140.82.164 with HTTP; Wed, 11 Jun 2014 02:20:09 -0700 (PDT)
In-Reply-To: <CAJgQjQ-b+me5O_ZRqty_OniGFLPai0AFTu5mnZrhGYG9s-ekbg@mail.gmail.com>
References: <CA+B-+fw8EkK1ECg=GPACiJUCMRS6C4vVmQC9dwAfpxvcoPDZtA@mail.gmail.com>
	<CAJgQjQ9y_B1LATjt6c1-PmmytBzf2R1XyFVhMayVK2Hay5stMQ@mail.gmail.com>
	<CA+B-+fxE2rMOf_uURn7OtF_HXpjgZFSRZWD4QS=ZDyyvT-HTAQ@mail.gmail.com>
	<CAJgQjQ9UWjURVD2U0uTUUc2DHH5-_VXQ=uH8mFYZvjLgCLSr9w@mail.gmail.com>
	<CA+B-+fx64WwiSK_ymuvEXMnw2sd+F5fCL3-mQiJbexqyJgzbww@mail.gmail.com>
	<CA+B-+fxircx6nrzHnhoMjDRzGFnY+R7xvjwU49GjkfT39vVs8A@mail.gmail.com>
	<CA+B-+fyO10rcr5bXUaxQOodotmy7njwJujzukzj-ZU8zT-oAuA@mail.gmail.com>
	<CAJgQjQ-b+me5O_ZRqty_OniGFLPai0AFTu5mnZrhGYG9s-ekbg@mail.gmail.com>
Date: Wed, 11 Jun 2014 02:20:09 -0700
Message-ID: <CA+B-+fyx_aACX5igiY+ibmk6y1OOCY0QmoRO7qpo2SY-+FqYcw@mail.gmail.com>
Subject: Re: Constraint Solver for Spark
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113944026e96f304fb8bf304
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113944026e96f304fb8bf304
Content-Type: text/plain; charset=UTF-8

I got it...ALS formulation is solving the matrix completion problem....

To convert the problem to matrix factorization or take user feedback
(missing entries means the user hate the site ?), we should put 0 to the
missing entries (or may be -1)...in that case we have to use computeYtY and
accumulate over users in each block to generate full gram matrix...and
after that while computing userXy(index) we have to be careful in putting
0/-1 for rest of the features...

Is implicit feedback trying to do something like this ?

Basically I am trying to see if it is possible to cache the gram matrix and
it's cholesky factorization, and then call the QpSolver multiple times with
updated gradient term...I am expecting better runtimes than dposv when
ranks are high...

But seems like that's not possible without a broadcast step which might
kill all the runtime gain...


On Wed, Jun 11, 2014 at 12:21 AM, Xiangrui Meng <mengxr@gmail.com> wrote:

> For explicit feedback, ALS uses only observed ratings for computation.
> So XtXs are not the same. -Xiangrui
>
> On Tue, Jun 10, 2014 at 8:58 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
> > Sorry last one went out by mistake:
> >
> > Is not for users (0 to numUsers), fullXtX is same ? In the ALS
> formulation
> > this is W^TW or H^TH which should be same for all the users ? Why we are
> > reading userXtX(index) and adding it to fullXtX in the loop over all
> > numUsers ?
> >
> > // Solve the least-squares problem for each user and return the new
> feature
> > vectors
> >
> >     Array.range(0, numUsers).map { index =>
> >
> >       // Compute the full XtX matrix from the lower-triangular part we
> got
> > above
> >
> >       fillFullMatrix(userXtX(index), fullXtX)
> >
> >       // Add regularization
> >
> >       var i = 0
> >
> >       while (i < rank) {
> >
> >         fullXtX.data(i * rank + i) += lambda
> >
> >         i += 1
> >
> >       }
> >
> >       // Solve the resulting matrix, which is symmetric and
> > positive-definite
> >
> >       algo match {
> >
> >         case ALSAlgo.Implicit =>
> > Solve.solvePositive(fullXtX.addi(YtY.get.value),
> > userXy(index)).data
> >
> >         case ALSAlgo.Explicit => Solve.solvePositive(fullXtX, userXy
> > (index)).data
> >
> >       }
> >
> >     }
> >
> >
> > On Tue, Jun 10, 2014 at 8:56 PM, Debasish Das <debasish.das83@gmail.com>
> > wrote:
> >
> >> Hi,
> >>
> >> I am bit confused wiht the code here:
> >>
> >> // Solve the least-squares problem for each user and return the new
> >> feature vectors
> >>
> >>     Array.range(0, numUsers).map { index =>
> >>
> >>       // Compute the full XtX matrix from the lower-triangular part we
> >> got above
> >>
> >>       fillFullMatrix(userXtX(index), fullXtX)
> >>
> >>       // Add regularization
> >>
> >>       var i = 0
> >>
> >>       while (i < rank) {
> >>
> >>         fullXtX.data(i * rank + i) += lambda
> >>
> >>         i += 1
> >>
> >>       }
> >>
> >>       // Solve the resulting matrix, which is symmetric and
> >> positive-definite
> >>
> >>       algo match {
> >>
> >>         case ALSAlgo.Implicit =>
> Solve.solvePositive(fullXtX.addi(YtY.get.value),
> >> userXy(index)).data
> >>
> >>         case ALSAlgo.Explicit => Solve.solvePositive(fullXtX, userXy
> >> (index)).data
> >>
> >>       }
> >>
> >>     }
> >>
> >>
> >> On Fri, Jun 6, 2014 at 10:42 AM, Debasish Das <debasish.das83@gmail.com
> >
> >> wrote:
> >>
> >>> Hi Xiangrui,
> >>>
> >>> It's not the linear constraint, It is quadratic inequality with slack,
> >>> first order taylor approximation of off diagonal cross terms and a
> cyclic
> >>> coordinate descent, which we think will yield orthogonality....It's
> still
> >>> under works...
> >>>
> >>> Also we want to put a L1 constraint as set of linear equations when
> >>> solving for ALS...
> >>>
> >>> I will create the JIRA...as I see it, this will evolve to a generic
> >>> constraint solver for machine learning problems that has a QP
> >>> structure....ALS is one example....another example is kernel SVMs...
> >>>
> >>> I did not know that lgpl solver can be added to the classpath....if it
> >>> can be then definitely we should add these in ALS.scala...
> >>>
> >>> Thanks.
> >>> Deb
> >>>
> >>>
> >>>
> >>> On Thu, Jun 5, 2014 at 11:31 PM, Xiangrui Meng <mengxr@gmail.com>
> wrote:
> >>>
> >>>> I don't quite understand why putting linear constraints can promote
> >>>> orthogonality. For the interfaces, if the subproblem is determined by
> >>>> Y^T Y and Y^T b for each iteration, then the least squares solver, the
> >>>> non-negative least squares solver, or your convex solver is simply a
> >>>> function
> >>>>
> >>>> (A, b) -> x.
> >>>>
> >>>> You can define it as an interface, and make the solver pluggable by
> >>>> adding a setter to ALS. If you want to use your lgpl solver, just
> >>>> include it in the classpath. Creating two separate files still seems
> >>>> unnecessary to me. Could you create a JIRA and we can move our
> >>>> discussion there? Thanks!
> >>>>
> >>>> Best,
> >>>> Xiangrui
> >>>>
> >>>> On Thu, Jun 5, 2014 at 7:20 PM, Debasish Das <
> debasish.das83@gmail.com>
> >>>> wrote:
> >>>> > Hi Xiangrui,
> >>>> >
> >>>> > For orthogonality properties in the factors we need a constraint
> solver
> >>>> > other than the usuals (l1, upper and lower bounds, l2 etc)
> >>>> >
> >>>> > The interface of constraint solver is standard and I can add it in
> >>>> mllib
> >>>> > optimization....
> >>>> >
> >>>> > But I am not sure how will I call the gpl licensed ipm solver from
> >>>> > mllib....assume the solver interface is as follows:
> >>>> >
> >>>> > Qpsolver (densematrix h, array [double] f, int linearEquality, int
> >>>> > linearInequality, bool lb, bool ub)
> >>>> >
> >>>> > And then I have functions to update equalities, inequalities, bounds
> >>>> etc
> >>>> > followed by the run which generates the solution....
> >>>> >
> >>>> > For l1 constraints I have to use epigraph formulation which needs a
> >>>> > variable transformation before the solve....
> >>>> >
> >>>> > I was thinking that for the problems that does not need constraints
> >>>> people
> >>>> > will use ALS.scala and ConstrainedALS.scala will have the
> constrained
> >>>> > formulations....
> >>>> >
> >>>> > I can point you to the code once it is ready and then you can guide
> me
> >>>> how
> >>>> > to refactor it to mllib als ?
> >>>> >
> >>>> > Thanks.
> >>>> > Deb
> >>>> > Hi Deb,
> >>>> >
> >>>> > Why do you want to make those methods public? If you only need to
> >>>> > replace the solver for subproblems. You can try to make the solver
> >>>> > pluggable. Now it supports least squares and non-negative least
> >>>> > squares. You can define an interface for the subproblem solvers and
> >>>> > maintain the IPM solver at your own code base, if the only
> information
> >>>> > you need is Y^T Y and Y^T b.
> >>>> >
> >>>> > Btw, just curious, what is the use case for quadratic constraints?
> >>>> >
> >>>> > Best,
> >>>> > Xiangrui
> >>>> >
> >>>> > On Thu, Jun 5, 2014 at 3:38 PM, Debasish Das <
> debasish.das83@gmail.com
> >>>> >
> >>>> > wrote:
> >>>> >> Hi,
> >>>> >>
> >>>> >> We are adding a constrained ALS solver in Spark to solve matrix
> >>>> >> factorization use-cases which needs additional constraints (bounds,
> >>>> >> equality, inequality, quadratic constraints)
> >>>> >>
> >>>> >> We are using a native version of a primal dual SOCP solver due to
> its
> >>>> > small
> >>>> >> memory footprint and sparse ccs matrix computation it uses...The
> >>>> solver
> >>>> >> depends on AMD and LDL packages from Timothy Davis for sparse ccs
> >>>> matrix
> >>>> >> algebra (released under lgpl)...
> >>>> >>
> >>>> >> Due to GPL dependencies, it won't be possible to release the code
> as
> >>>> > Apache
> >>>> >> license for now...If we get good results on our use-cases, we will
> >>>> plan to
> >>>> >> write a version in breeze/modify joptimizer for sparse ccs
> >>>> operations...
> >>>> >>
> >>>> >> I derived ConstrainedALS from Spark mllib ALS and I am comparing
> the
> >>>> >> performance with default ALS and non-negative ALS as baseline. Plan
> >>>> is to
> >>>> >> release the code as GPL license for community review...I have kept
> the
> >>>> >> package structure as org.apache.spark.mllib.recommendation
> >>>> >>
> >>>> >> There are some private functions defined in ALS, which I would
> like to
> >>>> >> reuse....Is it possible to take the private out from the following
> >>>> >> functions:
> >>>> >>
> >>>> >> 1. makeLinkRDDs
> >>>> >> 2. makeInLinkBlock
> >>>> >> 3. makeOutLinkBlock
> >>>> >> 4. randomFactor
> >>>> >> 5. unblockFactors
> >>>> >>
> >>>> >> I don't want to copy any code.... I can ask for a PR to make these
> >>>> >> changes...
> >>>> >>
> >>>> >> Thanks.
> >>>> >> Deb
> >>>>
> >>>
> >>>
> >>
>

--001a113944026e96f304fb8bf304--

From dev-return-8013-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 11 13:51:55 2014
Return-Path: <dev-return-8013-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A9EE311BFD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Jun 2014 13:51:55 +0000 (UTC)
Received: (qmail 73783 invoked by uid 500); 11 Jun 2014 13:51:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73731 invoked by uid 500); 11 Jun 2014 13:51:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73720 invoked by uid 99); 11 Jun 2014 13:51:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 13:51:55 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of henry.yijieshen@gmail.com designates 209.85.160.54 as permitted sender)
Received: from [209.85.160.54] (HELO mail-pb0-f54.google.com) (209.85.160.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 13:51:49 +0000
Received: by mail-pb0-f54.google.com with SMTP id jt11so7488533pbb.13
        for <dev@spark.apache.org>; Wed, 11 Jun 2014 06:51:24 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=from:content-type:message-id:mime-version:subject:date:references
         :to:in-reply-to;
        bh=p7QNaEYxKSYo8FaqDIQliFdWlUIXBrcbOHsM1jxuFMc=;
        b=cy3fkqb9aE8iyovHGC++7xzwHcyFAdrPm9bT6OlTaX74gvscmHO1+5tvlJz2sAh2D8
         0OhQo2jpFlF/A+J9R//N3Yc9g9IBvQN3PcI7cxFXrRt1X+TJDVCtez1LwryJToA8Si+6
         7Besvqb0DSYn4ZDrf3oTXO+wy4pWAP9gHJDXq5ge7Jdwazp+dYZstkr69Up8XO7LPs4e
         ZhLDMMlGjmafQxH/Y+uBqFXZAkXewtCsNscf8MU5TZSiLecvbsNDth8bvT+tTwxwO5Ue
         IzfKcCzhHy6P5LKPkDJZUVG3HuIFUMC2CKV4rqyrpjvwehvzo3v6bAm5i3Ox4Y5gKWrx
         VnWg==
X-Received: by 10.68.103.165 with SMTP id fx5mr5386126pbb.118.1402494684371;
        Wed, 11 Jun 2014 06:51:24 -0700 (PDT)
Received: from [10.30.1.154] ([159.226.43.30])
        by mx.google.com with ESMTPSA id av2sm75226449pbc.16.2014.06.11.06.51.22
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 11 Jun 2014 06:51:23 -0700 (PDT)
From: Yijie Shen <henry.yijieshen@gmail.com>
Content-Type: multipart/alternative; boundary="Apple-Mail=_AF3BE7DE-8AFB-4F3D-9DA8-68D10E6B8C35"
Message-Id: <AF958CC3-2FFD-4A16-80A8-AA56DF239090@gmail.com>
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: Run ScalaTest inside Intellij IDEA 
Date: Wed, 11 Jun 2014 21:51:19 +0800
References: <811039BF-0610-4BAC-BFD7-41BBC981768C@gmail.com> <CABKvOWstGq-QGNHZnmvRptYcg5idh7LACOk1z3jn5bR=9L_Qrw@mail.gmail.com>
To: Qiuzhuang Lian <qiuzhuang.lian@gmail.com>,
 dev@spark.apache.org
In-Reply-To: <CABKvOWstGq-QGNHZnmvRptYcg5idh7LACOk1z3jn5bR=9L_Qrw@mail.gmail.com>
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_AF3BE7DE-8AFB-4F3D-9DA8-68D10E6B8C35
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=utf-8

Thx Qiuzhuang, the problems disappeared after I add assembly jar at the =
head of list dependencies in *.iml, but while running test in Spark =
SQL(SQLQuerySuite in sql-core), another two error occurs:

Error 1:=20
Error:scalac:=20
     while compiling: =
/Users/yijie/code/apache.spark.master/sql/core/src/main/scala/org/apache/s=
park/sql/test/TestSQLContext.scala
        during phase: jvm
     library version: version 2.10.4
    compiler version: version 2.10.4
  reconstructed args: -Xmax-classfile-name 120 -deprecation =
-P:genjavadoc:out=3D/Users/yijie/code/apache.spark.master/sql/core/target/=
java -feature -classpath =
/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/ant-ja=
vafx.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/l=
ib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/=
lib/javafx-doclet.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Co=
ntents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_5=
1.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk=
1.7.0_51.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachine=
s/jdk1.7.0_51.jdk/Contents/Home/lib/tools.jar:/Library/Java/JavaVirtualMac=
hines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/Jav=
aVirtualMachines/jdk1.7.0_51.jdk/Conte=E2=80=A6
=E2=80=A6
...
=
/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.j=
ar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib=
/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/j=
re/classes:/Users/yijie/code/apache.spark.master/lib_managed/jars/scala-li=
brary-2.10.4.jar =
-Xplugin:/Users/yijie/code/apache.spark.master/lib_managed/jars/genjavadoc=
-plugin_2.10.4-0.5.jar =
-Xplugin:/Users/yijie/code/apache.spark.master/lib_managed/jars/genjavadoc=
-plugin_2.10.4-0.5.jar
  last tree to typer: Literal(Constant(parquet.io.api.Converter))
              symbol: null
   symbol definition: null
                 tpe: Class(classOf[parquet.io.api.Converter])
       symbol owners:=20
      context owners: object TestSQLContext -> package test
=3D=3D Enclosing template or block =3D=3D
Template( // val <local TestSQLContext>: <notype> in object =
TestSQLContext, tree.tpe=3Dorg.apache.spark.sql.test.TestSQLContext.type
  "org.apache.spark.sql.SQLContext" // parents
  ValDef(
    private
    "_"
    <tpt>
    <empty>
  )
  // 2 statements
  DefDef( // private def readResolve(): Object in object TestSQLContext
    <method> private <synthetic>
    "readResolve"
    []
    List(Nil)
    <tpt> // tree.tpe=3DObject
    test.this."TestSQLContext" // object TestSQLContext in package test, =
tree.tpe=3Dorg.apache.spark.sql.test.TestSQLContext.type
  )
  DefDef( // def <init>(): org.apache.spark.sql.test.TestSQLContext.type =
in object TestSQLContext
    <method>
    "<init>"
    []
    List(Nil)
    <tpt> // tree.tpe=3Dorg.apache.spark.sql.test.TestSQLContext.type
    Block( // tree.tpe=3DUnit
      Apply( // def <init>(sparkContext: org.apache.spark.SparkContext): =
org.apache.spark.sql.SQLContext in class SQLContext, =
tree.tpe=3Dorg.apache.spark.sql.SQLContext
        TestSQLContext.super."<init>" // def <init>(sparkContext: =
org.apache.spark.SparkContext): org.apache.spark.sql.SQLContext in class =
SQLContext, tree.tpe=3D(sparkContext: =
org.apache.spark.SparkContext)org.apache.spark.sql.SQLContext
        Apply( // def <init>(master: String,appName: String,conf: =
org.apache.spark.SparkConf): org.apache.spark.SparkContext in class =
SparkContext, tree.tpe=3Dorg.apache.spark.SparkContext
          new org.apache.spark.SparkContext."<init>" // def =
<init>(master: String,appName: String,conf: org.apache.spark.SparkConf): =
org.apache.spark.SparkContext in class SparkContext, tree.tpe=3D(master: =
String, appName: String, conf: =
org.apache.spark.SparkConf)org.apache.spark.SparkContext
          // 3 arguments
          "local"
          "TestSQLContext"
          Apply( // def <init>(): org.apache.spark.SparkConf in class =
SparkConf, tree.tpe=3Dorg.apache.spark.SparkConf
            new org.apache.spark.SparkConf."<init>" // def <init>(): =
org.apache.spark.SparkConf in class SparkConf, =
tree.tpe=3D()org.apache.spark.SparkConf
            Nil
          )
        )
      )
      ()
    )
  )
)
=3D=3D Expanded type of tree =3D=3D
ConstantType(value =3D Constant(parquet.io.api.Converter))
uncaught exception during compilation: java.lang.AssertionError

Error 2:

Error:scalac: Error: assertion failed: List(object package$DebugNode, =
object package$DebugNode)
java.lang.AssertionError: assertion failed: List(object =
package$DebugNode, object package$DebugNode)
	at =
scala.reflect.internal.Symbols$Symbol.suchThat(Symbols.scala:1678)
	at =
scala.reflect.internal.Symbols$ClassSymbol.companionModule0(Symbols.scala:=
2988)
	at =
scala.reflect.internal.Symbols$ClassSymbol.companionModule(Symbols.scala:2=
991)
	at =
scala.tools.nsc.backend.jvm.GenASM$JPlainBuilder.genClass(GenASM.scala:137=
1)
	at =
scala.tools.nsc.backend.jvm.GenASM$AsmPhase.run(GenASM.scala:120)
	at =
scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
	at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
	at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
	at scala.tools.nsc.Global$Run.compile(Global.scala:1662)
	at xsbt.CachedCompiler0.run(CompilerInterface.scala:126)
	at xsbt.CachedCompiler0.run(CompilerInterface.scala:102)
	at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at =
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:=
57)
	at =
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorIm=
pl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at =
sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)
	at =
sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)
	at =
sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
	at =
org.jetbrains.jps.incremental.scala.local.IdeaIncrementalCompiler.compile(=
IdeaIncrementalCompiler.scala:28)
	at =
org.jetbrains.jps.incremental.scala.local.LocalServer.compile(LocalServer.=
scala:25)
	at =
org.jetbrains.jps.incremental.scala.remote.Main$.make(Main.scala:64)
	at =
org.jetbrains.jps.incremental.scala.remote.Main$.nailMain(Main.scala:22)
	at =
org.jetbrains.jps.incremental.scala.remote.Main.nailMain(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at =
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:=
57)
	at =
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorIm=
pl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.martiansoftware.nailgun.NGSession.run(NGSession.java:319)


On Jun 11, 2014, at 11:17 AM, Qiuzhuang Lian <qiuzhuang.lian@gmail.com> =
wrote:

> I also run into this problem when running examples in IDEA. The issue =
looks that it uses depends on too many jars and that the classpath seems =
to have length limit. So I import the assembly jar and put the head of =
the list dependent path and it works.
>=20
> Thanks,
> Qiuzhuang
>=20
>=20
> On Wed, Jun 11, 2014 at 10:39 AM, =E7=94=B3=E6=AF=85=E6=9D=B0 =
<henry.yijieshen@gmail.com> wrote:
> Hi All,
>=20
> I want to run ScalaTest Suite in IDEA directly, but it seems didn=E2=80=99=
t pass the make phase before test running.
> The problems are as follows:
>=20
> =
/Users/yijie/code/apache.spark.master/core/src/main/scala/org/apache/spark=
/executor/MesosExecutorBackend.scala
> Error:(44, 35) type mismatch;
>  found   : org.apache.mesos.protobuf.ByteString
>  required: com.google.protobuf.ByteString
>       .setData(ByteString.copyFrom(data))
>                                   ^
> =
/Users/yijie/code/apache.spark.master/core/src/main/scala/org/apache/spark=
/scheduler/cluster/mesos/MesosSchedulerBackend.scala
> Error:(119, 35) type mismatch;
>  found   : org.apache.mesos.protobuf.ByteString
>  required: com.google.protobuf.ByteString
>       .setData(ByteString.copyFrom(createExecArg()))
>                                   ^
> Error:(257, 35) type mismatch;
>  found   : org.apache.mesos.protobuf.ByteString
>  required: com.google.protobuf.ByteString
>       .setData(ByteString.copyFrom(task.serializedTask))
>                                   ^
>=20
> Before I run test in IDEA, I build spark through =E2=80=99sbt/sbt =
assembly=E2=80=99,
> import projects into IDEA after =E2=80=99sbt/sbt gen-idea=E2=80=99,
> and able to run test in Terminal =E2=80=99sbt/sbt test=E2=80=99
>=20
> Are there anything I leave out in order to run/debug testsuite inside =
IDEA?
>=20
> Best regards,
> Yijie
>=20


--Apple-Mail=_AF3BE7DE-8AFB-4F3D-9DA8-68D10E6B8C35--

From dev-return-8014-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 11 14:17:18 2014
Return-Path: <dev-return-8014-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 559F711CEF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Jun 2014 14:17:18 +0000 (UTC)
Received: (qmail 41693 invoked by uid 500); 11 Jun 2014 14:17:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41618 invoked by uid 500); 11 Jun 2014 14:17:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41607 invoked by uid 99); 11 Jun 2014 14:17:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 14:17:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of qiuzhuang.lian@gmail.com designates 209.85.128.169 as permitted sender)
Received: from [209.85.128.169] (HELO mail-ve0-f169.google.com) (209.85.128.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 14:17:10 +0000
Received: by mail-ve0-f169.google.com with SMTP id pa12so4802092veb.28
        for <dev@spark.apache.org>; Wed, 11 Jun 2014 07:16:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=S/MGaJFWiTIanCwGnj7EpA2mJrnthyPA0ffOOhUrwNQ=;
        b=cLCBbiwhl5uXgX9LFR9BnvU+dgGoeQw0vkqfcItj609esTV2gi7renOyjStX6dM89x
         WQ53mWFiB9sVC99N0kliXSJoF0sI5dlcm/9yx0Q4iue94/tEWYxrAlGmkTNAU4faUOPa
         oq3LxngyQBFl2QXkoGWqYG+IXccpW+neyZJRzzyNx7WlXkiqjx0WnrwCpStM/sPRVubK
         DfQKNyzF0KBx5GHCKjpxnbls3TGHwfCJaleNr/KbCgci8nbx+Velz5rWpgCbgVkJRGk5
         97Uu9dH8JIG6EIALZppkbqkcbQ/RkJzg8zQ+wT6re3VYnMHpb00kmZ0VLfXagKzvLdZY
         OQOA==
MIME-Version: 1.0
X-Received: by 10.58.141.168 with SMTP id rp8mr3286546veb.40.1402496209636;
 Wed, 11 Jun 2014 07:16:49 -0700 (PDT)
Received: by 10.220.196.11 with HTTP; Wed, 11 Jun 2014 07:16:49 -0700 (PDT)
In-Reply-To: <AF958CC3-2FFD-4A16-80A8-AA56DF239090@gmail.com>
References: <811039BF-0610-4BAC-BFD7-41BBC981768C@gmail.com>
	<CABKvOWstGq-QGNHZnmvRptYcg5idh7LACOk1z3jn5bR=9L_Qrw@mail.gmail.com>
	<AF958CC3-2FFD-4A16-80A8-AA56DF239090@gmail.com>
Date: Wed, 11 Jun 2014 22:16:49 +0800
Message-ID: <CABKvOWt6HXS+Sr8UwVyqK1QLvoJYydpNJD61jUH9uC4LrzMhuw@mail.gmail.com>
Subject: Re: Run ScalaTest inside Intellij IDEA
From: Qiuzhuang Lian <qiuzhuang.lian@gmail.com>
To: Yijie Shen <henry.yijieshen@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b6725e05fd14504fb9018f7
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6725e05fd14504fb9018f7
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I run into this issue too today via 'mvn install -DskipTests' command
today, then I issue a mvn clean and rebuild and it works.

Thanks,
Qiuzhuang


On Wed, Jun 11, 2014 at 9:51 PM, Yijie Shen <henry.yijieshen@gmail.com>
wrote:

> Thx Qiuzhuang, the problems disappeared after I add assembly jar at the
> head of list dependencies in *.iml, but while running test in Spark
> SQL(SQLQuerySuite in sql-core), another two error occurs:
>
> Error 1:
> Error:scalac:
>      while compiling:
> /Users/yijie/code/apache.spark.master/sql/core/src/main/scala/org/apache/=
spark/sql/test/TestSQLContext.scala
>         during phase: jvm
>      library version: version 2.10.4
>     compiler version: version 2.10.4
>   reconstructed args: -Xmax-classfile-name 120 -deprecation
> -P:genjavadoc:out=3D/Users/yijie/code/apache.spark.master/sql/core/target=
/java
> -feature -classpath
> /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/ant-j=
avafx.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/l=
ib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/l=
ib/javafx-doclet.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Cont=
ents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.j=
dk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.7.=
0_51.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk=
1.7.0_51.jdk/Contents/Home/lib/tools.jar:/Library/Java/JavaVirtualMachines/=
jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtua=
lMachines/jdk1.7.0_51.jdk/Conte=E2=80=A6
> =E2=80=A6
> ...
> /Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.=
jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib=
/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jr=
e/classes:/Users/yijie/code/apache.spark.master/lib_managed/jars/scala-libr=
ary-2.10.4.jar
> -Xplugin:/Users/yijie/code/apache.spark.master/lib_managed/jars/genjavado=
c-plugin_2.10.4-0.5.jar
> -Xplugin:/Users/yijie/code/apache.spark.master/lib_managed/jars/genjavado=
c-plugin_2.10.4-0.5.jar
>   last tree to typer: Literal(Constant(parquet.io.api.Converter))
>               symbol: null
>    symbol definition: null
>                  tpe: Class(classOf[parquet.io.api.Converter])
>        symbol owners:
>       context owners: object TestSQLContext -> package test
> =3D=3D Enclosing template or block =3D=3D
> Template( // val <local TestSQLContext>: <notype> in object
> TestSQLContext, tree.tpe=3Dorg.apache.spark.sql.test.TestSQLContext.type
>   "org.apache.spark.sql.SQLContext" // parents
>   ValDef(
>     private
>     "_"
>     <tpt>
>     <empty>
>   )
>   // 2 statements
>   DefDef( // private def readResolve(): Object in object TestSQLContext
>     <method> private <synthetic>
>     "readResolve"
>     []
>     List(Nil)
>     <tpt> // tree.tpe=3DObject
>     test.this."TestSQLContext" // object TestSQLContext in package test,
> tree.tpe=3Dorg.apache.spark.sql.test.TestSQLContext.type
>   )
>   DefDef( // def <init>(): org.apache.spark.sql.test.TestSQLContext.type
> in object TestSQLContext
>     <method>
>     "<init>"
>     []
>     List(Nil)
>     <tpt> // tree.tpe=3Dorg.apache.spark.sql.test.TestSQLContext.type
>     Block( // tree.tpe=3DUnit
>       Apply( // def <init>(sparkContext: org.apache.spark.SparkContext):
> org.apache.spark.sql.SQLContext in class SQLContext,
> tree.tpe=3Dorg.apache.spark.sql.SQLContext
>         TestSQLContext.super."<init>" // def <init>(sparkContext:
> org.apache.spark.SparkContext): org.apache.spark.sql.SQLContext in class
> SQLContext, tree.tpe=3D(sparkContext:
> org.apache.spark.SparkContext)org.apache.spark.sql.SQLContext
>         Apply( // def <init>(master: String,appName: String,conf:
> org.apache.spark.SparkConf): org.apache.spark.SparkContext in class
> SparkContext, tree.tpe=3Dorg.apache.spark.SparkContext
>           new org.apache.spark.SparkContext."<init>" // def <init>(master=
:
> String,appName: String,conf: org.apache.spark.SparkConf):
> org.apache.spark.SparkContext in class SparkContext, tree.tpe=3D(master:
> String, appName: String, conf:
> org.apache.spark.SparkConf)org.apache.spark.SparkContext
>           // 3 arguments
>           "local"
>           "TestSQLContext"
>           Apply( // def <init>(): org.apache.spark.SparkConf in class
> SparkConf, tree.tpe=3Dorg.apache.spark.SparkConf
>             new org.apache.spark.SparkConf."<init>" // def <init>():
> org.apache.spark.SparkConf in class SparkConf,
> tree.tpe=3D()org.apache.spark.SparkConf
>             Nil
>           )
>         )
>       )
>       ()
>     )
>   )
> )
> =3D=3D Expanded type of tree =3D=3D
> ConstantType(value =3D Constant(parquet.io.api.Converter))
> uncaught exception during compilation: java.lang.AssertionError
>
> Error 2:
>
> Error:scalac: Error: assertion failed: List(object package$DebugNode,
> object package$DebugNode)
> java.lang.AssertionError: assertion failed: List(object package$DebugNode=
,
> object package$DebugNode)
> at scala.reflect.internal.Symbols$Symbol.suchThat(Symbols.scala:1678)
> at
> scala.reflect.internal.Symbols$ClassSymbol.companionModule0(Symbols.scala=
:2988)
> at
> scala.reflect.internal.Symbols$ClassSymbol.companionModule(Symbols.scala:=
2991)
> at
> scala.tools.nsc.backend.jvm.GenASM$JPlainBuilder.genClass(GenASM.scala:13=
71)
> at scala.tools.nsc.backend.jvm.GenASM$AsmPhase.run(GenASM.scala:120)
> at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
> at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
> at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
> at scala.tools.nsc.Global$Run.compile(Global.scala:1662)
> at xsbt.CachedCompiler0.run(CompilerInterface.scala:126)
> at xsbt.CachedCompiler0.run(CompilerInterface.scala:102)
> at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
> at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
> at java.lang.reflect.Method.invoke(Method.java:606)
> at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)
> at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)
> at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
> at
> org.jetbrains.jps.incremental.scala.local.IdeaIncrementalCompiler.compile=
(IdeaIncrementalCompiler.scala:28)
> at
> org.jetbrains.jps.incremental.scala.local.LocalServer.compile(LocalServer=
.scala:25)
> at org.jetbrains.jps.incremental.scala.remote.Main$.make(Main.scala:64)
> at org.jetbrains.jps.incremental.scala.remote.Main$.nailMain(Main.scala:2=
2)
> at org.jetbrains.jps.incremental.scala.remote.Main.nailMain(Main.scala)
> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
> at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
> at java.lang.reflect.Method.invoke(Method.java:606)
> at com.martiansoftware.nailgun.NGSession.run(NGSession.java:319)
>
>
> On Jun 11, 2014, at 11:17 AM, Qiuzhuang Lian <qiuzhuang.lian@gmail.com>
> wrote:
>
> I also run into this problem when running examples in IDEA. The issue
> looks that it uses depends on too many jars and that the classpath seems =
to
> have length limit. So I import the assembly jar and put the head of the
> list dependent path and it works.
>
> Thanks,
> Qiuzhuang
>
>
> On Wed, Jun 11, 2014 at 10:39 AM, =E7=94=B3=E6=AF=85=E6=9D=B0 <henry.yiji=
eshen@gmail.com> wrote:
>
>> Hi All,
>>
>> I want to run ScalaTest Suite in IDEA directly, but it seems didn=E2=80=
=99t pass
>> the make phase before test running.
>> The problems are as follows:
>>
>>
>> /Users/yijie/code/apache.spark.master/core/src/main/scala/org/apache/spa=
rk/executor/MesosExecutorBackend.scala
>> Error:(44, 35) type mismatch;
>>  found   : org.apache.mesos.protobuf.ByteString
>>  required: com.google.protobuf.ByteString
>>       .setData(ByteString.copyFrom(data))
>>                                   ^
>>
>> /Users/yijie/code/apache.spark.master/core/src/main/scala/org/apache/spa=
rk/scheduler/cluster/mesos/MesosSchedulerBackend.scala
>> Error:(119, 35) type mismatch;
>>  found   : org.apache.mesos.protobuf.ByteString
>>  required: com.google.protobuf.ByteString
>>       .setData(ByteString.copyFrom(createExecArg()))
>>                                   ^
>> Error:(257, 35) type mismatch;
>>  found   : org.apache.mesos.protobuf.ByteString
>>  required: com.google.protobuf.ByteString
>>       .setData(ByteString.copyFrom(task.serializedTask))
>>                                   ^
>>
>> Before I run test in IDEA, I build spark through =E2=80=99sbt/sbt assemb=
ly=E2=80=99,
>> import projects into IDEA after =E2=80=99sbt/sbt gen-idea=E2=80=99,
>> and able to run test in Terminal =E2=80=99sbt/sbt test=E2=80=99
>>
>> Are there anything I leave out in order to run/debug testsuite inside
>> IDEA?
>>
>> Best regards,
>> Yijie
>
>
>
>

--047d7b6725e05fd14504fb9018f7--

From dev-return-8015-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 11 15:45:59 2014
Return-Path: <dev-return-8015-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EADA211FF6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Jun 2014 15:45:59 +0000 (UTC)
Received: (qmail 50660 invoked by uid 500); 11 Jun 2014 15:45:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50486 invoked by uid 500); 11 Jun 2014 15:45:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50461 invoked by uid 99); 11 Jun 2014 15:45:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 15:45:58 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of shethsh@gmail.com designates 209.85.128.179 as permitted sender)
Received: from [209.85.128.179] (HELO mail-ve0-f179.google.com) (209.85.128.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 15:45:54 +0000
Received: by mail-ve0-f179.google.com with SMTP id oy12so11297183veb.38
        for <multiple recipients>; Wed, 11 Jun 2014 08:45:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=BpDfdMRnkxZCb6LktScyFxJ5QiaywANRheDYAkuZPGk=;
        b=E+SzPwZnN/4Kstb2+oWqxVbMQ4wsUDEyE+P47xNOgUWs4iKrHOa25TK8e2G1Oipuvd
         He19d2Hd7LlQ8d4Ih6bVltNsx9PCohwBNfOfG81ZxtK1szQixhFAJ3SV6CQe3JQ2YUqj
         kdD11LnCPHaaId4BoYIQdIpuxwAzqwz1Qkhkq0RP7PSwQ3AGgYUYskfrZxONPW0dC1mY
         TFInz10QlucAoY7OAt6FoLOXxGi+nmrmUO7zDLnXhWS5bDy6Eg//NLgA8LIxW4pBy7q9
         ZC3xPlMZIKmG8GbFlMesmZ5jEfugjtGgkGwZY06ZPsx14wuhI8hnTUHtmfc16Jq6wScg
         dYrg==
X-Received: by 10.52.190.162 with SMTP id gr2mr1002789vdc.71.1402501530331;
 Wed, 11 Jun 2014 08:45:30 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.155.227 with HTTP; Wed, 11 Jun 2014 08:44:50 -0700 (PDT)
From: SURAJ SHETH <shethsh@gmail.com>
Date: Wed, 11 Jun 2014 21:14:50 +0530
Message-ID: <CAL3khPvt2cSEr9vJ4jc=rwGMukWHfMK8aFZnn+PAFyJSKP7Ktg@mail.gmail.com>
Subject: MLLib : Decision Tree not getting built for 5 or more
 levels(maxDepth=5) and the one built for 3 levels is performing poorly
To: user@spark.apache.org, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0122ede48330ae04fb915554
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0122ede48330ae04fb915554
Content-Type: text/plain; charset=UTF-8

Hi,
I have been trying to build a Decision Tree using a dataset that I have.

Dataset Decription :

Train data size = 689,763

Test data size = 8,387,813

Each row in the dataset has 321 numerical features out of which 139th value
is the ground truth.

The number of positives in the dataset is low. Number of positives = 12028

There are absolutely no missing values in the dataset. This is ensured by
preprocessing the dataset.


The outcome against which we are building the tree is a binary variable
taking values 0 OR 1.

Due to a few reasons, I am building a Regression Tree and not a
Classification Tree.


When we have 3 levels(maxDepth = 3), we get the tree immediately(a few
minutes), but it is performing poorly. When I computed the correlation
coefficient between the ground truth scores and the scores obtained by the
tree, I get a correlation coefficient of 0.013140 which is very low.


Even looking at individual predictions manually, it is seen that the
predictions are almost same at around 0.07 to 0.09 irrespective of whether
the particular row is positive or negative in the ground truth.


When the maxDepth is set to 5, it doesn't complete building the tree even
after several hours.


When I include the ground truth in the train data, it builds the tree in a
very small amount of time and even the predictions are correct, accuracy is
around 100%.(As Expected)


So, I have two queries :

1) Why is the performance so poor when we have maxDepth = 3 ?

2) Why isn't building a Regression Decision Tree feasible with maxDepth = 5
?


Here is the core part of the code I am using :

    val ssc = new SparkContext(sparkMaster, "Spark exp 001", sparkHome,
jars)
    val labelRDD = ssc.textFile(hdfsNN + "Path to data /training/part*, 12)
                     .map{st =>
                       val parts = st.split(",").map(_.toDouble)
                       LabeledPoint(parts(138), Vectors.dense((parts take
138) ++ (parts drop 139)))}
    print(labelRDD.first)

    val model = DecisionTree.train(labelRDD, Regression, Variance, 3)
    val parsedData = ssc.textFile(hdfsNN + "Path to data /testing/part*",
12)
                     .map{st =>
                       val parts = st.split(",").map(_.toDouble)
                       LabeledPoint(parts(138), Vectors.dense((parts take
138) ++ (parts drop 139)))}
    val labelAndPreds = parsedData.map { point =>
        val prediction = model.predict(point.features)
        (point.label, prediction)
    }
    labelAndPreds.saveAsTextFile(hdfsNN + "Output path /labels")

When I build a Random Forest for the same dataset using Mahout, it builds
the forest in less than 5 minutes and gives a good accuracy. The amount of
memory and other resources available to Spark and to Mahout are comparable.

Spark had a memory of 30GB * 3 workers = 90GB in total.

Thanks and Regards,
Suraj Sheth

--089e0122ede48330ae04fb915554--

From dev-return-8016-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 11 16:18:18 2014
Return-Path: <dev-return-8016-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 94D20101EF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Jun 2014 16:18:18 +0000 (UTC)
Received: (qmail 35398 invoked by uid 500); 11 Jun 2014 16:18:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35205 invoked by uid 500); 11 Jun 2014 16:18:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35185 invoked by uid 99); 11 Jun 2014 16:18:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 16:18:16 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of suren.hiraman@sociocast.com designates 209.85.128.182 as permitted sender)
Received: from [209.85.128.182] (HELO mail-ve0-f182.google.com) (209.85.128.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 16:18:12 +0000
Received: by mail-ve0-f182.google.com with SMTP id jx11so555872veb.41
        for <dev@spark.apache.org>; Wed, 11 Jun 2014 09:17:51 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=IlHmrE35X4QV94BMDoNA69WhHwXkebnYx0TiIfjPau8=;
        b=XIQKruw5QVNT/S0rL81lSye0ELtJsvAqTMvgoDQnfkzQEB2lTkeLNgZj20YidLHYie
         sEZKjRztgYDKIzOWqb9kwZ5FsHZL/TSeMhFPgQ2eNvXDxlLzhsexw8vkBjMOa4VHdXsx
         OOMPR4BKoqZ3TQN1Q7T4p0GlBpUEjSGIj/HR7tuX12jq4BAy8c9rcQ5puzopD8Twjnmx
         cwt2OFWcB4pA7ik3FjmnfDuF46vHkBjI8OV/qVhZ5YLURR8qh861aCmdPjjhGors2mOK
         FXp/ERLPepRlGJP6kxIAyaSqrQsbifGDWP2QiWJ4svJU+51MMuYHtLBPOfF5w0N13GmU
         8hRA==
X-Gm-Message-State: ALoCoQl8m2gQYDZcDurW9oOBKRn2zBbIDMhRO/xbfX8Z9LkZ7dR2kdbXeYwS6bF/4L7pWBeHVtHD
MIME-Version: 1.0
X-Received: by 10.58.29.16 with SMTP id f16mr40750755veh.23.1402503471639;
 Wed, 11 Jun 2014 09:17:51 -0700 (PDT)
Received: by 10.58.143.49 with HTTP; Wed, 11 Jun 2014 09:17:51 -0700 (PDT)
In-Reply-To: <CALWDz_uNNJMv03m0B8QMOxdOgRXEz_eVLf49bkXp_jXM+NsWcg@mail.gmail.com>
References: <CALWDz_uNNJMv03m0B8QMOxdOgRXEz_eVLf49bkXp_jXM+NsWcg@mail.gmail.com>
Date: Wed, 11 Jun 2014 12:17:51 -0400
Message-ID: <CALWDz_vjAnRg-cOjCEhn=sXducZRZuV84-TPZDoTqr3TXy0O9A@mail.gmail.com>
Subject: Re: Error During ReceivingConnection
From: Surendranauth Hiraman <suren.hiraman@velos.io>
To: Surendranauth Hiraman <suren.hiraman@velos.io>
Cc: user@spark.apache.org, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b6dd0843942a904fb91c9ac
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6dd0843942a904fb91c9ac
Content-Type: text/plain; charset=UTF-8

It looks like this was due to another executor on a different node closing
the connection on its side. I found the entries below in the remote side's
logs.

Can anyone comment on why one ConnectionManager would close its connection
to another node and what could be tuned to avoid this? It did not have any
errors on its side.


This is from the ConnectionManager on the side shutting down the
connection, not the ConnectionManager that had the "Connection Reset By
Peer".

14/06/10 18:51:14 INFO network.ConnectionManager: Removing
ReceivingConnection to ConnectionManagerId(172.16.25.125,45610)

14/06/10 18:51:14 INFO network.ConnectionManager: Removing
SendingConnection to ConnectionManagerId(172.16.25.125,45610)




On Wed, Jun 11, 2014 at 8:38 AM, Surendranauth Hiraman <
suren.hiraman@velos.io> wrote:

> I have a somewhat large job (10 GB input data but generates about 500 GB
> of data after many stages).
>
> Most tasks completed but a few stragglers on the same node/executor are
> still active (but doing nothing) after about 16 hours.
>
> At about 3 to 4 hours in, the tasks that are hanging have the following in
> the work logs.
>
> Any idea what config to tweak for this?
>
>
> 14/06/10 18:51:10 WARN network.ReceivingConnection: Error reading from
> connection to ConnectionManagerId(172.16.25.108,37693)
> java.io.IOException: Connection reset by peer
> at sun.nio.ch.FileDispatcher.read0(Native Method)
>  at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
> at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:251)
>  at sun.nio.ch.IOUtil.read(IOUtil.java:224)
> at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:254)
>  at
> org.apache.spark.network.ReceivingConnection.read(Connection.scala:534)
> at
> org.apache.spark.network.ConnectionManager$$anon$6.run(ConnectionManager.scala:175)
>  at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
> at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
>  at java.lang.Thread.run(Thread.java:679)
> 14/06/10 18:51:10 INFO network.ConnectionManager: Handling connection
> error on connection to ConnectionManagerId(172.16.25.108,37693)
> 14/06/10 18:51:10 INFO network.ConnectionManager: Removing
> ReceivingConnection to ConnectionManagerId(172.16.25.108,37693)
> 14/06/10 18:51:10 INFO network.ConnectionManager: Removing
> SendingConnection to ConnectionManagerId(172.16.25.108,37693)
> 14/06/10 18:51:10 INFO network.ConnectionManager: Removing
> ReceivingConnection to ConnectionManagerId(172.16.25.108,37693)
> 14/06/10 18:51:10 ERROR network.ConnectionManager: Corresponding
> SendingConnectionManagerId not found
> 14/06/10 18:51:10 INFO network.ConnectionManager: Removing
> ReceivingConnection to ConnectionManagerId(172.16.25.108,37693)
> 14/06/10 18:51:10 ERROR network.ConnectionManager: Corresponding
> SendingConnectionManagerId not found
> 14/06/10 18:51:14 WARN network.ReceivingConnection: Error reading from
> connection to ConnectionManagerId(172.16.25.97,54918)
> java.io.IOException: Connection reset by peer
>  at sun.nio.ch.FileDispatcher.read0(Native Method)
> at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
>  at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:251)
> at sun.nio.ch.IOUtil.read(IOUtil.java:224)
>  at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:254)
> at org.apache.spark.network.ReceivingConnection.read(Connection.scala:534)
>  at
> org.apache.spark.network.ConnectionManager$$anon$6.run(ConnectionManager.scala:175)
> at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
>  at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
> at java.lang.Thread.run(Thread.java:679)
> 14/06/10 18:51:14 INFO network.ConnectionManager: Handling connection
> error on connection to ConnectionManagerId(172.16.25.97,54918)
> 14/06/10 18:51:14 INFO network.ConnectionManager: Removing
> ReceivingConnection to ConnectionManagerId(172.16.25.97,54918)
> 14/06/10 18:51:14 INFO network.ConnectionManager: Removing
> SendingConnection to ConnectionManagerId(172.16.25.97,54918)
> 14/06/10 18:51:14 INFO network.ConnectionManager: Removing
> ReceivingConnection to ConnectionManagerId(172.16.25.97,54918)
> 14/06/10 18:51:14 ERROR network.ConnectionManager: Corresponding
> SendingConnectionManagerId not found
> 14/06/10 18:51:14 INFO network.ConnectionManager: Removing
> ReceivingConnection to ConnectionManagerId(172.16.25.97,54918)
> 14/06/10 18:51:14 ERROR network.ConnectionManager: Corresponding
> SendingConnectionManagerId not found
>
> --
>
> SUREN HIRAMAN, VP TECHNOLOGY
> Velos
> Accelerating Machine Learning
>
> 440 NINTH AVENUE, 11TH FLOOR
> NEW YORK, NY 10001
> O: (917) 525-2466 ext. 105
> F: 646.349.4063
> E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
> W: www.velos.io
>
>


-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io

--047d7b6dd0843942a904fb91c9ac--

From dev-return-8017-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 11 21:56:51 2014
Return-Path: <dev-return-8017-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8A5C51112E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Jun 2014 21:56:51 +0000 (UTC)
Received: (qmail 24986 invoked by uid 500); 11 Jun 2014 21:56:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24894 invoked by uid 500); 11 Jun 2014 21:56:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24878 invoked by uid 99); 11 Jun 2014 21:56:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 21:56:48 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of suren.hiraman@sociocast.com designates 209.85.128.173 as permitted sender)
Received: from [209.85.128.173] (HELO mail-ve0-f173.google.com) (209.85.128.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 21:56:44 +0000
Received: by mail-ve0-f173.google.com with SMTP id db11so855127veb.32
        for <dev@spark.apache.org>; Wed, 11 Jun 2014 14:56:24 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=yIHq4CCmrHuRjwCPVYuUBILmVyX0DhjI2SIMWwbI95o=;
        b=dMIAOhNTfzgqSo/ic2/Oz6QqPICGvt2+1AlwMng3BlApM4O1BpBEqlbq1vWi+zohgz
         llV/F1of8eYF1+7nVKO7v/2M6q9QWh+7ctg/J8Qc/j2cZtEsckIXVJCdyP7dkGYtCYGE
         CPv+IoRE/bl9staPXD1JXf1mhAgyExDwESXpQqioOkNrZcxcVlqTxzvW1LDdWeQzqY7Y
         nWZfDi4gCRVMDXT5+eg57S377IX+Jr7VbvkdAeW5mrltgDePfwXDavqqQ4CiuHxjUOTP
         ceDd2xmC1kbCOsDW4mV86efvSconsJSxr4dfretRkKod2yhtYeHZIC+5KwQW+7nof83T
         aMew==
X-Gm-Message-State: ALoCoQkJHMRq8xSZ4Qd+XvgQoPMd5skeHqs1GR/Y8hd/rXYjlHZbOHOICf73iT3IQ13ALMddzPYW
MIME-Version: 1.0
X-Received: by 10.220.59.65 with SMTP id k1mr40682698vch.22.1402523783871;
 Wed, 11 Jun 2014 14:56:23 -0700 (PDT)
Received: by 10.58.143.49 with HTTP; Wed, 11 Jun 2014 14:56:23 -0700 (PDT)
Date: Wed, 11 Jun 2014 17:56:23 -0400
Message-ID: <CALWDz_vN4zkvWViFv=0k-s3tAZ72zENU+d3birjHPEgzhfKcvw@mail.gmail.com>
Subject: Compression with DISK_ONLY persistence
From: Surendranauth Hiraman <suren.hiraman@velos.io>
To: "dev@spark.apache.org" <dev@spark.apache.org>, user@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c29502ed51af04fb968379
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c29502ed51af04fb968379
Content-Type: text/plain; charset=UTF-8

Hi,

Will spark.rdd.compress=true enable compression when using DISK_ONLY
persistence?



SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io

--001a11c29502ed51af04fb968379--

From dev-return-8018-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 11 22:22:27 2014
Return-Path: <dev-return-8018-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5DEC211239
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Jun 2014 22:22:27 +0000 (UTC)
Received: (qmail 71968 invoked by uid 500); 11 Jun 2014 22:22:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71922 invoked by uid 500); 11 Jun 2014 22:22:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71911 invoked by uid 99); 11 Jun 2014 22:22:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 22:22:26 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.212.171 as permitted sender)
Received: from [209.85.212.171] (HELO mail-wi0-f171.google.com) (209.85.212.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 22:22:22 +0000
Received: by mail-wi0-f171.google.com with SMTP id n15so5970576wiw.16
        for <dev@spark.apache.org>; Wed, 11 Jun 2014 15:21:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=qm/aKfyx7Y1FT7Esgm5pV0O4xahHWDvFBGX8q0hQ7Q0=;
        b=cLCrseA8CzF2dynN2x2WxIGL0lF6VhvOEDgP+WwBZjXE/bYiuHdkZBoMzftVDXe+hS
         cAVRufCXVqkoCpfnzUVIkzjfRzetuo6U0yWA54AbmG3D1cf0YvcgFqhHbMqzoxigrV47
         z0EdlzzCHbILz5BC9PIjEUEHw+S2ZBdP7F9so9u4AwFD6CiYG9AI6s+CHEGn5+JyAC9R
         rxcdp+fBZNEHSuNeDvLr0TNfxDKcqqQANmNsQaSGN8JR5oW9jCBxhY9AF5E6lSFdY+ik
         e7CehUk+AFcgza3Kbv6yx1ueDtejiR9lpOdHK3snuMdLKewSBSzCN87CWrUuyIp6Xd/y
         Ro8w==
MIME-Version: 1.0
X-Received: by 10.180.212.77 with SMTP id ni13mr842070wic.5.1402525319124;
 Wed, 11 Jun 2014 15:21:59 -0700 (PDT)
Received: by 10.194.169.234 with HTTP; Wed, 11 Jun 2014 15:21:59 -0700 (PDT)
In-Reply-To: <CA+B-+fyx_aACX5igiY+ibmk6y1OOCY0QmoRO7qpo2SY-+FqYcw@mail.gmail.com>
References: <CA+B-+fw8EkK1ECg=GPACiJUCMRS6C4vVmQC9dwAfpxvcoPDZtA@mail.gmail.com>
	<CAJgQjQ9y_B1LATjt6c1-PmmytBzf2R1XyFVhMayVK2Hay5stMQ@mail.gmail.com>
	<CA+B-+fxE2rMOf_uURn7OtF_HXpjgZFSRZWD4QS=ZDyyvT-HTAQ@mail.gmail.com>
	<CAJgQjQ9UWjURVD2U0uTUUc2DHH5-_VXQ=uH8mFYZvjLgCLSr9w@mail.gmail.com>
	<CA+B-+fx64WwiSK_ymuvEXMnw2sd+F5fCL3-mQiJbexqyJgzbww@mail.gmail.com>
	<CA+B-+fxircx6nrzHnhoMjDRzGFnY+R7xvjwU49GjkfT39vVs8A@mail.gmail.com>
	<CA+B-+fyO10rcr5bXUaxQOodotmy7njwJujzukzj-ZU8zT-oAuA@mail.gmail.com>
	<CAJgQjQ-b+me5O_ZRqty_OniGFLPai0AFTu5mnZrhGYG9s-ekbg@mail.gmail.com>
	<CA+B-+fyx_aACX5igiY+ibmk6y1OOCY0QmoRO7qpo2SY-+FqYcw@mail.gmail.com>
Date: Wed, 11 Jun 2014 15:21:59 -0700
Message-ID: <CAJgQjQ-wvbaE+QJ2Mngzpy=g5i7=4ym59JVrAMtijwv4uTbfPw@mail.gmail.com>
Subject: Re: Constraint Solver for Spark
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

You idea is close to what implicit feedback does. You can check the
paper, which is short and concise. In the ALS setting, all subproblems
are independent in each iteration. This is part of the reason why ALS
is scalable. If you have some global constraints that make the
subproblems no longer decoupled, that would certainly affects
scalability. -Xiangrui

On Wed, Jun 11, 2014 at 2:20 AM, Debasish Das <debasish.das83@gmail.com> wrote:
> I got it...ALS formulation is solving the matrix completion problem....
>
> To convert the problem to matrix factorization or take user feedback
> (missing entries means the user hate the site ?), we should put 0 to the
> missing entries (or may be -1)...in that case we have to use computeYtY and
> accumulate over users in each block to generate full gram matrix...and
> after that while computing userXy(index) we have to be careful in putting
> 0/-1 for rest of the features...
>
> Is implicit feedback trying to do something like this ?
>
> Basically I am trying to see if it is possible to cache the gram matrix and
> it's cholesky factorization, and then call the QpSolver multiple times with
> updated gradient term...I am expecting better runtimes than dposv when
> ranks are high...
>
> But seems like that's not possible without a broadcast step which might
> kill all the runtime gain...
>
>
> On Wed, Jun 11, 2014 at 12:21 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
>> For explicit feedback, ALS uses only observed ratings for computation.
>> So XtXs are not the same. -Xiangrui
>>
>> On Tue, Jun 10, 2014 at 8:58 PM, Debasish Das <debasish.das83@gmail.com>
>> wrote:
>> > Sorry last one went out by mistake:
>> >
>> > Is not for users (0 to numUsers), fullXtX is same ? In the ALS
>> formulation
>> > this is W^TW or H^TH which should be same for all the users ? Why we are
>> > reading userXtX(index) and adding it to fullXtX in the loop over all
>> > numUsers ?
>> >
>> > // Solve the least-squares problem for each user and return the new
>> feature
>> > vectors
>> >
>> >     Array.range(0, numUsers).map { index =>
>> >
>> >       // Compute the full XtX matrix from the lower-triangular part we
>> got
>> > above
>> >
>> >       fillFullMatrix(userXtX(index), fullXtX)
>> >
>> >       // Add regularization
>> >
>> >       var i = 0
>> >
>> >       while (i < rank) {
>> >
>> >         fullXtX.data(i * rank + i) += lambda
>> >
>> >         i += 1
>> >
>> >       }
>> >
>> >       // Solve the resulting matrix, which is symmetric and
>> > positive-definite
>> >
>> >       algo match {
>> >
>> >         case ALSAlgo.Implicit =>
>> > Solve.solvePositive(fullXtX.addi(YtY.get.value),
>> > userXy(index)).data
>> >
>> >         case ALSAlgo.Explicit => Solve.solvePositive(fullXtX, userXy
>> > (index)).data
>> >
>> >       }
>> >
>> >     }
>> >
>> >
>> > On Tue, Jun 10, 2014 at 8:56 PM, Debasish Das <debasish.das83@gmail.com>
>> > wrote:
>> >
>> >> Hi,
>> >>
>> >> I am bit confused wiht the code here:
>> >>
>> >> // Solve the least-squares problem for each user and return the new
>> >> feature vectors
>> >>
>> >>     Array.range(0, numUsers).map { index =>
>> >>
>> >>       // Compute the full XtX matrix from the lower-triangular part we
>> >> got above
>> >>
>> >>       fillFullMatrix(userXtX(index), fullXtX)
>> >>
>> >>       // Add regularization
>> >>
>> >>       var i = 0
>> >>
>> >>       while (i < rank) {
>> >>
>> >>         fullXtX.data(i * rank + i) += lambda
>> >>
>> >>         i += 1
>> >>
>> >>       }
>> >>
>> >>       // Solve the resulting matrix, which is symmetric and
>> >> positive-definite
>> >>
>> >>       algo match {
>> >>
>> >>         case ALSAlgo.Implicit =>
>> Solve.solvePositive(fullXtX.addi(YtY.get.value),
>> >> userXy(index)).data
>> >>
>> >>         case ALSAlgo.Explicit => Solve.solvePositive(fullXtX, userXy
>> >> (index)).data
>> >>
>> >>       }
>> >>
>> >>     }
>> >>
>> >>
>> >> On Fri, Jun 6, 2014 at 10:42 AM, Debasish Das <debasish.das83@gmail.com
>> >
>> >> wrote:
>> >>
>> >>> Hi Xiangrui,
>> >>>
>> >>> It's not the linear constraint, It is quadratic inequality with slack,
>> >>> first order taylor approximation of off diagonal cross terms and a
>> cyclic
>> >>> coordinate descent, which we think will yield orthogonality....It's
>> still
>> >>> under works...
>> >>>
>> >>> Also we want to put a L1 constraint as set of linear equations when
>> >>> solving for ALS...
>> >>>
>> >>> I will create the JIRA...as I see it, this will evolve to a generic
>> >>> constraint solver for machine learning problems that has a QP
>> >>> structure....ALS is one example....another example is kernel SVMs...
>> >>>
>> >>> I did not know that lgpl solver can be added to the classpath....if it
>> >>> can be then definitely we should add these in ALS.scala...
>> >>>
>> >>> Thanks.
>> >>> Deb
>> >>>
>> >>>
>> >>>
>> >>> On Thu, Jun 5, 2014 at 11:31 PM, Xiangrui Meng <mengxr@gmail.com>
>> wrote:
>> >>>
>> >>>> I don't quite understand why putting linear constraints can promote
>> >>>> orthogonality. For the interfaces, if the subproblem is determined by
>> >>>> Y^T Y and Y^T b for each iteration, then the least squares solver, the
>> >>>> non-negative least squares solver, or your convex solver is simply a
>> >>>> function
>> >>>>
>> >>>> (A, b) -> x.
>> >>>>
>> >>>> You can define it as an interface, and make the solver pluggable by
>> >>>> adding a setter to ALS. If you want to use your lgpl solver, just
>> >>>> include it in the classpath. Creating two separate files still seems
>> >>>> unnecessary to me. Could you create a JIRA and we can move our
>> >>>> discussion there? Thanks!
>> >>>>
>> >>>> Best,
>> >>>> Xiangrui
>> >>>>
>> >>>> On Thu, Jun 5, 2014 at 7:20 PM, Debasish Das <
>> debasish.das83@gmail.com>
>> >>>> wrote:
>> >>>> > Hi Xiangrui,
>> >>>> >
>> >>>> > For orthogonality properties in the factors we need a constraint
>> solver
>> >>>> > other than the usuals (l1, upper and lower bounds, l2 etc)
>> >>>> >
>> >>>> > The interface of constraint solver is standard and I can add it in
>> >>>> mllib
>> >>>> > optimization....
>> >>>> >
>> >>>> > But I am not sure how will I call the gpl licensed ipm solver from
>> >>>> > mllib....assume the solver interface is as follows:
>> >>>> >
>> >>>> > Qpsolver (densematrix h, array [double] f, int linearEquality, int
>> >>>> > linearInequality, bool lb, bool ub)
>> >>>> >
>> >>>> > And then I have functions to update equalities, inequalities, bounds
>> >>>> etc
>> >>>> > followed by the run which generates the solution....
>> >>>> >
>> >>>> > For l1 constraints I have to use epigraph formulation which needs a
>> >>>> > variable transformation before the solve....
>> >>>> >
>> >>>> > I was thinking that for the problems that does not need constraints
>> >>>> people
>> >>>> > will use ALS.scala and ConstrainedALS.scala will have the
>> constrained
>> >>>> > formulations....
>> >>>> >
>> >>>> > I can point you to the code once it is ready and then you can guide
>> me
>> >>>> how
>> >>>> > to refactor it to mllib als ?
>> >>>> >
>> >>>> > Thanks.
>> >>>> > Deb
>> >>>> > Hi Deb,
>> >>>> >
>> >>>> > Why do you want to make those methods public? If you only need to
>> >>>> > replace the solver for subproblems. You can try to make the solver
>> >>>> > pluggable. Now it supports least squares and non-negative least
>> >>>> > squares. You can define an interface for the subproblem solvers and
>> >>>> > maintain the IPM solver at your own code base, if the only
>> information
>> >>>> > you need is Y^T Y and Y^T b.
>> >>>> >
>> >>>> > Btw, just curious, what is the use case for quadratic constraints?
>> >>>> >
>> >>>> > Best,
>> >>>> > Xiangrui
>> >>>> >
>> >>>> > On Thu, Jun 5, 2014 at 3:38 PM, Debasish Das <
>> debasish.das83@gmail.com
>> >>>> >
>> >>>> > wrote:
>> >>>> >> Hi,
>> >>>> >>
>> >>>> >> We are adding a constrained ALS solver in Spark to solve matrix
>> >>>> >> factorization use-cases which needs additional constraints (bounds,
>> >>>> >> equality, inequality, quadratic constraints)
>> >>>> >>
>> >>>> >> We are using a native version of a primal dual SOCP solver due to
>> its
>> >>>> > small
>> >>>> >> memory footprint and sparse ccs matrix computation it uses...The
>> >>>> solver
>> >>>> >> depends on AMD and LDL packages from Timothy Davis for sparse ccs
>> >>>> matrix
>> >>>> >> algebra (released under lgpl)...
>> >>>> >>
>> >>>> >> Due to GPL dependencies, it won't be possible to release the code
>> as
>> >>>> > Apache
>> >>>> >> license for now...If we get good results on our use-cases, we will
>> >>>> plan to
>> >>>> >> write a version in breeze/modify joptimizer for sparse ccs
>> >>>> operations...
>> >>>> >>
>> >>>> >> I derived ConstrainedALS from Spark mllib ALS and I am comparing
>> the
>> >>>> >> performance with default ALS and non-negative ALS as baseline. Plan
>> >>>> is to
>> >>>> >> release the code as GPL license for community review...I have kept
>> the
>> >>>> >> package structure as org.apache.spark.mllib.recommendation
>> >>>> >>
>> >>>> >> There are some private functions defined in ALS, which I would
>> like to
>> >>>> >> reuse....Is it possible to take the private out from the following
>> >>>> >> functions:
>> >>>> >>
>> >>>> >> 1. makeLinkRDDs
>> >>>> >> 2. makeInLinkBlock
>> >>>> >> 3. makeOutLinkBlock
>> >>>> >> 4. randomFactor
>> >>>> >> 5. unblockFactors
>> >>>> >>
>> >>>> >> I don't want to copy any code.... I can ask for a PR to make these
>> >>>> >> changes...
>> >>>> >>
>> >>>> >> Thanks.
>> >>>> >> Deb
>> >>>>
>> >>>
>> >>>
>> >>
>>

From dev-return-8019-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 11 22:47:57 2014
Return-Path: <dev-return-8019-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B68B81132C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Jun 2014 22:47:57 +0000 (UTC)
Received: (qmail 36049 invoked by uid 500); 11 Jun 2014 22:47:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35683 invoked by uid 500); 11 Jun 2014 22:47:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35663 invoked by uid 99); 11 Jun 2014 22:47:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 22:47:55 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.43 as permitted sender)
Received: from [209.85.160.43] (HELO mail-pb0-f43.google.com) (209.85.160.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Jun 2014 22:47:50 +0000
Received: by mail-pb0-f43.google.com with SMTP id up15so288591pbc.2
        for <multiple recipients>; Wed, 11 Jun 2014 15:47:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :message-id:references:to;
        bh=BUTAQURFKbS17a5yPF1cvOqKPZ9IT8mAyI9Q0Wuo53k=;
        b=vDeQUH+Ht2ocGeQiAR7aVjVFTcSL+/4qAJv2zx//nRsEjkgZFQoRt7cT2myEzzOUGO
         U6yfo3mKBintE0TquD8u3HxeN67kPW3C4viy4W4enEev/0yKxZ0ANc7zZQO/mn+13UVi
         XLCaYH7/gM9nGrQiskSt0mKfNlNmQNjZdVb0me7tXE44/Mis6E7IY2rs3xhMl8F2qGgE
         6QpKGt/ZJhs5jOFVJK2SRjOjnLdcb4q68oeXq+gna44hOzPmnXsPNIyH2YLpR2cOUaUj
         LVNpUtmjiE8kxP0N26DbBxq6Mjblwc8fz6f1uFkwUvugmuLdZ6KtSbbE3YEHm/kapWLm
         Wjbg==
X-Received: by 10.66.102.39 with SMTP id fl7mr16652169pab.43.1402526849937;
        Wed, 11 Jun 2014 15:47:29 -0700 (PDT)
Received: from [192.168.1.105] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id oa3sm76910692pbb.15.2014.06.11.15.47.26
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 11 Jun 2014 15:47:27 -0700 (PDT)
Content-Type: multipart/alternative; boundary="Apple-Mail=_20CA4321-CEFD-4043-8B06-5317FFB831BD"
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
Subject: Re: Compression with DISK_ONLY persistence
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CALWDz_vN4zkvWViFv=0k-s3tAZ72zENU+d3birjHPEgzhfKcvw@mail.gmail.com>
Date: Wed, 11 Jun 2014 15:47:24 -0700
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Message-Id: <BF0FF897-FFA3-4C8E-A5DA-5F432E72F0D7@gmail.com>
References: <CALWDz_vN4zkvWViFv=0k-s3tAZ72zENU+d3birjHPEgzhfKcvw@mail.gmail.com>
To: user@spark.apache.org
X-Mailer: Apple Mail (2.1878.2)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_20CA4321-CEFD-4043-8B06-5317FFB831BD
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=windows-1252

Yes, actually even if you don=92t set it to true, on-disk data is =
compressed. (This setting only affects serialized data in memory).

Matei

On Jun 11, 2014, at 2:56 PM, Surendranauth Hiraman =
<suren.hiraman@velos.io> wrote:

> Hi,
>=20
> Will spark.rdd.compress=3Dtrue enable compression when using DISK_ONLY =
persistence?=20
>=20
>=20
>                                                            =20
> SUREN HIRAMAN, VP TECHNOLOGY
> Velos
> Accelerating Machine Learning
>=20
> 440 NINTH AVENUE, 11TH FLOOR
> NEW YORK, NY 10001
> O: (917) 525-2466 ext. 105
> F: 646.349.4063
> E: suren.hiraman@velos.io
> W: www.velos.io
>=20


--Apple-Mail=_20CA4321-CEFD-4043-8B06-5317FFB831BD--

From dev-return-8020-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun 12 01:07:32 2014
Return-Path: <dev-return-8020-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6079411722
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 12 Jun 2014 01:07:32 +0000 (UTC)
Received: (qmail 59010 invoked by uid 500); 12 Jun 2014 01:07:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58951 invoked by uid 500); 12 Jun 2014 01:07:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58940 invoked by uid 99); 12 Jun 2014 01:07:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 12 Jun 2014 01:07:31 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of henry.yijieshen@gmail.com designates 209.85.192.170 as permitted sender)
Received: from [209.85.192.170] (HELO mail-pd0-f170.google.com) (209.85.192.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 12 Jun 2014 01:07:25 +0000
Received: by mail-pd0-f170.google.com with SMTP id g10so376881pdj.15
        for <dev@spark.apache.org>; Wed, 11 Jun 2014 18:07:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :message-id:references:to;
        bh=lHaVewU5+ToFmqGYTzK608V97IDE8ReT0RIRIQZWwXU=;
        b=UFd3tvzBpAcBYSfKcjjU988cfMbhHUl6ZDgFd8LhxjshLyi9mltXacyNyKBxFyepeD
         +5lWgUmkbapjJ4XP0Q8LAzmZvdkLwUJ1/zFA8Jvs5XcRDYSK+Z06Bjiuw6pwYIQSB322
         4kIK2YPJSmRGSB75wMyumNoYZap4jjKbO4KlFnbySyMLVSMnp7EvxuVDHp36CxHz6j2z
         +2/YkOmGXvpC6zQieImyePpSUPl0gKKGHzV+vH0YPlI8ThfIqgJ0oZl9TkMJpXP5sz7d
         yz3njRKGCthdbSFP/edJlG4kj5ORzOTYLO+/tiENgu8Dd3+n5sgg/89+t+lfk37TBldp
         5Mdw==
X-Received: by 10.66.65.204 with SMTP id z12mr17394750pas.60.1402535225032;
        Wed, 11 Jun 2014 18:07:05 -0700 (PDT)
Received: from [10.30.1.154] ([159.226.43.30])
        by mx.google.com with ESMTPSA id pn4sm77261498pbb.7.2014.06.11.18.07.03
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 11 Jun 2014 18:07:04 -0700 (PDT)
Content-Type: multipart/alternative; boundary="Apple-Mail=_EF43DAC4-DE51-468C-8493-E4AAEC563523"
Mime-Version: 1.0 (Mac OS X Mail 7.2 \(1874\))
Subject: Re: Run ScalaTest inside Intellij IDEA 
From: Yijie Shen <henry.yijieshen@gmail.com>
In-Reply-To: <CABKvOWt6HXS+Sr8UwVyqK1QLvoJYydpNJD61jUH9uC4LrzMhuw@mail.gmail.com>
Date: Thu, 12 Jun 2014 09:07:00 +0800
Cc: dev@spark.apache.org
Message-Id: <DFE1084A-3C58-44C2-9B7C-9161E76E5728@gmail.com>
References: <811039BF-0610-4BAC-BFD7-41BBC981768C@gmail.com> <CABKvOWstGq-QGNHZnmvRptYcg5idh7LACOk1z3jn5bR=9L_Qrw@mail.gmail.com> <AF958CC3-2FFD-4A16-80A8-AA56DF239090@gmail.com> <CABKvOWt6HXS+Sr8UwVyqK1QLvoJYydpNJD61jUH9uC4LrzMhuw@mail.gmail.com>
To: Qiuzhuang Lian <qiuzhuang.lian@gmail.com>
X-Mailer: Apple Mail (2.1874)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_EF43DAC4-DE51-468C-8493-E4AAEC563523
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=utf-8

I got a clean version of the master branch, and do the steps as follows:

export MAVEN_OPTS=3D"-Xmx2g -XX:MaxPermSize=3D512M =
-XX:ReservedCodeCacheSize=3D512m=E2=80=9D
mvn -U -Dhadoop.version=3D2.2.0 -DskipTests clean package

after these steps, I open the project in IDEA through pom.xml in the =
root folder, but while run the same test SQLQuerySuite in sql-core, the =
two errors above still occurs, any ideas?=20

On Jun 11, 2014, at 10:16 PM, Qiuzhuang Lian <qiuzhuang.lian@gmail.com> =
wrote:

> I run into this issue too today via 'mvn install -DskipTests' command =
today, then I issue a mvn clean and rebuild and it works.
>=20
> Thanks,
> Qiuzhuang
>=20
>=20
> On Wed, Jun 11, 2014 at 9:51 PM, Yijie Shen =
<henry.yijieshen@gmail.com> wrote:
> Thx Qiuzhuang, the problems disappeared after I add assembly jar at =
the head of list dependencies in *.iml, but while running test in Spark =
SQL(SQLQuerySuite in sql-core), another two error occurs:
>=20
> Error 1:=20
> Error:scalac:=20
>      while compiling: =
/Users/yijie/code/apache.spark.master/sql/core/src/main/scala/org/apache/s=
park/sql/test/TestSQLContext.scala
>         during phase: jvm
>      library version: version 2.10.4
>     compiler version: version 2.10.4
>   reconstructed args: -Xmax-classfile-name 120 -deprecation =
-P:genjavadoc:out=3D/Users/yijie/code/apache.spark.master/sql/core/target/=
java -feature -classpath =
/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/ant-ja=
vafx.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/l=
ib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/=
lib/javafx-doclet.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Co=
ntents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_5=
1.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk=
1.7.0_51.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachine=
s/jdk1.7.0_51.jdk/Contents/Home/lib/tools.jar:/Library/Java/JavaVirtualMac=
hines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/Jav=
aVirtualMachines/jdk1.7.0_51.jdk/Conte=E2=80=A6
> =E2=80=A6
> ...
> =
/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.j=
ar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib=
/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/j=
re/classes:/Users/yijie/code/apache.spark.master/lib_managed/jars/scala-li=
brary-2.10.4.jar =
-Xplugin:/Users/yijie/code/apache.spark.master/lib_managed/jars/genjavadoc=
-plugin_2.10.4-0.5.jar =
-Xplugin:/Users/yijie/code/apache.spark.master/lib_managed/jars/genjavadoc=
-plugin_2.10.4-0.5.jar
>   last tree to typer: Literal(Constant(parquet.io.api.Converter))
>               symbol: null
>    symbol definition: null
>                  tpe: Class(classOf[parquet.io.api.Converter])
>        symbol owners:=20
>       context owners: object TestSQLContext -> package test
> =3D=3D Enclosing template or block =3D=3D
> Template( // val <local TestSQLContext>: <notype> in object =
TestSQLContext, tree.tpe=3Dorg.apache.spark.sql.test.TestSQLContext.type
>   "org.apache.spark.sql.SQLContext" // parents
>   ValDef(
>     private
>     "_"
>     <tpt>
>     <empty>
>   )
>   // 2 statements
>   DefDef( // private def readResolve(): Object in object =
TestSQLContext
>     <method> private <synthetic>
>     "readResolve"
>     []
>     List(Nil)
>     <tpt> // tree.tpe=3DObject
>     test.this."TestSQLContext" // object TestSQLContext in package =
test, tree.tpe=3Dorg.apache.spark.sql.test.TestSQLContext.type
>   )
>   DefDef( // def <init>(): =
org.apache.spark.sql.test.TestSQLContext.type in object TestSQLContext
>     <method>
>     "<init>"
>     []
>     List(Nil)
>     <tpt> // tree.tpe=3Dorg.apache.spark.sql.test.TestSQLContext.type
>     Block( // tree.tpe=3DUnit
>       Apply( // def <init>(sparkContext: =
org.apache.spark.SparkContext): org.apache.spark.sql.SQLContext in class =
SQLContext, tree.tpe=3Dorg.apache.spark.sql.SQLContext
>         TestSQLContext.super."<init>" // def <init>(sparkContext: =
org.apache.spark.SparkContext): org.apache.spark.sql.SQLContext in class =
SQLContext, tree.tpe=3D(sparkContext: =
org.apache.spark.SparkContext)org.apache.spark.sql.SQLContext
>         Apply( // def <init>(master: String,appName: String,conf: =
org.apache.spark.SparkConf): org.apache.spark.SparkContext in class =
SparkContext, tree.tpe=3Dorg.apache.spark.SparkContext
>           new org.apache.spark.SparkContext."<init>" // def =
<init>(master: String,appName: String,conf: org.apache.spark.SparkConf): =
org.apache.spark.SparkContext in class SparkContext, tree.tpe=3D(master: =
String, appName: String, conf: =
org.apache.spark.SparkConf)org.apache.spark.SparkContext
>           // 3 arguments
>           "local"
>           "TestSQLContext"
>           Apply( // def <init>(): org.apache.spark.SparkConf in class =
SparkConf, tree.tpe=3Dorg.apache.spark.SparkConf
>             new org.apache.spark.SparkConf."<init>" // def <init>(): =
org.apache.spark.SparkConf in class SparkConf, =
tree.tpe=3D()org.apache.spark.SparkConf
>             Nil
>           )
>         )
>       )
>       ()
>     )
>   )
> )
> =3D=3D Expanded type of tree =3D=3D
> ConstantType(value =3D Constant(parquet.io.api.Converter))
> uncaught exception during compilation: java.lang.AssertionError
>=20
> Error 2:
>=20
> Error:scalac: Error: assertion failed: List(object package$DebugNode, =
object package$DebugNode)
> java.lang.AssertionError: assertion failed: List(object =
package$DebugNode, object package$DebugNode)
> 	at =
scala.reflect.internal.Symbols$Symbol.suchThat(Symbols.scala:1678)
> 	at =
scala.reflect.internal.Symbols$ClassSymbol.companionModule0(Symbols.scala:=
2988)
> 	at =
scala.reflect.internal.Symbols$ClassSymbol.companionModule(Symbols.scala:2=
991)
> 	at =
scala.tools.nsc.backend.jvm.GenASM$JPlainBuilder.genClass(GenASM.scala:137=
1)
> 	at =
scala.tools.nsc.backend.jvm.GenASM$AsmPhase.run(GenASM.scala:120)
> 	at =
scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
> 	at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
> 	at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
> 	at scala.tools.nsc.Global$Run.compile(Global.scala:1662)
> 	at xsbt.CachedCompiler0.run(CompilerInterface.scala:126)
> 	at xsbt.CachedCompiler0.run(CompilerInterface.scala:102)
> 	at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
> 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> 	at =
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:=
57)
> 	at =
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorIm=
pl.java:43)
> 	at java.lang.reflect.Method.invoke(Method.java:606)
> 	at =
sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)
> 	at =
sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)
> 	at =
sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
> 	at =
org.jetbrains.jps.incremental.scala.local.IdeaIncrementalCompiler.compile(=
IdeaIncrementalCompiler.scala:28)
> 	at =
org.jetbrains.jps.incremental.scala.local.LocalServer.compile(LocalServer.=
scala:25)
> 	at =
org.jetbrains.jps.incremental.scala.remote.Main$.make(Main.scala:64)
> 	at =
org.jetbrains.jps.incremental.scala.remote.Main$.nailMain(Main.scala:22)
> 	at =
org.jetbrains.jps.incremental.scala.remote.Main.nailMain(Main.scala)
> 	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> 	at =
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:=
57)
> 	at =
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorIm=
pl.java:43)
> 	at java.lang.reflect.Method.invoke(Method.java:606)
> 	at com.martiansoftware.nailgun.NGSession.run(NGSession.java:319)
>=20
>=20
> On Jun 11, 2014, at 11:17 AM, Qiuzhuang Lian =
<qiuzhuang.lian@gmail.com> wrote:
>=20
>> I also run into this problem when running examples in IDEA. The issue =
looks that it uses depends on too many jars and that the classpath seems =
to have length limit. So I import the assembly jar and put the head of =
the list dependent path and it works.
>>=20
>> Thanks,
>> Qiuzhuang
>>=20
>>=20
>> On Wed, Jun 11, 2014 at 10:39 AM, =E7=94=B3=E6=AF=85=E6=9D=B0 =
<henry.yijieshen@gmail.com> wrote:
>> Hi All,
>>=20
>> I want to run ScalaTest Suite in IDEA directly, but it seems didn=E2=80=
=99t pass the make phase before test running.
>> The problems are as follows:
>>=20
>> =
/Users/yijie/code/apache.spark.master/core/src/main/scala/org/apache/spark=
/executor/MesosExecutorBackend.scala
>> Error:(44, 35) type mismatch;
>>  found   : org.apache.mesos.protobuf.ByteString
>>  required: com.google.protobuf.ByteString
>>       .setData(ByteString.copyFrom(data))
>>                                   ^
>> =
/Users/yijie/code/apache.spark.master/core/src/main/scala/org/apache/spark=
/scheduler/cluster/mesos/MesosSchedulerBackend.scala
>> Error:(119, 35) type mismatch;
>>  found   : org.apache.mesos.protobuf.ByteString
>>  required: com.google.protobuf.ByteString
>>       .setData(ByteString.copyFrom(createExecArg()))
>>                                   ^
>> Error:(257, 35) type mismatch;
>>  found   : org.apache.mesos.protobuf.ByteString
>>  required: com.google.protobuf.ByteString
>>       .setData(ByteString.copyFrom(task.serializedTask))
>>                                   ^
>>=20
>> Before I run test in IDEA, I build spark through =E2=80=99sbt/sbt =
assembly=E2=80=99,
>> import projects into IDEA after =E2=80=99sbt/sbt gen-idea=E2=80=99,
>> and able to run test in Terminal =E2=80=99sbt/sbt test=E2=80=99
>>=20
>> Are there anything I leave out in order to run/debug testsuite inside =
IDEA?
>>=20
>> Best regards,
>> Yijie
>>=20
>=20
>=20


--Apple-Mail=_EF43DAC4-DE51-468C-8493-E4AAEC563523--

From dev-return-8021-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun 12 13:04:40 2014
Return-Path: <dev-return-8021-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 483F411771
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 12 Jun 2014 13:04:40 +0000 (UTC)
Received: (qmail 66845 invoked by uid 500); 12 Jun 2014 13:04:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66787 invoked by uid 500); 12 Jun 2014 13:04:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66776 invoked by uid 99); 12 Jun 2014 13:04:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 12 Jun 2014 13:04:38 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS,UNPARSEABLE_RELAY
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of genmao.ygm@alibaba-inc.com designates 42.120.133.18 as permitted sender)
Received: from [42.120.133.18] (HELO out4133-18.mail.aliyun.com) (42.120.133.18)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 12 Jun 2014 13:04:34 +0000
DKIM-Signature:v=1; a=rsa-sha256; c=relaxed/relaxed;
	d=alibaba-inc.com; s=default;
	t=1402578242; h=From:To:Subject:Date:Message-ID:MIME-Version:Content-Type;
	bh=/uAUmdlbmb8fhMFCCNyuQB5jTxhFA434UmlhK85buG8=;
	b=Q9dtqsY3EGBv+aOQ9UuEEc8KkHmyW7qpieFTbtlpZZ4eAJyd+5LVHiBvEj96u9wAsZPswfPolwxP8+LKtCRDEQ7QigXvv1LpyroT4edKLwp5kSQLWmuWMGyRdTUmpf/yw95qr26oiIzD6fKT/9NmNPdtV34wUrMjScCbGlKrAR8=
X-Alimail-AntiSpam:AC=PASS;BC=-1|-1;BR=01201311R201e4;FP=0|-1|-1|-1|0|-1|-1|-1;HT=r41g03021;MF=genmao.ygm@alibaba-inc.com;PH=DS;RN=1;RT=1;SR=0;
Received: from WINU8V44P2GIG9(mailfrom:genmao.ygm@alibaba-inc.com ip:42.120.74.148)
          by smtp.aliyun-inc.com(127.0.0.1);
          Thu, 12 Jun 2014 21:03:59 +0800
Reply-To: "=?GBK?B?0+C4+cOvKNPguPnDryk=?=" <genmao.ygm@alibaba-inc.com>
From: "=?GBK?B?0+C4+cOvKNPguPnDryk=?=" <genmao.ygm@alibaba-inc.com>
To: <dev@spark.apache.org>
Subject: GraphX can not unpersist edges of old graph?
Date: Thu, 12 Jun 2014 21:03:59 +0800
Message-ID: <005701cf863e$c6b2e1a0$5418a4e0$@alibaba-inc.com>
MIME-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_000_0058_01CF8681.D4D6E4F0"
X-Mailer: Microsoft Outlook 14.0
Thread-Index: Ac+GPkCVp/vBMEZkR8OPbLXerWrSIw==
Content-Language: zh-cn
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_000_0058_01CF8681.D4D6E4F0
Content-Type: text/plain;
	charset="gb2312"
Content-Transfer-Encoding: 7bit

Hi All,

         

I know the benefit of RDD caching, but abuse of using cache may cause memory
leak. In graphx, we can cache a graph by using graph.cache(), and many
transformation of graph create and cache new edges, like partitionBy() and
subgraph(). However, I can not find an interface to unpersist edges. I
wonder the purpose of design like this. It can indeed improve performance.
But, it may lead to memory leak if not unpersist edges in some cases. For
example, in Spark-1.0,  the pregel can not unpersist edges of old graph
effectively, and leads to memory leak.

 

 

Much Thanks!

 


------=_NextPart_000_0058_01CF8681.D4D6E4F0--


From dev-return-8022-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun 12 20:48:19 2014
Return-Path: <dev-return-8022-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 27ECF1191D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 12 Jun 2014 20:48:19 +0000 (UTC)
Received: (qmail 93564 invoked by uid 500); 12 Jun 2014 20:48:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93497 invoked by uid 500); 12 Jun 2014 20:48:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93486 invoked by uid 99); 12 Jun 2014 20:48:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 12 Jun 2014 20:48:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ankurdave@gmail.com designates 209.85.128.170 as permitted sender)
Received: from [209.85.128.170] (HELO mail-ve0-f170.google.com) (209.85.128.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 12 Jun 2014 20:48:14 +0000
Received: by mail-ve0-f170.google.com with SMTP id oz11so2413405veb.1
        for <dev@spark.apache.org>; Thu, 12 Jun 2014 13:47:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=764Z4uHUSdM3SuSYjPQlwPojE/vYpRUE2+gHETvUDhg=;
        b=Bj5mxN1ApnzZ+l1s6NTbkOZEgX422I5EHFvLhrFJrLXZ+ZqkqoxhdLKoqjt1b7mDuh
         81Bx19N6qR+uqNkRS4buwt1QK1THYc1SSkmTjVym6NVh1K31gMBvFxjs+A65AItC+w75
         KTWrR5tntb+7d0bdAbDREoCG5n3B5wD0xbygp8mT/O1MLm4m2B3mr0MnmVJKprUKxAms
         BYw7YHiNRCiz3PmUYYFHLJ0V/vaY3nim59Gxfj/KrD/lnwykSBgqLaJz2hpQ8tSZH7iQ
         m1QLC/HINirnU+gd8k6FFdm7Z8TtP5foUU2d5DDCl2paX1EqSPS6kw22Ce7Y1m8ZEaMB
         7duQ==
X-Received: by 10.220.167.2 with SMTP id o2mr46246474vcy.8.1402606073251; Thu,
 12 Jun 2014 13:47:53 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.95.166 with HTTP; Thu, 12 Jun 2014 13:47:33 -0700 (PDT)
In-Reply-To: <005701cf863e$c6b2e1a0$5418a4e0$@alibaba-inc.com>
References: <005701cf863e$c6b2e1a0$5418a4e0$@alibaba-inc.com>
From: Ankur Dave <ankurdave@gmail.com>
Date: Thu, 12 Jun 2014 13:47:33 -0700
Message-ID: <CAK1A71yTnmgfGwMMMzePQJEp8VRcPfOEjUaKU8MSx++Qywx0_w@mail.gmail.com>
Subject: Re: GraphX can not unpersist edges of old graph?
To: dev@spark.apache.org, =?UTF-8?B?5L2Z5qC56IyCKOS9meagueiMgik=?= <genmao.ygm@alibaba-inc.com>
Content-Type: multipart/alternative; boundary=089e011618cec18c6004fba9ac42
X-Virus-Checked: Checked by ClamAV on apache.org

--089e011618cec18c6004fba9ac42
Content-Type: text/plain; charset=UTF-8

We didn't provide an unpersist API for Graph because the internal
dependency structure of a graph can make it hard to unpersist correctly in
a way that avoids recomputation. However, you can directly unpersist a
graph's vertices and edges RDDs using graph.vertices.unpersist() and
graph.edges.unpersist().

By the way, the memory leak bug with Pregel (SPARK-2025
<https://issues.apache.org/jira/browse/SPARK-2025>) is fixed in master.

Ankur <http://www.ankurdave.com/>

--089e011618cec18c6004fba9ac42--

From dev-return-8023-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 13 01:59:59 2014
Return-Path: <dev-return-8023-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A904F1031D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Jun 2014 01:59:59 +0000 (UTC)
Received: (qmail 86688 invoked by uid 500); 13 Jun 2014 01:59:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86630 invoked by uid 500); 13 Jun 2014 01:59:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 78595 invoked by uid 99); 13 Jun 2014 01:54:12 -0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of 396154235@qq.com does not designate 216.139.236.26 as permitted sender)
Date: Thu, 12 Jun 2014 18:53:46 -0700 (PDT)
From: Yanjie Gao <396154235@qq.com>
To: dev@spark.incubator.apache.org
Message-ID: <1402624426683-7003.post@n3.nabble.com>
Subject: Big-Endian (IBM Power7) Spark Serialization issue
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi All,
We are facing a Serialization issue ,



This issue has been submit to JIRA

https://issues.apache.org/jira/browse/SPARK-2018





We have an application run on Spark on Power7 System .
But we meet an important issue about serialization.
The example HdfsWordCount can meet the problem.
./bin/run-example org.apache.spark.examples.streaming.HdfsWordCount localdir
We used Power7 (Big-Endian arch) and Redhat 6.4.
Big-Endian is the main cause since the example ran successfully in another
Power-based Little Endian setup.
here is the exception stack and log:
Spark Executor Command: "/opt/ibm/java-ppc64-70//bin/java" "-cp"
"/home/test1/spark-1.0.0-bin-hadoop2/lib::/home/test1/src/spark-1.0.0-bin-hadoop2/conf:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/spark-assembly-1.0.0-hadoop2.2.0.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-rdbms-3.2.1.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-api-jdo-3.2.1.jar:/home/test1/src/spark-1.0.0-bin-hadoop2/lib/datanucleus-core-3.2.2.jar:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/:/home/test1/src/hadoop-2.3.0-cdh5.0.0/etc/hadoop/"
"-XX:MaxPermSize=128m" "-Xdebug"
"-Xrunjdwp:transport=dt_socket,address=99999,server=y,suspend=n" "-Xms512M"
"-Xmx512M" "org.apache.spark.executor.CoarseGrainedExecutorBackend"
"akka.tcp://spark@9.186.105.141:60253/user/CoarseGrainedScheduler" "2"
"p7hvs7br16" "4" "akka.tcp://sparkWorker@p7hvs7br16:59240/user/Worker"
"app-20140604023054-0000"
========================================
14/06/04 02:31:20 WARN util.NativeCodeLoader: Unable to load native-hadoop
library for your platform... using builtin-java classes where applicable
14/06/04 02:31:21 INFO spark.SecurityManager: Changing view acls to:
test1,yifeng
14/06/04 02:31:21 INFO spark.SecurityManager: SecurityManager:
authentication disabled; ui acls disabled; users with view permissions:
Set(test1, yifeng)
14/06/04 02:31:22 INFO slf4j.Slf4jLogger: Slf4jLogger started
14/06/04 02:31:22 INFO Remoting: Starting remoting
14/06/04 02:31:22 INFO Remoting: Remoting started; listening on addresses
:[akka.tcp://sparkExecutor@p7hvs7br16:39658]
14/06/04 02:31:22 INFO Remoting: Remoting now listens on addresses:
[akka.tcp://sparkExecutor@p7hvs7br16:39658]
14/06/04 02:31:22 INFO executor.CoarseGrainedExecutorBackend: Connecting to
driver: akka.tcp://spark@9.186.105.141:60253/user/CoarseGrainedScheduler
14/06/04 02:31:22 INFO worker.WorkerWatcher: Connecting to worker
akka.tcp://sparkWorker@p7hvs7br16:59240/user/Worker
14/06/04 02:31:23 INFO worker.WorkerWatcher: Successfully connected to
akka.tcp://sparkWorker@p7hvs7br16:59240/user/Worker
14/06/04 02:31:24 INFO executor.CoarseGrainedExecutorBackend: Successfully
registered with driver
14/06/04 02:31:24 INFO spark.SecurityManager: Changing view acls to:
test1,yifeng
14/06/04 02:31:24 INFO spark.SecurityManager: SecurityManager:
authentication disabled; ui acls disabled; users with view permissions:
Set(test1, yifeng)
14/06/04 02:31:24 INFO slf4j.Slf4jLogger: Slf4jLogger started
14/06/04 02:31:24 INFO Remoting: Starting remoting
14/06/04 02:31:24 INFO Remoting: Remoting started; listening on addresses
:[akka.tcp://spark@p7hvs7br16:58990]
14/06/04 02:31:24 INFO Remoting: Remoting now listens on addresses:
[akka.tcp://spark@p7hvs7br16:58990]
14/06/04 02:31:24 INFO spark.SparkEnv: Connecting to MapOutputTracker:
akka.tcp://spark@9.186.105.141:60253/user/MapOutputTracker
14/06/04 02:31:25 INFO spark.SparkEnv: Connecting to BlockManagerMaster:
akka.tcp://spark@9.186.105.141:60253/user/BlockManagerMaster
14/06/04 02:31:25 INFO storage.DiskBlockManager: Created local directory at
/tmp/spark-local-20140604023125-3f61
14/06/04 02:31:25 INFO storage.MemoryStore: MemoryStore started with
capacity 307.2 MB.
14/06/04 02:31:25 INFO network.ConnectionManager: Bound socket to port 39041
with id = ConnectionManagerId(p7hvs7br16,39041)
14/06/04 02:31:25 INFO storage.BlockManagerMaster: Trying to register
BlockManager
14/06/04 02:31:25 INFO storage.BlockManagerMaster: Registered BlockManager
14/06/04 02:31:25 INFO spark.HttpFileServer: HTTP File server directory is
/tmp/spark-7bce4e43-2833-4666-93af-bd97c327497b
14/06/04 02:31:25 INFO spark.HttpServer: Starting HTTP Server
14/06/04 02:31:25 INFO server.Server: jetty-8.y.z-SNAPSHOT
14/06/04 02:31:26 INFO server.AbstractConnector: Started
SocketConnector@0.0.0.0:39958
14/06/04 02:31:26 INFO executor.CoarseGrainedExecutorBackend: Got assigned
task 2
14/06/04 02:31:26 INFO executor.Executor: Running task ID 2
14/06/04 02:31:26 ERROR executor.Executor: Exception in task ID 2
java.io.InvalidClassException: scala.reflect.ClassTag$$anon$1; local class
incompatible: stream classdesc serialVersionUID = -8102093212602380348,
local class serialVersionUID = -4937928798201944954
at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:678)
at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1678)
at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1573)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1827)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)
at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:607)
at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1078)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1949)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)
at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:607)
at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1078)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1949)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)
at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:607)
at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1078)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1949)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)
at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:607)
at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1078)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1949)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)
at scala.collection.immutable.$colon$colon.readObject(List.scala:362)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:607)
at java.lang.reflect.Method.invoke(Method.java:607)
at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1078)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1949)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2047)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1971)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1854)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)
at
org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)
at
org.apache.spark.scheduler.ResultTask$.deserializeInfo(ResultTask.scala:61)
at org.apache.spark.scheduler.ResultTask.readExternal(ResultTask.scala:141)
at java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1893)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1852)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1406)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:409)
at
org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)
at
org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:85)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:169)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:781)
14/06/04 02:31:26 ERROR executor.CoarseGrainedExecutorBackend: Driver
Disassociated [akka.tcp://sparkExecutor@p7hvs7br16:39658] ->
[akka.tcp://spark@9.186.105.141:60253] disassociated! Shutting down.

Now I have some new detection.
(1) Compare
server Spark exec mode pass or not jdk 
x86(Little Endian) Local+cluster pass x86
p8(Little Endian) Local+cluster pass IBM(little endian)
P7(Big Endian) Local mode pass(I change some jar classpath then can't pass)
IBM(Big endian)
P7 (Big Endian) Cluster mode not IBM(Big endian)
(2) The Exception priciple
2.1 Main Error : Exception in thread "main" org.apache.spark.SparkException:
Job aborted due to stage failure: Task 1.0:0 failed 4 times, most recent
failure: Exception failure in TID 3 on host arlab105.austin.ibm.com:
java.io.InvalidClassException: org.apache.spark.SerializableWritable; local
class incompatible: stream classdesc serialVersionUID = 6301214776158303468,
local class serialVersionUID = -7785455416944904980
(other may has the same reason)
2.2 Exception in thread "main" org.apache.spark.SparkException: Job aborted
due to stage failure: Task 0.0:0 failed 1 times, most recent failure:
Exception failure in TID 1 on host localhost: java.io.InvalidClassException:
scala.Tuple2; invalid descriptor for field _1
Now we analysis 2.1 Bug .
refer:
serialVersionUID has two generate method
1 default 1Lprivate static final long serialVersionUID = 1L
2 Generated by hash. Class name ,interface name ,method ,attribute can
affect the result.
Our error is not 1L .So it generated by method 2.
UID is used when the process deserialize the byte array .the process read
the local class file ,and find the class's UID.If it is diff with the array
.Then throw the Exception.
Let's see the work flow of Spark Serilization
Local mode 
once serialize 
object ----serialize(thread1 or thread2)-->array----deserialize(thread2 or
process2)--->object
Cluster mode
twice serialize
object ---serialize(thread1 or thread2)-->array-Actor send message serialize
--->message-->Actor receive and deserialize it ----->array
------deserialize(thread2 or process2)--->object
summary:
let't compare (1) 's four situation.
I think the reason is that IBM jdk and (scala lib and akka lib) may have
some intersection of some class. But they compile in diff platform use diff
javac .They may generate diff UID. 
In run time .jvm may load the same class from diff .class file.
(3)Method to fix it.
I think
The reason is the same class load diff class file.
There are two method .May be there are other better method.
4.1 Let the two file has the same version UID:Compile scala lib and akka lib
in P7 platform
4.2 Let the two loader load the same Jar. Use some method like extend class
loader or OSGI .We force the jvm to load the same class file.(The difficult
thing is that classes is in jar and class num is too large .)




But  I want to know  what's the real reason of this bug ?
How can we fast fix this bug?


Thanks a lot

Best Regards!
Yanjie Gao 



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Big-Endian-IBM-Power7-Spark-Serialization-issue-tp7003.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8024-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 13 04:02:01 2014
Return-Path: <dev-return-8024-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0E5C81062E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Jun 2014 04:02:00 +0000 (UTC)
Received: (qmail 1691 invoked by uid 500); 13 Jun 2014 04:01:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1622 invoked by uid 500); 13 Jun 2014 04:01:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 47636 invoked by uid 99); 10 Jun 2014 09:43:45 -0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of learnings.chitturi@gmail.com designates 209.85.192.68 as permitted sender)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=LcK3oWbO57eSB/GyDnWyj1yAMbr3GSSgzblHwg6KHTE=;
        b=MVN9SSeG+HxRbjEsbZ6ilkz1TXWJLGmO33RTddcP7ObPbM6UZGpS4TUe5+McO0N6Z8
         flShTOsYyOEbrKGlIv526f1eDeEIqxvu7IRpVDSYz+xPExgCyodpRV7sJs/ub+zAZ1VE
         SzZXBUWcnPuk/05ZAtsY6V8CtAV2NrH1RAgbzliNdxp/zWASGI2GuQ8xSxnJXxXg3bXC
         CXscouMt+yh+Zf1htuh3bV3DBw8+sL/yOw4br6ubDw5UJRhobZb77eQPWxJA91OobRej
         Frd1K22fh+5aX+dZ3oEBF1ek7AnE9P7DLzsgCnkNTTXHSv89+bbd/xcWCZZraPTbOLC3
         oBhQ==
MIME-Version: 1.0
X-Received: by 10.140.109.70 with SMTP id k64mr37919359qgf.92.1402393397943;
 Tue, 10 Jun 2014 02:43:17 -0700 (PDT)
Date: Tue, 10 Jun 2014 15:13:17 +0530
Message-ID: <CABXsDPp-MX=9PaMUUOgHXHaW3bth3cNhHeqvx5zzSSGTzNPyKA@mail.gmail.com>
Subject: Subscription request for developer community
From: Priya Ch <learnings.chitturi@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1139c10651e52404fb782810
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1139c10651e52404fb782810
Content-Type: text/plain; charset=UTF-8

Please accept the request

--001a1139c10651e52404fb782810--

From dev-return-8025-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 13 05:11:20 2014
Return-Path: <dev-return-8025-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8EC78107DD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Jun 2014 05:11:20 +0000 (UTC)
Received: (qmail 90574 invoked by uid 500); 13 Jun 2014 05:11:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90471 invoked by uid 500); 13 Jun 2014 05:11:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90451 invoked by uid 99); 13 Jun 2014 05:11:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Jun 2014 05:11:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.43 as permitted sender)
Received: from [209.85.160.43] (HELO mail-pb0-f43.google.com) (209.85.160.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Jun 2014 05:11:14 +0000
Received: by mail-pb0-f43.google.com with SMTP id up15so1780980pbc.16
        for <multiple recipients>; Thu, 12 Jun 2014 22:10:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=from:content-type:subject:message-id:date:to:mime-version;
        bh=/qrU9zfljHuYCManvsJg+IEc5UdONFMIo97iXPy/yPg=;
        b=v0L5qegPDbBOx1bSK3nwoVCwxfFx2QkgI1Mlqh9IVdq+hGqBIKGHi3XHyIuptg3xPN
         Ve5UE4RRSvaHr9mvp3Vxc0wXHN4JrYE7BLFSMxGSFxa3uVsHEgGHsNJA/S7/6fzs2CAf
         a2lTWUaoA6OyW5U9vR+hMouytodC69BYarF3BUFaO/6kg2+7d3RfVBnpjTWEUmYUl9xk
         F2uh/+IXKZx/rKBx/PAJkZLzF0JOZ8584oLC3y0/wGGcjunsseVHVZGH5iGKG8Z1h5z4
         sqDlDAQO5QG3KGLVpgNuJx/w+o+0sJq+H2NwMKYgOZvLM4Xq0LkA+eaS/I2LwMXqDP3+
         7qFw==
X-Received: by 10.66.254.166 with SMTP id aj6mr471115pad.11.1402636253642;
        Thu, 12 Jun 2014 22:10:53 -0700 (PDT)
Received: from [192.168.1.106] (c-76-102-205-42.hsd1.ca.comcast.net. [76.102.205.42])
        by mx.google.com with ESMTPSA id jt7sm1179625pbc.46.2014.06.12.22.10.52
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 12 Jun 2014 22:10:52 -0700 (PDT)
From: Matei Zaharia <matei.zaharia@gmail.com>
Content-Type: multipart/alternative; boundary="Apple-Mail=_303FED2C-3BC3-415A-BC83-0B20046B8FBF"
Subject: Fwd: ApacheCon CFP closes June 25
Message-Id: <C0FF7A52-46B0-4BD7-B692-C2FC9D444A91@gmail.com>
Date: Thu, 12 Jun 2014 22:10:50 -0700
To: user@spark.apache.org,
 dev@spark.apache.org
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
X-Mailer: Apple Mail (2.1878.2)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_303FED2C-3BC3-415A-BC83-0B20046B8FBF
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=windows-1252

(I=92m forwarding this message on behalf of the ApacheCon organizers, =
who=92d like to see involvement from every Apache project!)

As you may be aware, ApacheCon will be held this year in Budapest, on =
November 17-23. (See http://apachecon.eu for more info.)

The Call For Papers for that conference is still open, but will be =
closing soon. We need you talk proposals, to represent Spark at =
ApacheCon. We need all kinds of talks - deep technical talks, hands-on =
tutorials, introductions for beginners, or case studies about the =
awesome stuff you're doing with Spark.

Please consider submitting a proposal, at =
http://events.linuxfoundation.org//events/apachecon-europe/program/cfp

Thanks!

Matei=

--Apple-Mail=_303FED2C-3BC3-415A-BC83-0B20046B8FBF--

From dev-return-8026-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 13 07:16:41 2014
Return-Path: <dev-return-8026-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4847910AC2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Jun 2014 07:16:41 +0000 (UTC)
Received: (qmail 52556 invoked by uid 500); 13 Jun 2014 07:16:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52412 invoked by uid 500); 13 Jun 2014 07:16:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52262 invoked by uid 99); 13 Jun 2014 07:16:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Jun 2014 07:16:39 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.52] (HELO mail-qa0-f52.google.com) (209.85.216.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Jun 2014 07:16:34 +0000
Received: by mail-qa0-f52.google.com with SMTP id w8so3044076qac.11
        for <dev@spark.apache.org>; Fri, 13 Jun 2014 00:16:13 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=MB6iOkOemtO+bVZJVAm+K6ZFELTBD+Fx9F8NK03BKqA=;
        b=ieImj/tpf0VG93CVFgZ4ie96e/cZ/sLj/ds24O2qJ6d4Jed5E0q+KuDiVLCKsTqtVq
         7bHEbGCgUyVuQp2Pp8BSCbRTtDKOiyCXbvtLZZP3cIjz2cFa2byEQhgF0/S91P8OjA7m
         HxM9zPioLcyCyJKjQ8N86JRNMvLiYTO1tmmc99Oine1ZbB93WIZThLX7CUT+R0F8NArx
         6QAzt3DIuYKWsGFofemFwBD/pQdJFTdg3Zce3MNczIKjAiFY3bMO3Ho78HNEgFllGAgZ
         iFZhMU2epm2vuQ25L4ZE8/Jc2swYBAl/+q0d9epBqu2cWmsrrM/oovdicQPYzT/+ysQL
         gELg==
X-Gm-Message-State: ALoCoQnwFgC7QSOXnUSMmWmLy0eqHd3FHTmGjhsE2TRkaeSyxZtdA/E2jaqXWxY/2BFXXfWqeACO
X-Received: by 10.224.24.134 with SMTP id v6mr997175qab.88.1402643773272; Fri,
 13 Jun 2014 00:16:13 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.49.231 with HTTP; Fri, 13 Jun 2014 00:15:53 -0700 (PDT)
From: Reynold Xin <rxin@databricks.com>
Date: Fri, 13 Jun 2014 00:15:53 -0700
Message-ID: <CAPh_B=aybZ1QudGVqg5UkPJBH5xqC1OZxxfBZ21ZLBj3S5=3+A@mail.gmail.com>
Subject: openstack swift integration with Spark
To: user@spark.apache.org, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c252c6da616104fbb27304
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c252c6da616104fbb27304
Content-Type: text/plain; charset=UTF-8

If you are interested in openstack/swift integration with Spark, please
drop me a line. We are looking into improving the integration.

Thanks.

--001a11c252c6da616104fbb27304--

From dev-return-8027-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jun 14 02:54:54 2014
Return-Path: <dev-return-8027-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BED0711FF7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 14 Jun 2014 02:54:54 +0000 (UTC)
Received: (qmail 64351 invoked by uid 500); 14 Jun 2014 02:54:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64292 invoked by uid 500); 14 Jun 2014 02:54:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 5457 invoked by uid 99); 14 Jun 2014 01:41:09 -0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of 396154235@qq.com does not designate 216.139.236.26 as permitted sender)
Date: Fri, 13 Jun 2014 18:40:44 -0700 (PDT)
From: Yanjie Gao <396154235@qq.com>
To: dev@spark.incubator.apache.org
Message-ID: <1402710044343-7007.post@n3.nabble.com>
In-Reply-To: <1402624426683-7003.post@n3.nabble.com>
References: <1402624426683-7003.post@n3.nabble.com>
Subject: Re: Big-Endian (IBM Power7) Spark Serialization issue
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi All
Yesterday,we rewrite the class Serializable's UID  as static .Then find that
the Executor could resolve the class .But it also thow the Exception
imcompitable UID.We also debug the program in remote mode found jvm use
bigendian in master and slave.

Once we try ./run-example HDFSWordCount on Spark-0.9.1 it failed,but run on
1.0.0 then it works.
We  think that Spark-1.0.0  may  fix something.


But run on standalone mode use kyro or java serilazer  it also has the
problem  .

May be  Spark cause this problem  on  Big Endian environment?

Thanks a lot!



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Big-Endian-IBM-Power7-Spark-Serialization-issue-tp7003p7007.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8028-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jun 14 06:07:26 2014
Return-Path: <dev-return-8028-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 69061112A8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 14 Jun 2014 06:07:26 +0000 (UTC)
Received: (qmail 4492 invoked by uid 500); 14 Jun 2014 06:07:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3706 invoked by uid 500); 14 Jun 2014 06:07:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 3496 invoked by uid 99); 14 Jun 2014 06:07:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 14 Jun 2014 06:07:24 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [106.10.149.66] (HELO nm16-vm3.bullet.mail.sg3.yahoo.com) (106.10.149.66)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 14 Jun 2014 06:07:20 +0000
Received: from [106.10.166.63] by nm16.bullet.mail.sg3.yahoo.com with NNFMP; 14 Jun 2014 06:06:57 -0000
Received: from [106.10.150.24] by tm20.bullet.mail.sg3.yahoo.com with NNFMP; 14 Jun 2014 06:06:57 -0000
Received: from [127.0.0.1] by omp1025.mail.sg3.yahoo.com with NNFMP; 14 Jun 2014 06:06:57 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 174742.53321.bm@omp1025.mail.sg3.yahoo.com
Received: (qmail 32117 invoked by uid 60001); 14 Jun 2014 06:06:57 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.co.in; s=s1024; t=1402726017; bh=ogIQUz/OhUhJZlz5XffvCRtW7ZeLeUcnMZEeYgzVSl4=; h=Message-ID:Date:From:Subject:To:In-Reply-To:MIME-Version:Content-Type; b=ih4jvttEgmrgdLwQ2egSJBlZHQCNxL7mJ/JJAtAp6+hUQMj0T7z4XraqoBqfENxZ71bvROMMuqWVaKaVm3fDRDt4Ihh5qGPhui6psEz4ad11JulyIK9iT6eYs/gUrk2DFxz90cmDmgCNSCR7TN06MK/YsZvagvnpeSTijHRXt8U=
X-YMail-OSG: ByuVzD0VM1lQAvOgLSnjsrYw0yCJZ5DOUP9yW_AUX9sEaI6
 KBLC_xbjuQeP92UXubU4ZeQfQBJOViBYS1WWPuViuNyjFbWQ3qsqlTmix5HT
 nSSuRY_CFUmJL0V20djELlRSOfxB1Tt_iSsiafWcE7w8PbN7MqpyQQHb0yMT
 ETjvfaysvhTd9y6GQpRNhzHcds0bhwNglbOJZ92eQTAu2_GtFbFxo4cB0nyN
 ZqEuAADWZgA.kHLSI6bsTRd.NLBj9xLUrTVxOrp_Xs0FGQGDb.BoAYn6bUrp
 XVubon9Isj3vVQ1FL7kDBDcES1GNQ1pvVCtkQumbnJrtM_1RAZhv7pqlbtD0
 7Nhn7xGpQbKuKpv045UD1_zHBB0hxxnygZoISt1IbLFjGo65mULHMOeP25IM
 AbiPhQhwWHk2ZlF_TmfYhvaC2josJJQDSkgcspg63KdxG22oTFkYfl0CRYeM
 Ds0_L.qmoz.xq4ZCpXpbkNs8PfCwb9JK3ajPYnzNShIKgyQQYNAUqUs2t9lE
 FzpCWFYR2kCnd1FiX315gVx6KNzlmiRors.e8YHu0Guwge5hR2uauR9GtDH5
 W_gswXoqciYhiclQZP_cuwbJWiUWhY153o63L6ShjxbC1.Dqlr4Zpzi3jjcU
 p0eaKMOks.h_rlzWlHMybQV6ABDKy4VKIcEzIMo2sQD5AXCYPQuUa47Qmck.
 Dmd3o7pp7.0BWDPtLOA--
Received: from [110.225.62.48] by web194604.mail.sg3.yahoo.com via HTTP; Sat, 14 Jun 2014 14:06:57 SGT
X-Rocket-MIMEInfo: 002.001,SGkgQWxsCgpJcyB0aGVyZSBhbnkgY29tbXVuaWNhdGlvbiBiZXR3ZWVuIFNwYXJrIE1BU1RFUiBub2RlIGFuZCBIYWRvb3AgTmFtZU5vZGUgd2hpbGUgZGlzdHJpYnV0aW5nIHdvcmsgdG8gV09SS0VSIG5vZGVzLCBsaWtlIHdlIGhhdmUgaW4gTWFwUmVkdWNlLgoKUGxlYXNlIHN1Z2dlc3QKClRJQQoKLS3CoApBbmlzaCBTbmVoCiJFeHBlcmllbmNlIGlzIHRoZSBiZXN0IHRlYWNoZXIuIgpodHRwOi8vaW4ubGlua2VkaW4uY29tL2luL2FuaXNoc25laAoKATABAQEB
X-Mailer: YahooMailAndroidMobile/4.0.2 YahooMailWebService/0.8.190.668
Message-ID: <1402726017.77063.YahooMailAndroidMobile@web194604.mail.sg3.yahoo.com>
Date: Sat, 14 Jun 2014 14:06:57 +0800 (SGT)
From: "anishsneh@yahoo.co.in" <anishsneh@yahoo.co.in>
Subject: Fw: How Spark Choose Worker Nodes for respective HDFS block
To: "user@spark.apache.org" <user@spark.apache.org>,
  "dev@spark.apache.org" <dev@spark.apache.org>
In-Reply-To: <1402694270.9384.YahooMailAndroidMobile@web194603.mail.sg3.yahoo.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="1512302668-993386742-1402726017=:77063"
X-Virus-Checked: Checked by ClamAV on apache.org

--1512302668-993386742-1402726017=:77063
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

Hi All=0A=0AIs there any communication between Spark MASTER node and Hadoop=
 NameNode while distributing work to WORKER nodes, like we have in MapReduc=
e.=0A=0APlease suggest=0A=0ATIA=0A=0A--=A0=0AAnish Sneh=0A"Experience is th=
e best teacher."=0Ahttp://in.linkedin.com/in/anishsneh=0A=0A
--1512302668-993386742-1402726017=:77063--

From dev-return-8029-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jun 14 15:53:18 2014
Return-Path: <dev-return-8029-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C9A7311A6D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 14 Jun 2014 15:53:18 +0000 (UTC)
Received: (qmail 29958 invoked by uid 500); 14 Jun 2014 15:53:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 29889 invoked by uid 500); 14 Jun 2014 15:53:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 29876 invoked by uid 99); 14 Jun 2014 15:53:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 14 Jun 2014 15:53:18 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matt@redhat.com designates 209.132.183.28 as permitted sender)
Received: from [209.132.183.28] (HELO mx1.redhat.com) (209.132.183.28)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 14 Jun 2014 15:53:16 +0000
Received: from int-mx14.intmail.prod.int.phx2.redhat.com (int-mx14.intmail.prod.int.phx2.redhat.com [10.5.11.27])
	by mx1.redhat.com (8.14.4/8.14.4) with ESMTP id s5EFqoep009139
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-GCM-SHA384 bits=256 verify=OK)
	for <dev@spark.apache.org>; Sat, 14 Jun 2014 11:52:51 -0400
Received: from eeyore.local (ovpn01.gateway.prod.ext.phx2.redhat.com [10.5.9.1])
	by int-mx14.intmail.prod.int.phx2.redhat.com (8.14.4/8.14.4) with ESMTP id s5EFqnTq007866
	for <dev@spark.apache.org>; Sat, 14 Jun 2014 11:52:50 -0400
Message-ID: <539C6FD1.2000209@redhat.com>
Date: Sat, 14 Jun 2014 11:52:49 -0400
From: Matthew Farrellee <matt@redhat.com>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20100101 Thunderbird/24.5.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: Re: Apache Spark and Swift object store
References: <OF4A260C43.5AF736DE-ONC2257CF1.002B1BBC-C2257CF1.002C1CC0@il.ibm.com>
In-Reply-To: <OF4A260C43.5AF736DE-ONC2257CF1.002B1BBC-C2257CF1.002C1CC0@il.ibm.com>
Content-Type: text/plain; charset=ISO-8859-1; format=flowed
Content-Transfer-Encoding: 7bit
X-Scanned-By: MIMEDefang 2.68 on 10.5.11.27
X-Virus-Checked: Checked by ClamAV on apache.org

On 06/08/2014 04:01 AM, Gil Vernik wrote:
> Hello everyone,
>
> I would like to initiate discussion about integration Apache Spark and
> Openstack Swift.
> (https://issues.apache.org/jira/browse/SPARK-938 was created while ago)
>
> I created a patch (https://github.com/apache/spark/pull/1010) that
> provides initial information how to connect Swift and Spark. Currently it
> uses Hadoop 2.3.0 and only stand alone mode of Spark. This patch is mainly
> used to provide community a way to experiment with this integration.
> I have it fully working on my private cluster and it works very well,
> allowing me to make various analytics using Spark.
>
> My next planned patches will include information how to configure Swift
> for other cluster deployment of Spark and also information how to
> integrate Spark and Swift with earlier versions of Hadoop.
> I am confident that the integration between Spark and Swift is very
> important future that will  benefit greatly for the exposure of Spark.
>
> The integration between Spark and Swift is very similar to how Spark
> integrates with S3.
>
> Will be great to hear comments / suggestions / remarks from the community!
>
> All the best,
> Gil Vernik.

gil,

the sahara project within openstack is also taking on this effort.

https://wiki.openstack.org/wiki/Sahara/SparkPlugin

there's currently a plugin to provision a spark cluster on openstack and 
folks on #openstack-sahara will be very interested to hear what you're 
working on.

the theory atm is that the work done to create the swift dfs plugin will 
easily integrate spark and swift, and it's great to see that your patch 
suggests this works in practice.

best,


matt


From dev-return-8030-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jun 14 16:46:24 2014
Return-Path: <dev-return-8030-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 968FB11B17
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 14 Jun 2014 16:46:24 +0000 (UTC)
Received: (qmail 62683 invoked by uid 500); 14 Jun 2014 16:46:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62607 invoked by uid 500); 14 Jun 2014 16:46:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62596 invoked by uid 99); 14 Jun 2014 16:46:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 14 Jun 2014 16:46:23 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of gilv@il.ibm.com designates 195.75.94.113 as permitted sender)
Received: from [195.75.94.113] (HELO e06smtp17.uk.ibm.com) (195.75.94.113)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 14 Jun 2014 16:46:17 +0000
Received: from /spool/local
	by e06smtp17.uk.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <gilv@il.ibm.com>;
	Sat, 14 Jun 2014 17:45:56 +0100
Received: from d06dlp03.portsmouth.uk.ibm.com (9.149.20.15)
	by e06smtp17.uk.ibm.com (192.168.101.147) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Sat, 14 Jun 2014 17:45:54 +0100
Received: from b06cxnps4074.portsmouth.uk.ibm.com (d06relay11.portsmouth.uk.ibm.com [9.149.109.196])
	by d06dlp03.portsmouth.uk.ibm.com (Postfix) with ESMTP id 0D5CA1B08040
	for <dev@spark.apache.org>; Sat, 14 Jun 2014 17:46:21 +0100 (BST)
Received: from d06av12.portsmouth.uk.ibm.com (d06av12.portsmouth.uk.ibm.com [9.149.37.247])
	by b06cxnps4074.portsmouth.uk.ibm.com (8.13.8/8.13.8/NCO v10.0) with ESMTP id s5EGjrZC25559084
	for <dev@spark.apache.org>; Sat, 14 Jun 2014 16:45:53 GMT
Received: from d06av12.portsmouth.uk.ibm.com (localhost [127.0.0.1])
	by d06av12.portsmouth.uk.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id s5EGjrCc005188
	for <dev@spark.apache.org>; Sat, 14 Jun 2014 10:45:53 -0600
Received: from d06ml319.portsmouth.uk.ibm.com (d06ml319.portsmouth.uk.ibm.com [9.149.76.146])
	by d06av12.portsmouth.uk.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id s5EGjqaV005180
	for <dev@spark.apache.org>; Sat, 14 Jun 2014 10:45:52 -0600
In-Reply-To: <539C6FD1.2000209@redhat.com>
References: <OF4A260C43.5AF736DE-ONC2257CF1.002B1BBC-C2257CF1.002C1CC0@il.ibm.com> <539C6FD1.2000209@redhat.com>
To: dev@spark.apache.org
MIME-Version: 1.0
Subject: Re: Apache Spark and Swift object store
X-KeepSent: 379724F5:BF894F51-C2257CF7:005CBE2E;
 type=4; name=$KeepSent
X-Mailer: Lotus Notes Release 8.5.3FP4 SHF39 May 13, 2013
From: Gil Vernik <GILV@il.ibm.com>
Message-ID: <OF379724F5.BF894F51-ONC2257CF7.005CBE2E-C2257CF7.005D95D6@il.ibm.com>
Date: Sat, 14 Jun 2014 20:02:11 +0300
X-MIMETrack: Serialize by Router on D06ML319/06/M/IBM(Release 8.5.3FP5IF1HF3|November 07, 2013) at
 14/06/2014 19:45:52,
	Serialize complete at 14/06/2014 19:45:52
Content-Type: multipart/alternative; boundary="=_alternative 005D94A8C2257CF7_="
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 14061416-0542-0000-0000-0000097DD086
X-Virus-Checked: Checked by ClamAV on apache.org

--=_alternative 005D94A8C2257CF7_=
Content-Type: text/plain; charset="US-ASCII"

Hi Matthew,

Thanks for the information about Spark plugin for  Sahara.
I will send emails to Sahara and Swift mailing lists notifying them about 
this patch.

All the best,
Gil.





From:   Matthew Farrellee <matt@redhat.com>
To:     dev@spark.apache.org, 
Date:   14/06/2014 06:53 PM
Subject:        Re: Apache Spark and Swift object store



On 06/08/2014 04:01 AM, Gil Vernik wrote:
> Hello everyone,
>
> I would like to initiate discussion about integration Apache Spark and
> Openstack Swift.
> (https://issues.apache.org/jira/browse/SPARK-938 was created while ago)
>
> I created a patch (https://github.com/apache/spark/pull/1010) that
> provides initial information how to connect Swift and Spark. Currently 
it
> uses Hadoop 2.3.0 and only stand alone mode of Spark. This patch is 
mainly
> used to provide community a way to experiment with this integration.
> I have it fully working on my private cluster and it works very well,
> allowing me to make various analytics using Spark.
>
> My next planned patches will include information how to configure Swift
> for other cluster deployment of Spark and also information how to
> integrate Spark and Swift with earlier versions of Hadoop.
> I am confident that the integration between Spark and Swift is very
> important future that will  benefit greatly for the exposure of Spark.
>
> The integration between Spark and Swift is very similar to how Spark
> integrates with S3.
>
> Will be great to hear comments / suggestions / remarks from the 
community!
>
> All the best,
> Gil Vernik.

gil,

the sahara project within openstack is also taking on this effort.

https://wiki.openstack.org/wiki/Sahara/SparkPlugin

there's currently a plugin to provision a spark cluster on openstack and 
folks on #openstack-sahara will be very interested to hear what you're 
working on.

the theory atm is that the work done to create the swift dfs plugin will 
easily integrate spark and swift, and it's great to see that your patch 
suggests this works in practice.

best,


matt



--=_alternative 005D94A8C2257CF7_=--


From dev-return-8031-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun 15 01:06:48 2014
Return-Path: <dev-return-8031-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 70D461121A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 15 Jun 2014 01:06:48 +0000 (UTC)
Received: (qmail 22239 invoked by uid 500); 15 Jun 2014 01:06:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22175 invoked by uid 500); 15 Jun 2014 01:06:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22163 invoked by uid 99); 15 Jun 2014 01:06:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 15 Jun 2014 01:06:48 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.223.174 as permitted sender)
Received: from [209.85.223.174] (HELO mail-ie0-f174.google.com) (209.85.223.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 15 Jun 2014 01:06:42 +0000
Received: by mail-ie0-f174.google.com with SMTP id lx4so3626334iec.5
        for <dev@spark.apache.org>; Sat, 14 Jun 2014 18:06:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:subject:mime-version:content-type;
        bh=imAaGiWiSrQ/2S866ITIYuLWtZj2/q1pZ7Uv5juL1QE=;
        b=wlaxPQ4afGdMtSIK0rxqDVItDjUfCO88iXlfttmmBSID6j9W8o7lsO5KN+amNCbDqP
         qiJFE3nzA//tW8gpJAA6We22wUoJOIUPJ1bBrVEJ8ZxWkyLhvl1vdhsCaobVCvzmMLc7
         0iZvTF+eHI/vZXlVzgBqSD2lcjFchG4ngg60HjmOjiavKsYE2JtAKQl0w7klBzBScIev
         kmbeUCt4XZMixFEh6o/GKRGC2YiejyL1EnvHNLBHYRJg90WraMdMepTOdktkf7NYtu9z
         qk6sjW5iKOj0ACUEwr69FN8uRqJjM7xH7WSz9EQ1tY+5OlsHarBsc+vFKdIJwXC+vJ3R
         rtAg==
X-Received: by 10.50.43.232 with SMTP id z8mr15586872igl.32.1402794382176;
        Sat, 14 Jun 2014 18:06:22 -0700 (PDT)
Received: from [192.168.2.13] ([69.159.114.117])
        by mx.google.com with ESMTPSA id w5sm7994672igk.9.2014.06.14.18.06.21
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Sat, 14 Jun 2014 18:06:21 -0700 (PDT)
Date: Sat, 14 Jun 2014 21:15:40 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Message-ID: <61D9B5AD166F48F8AA5C5CD82F85EDFE@gmail.com>
Subject: review of two PRs
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="539cf3bc_2e8a6394_1d3"
X-Virus-Checked: Checked by ClamAV on apache.org

--539cf3bc_2e8a6394_1d3
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

Hi, all 

Any admin wants to review these two PRs?

https://github.com/apache/spark/pull/637 (to make DAGScheduler self-contained)

https://github.com/apache/spark/pull/731 (enable multiple executors per Worker)

Thanks 

-- 
Nan Zhu


--539cf3bc_2e8a6394_1d3--


From dev-return-8032-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 16 02:05:04 2014
Return-Path: <dev-return-8032-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 39EFC117BA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Jun 2014 02:05:04 +0000 (UTC)
Received: (qmail 11010 invoked by uid 500); 16 Jun 2014 02:05:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10949 invoked by uid 500); 16 Jun 2014 02:05:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10931 invoked by uid 99); 16 Jun 2014 02:05:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 02:05:03 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of chenguancheng@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 02:04:58 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <chenguancheng@gmail.com>)
	id 1WwMHm-0005fV-38
	for dev@spark.incubator.apache.org; Sun, 15 Jun 2014 19:04:38 -0700
Date: Sun, 15 Jun 2014 19:04:38 -0700 (PDT)
From: gchen <chenguancheng@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1402884278089-7012.post@n3.nabble.com>
In-Reply-To: <1402710044343-7007.post@n3.nabble.com>
References: <1402624426683-7003.post@n3.nabble.com> <1402710044343-7007.post@n3.nabble.com>
Subject: Re: Big-Endian (IBM Power7) Spark Serialization issue
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

To anyone who is interested in this issue, the root cause if from a third
party code com.ning.compress.lzf.impl.UnsafeChunkEncoderBE class since they
have a broken implementation. A bug will be raised in Ning project, thanks.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Big-Endian-IBM-Power7-Spark-Serialization-issue-tp7003p7012.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8033-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 16 06:50:25 2014
Return-Path: <dev-return-8033-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7305411B95
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Jun 2014 06:50:25 +0000 (UTC)
Received: (qmail 46747 invoked by uid 500); 16 Jun 2014 06:50:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46692 invoked by uid 500); 16 Jun 2014 06:50:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46672 invoked by uid 99); 16 Jun 2014 06:50:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 06:50:24 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.179] (HELO mail-qc0-f179.google.com) (209.85.216.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 06:50:19 +0000
Received: by mail-qc0-f179.google.com with SMTP id x3so5954515qcv.38
        for <dev@spark.apache.org>; Sun, 15 Jun 2014 23:49:58 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=egbx6Y2dmLoWOmII7Yik/ld6Pgn/OwJ+I8QUola2XUU=;
        b=Y0ozUVUry+jyWi4FpXMIUfuMATbDKEZngxlq39bSE4MRGuLfvawcYPH65xzsNNT5e1
         m5ff6sjkgZEzH9MC0U/QQ1zkec9K0iZiHuKY2KETaEbAAS4pfwSWk7WmEj6At/0hMpJJ
         j8KK2p84W9zo+Fy1m5stBAf0eRruChR/3bmx2NavZT18Fi+Z+QaZ0hBqDGue8/1DZOGb
         Gx7GLIzXsx7uy7eckFUaaLzandrolZaBF8iZK6/wlmVgkmEmMqIWI/HtHLNKO5aphc9D
         SJn/ffUyYb8hWHw3Ggk8orwGvi3WsuIzlmTQVQtBXuDCci90Mhf83PBCT90MvTL+SI5s
         zHaQ==
X-Gm-Message-State: ALoCoQlRb1z6Gr+bdkRboiVL5c1lLk5UiUquBGY2UMYPYV6/31SWGqYfSN3ERDp7+/W6I6t334cT
X-Received: by 10.140.81.146 with SMTP id f18mr23080290qgd.47.1402901398579;
 Sun, 15 Jun 2014 23:49:58 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Sun, 15 Jun 2014 23:49:38 -0700 (PDT)
In-Reply-To: <1402884278089-7012.post@n3.nabble.com>
References: <1402624426683-7003.post@n3.nabble.com> <1402710044343-7007.post@n3.nabble.com>
 <1402884278089-7012.post@n3.nabble.com>
From: Reynold Xin <rxin@databricks.com>
Date: Sun, 15 Jun 2014 23:49:38 -0700
Message-ID: <CAPh_B=Z9QNVR6Cyscaa6RYUd+oUDLx0iap6QnP5Aj4=uBseovw@mail.gmail.com>
Subject: Re: Big-Endian (IBM Power7) Spark Serialization issue
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1299c84bf5204fbee6f10
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1299c84bf5204fbee6f10
Content-Type: text/plain; charset=UTF-8

Thanks for sending the update. Do you mind posting a link to the bug
reported in the lzf project here as well? Cheers.



On Sun, Jun 15, 2014 at 7:04 PM, gchen <chenguancheng@gmail.com> wrote:

> To anyone who is interested in this issue, the root cause if from a third
> party code com.ning.compress.lzf.impl.UnsafeChunkEncoderBE class since they
> have a broken implementation. A bug will be raised in Ning project, thanks.
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Big-Endian-IBM-Power7-Spark-Serialization-issue-tp7003p7012.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c1299c84bf5204fbee6f10--

From dev-return-8034-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 16 06:50:26 2014
Return-Path: <dev-return-8034-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 666C611B97
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Jun 2014 06:50:26 +0000 (UTC)
Received: (qmail 46894 invoked by uid 500); 16 Jun 2014 06:50:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46695 invoked by uid 500); 16 Jun 2014 06:50:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46671 invoked by uid 99); 16 Jun 2014 06:50:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 06:50:24 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.46] (HELO mail-qa0-f46.google.com) (209.85.216.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 06:50:19 +0000
Received: by mail-qa0-f46.google.com with SMTP id i13so6702181qae.5
        for <dev@spark.incubator.apache.org>; Sun, 15 Jun 2014 23:49:58 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=egbx6Y2dmLoWOmII7Yik/ld6Pgn/OwJ+I8QUola2XUU=;
        b=C31K0BdbXSlqSwND5Y5HBpYBxElL+dUuba6gv0nwR4Ofio3DXfLMOxk3Ms4frsxTHM
         mBYJvU+EL+SbGHCiJoveN1HIxmzRAoV9HSjSdYT2HJ5O6/y5GTrY4IC9yiOzZuve8dh6
         p4N/JhlnNI4q57XXPapAljxOtXvLUytGlkt248qJ6LxzKlJSELUv9t1njuJeoFi3NpzM
         EWGd8i2m7CffRvnbitczvTFAvdv2TaGgkpJMyuBHNsWm+ySSwnl2NZ5jYm3u1zeKBf60
         8EY3PHgOV4M/+9r/6TR0X9sHQVc+4aT1lgqwnrI2IOVI3nalkyLLCfSQfNnTuOkpkib4
         LFMQ==
X-Gm-Message-State: ALoCoQlPqtCLL5reep5RB81ZU4iNOpqBTumXLmIz8lmaKX2eQnUfYPxiaeaakta+Pvd/mxHEY/L+
X-Received: by 10.140.81.146 with SMTP id f18mr23080290qgd.47.1402901398579;
 Sun, 15 Jun 2014 23:49:58 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Sun, 15 Jun 2014 23:49:38 -0700 (PDT)
In-Reply-To: <1402884278089-7012.post@n3.nabble.com>
References: <1402624426683-7003.post@n3.nabble.com> <1402710044343-7007.post@n3.nabble.com>
 <1402884278089-7012.post@n3.nabble.com>
From: Reynold Xin <rxin@databricks.com>
Date: Sun, 15 Jun 2014 23:49:38 -0700
Message-ID: <CAPh_B=Z9QNVR6Cyscaa6RYUd+oUDLx0iap6QnP5Aj4=uBseovw@mail.gmail.com>
Subject: Re: Big-Endian (IBM Power7) Spark Serialization issue
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1299c84bf5204fbee6f10
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1299c84bf5204fbee6f10
Content-Type: text/plain; charset=UTF-8

Thanks for sending the update. Do you mind posting a link to the bug
reported in the lzf project here as well? Cheers.



On Sun, Jun 15, 2014 at 7:04 PM, gchen <chenguancheng@gmail.com> wrote:

> To anyone who is interested in this issue, the root cause if from a third
> party code com.ning.compress.lzf.impl.UnsafeChunkEncoderBE class since they
> have a broken implementation. A bug will be raised in Ning project, thanks.
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Big-Endian-IBM-Power7-Spark-Serialization-issue-tp7003p7012.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c1299c84bf5204fbee6f10--

From dev-return-8035-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 16 07:09:03 2014
Return-Path: <dev-return-8035-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 393CE11C09
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Jun 2014 07:09:03 +0000 (UTC)
Received: (qmail 76132 invoked by uid 500); 16 Jun 2014 07:09:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75914 invoked by uid 500); 16 Jun 2014 07:09:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75871 invoked by uid 99); 16 Jun 2014 07:09:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 07:09:02 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of bhaskie@gmail.com designates 209.85.220.169 as permitted sender)
Received: from [209.85.220.169] (HELO mail-vc0-f169.google.com) (209.85.220.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 07:08:59 +0000
Received: by mail-vc0-f169.google.com with SMTP id la4so4626697vcb.28
        for <dev@spark.apache.org>; Mon, 16 Jun 2014 00:08:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=XWG86OMt2rCjeYll6Mmzl8mNDcc5W9vGNqAtPSXH4oA=;
        b=FC3GBx8ofvQgICr8AE2dD5LP/B/pt8+IoHDbApHw7YMKCXqd73rJgen/tOsJY/YE12
         NUgyEFcVSnfJeAixPQw0p42axGoAUV/sn0V1wuGYe48uJc1HaCq9IZUgr77gVVEdWwuX
         tuDXoeh5b8opoLobUnFwqpBbqJMbTTVgIreEW7AyGkqlwExaMlhQUzpvIU9BNWUM9wbj
         /wLSGNf/qpF845Wo6SRqlGkLMFCv031a8jPUri9M2HJ/xLEQEpJFOdzqbT6TCkfEU8c2
         SS5MSjczJnuHIqC+4d58aYv56SUtRvavlSfQW6Eo+SsTJ1dQ5T9OI6TnUAZuyXuBcahP
         lvYA==
MIME-Version: 1.0
X-Received: by 10.221.37.1 with SMTP id tc1mr137294vcb.32.1402902514481; Mon,
 16 Jun 2014 00:08:34 -0700 (PDT)
Received: by 10.220.168.69 with HTTP; Mon, 16 Jun 2014 00:08:34 -0700 (PDT)
In-Reply-To: <CABPQxsto5abq4+GbyaW6MhoWx5rZh6XvGY=a6r19Py-pReiOmw@mail.gmail.com>
References: <CABPQxsto5abq4+GbyaW6MhoWx5rZh6XvGY=a6r19Py-pReiOmw@mail.gmail.com>
Date: Mon, 16 Jun 2014 12:38:34 +0530
Message-ID: <CABEX-kEZfAXotfZaCdSXZk-HDxRmfaOOdLM1TPEHDM0U7MCT3g@mail.gmail.com>
Subject: Re: Spark 1.1 Window and 1.0 Wrap-up
From: Bhaskar Dutta <bhaskie@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11334c3807bfb004fbeeb2bf
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11334c3807bfb004fbeeb2bf
Content-Type: text/plain; charset=UTF-8

On Tue, Jun 3, 2014 at 11:51 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> Hey All,
>
> I wanted to announce the the Spark 1.1 release window:
> June 1 - Merge window opens
> July 25 - Cut-off for new pull requests
> August 1 - Merge window closes (code freeze), QA period starts
> August 15+ - RC's and voting
>
>
Hi,

Do you have a plan for 1.0.1 as well?

Thanks,
Bhaskar

--001a11334c3807bfb004fbeeb2bf--

From dev-return-8036-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 16 09:50:46 2014
Return-Path: <dev-return-8036-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D179B11209
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Jun 2014 09:50:46 +0000 (UTC)
Received: (qmail 92972 invoked by uid 500); 16 Jun 2014 09:50:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92914 invoked by uid 500); 16 Jun 2014 09:50:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92902 invoked by uid 99); 16 Jun 2014 09:50:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 09:50:45 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.216.46 as permitted sender)
Received: from [209.85.216.46] (HELO mail-qa0-f46.google.com) (209.85.216.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 09:50:43 +0000
Received: by mail-qa0-f46.google.com with SMTP id i13so6973840qae.19
        for <dev@spark.apache.org>; Mon, 16 Jun 2014 02:50:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=aMAvYxHDF27j797b3/RwQoMVkEYtMghuMAVe2ymp12A=;
        b=hPyCeFwv29KrdpTuoYYhM/7Q4jBVshnVD8P1lnWj4PvrrBaJpMyo/IJWgfoupumS10
         MHI5dcez+d4nRLn0iQc+Ba5/UVJ7CxTKTCR0m/Ibl4H/Qm+QBfUatxXVZmtUfketk8Fy
         demoj3L9c8Ehhxfn4SfuNRnuw7LweXkw32cMZIi2Rl5mvH51nt/TDewSGPoxHUSId4Ww
         2gEIIDijmNyt5WMM/MPD6BuXeUjwXbb187Ug/n9lrqfUBkUz6UbsRR8uBZK1QXQODmDD
         Za+W36phMhwXgR9f1m67Pb2neKXrKXqHdxBHSsF8qW3hIA3pb7awCyOZdPkIpku2Czn0
         2Zdg==
MIME-Version: 1.0
X-Received: by 10.140.81.74 with SMTP id e68mr4174833qgd.77.1402912219042;
 Mon, 16 Jun 2014 02:50:19 -0700 (PDT)
Received: by 10.140.38.149 with HTTP; Mon, 16 Jun 2014 02:50:18 -0700 (PDT)
In-Reply-To: <1402884278089-7012.post@n3.nabble.com>
References: <1402624426683-7003.post@n3.nabble.com>
	<1402710044343-7007.post@n3.nabble.com>
	<1402884278089-7012.post@n3.nabble.com>
Date: Mon, 16 Jun 2014 15:20:18 +0530
Message-ID: <CAJiQeYKKAd=b_q-NzGZy8TY1N8=HgtRym2T0TcDn8_+cXZfyNA@mail.gmail.com>
Subject: Re: Big-Endian (IBM Power7) Spark Serialization issue
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

In that case, does it work if you use snappy instead of lzf ?


Regards,
Mridul


On Mon, Jun 16, 2014 at 7:34 AM, gchen <chenguancheng@gmail.com> wrote:
> To anyone who is interested in this issue, the root cause if from a third
> party code com.ning.compress.lzf.impl.UnsafeChunkEncoderBE class since they
> have a broken implementation. A bug will be raised in Ning project, thanks.
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Big-Endian-IBM-Power7-Spark-Serialization-issue-tp7003p7012.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8037-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 16 23:15:49 2014
Return-Path: <dev-return-8037-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4E1B911EF7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Jun 2014 23:15:49 +0000 (UTC)
Received: (qmail 50490 invoked by uid 500); 16 Jun 2014 23:15:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50422 invoked by uid 500); 16 Jun 2014 23:15:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50404 invoked by uid 99); 16 Jun 2014 23:15:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 23:15:48 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of chenguancheng@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 23:15:46 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <chenguancheng@gmail.com>)
	id 1Wwg7W-0006Sh-Bo
	for dev@spark.incubator.apache.org; Mon, 16 Jun 2014 16:15:22 -0700
Date: Mon, 16 Jun 2014 16:15:22 -0700 (PDT)
From: gchen <chenguancheng@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1402960522356-7016.post@n3.nabble.com>
In-Reply-To: <CAPh_B=Z9QNVR6Cyscaa6RYUd+oUDLx0iap6QnP5Aj4=uBseovw@mail.gmail.com>
References: <1402624426683-7003.post@n3.nabble.com> <1402710044343-7007.post@n3.nabble.com> <1402884278089-7012.post@n3.nabble.com> <CAPh_B=Z9QNVR6Cyscaa6RYUd+oUDLx0iap6QnP5Aj4=uBseovw@mail.gmail.com>
Subject: Re: Big-Endian (IBM Power7) Spark Serialization issue
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Reynold, thanks for your interest on this issue. The work here is part of
incorporating Spark into PowerLinux ecosystem. 

Here is the bug raised in ning by my colleague:
https://github.com/ning/compress/issues/37

Would you mind to share whether some insights of Spark's support for Big
Enidan Arch (such as POWER7)? Has that been fully tested before? Thanks.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Big-Endian-IBM-Power7-Spark-Serialization-issue-tp7003p7016.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8038-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 16 23:18:59 2014
Return-Path: <dev-return-8038-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 36CCB11F0B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Jun 2014 23:18:59 +0000 (UTC)
Received: (qmail 54696 invoked by uid 500); 16 Jun 2014 23:18:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54640 invoked by uid 500); 16 Jun 2014 23:18:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54619 invoked by uid 99); 16 Jun 2014 23:18:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 23:18:58 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.177] (HELO mail-qc0-f177.google.com) (209.85.216.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 23:18:55 +0000
Received: by mail-qc0-f177.google.com with SMTP id r5so5225520qcx.8
        for <dev@spark.apache.org>; Mon, 16 Jun 2014 16:18:30 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=wZLsEBdgu6txOaG2orB/nSQNyZ0lNUycFR9JHoTRJsw=;
        b=RMXamGZnj0O8SdV0JcpRmc4KmRhDkvUcYtuIGarOTj0YHuDNbf+XLTcvb6n70UiQpC
         Kadk5rgz9n8e/JhFIiIf7t6tYnQp3/4gcK3yeOtOZOHY+sjNImr0Sb2osiU2ze2pEa5/
         qKLbsPemzsSWsNn75vX26y05X8IIOMdbAlwnRf6CZBqtR7hWVdN2EYR4QsxR3//RX84k
         grttkj5tF+7sN3L5UOQOO94gw4urz+QzSi7kVM0DezcR/QeKb8iHZYaLJUi8x6V3G4kI
         +rLiNRvbLWSjA02jtXVrOTtLrLKvYCEL5fGcJw2wDcOYbhkIqWq6+Uc2pUNsHImqn7YV
         BB/Q==
X-Gm-Message-State: ALoCoQldaGO1enkHC/5cw/+4Ww8xjlPCVfobq4R2DUl7CXoJCgR8uL8h2gvAu1jDMYv4VvT/jeh1
X-Received: by 10.229.79.2 with SMTP id n2mr31164660qck.11.1402960710291; Mon,
 16 Jun 2014 16:18:30 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Mon, 16 Jun 2014 16:18:10 -0700 (PDT)
In-Reply-To: <1402960522356-7016.post@n3.nabble.com>
References: <1402624426683-7003.post@n3.nabble.com> <1402710044343-7007.post@n3.nabble.com>
 <1402884278089-7012.post@n3.nabble.com> <CAPh_B=Z9QNVR6Cyscaa6RYUd+oUDLx0iap6QnP5Aj4=uBseovw@mail.gmail.com>
 <1402960522356-7016.post@n3.nabble.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 16 Jun 2014 16:18:10 -0700
Message-ID: <CAPh_B=bbt3s1npkk4_YTWCY6r3z=xPQ7evOPFEKoqVi176OM-w@mail.gmail.com>
Subject: Re: Big-Endian (IBM Power7) Spark Serialization issue
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a1133a1a0c56a2f04fbfc3ecc
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133a1a0c56a2f04fbfc3ecc
Content-Type: text/plain; charset=UTF-8

I think you guys are / will be leading the effort on that :)



On Mon, Jun 16, 2014 at 4:15 PM, gchen <chenguancheng@gmail.com> wrote:

> Hi Reynold, thanks for your interest on this issue. The work here is part
> of
> incorporating Spark into PowerLinux ecosystem.
>
> Here is the bug raised in ning by my colleague:
> https://github.com/ning/compress/issues/37
>
> Would you mind to share whether some insights of Spark's support for Big
> Enidan Arch (such as POWER7)? Has that been fully tested before? Thanks.
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Big-Endian-IBM-Power7-Spark-Serialization-issue-tp7003p7016.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a1133a1a0c56a2f04fbfc3ecc--

From dev-return-8039-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 16 23:18:59 2014
Return-Path: <dev-return-8039-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 746FB11F0C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Jun 2014 23:18:59 +0000 (UTC)
Received: (qmail 55121 invoked by uid 500); 16 Jun 2014 23:18:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55002 invoked by uid 500); 16 Jun 2014 23:18:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54629 invoked by uid 99); 16 Jun 2014 23:18:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 23:18:58 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.179] (HELO mail-qc0-f179.google.com) (209.85.216.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 23:18:56 +0000
Received: by mail-qc0-f179.google.com with SMTP id x3so7644306qcv.38
        for <dev@spark.incubator.apache.org>; Mon, 16 Jun 2014 16:18:30 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=wZLsEBdgu6txOaG2orB/nSQNyZ0lNUycFR9JHoTRJsw=;
        b=biNU69GOD68fztA98i4GgVHOSlM/3yy8Uc51PyUi2Mw4F4tvgQTjjQfGL5VP4xK02N
         XK6MB34HVxt6waCEWSfxN8Wj6ok2zEkTMkhUZquuz1wOmDN88DeH+NQwjikKQe5r0zFE
         qRDKVUp0ig8Gy1rPQSJ9L3L6JWyvgvgr0/nY66vrRPLdUjBt5mNn19Y8XTUcaZex6K7p
         HFWPdnWXUAxCW4E8pB6ERLEwQu37DIjnZpuy87tZ/017UC08Cl8XQvG48nARm0MEVMS7
         NokImahlicSJ3HeDh5d4Xr+1I0wutQUSEddfcgFeINFSFFcVcBrzmQBhbz/YEP1k41zN
         Ogyg==
X-Gm-Message-State: ALoCoQlSN1UOCtxDiDs4r+Ed9nXTuRRmFv1+8yHt7aMdRQeujaaJmtSQGqw1YpC0XV+MIKRazU6n
X-Received: by 10.229.79.2 with SMTP id n2mr31164660qck.11.1402960710291; Mon,
 16 Jun 2014 16:18:30 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Mon, 16 Jun 2014 16:18:10 -0700 (PDT)
In-Reply-To: <1402960522356-7016.post@n3.nabble.com>
References: <1402624426683-7003.post@n3.nabble.com> <1402710044343-7007.post@n3.nabble.com>
 <1402884278089-7012.post@n3.nabble.com> <CAPh_B=Z9QNVR6Cyscaa6RYUd+oUDLx0iap6QnP5Aj4=uBseovw@mail.gmail.com>
 <1402960522356-7016.post@n3.nabble.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 16 Jun 2014 16:18:10 -0700
Message-ID: <CAPh_B=bbt3s1npkk4_YTWCY6r3z=xPQ7evOPFEKoqVi176OM-w@mail.gmail.com>
Subject: Re: Big-Endian (IBM Power7) Spark Serialization issue
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a1133a1a0c56a2f04fbfc3ecc
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133a1a0c56a2f04fbfc3ecc
Content-Type: text/plain; charset=UTF-8

I think you guys are / will be leading the effort on that :)



On Mon, Jun 16, 2014 at 4:15 PM, gchen <chenguancheng@gmail.com> wrote:

> Hi Reynold, thanks for your interest on this issue. The work here is part
> of
> incorporating Spark into PowerLinux ecosystem.
>
> Here is the bug raised in ning by my colleague:
> https://github.com/ning/compress/issues/37
>
> Would you mind to share whether some insights of Spark's support for Big
> Enidan Arch (such as POWER7)? Has that been fully tested before? Thanks.
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Big-Endian-IBM-Power7-Spark-Serialization-issue-tp7003p7016.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a1133a1a0c56a2f04fbfc3ecc--

From dev-return-8040-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 16 23:23:07 2014
Return-Path: <dev-return-8040-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9BB5A11F26
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Jun 2014 23:23:07 +0000 (UTC)
Received: (qmail 62300 invoked by uid 500); 16 Jun 2014 23:23:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62264 invoked by uid 500); 16 Jun 2014 23:23:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62253 invoked by uid 99); 16 Jun 2014 23:23:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 23:23:05 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of chenguancheng@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 23:23:03 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <chenguancheng@gmail.com>)
	id 1WwgEZ-0006mi-FF
	for dev@spark.incubator.apache.org; Mon, 16 Jun 2014 16:22:39 -0700
Date: Mon, 16 Jun 2014 16:22:39 -0700 (PDT)
From: gchen <chenguancheng@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1402960959463-7018.post@n3.nabble.com>
In-Reply-To: <CAPh_B=bbt3s1npkk4_YTWCY6r3z=xPQ7evOPFEKoqVi176OM-w@mail.gmail.com>
References: <1402624426683-7003.post@n3.nabble.com> <1402710044343-7007.post@n3.nabble.com> <1402884278089-7012.post@n3.nabble.com> <CAPh_B=Z9QNVR6Cyscaa6RYUd+oUDLx0iap6QnP5Aj4=uBseovw@mail.gmail.com> <1402960522356-7016.post@n3.nabble.com> <CAPh_B=bbt3s1npkk4_YTWCY6r3z=xPQ7evOPFEKoqVi176OM-w@mail.gmail.com>
Subject: Re: Big-Endian (IBM Power7) Spark Serialization issue
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Surely the community's kind support is essential:)



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Big-Endian-IBM-Power7-Spark-Serialization-issue-tp7003p7018.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8041-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 16 23:26:33 2014
Return-Path: <dev-return-8041-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 83E0D11F49
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Jun 2014 23:26:33 +0000 (UTC)
Received: (qmail 69635 invoked by uid 500); 16 Jun 2014 23:26:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69582 invoked by uid 500); 16 Jun 2014 23:26:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69571 invoked by uid 99); 16 Jun 2014 23:26:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 23:26:32 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of chenguancheng@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 23:26:28 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <chenguancheng@gmail.com>)
	id 1WwgHv-0006x5-Uv
	for dev@spark.incubator.apache.org; Mon, 16 Jun 2014 16:26:07 -0700
Date: Mon, 16 Jun 2014 16:26:07 -0700 (PDT)
From: gchen <chenguancheng@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1402961167947-7019.post@n3.nabble.com>
In-Reply-To: <CAPh_B=bbt3s1npkk4_YTWCY6r3z=xPQ7evOPFEKoqVi176OM-w@mail.gmail.com>
References: <1402624426683-7003.post@n3.nabble.com> <1402710044343-7007.post@n3.nabble.com> <1402884278089-7012.post@n3.nabble.com> <CAPh_B=Z9QNVR6Cyscaa6RYUd+oUDLx0iap6QnP5Aj4=uBseovw@mail.gmail.com> <1402960522356-7016.post@n3.nabble.com> <CAPh_B=bbt3s1npkk4_YTWCY6r3z=xPQ7evOPFEKoqVi176OM-w@mail.gmail.com>
Subject: Re: Big-Endian (IBM Power7) Spark Serialization issue
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I didn't find ning's source code in Spark git repository (or maybe I missed
it?), so next time when we meet bug caused by third party code, can we do
something (to fix the bug) based on the Spark repository?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Big-Endian-IBM-Power7-Spark-Serialization-issue-tp7003p7019.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8042-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 16 23:28:27 2014
Return-Path: <dev-return-8042-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A770811F6D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Jun 2014 23:28:27 +0000 (UTC)
Received: (qmail 76315 invoked by uid 500); 16 Jun 2014 23:28:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76249 invoked by uid 500); 16 Jun 2014 23:28:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75905 invoked by uid 99); 16 Jun 2014 23:28:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 23:28:26 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.175] (HELO mail-qc0-f175.google.com) (209.85.216.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Jun 2014 23:28:24 +0000
Received: by mail-qc0-f175.google.com with SMTP id i8so8824627qcq.20
        for <dev@spark.apache.org>; Mon, 16 Jun 2014 16:28:00 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=3QSX1BGM65W1skv7iFSs29LDtHPvgntQ0fMBzA+Y7SU=;
        b=fte9009CFdNIFu8CMtYRr6n1iSnSxO83U5R3IWer1jOR/hAEazHn/EumLhjWp0THHb
         Ul4Q4Ob2T9L2gOZZkSIOTQr3FW/qPPsjUD59fZHwzfMYYilT7C7/xFwH5MK1L2oUH4pq
         FgnpTvL8hgGHeTa4rULol3F+Ogcbkyu5SV5P/cApd8RpFCs+H/i+FieLmycNJE5zS5nw
         LSFvkf5ht2bPjKIr5kerzI0ZXA3C5nqxfe6z4+Hw9o75BMvRgXrpSftpWDwu7CeJ13qZ
         3lQ9LPj2W8geZ9SI2DWw10srJBo/wTON/OyhWnYSC5qLqjkJuaKpu3g9JZ1qJxRHDOix
         k3+A==
X-Gm-Message-State: ALoCoQmUzrsG90UE2UnCVTWOOCXZmHShIIzj1XDp+wvcV34nZ4x4MZN6qIbNgr5Y7hj+7GxZ81A7
X-Received: by 10.140.95.142 with SMTP id i14mr29015068qge.6.1402961280142;
 Mon, 16 Jun 2014 16:28:00 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Mon, 16 Jun 2014 16:27:40 -0700 (PDT)
In-Reply-To: <1402961167947-7019.post@n3.nabble.com>
References: <1402624426683-7003.post@n3.nabble.com> <1402710044343-7007.post@n3.nabble.com>
 <1402884278089-7012.post@n3.nabble.com> <CAPh_B=Z9QNVR6Cyscaa6RYUd+oUDLx0iap6QnP5Aj4=uBseovw@mail.gmail.com>
 <1402960522356-7016.post@n3.nabble.com> <CAPh_B=bbt3s1npkk4_YTWCY6r3z=xPQ7evOPFEKoqVi176OM-w@mail.gmail.com>
 <1402961167947-7019.post@n3.nabble.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 16 Jun 2014 16:27:40 -0700
Message-ID: <CAPh_B=aTo7DFjd8+yJ3nub54i0TRRwKh6xK-52W=8HYtwyfYxA@mail.gmail.com>
Subject: Re: Big-Endian (IBM Power7) Spark Serialization issue
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c168febc995804fbfc6031
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c168febc995804fbfc6031
Content-Type: text/plain; charset=UTF-8

It is here:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala


On Mon, Jun 16, 2014 at 4:26 PM, gchen <chenguancheng@gmail.com> wrote:

> I didn't find ning's source code in Spark git repository (or maybe I missed
> it?), so next time when we meet bug caused by third party code, can we do
> something (to fix the bug) based on the Spark repository?
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Big-Endian-IBM-Power7-Spark-Serialization-issue-tp7003p7019.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c168febc995804fbfc6031--

From dev-return-8043-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 17 04:25:07 2014
Return-Path: <dev-return-8043-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 75E36116A7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 17 Jun 2014 04:25:07 +0000 (UTC)
Received: (qmail 41737 invoked by uid 500); 17 Jun 2014 04:25:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41676 invoked by uid 500); 17 Jun 2014 04:25:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41664 invoked by uid 99); 17 Jun 2014 04:25:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 04:25:06 +0000
X-ASF-Spam-Status: No, hits=1.8 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of liqingyang1985@gmail.com designates 74.125.82.50 as permitted sender)
Received: from [74.125.82.50] (HELO mail-wg0-f50.google.com) (74.125.82.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 04:25:02 +0000
Received: by mail-wg0-f50.google.com with SMTP id x13so6373726wgg.21
        for <dev@spark.apache.org>; Mon, 16 Jun 2014 21:24:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=kFKSGYqG57l4H3yZwvFS7gepJ2Xa+30LlVIG7FP7lds=;
        b=UrEHvAdoDiJC0jf0rs1Jm472qartGX1LxV8R3JrYwXzD1K0zPJE9M7EqnJsRVmoQSb
         cs9PC11o1HXkg0OAYGLVz8pSlvsV7LweKAe8hPqj7zZNBSFoi+sARJ/uPhw50f74WfeT
         fGNaERpR/cFeH7oksrhhLxMj4XDUystobQLxXCTl/1EhM0HafZ/QO9PVZ3y7hYwLDrKM
         G0jZM5K4E4VFr+hmiG9r0yen9+LrTNUsy8GeBFvNEuYUUdSn8OG3I9sP6eEvXJp2Yfrq
         BUheGQjPnk7QQPAB7hN8Hd9iRevNdFswFX/ptRrl6Cn+QFBei07bP3zfdicQip7Bo1En
         Lo6Q==
MIME-Version: 1.0
X-Received: by 10.194.60.35 with SMTP id e3mr34565140wjr.12.1402979081056;
 Mon, 16 Jun 2014 21:24:41 -0700 (PDT)
Received: by 10.194.22.2 with HTTP; Mon, 16 Jun 2014 21:24:40 -0700 (PDT)
Date: Tue, 17 Jun 2014 12:24:40 +0800
Message-ID: <CABDsqqabX9k_POqY4SZ57qKuA5G5fH2zZ1Ujb7iDyk_U5th=uA@mail.gmail.com>
Subject: encounter jvm problem when integreation spark with mesos
From: qingyang li <liqingyang1985@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b86e8bac0f7f804fc008515
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b86e8bac0f7f804fc008515
Content-Type: text/plain; charset=UTF-8

hi, I encounter  jvm problem when integreation spark with mesos,
here is the log when i run "spark-shell":
-48ce131dc5af
14/06/17 12:24:55 INFO HttpServer: Starting HTTP Server
14/06/17 12:24:55 INFO SparkUI: Started Spark Web UI at
http://bigdata001:4040
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f94f4843d21, pid=5956, tid=140277175580416
#
# JRE version: OpenJDK Runtime Environment (7.0_51-b02) (build
1.7.0_51-mockbuild_2014_01_15_01_39-b00)
# Java VM: OpenJDK 64-Bit Server VM (24.45-b08 mixed mode linux-amd64
compressed oops)
# Problematic frame:
# V  [libjvm.so+0x5e5d21]  JNI_CreateJavaVM+0x6551
#
# Core dump written. Default location:
/home/zjw/spark/spark-0.9.0-incubating-bin-hadoop2/core or core.5956
#
# An error report file with more information is saved as:
# /tmp/jvm-5956/hs_error.log
#
# If you would like to submit a bug report, please include
# instructions on how to reproduce the bug and visit:
#   http://icedtea.classpath.org/bugzilla
#
bin/spark-shell: line 101:  5956 Aborted                 (core dumped)
$FWDIR/bin/spark-class $OPTIONS org.apache.spark.repl.Main "$@"

--047d7b86e8bac0f7f804fc008515--

From dev-return-8044-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 17 04:30:41 2014
Return-Path: <dev-return-8044-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 728F5116AD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 17 Jun 2014 04:30:41 +0000 (UTC)
Received: (qmail 50723 invoked by uid 500); 17 Jun 2014 04:30:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50663 invoked by uid 500); 17 Jun 2014 04:30:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50651 invoked by uid 99); 17 Jun 2014 04:30:40 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 04:30:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.128.182] (HELO mail-ve0-f182.google.com) (209.85.128.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 04:30:38 +0000
Received: by mail-ve0-f182.google.com with SMTP id oy12so5231274veb.27
        for <dev@spark.apache.org>; Mon, 16 Jun 2014 21:30:12 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=FyQU+2cocCSoxDf52lZ606yAZp5HEUvoVHS6PpK9NHU=;
        b=fRxBWhn3pzoVcKm8n1wX6Uy1CBXwLlrTyJbzZffoBA1TrGtxVAls0h6XxWUoNj49DO
         7Jo/nBxDPD6iDj7eL2rtjI2IClaLZceDDJQTtdHWeOf+9mLgcG+mgoP0oW4TBK+WU2Ci
         OVa30D4ouzPiHGh8L84I9/9YfGzhVLn/e7+WsBNgGNoASunUep3stbOEnVzNaQry9TwY
         0DweV8jVSwPMz8ln6pvtXGk7cnKUx+jGuVPPp9FUwYfM/QLcEVe5dAANNfMEHMnbTTuT
         rkCK81HSb6ENn74nBE8lPQe8XBWfGtSNKPIY7LA85SO8SUNq3zdHj+6G+SP+1YiUBTI3
         UB+A==
X-Gm-Message-State: ALoCoQmVDVVp0g6VVPRKunFAVU8GcUAnTkiK7XoygHjSW7XgCc2FMfrwQ6VJFECipHEPI+UrIyyf
X-Received: by 10.58.29.16 with SMTP id f16mr9478646veh.23.1402979411987;
        Mon, 16 Jun 2014 21:30:11 -0700 (PDT)
Received: from mail-ve0-f169.google.com (mail-ve0-f169.google.com [209.85.128.169])
        by mx.google.com with ESMTPSA id p8sm14925136veb.0.2014.06.16.21.30.10
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 16 Jun 2014 21:30:10 -0700 (PDT)
Received: by mail-ve0-f169.google.com with SMTP id pa12so7181201veb.28
        for <dev@spark.apache.org>; Mon, 16 Jun 2014 21:30:10 -0700 (PDT)
X-Received: by 10.58.230.101 with SMTP id sx5mr20056498vec.10.1402979410499;
 Mon, 16 Jun 2014 21:30:10 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Mon, 16 Jun 2014 21:29:50 -0700 (PDT)
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 16 Jun 2014 21:29:50 -0700
Message-ID: <CA+-p3AGJNA8y5Ss8fYBho_yqHA7iqzrDCNn4_4HSqLuCL_FF+A@mail.gmail.com>
Subject: Compile failure with SBT on master
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bdc853c63e2bd04fc00990c
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc853c63e2bd04fc00990c
Content-Type: text/plain; charset=UTF-8

I can't run sbt/sbt gen-idea on a clean checkout of Spark master.

I get resolution errors on junit#junit;4.10!junit.zip(source)

As shown below:

aash@aash-mbp /tmp/git/spark$ sbt/sbt gen-idea
Using /Library/Java/JavaVirtualMachines/jdk1.7.0_45.jdk/Contents/Home as
default JAVA_HOME.
Note, this will be overridden by -java-home if it is set.
[info] Loading project definition from
/private/tmp/git/spark/project/project
[info] Loading project definition from /private/tmp/git/spark/project
[info] Set current project to root (in build file:/private/tmp/git/spark/)
[info] Creating IDEA module for project 'assembly' ...
[info] Updating {file:/private/tmp/git/spark/}core...
[info] Resolving org.fusesource.jansi#jansi;1.4 ...
[warn] [FAILED     ] junit#junit;4.10!junit.zip(source):  (0ms)
[warn] ==== local: tried
[warn]   /Users/aash/.ivy2/local/junit/junit/4.10/sources/junit.zip
[warn] ==== public: tried
[warn]   http://repo1.maven.org/maven2/junit/junit/4.10/junit-4.10.zip
[warn] ==== Maven Repository: tried
[warn]   http://repo.maven.apache.org/maven2/junit/junit/4.10/junit-4.10.zip
[warn] ==== Apache Repository: tried
[warn]
https://repository.apache.org/content/repositories/releases/junit/junit/4.10/junit-4.10.zip
[warn] ==== JBoss Repository: tried
[warn]
https://repository.jboss.org/nexus/content/repositories/releases/junit/junit/4.10/junit-4.10.zip
[warn] ==== MQTT Repository: tried
[warn]
https://repo.eclipse.org/content/repositories/paho-releases/junit/junit/4.10/junit-4.10.zip
[warn] ==== Cloudera Repository: tried
[warn]
http://repository.cloudera.com/artifactory/cloudera-repos/junit/junit/4.10/junit-4.10.zip
[warn] ==== Pivotal Repository: tried
[warn]   http://repo.spring.io/libs-release/junit/junit/4.10/junit-4.10.zip
[warn] ==== Maven2 Local: tried
[warn]   file:/Users/aash/.m2/repository/junit/junit/4.10/junit-4.10.zip
[warn] ::::::::::::::::::::::::::::::::::::::::::::::
[warn] ::              FAILED DOWNLOADS            ::
[warn] :: ^ see resolution messages for details  ^ ::
[warn] ::::::::::::::::::::::::::::::::::::::::::::::
[warn] :: junit#junit;4.10!junit.zip(source)
[warn] ::::::::::::::::::::::::::::::::::::::::::::::
sbt.ResolveException: download failed: junit#junit;4.10!junit.zip(source)

By bumping the junit dependency to 4.11 I'm able to generate the IDE files.
 Are other people having this problem or does everyone use the maven
configuration?

Andrew

--047d7bdc853c63e2bd04fc00990c--

From dev-return-8045-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 17 04:32:08 2014
Return-Path: <dev-return-8045-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2BB93116BA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 17 Jun 2014 04:32:08 +0000 (UTC)
Received: (qmail 55527 invoked by uid 500); 17 Jun 2014 04:32:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55474 invoked by uid 500); 17 Jun 2014 04:32:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55463 invoked by uid 99); 17 Jun 2014 04:32:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 04:32:07 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.220.173] (HELO mail-vc0-f173.google.com) (209.85.220.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 04:32:02 +0000
Received: by mail-vc0-f173.google.com with SMTP id lf12so5847905vcb.18
        for <dev@spark.apache.org>; Mon, 16 Jun 2014 21:31:41 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=OdbhPzudLXdWpE3N89DWaaAYiXcRjEsl4HUYnuX5V3o=;
        b=IPj6wJIP8hOWJQ0G2yfsRyX3HT3Cbwi9in9leX8HzLpMjGT9xuxiq9r7mIFhlxWAl0
         H/bGQvcPjHmX5cjGV82e8DbnXvf1tzjwS4IdtHQkH3cDdSo3mV4jTJpxn4FCgqz1Pd2o
         eRfDA5BsacBAKoFmXiaT9UjP1f90UozX3HpIdVNAZYowjcUeGKOTxpbjrORyGDvVZxfN
         IDHhNG9gt6mMh1jtibrrnvjc1CPryQCDdvitze988EVwcCt1mHDBgGAK5v0w63JGZTDz
         fZF0d9KLp2RBJd1lsfKJGfXFZ5xU+c42UpEa5HW7LNK+09cHHZ1nd5SF6O3JQWJFslAw
         EWJg==
X-Gm-Message-State: ALoCoQlGq2jwjciSbGVMB2qcf5lek3gTVGwPtOqP92RhmqTujbIyFlDIW+P/FI06c1T8p9TcVbBO
X-Received: by 10.52.246.42 with SMTP id xt10mr16264931vdc.5.1402979501282;
        Mon, 16 Jun 2014 21:31:41 -0700 (PDT)
Received: from mail-vc0-f181.google.com (mail-vc0-f181.google.com [209.85.220.181])
        by mx.google.com with ESMTPSA id p8sm14928562veb.0.2014.06.16.21.31.40
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 16 Jun 2014 21:31:40 -0700 (PDT)
Received: by mail-vc0-f181.google.com with SMTP id il7so5940574vcb.12
        for <dev@spark.apache.org>; Mon, 16 Jun 2014 21:31:39 -0700 (PDT)
X-Received: by 10.53.8.162 with SMTP id dl2mr16405543vdd.24.1402979499764;
 Mon, 16 Jun 2014 21:31:39 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Mon, 16 Jun 2014 21:31:19 -0700 (PDT)
In-Reply-To: <CABDsqqabX9k_POqY4SZ57qKuA5G5fH2zZ1Ujb7iDyk_U5th=uA@mail.gmail.com>
References: <CABDsqqabX9k_POqY4SZ57qKuA5G5fH2zZ1Ujb7iDyk_U5th=uA@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 16 Jun 2014 21:31:19 -0700
Message-ID: <CA+-p3AFpOMc62-OxWy_1ujF4Uw2zxMzoBew-wdUEJAaP2e=-cg@mail.gmail.com>
Subject: Re: encounter jvm problem when integreation spark with mesos
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1135e18cb5f5eb04fc009e87
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1135e18cb5f5eb04fc009e87
Content-Type: text/plain; charset=UTF-8

Hi qingyang,

This looks like an issue with the open source version of the Java runtime
(called OpenJDK) that causes the JVM to fail.  Can you try using the JVM
released by Oracle and see if it has the same issue?

Thanks!
Andrew


On Mon, Jun 16, 2014 at 9:24 PM, qingyang li <liqingyang1985@gmail.com>
wrote:

> hi, I encounter  jvm problem when integreation spark with mesos,
> here is the log when i run "spark-shell":
> -48ce131dc5af
> 14/06/17 12:24:55 INFO HttpServer: Starting HTTP Server
> 14/06/17 12:24:55 INFO SparkUI: Started Spark Web UI at
> http://bigdata001:4040
> #
> # A fatal error has been detected by the Java Runtime Environment:
> #
> #  SIGSEGV (0xb) at pc=0x00007f94f4843d21, pid=5956, tid=140277175580416
> #
> # JRE version: OpenJDK Runtime Environment (7.0_51-b02) (build
> 1.7.0_51-mockbuild_2014_01_15_01_39-b00)
> # Java VM: OpenJDK 64-Bit Server VM (24.45-b08 mixed mode linux-amd64
> compressed oops)
> # Problematic frame:
> # V  [libjvm.so+0x5e5d21]  JNI_CreateJavaVM+0x6551
> #
> # Core dump written. Default location:
> /home/zjw/spark/spark-0.9.0-incubating-bin-hadoop2/core or core.5956
> #
> # An error report file with more information is saved as:
> # /tmp/jvm-5956/hs_error.log
> #
> # If you would like to submit a bug report, please include
> # instructions on how to reproduce the bug and visit:
> #   http://icedtea.classpath.org/bugzilla
> #
> bin/spark-shell: line 101:  5956 Aborted                 (core dumped)
> $FWDIR/bin/spark-class $OPTIONS org.apache.spark.repl.Main "$@"
>

--001a1135e18cb5f5eb04fc009e87--

From dev-return-8046-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 17 04:58:14 2014
Return-Path: <dev-return-8046-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AD0AB1173F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 17 Jun 2014 04:58:14 +0000 (UTC)
Received: (qmail 80244 invoked by uid 500); 17 Jun 2014 04:58:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80177 invoked by uid 500); 17 Jun 2014 04:58:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80160 invoked by uid 99); 17 Jun 2014 04:58:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 04:58:13 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.43 as permitted sender)
Received: from [209.85.213.43] (HELO mail-yh0-f43.google.com) (209.85.213.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 04:58:10 +0000
Received: by mail-yh0-f43.google.com with SMTP id a41so5109725yho.16
        for <dev@spark.apache.org>; Mon, 16 Jun 2014 21:57:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=herlfcg8bJbz9wmPJsrLXHk4g1g9hRH2N3O9zpawdwc=;
        b=mYoEWegA5FRxp0k84+wawRiuNcJrWmACJaqadM1db1OOxXmeo+5Dq27ZFtCuFautPl
         H2H3gsph4RsSTdeIIxHNpyKXaDaoStumlHuu9m1jISljmlLh9XJvKKEj/Ke+tSauQYK5
         oQF0RJFAXLwJD+Yaokk3QznGbRsrAg/O6BSwAZdA2m0rU9z0fSNL27KKy1mZl2VxgPVI
         CbD4FalHB5JqWWAMuQnxqnU/hqdWZkaZ1ExJDGPI6OJDLCRt8xOaSpCXDMubaGa7eN3y
         4iofyx0zjTlQukMZCSxzsUpFvikW+xTmM4iHqvOLzBZYyQHT3KbHFJ9iEltl3dMMMpXU
         Sn2g==
MIME-Version: 1.0
X-Received: by 10.236.159.67 with SMTP id r43mr41704704yhk.50.1402981069148;
 Mon, 16 Jun 2014 21:57:49 -0700 (PDT)
Received: by 10.170.55.137 with HTTP; Mon, 16 Jun 2014 21:57:49 -0700 (PDT)
In-Reply-To: <CA+-p3AGJNA8y5Ss8fYBho_yqHA7iqzrDCNn4_4HSqLuCL_FF+A@mail.gmail.com>
References: <CA+-p3AGJNA8y5Ss8fYBho_yqHA7iqzrDCNn4_4HSqLuCL_FF+A@mail.gmail.com>
Date: Mon, 16 Jun 2014 21:57:49 -0700
Message-ID: <CALte62yQPA+k9i0XOM7Zww4WhL0MA-4OHmOTZ0=Ck2iWmenn_A@mail.gmail.com>
Subject: Re: Compile failure with SBT on master
From: Ted Yu <yuzhihong@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=20cf30434c6440d9bb04fc00fc0c
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf30434c6440d9bb04fc00fc0c
Content-Type: text/plain; charset=UTF-8

I used the same command on Linux and it passed:

Linux k.net 2.6.32-220.23.1.el6.YAHOO.20120713.x86_64 #1 SMP Fri Jul 13
11:40:51 CDT 2012 x86_64 x86_64 x86_64 GNU/Linux

Cheers


On Mon, Jun 16, 2014 at 9:29 PM, Andrew Ash <andrew@andrewash.com> wrote:

> I can't run sbt/sbt gen-idea on a clean checkout of Spark master.
>
> I get resolution errors on junit#junit;4.10!junit.zip(source)
>
> As shown below:
>
> aash@aash-mbp /tmp/git/spark$ sbt/sbt gen-idea
> Using /Library/Java/JavaVirtualMachines/jdk1.7.0_45.jdk/Contents/Home as
> default JAVA_HOME.
> Note, this will be overridden by -java-home if it is set.
> [info] Loading project definition from
> /private/tmp/git/spark/project/project
> [info] Loading project definition from /private/tmp/git/spark/project
> [info] Set current project to root (in build file:/private/tmp/git/spark/)
> [info] Creating IDEA module for project 'assembly' ...
> [info] Updating {file:/private/tmp/git/spark/}core...
> [info] Resolving org.fusesource.jansi#jansi;1.4 ...
> [warn] [FAILED     ] junit#junit;4.10!junit.zip(source):  (0ms)
> [warn] ==== local: tried
> [warn]   /Users/aash/.ivy2/local/junit/junit/4.10/sources/junit.zip
> [warn] ==== public: tried
> [warn]   http://repo1.maven.org/maven2/junit/junit/4.10/junit-4.10.zip
> [warn] ==== Maven Repository: tried
> [warn]
> http://repo.maven.apache.org/maven2/junit/junit/4.10/junit-4.10.zip
> [warn] ==== Apache Repository: tried
> [warn]
>
> https://repository.apache.org/content/repositories/releases/junit/junit/4.10/junit-4.10.zip
> [warn] ==== JBoss Repository: tried
> [warn]
>
> https://repository.jboss.org/nexus/content/repositories/releases/junit/junit/4.10/junit-4.10.zip
> [warn] ==== MQTT Repository: tried
> [warn]
>
> https://repo.eclipse.org/content/repositories/paho-releases/junit/junit/4.10/junit-4.10.zip
> [warn] ==== Cloudera Repository: tried
> [warn]
>
> http://repository.cloudera.com/artifactory/cloudera-repos/junit/junit/4.10/junit-4.10.zip
> [warn] ==== Pivotal Repository: tried
> [warn]
> http://repo.spring.io/libs-release/junit/junit/4.10/junit-4.10.zip
> [warn] ==== Maven2 Local: tried
> [warn]   file:/Users/aash/.m2/repository/junit/junit/4.10/junit-4.10.zip
> [warn] ::::::::::::::::::::::::::::::::::::::::::::::
> [warn] ::              FAILED DOWNLOADS            ::
> [warn] :: ^ see resolution messages for details  ^ ::
> [warn] ::::::::::::::::::::::::::::::::::::::::::::::
> [warn] :: junit#junit;4.10!junit.zip(source)
> [warn] ::::::::::::::::::::::::::::::::::::::::::::::
> sbt.ResolveException: download failed: junit#junit;4.10!junit.zip(source)
>
> By bumping the junit dependency to 4.11 I'm able to generate the IDE files.
>  Are other people having this problem or does everyone use the maven
> configuration?
>
> Andrew
>

--20cf30434c6440d9bb04fc00fc0c--

From dev-return-8047-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 17 05:05:20 2014
Return-Path: <dev-return-8047-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B107F11782
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 17 Jun 2014 05:05:20 +0000 (UTC)
Received: (qmail 97805 invoked by uid 500); 17 Jun 2014 05:05:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97746 invoked by uid 500); 17 Jun 2014 05:05:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 97735 invoked by uid 99); 17 Jun 2014 05:05:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 05:05:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.128.171] (HELO mail-ve0-f171.google.com) (209.85.128.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 05:05:17 +0000
Received: by mail-ve0-f171.google.com with SMTP id jz11so7051080veb.16
        for <dev@spark.apache.org>; Mon, 16 Jun 2014 22:04:53 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=Qja8KJqGE/QInk47vguu8Ekxniul+/rfn6GdqkL9ifg=;
        b=WgtQ2rN1HDG9n6uEMPvy5WTx14YUfQm4iPhabMvnCvulKvFWDByVHbfrgBk6T8G0tA
         VPYwLqeLkIyV1pQFSGftbos5cgnQWLe8FMy/rt4EGyEGOIdGfkdci8AIQIw5x7GUJ4pm
         PYd2YnBtStdVSPVxc3zMd/6oucnQ4urkrpQ7Viy5wKYfDDUdP7o8tn+TRqDevTbzr5hm
         jFhfWw765S8mbThDDhNTzaf1DrF9TzYAqL3mKa/18Otqaw9OTQ4x5RoQ6pHnulGqLR6b
         zT+QQswpIT9cUnBBf7YDU4cJckjRJTjSB1tWO+aleSwojE9SdMcMuMbXBg+z69SBJGlk
         ZZlw==
X-Gm-Message-State: ALoCoQlE3Q/eOwzcORrUR+saEpP2HWPVavbLPBDX2ztinzM2a/sllImFFkqFfFcytC0P5UyqCiza
X-Received: by 10.52.110.105 with SMTP id hz9mr16765309vdb.9.1402981493233;
        Mon, 16 Jun 2014 22:04:53 -0700 (PDT)
Received: from mail-ve0-f171.google.com (mail-ve0-f171.google.com [209.85.128.171])
        by mx.google.com with ESMTPSA id y10sm15175735vem.12.2014.06.16.22.04.52
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 16 Jun 2014 22:04:52 -0700 (PDT)
Received: by mail-ve0-f171.google.com with SMTP id jz11so7173083veb.30
        for <dev@spark.apache.org>; Mon, 16 Jun 2014 22:04:51 -0700 (PDT)
X-Received: by 10.58.110.106 with SMTP id hz10mr5524020veb.30.1402981491719;
 Mon, 16 Jun 2014 22:04:51 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Mon, 16 Jun 2014 22:04:31 -0700 (PDT)
In-Reply-To: <CALte62yQPA+k9i0XOM7Zww4WhL0MA-4OHmOTZ0=Ck2iWmenn_A@mail.gmail.com>
References: <CA+-p3AGJNA8y5Ss8fYBho_yqHA7iqzrDCNn4_4HSqLuCL_FF+A@mail.gmail.com>
 <CALte62yQPA+k9i0XOM7Zww4WhL0MA-4OHmOTZ0=Ck2iWmenn_A@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 16 Jun 2014 22:04:31 -0700
Message-ID: <CA+-p3AGNZ094Jbg=sx+drrF==nJWah=DY1d+xRs+F0s3vAsDMg@mail.gmail.com>
Subject: Re: Compile failure with SBT on master
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b2ee47570c57e04fc0115ea
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b2ee47570c57e04fc0115ea
Content-Type: text/plain; charset=UTF-8

Maybe it's a Mac OS X thing?


On Mon, Jun 16, 2014 at 9:57 PM, Ted Yu <yuzhihong@gmail.com> wrote:

> I used the same command on Linux and it passed:
>
> Linux k.net 2.6.32-220.23.1.el6.YAHOO.20120713.x86_64 #1 SMP Fri Jul 13
> 11:40:51 CDT 2012 x86_64 x86_64 x86_64 GNU/Linux
>
> Cheers
>
>
> On Mon, Jun 16, 2014 at 9:29 PM, Andrew Ash <andrew@andrewash.com> wrote:
>
> > I can't run sbt/sbt gen-idea on a clean checkout of Spark master.
> >
> > I get resolution errors on junit#junit;4.10!junit.zip(source)
> >
> > As shown below:
> >
> > aash@aash-mbp /tmp/git/spark$ sbt/sbt gen-idea
> > Using /Library/Java/JavaVirtualMachines/jdk1.7.0_45.jdk/Contents/Home as
> > default JAVA_HOME.
> > Note, this will be overridden by -java-home if it is set.
> > [info] Loading project definition from
> > /private/tmp/git/spark/project/project
> > [info] Loading project definition from /private/tmp/git/spark/project
> > [info] Set current project to root (in build
> file:/private/tmp/git/spark/)
> > [info] Creating IDEA module for project 'assembly' ...
> > [info] Updating {file:/private/tmp/git/spark/}core...
> > [info] Resolving org.fusesource.jansi#jansi;1.4 ...
> > [warn] [FAILED     ] junit#junit;4.10!junit.zip(source):  (0ms)
> > [warn] ==== local: tried
> > [warn]   /Users/aash/.ivy2/local/junit/junit/4.10/sources/junit.zip
> > [warn] ==== public: tried
> > [warn]   http://repo1.maven.org/maven2/junit/junit/4.10/junit-4.10.zip
> > [warn] ==== Maven Repository: tried
> > [warn]
> > http://repo.maven.apache.org/maven2/junit/junit/4.10/junit-4.10.zip
> > [warn] ==== Apache Repository: tried
> > [warn]
> >
> >
> https://repository.apache.org/content/repositories/releases/junit/junit/4.10/junit-4.10.zip
> > [warn] ==== JBoss Repository: tried
> > [warn]
> >
> >
> https://repository.jboss.org/nexus/content/repositories/releases/junit/junit/4.10/junit-4.10.zip
> > [warn] ==== MQTT Repository: tried
> > [warn]
> >
> >
> https://repo.eclipse.org/content/repositories/paho-releases/junit/junit/4.10/junit-4.10.zip
> > [warn] ==== Cloudera Repository: tried
> > [warn]
> >
> >
> http://repository.cloudera.com/artifactory/cloudera-repos/junit/junit/4.10/junit-4.10.zip
> > [warn] ==== Pivotal Repository: tried
> > [warn]
> > http://repo.spring.io/libs-release/junit/junit/4.10/junit-4.10.zip
> > [warn] ==== Maven2 Local: tried
> > [warn]   file:/Users/aash/.m2/repository/junit/junit/4.10/junit-4.10.zip
> > [warn] ::::::::::::::::::::::::::::::::::::::::::::::
> > [warn] ::              FAILED DOWNLOADS            ::
> > [warn] :: ^ see resolution messages for details  ^ ::
> > [warn] ::::::::::::::::::::::::::::::::::::::::::::::
> > [warn] :: junit#junit;4.10!junit.zip(source)
> > [warn] ::::::::::::::::::::::::::::::::::::::::::::::
> > sbt.ResolveException: download failed: junit#junit;4.10!junit.zip(source)
> >
> > By bumping the junit dependency to 4.11 I'm able to generate the IDE
> files.
> >  Are other people having this problem or does everyone use the maven
> > configuration?
> >
> > Andrew
> >
>

--047d7b2ee47570c57e04fc0115ea--

From dev-return-8048-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 17 07:57:46 2014
Return-Path: <dev-return-8048-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A40F211B17
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 17 Jun 2014 07:57:46 +0000 (UTC)
Received: (qmail 68069 invoked by uid 500); 17 Jun 2014 07:57:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68019 invoked by uid 500); 17 Jun 2014 07:57:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68001 invoked by uid 99); 17 Jun 2014 07:57:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 07:57:45 +0000
X-ASF-Spam-Status: No, hits=1.8 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of liqingyang1985@gmail.com designates 74.125.82.172 as permitted sender)
Received: from [74.125.82.172] (HELO mail-we0-f172.google.com) (74.125.82.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 07:57:41 +0000
Received: by mail-we0-f172.google.com with SMTP id u57so6875338wes.31
        for <dev@spark.apache.org>; Tue, 17 Jun 2014 00:57:20 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=pKRPUrcLemYUipVGYh7qa2K0if2xKmS16Tad3ltuhl0=;
        b=wh9mk5kZu8XidzM7g47wloJkkItJE70B0zGm5qHr9Jctqzam+CvDeQ6nAvXdUr3qBd
         0vyYx6ya7st/ddmB0/+gNzMFx0ABgMFgk+uDqtz04WQ0nNIPLj7zdXLtDD9WQrTT9/9P
         Or2ResFpbC7OxLJX02OTjq4hK4RHlz8WkRmt0P+/w47XkU7nl+TMBzXPdclMeUpePDww
         D6ir0zp9L9V4nxcmf+dHoU+iEhrvEq04F9GD2MAjuFjPgRUOFbwDNQp44TV6MRVDG7vM
         uZo1EePSecwVYA3S8onU7RQy9CxMLUMs6pCx1KcrA+QCZo2EV+lHaPnkufwQh/h7tlnI
         LNDg==
MIME-Version: 1.0
X-Received: by 10.180.90.145 with SMTP id bw17mr34235671wib.43.1402991840764;
 Tue, 17 Jun 2014 00:57:20 -0700 (PDT)
Received: by 10.194.22.2 with HTTP; Tue, 17 Jun 2014 00:57:20 -0700 (PDT)
In-Reply-To: <CA+-p3AFpOMc62-OxWy_1ujF4Uw2zxMzoBew-wdUEJAaP2e=-cg@mail.gmail.com>
References: <CABDsqqabX9k_POqY4SZ57qKuA5G5fH2zZ1Ujb7iDyk_U5th=uA@mail.gmail.com>
	<CA+-p3AFpOMc62-OxWy_1ujF4Uw2zxMzoBew-wdUEJAaP2e=-cg@mail.gmail.com>
Date: Tue, 17 Jun 2014 15:57:20 +0800
Message-ID: <CABDsqqa96PjdFWjTAYV2Vy_oKDuZUEhNtoDBO854ywKOymcSPA@mail.gmail.com>
Subject: Re: encounter jvm problem when integreation spark with mesos
From: qingyang li <liqingyang1985@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d043be1c44aac3804fc037e1f
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043be1c44aac3804fc037e1f
Content-Type: text/plain; charset=UTF-8

somebody else has also encountered such problem:
http://mail-archives.apache.org/mod_mbox/spark-user/201404.mbox/%3CAFC0D60983129F4F9FBAD571AA422C9A5AF8F098@MAIL-MBX1.ad.renci.org%3E


2014-06-17 12:31 GMT+08:00 Andrew Ash <andrew@andrewash.com>:

> Hi qingyang,
>
> This looks like an issue with the open source version of the Java runtime
> (called OpenJDK) that causes the JVM to fail.  Can you try using the JVM
> released by Oracle and see if it has the same issue?
>
> Thanks!
> Andrew
>
>
> On Mon, Jun 16, 2014 at 9:24 PM, qingyang li <liqingyang1985@gmail.com>
> wrote:
>
> > hi, I encounter  jvm problem when integreation spark with mesos,
> > here is the log when i run "spark-shell":
> > -48ce131dc5af
> > 14/06/17 12:24:55 INFO HttpServer: Starting HTTP Server
> > 14/06/17 12:24:55 INFO SparkUI: Started Spark Web UI at
> > http://bigdata001:4040
> > #
> > # A fatal error has been detected by the Java Runtime Environment:
> > #
> > #  SIGSEGV (0xb) at pc=0x00007f94f4843d21, pid=5956, tid=140277175580416
> > #
> > # JRE version: OpenJDK Runtime Environment (7.0_51-b02) (build
> > 1.7.0_51-mockbuild_2014_01_15_01_39-b00)
> > # Java VM: OpenJDK 64-Bit Server VM (24.45-b08 mixed mode linux-amd64
> > compressed oops)
> > # Problematic frame:
> > # V  [libjvm.so+0x5e5d21]  JNI_CreateJavaVM+0x6551
> > #
> > # Core dump written. Default location:
> > /home/zjw/spark/spark-0.9.0-incubating-bin-hadoop2/core or core.5956
> > #
> > # An error report file with more information is saved as:
> > # /tmp/jvm-5956/hs_error.log
> > #
> > # If you would like to submit a bug report, please include
> > # instructions on how to reproduce the bug and visit:
> > #   http://icedtea.classpath.org/bugzilla
> > #
> > bin/spark-shell: line 101:  5956 Aborted                 (core dumped)
> > $FWDIR/bin/spark-class $OPTIONS org.apache.spark.repl.Main "$@"
> >
>

--f46d043be1c44aac3804fc037e1f--

From dev-return-8049-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 17 08:13:24 2014
Return-Path: <dev-return-8049-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3B86111B76
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 17 Jun 2014 08:13:24 +0000 (UTC)
Received: (qmail 85486 invoked by uid 500); 17 Jun 2014 08:13:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85424 invoked by uid 500); 17 Jun 2014 08:13:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85410 invoked by uid 99); 17 Jun 2014 08:13:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 08:13:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,T_REMOTE_IMAGE,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of andy.petrella@gmail.com designates 209.85.217.170 as permitted sender)
Received: from [209.85.217.170] (HELO mail-lb0-f170.google.com) (209.85.217.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 08:13:21 +0000
Received: by mail-lb0-f170.google.com with SMTP id 10so2669485lbg.1
        for <dev@spark.apache.org>; Tue, 17 Jun 2014 01:12:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=mJppOd7jk7PvOg7l6q4hw1BBcJe5DchPO/FFHbeCE/Q=;
        b=RytdNJzwOxlk0R36c3akunBlv9nzfbEhnMJyeEpamyYWkF0t1twdtlzvuN4JuPnyia
         kecGHBusAnr6qs87RCUsH4xVKWuq/YulNe3hGUN/tYJhc2gB1oDxcszKXiKXybre8OyR
         Nc018hg6kBSqaJpTGLOUq515/88cees2ibJrnNBFO82QNVbg3zC6RZFiPuosuaIhY2Av
         s75y3id/DQkBSd1JrNgvhjExTdyGZimEp+aof87tivUU5vgWuYEGZFQV9lPtrg9DRhA+
         kwhctNzqkBudaNxe/oMDYat9d4UyU92+DmIj0o9cZBIjX0yuQ3tBwuWiBXczC4yoCXKb
         iIuA==
X-Received: by 10.112.149.71 with SMTP id ty7mr16677662lbb.34.1402992777092;
 Tue, 17 Jun 2014 01:12:57 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.112.3.5 with HTTP; Tue, 17 Jun 2014 01:12:36 -0700 (PDT)
In-Reply-To: <CABDsqqa96PjdFWjTAYV2Vy_oKDuZUEhNtoDBO854ywKOymcSPA@mail.gmail.com>
References: <CABDsqqabX9k_POqY4SZ57qKuA5G5fH2zZ1Ujb7iDyk_U5th=uA@mail.gmail.com>
 <CA+-p3AFpOMc62-OxWy_1ujF4Uw2zxMzoBew-wdUEJAaP2e=-cg@mail.gmail.com> <CABDsqqa96PjdFWjTAYV2Vy_oKDuZUEhNtoDBO854ywKOymcSPA@mail.gmail.com>
From: andy petrella <andy.petrella@gmail.com>
Date: Tue, 17 Jun 2014 10:12:36 +0200
Message-ID: <CAKn3j0vsHPKZ7rr9mJ0f_PB0sv=hJZNz7iSQVEESBw=L+i_o5w@mail.gmail.com>
Subject: Re: encounter jvm problem when integreation spark with mesos
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b3a8ada19e59a04fc03b6ec
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a8ada19e59a04fc03b6ec
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Yep but no real resolution nor advances on this topic, since finally we've
chosen to stick with a "compatible" version of Mesos (0.14.1 ftm).
But I'm still convince it has to do with native libs clash :-s

 a=E2=84=95dy =E2=84=99etrella
about.me/noootsab
[image: a=E2=84=95dy =E2=84=99etrella on about.me]

<http://about.me/noootsab>


On Tue, Jun 17, 2014 at 9:57 AM, qingyang li <liqingyang1985@gmail.com>
wrote:

> somebody else has also encountered such problem:
>
> http://mail-archives.apache.org/mod_mbox/spark-user/201404.mbox/%3CAFC0D6=
0983129F4F9FBAD571AA422C9A5AF8F098@MAIL-MBX1.ad.renci.org%3E
>
>
> 2014-06-17 12:31 GMT+08:00 Andrew Ash <andrew@andrewash.com>:
>
> > Hi qingyang,
> >
> > This looks like an issue with the open source version of the Java runti=
me
> > (called OpenJDK) that causes the JVM to fail.  Can you try using the JV=
M
> > released by Oracle and see if it has the same issue?
> >
> > Thanks!
> > Andrew
> >
> >
> > On Mon, Jun 16, 2014 at 9:24 PM, qingyang li <liqingyang1985@gmail.com>
> > wrote:
> >
> > > hi, I encounter  jvm problem when integreation spark with mesos,
> > > here is the log when i run "spark-shell":
> > > -48ce131dc5af
> > > 14/06/17 12:24:55 INFO HttpServer: Starting HTTP Server
> > > 14/06/17 12:24:55 INFO SparkUI: Started Spark Web UI at
> > > http://bigdata001:4040
> > > #
> > > # A fatal error has been detected by the Java Runtime Environment:
> > > #
> > > #  SIGSEGV (0xb) at pc=3D0x00007f94f4843d21, pid=3D5956,
> tid=3D140277175580416
> > > #
> > > # JRE version: OpenJDK Runtime Environment (7.0_51-b02) (build
> > > 1.7.0_51-mockbuild_2014_01_15_01_39-b00)
> > > # Java VM: OpenJDK 64-Bit Server VM (24.45-b08 mixed mode linux-amd64
> > > compressed oops)
> > > # Problematic frame:
> > > # V  [libjvm.so+0x5e5d21]  JNI_CreateJavaVM+0x6551
> > > #
> > > # Core dump written. Default location:
> > > /home/zjw/spark/spark-0.9.0-incubating-bin-hadoop2/core or core.5956
> > > #
> > > # An error report file with more information is saved as:
> > > # /tmp/jvm-5956/hs_error.log
> > > #
> > > # If you would like to submit a bug report, please include
> > > # instructions on how to reproduce the bug and visit:
> > > #   http://icedtea.classpath.org/bugzilla
> > > #
> > > bin/spark-shell: line 101:  5956 Aborted                 (core dumped=
)
> > > $FWDIR/bin/spark-class $OPTIONS org.apache.spark.repl.Main "$@"
> > >
> >
>

--047d7b3a8ada19e59a04fc03b6ec--

From dev-return-8050-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 17 09:33:35 2014
Return-Path: <dev-return-8050-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6B41511F1F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 17 Jun 2014 09:33:35 +0000 (UTC)
Received: (qmail 34358 invoked by uid 500); 17 Jun 2014 09:33:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34295 invoked by uid 500); 17 Jun 2014 09:33:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34283 invoked by uid 99); 17 Jun 2014 09:33:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 09:33:33 +0000
X-ASF-Spam-Status: No, hits=1.8 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of liqingyang1985@gmail.com designates 74.125.82.176 as permitted sender)
Received: from [74.125.82.176] (HELO mail-we0-f176.google.com) (74.125.82.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 09:33:31 +0000
Received: by mail-we0-f176.google.com with SMTP id u56so6982017wes.35
        for <dev@spark.apache.org>; Tue, 17 Jun 2014 02:33:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=WADfnz2F4MBSF6TooMcuXK79+D3t8ZXHo+vodQ1QWms=;
        b=vC3C3ME4RcUGN3oHnvHs+DSZ/d4fqoLCaqxscFzSSP+ESRn/JjUg7NS/tGxW0mGygE
         XXm889+ANVZuLuEqy1QplvBfg9ZXU+mhx59zZ8YcF3TF/f3BwgQskC1wLIuexbGIsuku
         es2uxeHtb1r8QKO6PRhBPZKjx7sPuouFR1OGayyEKd7JIoqW12sjj5zHDiAb1LVaSfjq
         3arxPoB+wum3dulhu60P+Z6V2KIXk2xsJ8SuJx7VfeZqxjBvgN03iiX7jt79ALBBYAq1
         iY/WdHQ0HSGCHyAZaxfIztBN/jygitMMwE3pjDmViEkWOXYcXwchrD8dI6u6GQ1LX7Ss
         gvug==
MIME-Version: 1.0
X-Received: by 10.180.11.9 with SMTP id m9mr34630880wib.51.1402997587893; Tue,
 17 Jun 2014 02:33:07 -0700 (PDT)
Received: by 10.194.22.2 with HTTP; Tue, 17 Jun 2014 02:33:07 -0700 (PDT)
In-Reply-To: <CAKn3j0vsHPKZ7rr9mJ0f_PB0sv=hJZNz7iSQVEESBw=L+i_o5w@mail.gmail.com>
References: <CABDsqqabX9k_POqY4SZ57qKuA5G5fH2zZ1Ujb7iDyk_U5th=uA@mail.gmail.com>
	<CA+-p3AFpOMc62-OxWy_1ujF4Uw2zxMzoBew-wdUEJAaP2e=-cg@mail.gmail.com>
	<CABDsqqa96PjdFWjTAYV2Vy_oKDuZUEhNtoDBO854ywKOymcSPA@mail.gmail.com>
	<CAKn3j0vsHPKZ7rr9mJ0f_PB0sv=hJZNz7iSQVEESBw=L+i_o5w@mail.gmail.com>
Date: Tue, 17 Jun 2014 17:33:07 +0800
Message-ID: <CABDsqqaFGaL_fa9NgJqr=Qiu9WFnKYTFjt3cUZZZErkoncuoaw@mail.gmail.com>
Subject: Re: encounter jvm problem when integreation spark with mesos
From: qingyang li <liqingyang1985@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c258dcd8e2e804fc04d45f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c258dcd8e2e804fc04d45f
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

here is the core stack info:
-----------------------------
(gdb) bt
#0  0x00007fc0153fc925 in raise () from /lib64/libc.so.6
#1  0x00007fc0153fe105 in abort () from /lib64/libc.so.6
#2  0x00007fc014d78405 in os::abort(bool) ()
   from /home/zjw/jdk1.7/jdk1.7.0_51/jre/lib/amd64/server/libjvm.so
#3  0x00007fc014ef7347 in VMError::report_and_die() ()
   from /home/zjw/jdk1.7/jdk1.7.0_51/jre/lib/amd64/server/libjvm.so
#4  0x00007fc014d7cd8f in JVM_handle_linux_signal ()
   from /home/zjw/jdk1.7/jdk1.7.0_51/jre/lib/amd64/server/libjvm.so
#5  <signal handler called>
#6  0x00007fc014b96ce9 in jni_GetByteArrayElements ()
   from /home/zjw/jdk1.7/jdk1.7.0_51/jre/lib/amd64/server/libjvm.so
#7  0x00007fbff70f002c in GetByteArrayElements (env=3D<value optimized out>=
,
jobj=3D0x7fbf8c000f80)
    at /home/zjw/jdk1.7/jdk1.7.0_51//include/jni.h:1668
#8  construct<mesos::FrameworkInfo> (env=3D<value optimized out>,
jobj=3D0x7fbf8c000f80)
    at ../../src/java/jni/construct.cpp:123
#9  0x00007fbff70f51c8 in
Java_org_apache_mesos_MesosSchedulerDriver_initialize (env=3D0x7fc010d189e8=
,
    thiz=3D0x7fbfd94f6830) at
../../src/java/jni/org_apache_mesos_MesosSchedulerDriver.cpp:528



2014-06-17 16:12 GMT+08:00 andy petrella <andy.petrella@gmail.com>:

> Yep but no real resolution nor advances on this topic, since finally we'v=
e
> chosen to stick with a "compatible" version of Mesos (0.14.1 ftm).
> But I'm still convince it has to do with native libs clash :-s
>
>  a=E2=84=95dy =E2=84=99etrella
> about.me/noootsab
> [image: a=E2=84=95dy =E2=84=99etrella on about.me]
>
> <http://about.me/noootsab>
>
>
> On Tue, Jun 17, 2014 at 9:57 AM, qingyang li <liqingyang1985@gmail.com>
> wrote:
>
> > somebody else has also encountered such problem:
> >
> >
> http://mail-archives.apache.org/mod_mbox/spark-user/201404.mbox/%3CAFC0D6=
0983129F4F9FBAD571AA422C9A5AF8F098@MAIL-MBX1.ad.renci.org%3E
> >
> >
> > 2014-06-17 12:31 GMT+08:00 Andrew Ash <andrew@andrewash.com>:
> >
> > > Hi qingyang,
> > >
> > > This looks like an issue with the open source version of the Java
> runtime
> > > (called OpenJDK) that causes the JVM to fail.  Can you try using the
> JVM
> > > released by Oracle and see if it has the same issue?
> > >
> > > Thanks!
> > > Andrew
> > >
> > >
> > > On Mon, Jun 16, 2014 at 9:24 PM, qingyang li <liqingyang1985@gmail.co=
m
> >
> > > wrote:
> > >
> > > > hi, I encounter  jvm problem when integreation spark with mesos,
> > > > here is the log when i run "spark-shell":
> > > > -48ce131dc5af
> > > > 14/06/17 12:24:55 INFO HttpServer: Starting HTTP Server
> > > > 14/06/17 12:24:55 INFO SparkUI: Started Spark Web UI at
> > > > http://bigdata001:4040
> > > > #
> > > > # A fatal error has been detected by the Java Runtime Environment:
> > > > #
> > > > #  SIGSEGV (0xb) at pc=3D0x00007f94f4843d21, pid=3D5956,
> > tid=3D140277175580416
> > > > #
> > > > # JRE version: OpenJDK Runtime Environment (7.0_51-b02) (build
> > > > 1.7.0_51-mockbuild_2014_01_15_01_39-b00)
> > > > # Java VM: OpenJDK 64-Bit Server VM (24.45-b08 mixed mode linux-amd=
64
> > > > compressed oops)
> > > > # Problematic frame:
> > > > # V  [libjvm.so+0x5e5d21]  JNI_CreateJavaVM+0x6551
> > > > #
> > > > # Core dump written. Default location:
> > > > /home/zjw/spark/spark-0.9.0-incubating-bin-hadoop2/core or core.595=
6
> > > > #
> > > > # An error report file with more information is saved as:
> > > > # /tmp/jvm-5956/hs_error.log
> > > > #
> > > > # If you would like to submit a bug report, please include
> > > > # instructions on how to reproduce the bug and visit:
> > > > #   http://icedtea.classpath.org/bugzilla
> > > > #
> > > > bin/spark-shell: line 101:  5956 Aborted                 (core
> dumped)
> > > > $FWDIR/bin/spark-class $OPTIONS org.apache.spark.repl.Main "$@"
> > > >
> > >
> >
>

--001a11c258dcd8e2e804fc04d45f--

From dev-return-8051-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 17 15:03:14 2014
Return-Path: <dev-return-8051-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 09D33119BF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 17 Jun 2014 15:03:14 +0000 (UTC)
Received: (qmail 18958 invoked by uid 500); 17 Jun 2014 15:03:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18900 invoked by uid 500); 17 Jun 2014 15:03:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18888 invoked by uid 99); 17 Jun 2014 15:03:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 15:03:13 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.50 as permitted sender)
Received: from [209.85.213.50] (HELO mail-yh0-f50.google.com) (209.85.213.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 15:03:07 +0000
Received: by mail-yh0-f50.google.com with SMTP id t59so5601037yho.37
        for <dev@spark.apache.org>; Tue, 17 Jun 2014 08:02:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=RVC+jG6vGSw5hdINuq+4Y2fn5GYgEI5fyvoCZeiBVq4=;
        b=Gu/PAYcCq4PWkBgpciDz5YgniomAyojoG5W7BQtrRat3nmVfDdm885Z/Jh5+8n1gNd
         Jv/BnbA8J0//HVkhCeVeLvQqktL/uMTjmF40nY+qAH1yLvGwG5xY+CJ+u0ekmhLV7me+
         AXEmrCnSbBTY2v9Y116ZrUfZY4LsxoLakmHfOiOsYnEPaKQETPKriKcAOu13T8DOJapK
         NQQPfSIFJtfr03/aZfy3fa/UyqAgXQj/CtjwGyGF8OkZKaAltasQz0Fg+/1QnXPG9C07
         U98j8ReCie1L4jhT5Mn++Ko0ldhuaFCP2nM7CgYsAxGviA15mKu4cRWP+IY982h99a0W
         3aKw==
MIME-Version: 1.0
X-Received: by 10.236.20.114 with SMTP id o78mr46175192yho.91.1403017367305;
 Tue, 17 Jun 2014 08:02:47 -0700 (PDT)
Received: by 10.170.55.137 with HTTP; Tue, 17 Jun 2014 08:02:47 -0700 (PDT)
In-Reply-To: <CA+-p3AGNZ094Jbg=sx+drrF==nJWah=DY1d+xRs+F0s3vAsDMg@mail.gmail.com>
References: <CA+-p3AGJNA8y5Ss8fYBho_yqHA7iqzrDCNn4_4HSqLuCL_FF+A@mail.gmail.com>
	<CALte62yQPA+k9i0XOM7Zww4WhL0MA-4OHmOTZ0=Ck2iWmenn_A@mail.gmail.com>
	<CA+-p3AGNZ094Jbg=sx+drrF==nJWah=DY1d+xRs+F0s3vAsDMg@mail.gmail.com>
Date: Tue, 17 Jun 2014 08:02:47 -0700
Message-ID: <CALte62wVCPpYEyNuvw=nCxV4vE73FG2HCFQ1MsQgUbMOwdCn9w@mail.gmail.com>
Subject: Re: Compile failure with SBT on master
From: Ted Yu <yuzhihong@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01635714cac0d004fc096fb2
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01635714cac0d004fc096fb2
Content-Type: text/plain; charset=UTF-8

I didn't get that error on Mac either:

java version "1.7.0_55"
Java(TM) SE Runtime Environment (build 1.7.0_55-b13)
Java HotSpot(TM) 64-Bit Server VM (build 24.55-b03, mixed mode)

Darwin TYus-MacBook-Pro.local 12.5.0 Darwin Kernel Version 12.5.0: Sun Sep
29 13:33:47 PDT 2013; root:xnu-2050.48.12~1/RELEASE_X86_64 x86_64


On Mon, Jun 16, 2014 at 10:04 PM, Andrew Ash <andrew@andrewash.com> wrote:

> Maybe it's a Mac OS X thing?
>
>
> On Mon, Jun 16, 2014 at 9:57 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>
> > I used the same command on Linux and it passed:
> >
> > Linux k.net 2.6.32-220.23.1.el6.YAHOO.20120713.x86_64 #1 SMP Fri Jul 13
> > 11:40:51 CDT 2012 x86_64 x86_64 x86_64 GNU/Linux
> >
> > Cheers
> >
> >
> > On Mon, Jun 16, 2014 at 9:29 PM, Andrew Ash <andrew@andrewash.com>
> wrote:
> >
> > > I can't run sbt/sbt gen-idea on a clean checkout of Spark master.
> > >
> > > I get resolution errors on junit#junit;4.10!junit.zip(source)
> > >
> > > As shown below:
> > >
> > > aash@aash-mbp /tmp/git/spark$ sbt/sbt gen-idea
> > > Using /Library/Java/JavaVirtualMachines/jdk1.7.0_45.jdk/Contents/Home
> as
> > > default JAVA_HOME.
> > > Note, this will be overridden by -java-home if it is set.
> > > [info] Loading project definition from
> > > /private/tmp/git/spark/project/project
> > > [info] Loading project definition from /private/tmp/git/spark/project
> > > [info] Set current project to root (in build
> > file:/private/tmp/git/spark/)
> > > [info] Creating IDEA module for project 'assembly' ...
> > > [info] Updating {file:/private/tmp/git/spark/}core...
> > > [info] Resolving org.fusesource.jansi#jansi;1.4 ...
> > > [warn] [FAILED     ] junit#junit;4.10!junit.zip(source):  (0ms)
> > > [warn] ==== local: tried
> > > [warn]   /Users/aash/.ivy2/local/junit/junit/4.10/sources/junit.zip
> > > [warn] ==== public: tried
> > > [warn]   http://repo1.maven.org/maven2/junit/junit/4.10/junit-4.10.zip
> > > [warn] ==== Maven Repository: tried
> > > [warn]
> > > http://repo.maven.apache.org/maven2/junit/junit/4.10/junit-4.10.zip
> > > [warn] ==== Apache Repository: tried
> > > [warn]
> > >
> > >
> >
> https://repository.apache.org/content/repositories/releases/junit/junit/4.10/junit-4.10.zip
> > > [warn] ==== JBoss Repository: tried
> > > [warn]
> > >
> > >
> >
> https://repository.jboss.org/nexus/content/repositories/releases/junit/junit/4.10/junit-4.10.zip
> > > [warn] ==== MQTT Repository: tried
> > > [warn]
> > >
> > >
> >
> https://repo.eclipse.org/content/repositories/paho-releases/junit/junit/4.10/junit-4.10.zip
> > > [warn] ==== Cloudera Repository: tried
> > > [warn]
> > >
> > >
> >
> http://repository.cloudera.com/artifactory/cloudera-repos/junit/junit/4.10/junit-4.10.zip
> > > [warn] ==== Pivotal Repository: tried
> > > [warn]
> > > http://repo.spring.io/libs-release/junit/junit/4.10/junit-4.10.zip
> > > [warn] ==== Maven2 Local: tried
> > > [warn]
> file:/Users/aash/.m2/repository/junit/junit/4.10/junit-4.10.zip
> > > [warn] ::::::::::::::::::::::::::::::::::::::::::::::
> > > [warn] ::              FAILED DOWNLOADS            ::
> > > [warn] :: ^ see resolution messages for details  ^ ::
> > > [warn] ::::::::::::::::::::::::::::::::::::::::::::::
> > > [warn] :: junit#junit;4.10!junit.zip(source)
> > > [warn] ::::::::::::::::::::::::::::::::::::::::::::::
> > > sbt.ResolveException: download failed:
> junit#junit;4.10!junit.zip(source)
> > >
> > > By bumping the junit dependency to 4.11 I'm able to generate the IDE
> > files.
> > >  Are other people having this problem or does everyone use the maven
> > > configuration?
> > >
> > > Andrew
> > >
> >
>

--089e01635714cac0d004fc096fb2--

From dev-return-8052-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 17 16:28:02 2014
Return-Path: <dev-return-8052-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 32CAD11CBB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 17 Jun 2014 16:28:02 +0000 (UTC)
Received: (qmail 47784 invoked by uid 500); 17 Jun 2014 16:28:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47719 invoked by uid 500); 17 Jun 2014 16:28:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47707 invoked by uid 99); 17 Jun 2014 16:28:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 16:28:01 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.223.181 as permitted sender)
Received: from [209.85.223.181] (HELO mail-ie0-f181.google.com) (209.85.223.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 16:27:54 +0000
Received: by mail-ie0-f181.google.com with SMTP id y20so6536098ier.40
        for <dev@spark.apache.org>; Tue, 17 Jun 2014 09:27:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:subject:mime-version:content-type;
        bh=ABqQmOAIZvMIQid34GyvR3SiER8QgAnNOMSS+MQdlJc=;
        b=CyCEObhZ4Y+sR0+VIzKUNqfcPyeZbmoLIkI3TIjsR8DXJpp+HTHc5ZvTvkKIpkIUku
         Io9BLxGmoN52xWU99LUAyM88jC/3NOXMLA+gSS0wzSdKri5CfkBCs4YpfnjlqgHD/xeZ
         lbX2eZScQ3mm8sDCWL6E6ID//Gw27YqtRqKZdnadDAfZsnRyiAnP6eSa0KGFh8vI/89k
         RDNUudE0F9yYdY7khtB1tbU9NG2v7nPsnYBpxL90d/FWp/lQG6zy8im84Gz44aGj9Rq9
         xwHqu1/iDGMWc1K7ExBubfuaJ1974K0aboqC4UTUZz2+Y/5X7f6+4L8899lFi/UY2Wit
         YaIQ==
X-Received: by 10.50.131.167 with SMTP id on7mr43359928igb.0.1403022454456;
        Tue, 17 Jun 2014 09:27:34 -0700 (PDT)
Received: from [192.168.2.11] (MTRLPQ02-1177746539.sdsl.bell.ca. [70.50.252.107])
        by mx.google.com with ESMTPSA id ci7sm30346128igb.11.2014.06.17.09.27.33
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Tue, 17 Jun 2014 09:27:34 -0700 (PDT)
Date: Tue, 17 Jun 2014 12:37:02 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Message-ID: <B1C13C66AC9341AF99C0D69B578B9F13@gmail.com>
Subject: anyone can mark this issue as resolved?
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53a06eae_10db9daa_22e"
X-Virus-Checked: Checked by ClamAV on apache.org

--53a06eae_10db9daa_22e
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

Hi, 

Just found it occasionally 

https://issues.apache.org/jira/browse/SPARK-1471 

Best, 

-- 
Nan Zhu


--53a06eae_10db9daa_22e--


From dev-return-8053-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 17 17:40:32 2014
Return-Path: <dev-return-8053-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 933F011EE1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 17 Jun 2014 17:40:32 +0000 (UTC)
Received: (qmail 34746 invoked by uid 500); 17 Jun 2014 17:40:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34691 invoked by uid 500); 17 Jun 2014 17:40:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34674 invoked by uid 99); 17 Jun 2014 17:40:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 17:40:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of suren.hiraman@sociocast.com designates 209.85.128.179 as permitted sender)
Received: from [209.85.128.179] (HELO mail-ve0-f179.google.com) (209.85.128.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 17:40:28 +0000
Received: by mail-ve0-f179.google.com with SMTP id sa20so6141767veb.10
        for <dev@spark.apache.org>; Tue, 17 Jun 2014 10:40:07 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=Hsw1vNNZ8xnJhU0t+t8cmQd0cSx0NNkVLE/BOkWagxQ=;
        b=fZUecVNaFlEhORkXIhKbSXP1G1p8xsuJo/ITmJNd8RJMRUYrSzUgkvS9G27OvJsz0n
         6h7w4qN0KN6I2uXJILQcdl9Pe1LCpoUL9tE6WGvA8FA4lFcenE0vL65Nw+AubqulBdxf
         2QHlvCrrJI7bnbOKVYf6Wj++2cx/oCGVBuCyDTXtiZ7lK/JHEx26JFm78PBnie1NUSAf
         PlfFkCSWbu9CIMQ1wTtSWGiX9JN75UdFRmy26tjHN0vxCIOopNpKV2A5BigAqvc3SeP1
         gG9Se+cRtEy8tBcTi/+IO6TRT9xgFn6tPjFon+K3TiLSZlqMtW40IGmPz4aXVDpWpZVc
         KFCQ==
X-Gm-Message-State: ALoCoQm+zut8VAdEgq9MR3FuZ5/WLZBnMj5w3a11QPF3WF3xLa0RFsLPTnFvnIB1jEAGIvlxl3NU
MIME-Version: 1.0
X-Received: by 10.58.160.10 with SMTP id xg10mr23678362veb.0.1403026806875;
 Tue, 17 Jun 2014 10:40:06 -0700 (PDT)
Received: by 10.58.143.49 with HTTP; Tue, 17 Jun 2014 10:40:06 -0700 (PDT)
In-Reply-To: <CAMgYSQ_WW8jLLu1_hYOLPfrJHLs2+N=VXGO0YvL9tPQ0_K=xjQ@mail.gmail.com>
References: <CAJfdq+ua55B7xux7ujNxHNkTTe4CcYhZdgsTW04BNWi6axF2qw@mail.gmail.com>
	<CAMAsSdKa41UWJ_RO70Ebh2AK2RPSQUTsCwVYo76HCPK8Z1f5RQ@mail.gmail.com>
	<CAMgYSQ_WW8jLLu1_hYOLPfrJHLs2+N=VXGO0YvL9tPQ0_K=xjQ@mail.gmail.com>
Date: Tue, 17 Jun 2014 13:40:06 -0400
Message-ID: <CALWDz_v5QXT=4eWxcTJcwsLcHwXjpgKFwqjFtjM43DscbFyfFA@mail.gmail.com>
Subject: Re: Java IO Stream Corrupted - Invalid Type AC?
From: Surendranauth Hiraman <suren.hiraman@velos.io>
To: user@spark.apache.org, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b676c846f403204fc0ba290
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b676c846f403204fc0ba290
Content-Type: text/plain; charset=UTF-8

Matt/Ryan,

Did you make any headway on this? My team is running into this also.
Doesn't happen on smaller datasets. Our input set is about 10 GB but we
generate 100s of GBs in the flow itself.

-Suren




On Fri, Jun 6, 2014 at 5:19 PM, Ryan Compton <compton.ryan@gmail.com> wrote:

> Just ran into this today myself. I'm on branch-1.0 using a CDH3
> cluster (no modifications to Spark or its dependencies). The error
> appeared trying to run GraphX's .connectedComponents() on a ~200GB
> edge list (GraphX worked beautifully on smaller data).
>
> Here's the stacktrace (it's quite similar to yours
> https://imgur.com/7iBA4nJ ).
>
> 14/06/05 20:02:28 ERROR scheduler.TaskSetManager: Task 5.599:39 failed
> 4 times; aborting job
> 14/06/05 20:02:28 INFO scheduler.DAGScheduler: Failed to run reduce at
> VertexRDD.scala:100
> Exception in thread "main" org.apache.spark.SparkException: Job
> aborted due to stage failure: Task 5.599:39 failed 4 times, most
> recent failure: Exception failure in TID 29735 on host node18:
> java.io.StreamCorruptedException: invalid type code: AC
>         java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1355)
>         java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
>
> org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)
>
> org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:125)
>         org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
>         scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
>
> org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
>
> org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
>         scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
>         scala.collection.Iterator$class.foreach(Iterator.scala:727)
>         scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
>
> org.apache.spark.graphx.impl.VertexPartitionBaseOps.innerJoinKeepLeft(VertexPartitionBaseOps.scala:192)
>
> org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:78)
>
> org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:75)
>
> org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
>         scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>         scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
>         scala.collection.Iterator$class.foreach(Iterator.scala:727)
>         scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
>
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
>
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
>         org.apache.spark.scheduler.Task.run(Task.scala:51)
>
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
>
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>         java.lang.Thread.run(Thread.java:662)
> Driver stacktrace:
> at org.apache.spark.scheduler.DAGScheduler.org
> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)
> at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1017)
> at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1015)
> at
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> at
> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1015)
> at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
> at
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
> at scala.Option.foreach(Option.scala:236)
> at
> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:633)
> at
> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1207)
> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
> at
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
> at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
> at
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
> at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
> at
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
> 14/06/05 20:02:28 INFO scheduler.TaskSchedulerImpl: Cancelling stage 5
>
> On Wed, Jun 4, 2014 at 7:50 AM, Sean Owen <sowen@cloudera.com> wrote:
> > On Wed, Jun 4, 2014 at 3:33 PM, Matt Kielo <mkielo@oculusinfo.com>
> wrote:
> >> Im trying run some spark code on a cluster but I keep running into a
> >> "java.io.StreamCorruptedException: invalid type code: AC" error. My task
> >> involves analyzing ~50GB of data (some operations involve sorting) then
> >> writing them out to a JSON file. Im running the analysis on each of the
> >> data's ~10 columns and have never had a successful run. My program
> seems to
> >> run for a varying amount of time each time (~between 5-30 minutes) but
> it
> >> always terminates with this error.
> >
> > I can tell you that this usually means somewhere something wrote
> > objects to the same OutputStream with multiple ObjectOutputStreams. AC
> > is a header value.
> >
> > I don't obviously see where/how that could happen, but maybe it rings
> > a bell for someone. This could happen if an OutputStream is reused
> > across object serializations but new ObjectOutputStreams are opened,
> > for example.
>



-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io

--047d7b676c846f403204fc0ba290--

From dev-return-8054-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 17 22:58:39 2014
Return-Path: <dev-return-8054-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F36CF11A3B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 17 Jun 2014 22:58:38 +0000 (UTC)
Received: (qmail 44178 invoked by uid 500); 17 Jun 2014 22:58:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44118 invoked by uid 500); 17 Jun 2014 22:58:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44106 invoked by uid 99); 17 Jun 2014 22:58:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 22:58:38 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of miccagiann@gmail.com designates 209.85.213.169 as permitted sender)
Received: from [209.85.213.169] (HELO mail-ig0-f169.google.com) (209.85.213.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Jun 2014 22:58:33 +0000
Received: by mail-ig0-f169.google.com with SMTP id a13so5612735igq.4
        for <dev@spark.incubator.apache.org>; Tue, 17 Jun 2014 15:58:09 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=Ztfkgzm5WPzdjbm+0GGEp20f2cSsHe7M2Ky6Y1f3W3c=;
        b=qkTWA4+YersdTV7nQTWIQU58Yh8AUENJ1E4rb2IfzIqipJpODfTY9iCXiZs68BkG9G
         ufmAFVEs1Aj76fTyMwZN+RMwdBvdLB6hq0l2UDpP1gqkoQCiupyfByWpIump0g8OG7Dd
         r5FelF6/sMS6wIMkoP29PyDsL0qaupyyHWzNS87bcEoPZnrWJOdphph2vhaF26vEIH3U
         fYXXXP4i4PlAOFz27S3oz3dlgP89HZe8KojJM4zQP7E279kE3DNNWoWtdYPEEC8lP99f
         F68NS/Atm7+muKpGaoI7J5TGcmmaS2dJXpurv4iPeJOrOLY924xIrlIN5tzcDeK96+h1
         V42A==
MIME-Version: 1.0
X-Received: by 10.50.78.66 with SMTP id z2mr827603igw.27.1403045889041; Tue,
 17 Jun 2014 15:58:09 -0700 (PDT)
Received: by 10.50.230.161 with HTTP; Tue, 17 Jun 2014 15:58:08 -0700 (PDT)
Date: Tue, 17 Jun 2014 18:58:08 -0400
Message-ID: <CAAADf7K_bqDKD91UFCtd8w6g_P96ZCNJAHfpLu8zxZmKfcShoQ@mail.gmail.com>
Subject: Contribute to Spark - Need a mentor.
From: Michael Giannakopoulos <miccagiann@gmail.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/mixed; boundary=047d7b6d9f94d2926504fc101320
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6d9f94d2926504fc101320
Content-Type: multipart/alternative; boundary=047d7b6d9f94d2926104fc10131e

--047d7b6d9f94d2926104fc10131e
Content-Type: text/plain; charset=UTF-8

Hi all,

My name is Michael Giannakopoulos and I am a recent M.Sc. graduate
from University of Toronto majoring in Computer Science. I would like to
contribute in the development of this open source project. Is it possible
to work
under the supervision of a mentor? I specialize in Data Analytics,
Data Management and Machine Learning. I am currently learning the
Scala language and I have experience using Java, C++, C and Matlab. I have
already read the Spark and Shark papers.

Together with this mail you will find attached my resume.

Thank you so much for your time and your help,
Michael

--047d7b6d9f94d2926104fc10131e
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div>Hi all,</div><div><br></div><div>My name is Michael G=
iannakopoulos and I am a recent M.Sc. graduate=C2=A0</div><div>from Univers=
ity of Toronto majoring in Computer Science. I would like to=C2=A0</div><di=
v>contribute in the development of this open source project. Is it possible=
 to work=C2=A0</div>
<div>under the supervision of a mentor? I specialize in Data Analytics,=C2=
=A0</div><div>Data Management and Machine Learning. I am currently learning=
 the=C2=A0</div><div>Scala language and I have experience using Java, C++, =
C and Matlab. I have</div>
<div>already read the Spark and Shark papers.</div><div><br></div><div>Toge=
ther with this mail you will find attached my resume.</div><div><br></div><=
div>Thank you so much for your time and your help,</div><div>Michael</div>
</div>

--047d7b6d9f94d2926104fc10131e--
--047d7b6d9f94d2926504fc101320--

From dev-return-8055-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 18 00:00:30 2014
Return-Path: <dev-return-8055-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6763B11D33
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Jun 2014 00:00:30 +0000 (UTC)
Received: (qmail 68094 invoked by uid 500); 18 Jun 2014 00:00:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68031 invoked by uid 500); 18 Jun 2014 00:00:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68020 invoked by uid 99); 18 Jun 2014 00:00:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 00:00:28 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of xwei@palantir.com designates 66.70.54.21 as permitted sender)
Received: from [66.70.54.21] (HELO mxw1.palantir.com) (66.70.54.21)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 00:00:23 +0000
Received: from EX02-WEST.YOJOE.local ([169.254.1.252]) by
 EX03-WEST.YOJOE.local ([169.254.2.112]) with mapi id 14.03.0158.001; Tue, 17
 Jun 2014 17:00:01 -0700
From: Xiaokai Wei <xwei@palantir.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
CC: Jason Zhao <jasonz@palantir.com>
Subject: Contributing to MLlib on GLM
Thread-Topic: Contributing to MLlib on GLM
Thread-Index: AQHPiog/r6mk1iBN0UqEXXCsX5jdrw==
Date: Wed, 18 Jun 2014 00:00:00 +0000
Message-ID: <CFC6248F.577%xwei@palantir.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: yes
X-MS-TNEF-Correlator:
x-originating-ip: [10.100.94.166]
Content-Type: multipart/signed; protocol="application/pkcs7-signature";
	micalg=sha1; boundary="B_3485869199_1133745"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--B_3485869199_1133745
Content-type: multipart/alternative;
	boundary="B_3485869199_1119870"


--B_3485869199_1119870
Content-type: text/plain;
	charset="US-ASCII"
Content-transfer-encoding: 7bit

Hi,

I am an intern at PalantirTech and we are building some stuff on top of
MLlib. In Particular, GLM is of great interest to us.  Though
GeneralizedLinearModel in MLlib 1.0.0 has some important GLMs such as
Logistic Regression, Linear Regression, some other important GLMs like
Poisson Regression are still missing.

I am curious that if anyone is already working on other GLMs (e.g. Poisson,
Gamma). If not, we would like to contribute to MLlib on GLM. Is adding more
GLMs on the roadmap of MLlib?


Sincerely,

Xiaokai



--B_3485869199_1119870
Content-type: text/html;
	charset="US-ASCII"
Content-transfer-encoding: quoted-printable

<html><head></head><body style=3D"word-wrap: break-word; -webkit-nbsp-mode: s=
pace; -webkit-line-break: after-white-space; color: rgb(0, 0, 0); font-size:=
 14px; font-family: Calibri, sans-serif;"><div>Hi,</div><div><br></div><div>=
I am an intern at PalantirTech and we are building some stuff on top of MLli=
b. In Particular, GLM is of great interest to us. &nbsp;Though GeneralizedLi=
nearModel in MLlib 1.0.0 has some important GLMs such as Logistic Regression=
, Linear Regression, some other important GLMs like Poisson Regression are s=
till missing.&nbsp;</div><div><br></div><div>I am curious that if anyone is =
already working on other GLMs (e.g. Poisson, Gamma). If not, we would like t=
o contribute to MLlib on GLM. Is adding more GLMs on the roadmap of MLlib?</=
div><div><br></div><div><br></div><div>Sincerely,</div><div><br></div><div>X=
iaokai</div></body></html>

--B_3485869199_1119870--

--B_3485869199_1133745
Content-Type: application/pkcs7-signature; name="smime.p7s"
Content-Transfer-Encoding: base64
Content-Disposition: attachment; filename="smime.p7s"

MIISIgYJKoZIhvcNAQcCoIISEzCCEg8CAQExCzAJBgUrDgMCGgUAMAsGCSqGSIb3DQEHAaCC
D+4wggVeMIIERqADAgECAhAB6msEqm41R/Bx0OulGCIrMA0GCSqGSIb3DQEBBQUAMGIxCzAJ
BgNVBAYTAlVTMRUwEwYDVQQKEwxEaWdpQ2VydCBJbmMxGTAXBgNVBAsTEHd3dy5kaWdpY2Vy
dC5jb20xITAfBgNVBAMTGERpZ2lDZXJ0IEFzc3VyZWQgSUQgQ0EtMTAeFw0xNDA1MTQwMDAw
MDBaFw0xNTA1MTMxMjAwMDBaMIGTMQswCQYDVQQGEwJVUzETMBEGA1UECBMKQ2FsaWZvcm5p
YTESMBAGA1UEBxMJUGFsbyBBbHRvMSMwIQYDVQQKExpQYWxhbnRpciBUZWNobm9sb2dpZXMg
SW5jLjEUMBIGA1UEAxMLWGlhb2thaSBXZWkxIDAeBgkqhkiG9w0BCQEWEXh3ZWlAcGFsYW50
aXIuY29tMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEA2xJuNwYJloGxm2DyqXtr
duxjSR30ClKRAgIPoTWCe3XSPzuoUUsqNYlF9WPb/ylMKD1Z7SA+mBoF/hjQeDPVom+9U4Ea
WomhLJyHV5XskN9WgJLRp9VXPksCNqCKgWSlJclL+QRLSSAZS5X/ZJ2vzi0ihpYz9nlWmgVF
b6LvfMnsXkjcb2Ze8Z95Ot0WqX6ESO4zqCxcxUA4SS1I/xcisevdaBN83bAQtOt1BIknrW6n
VulJthU6gWB64OB9npEmGUbz/x1yb4PxpGUfwck006CrrsQiZAalGijcyJfq5Wx/k0rkymJ9
eO5G+ec7YCUXhEZ12frq7DTd+p7mh68RXwIDAQABo4IB3DCCAdgwHwYDVR0jBBgwFoAUFQAS
KxOYspkH7R7for5XDStnAs0wHQYDVR0OBBYEFCDSi86Mrk5rfarYlNxtFYZrEGvuMAwGA1Ud
EwEB/wQCMAAwHAYDVR0RBBUwE4EReHdlaUBwYWxhbnRpci5jb20wDgYDVR0PAQH/BAQDAgWg
MB0GA1UdJQQWMBQGCCsGAQUFBwMCBggrBgEFBQcDBDBDBgNVHSAEPDA6MDgGCmCGSAGG/WwE
AQIwKjAoBggrBgEFBQcCARYcaHR0cHM6Ly93d3cuZGlnaWNlcnQuY29tL0NQUzB9BgNVHR8E
djB0MDigNqA0hjJodHRwOi8vY3JsMy5kaWdpY2VydC5jb20vRGlnaUNlcnRBc3N1cmVkSURD
QS0xLmNybDA4oDagNIYyaHR0cDovL2NybDQuZGlnaWNlcnQuY29tL0RpZ2lDZXJ0QXNzdXJl
ZElEQ0EtMS5jcmwwdwYIKwYBBQUHAQEEazBpMCQGCCsGAQUFBzABhhhodHRwOi8vb2NzcC5k
aWdpY2VydC5jb20wQQYIKwYBBQUHMAKGNWh0dHA6Ly9jYWNlcnRzLmRpZ2ljZXJ0LmNvbS9E
aWdpQ2VydEFzc3VyZWRJRENBLTEuY3J0MA0GCSqGSIb3DQEBBQUAA4IBAQBS0T75xQkXGyqr
K+06FyCQPNSnwDD7+jNysrDI3xZymDMbMMhiEs1RvxlTuerTCMcw6oWkKQx8+fHcXBmDwXOa
+aBB6iUBFvMqzigipe857lA8FWUVjl2LHmSvtuSnM8uBk4wDmx7vTITH6ccn9TWh100qyfJn
HDrd2nAs7ac+gfDYBNFbwUE+3fwooyR+zdIquCgkiB3oNmdTVAcrUi7IuT+CMEyeDcEgib8j
BDFNAOlB5WBSj3wurVEVQg9Uf9xsH+USqFU7tqOeYDbV015153x8RRq3wgczR3D710K1fbUf
uacrN6wQm+yz1eMtShG66Gc/3pdio03f1cuLL5VPMIIGzTCCBbWgAwIBAgIQBv35A5YDreoA
Cus/J7u6GzANBgkqhkiG9w0BAQUFADBlMQswCQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNl
cnQgSW5jMRkwFwYDVQQLExB3d3cuZGlnaWNlcnQuY29tMSQwIgYDVQQDExtEaWdpQ2VydCBB
c3N1cmVkIElEIFJvb3QgQ0EwHhcNMDYxMTEwMDAwMDAwWhcNMjExMTEwMDAwMDAwWjBiMQsw
CQYDVQQGEwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3d3cuZGlnaWNl
cnQuY29tMSEwHwYDVQQDExhEaWdpQ2VydCBBc3N1cmVkIElEIENBLTEwggEiMA0GCSqGSIb3
DQEBAQUAA4IBDwAwggEKAoIBAQDogi2Z+crCQpWlgHNAcNKeVlRcqcTSQQaPyTP8TUWRXIGf
7Syc+BZZ3561JBXCmLm0d0ncicQK2q/LXmvtrbBxMevPOkAMRk2T7It6NggDqww0/hhJgv7H
xzFIgHweog+SDlDJxofrNj/YMMP/pvf7os1vcyP+rFYFkPAyIRaJxnCI+QWXfaPHQ90C6Ds9
7bFBo+0/vtuVSMTuHrPyvAwrmdDGXRJCgeGDboJzPyZLFJCuWWYKxI2+0s4Grq2Eb0iEm09A
ufFM8q+Y+/bOQF1c9qjxL6/siSLyaxhlscFzrdfx2M8eCnRcQrhofrfVdwonVnwPYqQ/MhRg
lf0HBKIJAgMBAAGjggN6MIIDdjAOBgNVHQ8BAf8EBAMCAYYwOwYDVR0lBDQwMgYIKwYBBQUH
AwEGCCsGAQUFBwMCBggrBgEFBQcDAwYIKwYBBQUHAwQGCCsGAQUFBwMIMIIB0gYDVR0gBIIB
yTCCAcUwggG0BgpghkgBhv1sAAEEMIIBpDA6BggrBgEFBQcCARYuaHR0cDovL3d3dy5kaWdp
Y2VydC5jb20vc3NsLWNwcy1yZXBvc2l0b3J5Lmh0bTCCAWQGCCsGAQUFBwICMIIBVh6CAVIA
QQBuAHkAIAB1AHMAZQAgAG8AZgAgAHQAaABpAHMAIABDAGUAcgB0AGkAZgBpAGMAYQB0AGUA
IABjAG8AbgBzAHQAaQB0AHUAdABlAHMAIABhAGMAYwBlAHAAdABhAG4AYwBlACAAbwBmACAA
dABoAGUAIABEAGkAZwBpAEMAZQByAHQAIABDAFAALwBDAFAAUwAgAGEAbgBkACAAdABoAGUA
IABSAGUAbAB5AGkAbgBnACAAUABhAHIAdAB5ACAAQQBnAHIAZQBlAG0AZQBuAHQAIAB3AGgA
aQBjAGgAIABsAGkAbQBpAHQAIABsAGkAYQBiAGkAbABpAHQAeQAgAGEAbgBkACAAYQByAGUA
IABpAG4AYwBvAHIAcABvAHIAYQB0AGUAZAAgAGgAZQByAGUAaQBuACAAYgB5ACAAcgBlAGYA
ZQByAGUAbgBjAGUALjALBglghkgBhv1sAxUwEgYDVR0TAQH/BAgwBgEB/wIBADB5BggrBgEF
BQcBAQRtMGswJAYIKwYBBQUHMAGGGGh0dHA6Ly9vY3NwLmRpZ2ljZXJ0LmNvbTBDBggrBgEF
BQcwAoY3aHR0cDovL2NhY2VydHMuZGlnaWNlcnQuY29tL0RpZ2lDZXJ0QXNzdXJlZElEUm9v
dENBLmNydDCBgQYDVR0fBHoweDA6oDigNoY0aHR0cDovL2NybDMuZGlnaWNlcnQuY29tL0Rp
Z2lDZXJ0QXNzdXJlZElEUm9vdENBLmNybDA6oDigNoY0aHR0cDovL2NybDQuZGlnaWNlcnQu
Y29tL0RpZ2lDZXJ0QXNzdXJlZElEUm9vdENBLmNybDAdBgNVHQ4EFgQUFQASKxOYspkH7R7f
or5XDStnAs0wHwYDVR0jBBgwFoAUReuir/SSy4IxLVGLp6chnfNtyA8wDQYJKoZIhvcNAQEF
BQADggEBAEZQPsm3KCSnOB22WymvUs9S6TFHq1Zce9UNC0Gz7+x1H3Q48rJcYaKclcNQ5IK5
I9G6OoZyrTh4rHVdFxc0ckeFlFbR67s2hHfMJKXzBBlVqefj56tizfuLLZDCwNK1lL1eT7EF
0g49GqkUW6aGMWKoqDPkmzmnxPXOHXh2lCVz5Cqrz5x2S+1fwksW5EtwTACJHvzFebxMElf+
X+EevAJdqP77BzhPDcZdkbkPZ0XN1oPt55INjbFpjE/7WeAjD9KqrgB87pxCDs+R1ye3Fu4P
w718CqDuLAhVhSK46xgaTfwqIa1JMYNHlXdx3LEbS0scEJx3FMGdTy9alQgpECYwggO3MIIC
n6ADAgECAhAM5+DlF9hG/o/lYPwb8DA5MA0GCSqGSIb3DQEBBQUAMGUxCzAJBgNVBAYTAlVT
MRUwEwYDVQQKEwxEaWdpQ2VydCBJbmMxGTAXBgNVBAsTEHd3dy5kaWdpY2VydC5jb20xJDAi
BgNVBAMTG0RpZ2lDZXJ0IEFzc3VyZWQgSUQgUm9vdCBDQTAeFw0wNjExMTAwMDAwMDBaFw0z
MTExMTAwMDAwMDBaMGUxCzAJBgNVBAYTAlVTMRUwEwYDVQQKEwxEaWdpQ2VydCBJbmMxGTAX
BgNVBAsTEHd3dy5kaWdpY2VydC5jb20xJDAiBgNVBAMTG0RpZ2lDZXJ0IEFzc3VyZWQgSUQg
Um9vdCBDQTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAK0OFc7kQ4BcsYfzt2D5
cRKlrtwmlIiq9M71IDkoWGAM+IDaqRWVMmE8tbEohIqK3J8KDIMXeo+QrIrneVNcMYQq9g+Y
MjZ2zN7dPKii72r7IfJSYd+fINcf4rHZ/hhk0hJbX/lYGDW8R82hNvlrf9SwOD7BG8OMM9nY
Lxj+KA+zp4PWw25EwGE1lhb+WZyLdm3X8aJLDSv/C3LanmDQjpA1xnhVhyChz+VtCshJfDGY
M2wi6YfQMlqiuhOCEe05F52ZOnKh5vqk2dUXMXWuhX0irj8BRob2KHnIsdrkVxfEfhwOsLSS
plazvbKX7aqn8LfFqD+VFtD/oZbrCF8Yd08CAwEAAaNjMGEwDgYDVR0PAQH/BAQDAgGGMA8G
A1UdEwEB/wQFMAMBAf8wHQYDVR0OBBYEFEXroq/0ksuCMS1Ri6enIZ3zbcgPMB8GA1UdIwQY
MBaAFEXroq/0ksuCMS1Ri6enIZ3zbcgPMA0GCSqGSIb3DQEBBQUAA4IBAQCiDrzf4u3w43Jz
emSUv/dyZtgy5EJ1Yq6H6/LV2d5Ws5/MzhQouQ2XYFwSTFjk0z2DSUVYlzVpGqhH6lbGeasS
2GeBhN9/CTyU5rgmLCC9PbMoifdf/yLil4Qf6WXvh+DfwWdJs13rsgkq6ybteL59PyvztyY1
bV+JAbZJW58BBZurPSXBzLZ/wvFvhsb6ZGjrgS2U60K3+owe3WLxvlBnt2y98/Efaww2BxZ/
N3ypW2168RJGYIPXJwS+S86XvsNnKmgR34DnDDNmvxMNFG7zfx9jEB76jRslbWyPpbdhAbHS
oyahEHGdreLD+cOZUbcrBwjOLuZQsqf6CkUvovDyMYIB/DCCAfgCAQEwdjBiMQswCQYDVQQG
EwJVUzEVMBMGA1UEChMMRGlnaUNlcnQgSW5jMRkwFwYDVQQLExB3d3cuZGlnaWNlcnQuY29t
MSEwHwYDVQQDExhEaWdpQ2VydCBBc3N1cmVkIElEIENBLTECEAHqawSqbjVH8HHQ66UYIisw
CQYFKw4DAhoFAKBdMCMGCSqGSIb3DQEJBDEWBBQnHuAx6WvtHW3OveMND87swEWT3DAYBgkq
hkiG9w0BCQMxCwYJKoZIhvcNAQcBMBwGCSqGSIb3DQEJBTEPFw0xNDA2MTcyMzU5NTlaMA0G
CSqGSIb3DQEBAQUABIIBABz6DyW4msxYZLVot3ABa6dZy/z2DUjMaK3HtLGX3w+fjqRfmK4L
VVrp5tyPBX1wAqYg07IdQ4Fnd5knGgLEgZtVQEDVwXvvTav9ePH+/l2oZ2Tdn1EGG3zUvmPD
hJ5guv42wgce++ixfLhteQ7fwzNWBBoiE1Pp2nXGO0UxNvo5aW+kqI+pBii4cHH+uDp8juQY
Xviue8EiZvudzKjF2WDnGs/NMmCwmDx6wrCsN6xWDux0PGxSMeKYoZTXcux5m49tqa9My1Vo
grYRnuj71WkRH9pbj0qGsGaIpS3IvH4n6dqVkSv7PjoXS5db1sXOzJbxptXuPUoUzRdM6Jt5
Yfs=

--B_3485869199_1133745--

From dev-return-8056-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 18 01:11:07 2014
Return-Path: <dev-return-8056-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E99AD11F08
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Jun 2014 01:11:06 +0000 (UTC)
Received: (qmail 83385 invoked by uid 500); 18 Jun 2014 01:11:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83346 invoked by uid 500); 18 Jun 2014 01:11:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83334 invoked by uid 99); 18 Jun 2014 01:11:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 01:11:05 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of henry.saputra@gmail.com designates 74.125.82.44 as permitted sender)
Received: from [74.125.82.44] (HELO mail-wg0-f44.google.com) (74.125.82.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 01:11:01 +0000
Received: by mail-wg0-f44.google.com with SMTP id x13so105643wgg.15
        for <dev@spark.apache.org>; Tue, 17 Jun 2014 18:10:37 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type:content-transfer-encoding;
        bh=0vGoNw2sZYvq+CgXh9KWk/Ea5+oWyOqoa06mCZpmYKU=;
        b=bhHAZMxxHDbmR3T/4g4Fyyj/GfngOWAHcUii4ZIcHKNlCBwUndx8LlZxdVWOUDIupc
         214NOQFi/gNYeWrr/PL8OtB1hOKdDCscKvW/HB3YXnYKSVxTydrh8MxvbEYts1htxQsN
         8PlrBaGEO9w0KhKvHT3f59Z9FHA18akkg9UCVqbFxYdUBdCEPSu1B66XWIa99yv2lT6x
         2B9DiC/IZ1UTF5Z9h04QScY4Ntc/VvIcwe9P/pbtdV2tn3i/uzwuWC3qZC8Gd07jdv3v
         vc2N431JaQJZdace2+0F4qVzgnKhupkVeRPGFTjd0ubyOx43Tnql1WUDg/M224zR3BJW
         Qh5Q==
MIME-Version: 1.0
X-Received: by 10.194.83.39 with SMTP id n7mr41908014wjy.58.1403053837860;
 Tue, 17 Jun 2014 18:10:37 -0700 (PDT)
Received: by 10.216.165.71 with HTTP; Tue, 17 Jun 2014 18:10:37 -0700 (PDT)
In-Reply-To: <AF958CC3-2FFD-4A16-80A8-AA56DF239090@gmail.com>
References: <811039BF-0610-4BAC-BFD7-41BBC981768C@gmail.com>
	<CABKvOWstGq-QGNHZnmvRptYcg5idh7LACOk1z3jn5bR=9L_Qrw@mail.gmail.com>
	<AF958CC3-2FFD-4A16-80A8-AA56DF239090@gmail.com>
Date: Tue, 17 Jun 2014 18:10:37 -0700
Message-ID: <CALuGr6bm7yKxkN4uKjHByYmpqY2=387PLqzErSo77sTcGHke1A@mail.gmail.com>
Subject: Re: Run ScalaTest inside Intellij IDEA
From: Henry Saputra <henry.saputra@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

I got stuck on this one too after did git pull from master.

Have not been able to resolve it yet =3D(


- Henry

On Wed, Jun 11, 2014 at 6:51 AM, Yijie Shen <henry.yijieshen@gmail.com> wro=
te:
> Thx Qiuzhuang, the problems disappeared after I add assembly jar at the h=
ead of list dependencies in *.iml, but while running test in Spark SQL(SQLQ=
uerySuite in sql-core), another two error occurs:
>
> Error 1:
> Error:scalac:
>      while compiling: /Users/yijie/code/apache.spark.master/sql/core/src/=
main/scala/org/apache/spark/sql/test/TestSQLContext.scala
>         during phase: jvm
>      library version: version 2.10.4
>     compiler version: version 2.10.4
>   reconstructed args: -Xmax-classfile-name 120 -deprecation -P:genjavadoc=
:out=3D/Users/yijie/code/apache.spark.master/sql/core/target/java -feature =
-classpath /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/=
lib/ant-javafx.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Conten=
ts/Home/lib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Conten=
ts/Home/lib/javafx-doclet.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51=
.jdk/Contents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1=
.7.0_51.jdk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachine=
s/jdk1.7.0_51.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMac=
hines/jdk1.7.0_51.jdk/Contents/Home/lib/tools.jar:/Library/Java/JavaVirtual=
Machines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/J=
avaVirtualMachines/jdk1.7.0_51.jdk/Conte=E2=80=A6
> =E2=80=A6
> ...
> /Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.=
jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib=
/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jr=
e/classes:/Users/yijie/code/apache.spark.master/lib_managed/jars/scala-libr=
ary-2.10.4.jar -Xplugin:/Users/yijie/code/apache.spark.master/lib_managed/j=
ars/genjavadoc-plugin_2.10.4-0.5.jar -Xplugin:/Users/yijie/code/apache.spar=
k.master/lib_managed/jars/genjavadoc-plugin_2.10.4-0.5.jar
>   last tree to typer: Literal(Constant(parquet.io.api.Converter))
>               symbol: null
>    symbol definition: null
>                  tpe: Class(classOf[parquet.io.api.Converter])
>        symbol owners:
>       context owners: object TestSQLContext -> package test
> =3D=3D Enclosing template or block =3D=3D
> Template( // val <local TestSQLContext>: <notype> in object TestSQLContex=
t, tree.tpe=3Dorg.apache.spark.sql.test.TestSQLContext.type
>   "org.apache.spark.sql.SQLContext" // parents
>   ValDef(
>     private
>     "_"
>     <tpt>
>     <empty>
>   )
>   // 2 statements
>   DefDef( // private def readResolve(): Object in object TestSQLContext
>     <method> private <synthetic>
>     "readResolve"
>     []
>     List(Nil)
>     <tpt> // tree.tpe=3DObject
>     test.this."TestSQLContext" // object TestSQLContext in package test, =
tree.tpe=3Dorg.apache.spark.sql.test.TestSQLContext.type
>   )
>   DefDef( // def <init>(): org.apache.spark.sql.test.TestSQLContext.type =
in object TestSQLContext
>     <method>
>     "<init>"
>     []
>     List(Nil)
>     <tpt> // tree.tpe=3Dorg.apache.spark.sql.test.TestSQLContext.type
>     Block( // tree.tpe=3DUnit
>       Apply( // def <init>(sparkContext: org.apache.spark.SparkContext): =
org.apache.spark.sql.SQLContext in class SQLContext, tree.tpe=3Dorg.apache.=
spark.sql.SQLContext
>         TestSQLContext.super."<init>" // def <init>(sparkContext: org.apa=
che.spark.SparkContext): org.apache.spark.sql.SQLContext in class SQLContex=
t, tree.tpe=3D(sparkContext: org.apache.spark.SparkContext)org.apache.spark=
.sql.SQLContext
>         Apply( // def <init>(master: String,appName: String,conf: org.apa=
che.spark.SparkConf): org.apache.spark.SparkContext in class SparkContext, =
tree.tpe=3Dorg.apache.spark.SparkContext
>           new org.apache.spark.SparkContext."<init>" // def <init>(master=
: String,appName: String,conf: org.apache.spark.SparkConf): org.apache.spar=
k.SparkContext in class SparkContext, tree.tpe=3D(master: String, appName: =
String, conf: org.apache.spark.SparkConf)org.apache.spark.SparkContext
>           // 3 arguments
>           "local"
>           "TestSQLContext"
>           Apply( // def <init>(): org.apache.spark.SparkConf in class Spa=
rkConf, tree.tpe=3Dorg.apache.spark.SparkConf
>             new org.apache.spark.SparkConf."<init>" // def <init>(): org.=
apache.spark.SparkConf in class SparkConf, tree.tpe=3D()org.apache.spark.Sp=
arkConf
>             Nil
>           )
>         )
>       )
>       ()
>     )
>   )
> )
> =3D=3D Expanded type of tree =3D=3D
> ConstantType(value =3D Constant(parquet.io.api.Converter))
> uncaught exception during compilation: java.lang.AssertionError
>
> Error 2:
>
> Error:scalac: Error: assertion failed: List(object package$DebugNode, obj=
ect package$DebugNode)
> java.lang.AssertionError: assertion failed: List(object package$DebugNode=
, object package$DebugNode)
>         at scala.reflect.internal.Symbols$Symbol.suchThat(Symbols.scala:1=
678)
>         at scala.reflect.internal.Symbols$ClassSymbol.companionModule0(Sy=
mbols.scala:2988)
>         at scala.reflect.internal.Symbols$ClassSymbol.companionModule(Sym=
bols.scala:2991)
>         at scala.tools.nsc.backend.jvm.GenASM$JPlainBuilder.genClass(GenA=
SM.scala:1371)
>         at scala.tools.nsc.backend.jvm.GenASM$AsmPhase.run(GenASM.scala:1=
20)
>         at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1=
583)
>         at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
>         at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
>         at scala.tools.nsc.Global$Run.compile(Global.scala:1662)
>         at xsbt.CachedCompiler0.run(CompilerInterface.scala:126)
>         at xsbt.CachedCompiler0.run(CompilerInterface.scala:102)
>         at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
>         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccess=
orImpl.java:57)
>         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMeth=
odAccessorImpl.java:43)
>         at java.lang.reflect.Method.invoke(Method.java:606)
>         at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:10=
2)
>         at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala=
:48)
>         at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala=
:41)
>         at org.jetbrains.jps.incremental.scala.local.IdeaIncrementalCompi=
ler.compile(IdeaIncrementalCompiler.scala:28)
>         at org.jetbrains.jps.incremental.scala.local.LocalServer.compile(=
LocalServer.scala:25)
>         at org.jetbrains.jps.incremental.scala.remote.Main$.make(Main.sca=
la:64)
>         at org.jetbrains.jps.incremental.scala.remote.Main$.nailMain(Main=
.scala:22)
>         at org.jetbrains.jps.incremental.scala.remote.Main.nailMain(Main.=
scala)
>         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccess=
orImpl.java:57)
>         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMeth=
odAccessorImpl.java:43)
>         at java.lang.reflect.Method.invoke(Method.java:606)
>         at com.martiansoftware.nailgun.NGSession.run(NGSession.java:319)
>
>
> On Jun 11, 2014, at 11:17 AM, Qiuzhuang Lian <qiuzhuang.lian@gmail.com> w=
rote:
>
>> I also run into this problem when running examples in IDEA. The issue lo=
oks that it uses depends on too many jars and that the classpath seems to h=
ave length limit. So I import the assembly jar and put the head of the list=
 dependent path and it works.
>>
>> Thanks,
>> Qiuzhuang
>>
>>
>> On Wed, Jun 11, 2014 at 10:39 AM, =E7=94=B3=E6=AF=85=E6=9D=B0 <henry.yij=
ieshen@gmail.com> wrote:
>> Hi All,
>>
>> I want to run ScalaTest Suite in IDEA directly, but it seems didn=E2=80=
=99t pass the make phase before test running.
>> The problems are as follows:
>>
>> /Users/yijie/code/apache.spark.master/core/src/main/scala/org/apache/spa=
rk/executor/MesosExecutorBackend.scala
>> Error:(44, 35) type mismatch;
>>  found   : org.apache.mesos.protobuf.ByteString
>>  required: com.google.protobuf.ByteString
>>       .setData(ByteString.copyFrom(data))
>>                                   ^
>> /Users/yijie/code/apache.spark.master/core/src/main/scala/org/apache/spa=
rk/scheduler/cluster/mesos/MesosSchedulerBackend.scala
>> Error:(119, 35) type mismatch;
>>  found   : org.apache.mesos.protobuf.ByteString
>>  required: com.google.protobuf.ByteString
>>       .setData(ByteString.copyFrom(createExecArg()))
>>                                   ^
>> Error:(257, 35) type mismatch;
>>  found   : org.apache.mesos.protobuf.ByteString
>>  required: com.google.protobuf.ByteString
>>       .setData(ByteString.copyFrom(task.serializedTask))
>>                                   ^
>>
>> Before I run test in IDEA, I build spark through =E2=80=99sbt/sbt assemb=
ly=E2=80=99,
>> import projects into IDEA after =E2=80=99sbt/sbt gen-idea=E2=80=99,
>> and able to run test in Terminal =E2=80=99sbt/sbt test=E2=80=99
>>
>> Are there anything I leave out in order to run/debug testsuite inside ID=
EA?
>>
>> Best regards,
>> Yijie
>>
>

From dev-return-8057-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 18 02:14:13 2014
Return-Path: <dev-return-8057-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E833D11092
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Jun 2014 02:14:13 +0000 (UTC)
Received: (qmail 76360 invoked by uid 500); 18 Jun 2014 02:14:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76295 invoked by uid 500); 18 Jun 2014 02:14:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76284 invoked by uid 99); 18 Jun 2014 02:14:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 02:14:13 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [219.142.118.228] (HELO SINA-HUB01.staff.sina.com.cn) (219.142.118.228)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 02:14:08 +0000
Received: from [10.221.12.58] (10.221.12.58) by mail.staff.sina.com.cn
 (10.210.97.51) with Microsoft SMTP Server (TLS) id 14.2.247.3; Wed, 18 Jun
 2014 10:13:45 +0800
From: Gang Bai <baigang@staff.sina.com.cn>
Content-Type: multipart/alternative;
	boundary="Apple-Mail=_46EB07B9-B478-4EFA-9E35-78775D577F31"
Subject: Int tolerance in LBFGS.setConvergenceTol causes problems
Message-ID: <CBC8D0E3-9B6C-478A-B025-1DE0B38E5BF4@staff.sina.com.cn>
Date: Wed, 18 Jun 2014 10:13:42 +0800
To: <dev@spark.apache.org>
MIME-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
X-Mailer: Apple Mail (2.1878.2)
X-Originating-IP: [10.221.12.58]
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_46EB07B9-B478-4EFA-9E35-78775D577F31
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain; charset="windows-1252"

Hi folks,

I am implementing a regression model for count data which uses LBFGS for =
parameter estimation. Following the patterns in mllib.regression, I =
created an object PoissonRegressionModelWithLBFGS, which creates a new =
instance of class PoissonRegressionModelWithLBFGS and invokes the run =
method to get the weights/parameters.=20

The implementations are straightforward. But I encountered a problem =
while using class LBFGS. The parameter of setConvergenceTol is of type =
Int rather than Double. This leads to the inability to specify a =
tolerance less than 1.0 and a type mismatch error when passing a Double =
value. In fact, the class LBFGS internally uses convergenceTol as a =
Double var. So we can safely change the parameter of setConvergenceTol =
from type Int to type Double.=20

I=92ve created a pull request for this. Please take a review here: =
https://github.com/apache/spark/pull/1104/files

Best regards,
Gang=

--Apple-Mail=_46EB07B9-B478-4EFA-9E35-78775D577F31--

From dev-return-8058-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 18 02:37:29 2014
Return-Path: <dev-return-8058-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 62FBF110F3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Jun 2014 02:37:29 +0000 (UTC)
Received: (qmail 961 invoked by uid 500); 18 Jun 2014 02:37:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 904 invoked by uid 500); 18 Jun 2014 02:37:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 893 invoked by uid 99); 18 Jun 2014 02:37:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 02:37:28 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.45 as permitted sender)
Received: from [209.85.216.45] (HELO mail-qa0-f45.google.com) (209.85.216.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 02:37:26 +0000
Received: by mail-qa0-f45.google.com with SMTP id v10so183884qac.18
        for <dev@spark.apache.org>; Tue, 17 Jun 2014 19:37:01 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=T3AREM+zWNT6yY0kskyOHqKsqaZI5UqcJLnQ6eVphR0=;
        b=hhY2hx4McPlveUGv7Ukoftq9xqFwcuSd9Q0kvbo8dEY4lUQ/3Gpq7iUG4g8pDShqz1
         EeMfNc8ixGAddPF+FLICYww32q/Uo14dCGEqDC+rZUYWIDcIkBWxrcsWBKqXqj5U+7/6
         /D08ZLz69KGmk10Jso2CxLWbO1Nbb1lMkz7hl1dAYR97m0HeM8JRiSP64fsgFjHPE2Qs
         r/B7r330OZXkMXo/SweO86GcYlN3z8pKzNxrnv5p4OAcZEC5YCb//q4z3MzwaamMHSed
         YTAG914PRt/Au4f2enaJUd29xV6+HxLO+aopu2TKcbTgFOg3ruAM2NvQ0b1jMvshsRpm
         a3Uw==
X-Gm-Message-State: ALoCoQlqZ6N3BMDvPSeVqiatskLDGtHQ0Ll2YW7VBY1/0rIXoGhvEg0eJui/ZE1gEtTs7IAn+c/D
MIME-Version: 1.0
X-Received: by 10.224.111.196 with SMTP id t4mr41699269qap.63.1403059021601;
 Tue, 17 Jun 2014 19:37:01 -0700 (PDT)
Received: by 10.140.106.202 with HTTP; Tue, 17 Jun 2014 19:37:01 -0700 (PDT)
In-Reply-To: <CFC6248F.577%xwei@palantir.com>
References: <CFC6248F.577%xwei@palantir.com>
Date: Tue, 17 Jun 2014 19:37:01 -0700
Message-ID: <CACBYxK+Qy0XNdcpouLfqkg9YWUVDcCUbhqL-f0af5FNgfHX6ig@mail.gmail.com>
Subject: Re: Contributing to MLlib on GLM
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: Jason Zhao <jasonz@palantir.com>
Content-Type: multipart/alternative; boundary=001a11c2d60094ecc204fc13222d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2d60094ecc204fc13222d
Content-Type: text/plain; charset=UTF-8

Hi Xiaokai,

I think MLLib is definitely interested in supporting additional GLMs.  I'm
not aware of anybody working on this at the moment.

-Sandy


On Tue, Jun 17, 2014 at 5:00 PM, Xiaokai Wei <xwei@palantir.com> wrote:

> Hi,
>
> I am an intern at PalantirTech and we are building some stuff on top of
> MLlib. In Particular, GLM is of great interest to us.  Though
> GeneralizedLinearModel in MLlib 1.0.0 has some important GLMs such as
> Logistic Regression, Linear Regression, some other important GLMs like
> Poisson Regression are still missing.
>
> I am curious that if anyone is already working on other GLMs (e.g.
> Poisson, Gamma). If not, we would like to contribute to MLlib on GLM. Is
> adding more GLMs on the roadmap of MLlib?
>
>
> Sincerely,
>
> Xiaokai
>

--001a11c2d60094ecc204fc13222d--

From dev-return-8059-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 18 02:48:03 2014
Return-Path: <dev-return-8059-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5ABFD11123
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Jun 2014 02:48:03 +0000 (UTC)
Received: (qmail 15650 invoked by uid 500); 18 Jun 2014 02:48:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15594 invoked by uid 500); 18 Jun 2014 02:48:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15574 invoked by uid 99); 18 Jun 2014 02:48:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 02:48:02 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of chenguancheng@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 02:48:00 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <chenguancheng@gmail.com>)
	id 1Wx5uR-0007qd-N9
	for dev@spark.incubator.apache.org; Tue, 17 Jun 2014 19:47:35 -0700
Date: Tue, 17 Jun 2014 19:47:35 -0700 (PDT)
From: gchen <chenguancheng@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1403059655706-7037.post@n3.nabble.com>
In-Reply-To: <CAPh_B=aTo7DFjd8+yJ3nub54i0TRRwKh6xK-52W=8HYtwyfYxA@mail.gmail.com>
References: <1402624426683-7003.post@n3.nabble.com> <1402710044343-7007.post@n3.nabble.com> <1402884278089-7012.post@n3.nabble.com> <CAPh_B=Z9QNVR6Cyscaa6RYUd+oUDLx0iap6QnP5Aj4=uBseovw@mail.gmail.com> <1402960522356-7016.post@n3.nabble.com> <CAPh_B=bbt3s1npkk4_YTWCY6r3z=xPQ7evOPFEKoqVi176OM-w@mail.gmail.com> <1402961167947-7019.post@n3.nabble.com> <CAPh_B=aTo7DFjd8+yJ3nub54i0TRRwKh6xK-52W=8HYtwyfYxA@mail.gmail.com>
Subject: Re: Big-Endian (IBM Power7) Spark Serialization issue
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Cool, so maybe when we swith to Snappy instead of LZF, we can workaround the
bug until the LZF upstream fix it, right?

In addition, is it valuable to add support for other compression codecs such
as LZ4? We observed 5% end-to-end improvement using LZ4 vs Snappy in
Terasort (Hadoop MR).



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Big-Endian-IBM-Power7-Spark-Serialization-issue-tp7003p7037.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8060-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 18 03:07:53 2014
Return-Path: <dev-return-8060-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 128F6111D5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Jun 2014 03:07:53 +0000 (UTC)
Received: (qmail 37756 invoked by uid 500); 18 Jun 2014 03:07:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37680 invoked by uid 500); 18 Jun 2014 03:07:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37669 invoked by uid 99); 18 Jun 2014 03:07:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 03:07:52 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dbtsai@stanford.edu designates 171.67.219.82 as permitted sender)
Received: from [171.67.219.82] (HELO smtp.stanford.edu) (171.67.219.82)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 03:07:50 +0000
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id 1774A34176F
	for <dev@spark.apache.org>; Tue, 17 Jun 2014 20:07:24 -0700 (PDT)
Received: from mail-qg0-f42.google.com (mail-qg0-f42.google.com [209.85.192.42])
	(using TLSv1 with cipher ECDHE-RSA-RC4-SHA (128/128 bits))
	(No client certificate requested)
	(Authenticated sender: dbtsai)
	by smtp.stanford.edu (Postfix) with ESMTPSA id B67A83416D0
	for <dev@spark.apache.org>; Tue, 17 Jun 2014 20:07:23 -0700 (PDT)
Received: by mail-qg0-f42.google.com with SMTP id e89so215570qgf.15
        for <dev@spark.apache.org>; Tue, 17 Jun 2014 20:07:22 -0700 (PDT)
X-Gm-Message-State: ALoCoQmQFL31m04ldGFN//glyz6sg5hNDoWKjQoVPniMfLPkRBMMpb2m1jpxzLWAI6i17gtkuIDH
MIME-Version: 1.0
X-Received: by 10.140.18.197 with SMTP id 63mr9279231qgf.105.1403060842898;
 Tue, 17 Jun 2014 20:07:22 -0700 (PDT)
Received: by 10.229.220.7 with HTTP; Tue, 17 Jun 2014 20:07:22 -0700 (PDT)
In-Reply-To: <CBC8D0E3-9B6C-478A-B025-1DE0B38E5BF4@staff.sina.com.cn>
References: <CBC8D0E3-9B6C-478A-B025-1DE0B38E5BF4@staff.sina.com.cn>
Date: Tue, 17 Jun 2014 20:07:22 -0700
Message-ID: <CAEYYnxZdEYWkS8bUxLJ0C+oUoLbs_-JKEZw4Feu_e9CnCmSgPg@mail.gmail.com>
Subject: Re: Int tolerance in LBFGS.setConvergenceTol causes problems
From: DB Tsai <dbtsai@stanford.edu>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Gang,

This is a bug, and I'm the one who did it :) Just add the comment to your P=
R.

Thanks.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Tue, Jun 17, 2014 at 7:13 PM, Gang Bai <baigang@staff.sina.com.cn> wrote=
:
> Hi folks,
>
> I am implementing a regression model for count data which uses LBFGS for =
parameter estimation. Following the patterns in mllib.regression, I created=
 an object PoissonRegressionModelWithLBFGS, which creates a new instance of=
 class PoissonRegressionModelWithLBFGS and invokes the run method to get th=
e weights/parameters.
>
> The implementations are straightforward. But I encountered a problem whil=
e using class LBFGS. The parameter of setConvergenceTol is of type Int rath=
er than Double. This leads to the inability to specify a tolerance less tha=
n 1.0 and a type mismatch error when passing a Double value. In fact, the c=
lass LBFGS internally uses convergenceTol as a Double var. So we can safely=
 change the parameter of setConvergenceTol from type Int to type Double.
>
> I=E2=80=99ve created a pull request for this. Please take a review here: =
https://github.com/apache/spark/pull/1104/files
>
> Best regards,
> Gang

From dev-return-8061-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 18 04:07:41 2014
Return-Path: <dev-return-8061-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8EA83112D6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Jun 2014 04:07:41 +0000 (UTC)
Received: (qmail 9036 invoked by uid 500); 18 Jun 2014 04:07:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8970 invoked by uid 500); 18 Jun 2014 04:07:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8959 invoked by uid 99); 18 Jun 2014 04:07:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 04:07:39 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.51] (HELO mail-qa0-f51.google.com) (209.85.216.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 04:07:37 +0000
Received: by mail-qa0-f51.google.com with SMTP id j7so247898qaq.10
        for <dev@spark.apache.org>; Tue, 17 Jun 2014 21:07:12 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=rahmJ6LVf0JDnjhPU6RWDGySyr+jOAVolkbQugRErO8=;
        b=LsHf9eQ6HY/uo9zxIypLV2/DWJsPn1J8vZFQZ7tsqfJDxwmnJljFW+YNaho8oPV9oc
         I/NNVGKB8Pyf+UEaOBCe1xXpowxR1pKZ7Cy9oNq6fTsKkyC3BZhjHDbNil3geFZ7CMF0
         matDRSk3uPb13ZDw3YLKxkHDr4LpRo4pMIkV2WsVT0TkIThHDJzOxJJ/Q8FAIUHaUyJR
         /+lKAd+ePqx8E1U388r4KxUOjqM/8T0gm6E+zQjhj+4TiM03wZnAUEJ7IRrR+LBcmUow
         JXeWk5Mn4r1qrplNwtx9ODeZD9u1cPsAGxngLgTmeiYbYQ8AhLNe5maEr00OpkhfwI7D
         GENQ==
X-Gm-Message-State: ALoCoQl4iQYJDOOv4lODtcp/wIG/6vUr5Q7rfL65Oe/lFWTR660Jn/V1tFfPPsZmcg1Knq8yGueB
MIME-Version: 1.0
X-Received: by 10.224.111.196 with SMTP id t4mr42092940qap.63.1403064432627;
 Tue, 17 Jun 2014 21:07:12 -0700 (PDT)
Received: by 10.96.98.100 with HTTP; Tue, 17 Jun 2014 21:07:12 -0700 (PDT)
In-Reply-To: <1403059655706-7037.post@n3.nabble.com>
References: <1402624426683-7003.post@n3.nabble.com>
	<1402710044343-7007.post@n3.nabble.com>
	<1402884278089-7012.post@n3.nabble.com>
	<CAPh_B=Z9QNVR6Cyscaa6RYUd+oUDLx0iap6QnP5Aj4=uBseovw@mail.gmail.com>
	<1402960522356-7016.post@n3.nabble.com>
	<CAPh_B=bbt3s1npkk4_YTWCY6r3z=xPQ7evOPFEKoqVi176OM-w@mail.gmail.com>
	<1402961167947-7019.post@n3.nabble.com>
	<CAPh_B=aTo7DFjd8+yJ3nub54i0TRRwKh6xK-52W=8HYtwyfYxA@mail.gmail.com>
	<1403059655706-7037.post@n3.nabble.com>
Date: Tue, 17 Jun 2014 21:07:12 -0700
Message-ID: <CAPh_B=b=Pfwm0NsXn21ZO2AAbcqWtTcYAsALqoMN+B5iZqoSFQ@mail.gmail.com>
Subject: Re: Big-Endian (IBM Power7) Spark Serialization issue
From: Reynold Xin <rxin@databricks.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2d6001aa59b04fc1465c3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2d6001aa59b04fc1465c3
Content-Type: text/plain; charset=UTF-8

It is actually pluggable. You can implement new compression codecs and just
change the config variable to use those.

On Tuesday, June 17, 2014, gchen <chenguancheng@gmail.com> wrote:

> Cool, so maybe when we swith to Snappy instead of LZF, we can workaround
> the
> bug until the LZF upstream fix it, right?
>
> In addition, is it valuable to add support for other compression codecs
> such
> as LZ4? We observed 5% end-to-end improvement using LZ4 vs Snappy in
> Terasort (Hadoop MR).
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Big-Endian-IBM-Power7-Spark-Serialization-issue-tp7003p7037.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c2d6001aa59b04fc1465c3--

From dev-return-8062-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 18 04:47:51 2014
Return-Path: <dev-return-8062-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6E3701138B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Jun 2014 04:47:51 +0000 (UTC)
Received: (qmail 58910 invoked by uid 500); 18 Jun 2014 04:47:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58844 invoked by uid 500); 18 Jun 2014 04:47:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58831 invoked by uid 99); 18 Jun 2014 04:47:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 04:47:50 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.128.182] (HELO mail-ve0-f182.google.com) (209.85.128.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 04:47:46 +0000
Received: by mail-ve0-f182.google.com with SMTP id oy12so294503veb.27
        for <dev@spark.apache.org>; Tue, 17 Jun 2014 21:47:21 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=knNjp9WH2Wu1kFQ4mT1kqfQb7Pa79QDwcMBWx5RYbBc=;
        b=eEz7/zCR0o3fQH+0TsYvMxw9l7ZBrp5USEr5quICww9ZwGo4GLy3T44YLlGzDhUlvE
         0ZcWeTuWqShhVEJm8u3Q0cAcbymOa1Ov2UZEguEvl5k9qxCArMH/7Bk2i4yaFEK8lWqc
         0DsN1PqjWzKpttZ37e9B/kHx+IdZG0SMBu+6eFnNAYjeeUXcNdgkAilcr1L6Z94PK53R
         jrsJz6YdCK08z6uKA90OG4nOug39KnpWj2lmdPW3QKYe7iT4ULG/gzFuS0K7uzSME4i9
         Io/kdGVGAGIGOnHr6Bi37dnd852R22k/TgY593or8gGgLFgDW2YVagl48PjwOzTq1jMN
         72kA==
X-Gm-Message-State: ALoCoQmnyXVFub9OXOKBAZtpfUreo2VRx5eIdOzmqK75Q2LqsvbgV0Y92qqOvH+MIcPLGHaOIEZ3
X-Received: by 10.221.27.8 with SMTP id ro8mr11117868vcb.30.1403066841872;
        Tue, 17 Jun 2014 21:47:21 -0700 (PDT)
Received: from mail-ve0-f180.google.com (mail-ve0-f180.google.com [209.85.128.180])
        by mx.google.com with ESMTPSA id lb8sm1695303vdb.0.2014.06.17.21.47.20
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 17 Jun 2014 21:47:20 -0700 (PDT)
Received: by mail-ve0-f180.google.com with SMTP id jw12so282914veb.39
        for <dev@spark.apache.org>; Tue, 17 Jun 2014 21:47:20 -0700 (PDT)
X-Received: by 10.221.9.72 with SMTP id ov8mr12642140vcb.27.1403066840215;
 Tue, 17 Jun 2014 21:47:20 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.109.65 with HTTP; Tue, 17 Jun 2014 21:47:00 -0700 (PDT)
In-Reply-To: <CACBYxK+Qy0XNdcpouLfqkg9YWUVDcCUbhqL-f0af5FNgfHX6ig@mail.gmail.com>
References: <CFC6248F.577%xwei@palantir.com> <CACBYxK+Qy0XNdcpouLfqkg9YWUVDcCUbhqL-f0af5FNgfHX6ig@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Wed, 18 Jun 2014 00:47:00 -0400
Message-ID: <CA+-p3AEjEsrwUwXJw5djsu5=6b0LUrjgE7mUK=U+fG3zsSkD3w@mail.gmail.com>
Subject: Re: Contributing to MLlib on GLM
To: dev@spark.apache.org
Cc: Jason Zhao <jasonz@palantir.com>
Content-Type: multipart/alternative; boundary=089e011777219b7a9a04fc14f45c
X-Virus-Checked: Checked by ClamAV on apache.org

--089e011777219b7a9a04fc14f45c
Content-Type: text/plain; charset=UTF-8

Hi Xiaokai,

Also take a look through Xiangrui's slides from HadoopSummit a few weeks
back: http://www.slideshare.net/xrmeng/m-llib-hadoopsummit  The roadmap
starting at slide 51 will probably be interesting to you.

Andrew


On Tue, Jun 17, 2014 at 7:37 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:

> Hi Xiaokai,
>
> I think MLLib is definitely interested in supporting additional GLMs.  I'm
> not aware of anybody working on this at the moment.
>
> -Sandy
>
>
> On Tue, Jun 17, 2014 at 5:00 PM, Xiaokai Wei <xwei@palantir.com> wrote:
>
> > Hi,
> >
> > I am an intern at PalantirTech and we are building some stuff on top of
> > MLlib. In Particular, GLM is of great interest to us.  Though
> > GeneralizedLinearModel in MLlib 1.0.0 has some important GLMs such as
> > Logistic Regression, Linear Regression, some other important GLMs like
> > Poisson Regression are still missing.
> >
> > I am curious that if anyone is already working on other GLMs (e.g.
> > Poisson, Gamma). If not, we would like to contribute to MLlib on GLM. Is
> > adding more GLMs on the roadmap of MLlib?
> >
> >
> > Sincerely,
> >
> > Xiaokai
> >
>

--089e011777219b7a9a04fc14f45c--

From dev-return-8063-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 18 05:42:58 2014
Return-Path: <dev-return-8063-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AEE06114D0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Jun 2014 05:42:58 +0000 (UTC)
Received: (qmail 43848 invoked by uid 500); 18 Jun 2014 05:42:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43758 invoked by uid 500); 18 Jun 2014 05:42:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42754 invoked by uid 99); 18 Jun 2014 05:42:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 05:42:56 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.48 as permitted sender)
Received: from [209.85.219.48] (HELO mail-oa0-f48.google.com) (209.85.219.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 05:42:53 +0000
Received: by mail-oa0-f48.google.com with SMTP id m1so753920oag.21
        for <multiple recipients>; Tue, 17 Jun 2014 22:42:32 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=i5XJMEA/YNpawzIN7MvMjppOHYslDuVkxCXjS6b8igQ=;
        b=SJ3IUOqumX2dD9NB0auqWX+/kPaID2sc4QdnnLPIk7R37WpTDROdF09QaPxLdDI1bV
         SgryJikFvSvE0OmOTCIvXDO0Bl7rwotPvJfCnf+lZ+EQUSzFWszK7xWFTnqNB2r472ZE
         kDY4Dam+FaIhXHfrHcfXAvzPaKjS1IrfqKFYBaR3lu/Qe2rkTypUa03yH8zMk5lvfrQP
         XdK3Ra8/9Z8mW8l2aBC33qJbF/JF87/EeNMUXq+tdFMa576peFoc7QJZ9iIdaaKGJHsO
         Ww8Cyx2cRBFH6LKbbx9UNQ9RJ7Gg90uh/7ye3IigLpokPJEe5voxyTtL7lGwNYQ6b9h9
         mujA==
MIME-Version: 1.0
X-Received: by 10.60.43.199 with SMTP id y7mr18659677oel.58.1403070152424;
 Tue, 17 Jun 2014 22:42:32 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Tue, 17 Jun 2014 22:42:32 -0700 (PDT)
In-Reply-To: <CALWDz_v5QXT=4eWxcTJcwsLcHwXjpgKFwqjFtjM43DscbFyfFA@mail.gmail.com>
References: <CAJfdq+ua55B7xux7ujNxHNkTTe4CcYhZdgsTW04BNWi6axF2qw@mail.gmail.com>
	<CAMAsSdKa41UWJ_RO70Ebh2AK2RPSQUTsCwVYo76HCPK8Z1f5RQ@mail.gmail.com>
	<CAMgYSQ_WW8jLLu1_hYOLPfrJHLs2+N=VXGO0YvL9tPQ0_K=xjQ@mail.gmail.com>
	<CALWDz_v5QXT=4eWxcTJcwsLcHwXjpgKFwqjFtjM43DscbFyfFA@mail.gmail.com>
Date: Tue, 17 Jun 2014 22:42:32 -0700
Message-ID: <CABPQxst1p4frPW_+9OLjgFhCy6_Nt+F5rUvtcwfXLRDFzd4dOQ@mail.gmail.com>
Subject: Re: Java IO Stream Corrupted - Invalid Type AC?
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: user@spark.apache.org
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Out of curiosity - are you guys using speculation, shuffle
consolidation, or any other non-default option? If so that would help
narrow down what's causing this corruption.

On Tue, Jun 17, 2014 at 10:40 AM, Surendranauth Hiraman
<suren.hiraman@velos.io> wrote:
> Matt/Ryan,
>
> Did you make any headway on this? My team is running into this also.
> Doesn't happen on smaller datasets. Our input set is about 10 GB but we
> generate 100s of GBs in the flow itself.
>
> -Suren
>
>
>
>
> On Fri, Jun 6, 2014 at 5:19 PM, Ryan Compton <compton.ryan@gmail.com> wrote:
>
>> Just ran into this today myself. I'm on branch-1.0 using a CDH3
>> cluster (no modifications to Spark or its dependencies). The error
>> appeared trying to run GraphX's .connectedComponents() on a ~200GB
>> edge list (GraphX worked beautifully on smaller data).
>>
>> Here's the stacktrace (it's quite similar to yours
>> https://imgur.com/7iBA4nJ ).
>>
>> 14/06/05 20:02:28 ERROR scheduler.TaskSetManager: Task 5.599:39 failed
>> 4 times; aborting job
>> 14/06/05 20:02:28 INFO scheduler.DAGScheduler: Failed to run reduce at
>> VertexRDD.scala:100
>> Exception in thread "main" org.apache.spark.SparkException: Job
>> aborted due to stage failure: Task 5.599:39 failed 4 times, most
>> recent failure: Exception failure in TID 29735 on host node18:
>> java.io.StreamCorruptedException: invalid type code: AC
>>         java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1355)
>>         java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
>>
>> org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)
>>
>> org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:125)
>>         org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
>>         scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
>>
>> org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
>>
>> org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
>>         scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
>>         scala.collection.Iterator$class.foreach(Iterator.scala:727)
>>         scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
>>
>> org.apache.spark.graphx.impl.VertexPartitionBaseOps.innerJoinKeepLeft(VertexPartitionBaseOps.scala:192)
>>
>> org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:78)
>>
>> org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:75)
>>
>> org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
>>         scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>>         scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
>>         scala.collection.Iterator$class.foreach(Iterator.scala:727)
>>         scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
>>
>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
>>
>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
>>         org.apache.spark.scheduler.Task.run(Task.scala:51)
>>
>> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
>>
>> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>>
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>>         java.lang.Thread.run(Thread.java:662)
>> Driver stacktrace:
>> at org.apache.spark.scheduler.DAGScheduler.org
>> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)
>> at
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1017)
>> at
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1015)
>> at
>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>> at
>> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1015)
>> at
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
>> at
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
>> at scala.Option.foreach(Option.scala:236)
>> at
>> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:633)
>> at
>> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1207)
>> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>> at
>> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>> at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>> at
>> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>> at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>> at
>> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>> 14/06/05 20:02:28 INFO scheduler.TaskSchedulerImpl: Cancelling stage 5
>>
>> On Wed, Jun 4, 2014 at 7:50 AM, Sean Owen <sowen@cloudera.com> wrote:
>> > On Wed, Jun 4, 2014 at 3:33 PM, Matt Kielo <mkielo@oculusinfo.com>
>> wrote:
>> >> Im trying run some spark code on a cluster but I keep running into a
>> >> "java.io.StreamCorruptedException: invalid type code: AC" error. My task
>> >> involves analyzing ~50GB of data (some operations involve sorting) then
>> >> writing them out to a JSON file. Im running the analysis on each of the
>> >> data's ~10 columns and have never had a successful run. My program
>> seems to
>> >> run for a varying amount of time each time (~between 5-30 minutes) but
>> it
>> >> always terminates with this error.
>> >
>> > I can tell you that this usually means somewhere something wrote
>> > objects to the same OutputStream with multiple ObjectOutputStreams. AC
>> > is a header value.
>> >
>> > I don't obviously see where/how that could happen, but maybe it rings
>> > a bell for someone. This could happen if an OutputStream is reused
>> > across object serializations but new ObjectOutputStreams are opened,
>> > for example.
>>
>
>
>
> --
>
> SUREN HIRAMAN, VP TECHNOLOGY
> Velos
> Accelerating Machine Learning
>
> 440 NINTH AVENUE, 11TH FLOOR
> NEW YORK, NY 10001
> O: (917) 525-2466 ext. 105
> F: 646.349.4063
> E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
> W: www.velos.io

From dev-return-8064-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 18 08:19:59 2014
Return-Path: <dev-return-8064-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B7F29118F3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Jun 2014 08:19:59 +0000 (UTC)
Received: (qmail 61436 invoked by uid 500); 18 Jun 2014 08:19:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61372 invoked by uid 500); 18 Jun 2014 08:19:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61361 invoked by uid 99); 18 Jun 2014 08:19:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 08:19:58 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.46] (HELO mail-qg0-f46.google.com) (209.85.192.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 08:19:56 +0000
Received: by mail-qg0-f46.google.com with SMTP id q107so419783qgd.33
        for <dev@spark.apache.org>; Wed, 18 Jun 2014 01:19:31 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=93vJZ8Edwau39ebH1nbVExm5C4HOxucApxU5o5OZANY=;
        b=B6s+/SPVXg+N/xFOLsM35kIeu7loa09DxgqgL7kCY7HIPsdZDoXECjjOHJqZahOBQg
         dBbryDS2Ik6ZOjRrgPTfYXrtRmSj25gdphOxLYgEB5k5SBX4gTo9P3C6JQAKGaDy1BeC
         KZ+61i+xD9D6GNqQTbhiltu9FKeAgQWiDPr+AajsYUxLRejSTilubW18CLau9xLrkud5
         RJu0lS2uXg4+srQx/ZIGs8HKd4EjM6I2D49QhODfkSaXEqaeXFVLnOpi0dvvSk+83ht4
         6O+bjVQ6Re9NMh4nJx+DW1R5L1avPnCf85tuP8X3ApVQ5+8nOUfxyLi36494TdLb9m1S
         67vQ==
X-Gm-Message-State: ALoCoQlGzdRF1PkjVdAnxavomUaXY47F8ZYNWmkMcrl2OOYFRxLlfE6/DplGorg/8PK7An2ezB/0
X-Received: by 10.140.95.142 with SMTP id i14mr681218qge.6.1403079571822; Wed,
 18 Jun 2014 01:19:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Wed, 18 Jun 2014 01:19:11 -0700 (PDT)
In-Reply-To: <CAAADf7K_bqDKD91UFCtd8w6g_P96ZCNJAHfpLu8zxZmKfcShoQ@mail.gmail.com>
References: <CAAADf7K_bqDKD91UFCtd8w6g_P96ZCNJAHfpLu8zxZmKfcShoQ@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 18 Jun 2014 01:19:11 -0700
Message-ID: <CAPh_B=bH7sVKn9ByzZSc_bYDprhDOg3Kq2QsOrFEcvzncA5+1g@mail.gmail.com>
Subject: Re: Contribute to Spark - Need a mentor.
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c168fe786f9e04fc17eb50
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c168fe786f9e04fc17eb50
Content-Type: text/plain; charset=UTF-8

Hi Michael,

Unfortunately the Apache mailing list filters out attachments. That said,
you can usually just start by looking at the JIRA for Spark and find issues
tagged with the starter tag and work on them. You can submit pull requests
to the github repo or email the dev list for feedbacks on specific issues.
https://github.com/apache/spark


On Tue, Jun 17, 2014 at 3:58 PM, Michael Giannakopoulos <
miccagiann@gmail.com> wrote:

> Hi all,
>
> My name is Michael Giannakopoulos and I am a recent M.Sc. graduate
> from University of Toronto majoring in Computer Science. I would like to
> contribute in the development of this open source project. Is it possible
> to work
> under the supervision of a mentor? I specialize in Data Analytics,
> Data Management and Machine Learning. I am currently learning the
> Scala language and I have experience using Java, C++, C and Matlab. I have
> already read the Spark and Shark papers.
>
> Together with this mail you will find attached my resume.
>
> Thank you so much for your time and your help,
> Michael
>

--001a11c168fe786f9e04fc17eb50--

From dev-return-8065-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 18 12:50:22 2014
Return-Path: <dev-return-8065-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0BDA011160
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Jun 2014 12:50:22 +0000 (UTC)
Received: (qmail 74458 invoked by uid 500); 18 Jun 2014 12:50:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74400 invoked by uid 500); 18 Jun 2014 12:50:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74384 invoked by uid 99); 18 Jun 2014 12:50:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 12:50:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of suren.hiraman@sociocast.com designates 209.85.220.182 as permitted sender)
Received: from [209.85.220.182] (HELO mail-vc0-f182.google.com) (209.85.220.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 12:50:17 +0000
Received: by mail-vc0-f182.google.com with SMTP id il7so714991vcb.41
        for <dev@spark.apache.org>; Wed, 18 Jun 2014 05:49:56 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=PjPGyt4jZWl3mA99TuLRALgEHsqSYXRdiDSBAj8Dk+o=;
        b=Dya3XTrYHmDsRMt7N2waJN18qzi2YhdEn2hKBbmx5D8RRzoUiHbDe5W1OPL/NxWtSb
         SNmhLHCWAIWILuvbVzVBmF/EDX+YaiwaDDaLcQ61D2vBaF0Qf3s56x1NwNZ9yMtslGfG
         Zc+RhQSHZfUFI9ZxJr/+evGWjyN8FMqku1n4gI980vB84TWdasPywFkoTzzBne3UT9my
         BLphLmmu/YkKEkJDG6Q+Gn+GDnswox3kAUcntlWmOZd7nenXe/OATC1zWgjr8JU0kTVZ
         uLMbnimfMSHpF710qUtmImKIEjCsdeSxxzAoVoVHn1uBUjUV9RW2tyj3yQSExjmggtE3
         2fIg==
X-Gm-Message-State: ALoCoQl+KknFTf4mTkcOvWDhOoEN+HGzZnnR7S87/WmgORv+iLnpVSXyByVJfcHZnIgXIgGifJAE
MIME-Version: 1.0
X-Received: by 10.220.166.9 with SMTP id k9mr1139123vcy.20.1403095796422; Wed,
 18 Jun 2014 05:49:56 -0700 (PDT)
Received: by 10.58.143.49 with HTTP; Wed, 18 Jun 2014 05:49:56 -0700 (PDT)
In-Reply-To: <CABPQxst1p4frPW_+9OLjgFhCy6_Nt+F5rUvtcwfXLRDFzd4dOQ@mail.gmail.com>
References: <CAJfdq+ua55B7xux7ujNxHNkTTe4CcYhZdgsTW04BNWi6axF2qw@mail.gmail.com>
	<CAMAsSdKa41UWJ_RO70Ebh2AK2RPSQUTsCwVYo76HCPK8Z1f5RQ@mail.gmail.com>
	<CAMgYSQ_WW8jLLu1_hYOLPfrJHLs2+N=VXGO0YvL9tPQ0_K=xjQ@mail.gmail.com>
	<CALWDz_v5QXT=4eWxcTJcwsLcHwXjpgKFwqjFtjM43DscbFyfFA@mail.gmail.com>
	<CABPQxst1p4frPW_+9OLjgFhCy6_Nt+F5rUvtcwfXLRDFzd4dOQ@mail.gmail.com>
Date: Wed, 18 Jun 2014 08:49:56 -0400
Message-ID: <CALWDz_tF83PTwHFQENKyxSKZ+eBZryKA=cZckEPSRm6wj-vMww@mail.gmail.com>
Subject: Re: Java IO Stream Corrupted - Invalid Type AC?
From: Surendranauth Hiraman <suren.hiraman@velos.io>
To: user@spark.apache.org
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b3a8ae6882df604fc1bb237
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a8ae6882df604fc1bb237
Content-Type: text/plain; charset=UTF-8

Patrick,

My team is using shuffle consolidation but not speculation. We are also
using persist(DISK_ONLY) for caching.

Here are some config changes that are in our work-in-progress.

We've been trying for 2 weeks to get our production flow (maybe around
50-70 stages, a few forks and joins with up to 20 branches in the forks) to
run end to end without any success, running into other problems besides
this one as well. For example, we have run into situations where saving to
HDFS just hangs on a couple of tasks, which are printing out nothing in
their logs and not taking any CPU. For testing, our input data is 10 GB
across 320 input splits and generates maybe around 200-300 GB of
intermediate and final data.


        conf.set("spark.executor.memory", "14g")     // TODO make this
configurable

        // shuffle configs
        conf.set("spark.default.parallelism", "320") // TODO make this
configurable
        conf.set("spark.shuffle.consolidateFiles","true")

        conf.set("spark.shuffle.file.buffer.kb", "200")
        conf.set("spark.reducer.maxMbInFlight", "96")

        conf.set("spark.rdd.compress","true"

        // we ran into a problem with the default timeout of 60 seconds
        // this is also being set in the master's spark-env.sh. Not sure if
it needs to be in both places
        conf.set("spark.worker.timeout","180")

        // akka settings
        conf.set("spark.akka.threads", "300")
        conf.set("spark.akka.timeout", "180")
        conf.set("spark.akka.frameSize", "100")
        conf.set("spark.akka.batchSize", "30")
        conf.set("spark.akka.askTimeout", "30")

        // block manager
        conf.set("spark.storage.blockManagerTimeoutIntervalMs", "180000")
        conf.set("spark.blockManagerHeartBeatMs", "80000")

-Suren



On Wed, Jun 18, 2014 at 1:42 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> Out of curiosity - are you guys using speculation, shuffle
> consolidation, or any other non-default option? If so that would help
> narrow down what's causing this corruption.
>
> On Tue, Jun 17, 2014 at 10:40 AM, Surendranauth Hiraman
> <suren.hiraman@velos.io> wrote:
> > Matt/Ryan,
> >
> > Did you make any headway on this? My team is running into this also.
> > Doesn't happen on smaller datasets. Our input set is about 10 GB but we
> > generate 100s of GBs in the flow itself.
> >
> > -Suren
> >
> >
> >
> >
> > On Fri, Jun 6, 2014 at 5:19 PM, Ryan Compton <compton.ryan@gmail.com>
> wrote:
> >
> >> Just ran into this today myself. I'm on branch-1.0 using a CDH3
> >> cluster (no modifications to Spark or its dependencies). The error
> >> appeared trying to run GraphX's .connectedComponents() on a ~200GB
> >> edge list (GraphX worked beautifully on smaller data).
> >>
> >> Here's the stacktrace (it's quite similar to yours
> >> https://imgur.com/7iBA4nJ ).
> >>
> >> 14/06/05 20:02:28 ERROR scheduler.TaskSetManager: Task 5.599:39 failed
> >> 4 times; aborting job
> >> 14/06/05 20:02:28 INFO scheduler.DAGScheduler: Failed to run reduce at
> >> VertexRDD.scala:100
> >> Exception in thread "main" org.apache.spark.SparkException: Job
> >> aborted due to stage failure: Task 5.599:39 failed 4 times, most
> >> recent failure: Exception failure in TID 29735 on host node18:
> >> java.io.StreamCorruptedException: invalid type code: AC
> >>
> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1355)
> >>         java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
> >>
> >>
> org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)
> >>
> >>
> org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:125)
> >>
> org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
> >>         scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
> >>
> >>
> org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
> >>
> >>
> org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
> >>         scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
> >>         scala.collection.Iterator$class.foreach(Iterator.scala:727)
> >>         scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
> >>
> >>
> org.apache.spark.graphx.impl.VertexPartitionBaseOps.innerJoinKeepLeft(VertexPartitionBaseOps.scala:192)
> >>
> >>
> org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:78)
> >>
> >>
> org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:75)
> >>
> >>
> org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
> >>         scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
> >>         scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
> >>         scala.collection.Iterator$class.foreach(Iterator.scala:727)
> >>         scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
> >>
> >>
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
> >>
> >>
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
> >>         org.apache.spark.scheduler.Task.run(Task.scala:51)
> >>
> >> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
> >>
> >>
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
> >>
> >>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
> >>         java.lang.Thread.run(Thread.java:662)
> >> Driver stacktrace:
> >> at org.apache.spark.scheduler.DAGScheduler.org
> >>
> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)
> >> at
> >>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1017)
> >> at
> >>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1015)
> >> at
> >>
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> >> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> >> at
> >>
> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1015)
> >> at
> >>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
> >> at
> >>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
> >> at scala.Option.foreach(Option.scala:236)
> >> at
> >>
> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:633)
> >> at
> >>
> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1207)
> >> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
> >> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
> >> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
> >> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
> >> at
> >>
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
> >> at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
> >> at
> >>
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
> >> at
> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
> >> at
> >>
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
> >> 14/06/05 20:02:28 INFO scheduler.TaskSchedulerImpl: Cancelling stage 5
> >>
> >> On Wed, Jun 4, 2014 at 7:50 AM, Sean Owen <sowen@cloudera.com> wrote:
> >> > On Wed, Jun 4, 2014 at 3:33 PM, Matt Kielo <mkielo@oculusinfo.com>
> >> wrote:
> >> >> Im trying run some spark code on a cluster but I keep running into a
> >> >> "java.io.StreamCorruptedException: invalid type code: AC" error. My
> task
> >> >> involves analyzing ~50GB of data (some operations involve sorting)
> then
> >> >> writing them out to a JSON file. Im running the analysis on each of
> the
> >> >> data's ~10 columns and have never had a successful run. My program
> >> seems to
> >> >> run for a varying amount of time each time (~between 5-30 minutes)
> but
> >> it
> >> >> always terminates with this error.
> >> >
> >> > I can tell you that this usually means somewhere something wrote
> >> > objects to the same OutputStream with multiple ObjectOutputStreams. AC
> >> > is a header value.
> >> >
> >> > I don't obviously see where/how that could happen, but maybe it rings
> >> > a bell for someone. This could happen if an OutputStream is reused
> >> > across object serializations but new ObjectOutputStreams are opened,
> >> > for example.
> >>
> >
> >
> >
> > --
> >
> > SUREN HIRAMAN, VP TECHNOLOGY
> > Velos
> > Accelerating Machine Learning
> >
> > 440 NINTH AVENUE, 11TH FLOOR
> > NEW YORK, NY 10001
> > O: (917) 525-2466 ext. 105
> > F: 646.349.4063
> > E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
> > W: www.velos.io
>



-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io

--047d7b3a8ae6882df604fc1bb237--

From dev-return-8066-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 18 19:15:35 2014
Return-Path: <dev-return-8066-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 185C011208
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Jun 2014 19:15:35 +0000 (UTC)
Received: (qmail 35793 invoked by uid 500); 18 Jun 2014 19:15:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35736 invoked by uid 500); 18 Jun 2014 19:15:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35724 invoked by uid 99); 18 Jun 2014 19:15:33 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 19:15:33 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.216.45 as permitted sender)
Received: from [209.85.216.45] (HELO mail-qa0-f45.google.com) (209.85.216.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 19:15:30 +0000
Received: by mail-qa0-f45.google.com with SMTP id v10so1081929qac.4
        for <dev@spark.apache.org>; Wed, 18 Jun 2014 12:15:09 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=sYOAAi752xwj7ODN76lr+t7Tf544Yxdo+wD0/qMe0MM=;
        b=XBr+OXWEUltLX+D6UbfqtJ75vFtB+hu9OaA2hHJlvLPrg6mm2n0xx6VpIEk2n9DrIq
         mAwsVBaFFMz+8SEJGgG8dNHuaRfF1NrjquTOkAXk0+JNa5gbV+s8zBqpl48RFqA62Vc3
         ckEeJJ21pSq4NQ3UK9IVoAcdH7U0w78aC43jp91ckbonBDxFBPIyaTB9/OCLdJZ9sD3k
         AAeSD5HnPc75j6T9cZQwHlj5B923kQGrp1/C4HiH6LKwfHk0aFRFMdLfjPdxDXbbVW8P
         oG/+1TRuxyAD8KekADUUhnUqAUiESuNV1lolM8chQNxBVNWFaDDuOk0JoMGf0ZPlY5r8
         UTWw==
MIME-Version: 1.0
X-Received: by 10.224.47.148 with SMTP id n20mr161235qaf.90.1403118909593;
 Wed, 18 Jun 2014 12:15:09 -0700 (PDT)
Received: by 10.140.38.149 with HTTP; Wed, 18 Jun 2014 12:15:09 -0700 (PDT)
In-Reply-To: <CALWDz_tF83PTwHFQENKyxSKZ+eBZryKA=cZckEPSRm6wj-vMww@mail.gmail.com>
References: <CAJfdq+ua55B7xux7ujNxHNkTTe4CcYhZdgsTW04BNWi6axF2qw@mail.gmail.com>
	<CAMAsSdKa41UWJ_RO70Ebh2AK2RPSQUTsCwVYo76HCPK8Z1f5RQ@mail.gmail.com>
	<CAMgYSQ_WW8jLLu1_hYOLPfrJHLs2+N=VXGO0YvL9tPQ0_K=xjQ@mail.gmail.com>
	<CALWDz_v5QXT=4eWxcTJcwsLcHwXjpgKFwqjFtjM43DscbFyfFA@mail.gmail.com>
	<CABPQxst1p4frPW_+9OLjgFhCy6_Nt+F5rUvtcwfXLRDFzd4dOQ@mail.gmail.com>
	<CALWDz_tF83PTwHFQENKyxSKZ+eBZryKA=cZckEPSRm6wj-vMww@mail.gmail.com>
Date: Thu, 19 Jun 2014 00:45:09 +0530
Message-ID: <CAJiQeYJRqsLSaV=X2uB_cQq+iAcvrdWoCrBU3dLy-7QvxMaVeA@mail.gmail.com>
Subject: Re: Java IO Stream Corrupted - Invalid Type AC?
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Wed, Jun 18, 2014 at 6:19 PM, Surendranauth Hiraman
<suren.hiraman@velos.io> wrote:
> Patrick,
>
> My team is using shuffle consolidation but not speculation. We are also
> using persist(DISK_ONLY) for caching.


Use of shuffle consolidation is probably what is causing the issue.
Would be good idea to try again with that turned off (which is the default).

It should get fixed most likely in 1.1 timeframe.


Regards,
Mridul


>
> Here are some config changes that are in our work-in-progress.
>
> We've been trying for 2 weeks to get our production flow (maybe around
> 50-70 stages, a few forks and joins with up to 20 branches in the forks) to
> run end to end without any success, running into other problems besides
> this one as well. For example, we have run into situations where saving to
> HDFS just hangs on a couple of tasks, which are printing out nothing in
> their logs and not taking any CPU. For testing, our input data is 10 GB
> across 320 input splits and generates maybe around 200-300 GB of
> intermediate and final data.
>
>
>         conf.set("spark.executor.memory", "14g")     // TODO make this
> configurable
>
>         // shuffle configs
>         conf.set("spark.default.parallelism", "320") // TODO make this
> configurable
>         conf.set("spark.shuffle.consolidateFiles","true")
>
>         conf.set("spark.shuffle.file.buffer.kb", "200")
>         conf.set("spark.reducer.maxMbInFlight", "96")
>
>         conf.set("spark.rdd.compress","true"
>
>         // we ran into a problem with the default timeout of 60 seconds
>         // this is also being set in the master's spark-env.sh. Not sure if
> it needs to be in both places
>         conf.set("spark.worker.timeout","180")
>
>         // akka settings
>         conf.set("spark.akka.threads", "300")
>         conf.set("spark.akka.timeout", "180")
>         conf.set("spark.akka.frameSize", "100")
>         conf.set("spark.akka.batchSize", "30")
>         conf.set("spark.akka.askTimeout", "30")
>
>         // block manager
>         conf.set("spark.storage.blockManagerTimeoutIntervalMs", "180000")
>         conf.set("spark.blockManagerHeartBeatMs", "80000")
>
> -Suren
>
>
>
> On Wed, Jun 18, 2014 at 1:42 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>
>> Out of curiosity - are you guys using speculation, shuffle
>> consolidation, or any other non-default option? If so that would help
>> narrow down what's causing this corruption.
>>
>> On Tue, Jun 17, 2014 at 10:40 AM, Surendranauth Hiraman
>> <suren.hiraman@velos.io> wrote:
>> > Matt/Ryan,
>> >
>> > Did you make any headway on this? My team is running into this also.
>> > Doesn't happen on smaller datasets. Our input set is about 10 GB but we
>> > generate 100s of GBs in the flow itself.
>> >
>> > -Suren
>> >
>> >
>> >
>> >
>> > On Fri, Jun 6, 2014 at 5:19 PM, Ryan Compton <compton.ryan@gmail.com>
>> wrote:
>> >
>> >> Just ran into this today myself. I'm on branch-1.0 using a CDH3
>> >> cluster (no modifications to Spark or its dependencies). The error
>> >> appeared trying to run GraphX's .connectedComponents() on a ~200GB
>> >> edge list (GraphX worked beautifully on smaller data).
>> >>
>> >> Here's the stacktrace (it's quite similar to yours
>> >> https://imgur.com/7iBA4nJ ).
>> >>
>> >> 14/06/05 20:02:28 ERROR scheduler.TaskSetManager: Task 5.599:39 failed
>> >> 4 times; aborting job
>> >> 14/06/05 20:02:28 INFO scheduler.DAGScheduler: Failed to run reduce at
>> >> VertexRDD.scala:100
>> >> Exception in thread "main" org.apache.spark.SparkException: Job
>> >> aborted due to stage failure: Task 5.599:39 failed 4 times, most
>> >> recent failure: Exception failure in TID 29735 on host node18:
>> >> java.io.StreamCorruptedException: invalid type code: AC
>> >>
>> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1355)
>> >>         java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
>> >>
>> >>
>> org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)
>> >>
>> >>
>> org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:125)
>> >>
>> org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
>> >>         scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
>> >>
>> >>
>> org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
>> >>
>> >>
>> org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
>> >>         scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
>> >>         scala.collection.Iterator$class.foreach(Iterator.scala:727)
>> >>         scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
>> >>
>> >>
>> org.apache.spark.graphx.impl.VertexPartitionBaseOps.innerJoinKeepLeft(VertexPartitionBaseOps.scala:192)
>> >>
>> >>
>> org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:78)
>> >>
>> >>
>> org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:75)
>> >>
>> >>
>> org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
>> >>         scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>> >>         scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
>> >>         scala.collection.Iterator$class.foreach(Iterator.scala:727)
>> >>         scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
>> >>
>> >>
>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
>> >>
>> >>
>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
>> >>         org.apache.spark.scheduler.Task.run(Task.scala:51)
>> >>
>> >> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
>> >>
>> >>
>> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>> >>
>> >>
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>> >>         java.lang.Thread.run(Thread.java:662)
>> >> Driver stacktrace:
>> >> at org.apache.spark.scheduler.DAGScheduler.org
>> >>
>> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)
>> >> at
>> >>
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1017)
>> >> at
>> >>
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1015)
>> >> at
>> >>
>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>> >> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>> >> at
>> >>
>> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1015)
>> >> at
>> >>
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
>> >> at
>> >>
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
>> >> at scala.Option.foreach(Option.scala:236)
>> >> at
>> >>
>> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:633)
>> >> at
>> >>
>> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1207)
>> >> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>> >> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>> >> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>> >> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>> >> at
>> >>
>> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>> >> at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>> >> at
>> >>
>> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>> >> at
>> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>> >> at
>> >>
>> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>> >> 14/06/05 20:02:28 INFO scheduler.TaskSchedulerImpl: Cancelling stage 5
>> >>
>> >> On Wed, Jun 4, 2014 at 7:50 AM, Sean Owen <sowen@cloudera.com> wrote:
>> >> > On Wed, Jun 4, 2014 at 3:33 PM, Matt Kielo <mkielo@oculusinfo.com>
>> >> wrote:
>> >> >> Im trying run some spark code on a cluster but I keep running into a
>> >> >> "java.io.StreamCorruptedException: invalid type code: AC" error. My
>> task
>> >> >> involves analyzing ~50GB of data (some operations involve sorting)
>> then
>> >> >> writing them out to a JSON file. Im running the analysis on each of
>> the
>> >> >> data's ~10 columns and have never had a successful run. My program
>> >> seems to
>> >> >> run for a varying amount of time each time (~between 5-30 minutes)
>> but
>> >> it
>> >> >> always terminates with this error.
>> >> >
>> >> > I can tell you that this usually means somewhere something wrote
>> >> > objects to the same OutputStream with multiple ObjectOutputStreams. AC
>> >> > is a header value.
>> >> >
>> >> > I don't obviously see where/how that could happen, but maybe it rings
>> >> > a bell for someone. This could happen if an OutputStream is reused
>> >> > across object serializations but new ObjectOutputStreams are opened,
>> >> > for example.
>> >>
>> >
>> >
>> >
>> > --
>> >
>> > SUREN HIRAMAN, VP TECHNOLOGY
>> > Velos
>> > Accelerating Machine Learning
>> >
>> > 440 NINTH AVENUE, 11TH FLOOR
>> > NEW YORK, NY 10001
>> > O: (917) 525-2466 ext. 105
>> > F: 646.349.4063
>> > E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
>> > W: www.velos.io
>>
>
>
>
> --
>
> SUREN HIRAMAN, VP TECHNOLOGY
> Velos
> Accelerating Machine Learning
>
> 440 NINTH AVENUE, 11TH FLOOR
> NEW YORK, NY 10001
> O: (917) 525-2466 ext. 105
> F: 646.349.4063
> E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
> W: www.velos.io

From dev-return-8067-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 18 22:23:25 2014
Return-Path: <dev-return-8067-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6161E11AAF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Jun 2014 22:23:25 +0000 (UTC)
Received: (qmail 13973 invoked by uid 500); 18 Jun 2014 22:23:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13911 invoked by uid 500); 18 Jun 2014 22:23:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13900 invoked by uid 99); 18 Jun 2014 22:23:24 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 22:23:24 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of wibenton@redhat.com designates 209.132.183.24 as permitted sender)
Received: from [209.132.183.24] (HELO mx3-phx2.redhat.com) (209.132.183.24)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Jun 2014 22:23:21 +0000
Received: from zmail14.collab.prod.int.phx2.redhat.com (zmail14.collab.prod.int.phx2.redhat.com [10.5.83.16])
	by mx3-phx2.redhat.com (8.13.8/8.13.8) with ESMTP id s5IMMtRl030043
	for <dev@spark.apache.org>; Wed, 18 Jun 2014 18:22:55 -0400
Date: Wed, 18 Jun 2014 18:22:54 -0400 (EDT)
From: Will Benton <willb@redhat.com>
To: dev@spark.apache.org
Message-ID: <1351204415.36085483.1403130174646.JavaMail.zimbra@redhat.com>
In-Reply-To: <1227418919.36083781.1403129856067.JavaMail.zimbra@redhat.com>
Subject: question about Hive compatiblilty tests
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.12]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - FF29 (Mac)/8.0.6_GA_5922)
Thread-Topic: question about Hive compatiblilty tests
Thread-Index: DjBMpmqoIVfPYqLvI5aBLNRYwADLuA==
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all,

Does a "Failed to generate golden answer for query" message from HiveComparisonTests indicate that it isn't possible to run the query in question under Hive from Spark's test suite rather than anything about Spark's implementation of HiveQL?  The stack trace I'm getting implicates Hive code and not Spark code, but I wanted to make sure I wasn't missing something.


thanks,
wb

From dev-return-8068-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun 19 00:35:55 2014
Return-Path: <dev-return-8068-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 598D811F12
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Jun 2014 00:35:55 +0000 (UTC)
Received: (qmail 21265 invoked by uid 500); 19 Jun 2014 00:35:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21203 invoked by uid 500); 19 Jun 2014 00:35:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21191 invoked by uid 99); 19 Jun 2014 00:35:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 00:35:54 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.54 as permitted sender)
Received: from [209.85.219.54] (HELO mail-oa0-f54.google.com) (209.85.219.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 00:35:50 +0000
Received: by mail-oa0-f54.google.com with SMTP id eb12so3650219oac.27
        for <dev@spark.apache.org>; Wed, 18 Jun 2014 17:35:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=cesrq9isVFIl6XngHdlc811r1nxVEiHok3vj2H80gko=;
        b=lgc3YXYD30XjJb8kEChFyjuRyB2d2KrbbT+EJl9eR75azPTv/8NvVAl57ds/BhxHr/
         Jh0rkcadH3FLBmDLvRItTtR+5CCRe8gWX6j2BHw1MTHRAec1cUuFhdqCjDBPTw/GpaT9
         2+bu2wtPH3x5UiU4exyPcz1ps64JNjFdJYYuZeYbVJ3MxkDivLLwKxe7P8DZ2PbUrDoi
         23cVU3rDGKK9B33mdBIYV5buD7p5mT9cf595jwCacNCfkIkxOudCSxnYdJmFQZkowgxn
         SyFedKr6AjuzHDl/6Y1ycaJkz2P74KkzTR5zDrS1ufy82IWyqvDz5UDiUHYhUGFWDbXH
         hFpA==
MIME-Version: 1.0
X-Received: by 10.60.15.231 with SMTP id a7mr1427564oed.50.1403138130354; Wed,
 18 Jun 2014 17:35:30 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Wed, 18 Jun 2014 17:35:30 -0700 (PDT)
In-Reply-To: <CAJiQeYJRqsLSaV=X2uB_cQq+iAcvrdWoCrBU3dLy-7QvxMaVeA@mail.gmail.com>
References: <CAJfdq+ua55B7xux7ujNxHNkTTe4CcYhZdgsTW04BNWi6axF2qw@mail.gmail.com>
	<CAMAsSdKa41UWJ_RO70Ebh2AK2RPSQUTsCwVYo76HCPK8Z1f5RQ@mail.gmail.com>
	<CAMgYSQ_WW8jLLu1_hYOLPfrJHLs2+N=VXGO0YvL9tPQ0_K=xjQ@mail.gmail.com>
	<CALWDz_v5QXT=4eWxcTJcwsLcHwXjpgKFwqjFtjM43DscbFyfFA@mail.gmail.com>
	<CABPQxst1p4frPW_+9OLjgFhCy6_Nt+F5rUvtcwfXLRDFzd4dOQ@mail.gmail.com>
	<CALWDz_tF83PTwHFQENKyxSKZ+eBZryKA=cZckEPSRm6wj-vMww@mail.gmail.com>
	<CAJiQeYJRqsLSaV=X2uB_cQq+iAcvrdWoCrBU3dLy-7QvxMaVeA@mail.gmail.com>
Date: Wed, 18 Jun 2014 17:35:30 -0700
Message-ID: <CABPQxssMbdRz_TxMgqD6efbC2CYsx-yuYsown4Sk_anATPTDcw@mail.gmail.com>
Subject: Re: Java IO Stream Corrupted - Invalid Type AC?
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Just wondering, do you get this particular exception if you are not
consolidating shuffle data?

On Wed, Jun 18, 2014 at 12:15 PM, Mridul Muralidharan <mridul@gmail.com> wrote:
> On Wed, Jun 18, 2014 at 6:19 PM, Surendranauth Hiraman
> <suren.hiraman@velos.io> wrote:
>> Patrick,
>>
>> My team is using shuffle consolidation but not speculation. We are also
>> using persist(DISK_ONLY) for caching.
>
>
> Use of shuffle consolidation is probably what is causing the issue.
> Would be good idea to try again with that turned off (which is the default).
>
> It should get fixed most likely in 1.1 timeframe.
>
>
> Regards,
> Mridul
>
>
>>
>> Here are some config changes that are in our work-in-progress.
>>
>> We've been trying for 2 weeks to get our production flow (maybe around
>> 50-70 stages, a few forks and joins with up to 20 branches in the forks) to
>> run end to end without any success, running into other problems besides
>> this one as well. For example, we have run into situations where saving to
>> HDFS just hangs on a couple of tasks, which are printing out nothing in
>> their logs and not taking any CPU. For testing, our input data is 10 GB
>> across 320 input splits and generates maybe around 200-300 GB of
>> intermediate and final data.
>>
>>
>>         conf.set("spark.executor.memory", "14g")     // TODO make this
>> configurable
>>
>>         // shuffle configs
>>         conf.set("spark.default.parallelism", "320") // TODO make this
>> configurable
>>         conf.set("spark.shuffle.consolidateFiles","true")
>>
>>         conf.set("spark.shuffle.file.buffer.kb", "200")
>>         conf.set("spark.reducer.maxMbInFlight", "96")
>>
>>         conf.set("spark.rdd.compress","true"
>>
>>         // we ran into a problem with the default timeout of 60 seconds
>>         // this is also being set in the master's spark-env.sh. Not sure if
>> it needs to be in both places
>>         conf.set("spark.worker.timeout","180")
>>
>>         // akka settings
>>         conf.set("spark.akka.threads", "300")
>>         conf.set("spark.akka.timeout", "180")
>>         conf.set("spark.akka.frameSize", "100")
>>         conf.set("spark.akka.batchSize", "30")
>>         conf.set("spark.akka.askTimeout", "30")
>>
>>         // block manager
>>         conf.set("spark.storage.blockManagerTimeoutIntervalMs", "180000")
>>         conf.set("spark.blockManagerHeartBeatMs", "80000")
>>
>> -Suren
>>
>>
>>
>> On Wed, Jun 18, 2014 at 1:42 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>>
>>> Out of curiosity - are you guys using speculation, shuffle
>>> consolidation, or any other non-default option? If so that would help
>>> narrow down what's causing this corruption.
>>>
>>> On Tue, Jun 17, 2014 at 10:40 AM, Surendranauth Hiraman
>>> <suren.hiraman@velos.io> wrote:
>>> > Matt/Ryan,
>>> >
>>> > Did you make any headway on this? My team is running into this also.
>>> > Doesn't happen on smaller datasets. Our input set is about 10 GB but we
>>> > generate 100s of GBs in the flow itself.
>>> >
>>> > -Suren
>>> >
>>> >
>>> >
>>> >
>>> > On Fri, Jun 6, 2014 at 5:19 PM, Ryan Compton <compton.ryan@gmail.com>
>>> wrote:
>>> >
>>> >> Just ran into this today myself. I'm on branch-1.0 using a CDH3
>>> >> cluster (no modifications to Spark or its dependencies). The error
>>> >> appeared trying to run GraphX's .connectedComponents() on a ~200GB
>>> >> edge list (GraphX worked beautifully on smaller data).
>>> >>
>>> >> Here's the stacktrace (it's quite similar to yours
>>> >> https://imgur.com/7iBA4nJ ).
>>> >>
>>> >> 14/06/05 20:02:28 ERROR scheduler.TaskSetManager: Task 5.599:39 failed
>>> >> 4 times; aborting job
>>> >> 14/06/05 20:02:28 INFO scheduler.DAGScheduler: Failed to run reduce at
>>> >> VertexRDD.scala:100
>>> >> Exception in thread "main" org.apache.spark.SparkException: Job
>>> >> aborted due to stage failure: Task 5.599:39 failed 4 times, most
>>> >> recent failure: Exception failure in TID 29735 on host node18:
>>> >> java.io.StreamCorruptedException: invalid type code: AC
>>> >>
>>> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1355)
>>> >>         java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
>>> >>
>>> >>
>>> org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)
>>> >>
>>> >>
>>> org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:125)
>>> >>
>>> org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
>>> >>         scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
>>> >>
>>> >>
>>> org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
>>> >>
>>> >>
>>> org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
>>> >>         scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
>>> >>         scala.collection.Iterator$class.foreach(Iterator.scala:727)
>>> >>         scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
>>> >>
>>> >>
>>> org.apache.spark.graphx.impl.VertexPartitionBaseOps.innerJoinKeepLeft(VertexPartitionBaseOps.scala:192)
>>> >>
>>> >>
>>> org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:78)
>>> >>
>>> >>
>>> org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:75)
>>> >>
>>> >>
>>> org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
>>> >>         scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>>> >>         scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
>>> >>         scala.collection.Iterator$class.foreach(Iterator.scala:727)
>>> >>         scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
>>> >>
>>> >>
>>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
>>> >>
>>> >>
>>> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
>>> >>         org.apache.spark.scheduler.Task.run(Task.scala:51)
>>> >>
>>> >> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
>>> >>
>>> >>
>>> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
>>> >>
>>> >>
>>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
>>> >>         java.lang.Thread.run(Thread.java:662)
>>> >> Driver stacktrace:
>>> >> at org.apache.spark.scheduler.DAGScheduler.org
>>> >>
>>> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)
>>> >> at
>>> >>
>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1017)
>>> >> at
>>> >>
>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1015)
>>> >> at
>>> >>
>>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>>> >> at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>>> >> at
>>> >>
>>> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1015)
>>> >> at
>>> >>
>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
>>> >> at
>>> >>
>>> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
>>> >> at scala.Option.foreach(Option.scala:236)
>>> >> at
>>> >>
>>> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:633)
>>> >> at
>>> >>
>>> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1207)
>>> >> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>>> >> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>>> >> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>>> >> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>>> >> at
>>> >>
>>> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>>> >> at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>>> >> at
>>> >>
>>> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>>> >> at
>>> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>>> >> at
>>> >>
>>> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>>> >> 14/06/05 20:02:28 INFO scheduler.TaskSchedulerImpl: Cancelling stage 5
>>> >>
>>> >> On Wed, Jun 4, 2014 at 7:50 AM, Sean Owen <sowen@cloudera.com> wrote:
>>> >> > On Wed, Jun 4, 2014 at 3:33 PM, Matt Kielo <mkielo@oculusinfo.com>
>>> >> wrote:
>>> >> >> Im trying run some spark code on a cluster but I keep running into a
>>> >> >> "java.io.StreamCorruptedException: invalid type code: AC" error. My
>>> task
>>> >> >> involves analyzing ~50GB of data (some operations involve sorting)
>>> then
>>> >> >> writing them out to a JSON file. Im running the analysis on each of
>>> the
>>> >> >> data's ~10 columns and have never had a successful run. My program
>>> >> seems to
>>> >> >> run for a varying amount of time each time (~between 5-30 minutes)
>>> but
>>> >> it
>>> >> >> always terminates with this error.
>>> >> >
>>> >> > I can tell you that this usually means somewhere something wrote
>>> >> > objects to the same OutputStream with multiple ObjectOutputStreams. AC
>>> >> > is a header value.
>>> >> >
>>> >> > I don't obviously see where/how that could happen, but maybe it rings
>>> >> > a bell for someone. This could happen if an OutputStream is reused
>>> >> > across object serializations but new ObjectOutputStreams are opened,
>>> >> > for example.
>>> >>
>>> >
>>> >
>>> >
>>> > --
>>> >
>>> > SUREN HIRAMAN, VP TECHNOLOGY
>>> > Velos
>>> > Accelerating Machine Learning
>>> >
>>> > 440 NINTH AVENUE, 11TH FLOOR
>>> > NEW YORK, NY 10001
>>> > O: (917) 525-2466 ext. 105
>>> > F: 646.349.4063
>>> > E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
>>> > W: www.velos.io
>>>
>>
>>
>>
>> --
>>
>> SUREN HIRAMAN, VP TECHNOLOGY
>> Velos
>> Accelerating Machine Learning
>>
>> 440 NINTH AVENUE, 11TH FLOOR
>> NEW YORK, NY 10001
>> O: (917) 525-2466 ext. 105
>> F: 646.349.4063
>> E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
>> W: www.velos.io

From dev-return-8069-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun 19 00:39:59 2014
Return-Path: <dev-return-8069-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5FA9A11F43
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Jun 2014 00:39:59 +0000 (UTC)
Received: (qmail 41707 invoked by uid 500); 19 Jun 2014 00:39:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41651 invoked by uid 500); 19 Jun 2014 00:39:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41635 invoked by uid 99); 19 Jun 2014 00:39:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 00:39:58 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of suren.hiraman@sociocast.com designates 209.85.220.178 as permitted sender)
Received: from [209.85.220.178] (HELO mail-vc0-f178.google.com) (209.85.220.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 00:39:55 +0000
Received: by mail-vc0-f178.google.com with SMTP id ij19so1549786vcb.23
        for <dev@spark.apache.org>; Wed, 18 Jun 2014 17:39:31 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=hcNLRZEJZjlqaoA9Ft67fo9YKIN2VwZifl3qxngmPcY=;
        b=mJZtx7WoXvtaNXPTMwOlnQBVPCZPjPO4symh9URo9nYYTBiJPZ3OoQ0+mkVg11K0Kc
         +4dhNJrfOuFI+ktgp219cIbsxRs/nvFyZzVvB0BP/2pTuuzLDrATo6hepCG4H5/FAsBv
         kbP6PCfqW+Uj4i3XnKtoXsSmThmddbXUU24PYJwCME5YzmubPxRvEoAx4UyJHfpuRTZU
         m7YByV1aagJpa74nXLXioP1kXFkHvW3tfzy/CiCV/rLLURg3dMCe53wJHpMfYSlfVqbh
         E9ipZemgyRMK3IuHsL5DlddStiZJ6Ayzl1jeaW5f1SeQ5eJ3kqdiYyhCrYp97dQ2Rrey
         yEsw==
X-Gm-Message-State: ALoCoQms0k8QskQc4bBsfV9HwQLjDLe3q8VWR2/UcFLV1rs0uhol1LUhJbNO2Re0JUfnKNYEgz50
MIME-Version: 1.0
X-Received: by 10.58.211.229 with SMTP id nf5mr905888vec.19.1403138371019;
 Wed, 18 Jun 2014 17:39:31 -0700 (PDT)
Received: by 10.58.143.49 with HTTP; Wed, 18 Jun 2014 17:39:30 -0700 (PDT)
In-Reply-To: <CABPQxssMbdRz_TxMgqD6efbC2CYsx-yuYsown4Sk_anATPTDcw@mail.gmail.com>
References: <CAJfdq+ua55B7xux7ujNxHNkTTe4CcYhZdgsTW04BNWi6axF2qw@mail.gmail.com>
	<CAMAsSdKa41UWJ_RO70Ebh2AK2RPSQUTsCwVYo76HCPK8Z1f5RQ@mail.gmail.com>
	<CAMgYSQ_WW8jLLu1_hYOLPfrJHLs2+N=VXGO0YvL9tPQ0_K=xjQ@mail.gmail.com>
	<CALWDz_v5QXT=4eWxcTJcwsLcHwXjpgKFwqjFtjM43DscbFyfFA@mail.gmail.com>
	<CABPQxst1p4frPW_+9OLjgFhCy6_Nt+F5rUvtcwfXLRDFzd4dOQ@mail.gmail.com>
	<CALWDz_tF83PTwHFQENKyxSKZ+eBZryKA=cZckEPSRm6wj-vMww@mail.gmail.com>
	<CAJiQeYJRqsLSaV=X2uB_cQq+iAcvrdWoCrBU3dLy-7QvxMaVeA@mail.gmail.com>
	<CABPQxssMbdRz_TxMgqD6efbC2CYsx-yuYsown4Sk_anATPTDcw@mail.gmail.com>
Date: Wed, 18 Jun 2014 20:39:30 -0400
Message-ID: <CALWDz_u7okNCipmddv8ktUc-2ya8pRjekJJSOKPNBcQ6ShS8MA@mail.gmail.com>
Subject: Re: Java IO Stream Corrupted - Invalid Type AC?
From: Surendranauth Hiraman <suren.hiraman@velos.io>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdc18242cfc2404fc259c14
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc18242cfc2404fc259c14
Content-Type: text/plain; charset=UTF-8

Good question. At this point, I'd have to re-run it to know for sure. We've
been trying various different things, so I'd have to reset the flow config
back to that state.

I can say that by removing persist(DISK_ONLY), the flows are running more
stably, probably due to removing disk contention. We won't be able to run
our full production flows without some type of disk persistence but for
testing, this is how we are continuing to try for now.

I can try tomorrow if you'd like.

-Suren



On Wed, Jun 18, 2014 at 8:35 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Just wondering, do you get this particular exception if you are not
> consolidating shuffle data?
>
> On Wed, Jun 18, 2014 at 12:15 PM, Mridul Muralidharan <mridul@gmail.com>
> wrote:
> > On Wed, Jun 18, 2014 at 6:19 PM, Surendranauth Hiraman
> > <suren.hiraman@velos.io> wrote:
> >> Patrick,
> >>
> >> My team is using shuffle consolidation but not speculation. We are also
> >> using persist(DISK_ONLY) for caching.
> >
> >
> > Use of shuffle consolidation is probably what is causing the issue.
> > Would be good idea to try again with that turned off (which is the
> default).
> >
> > It should get fixed most likely in 1.1 timeframe.
> >
> >
> > Regards,
> > Mridul
> >
> >
> >>
> >> Here are some config changes that are in our work-in-progress.
> >>
> >> We've been trying for 2 weeks to get our production flow (maybe around
> >> 50-70 stages, a few forks and joins with up to 20 branches in the
> forks) to
> >> run end to end without any success, running into other problems besides
> >> this one as well. For example, we have run into situations where saving
> to
> >> HDFS just hangs on a couple of tasks, which are printing out nothing in
> >> their logs and not taking any CPU. For testing, our input data is 10 GB
> >> across 320 input splits and generates maybe around 200-300 GB of
> >> intermediate and final data.
> >>
> >>
> >>         conf.set("spark.executor.memory", "14g")     // TODO make this
> >> configurable
> >>
> >>         // shuffle configs
> >>         conf.set("spark.default.parallelism", "320") // TODO make this
> >> configurable
> >>         conf.set("spark.shuffle.consolidateFiles","true")
> >>
> >>         conf.set("spark.shuffle.file.buffer.kb", "200")
> >>         conf.set("spark.reducer.maxMbInFlight", "96")
> >>
> >>         conf.set("spark.rdd.compress","true"
> >>
> >>         // we ran into a problem with the default timeout of 60 seconds
> >>         // this is also being set in the master's spark-env.sh. Not
> sure if
> >> it needs to be in both places
> >>         conf.set("spark.worker.timeout","180")
> >>
> >>         // akka settings
> >>         conf.set("spark.akka.threads", "300")
> >>         conf.set("spark.akka.timeout", "180")
> >>         conf.set("spark.akka.frameSize", "100")
> >>         conf.set("spark.akka.batchSize", "30")
> >>         conf.set("spark.akka.askTimeout", "30")
> >>
> >>         // block manager
> >>         conf.set("spark.storage.blockManagerTimeoutIntervalMs",
> "180000")
> >>         conf.set("spark.blockManagerHeartBeatMs", "80000")
> >>
> >> -Suren
> >>
> >>
> >>
> >> On Wed, Jun 18, 2014 at 1:42 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >>
> >>> Out of curiosity - are you guys using speculation, shuffle
> >>> consolidation, or any other non-default option? If so that would help
> >>> narrow down what's causing this corruption.
> >>>
> >>> On Tue, Jun 17, 2014 at 10:40 AM, Surendranauth Hiraman
> >>> <suren.hiraman@velos.io> wrote:
> >>> > Matt/Ryan,
> >>> >
> >>> > Did you make any headway on this? My team is running into this also.
> >>> > Doesn't happen on smaller datasets. Our input set is about 10 GB but
> we
> >>> > generate 100s of GBs in the flow itself.
> >>> >
> >>> > -Suren
> >>> >
> >>> >
> >>> >
> >>> >
> >>> > On Fri, Jun 6, 2014 at 5:19 PM, Ryan Compton <compton.ryan@gmail.com
> >
> >>> wrote:
> >>> >
> >>> >> Just ran into this today myself. I'm on branch-1.0 using a CDH3
> >>> >> cluster (no modifications to Spark or its dependencies). The error
> >>> >> appeared trying to run GraphX's .connectedComponents() on a ~200GB
> >>> >> edge list (GraphX worked beautifully on smaller data).
> >>> >>
> >>> >> Here's the stacktrace (it's quite similar to yours
> >>> >> https://imgur.com/7iBA4nJ ).
> >>> >>
> >>> >> 14/06/05 20:02:28 ERROR scheduler.TaskSetManager: Task 5.599:39
> failed
> >>> >> 4 times; aborting job
> >>> >> 14/06/05 20:02:28 INFO scheduler.DAGScheduler: Failed to run reduce
> at
> >>> >> VertexRDD.scala:100
> >>> >> Exception in thread "main" org.apache.spark.SparkException: Job
> >>> >> aborted due to stage failure: Task 5.599:39 failed 4 times, most
> >>> >> recent failure: Exception failure in TID 29735 on host node18:
> >>> >> java.io.StreamCorruptedException: invalid type code: AC
> >>> >>
> >>> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1355)
> >>> >>
> java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
> >>> >>
> >>> >>
> >>>
> org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)
> >>> >>
> >>> >>
> >>>
> org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:125)
> >>> >>
> >>> org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
> >>> >>
> scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
> >>> >>
> >>> >>
> >>>
> org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30)
> >>> >>
> >>> >>
> >>>
> org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
> >>> >>
> scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
> >>> >>         scala.collection.Iterator$class.foreach(Iterator.scala:727)
> >>> >>
> scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
> >>> >>
> >>> >>
> >>>
> org.apache.spark.graphx.impl.VertexPartitionBaseOps.innerJoinKeepLeft(VertexPartitionBaseOps.scala:192)
> >>> >>
> >>> >>
> >>>
> org.apache.spark.graphx.impl.EdgePartition.updateVertices(EdgePartition.scala:78)
> >>> >>
> >>> >>
> >>>
> org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:75)
> >>> >>
> >>> >>
> >>>
> org.apache.spark.graphx.impl.ReplicatedVertexView$$anonfun$2$$anonfun$apply$1.apply(ReplicatedVertexView.scala:73)
> >>> >>         scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
> >>> >>
> scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
> >>> >>         scala.collection.Iterator$class.foreach(Iterator.scala:727)
> >>> >>
> scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
> >>> >>
> >>> >>
> >>>
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
> >>> >>
> >>> >>
> >>>
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
> >>> >>         org.apache.spark.scheduler.Task.run(Task.scala:51)
> >>> >>
> >>> >>
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
> >>> >>
> >>> >>
> >>>
> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
> >>> >>
> >>> >>
> >>>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
> >>> >>         java.lang.Thread.run(Thread.java:662)
> >>> >> Driver stacktrace:
> >>> >> at org.apache.spark.scheduler.DAGScheduler.org
> >>> >>
> >>>
> $apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1033)
> >>> >> at
> >>> >>
> >>>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1017)
> >>> >> at
> >>> >>
> >>>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1015)
> >>> >> at
> >>> >>
> >>>
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> >>> >> at
> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> >>> >> at
> >>> >>
> >>>
> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1015)
> >>> >> at
> >>> >>
> >>>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
> >>> >> at
> >>> >>
> >>>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:633)
> >>> >> at scala.Option.foreach(Option.scala:236)
> >>> >> at
> >>> >>
> >>>
> org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:633)
> >>> >> at
> >>> >>
> >>>
> org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1207)
> >>> >> at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
> >>> >> at akka.actor.ActorCell.invoke(ActorCell.scala:456)
> >>> >> at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
> >>> >> at akka.dispatch.Mailbox.run(Mailbox.scala:219)
> >>> >> at
> >>> >>
> >>>
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
> >>> >> at
> scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
> >>> >> at
> >>> >>
> >>>
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
> >>> >> at
> >>>
> scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
> >>> >> at
> >>> >>
> >>>
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
> >>> >> 14/06/05 20:02:28 INFO scheduler.TaskSchedulerImpl: Cancelling
> stage 5
> >>> >>
> >>> >> On Wed, Jun 4, 2014 at 7:50 AM, Sean Owen <sowen@cloudera.com>
> wrote:
> >>> >> > On Wed, Jun 4, 2014 at 3:33 PM, Matt Kielo <mkielo@oculusinfo.com
> >
> >>> >> wrote:
> >>> >> >> Im trying run some spark code on a cluster but I keep running
> into a
> >>> >> >> "java.io.StreamCorruptedException: invalid type code: AC" error.
> My
> >>> task
> >>> >> >> involves analyzing ~50GB of data (some operations involve
> sorting)
> >>> then
> >>> >> >> writing them out to a JSON file. Im running the analysis on each
> of
> >>> the
> >>> >> >> data's ~10 columns and have never had a successful run. My
> program
> >>> >> seems to
> >>> >> >> run for a varying amount of time each time (~between 5-30
> minutes)
> >>> but
> >>> >> it
> >>> >> >> always terminates with this error.
> >>> >> >
> >>> >> > I can tell you that this usually means somewhere something wrote
> >>> >> > objects to the same OutputStream with multiple
> ObjectOutputStreams. AC
> >>> >> > is a header value.
> >>> >> >
> >>> >> > I don't obviously see where/how that could happen, but maybe it
> rings
> >>> >> > a bell for someone. This could happen if an OutputStream is reused
> >>> >> > across object serializations but new ObjectOutputStreams are
> opened,
> >>> >> > for example.
> >>> >>
> >>> >
> >>> >
> >>> >
> >>> > --
> >>> >
> >>> > SUREN HIRAMAN, VP TECHNOLOGY
> >>> > Velos
> >>> > Accelerating Machine Learning
> >>> >
> >>> > 440 NINTH AVENUE, 11TH FLOOR
> >>> > NEW YORK, NY 10001
> >>> > O: (917) 525-2466 ext. 105
> >>> > F: 646.349.4063
> >>> > E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
> >>> > W: www.velos.io
> >>>
> >>
> >>
> >>
> >> --
> >>
> >> SUREN HIRAMAN, VP TECHNOLOGY
> >> Velos
> >> Accelerating Machine Learning
> >>
> >> 440 NINTH AVENUE, 11TH FLOOR
> >> NEW YORK, NY 10001
> >> O: (917) 525-2466 ext. 105
> >> F: 646.349.4063
> >> E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
> >> W: www.velos.io
>



-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io

--047d7bdc18242cfc2404fc259c14--

From dev-return-8070-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun 19 01:03:57 2014
Return-Path: <dev-return-8070-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2AD3B1104A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Jun 2014 01:03:57 +0000 (UTC)
Received: (qmail 95287 invoked by uid 500); 19 Jun 2014 01:03:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95218 invoked by uid 500); 19 Jun 2014 01:03:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95206 invoked by uid 99); 19 Jun 2014 01:03:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 01:03:56 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of doris.s.xin@gmail.com designates 209.85.219.45 as permitted sender)
Received: from [209.85.219.45] (HELO mail-oa0-f45.google.com) (209.85.219.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 01:03:50 +0000
Received: by mail-oa0-f45.google.com with SMTP id o6so3715672oag.32
        for <dev@spark.apache.org>; Wed, 18 Jun 2014 18:03:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=05erj3NZBPAm3xpI2cQ+L6upIh9u0KcEJWhPM561o/s=;
        b=NPFqFrw+gQ4QTTPo3rntoQ99rR4HhSvxvggPEotJZRd2GP+W5igpUzGVfd3Xt0oSGX
         W6WwhGh8FIUMmE5CTJhk3x9mmcH6L2y+nob6INqM7roAUm1w/uvvyPmgqnTXwGkai/11
         nF97kC4i6gOMRKNcNQvW+W/6RbYDISdX5CvSmG3OnrYTrjfOEqdYkqi6nYzoMqm1Kmf2
         Y2Kce8XCFEughvnwg9ORKQrKKdEnUhrx4Nt6xz0gQzZJLHk52cTH664LNZcnnRlvchR4
         7kM/vUvS7hmd8bkK9sWBU8YqV8LrUZmGWeji7eHnA9vAwJmc2WShiGJeEa0hKXqi7b5x
         CAhg==
MIME-Version: 1.0
X-Received: by 10.60.35.104 with SMTP id g8mr1398201oej.41.1403139810027; Wed,
 18 Jun 2014 18:03:30 -0700 (PDT)
Received: by 10.76.96.13 with HTTP; Wed, 18 Jun 2014 18:03:29 -0700 (PDT)
In-Reply-To: <CALuGr6bm7yKxkN4uKjHByYmpqY2=387PLqzErSo77sTcGHke1A@mail.gmail.com>
References: <811039BF-0610-4BAC-BFD7-41BBC981768C@gmail.com>
	<CABKvOWstGq-QGNHZnmvRptYcg5idh7LACOk1z3jn5bR=9L_Qrw@mail.gmail.com>
	<AF958CC3-2FFD-4A16-80A8-AA56DF239090@gmail.com>
	<CALuGr6bm7yKxkN4uKjHByYmpqY2=387PLqzErSo77sTcGHke1A@mail.gmail.com>
Date: Wed, 18 Jun 2014 18:03:29 -0700
Message-ID: <CAL90uaoQGvvPZ_dOkfOv-AjeNFhmQdJF5jDEVnyZgAh8YmdnjA@mail.gmail.com>
Subject: Re: Run ScalaTest inside Intellij IDEA
From: Doris Xin <doris.s.xin@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e013a1cfef26ed504fc25f1b2
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013a1cfef26ed504fc25f1b2
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Here's the JIRA on this known issue:
https://issues.apache.org/jira/browse/SPARK-1835

tl;dr: manually delete mesos-0.18.1.jar from lib_managed/jars after
running sbt/sbt
gen-idea. You should be able to run units inside Intellij after doing so.

Doris


On Tue, Jun 17, 2014 at 6:10 PM, Henry Saputra <henry.saputra@gmail.com>
wrote:

> I got stuck on this one too after did git pull from master.
>
> Have not been able to resolve it yet =3D(
>
>
> - Henry
>
> On Wed, Jun 11, 2014 at 6:51 AM, Yijie Shen <henry.yijieshen@gmail.com>
> wrote:
> > Thx Qiuzhuang, the problems disappeared after I add assembly jar at the
> head of list dependencies in *.iml, but while running test in Spark
> SQL(SQLQuerySuite in sql-core), another two error occurs:
> >
> > Error 1:
> > Error:scalac:
> >      while compiling:
> /Users/yijie/code/apache.spark.master/sql/core/src/main/scala/org/apache/=
spark/sql/test/TestSQLContext.scala
> >         during phase: jvm
> >      library version: version 2.10.4
> >     compiler version: version 2.10.4
> >   reconstructed args: -Xmax-classfile-name 120 -deprecation
> -P:genjavadoc:out=3D/Users/yijie/code/apache.spark.master/sql/core/target=
/java
> -feature -classpath
> /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/lib/ant-j=
avafx.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/l=
ib/dt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/l=
ib/javafx-doclet.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Cont=
ents/Home/lib/javafx-mx.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.j=
dk/Contents/Home/lib/jconsole.jar:/Library/Java/JavaVirtualMachines/jdk1.7.=
0_51.jdk/Contents/Home/lib/sa-jdi.jar:/Library/Java/JavaVirtualMachines/jdk=
1.7.0_51.jdk/Contents/Home/lib/tools.jar:/Library/Java/JavaVirtualMachines/=
jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtua=
lMachines/jdk1.7.0_51.jdk/Conte=E2=80=A6
> > =E2=80=A6
> > ...
> >
> /Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib/charsets.=
jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jre/lib=
/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home/jr=
e/classes:/Users/yijie/code/apache.spark.master/lib_managed/jars/scala-libr=
ary-2.10.4.jar
> -Xplugin:/Users/yijie/code/apache.spark.master/lib_managed/jars/genjavado=
c-plugin_2.10.4-0.5.jar
> -Xplugin:/Users/yijie/code/apache.spark.master/lib_managed/jars/genjavado=
c-plugin_2.10.4-0.5.jar
> >   last tree to typer: Literal(Constant(parquet.io.api.Converter))
> >               symbol: null
> >    symbol definition: null
> >                  tpe: Class(classOf[parquet.io.api.Converter])
> >        symbol owners:
> >       context owners: object TestSQLContext -> package test
> > =3D=3D Enclosing template or block =3D=3D
> > Template( // val <local TestSQLContext>: <notype> in object
> TestSQLContext, tree.tpe=3Dorg.apache.spark.sql.test.TestSQLContext.type
> >   "org.apache.spark.sql.SQLContext" // parents
> >   ValDef(
> >     private
> >     "_"
> >     <tpt>
> >     <empty>
> >   )
> >   // 2 statements
> >   DefDef( // private def readResolve(): Object in object TestSQLContext
> >     <method> private <synthetic>
> >     "readResolve"
> >     []
> >     List(Nil)
> >     <tpt> // tree.tpe=3DObject
> >     test.this."TestSQLContext" // object TestSQLContext in package test=
,
> tree.tpe=3Dorg.apache.spark.sql.test.TestSQLContext.type
> >   )
> >   DefDef( // def <init>(): org.apache.spark.sql.test.TestSQLContext.typ=
e
> in object TestSQLContext
> >     <method>
> >     "<init>"
> >     []
> >     List(Nil)
> >     <tpt> // tree.tpe=3Dorg.apache.spark.sql.test.TestSQLContext.type
> >     Block( // tree.tpe=3DUnit
> >       Apply( // def <init>(sparkContext: org.apache.spark.SparkContext)=
:
> org.apache.spark.sql.SQLContext in class SQLContext,
> tree.tpe=3Dorg.apache.spark.sql.SQLContext
> >         TestSQLContext.super."<init>" // def <init>(sparkContext:
> org.apache.spark.SparkContext): org.apache.spark.sql.SQLContext in class
> SQLContext, tree.tpe=3D(sparkContext:
> org.apache.spark.SparkContext)org.apache.spark.sql.SQLContext
> >         Apply( // def <init>(master: String,appName: String,conf:
> org.apache.spark.SparkConf): org.apache.spark.SparkContext in class
> SparkContext, tree.tpe=3Dorg.apache.spark.SparkContext
> >           new org.apache.spark.SparkContext."<init>" // def
> <init>(master: String,appName: String,conf: org.apache.spark.SparkConf):
> org.apache.spark.SparkContext in class SparkContext, tree.tpe=3D(master:
> String, appName: String, conf:
> org.apache.spark.SparkConf)org.apache.spark.SparkContext
> >           // 3 arguments
> >           "local"
> >           "TestSQLContext"
> >           Apply( // def <init>(): org.apache.spark.SparkConf in class
> SparkConf, tree.tpe=3Dorg.apache.spark.SparkConf
> >             new org.apache.spark.SparkConf."<init>" // def <init>():
> org.apache.spark.SparkConf in class SparkConf,
> tree.tpe=3D()org.apache.spark.SparkConf
> >             Nil
> >           )
> >         )
> >       )
> >       ()
> >     )
> >   )
> > )
> > =3D=3D Expanded type of tree =3D=3D
> > ConstantType(value =3D Constant(parquet.io.api.Converter))
> > uncaught exception during compilation: java.lang.AssertionError
> >
> > Error 2:
> >
> > Error:scalac: Error: assertion failed: List(object package$DebugNode,
> object package$DebugNode)
> > java.lang.AssertionError: assertion failed: List(object
> package$DebugNode, object package$DebugNode)
> >         at
> scala.reflect.internal.Symbols$Symbol.suchThat(Symbols.scala:1678)
> >         at
> scala.reflect.internal.Symbols$ClassSymbol.companionModule0(Symbols.scala=
:2988)
> >         at
> scala.reflect.internal.Symbols$ClassSymbol.companionModule(Symbols.scala:=
2991)
> >         at
> scala.tools.nsc.backend.jvm.GenASM$JPlainBuilder.genClass(GenASM.scala:13=
71)
> >         at
> scala.tools.nsc.backend.jvm.GenASM$AsmPhase.run(GenASM.scala:120)
> >         at
> scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
> >         at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
> >         at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
> >         at scala.tools.nsc.Global$Run.compile(Global.scala:1662)
> >         at xsbt.CachedCompiler0.run(CompilerInterface.scala:126)
> >         at xsbt.CachedCompiler0.run(CompilerInterface.scala:102)
> >         at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
> >         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> >         at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
> >         at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
> >         at java.lang.reflect.Method.invoke(Method.java:606)
> >         at
> sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)
> >         at
> sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)
> >         at
> sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
> >         at
> org.jetbrains.jps.incremental.scala.local.IdeaIncrementalCompiler.compile=
(IdeaIncrementalCompiler.scala:28)
> >         at
> org.jetbrains.jps.incremental.scala.local.LocalServer.compile(LocalServer=
.scala:25)
> >         at
> org.jetbrains.jps.incremental.scala.remote.Main$.make(Main.scala:64)
> >         at
> org.jetbrains.jps.incremental.scala.remote.Main$.nailMain(Main.scala:22)
> >         at
> org.jetbrains.jps.incremental.scala.remote.Main.nailMain(Main.scala)
> >         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> >         at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
> >         at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
> >         at java.lang.reflect.Method.invoke(Method.java:606)
> >         at com.martiansoftware.nailgun.NGSession.run(NGSession.java:319=
)
> >
> >
> > On Jun 11, 2014, at 11:17 AM, Qiuzhuang Lian <qiuzhuang.lian@gmail.com>
> wrote:
> >
> >> I also run into this problem when running examples in IDEA. The issue
> looks that it uses depends on too many jars and that the classpath seems =
to
> have length limit. So I import the assembly jar and put the head of the
> list dependent path and it works.
> >>
> >> Thanks,
> >> Qiuzhuang
> >>
> >>
> >> On Wed, Jun 11, 2014 at 10:39 AM, =E7=94=B3=E6=AF=85=E6=9D=B0 <henry.y=
ijieshen@gmail.com>
> wrote:
> >> Hi All,
> >>
> >> I want to run ScalaTest Suite in IDEA directly, but it seems didn=E2=
=80=99t
> pass the make phase before test running.
> >> The problems are as follows:
> >>
> >>
> /Users/yijie/code/apache.spark.master/core/src/main/scala/org/apache/spar=
k/executor/MesosExecutorBackend.scala
> >> Error:(44, 35) type mismatch;
> >>  found   : org.apache.mesos.protobuf.ByteString
> >>  required: com.google.protobuf.ByteString
> >>       .setData(ByteString.copyFrom(data))
> >>                                   ^
> >>
> /Users/yijie/code/apache.spark.master/core/src/main/scala/org/apache/spar=
k/scheduler/cluster/mesos/MesosSchedulerBackend.scala
> >> Error:(119, 35) type mismatch;
> >>  found   : org.apache.mesos.protobuf.ByteString
> >>  required: com.google.protobuf.ByteString
> >>       .setData(ByteString.copyFrom(createExecArg()))
> >>                                   ^
> >> Error:(257, 35) type mismatch;
> >>  found   : org.apache.mesos.protobuf.ByteString
> >>  required: com.google.protobuf.ByteString
> >>       .setData(ByteString.copyFrom(task.serializedTask))
> >>                                   ^
> >>
> >> Before I run test in IDEA, I build spark through =E2=80=99sbt/sbt asse=
mbly=E2=80=99,
> >> import projects into IDEA after =E2=80=99sbt/sbt gen-idea=E2=80=99,
> >> and able to run test in Terminal =E2=80=99sbt/sbt test=E2=80=99
> >>
> >> Are there anything I leave out in order to run/debug testsuite inside
> IDEA?
> >>
> >> Best regards,
> >> Yijie
> >>
> >
>

--089e013a1cfef26ed504fc25f1b2--

From dev-return-8071-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun 19 01:20:05 2014
Return-Path: <dev-return-8071-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8FE731109D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Jun 2014 01:20:05 +0000 (UTC)
Received: (qmail 20756 invoked by uid 500); 19 Jun 2014 01:20:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20700 invoked by uid 500); 19 Jun 2014 01:20:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20684 invoked by uid 99); 19 Jun 2014 01:20:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 01:20:02 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 01:20:00 +0000
Received: by mail-qg0-f42.google.com with SMTP id e89so1553597qgf.29
        for <dev@spark.apache.org>; Wed, 18 Jun 2014 18:19:35 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=P5f7av+HRAwgCPGUY2anSTf3F2mGGJkDuVZApIudpB8=;
        b=Nr4oHjr0glsHdWgC6xHnhVoyABf95bnQLorkiqgHhS8u/1SEZjLAsuQg3lGjbQXDUH
         ILErrghck0XgKXmmFMHJqktoItW9o5gM3/ImRgRhxOVhCIhV8aYtWMqPD423OsL338MX
         wjrlsaQGlrtAJMa9EteOA64hOTOXPod62Yt3Bc8DS+Ff0CR5brh7Xt0bv8GyKz7Lq9P2
         ElPQ+fvAn4Q7G/pIoNOHahD4go2+j2z1WrjeuQ1CLtJy13UjaHXqfyyK3vnamb4NQ8B8
         wRrZXjizy7fOydgVE+z0VgAR4Qe9bEHNCIJHPlIsYaPQGBOmk1kpHsp1k0bz+YyGmL96
         bN2g==
X-Gm-Message-State: ALoCoQn+ximhmbsMKhLvcRvEw7U8l3mrO7CkHyJEvr90FVsQw0FiiccFMRabFT4j/tL2ntHao4AL
X-Received: by 10.224.114.17 with SMTP id c17mr2549179qaq.68.1403140775922;
 Wed, 18 Jun 2014 18:19:35 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.209.3 with HTTP; Wed, 18 Jun 2014 18:19:15 -0700 (PDT)
In-Reply-To: <1351204415.36085483.1403130174646.JavaMail.zimbra@redhat.com>
References: <1227418919.36083781.1403129856067.JavaMail.zimbra@redhat.com> <1351204415.36085483.1403130174646.JavaMail.zimbra@redhat.com>
From: Michael Armbrust <michael@databricks.com>
Date: Thu, 19 Jun 2014 03:19:15 +0200
Message-ID: <CAAswR-6N+KmhuSbpRPde5Hvx=YY0FLMXF10GfPX5J_PWysJiLA@mail.gmail.com>
Subject: Re: question about Hive compatiblilty tests
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bdca10e84e21404fc262bc0
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdca10e84e21404fc262bc0
Content-Type: text/plain; charset=UTF-8

I assume you are adding tests?  because that is the only time you should
see that message.

That error could mean a couple of things:
 1) The query is invalid and hive threw an exception
 2) Your Hive setup is bad.

Regarding #2, you need to have the source for Hive 0.12.0 available and
built as well as a hadoop installation.  You also have to have the
environment vars set as specified here:
https://github.com/apache/spark/tree/master/sql

Michael


On Thu, Jun 19, 2014 at 12:22 AM, Will Benton <willb@redhat.com> wrote:

> Hi all,
>
> Does a "Failed to generate golden answer for query" message from
> HiveComparisonTests indicate that it isn't possible to run the query in
> question under Hive from Spark's test suite rather than anything about
> Spark's implementation of HiveQL?  The stack trace I'm getting implicates
> Hive code and not Spark code, but I wanted to make sure I wasn't missing
> something.
>
>
> thanks,
> wb
>

--047d7bdca10e84e21404fc262bc0--

From dev-return-8072-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun 19 02:48:55 2014
Return-Path: <dev-return-8072-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D32C2112E4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Jun 2014 02:48:55 +0000 (UTC)
Received: (qmail 60679 invoked by uid 500); 19 Jun 2014 02:48:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60618 invoked by uid 500); 19 Jun 2014 02:48:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60607 invoked by uid 99); 19 Jun 2014 02:48:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 02:48:54 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of wibenton@redhat.com designates 209.132.183.25 as permitted sender)
Received: from [209.132.183.25] (HELO mx4-phx2.redhat.com) (209.132.183.25)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 02:48:50 +0000
Received: from zmail14.collab.prod.int.phx2.redhat.com (zmail14.collab.prod.int.phx2.redhat.com [10.5.83.16])
	by mx4-phx2.redhat.com (8.13.8/8.13.8) with ESMTP id s5J2mO0l011780
	for <dev@spark.apache.org>; Wed, 18 Jun 2014 22:48:24 -0400
Date: Wed, 18 Jun 2014 22:48:24 -0400 (EDT)
From: Will Benton <willb@redhat.com>
To: dev@spark.apache.org
Message-ID: <164607810.36162509.1403146104102.JavaMail.zimbra@redhat.com>
In-Reply-To: <CAAswR-6N+KmhuSbpRPde5Hvx=YY0FLMXF10GfPX5J_PWysJiLA@mail.gmail.com>
References: <1227418919.36083781.1403129856067.JavaMail.zimbra@redhat.com> <1351204415.36085483.1403130174646.JavaMail.zimbra@redhat.com> <CAAswR-6N+KmhuSbpRPde5Hvx=YY0FLMXF10GfPX5J_PWysJiLA@mail.gmail.com>
Subject: Re: question about Hive compatiblilty tests
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.12]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - FF29 (Mac)/8.0.6_GA_5922)
Thread-Topic: question about Hive compatiblilty tests
Thread-Index: VplPTFtdsTYt6UHz30bQi+e0GhdxRA==
X-Virus-Checked: Checked by ClamAV on apache.org

> I assume you are adding tests?  because that is the only time you should
> see that message.

Yes, I had added the HAVING test to the whitelist.

> That error could mean a couple of things:
>  1) The query is invalid and hive threw an exception
>  2) Your Hive setup is bad.
> 
> Regarding #2, you need to have the source for Hive 0.12.0 available and
> built as well as a hadoop installation.  You also have to have the
> environment vars set as specified here:
> https://github.com/apache/spark/tree/master/sql

Thanks!  The other Hive compatibility tests seem to work, so I'll dig in a bit more to see if I can figure out what's happening here.


best,
wb


From dev-return-8073-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun 19 14:48:14 2014
Return-Path: <dev-return-8073-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B771111835
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Jun 2014 14:48:14 +0000 (UTC)
Received: (qmail 32327 invoked by uid 500); 19 Jun 2014 14:48:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32265 invoked by uid 500); 19 Jun 2014 14:48:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32254 invoked by uid 99); 19 Jun 2014 14:48:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 14:48:12 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.178] (HELO mail-pd0-f178.google.com) (209.85.192.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 14:48:10 +0000
Received: by mail-pd0-f178.google.com with SMTP id r10so1914728pdi.9
        for <dev@spark.apache.org>; Thu, 19 Jun 2014 07:47:45 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:sender:date:message-id:subject:from
         :to:content-type;
        bh=i90vGa5YKAkh5IloT8ncOfkHjg63/E8/6xcAT/oaxGQ=;
        b=aZxwfqNjFeig85AiADjI3rjIDch5LS1gGZLGppxwFwcIeIZS8xBicgkdH86tMOyv2m
         l28ya/X3LYoi9pQIUGxk7rtYnB3u2/QFDudMQ9lxasDKax7GW8wo91+ZHr4ErU5cCdIt
         dmlZbjfCN2DdgAAcQegka4D5VxNOdRRl8TYdYCHf72Nyq8FZBIoXZjcC6u8wQOnBDSF2
         1Q5KPk32u2kYYwqb8H0tsjjdwWk9xamKPNmXeffdPW4x/IWlSr1SiVmvFKMPWnwAZNc5
         iF1Ma8tK/7pZKuqAuWn3DPfLznSWcV9qcjeC7sep/8u+SbEthHQeTfn0Tswl+C7t4SHs
         2bpQ==
X-Gm-Message-State: ALoCoQm4neAUjNiFTXjrrB7VfJmj3UDLCuFdrjt7oM3gpnkmX/uhCWQQRptxLFddmZ43h5jhWs1j
MIME-Version: 1.0
X-Received: by 10.68.194.229 with SMTP id hz5mr6011228pbc.91.1403189265291;
 Thu, 19 Jun 2014 07:47:45 -0700 (PDT)
Sender: mark@coactus.com
Received: by 10.70.22.134 with HTTP; Thu, 19 Jun 2014 07:47:45 -0700 (PDT)
X-Originating-IP: [192.0.216.13]
Date: Thu, 19 Jun 2014 10:47:45 -0400
X-Google-Sender-Auth: -PLQLLMjO4ssceQTiY6v-dTcf5Q
Message-ID: <CALcoZiqBFbcU=tdTLosT+x5DfzSxyeUvjFYDmGMXxTigvKNhhg@mail.gmail.com>
Subject: Problems with Pyspark + Dill tests
From: Mark Baker <distobj@acm.org>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi. As part of my attempt to port Pyspark to Python 3, I've
re-applied, with modifications, Josh's old commit for using Dill with
Pyspark (as Dill already supports Python 3). Alas, I ran into an odd
problem that I could use some help with.

Josh's old commit;

https://github.com/JoshRosen/incubator-spark/commit/2ac8986f3009f0dc133b11d16887fc8ddb33c3d1

My Dill branch;

https://github.com/distobj/spark/tree/dill

(Note; I've been running this in a virtualenv into which I
pip-installed dill. I haven't yet figured out the new way to package
it in python/lib as was done for py4j)

So the problem is that run_tests is failing with this pickle.py error
on most of the tests (those using .cache() it seems, unsurprisingly);

    PicklingError: Can't pickle <type '_sre.SRE_Pattern'>: it's not
found as _sre.SRE_Pattern

What's odd is that the same doctests work fine when run from the shell.

TIA for any ideas...

From dev-return-8074-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun 19 18:19:40 2014
Return-Path: <dev-return-8074-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 14F28110D0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Jun 2014 18:19:40 +0000 (UTC)
Received: (qmail 46835 invoked by uid 500); 19 Jun 2014 18:19:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46779 invoked by uid 500); 19 Jun 2014 18:19:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46766 invoked by uid 99); 19 Jun 2014 18:19:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 18:19:39 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of suren.hiraman@sociocast.com designates 209.85.128.180 as permitted sender)
Received: from [209.85.128.180] (HELO mail-ve0-f180.google.com) (209.85.128.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 18:19:36 +0000
Received: by mail-ve0-f180.google.com with SMTP id jw12so2688937veb.11
        for <dev@spark.apache.org>; Thu, 19 Jun 2014 11:19:11 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=HSnCzYUBqRYnEHLj2DIDptFJbbsPMPBdwBqg1D8j1vA=;
        b=QVRjbP8X2tcuBlJ0Lh4gTYJXA+nK0q9p40VXbb5fnTGhU/kILy5/OzCW+j3Z3E26BM
         0/bpYKha9tz0icAszAvnGMqzFQTAWEed3YLpVrZ84tIJuG/f5YeD/cLuDglhVGy/QG9s
         cGDddUk/feLdDZk7IJTU09g2SdgPg4NgCMpJxmMjS/ocJz4JLxBPmqNevdzri1uQaNaR
         xZhKoALea+OdHyzVLaGRUug6i+X0cCEfrs/IbBirpi749lYrTFo+s9SQvP/4SQYrnDvf
         sFcTWptU7ks31iin6HVBgI168Hhe38VZSEDhyQH0yGvmFGRiuGIGDZWJXyONHbF2Fag3
         1IBw==
X-Gm-Message-State: ALoCoQlXEWjigAR5+hHFIb1Oc5KO0rLCZPUH9Xiw3Z/4bvNjW5XwzONliChHWx40XQOfMIRLDmqi
MIME-Version: 1.0
X-Received: by 10.53.2.129 with SMTP id bo1mr1388266vdd.77.1403201951756; Thu,
 19 Jun 2014 11:19:11 -0700 (PDT)
Received: by 10.58.143.49 with HTTP; Thu, 19 Jun 2014 11:19:11 -0700 (PDT)
In-Reply-To: <CALWDz_tMYb9RfA-kFV0036LHvXELe5eAGi8OPO9GGZEg8oJUwg@mail.gmail.com>
References: <CALWDz_sRpnBEqRP9fR9fT+VEpquAkC4kJpMxd1rCKmXCvx5sWw@mail.gmail.com>
	<CALWDz_tMYb9RfA-kFV0036LHvXELe5eAGi8OPO9GGZEg8oJUwg@mail.gmail.com>
Date: Thu, 19 Jun 2014 14:19:11 -0400
Message-ID: <CALWDz_tC=uvhJSJOQ3MvAr5LGeUZ0rDrGQQYzrJqKYnD6F4JmQ@mail.gmail.com>
Subject: Re: Trailing Tasks Saving to HDFS
From: Surendranauth Hiraman <suren.hiraman@velos.io>
To: Surendranauth Hiraman <suren.hiraman@velos.io>
Cc: user@spark.apache.org, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11362044e20da604fc34696b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11362044e20da604fc34696b
Content-Type: text/plain; charset=UTF-8

I've created an issue for this but if anyone has any advice, please let me
know.

Basically, on about 10 GBs of data, saveAsTextFile() to HDFS hangs on two
remaining tasks (out of 320). Those tasks seem to be waiting on data from
another task on another node. Eventually (about 2 hours later) they time
out with a connection reset by peer.

All the data actually seems to be on HDFS as the expected part files. It
just seems like the remaining tasks have corrupted "metadata", so that they
do not realize that they are done. Just a guess though.

https://issues.apache.org/jira/browse/SPARK-2202

-Suren




On Wed, Jun 18, 2014 at 8:35 PM, Surendranauth Hiraman <
suren.hiraman@velos.io> wrote:

> Looks like eventually there was some type of reset or timeout and the
> tasks have been reassigned. I'm guessing they'll keep failing until max
> failure count.
>
> The machine it disconnected from was a remote machine, though I've seen
> such failures from connections to itself with other problems. The log lines
> from the remote machine are also below.
>
> Any thoughts or guesses would be appreciated!
>
> *"HUNG" WORKER*
>
> 14/06/18 19:41:18 WARN network.ReceivingConnection: Error reading from
> connection to ConnectionManagerId(172.16.25.103,57626)
>
> java.io.IOException: Connection reset by peer
>
> at sun.nio.ch.FileDispatcher.read0(Native Method)
>
> at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
>
> at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:251)
>
> at sun.nio.ch.IOUtil.read(IOUtil.java:224)
>
> at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:254)
>
> at org.apache.spark.network.ReceivingConnection.read(Connection.scala:496)
>
> at
> org.apache.spark.network.ConnectionManager$$anon$6.run(ConnectionManager.scala:175)
>
> at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
>
> at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
>
> at java.lang.Thread.run(Thread.java:679)
>
> 14/06/18 19:41:18 INFO network.ConnectionManager: Handling connection
> error on connection to ConnectionManagerId(172.16.25.103,57626)
>
> 14/06/18 19:41:18 INFO network.ConnectionManager: Removing
> ReceivingConnection to ConnectionManagerId(172.16.25.103,57626)
>
> 14/06/18 19:41:18 INFO network.ConnectionManager: Removing
> SendingConnection to ConnectionManagerId(172.16.25.103,57626)
>
> 14/06/18 19:41:18 INFO network.ConnectionManager: Removing
> ReceivingConnection to ConnectionManagerId(172.16.25.103,57626)
>
> 14/06/18 19:41:18 ERROR network.ConnectionManager: Corresponding
> SendingConnectionManagerId not found
>
>
> *REMOTE WORKER*
>
> 14/06/18 19:41:18 INFO network.ConnectionManager: Removing
> ReceivingConnection to ConnectionManagerId(172.16.25.124,55610)
>
> 14/06/18 19:41:18 ERROR network.ConnectionManager: Corresponding
> SendingConnectionManagerId not found
>
>
>
> On Wed, Jun 18, 2014 at 7:16 PM, Surendranauth Hiraman <
> suren.hiraman@velos.io> wrote:
>
>> I have a flow that ends with saveAsTextFile() to HDFS.
>>
>> It seems all the expected files per partition have been written out,
>> based on the number of part files and the file sizes.
>>
>> But the driver logs show 2 tasks still not completed and has no activity
>> and the worker logs show no activity for those two tasks for a while now.
>>
>> Has anyone run into this situation? It's happened to me a couple of times
>> now.
>>
>> Thanks.
>>
>> -- Suren
>>
>> SUREN HIRAMAN, VP TECHNOLOGY
>> Velos
>> Accelerating Machine Learning
>>
>> 440 NINTH AVENUE, 11TH FLOOR
>> NEW YORK, NY 10001
>> O: (917) 525-2466 ext. 105
>> F: 646.349.4063
>> E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
>> W: www.velos.io
>>
>>
>
>
> --
>
> SUREN HIRAMAN, VP TECHNOLOGY
> Velos
> Accelerating Machine Learning
>
> 440 NINTH AVENUE, 11TH FLOOR
> NEW YORK, NY 10001
> O: (917) 525-2466 ext. 105
> F: 646.349.4063
> E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
> W: www.velos.io
>
>


-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io

--001a11362044e20da604fc34696b--

From dev-return-8075-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun 19 18:38:50 2014
Return-Path: <dev-return-8075-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 31AF2111E8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Jun 2014 18:38:50 +0000 (UTC)
Received: (qmail 7370 invoked by uid 500); 19 Jun 2014 18:38:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7311 invoked by uid 500); 19 Jun 2014 18:38:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7299 invoked by uid 99); 19 Jun 2014 18:38:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 18:38:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.213.175 as permitted sender)
Received: from [209.85.213.175] (HELO mail-ig0-f175.google.com) (209.85.213.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 18:38:43 +0000
Received: by mail-ig0-f175.google.com with SMTP id h3so2452142igd.8
        for <dev@spark.apache.org>; Thu, 19 Jun 2014 11:38:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:subject:mime-version:content-type;
        bh=+ECwXNXUFl8tua4EAILXNEqx5F8OcY6DzgWHPiOOHSs=;
        b=lPbfcwLEhh2L1146RcInnonRc2y3ztm7UfIazdes2cXiRjhCsgCw/2rY7RrhqTyiFE
         mjFqnGf0LDyQlVXsuuRHD+Z80G6zadVJ0vrU0IkEV0ZIcSL2p5vVD3VuxKfqqaYqnEdf
         a7Jdh6+ylj+fflxpQZvzC8gftUrk37QyX9/Xbthj3mvTkoR4g+z2/GxwidFDYaONfkem
         +hCwGI8D4hu8UcNsqYw0jVNDldYSgpgCY+y/DqQZUUtKzUJXyBPkOIn5xhGVy54TwT0a
         RSZbueO8a0+q1kzfVrhYn85meGfryfgqn/jadHLqHDggs3zEwtnHbZbl4UgDHf5vi/k5
         vV4g==
X-Received: by 10.50.1.111 with SMTP id 15mr10227737igl.7.1403203099243;
        Thu, 19 Jun 2014 11:38:19 -0700 (PDT)
Received: from [192.168.2.11] (MTRLPQ02-1177746539.sdsl.bell.ca. [70.50.252.107])
        by mx.google.com with ESMTPSA id rr3sm9284822igb.0.2014.06.19.11.38.18
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Thu, 19 Jun 2014 11:38:18 -0700 (PDT)
Date: Thu, 19 Jun 2014 14:47:50 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Message-ID: <82AC842388FE42BFBEF3FCF349C06E94@gmail.com>
Subject: assign SPARK-2126 to me?
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53a33056_4df72e4e_20d"
X-Virus-Checked: Checked by ClamAV on apache.org

--53a33056_4df72e4e_20d
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

Hi, all

Any admin can assign this issue https://issues.apache.org/jira/browse/SPARK-2126 to me?

I have started working on this

Thanks,

-- 
Nan Zhu


--53a33056_4df72e4e_20d--


From dev-return-8076-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun 19 19:09:09 2014
Return-Path: <dev-return-8076-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 21BC51138E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Jun 2014 19:09:09 +0000 (UTC)
Received: (qmail 6389 invoked by uid 500); 19 Jun 2014 19:09:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6287 invoked by uid 500); 19 Jun 2014 19:09:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5440 invoked by uid 99); 19 Jun 2014 19:09:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 19:09:07 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.49 as permitted sender)
Received: from [209.85.219.49] (HELO mail-oa0-f49.google.com) (209.85.219.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 19:09:01 +0000
Received: by mail-oa0-f49.google.com with SMTP id i7so6062269oag.22
        for <multiple recipients>; Thu, 19 Jun 2014 12:08:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=LnzZdzX81g/bkv918xvuhZZTEQ3Fw9528gcvbIYblRY=;
        b=gFkKi1kl+gvF09/TCZlB4bpN/WojM8jrHaDYZ/rFcbjbYqJ8U06fkBp/qwfWkpnEFW
         kAboWbUqkHzzLQYwTSri9cDSG9zgnmVIL46t2z/aE9wiUwvZm++xCrpk6euT0qeryA+i
         VKrQa/jVgvgxN34t8U8TzpNiEfJC3XB7sTuSYkVuK0QzMGWrGac2IDnklVYxA3Nlg57v
         hOlrnOZroLW70Gms9Y01IRkwruEjqz5JBHoHSl+Je5GNES8LLgAu8CrUflIgCv7pWMp8
         EeeqRXe2HBn2Q98q+/bRKxISiSdrbBwUvyaq9epSuu8gOXnxHW8br2eyLFG/Mf/oqPx6
         JD4w==
MIME-Version: 1.0
X-Received: by 10.60.79.104 with SMTP id i8mr4606575oex.67.1403204921347; Thu,
 19 Jun 2014 12:08:41 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Thu, 19 Jun 2014 12:08:41 -0700 (PDT)
In-Reply-To: <CALWDz_tC=uvhJSJOQ3MvAr5LGeUZ0rDrGQQYzrJqKYnD6F4JmQ@mail.gmail.com>
References: <CALWDz_sRpnBEqRP9fR9fT+VEpquAkC4kJpMxd1rCKmXCvx5sWw@mail.gmail.com>
	<CALWDz_tMYb9RfA-kFV0036LHvXELe5eAGi8OPO9GGZEg8oJUwg@mail.gmail.com>
	<CALWDz_tC=uvhJSJOQ3MvAr5LGeUZ0rDrGQQYzrJqKYnD6F4JmQ@mail.gmail.com>
Date: Thu, 19 Jun 2014 12:08:41 -0700
Message-ID: <CABPQxsuBR+iCy_Ur45Gf3xeLfoQW7gf7kE6w1zPoqRseP3dAvQ@mail.gmail.com>
Subject: Re: Trailing Tasks Saving to HDFS
From: Patrick Wendell <pwendell@gmail.com>
To: user@spark.apache.org
Cc: Surendranauth Hiraman <suren.hiraman@velos.io>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I'll make a comment on the JIRA - thanks for reporting this, let's get
to the bottom of it.

On Thu, Jun 19, 2014 at 11:19 AM, Surendranauth Hiraman
<suren.hiraman@velos.io> wrote:
> I've created an issue for this but if anyone has any advice, please let me
> know.
>
> Basically, on about 10 GBs of data, saveAsTextFile() to HDFS hangs on two
> remaining tasks (out of 320). Those tasks seem to be waiting on data from
> another task on another node. Eventually (about 2 hours later) they time out
> with a connection reset by peer.
>
> All the data actually seems to be on HDFS as the expected part files. It
> just seems like the remaining tasks have corrupted "metadata", so that they
> do not realize that they are done. Just a guess though.
>
> https://issues.apache.org/jira/browse/SPARK-2202
>
> -Suren
>
>
>
>
> On Wed, Jun 18, 2014 at 8:35 PM, Surendranauth Hiraman
> <suren.hiraman@velos.io> wrote:
>>
>> Looks like eventually there was some type of reset or timeout and the
>> tasks have been reassigned. I'm guessing they'll keep failing until max
>> failure count.
>>
>> The machine it disconnected from was a remote machine, though I've seen
>> such failures from connections to itself with other problems. The log lines
>> from the remote machine are also below.
>>
>> Any thoughts or guesses would be appreciated!
>>
>> "HUNG" WORKER
>>
>> 14/06/18 19:41:18 WARN network.ReceivingConnection: Error reading from
>> connection to ConnectionManagerId(172.16.25.103,57626)
>>
>> java.io.IOException: Connection reset by peer
>>
>> at sun.nio.ch.FileDispatcher.read0(Native Method)
>>
>> at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39)
>>
>> at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:251)
>>
>> at sun.nio.ch.IOUtil.read(IOUtil.java:224)
>>
>> at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:254)
>>
>> at org.apache.spark.network.ReceivingConnection.read(Connection.scala:496)
>>
>> at
>> org.apache.spark.network.ConnectionManager$$anon$6.run(ConnectionManager.scala:175)
>>
>> at
>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
>>
>> at
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
>>
>> at java.lang.Thread.run(Thread.java:679)
>>
>> 14/06/18 19:41:18 INFO network.ConnectionManager: Handling connection
>> error on connection to ConnectionManagerId(172.16.25.103,57626)
>>
>> 14/06/18 19:41:18 INFO network.ConnectionManager: Removing
>> ReceivingConnection to ConnectionManagerId(172.16.25.103,57626)
>>
>> 14/06/18 19:41:18 INFO network.ConnectionManager: Removing
>> SendingConnection to ConnectionManagerId(172.16.25.103,57626)
>>
>> 14/06/18 19:41:18 INFO network.ConnectionManager: Removing
>> ReceivingConnection to ConnectionManagerId(172.16.25.103,57626)
>>
>> 14/06/18 19:41:18 ERROR network.ConnectionManager: Corresponding
>> SendingConnectionManagerId not found
>>
>>
>> REMOTE WORKER
>>
>> 14/06/18 19:41:18 INFO network.ConnectionManager: Removing
>> ReceivingConnection to ConnectionManagerId(172.16.25.124,55610)
>>
>> 14/06/18 19:41:18 ERROR network.ConnectionManager: Corresponding
>> SendingConnectionManagerId not found
>>
>>
>>
>>
>> On Wed, Jun 18, 2014 at 7:16 PM, Surendranauth Hiraman
>> <suren.hiraman@velos.io> wrote:
>>>
>>> I have a flow that ends with saveAsTextFile() to HDFS.
>>>
>>> It seems all the expected files per partition have been written out,
>>> based on the number of part files and the file sizes.
>>>
>>> But the driver logs show 2 tasks still not completed and has no activity
>>> and the worker logs show no activity for those two tasks for a while now.
>>>
>>> Has anyone run into this situation? It's happened to me a couple of times
>>> now.
>>>
>>> Thanks.
>>>
>>> -- Suren
>>>
>>> SUREN HIRAMAN, VP TECHNOLOGY
>>> Velos
>>> Accelerating Machine Learning
>>>
>>> 440 NINTH AVENUE, 11TH FLOOR
>>> NEW YORK, NY 10001
>>> O: (917) 525-2466 ext. 105
>>> F: 646.349.4063
>>> E: suren.hiraman@velos.io
>>> W: www.velos.io
>>>
>>
>>
>>
>> --
>>
>> SUREN HIRAMAN, VP TECHNOLOGY
>> Velos
>> Accelerating Machine Learning
>>
>> 440 NINTH AVENUE, 11TH FLOOR
>> NEW YORK, NY 10001
>> O: (917) 525-2466 ext. 105
>> F: 646.349.4063
>> E: suren.hiraman@velos.io
>> W: www.velos.io
>>
>
>
>
> --
>
> SUREN HIRAMAN, VP TECHNOLOGY
> Velos
> Accelerating Machine Learning
>
> 440 NINTH AVENUE, 11TH FLOOR
> NEW YORK, NY 10001
> O: (917) 525-2466 ext. 105
> F: 646.349.4063
> E: suren.hiraman@velos.io
> W: www.velos.io
>

From dev-return-8077-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun 19 19:57:21 2014
Return-Path: <dev-return-8077-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 24A12116D4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Jun 2014 19:57:21 +0000 (UTC)
Received: (qmail 24054 invoked by uid 500); 19 Jun 2014 19:57:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23993 invoked by uid 500); 19 Jun 2014 19:57:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23979 invoked by uid 99); 19 Jun 2014 19:57:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 19:57:20 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rosenville@gmail.com designates 209.85.220.45 as permitted sender)
Received: from [209.85.220.45] (HELO mail-pa0-f45.google.com) (209.85.220.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 19:57:17 +0000
Received: by mail-pa0-f45.google.com with SMTP id rd3so2233599pab.18
        for <dev@spark.apache.org>; Thu, 19 Jun 2014 12:56:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=2pbu3aF4yadf+Hah28DZAKYs9TIezknUTIupbwm/7Ow=;
        b=uW3glKRXwqYJqW1wYCFOVnNh8ejqB+jB6RW13Ebp1gmzUOKe9Y46kh+cWfkDDAyR68
         noKkjfS3PstiNhXvc8WSyEvd935ntOp+cR5sVsgCTHOyUbcBXRjAxk0RgWTIj1qMN28W
         xnG+nvOe54guIG8bUmn4ZpAvY9GLlAu5vUj2rkNcCsTOCtEPlRKD5XRDMpbubGUTORdu
         kRDfa/YZ5L+EG1JmPQtjXi9JS+YCgY2M/jO065RghULNoxuycfwaDSUilwNzdrwHm7eJ
         kobiPfANZR/0yotDCbk3+oxmbSb8HgoV8cjUihSSpey3VO9VJ4yoi94sKZNDnaBwb24W
         ++8w==
X-Received: by 10.66.248.228 with SMTP id yp4mr8390624pac.94.1403207812992;
        Thu, 19 Jun 2014 12:56:52 -0700 (PDT)
Received: from dhcp-47-206.eecs.berkeley.edu (dhcp-47-206.EECS.Berkeley.EDU. [128.32.47.206])
        by mx.google.com with ESMTPSA id lq6sm31340443pab.48.2014.06.19.12.56.52
        for <dev@spark.apache.org>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 19 Jun 2014 12:56:52 -0700 (PDT)
Date: Thu, 19 Jun 2014 12:56:51 -0700
From: Josh Rosen <rosenville@gmail.com>
To: dev@spark.apache.org
Message-ID: <etPan.53a34084.6b8b4567.100@dhcp-47-206.eecs.berkeley.edu>
In-Reply-To: <CALcoZiqBFbcU=tdTLosT+x5DfzSxyeUvjFYDmGMXxTigvKNhhg@mail.gmail.com>
References: <CALcoZiqBFbcU=tdTLosT+x5DfzSxyeUvjFYDmGMXxTigvKNhhg@mail.gmail.com>
Subject: Re: Problems with Pyspark + Dill tests
X-Mailer: Airmail Beta (242)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53a34084_327b23c6_100"
X-Virus-Checked: Checked by ClamAV on apache.org

--53a34084_327b23c6_100
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

Thanks for helping with the Dill integration; I had some early first atte=
mpts, but had to set them aside when I got busy with some other work.

Just to bring everyone up to speed regarding context:
There are some objects that PySpark=E2=80=99s =60cloudpickle=60 library d=
oesn=E2=80=99t serialize properly, such as operator.getattr (https://issu=
es.apache.org/jira/browse/SPARK-791) or NamedTuples (https://issues.apach=
e.org/jira/browse/SPARK-1687).
My early attempt at replacing CloudPickle with Dill ran into problems bec=
ause of slight differences in how Dill pickles functions defined in docte=
sts versus functions defined elsewhere. =C2=A0I opened a bug report for t=
his with the Dill developers (https://github.com/uqfoundation/dill/issues=
/18), who subsequently fixed the bug (https://github.com/uqfoundation/dil=
l/pull/29).
It looks like there=E2=80=99s already a couple of Dill issues with exampl=
es of the =E2=80=9CCan=E2=80=99t pickle =5F it=E2=80=99s not found as =5F=
=E2=80=9D bug (https://github.com/uqfoundation/dill/search=3Fq=3D%22not+f=
ound+as%22&type=3DIssues). =C2=A0If you can find a small test case that r=
eproduces this bug, I=E2=80=99d consider opening a new Dill issue.

- Josh
On June 19, 2014 at 7:48:13 AM, Mark Baker (distobj=40acm.org) wrote:

Hi. As part of my attempt to port Pyspark to Python 3, I've =20
re-applied, with modifications, Josh's old commit for using Dill with =20
Pyspark (as Dill already supports Python 3). Alas, I ran into an odd =20
problem that I could use some help with. =20

Josh's old commit; =20

https://github.com/JoshRosen/incubator-spark/commit/2ac8986f3009f0dc133b1=
1d16887fc8ddb33c3d1 =20

My Dill branch; =20

https://github.com/distobj/spark/tree/dill =20

(Note; I've been running this in a virtualenv into which I =20
pip-installed dill. I haven't yet figured out the new way to package =20
it in python/lib as was done for py4j) =20

So the problem is that run=5Ftests is failing with this pickle.py error =20
on most of the tests (those using .cache() it seems, unsurprisingly); =20

PicklingError: Can't pickle <type '=5Fsre.SRE=5FPattern'>: it's not =20
found as =5Fsre.SRE=5FPattern =20

What's odd is that the same doctests work fine when run from the shell. =20

TIA for any ideas... =20

--53a34084_327b23c6_100--


From dev-return-8078-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun 19 22:13:34 2014
Return-Path: <dev-return-8078-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8F7CF11D6F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Jun 2014 22:13:34 +0000 (UTC)
Received: (qmail 45646 invoked by uid 500); 19 Jun 2014 22:13:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45587 invoked by uid 500); 19 Jun 2014 22:13:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45575 invoked by uid 99); 19 Jun 2014 22:13:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 22:13:33 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of langel.groups@gmail.com designates 74.125.82.51 as permitted sender)
Received: from [74.125.82.51] (HELO mail-wg0-f51.google.com) (74.125.82.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Jun 2014 22:13:30 +0000
Received: by mail-wg0-f51.google.com with SMTP id x12so2854585wgg.34
        for <dev@spark.apache.org>; Thu, 19 Jun 2014 15:13:06 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=yo5436tCE0K3q/06ahFu5c/Ib8N9gLp1Y8TUoFoNEMQ=;
        b=0637JEaHa9d+yBNwc/0498tQWLRjaOP720fCS6SFkRNomqNXQ0SlYDKrzeIDOWkd8n
         k5vx6Gi/3+zj2gENq9mF2dGqh05z6tJSgqnCoeBpmodoM1b6D6aiO4HRzdzO2LJoBBVZ
         MWIhM7v1MbLqtqrCAMYBLUx1qAEzGTa7O5nsndUHVGkfnuScmzh/xU1EYS33Yw0DPrfv
         Uy0coD4Z+zWAXsI1vx23ZEGVfoidZHFspeRs3La4GV1JbExr9TdWwWtkpDf+NNI6QSxj
         E5f/SateF01/lFlIYACtM2aL9ypiPnYBqycvs1qVZFO5ByR/3cVbskZ7BMvmLa2GdI0I
         plfw==
MIME-Version: 1.0
X-Received: by 10.194.78.141 with SMTP id b13mr7737947wjx.111.1403215986384;
 Thu, 19 Jun 2014 15:13:06 -0700 (PDT)
Received: by 10.194.177.138 with HTTP; Thu, 19 Jun 2014 15:13:06 -0700 (PDT)
Date: Thu, 19 Jun 2014 23:13:06 +0100
Message-ID: <CAGzJ1gRrCyu+d9y5Uvmz=Ey9qQxm5kfSkXf2dOxiPjpobGO0Bw@mail.gmail.com>
Subject: Rationale behind scala enumerations instead of sealed traits and case objects
From: =?UTF-8?Q?Luis_=C3=81ngel_Vicente_S=C3=A1nchez?= <langel.groups@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bfcfc8269653a04fc37ae27
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfcfc8269653a04fc37ae27
Content-Type: text/plain; charset=UTF-8

While I was trying to execute a job using spark-submit, I discover a
scala.MatchError at runtime... a DriverStateChanged.FAILED message was send
to an actor, and the match statement used was not taking that value into
account.

When I inspected that DriverStateChange.scala file I discovered that it's a
scala enumeration; if a sealed trait and case objects would have used, the
compiler would have refused to compile spark until that match statement
would have covered all possible values.

As a scala developer I prefer to catch that kind of errors at compile time
and I would like to understand why a scala enumeration has been used
instead of a sealed trait + case object. If changing that to a sealed (and
possible others) keeps binary compatibility, would that kind of PR be
welcomed?

Kind regards,

Luis

--047d7bfcfc8269653a04fc37ae27--

From dev-return-8079-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 20 19:04:43 2014
Return-Path: <dev-return-8079-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 50FB9110DA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 20 Jun 2014 19:04:43 +0000 (UTC)
Received: (qmail 60148 invoked by uid 500); 20 Jun 2014 19:04:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60090 invoked by uid 500); 20 Jun 2014 19:04:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60079 invoked by uid 99); 20 Jun 2014 19:04:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Jun 2014 19:04:42 +0000
X-ASF-Spam-Status: No, hits=-3.7 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_HELO_PASS,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of wibenton@redhat.com designates 209.132.183.24 as permitted sender)
Received: from [209.132.183.24] (HELO mx3-phx2.redhat.com) (209.132.183.24)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Jun 2014 19:04:37 +0000
Received: from zmail14.collab.prod.int.phx2.redhat.com (zmail14.collab.prod.int.phx2.redhat.com [10.5.83.16])
	by mx3-phx2.redhat.com (8.13.8/8.13.8) with ESMTP id s5KJ4GOt020149
	for <dev@spark.apache.org>; Fri, 20 Jun 2014 15:04:16 -0400
Date: Fri, 20 Jun 2014 15:04:16 -0400 (EDT)
From: Will Benton <willb@redhat.com>
To: dev@spark.apache.org
Message-ID: <641768824.37830898.1403291056624.JavaMail.zimbra@redhat.com>
In-Reply-To: <CALEZFQwLNKsTZa9GYpcrBVSjbH45gjp+Pn0NZVANONP4Sor+aQ@mail.gmail.com>
References: <1400258494709-6593.post@n3.nabble.com> <CAAsvFPk36+vXkZijCdZGxJHRSJcvPon3BopPNacNxXvDB=FF0Q@mail.gmail.com> <CAAsvFP=8e079xR0JrHEQ2tM56t-Ag2Lt8JqthUum0veMcEOd7g@mail.gmail.com> <CALEZFQwLNKsTZa9GYpcrBVSjbH45gjp+Pn0NZVANONP4Sor+aQ@mail.gmail.com>
Subject: Re: Scala examples for Spark do not work as written in
 documentation
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.12]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - FF29 (Mac)/8.0.6_GA_5922)
Thread-Topic: Scala examples for Spark do not work as written in documentation
Thread-Index: 29hSDfDKPa/bAOCAVPuiISIx/667kw==
X-Virus-Checked: Checked by ClamAV on apache.org

Hey, sorry to reanimate this thread, but just a quick question:  why do the examples (on http://spark.apache.org/examples.html) use "spark" for the SparkContext reference?  This is minor, but it seems like it could be a little confusing for people who want to run them in the shell and need to change "spark" to "sc".  (I noticed because this was a speedbump for a colleague who is trying out Spark.)


thanks,
wb

----- Original Message -----
> From: "Andy Konwinski" <andykonwinski@gmail.com>
> To: dev@spark.apache.org
> Sent: Tuesday, May 20, 2014 4:06:33 PM
> Subject: Re: Scala examples for Spark do not work as written in documentation
> 
> I fixed the bug, but I kept the parameter "i" instead of "_" since that (1)
> keeps it more parallel to the python and java versions which also use
> functions with a named variable and (2) doesn't require readers to know
> this particular use of the "_" syntax in Scala.
> 
> Thanks for catching this Glenn.
> 
> Andy
> 
> 
> On Fri, May 16, 2014 at 12:38 PM, Mark Hamstra
> <mark@clearstorydata.com>wrote:
> 
> > Sorry, looks like an extra line got inserted in there.  One more try:
> >
> > val count = spark.parallelize(1 to NUM_SAMPLES).map { _ =>
> >   val x = Math.random()
> >   val y = Math.random()
> >   if (x*x + y*y < 1) 1 else 0
> > }.reduce(_ + _)
> >
> >
> >
> > On Fri, May 16, 2014 at 12:36 PM, Mark Hamstra <mark@clearstorydata.com
> > >wrote:
> >
> > > Actually, the better way to write the multi-line closure would be:
> > >
> > > val count = spark.parallelize(1 to NUM_SAMPLES).map { _ =>
> > >
> > >   val x = Math.random()
> > >   val y = Math.random()
> > >   if (x*x + y*y < 1) 1 else 0
> > > }.reduce(_ + _)
> > >
> > >
> > > On Fri, May 16, 2014 at 9:41 AM, GlennStrycker <glenn.strycker@gmail.com
> > >wrote:
> > >
> > >> On the webpage http://spark.apache.org/examples.html, there is an
> > example
> > >> written as
> > >>
> > >> val count = spark.parallelize(1 to NUM_SAMPLES).map(i =>
> > >>   val x = Math.random()
> > >>   val y = Math.random()
> > >>   if (x*x + y*y < 1) 1 else 0
> > >> ).reduce(_ + _)
> > >> println("Pi is roughly " + 4.0 * count / NUM_SAMPLES)
> > >>
> > >> This does not execute in Spark, which gives me an error:
> > >> <console>:2: error: illegal start of simple expression
> > >>          val x = Math.random()
> > >>          ^
> > >>
> > >> If I rewrite the query slightly, adding in {}, it works:
> > >>
> > >> val count = spark.parallelize(1 to 10000).map(i =>
> > >>    {
> > >>    val x = Math.random()
> > >>    val y = Math.random()
> > >>    if (x*x + y*y < 1) 1 else 0
> > >>    }
> > >> ).reduce(_ + _)
> > >> println("Pi is roughly " + 4.0 * count / 10000.0)
> > >>
> > >>
> > >>
> > >>
> > >>
> > >> --
> > >> View this message in context:
> > >>
> > http://apache-spark-developers-list.1001551.n3.nabble.com/Scala-examples-for-Spark-do-not-work-as-written-in-documentation-tp6593.html
> > >> Sent from the Apache Spark Developers List mailing list archive at
> > >> Nabble.com.
> > >>
> > >
> > >
> >
> 

From dev-return-8080-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 20 22:02:17 2014
Return-Path: <dev-return-8080-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 037E2116D6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 20 Jun 2014 22:02:17 +0000 (UTC)
Received: (qmail 55961 invoked by uid 500); 20 Jun 2014 22:02:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55902 invoked by uid 500); 20 Jun 2014 22:02:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55890 invoked by uid 99); 20 Jun 2014 22:02:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Jun 2014 22:02:16 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.42 as permitted sender)
Received: from [209.85.219.42] (HELO mail-oa0-f42.google.com) (209.85.219.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Jun 2014 22:02:13 +0000
Received: by mail-oa0-f42.google.com with SMTP id eb12so8077538oac.1
        for <dev@spark.apache.org>; Fri, 20 Jun 2014 15:01:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type:content-transfer-encoding;
        bh=xTpgOtGPIUkWiaUoos8vUzykgNgqxydvvAyYvQY9Mk0=;
        b=V1w5LsQ38JGAZ9cpO5Wv2KfaFXKHy10s8FtUMGkhrFJNH524X/uPjAI6DeF9ak6tpm
         rQKIxfcgJ1O+p+W9rblmrkdtm8zdQ8yMz1+jw+naPXMqtI/CrEmQolfZsykYTKU90c7P
         VOjC41o8tLty81uJyMppEfuMxlCrrw9aXtJVRXjr6IahpZH2oMht8de35NekdHyDz1GI
         uo4Z02iwqDB4R+krYg7SLBlO0phhAklQyZi9Dzzh9wcqv/j+l2IxNmS4hUVfXZ7QMO6/
         dygeMq7cNT1OaFe6FqJ9qQVVUqhIRkFxI9OvCoSnclohbDy2pvjz+qKwsJXXi3He1HE1
         Yxgg==
MIME-Version: 1.0
X-Received: by 10.182.251.170 with SMTP id zl10mr6273196obc.5.1403301709233;
 Fri, 20 Jun 2014 15:01:49 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Fri, 20 Jun 2014 15:01:49 -0700 (PDT)
In-Reply-To: <641768824.37830898.1403291056624.JavaMail.zimbra@redhat.com>
References: <1400258494709-6593.post@n3.nabble.com>
	<CAAsvFPk36+vXkZijCdZGxJHRSJcvPon3BopPNacNxXvDB=FF0Q@mail.gmail.com>
	<CAAsvFP=8e079xR0JrHEQ2tM56t-Ag2Lt8JqthUum0veMcEOd7g@mail.gmail.com>
	<CALEZFQwLNKsTZa9GYpcrBVSjbH45gjp+Pn0NZVANONP4Sor+aQ@mail.gmail.com>
	<641768824.37830898.1403291056624.JavaMail.zimbra@redhat.com>
Date: Fri, 20 Jun 2014 15:01:49 -0700
Message-ID: <CABPQxsu8R=_kZ7_cF=ZvY_mg0oOWsBu=MTqefUgfDTturPEPvQ@mail.gmail.com>
Subject: Re: Scala examples for Spark do not work as written in documentation
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Those are pretty old - but I think the reason Matei did that was to
make it less confusing for brand new users. `spark` is actually a
valid identifier because it's just a variable name (val spark =3D new
SparkContext()) but I agree this could be confusing for users who want
to drop into the shell.

On Fri, Jun 20, 2014 at 12:04 PM, Will Benton <willb@redhat.com> wrote:
> Hey, sorry to reanimate this thread, but just a quick question:  why do t=
he examples (on http://spark.apache.org/examples.html) use "spark" for the =
SparkContext reference?  This is minor, but it seems like it could be a lit=
tle confusing for people who want to run them in the shell and need to chan=
ge "spark" to "sc".  (I noticed because this was a speedbump for a colleagu=
e who is trying out Spark.)
>
>
> thanks,
> wb
>
> ----- Original Message -----
>> From: "Andy Konwinski" <andykonwinski@gmail.com>
>> To: dev@spark.apache.org
>> Sent: Tuesday, May 20, 2014 4:06:33 PM
>> Subject: Re: Scala examples for Spark do not work as written in document=
ation
>>
>> I fixed the bug, but I kept the parameter "i" instead of "_" since that =
(1)
>> keeps it more parallel to the python and java versions which also use
>> functions with a named variable and (2) doesn't require readers to know
>> this particular use of the "_" syntax in Scala.
>>
>> Thanks for catching this Glenn.
>>
>> Andy
>>
>>
>> On Fri, May 16, 2014 at 12:38 PM, Mark Hamstra
>> <mark@clearstorydata.com>wrote:
>>
>> > Sorry, looks like an extra line got inserted in there.  One more try:
>> >
>> > val count =3D spark.parallelize(1 to NUM_SAMPLES).map { _ =3D>
>> >   val x =3D Math.random()
>> >   val y =3D Math.random()
>> >   if (x*x + y*y < 1) 1 else 0
>> > }.reduce(_ + _)
>> >
>> >
>> >
>> > On Fri, May 16, 2014 at 12:36 PM, Mark Hamstra <mark@clearstorydata.co=
m
>> > >wrote:
>> >
>> > > Actually, the better way to write the multi-line closure would be:
>> > >
>> > > val count =3D spark.parallelize(1 to NUM_SAMPLES).map { _ =3D>
>> > >
>> > >   val x =3D Math.random()
>> > >   val y =3D Math.random()
>> > >   if (x*x + y*y < 1) 1 else 0
>> > > }.reduce(_ + _)
>> > >
>> > >
>> > > On Fri, May 16, 2014 at 9:41 AM, GlennStrycker <glenn.strycker@gmail=
.com
>> > >wrote:
>> > >
>> > >> On the webpage http://spark.apache.org/examples.html, there is an
>> > example
>> > >> written as
>> > >>
>> > >> val count =3D spark.parallelize(1 to NUM_SAMPLES).map(i =3D>
>> > >>   val x =3D Math.random()
>> > >>   val y =3D Math.random()
>> > >>   if (x*x + y*y < 1) 1 else 0
>> > >> ).reduce(_ + _)
>> > >> println("Pi is roughly " + 4.0 * count / NUM_SAMPLES)
>> > >>
>> > >> This does not execute in Spark, which gives me an error:
>> > >> <console>:2: error: illegal start of simple expression
>> > >>          val x =3D Math.random()
>> > >>          ^
>> > >>
>> > >> If I rewrite the query slightly, adding in {}, it works:
>> > >>
>> > >> val count =3D spark.parallelize(1 to 10000).map(i =3D>
>> > >>    {
>> > >>    val x =3D Math.random()
>> > >>    val y =3D Math.random()
>> > >>    if (x*x + y*y < 1) 1 else 0
>> > >>    }
>> > >> ).reduce(_ + _)
>> > >> println("Pi is roughly " + 4.0 * count / 10000.0)
>> > >>
>> > >>
>> > >>
>> > >>
>> > >>
>> > >> --
>> > >> View this message in context:
>> > >>
>> > http://apache-spark-developers-list.1001551.n3.nabble.com/Scala-exampl=
es-for-Spark-do-not-work-as-written-in-documentation-tp6593.html
>> > >> Sent from the Apache Spark Developers List mailing list archive at
>> > >> Nabble.com.
>> > >>
>> > >
>> > >
>> >
>>

From dev-return-8081-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jun 21 00:48:26 2014
Return-Path: <dev-return-8081-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3311511B84
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 21 Jun 2014 00:48:26 +0000 (UTC)
Received: (qmail 26512 invoked by uid 500); 21 Jun 2014 00:48:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26441 invoked by uid 500); 21 Jun 2014 00:48:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26429 invoked by uid 99); 21 Jun 2014 00:48:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Jun 2014 00:48:25 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of liquanpei@gmail.com designates 74.125.82.180 as permitted sender)
Received: from [74.125.82.180] (HELO mail-we0-f180.google.com) (74.125.82.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Jun 2014 00:48:22 +0000
Received: by mail-we0-f180.google.com with SMTP id x48so4479880wes.39
        for <dev@spark.apache.org>; Fri, 20 Jun 2014 17:47:58 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=XltkaIi/hGktVBLS1bcny3L3YWrN6jABt4t1yqqLfLI=;
        b=nARnbM1slZpN9+q5bk8S2WGfwWvEO+IMMjiGemd5YnR7rUKSsoeiwKZWpHggYJCbFz
         YhxIu/LCylDT/jW1UH541sveHxdXi6dKZ3qiALICxsfnqao0W6rtAbOfU/FRWAZqDyVj
         WZyQ+l4a5wO29CuzGltCUeHWKuA19ESq9jfNSyRwHdHtmRoNEOkwIH6ck8LuwjPKGPUQ
         A+CRAVuMTM0NjhlVyAiI495VHq7Yif4Khja4kZgCFvGqXVRrRYyv4hTptLUTJsSxnsK7
         taAFuHuUpycKKynzPxZgvYORva+KoJAgr+2CrnF+CwnLR76LRZGze6U5En7j8MX48cWb
         KPFg==
MIME-Version: 1.0
X-Received: by 10.194.80.7 with SMTP id n7mr8200021wjx.8.1403311678535; Fri,
 20 Jun 2014 17:47:58 -0700 (PDT)
Received: by 10.180.96.129 with HTTP; Fri, 20 Jun 2014 17:47:58 -0700 (PDT)
Date: Fri, 20 Jun 2014 17:47:58 -0700
Message-ID: <CAJmC80_8YR6v=OhZpkEf+C8nNePt8z6LubvpOkMJ2=9ZBCn7Hg@mail.gmail.com>
Subject: Current status of Sparrow
From: Liquan Pei <liquanpei@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7beb9c801bbfeb04fc4df655
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7beb9c801bbfeb04fc4df655
Content-Type: text/plain; charset=UTF-8

Hi

What is the current status of Sparrow integration with Spark? I would like
to integrate Sparrow with Spark 1.0 on a 100 node cluster. Any suggestions?

Thanks a lot for your help!
Liquan

--047d7beb9c801bbfeb04fc4df655--

From dev-return-8082-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jun 21 20:53:03 2014
Return-Path: <dev-return-8082-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DBBDB11B8D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 21 Jun 2014 20:53:03 +0000 (UTC)
Received: (qmail 31759 invoked by uid 500); 21 Jun 2014 20:53:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31698 invoked by uid 500); 21 Jun 2014 20:53:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 55141 invoked by uid 99); 21 Jun 2014 14:52:58 -0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FROM_EXCESS_BASE64,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_SOFTFAIL
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of guxiaobo1982@qq.com does not designate 54.206.16.166 as permitted sender)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=qq.com; s=s201307;
	t=1403362344; bh=ILUvhSGYbL+EUoYUkaSq7dkD0Mb6LBc3dkXdVHwQgIg=;
	h=X-QQ-SSF:X-HAS-ATTACH:X-QQ-BUSINESS-ORIGIN:X-Originating-IP:
	 X-QQ-STYLE:X-QQ-mid:From:To:Subject:Mime-Version:Content-Type:Content-Transfer-Encoding:Date:
	 X-Priority:Message-ID:X-QQ-MIME:X-Mailer:X-QQ-Mailer:
	 X-QQ-SENDSIZE;
	b=MQ6wm8TV3uPzwxDnkssLd11mn/tDqFOOe67KS0F73RY0/98r1FqHTMzmsVN2QQQrJ
	 dHjLVWO5dfI2lo8oRLZXC3eXcbz51Kix+3yuZDbDuE9idQJAqm6/6FcisP0gWIwS1b
	 +JHT6miq0JUgTIXftC8DaMH6gp+gXaINKvyxdEFM=
X-QQ-SSF: 000000000000005000000000000000U
X-HAS-ATTACH: no
X-QQ-BUSINESS-ORIGIN: 2
X-Originating-IP: 58.60.42.219
X-QQ-STYLE: 
X-QQ-mid: webenglish5t1403362342t966888
From: "=?utf-8?B?Z3V4aWFvYm8xOTgy?=" <guxiaobo1982@qq.com>
To: "=?utf-8?B?ZGV2?=" <dev@spark.incubator.apache.org>
Subject: What about a general schema registration method for JavaSchemaRDD?
Mime-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_53A59C26_D4478E20_49113362"
Content-Transfer-Encoding: 8Bit
Date: Sat, 21 Jun 2014 22:52:22 +0800
X-Priority: 3
Message-ID: <tencent_5E84A024470691FD14BE3264@qq.com>
X-QQ-MIME: TCMime 1.0 by Tencent
X-Mailer: QQMail 2.x
X-QQ-Mailer: QQMail 2.x
X-QQ-SENDSIZE: 520
X-QQ-Bgrelay: 1
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_53A59C26_D4478E20_49113362
Content-Type: text/plain;
	charset="utf-8"
Content-Transfer-Encoding: base64
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------=_NextPart_53A59C26_D4478E20_49113362--




From dev-return-8083-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jun 21 21:34:46 2014
Return-Path: <dev-return-8083-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E065A11C5D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 21 Jun 2014 21:34:46 +0000 (UTC)
Received: (qmail 56600 invoked by uid 500); 21 Jun 2014 21:34:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 56535 invoked by uid 500); 21 Jun 2014 21:34:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56521 invoked by uid 99); 21 Jun 2014 21:34:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Jun 2014 21:34:45 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.46] (HELO mail-qg0-f46.google.com) (209.85.192.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Jun 2014 21:34:41 +0000
Received: by mail-qg0-f46.google.com with SMTP id q107so4673129qgd.33
        for <dev@spark.apache.org>; Sat, 21 Jun 2014 14:34:21 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=RXuyo/LAIQ0hUBbf5BxEb2+DNBBCF6D1LXISMvY6Y0E=;
        b=QIexJWJzOHbrffBv2KlFXGe0IlrdcMOQT/B+Uw2O/CNJJaSYEHzenFBPoJrmYp6kDf
         uDwSlgFTb4EIgxuj+XI+4AmD8sAMjPcd4v1NebdsnafF1vHOYWfJQA+aThJyZuuYYA5s
         aGIEY12hhjkd9emQZpdsTzpyZqSkakCwKnqxoym7BUGJQ11puSx1dDssxOhW2Ij4qaxS
         RsMGc3l1aRZ9lg/a2aDhpZEM2PyoWk3RV1Hb/dKn1utoRWvuauEP/pmvRqQl5Wk/Ozp/
         +IGHfCDkXS5SqPbd+LFvX6A1V3bCY0bPZpWEp/bVNUHJ8R8QZgQQ18y1yO+nQtMlNgNe
         z18g==
X-Gm-Message-State: ALoCoQlxXvEi3jRpJxvd7/a9Wp8/NknrA5qDA5T5RN7Bxri3w8e44M7FPldIBCf4mx1HKbwpEakM
X-Received: by 10.140.26.179 with SMTP id 48mr17449376qgv.51.1403386460935;
 Sat, 21 Jun 2014 14:34:20 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Sat, 21 Jun 2014 14:34:00 -0700 (PDT)
In-Reply-To: <tencent_5E84A024470691FD14BE3264@qq.com>
References: <tencent_5E84A024470691FD14BE3264@qq.com>
From: Reynold Xin <rxin@databricks.com>
Date: Sat, 21 Jun 2014 14:34:00 -0700
Message-ID: <CAPh_B=a7oq616YTc+TsME1dHTZQG+FYU+0kF-CFMZ3rEM5fX0g@mail.gmail.com>
Subject: Re: What about a general schema registration method for JavaSchemaRDD?
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c035f47cae6704fc5f5fd2
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c035f47cae6704fc5f5fd2
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Thanks for the message.


There is an open issue about the public type / schema system that is
related to this topic: https://issues.apache.org/jira/browse/SPARK-2179

You probably want to comment on that ticket as well.



On Sat, Jun 21, 2014 at 7:52 AM, guxiaobo1982 <guxiaobo1982@qq.com> wrote:

> Hi ,
>
> The current implementation of JavaSchemaRDD need a special JavaBean class
> to define schema information for tables, but when developing applications
> using the Spark SQL API, table is a more dynamic component, the awkward
> thing is when new tables are defined, we must create a new JavaBean, and
> redeploy the whole application. So here comes an idea regarding a more
> general schema registration method,
>
>
>
> Step1: Defile a new Java class named RowSchema in API to define column
> information, column name and data type are most important ones.
>
>
>
> Step2: the actual data is store just as JavaRDD<Row>;
>
>
>
> Step3:When loading data into JavaRDD<Row>, the API provides a general map
> function, which takes a RowSchema object as parameter, to map each line t=
o
> a Row object.
>
>
>
> Step4: Add a new applySchema method , which takes a RowSchema object as
> parameter, to the JavaSQLContext class ,
>
>
>
> Step 5: The registerAsTable and all other SQL releated methods of
> JavaSQLContext class should take care of the difference of defining schem=
a
> throw JavaBean and RowSchemas.(That=E2=80=99s the work of the API layer)
>
>
>
> The API is something like this:
>
>
>
> Public Class RowSchema{
>
> Public RowSchema(List<String> colNames, List<String> colDataTypes);
>
> Public String getColName(integer i);//return column string of column I;
>
> Public integer getColDataType(integer i);//return data type of column I;
>
> Public integer getColNumber();// return number of columns
>
>
>
> };
>
> RowSchema rs =3D new RowSchema(=E2=80=A6=E2=80=A6);
>
>
>
> JavaRDD<Row> table =3D ctx.textFile(=E2=80=9Cfile path=E2=80=9D).map(rs);
>
>
>
> JavaSchemaRDD schemaPeople =3D sqlCtx.applySchema(table, rs);
>
> schemaPeople.registerAsTable("people");
>
> Regards,
>
>
>  Xiaobo Gu

--001a11c035f47cae6704fc5f5fd2--

From dev-return-8084-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun 22 07:25:16 2014
Return-Path: <dev-return-8084-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 53301113C3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 22 Jun 2014 07:25:16 +0000 (UTC)
Received: (qmail 86632 invoked by uid 500); 22 Jun 2014 07:25:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86575 invoked by uid 500); 22 Jun 2014 07:25:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86563 invoked by uid 99); 22 Jun 2014 07:25:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 22 Jun 2014 07:25:15 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.44 as permitted sender)
Received: from [209.85.219.44] (HELO mail-oa0-f44.google.com) (209.85.219.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 22 Jun 2014 07:25:11 +0000
Received: by mail-oa0-f44.google.com with SMTP id i7so9077676oag.17
        for <dev@spark.apache.org>; Sun, 22 Jun 2014 00:24:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=sREKh0NM8C5kvNAUBYfuvJ4VDhmVpc+8mScw3cugOyk=;
        b=za9jlxcP0jVJTrqy2PLDo6fKQZQfdDtOEx0HzIZn3SgciK9O/WGzdqNHWmFf88LJbN
         +YPgxIHrbA9iuvz/31eIHgwE3xv5k6HCTUDLbLN5kBRFkLxSkq+ED232hRxEsVWA/LuC
         PtkUhCTR6RgslwPFEHk+chHpL8kzHVVbcNlM0216qbA2RsYYo2aZdeXaZCDSjk81tSbo
         h4R0YhtGVbgyZTwHlnl0bHZFx0f0O9YRw8ldDo/fKFBOQXvBvlUQ0V9sOA10VhwomtQa
         cqHofQSIOBaNj59Fug0iY17SKXHSg7Qgq12wS1vWq9i4WtnWOxgZ1lJ4b+y653qko5Dn
         IRgQ==
MIME-Version: 1.0
X-Received: by 10.182.60.4 with SMTP id d4mr14463314obr.4.1403421890391; Sun,
 22 Jun 2014 00:24:50 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Sun, 22 Jun 2014 00:24:50 -0700 (PDT)
Date: Sun, 22 Jun 2014 00:24:50 -0700
Message-ID: <CABPQxsvUMnvU7ZLkGArVyTTV0xo-Vfmk5NAcpEJhxXJKcp5a+A@mail.gmail.com>
Subject: Assorted project updates (tests, build, etc)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey All,

1. The original test infrastructure hosted by the AMPLab has been
fully restored and also expanded with many more executor slots for
tests. Thanks to Matt Massie at the Amplab for helping with this.

2. We now have a nightly build matrix across different Hadoop
versions. It appears that the Maven build is failing tests with some
of the newer Hadoop versions. If people from the community are
interested, diagnosing and fixing test issues would be welcome patches
(they are all dependency related).

https://issues.apache.org/jira/browse/SPARK-2232

3. Prashant Sharma has spent a lot of time to make it possible for our
sbt build to read dependencies from Maven. This will save us a huge
amount of headache keeping the builds consistent. I just wanted to
give a heads up to users about this - we should retain compatibility
with features of the sbt build, but if you are e.g. hooking into deep
internals of our build it may affect you. I'm hoping this can be
updated and merged in the next week:

https://github.com/apache/spark/pull/77

4. We've moved most of the documentation over to recommending users
build with Maven when creating official packages. This is just to
provide a single "reference build" of Spark since it's the one we test
and package for releases, we make sure all recursive dependencies are
correct, etc. I'd recommend that all downstream packagers use this
build.

For day-to-day development I imagine sbt will remain more popular
(repl, incremental builds, etc). Prashant's work allows us to get the
"best of both worlds" which is great.

- Patrick

From dev-return-8085-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun 22 17:27:30 2014
Return-Path: <dev-return-8085-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4DC6811B50
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 22 Jun 2014 17:27:30 +0000 (UTC)
Received: (qmail 89415 invoked by uid 500); 22 Jun 2014 17:27:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89347 invoked by uid 500); 22 Jun 2014 17:27:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89335 invoked by uid 99); 22 Jun 2014 17:27:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 22 Jun 2014 17:27:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mark@clearstorydata.com designates 74.125.82.173 as permitted sender)
Received: from [74.125.82.173] (HELO mail-we0-f173.google.com) (74.125.82.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 22 Jun 2014 17:27:25 +0000
Received: by mail-we0-f173.google.com with SMTP id t60so5754562wes.4
        for <dev@spark.apache.org>; Sun, 22 Jun 2014 10:27:03 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=lpNBbQyF1qelDtypBoSJyEbSEel+vSVqEIREqce7iV8=;
        b=PrZWrFI/M6DlBQ75YE0j+3yFZeU+EY7SoGo2kZO3NfUzVL/z1Nkp5HQ+YiZl/ecsFy
         DhmEw57dLTH2fCivIAqGXBHgSvwPV6z8HgpDUaXVm7rvuqshsQu8crH1sJmoaiNkfTEB
         2o1SDxvcr5dmssTEuPK3btjenNK9G3GIzUCWUej6J7675JEojeanpxdEoSytnizSMHf/
         c22RgvIpqHLVWQE2CXxv43Ktt2XV6Pg8VwgVpoUCB5HHt19+BM40hYz/ZaMZ5/qV3sGF
         LGHyxZpMhxHWxAbrqm4PLcXb17WhuUo07KfogGoOJi/uveyEH/gksTyGWtV/YdgxU2D1
         TSIQ==
X-Gm-Message-State: ALoCoQmAkChXaoLDgSYnn/zxE5iUVtbz5NkdiNoxaL5CspcsYMNJquwi4XC03osyL4W7jXoeg6OM
MIME-Version: 1.0
X-Received: by 10.194.78.141 with SMTP id b13mr11535994wjx.111.1403458023068;
 Sun, 22 Jun 2014 10:27:03 -0700 (PDT)
Received: by 10.216.161.68 with HTTP; Sun, 22 Jun 2014 10:27:03 -0700 (PDT)
In-Reply-To: <CABPQxsvUMnvU7ZLkGArVyTTV0xo-Vfmk5NAcpEJhxXJKcp5a+A@mail.gmail.com>
References: <CABPQxsvUMnvU7ZLkGArVyTTV0xo-Vfmk5NAcpEJhxXJKcp5a+A@mail.gmail.com>
Date: Sun, 22 Jun 2014 10:27:03 -0700
Message-ID: <CAAsvFP=OS7L8O5wxThHpVNdPtmViySk2rJ3w=uxivHiTyGL54A@mail.gmail.com>
Subject: Re: Assorted project updates (tests, build, etc)
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bfcfc82ec319f04fc700829
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfcfc82ec319f04fc700829
Content-Type: text/plain; charset=UTF-8

Just a couple of FYI notes: With Zinc and the scala-maven-plugin, repl and
incremental builds are also available to those doing day-to-day development
using Maven.  As long as you don't have to delve into the extra boilerplate
and verbosity of Maven's POMs relative to an SBT build file, there is
little day-to-day functional difference between the two -- if anything, I
find that Maven supports faster development cycles.


On Sun, Jun 22, 2014 at 12:24 AM, Patrick Wendell <pwendell@gmail.com>
wrote:

> Hey All,
>
> 1. The original test infrastructure hosted by the AMPLab has been
> fully restored and also expanded with many more executor slots for
> tests. Thanks to Matt Massie at the Amplab for helping with this.
>
> 2. We now have a nightly build matrix across different Hadoop
> versions. It appears that the Maven build is failing tests with some
> of the newer Hadoop versions. If people from the community are
> interested, diagnosing and fixing test issues would be welcome patches
> (they are all dependency related).
>
> https://issues.apache.org/jira/browse/SPARK-2232
>
> 3. Prashant Sharma has spent a lot of time to make it possible for our
> sbt build to read dependencies from Maven. This will save us a huge
> amount of headache keeping the builds consistent. I just wanted to
> give a heads up to users about this - we should retain compatibility
> with features of the sbt build, but if you are e.g. hooking into deep
> internals of our build it may affect you. I'm hoping this can be
> updated and merged in the next week:
>
> https://github.com/apache/spark/pull/77
>
> 4. We've moved most of the documentation over to recommending users
> build with Maven when creating official packages. This is just to
> provide a single "reference build" of Spark since it's the one we test
> and package for releases, we make sure all recursive dependencies are
> correct, etc. I'd recommend that all downstream packagers use this
> build.
>
> For day-to-day development I imagine sbt will remain more popular
> (repl, incremental builds, etc). Prashant's work allows us to get the
> "best of both worlds" which is great.
>
> - Patrick
>

--047d7bfcfc82ec319f04fc700829--

From dev-return-8086-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun 22 20:51:45 2014
Return-Path: <dev-return-8086-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3C1D111EB3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 22 Jun 2014 20:51:45 +0000 (UTC)
Received: (qmail 51045 invoked by uid 500); 22 Jun 2014 20:51:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50973 invoked by uid 500); 22 Jun 2014 20:51:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50962 invoked by uid 99); 22 Jun 2014 20:51:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 22 Jun 2014 20:51:44 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: unknown (athena.apache.org: error in processing during lookup of bshi@nd.edu)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 22 Jun 2014 20:51:39 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <bshi@nd.edu>)
	id 1WyojO-0005ba-IF
	for dev@spark.incubator.apache.org; Sun, 22 Jun 2014 13:51:18 -0700
Date: Sun, 22 Jun 2014 13:51:18 -0700 (PDT)
From: dash <bshi@nd.edu>
To: dev@spark.incubator.apache.org
Message-ID: <1403470278528-7065.post@n3.nabble.com>
Subject: GraphX's VertexRDD can not be materialized by calling count()
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi there,

Seems one can not materialize VertexRDD by simply calling count method,
which is overridden by VertexRDD. But if you call RDD's count, it could
materialize it.

Is this a feature that designed to get the count without materialize
VertexRDD? 

If so, do you guys think it is necessary to add a materialize method to
VertexRDD? I did that and ready to send a pull request, but I just want to
make sure this is reasonable. 

By the way, does count() is the cheapest way to materialize a RDD? Or it
just cost the same resources like other actions?

Best, 



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/GraphX-s-VertexRDD-can-not-be-materialized-by-calling-count-tp7065.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8087-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun 22 22:15:13 2014
Return-Path: <dev-return-8087-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 88FDA11FF0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 22 Jun 2014 22:15:13 +0000 (UTC)
Received: (qmail 14325 invoked by uid 500); 22 Jun 2014 22:15:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14269 invoked by uid 500); 22 Jun 2014 22:15:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14258 invoked by uid 99); 22 Jun 2014 22:15:12 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 22 Jun 2014 22:15:12 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: unknown (athena.apache.org: error in processing during lookup of bshi@nd.edu)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 22 Jun 2014 22:15:08 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <bshi@nd.edu>)
	id 1Wyq2A-0002O7-9v
	for dev@spark.incubator.apache.org; Sun, 22 Jun 2014 15:14:46 -0700
Date: Sun, 22 Jun 2014 15:14:46 -0700 (PDT)
From: dash <bshi@nd.edu>
To: dev@spark.incubator.apache.org
Message-ID: <1403475286251-7066.post@n3.nabble.com>
Subject: Checkpointed RDD still causing StackOverflow
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

I'm doing iterative computing now, and due to lineage chain, we need to
checkpoint the RDD in order to cut off lineage and prevent StackOverflow
error. 

The following code still having StackOverflowError, I checked
`isCheckpointed` and the result is true. Also, I write a function to count
the lineage, but the lineage is not big. Any idea about that? Please give me
some hit so I can dig into the source code and try to fix it.




Best,



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Checkpointed-RDD-still-causing-StackOverflow-tp7066.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8088-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 23 00:21:25 2014
Return-Path: <dev-return-8088-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 573B3111E1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Jun 2014 00:21:25 +0000 (UTC)
Received: (qmail 99447 invoked by uid 500); 23 Jun 2014 00:21:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99385 invoked by uid 500); 23 Jun 2014 00:21:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99352 invoked by uid 99); 23 Jun 2014 00:21:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Jun 2014 00:21:24 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 74.125.82.179 as permitted sender)
Received: from [74.125.82.179] (HELO mail-we0-f179.google.com) (74.125.82.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Jun 2014 00:21:20 +0000
Received: by mail-we0-f179.google.com with SMTP id w62so5945640wes.24
        for <dev@spark.apache.org>; Sun, 22 Jun 2014 17:20:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=z138vbMG8t71/3qGvkNkhBeH03RPzRIhe5ZgBKtnxsQ=;
        b=kzm91KUGTVuvwrIq55uW/8uiUiEVXjPqS9KTXAGO8waTMd1JFD51t9vsH3wqXY+OBf
         p8gsXpj1rvkthibjdTyyHtX5+XzSqZTJmwGW64c1Koql/f+Opi++2OsorMfAnfq7J2X9
         TxS4Tjtl7xwSqoSim/ddUKD+Ie4mmXAxEOTcKP/jSuvaQKyccjUjW3abeeRH3WEsrNH8
         xvDZDak6xqMJA1TzuP+1fVYfruVqlmnEnCzdkcDEAx8HRoQ750UTGVZJF6FbGCmHK/9i
         YvFc+rkFwkuSsdg/4X5fzAbfEhm1FTxY2ej33HlH0KauFUhbk2qezROIsOkPfQMLHbNb
         NSaA==
MIME-Version: 1.0
X-Received: by 10.180.99.136 with SMTP id eq8mr21563562wib.45.1403482859095;
 Sun, 22 Jun 2014 17:20:59 -0700 (PDT)
Received: by 10.194.169.234 with HTTP; Sun, 22 Jun 2014 17:20:59 -0700 (PDT)
In-Reply-To: <1403475286251-7066.post@n3.nabble.com>
References: <1403475286251-7066.post@n3.nabble.com>
Date: Sun, 22 Jun 2014 17:20:59 -0700
Message-ID: <CAJgQjQ_LHXictis2ynDsU24mh3D0GhfuMyGB2o4_Ni4_pH6+XA@mail.gmail.com>
Subject: Re: Checkpointed RDD still causing StackOverflow
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

After checkpoint(), please call count(). This is similar to cache(),
the RDD is only marked as to be checked with checkpoint(). -Xiangrui

On Sun, Jun 22, 2014 at 3:14 PM, dash <bshi@nd.edu> wrote:
> Hi,
>
> I'm doing iterative computing now, and due to lineage chain, we need to
> checkpoint the RDD in order to cut off lineage and prevent StackOverflow
> error.
>
> The following code still having StackOverflowError, I checked
> `isCheckpointed` and the result is true. Also, I write a function to count
> the lineage, but the lineage is not big. Any idea about that? Please give me
> some hit so I can dig into the source code and try to fix it.
>
>
>
>
> Best,
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Checkpointed-RDD-still-causing-StackOverflow-tp7066.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8089-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 23 00:21:26 2014
Return-Path: <dev-return-8089-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 09CA8111E2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Jun 2014 00:21:26 +0000 (UTC)
Received: (qmail 386 invoked by uid 500); 23 Jun 2014 00:21:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 282 invoked by uid 500); 23 Jun 2014 00:21:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99372 invoked by uid 99); 23 Jun 2014 00:21:24 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Jun 2014 00:21:24 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.212.171 as permitted sender)
Received: from [209.85.212.171] (HELO mail-wi0-f171.google.com) (209.85.212.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Jun 2014 00:21:23 +0000
Received: by mail-wi0-f171.google.com with SMTP id n15so3238325wiw.16
        for <dev@spark.incubator.apache.org>; Sun, 22 Jun 2014 17:20:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=z138vbMG8t71/3qGvkNkhBeH03RPzRIhe5ZgBKtnxsQ=;
        b=kzm91KUGTVuvwrIq55uW/8uiUiEVXjPqS9KTXAGO8waTMd1JFD51t9vsH3wqXY+OBf
         p8gsXpj1rvkthibjdTyyHtX5+XzSqZTJmwGW64c1Koql/f+Opi++2OsorMfAnfq7J2X9
         TxS4Tjtl7xwSqoSim/ddUKD+Ie4mmXAxEOTcKP/jSuvaQKyccjUjW3abeeRH3WEsrNH8
         xvDZDak6xqMJA1TzuP+1fVYfruVqlmnEnCzdkcDEAx8HRoQ750UTGVZJF6FbGCmHK/9i
         YvFc+rkFwkuSsdg/4X5fzAbfEhm1FTxY2ej33HlH0KauFUhbk2qezROIsOkPfQMLHbNb
         NSaA==
MIME-Version: 1.0
X-Received: by 10.180.99.136 with SMTP id eq8mr21563562wib.45.1403482859095;
 Sun, 22 Jun 2014 17:20:59 -0700 (PDT)
Received: by 10.194.169.234 with HTTP; Sun, 22 Jun 2014 17:20:59 -0700 (PDT)
In-Reply-To: <1403475286251-7066.post@n3.nabble.com>
References: <1403475286251-7066.post@n3.nabble.com>
Date: Sun, 22 Jun 2014 17:20:59 -0700
Message-ID: <CAJgQjQ_LHXictis2ynDsU24mh3D0GhfuMyGB2o4_Ni4_pH6+XA@mail.gmail.com>
Subject: Re: Checkpointed RDD still causing StackOverflow
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

After checkpoint(), please call count(). This is similar to cache(),
the RDD is only marked as to be checked with checkpoint(). -Xiangrui

On Sun, Jun 22, 2014 at 3:14 PM, dash <bshi@nd.edu> wrote:
> Hi,
>
> I'm doing iterative computing now, and due to lineage chain, we need to
> checkpoint the RDD in order to cut off lineage and prevent StackOverflow
> error.
>
> The following code still having StackOverflowError, I checked
> `isCheckpointed` and the result is true. Also, I write a function to count
> the lineage, but the lineage is not big. Any idea about that? Please give me
> some hit so I can dig into the source code and try to fix it.
>
>
>
>
> Best,
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Checkpointed-RDD-still-causing-StackOverflow-tp7066.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8090-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 23 01:37:39 2014
Return-Path: <dev-return-8090-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 89ECD112AE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Jun 2014 01:37:39 +0000 (UTC)
Received: (qmail 39017 invoked by uid 500); 23 Jun 2014 01:37:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38948 invoked by uid 500); 23 Jun 2014 01:37:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38937 invoked by uid 99); 23 Jun 2014 01:37:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Jun 2014 01:37:38 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: unknown (athena.apache.org: error in processing during lookup of bshi@nd.edu)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Jun 2014 01:37:34 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <bshi@nd.edu>)
	id 1WytC4-0007Ze-6Q
	for dev@spark.incubator.apache.org; Sun, 22 Jun 2014 18:37:12 -0700
Date: Sun, 22 Jun 2014 18:37:12 -0700 (PDT)
From: dash <bshi@nd.edu>
To: dev@spark.incubator.apache.org
Message-ID: <1403487432188-7068.post@n3.nabble.com>
In-Reply-To: <CAJgQjQ_LHXictis2ynDsU24mh3D0GhfuMyGB2o4_Ni4_pH6+XA@mail.gmail.com>
References: <1403475286251-7066.post@n3.nabble.com> <CAJgQjQ_LHXictis2ynDsU24mh3D0GhfuMyGB2o4_Ni4_pH6+XA@mail.gmail.com>
Subject: Re: Checkpointed RDD still causing StackOverflow
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Xiangrui,

According to my knowledge, calling count is for materialize the RDD, does
collect do the same thing since it also an action? I can not call count
because for a Graph object, count does not materialize the RDD. I already
send an issue on that.

My question is, why there still have stack overflow even if `isCheckpointed`
is true?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Checkpointed-RDD-still-causing-StackOverflow-tp7066p7068.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8091-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 23 17:28:12 2014
Return-Path: <dev-return-8091-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F002A1108F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Jun 2014 17:28:11 +0000 (UTC)
Received: (qmail 91783 invoked by uid 500); 23 Jun 2014 17:28:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91731 invoked by uid 500); 23 Jun 2014 17:28:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91720 invoked by uid 99); 23 Jun 2014 17:28:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Jun 2014 17:28:11 +0000
X-ASF-Spam-Status: No, hits=0.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_SOFTFAIL
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of keo@eecs.berkeley.edu does not designate 169.229.218.143 as permitted sender)
Received: from [169.229.218.143] (HELO cm02fe.IST.Berkeley.EDU) (169.229.218.143)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Jun 2014 17:28:06 +0000
Received: from mail-yh0-f42.google.com ([209.85.213.42])
	by cm02fe.ist.berkeley.edu with esmtpsa (TLSv1:RC4-SHA:128)
	(Exim 4.76)
	(auth plain:keo@eecs.berkeley.edu)
	(envelope-from <keo@eecs.berkeley.edu>)
	id 1Wz81q-0006aZ-9Q
	for dev@spark.apache.org; Mon, 23 Jun 2014 10:27:40 -0700
Received: by mail-yh0-f42.google.com with SMTP id i57so5245432yha.15
        for <dev@spark.apache.org>; Mon, 23 Jun 2014 10:27:38 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.236.44.41 with SMTP id m29mr38006644yhb.57.1403544458097;
 Mon, 23 Jun 2014 10:27:38 -0700 (PDT)
Received: by 10.170.220.11 with HTTP; Mon, 23 Jun 2014 10:27:38 -0700 (PDT)
In-Reply-To: <CAJmC80_8YR6v=OhZpkEf+C8nNePt8z6LubvpOkMJ2=9ZBCn7Hg@mail.gmail.com>
References: <CAJmC80_8YR6v=OhZpkEf+C8nNePt8z6LubvpOkMJ2=9ZBCn7Hg@mail.gmail.com>
Date: Mon, 23 Jun 2014 10:27:38 -0700
Message-ID: <CAKJXNjHO5cQsWMM5JvZfCqPY3W+XPici0CTv8Wt1zxJrBS6rew@mail.gmail.com>
Subject: Re: Current status of Sparrow
From: Kay Ousterhout <keo@eecs.berkeley.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01634aa8d9fd6b04fc8428bb
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01634aa8d9fd6b04fc8428bb
Content-Type: text/plain; charset=UTF-8

Hi Liquan,

Sparrow is not currently integrated into the Spark distribution, so if
you'd like to use Spark with Sparrow, you need to use a forked version of
Spark (https://github.com/kayousterhout/spark/tree/sparrow).  This version
of Spark was forked off an older version of Spark so some work will be
involved to bring this up to date with the latest version of Spark; I can
help with this.

Unfortunately there are also a few practical problems with using Sparrow
with Spark that may or may not be compatible with your target workload.
 Sparrow distributes scheduling over many Sparrow schedulers that are each
associated with their own Spark driver (this is where Sparrow's
improvements stem from -- there's no longer a single driver serving as the
bottleneck for your application, but all of the schedulers/drivers share
the same slots for scheduling tasks).  As a result, data stored in Spark's
block manager on one Spark driver (and created as part of a job scheduled
by the associated Sparrow scheduler) cannot be accessed by other Spark
drivers.  If you're storing data in Tachyon or have a workload where
different jobs have disjoint working sets, this won't be an issue.

-Kay


On Fri, Jun 20, 2014 at 5:47 PM, Liquan Pei <liquanpei@gmail.com> wrote:

> Hi
>
> What is the current status of Sparrow integration with Spark? I would like
> to integrate Sparrow with Spark 1.0 on a 100 node cluster. Any suggestions?
>
> Thanks a lot for your help!
> Liquan
>

--089e01634aa8d9fd6b04fc8428bb--

From dev-return-8092-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 23 21:20:35 2014
Return-Path: <dev-return-8092-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 77C3B119BB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Jun 2014 21:20:35 +0000 (UTC)
Received: (qmail 39094 invoked by uid 500); 23 Jun 2014 21:20:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39039 invoked by uid 500); 23 Jun 2014 21:20:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39012 invoked by uid 99); 23 Jun 2014 21:20:34 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Jun 2014 21:20:34 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.212.182 as permitted sender)
Received: from [209.85.212.182] (HELO mail-wi0-f182.google.com) (209.85.212.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Jun 2014 21:20:32 +0000
Received: by mail-wi0-f182.google.com with SMTP id bs8so4969079wib.9
        for <dev@spark.apache.org>; Mon, 23 Jun 2014 14:20:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=1K6NzI4gEzl7hQpv88lTyg2pa4318VnIs42iHOkKC6U=;
        b=C85uJZe7YUnQuLy0DBvv8vyO/+hVMNzI18IMhBoOvTN44tIfKrxYETdM6LNXSi+gdp
         yvheKSmEr95RuHXFezd1bUhS0tWg7oeMTowHOWuei68uEak3FD17d2ej+/F2JHmr6FpG
         4YbcLDS8qACAATl51GEk6UoVSpl0EkD+IvC950JCdRh/HTd5BPmTYo13Bb39/nAH3v5/
         YRHA6X7C8rMFZ1RagyT07uT5dYVtYpNbLWeeNbuHzwZvYBFjzCCB+hxMvuAgZPMMw7wA
         qZsrPFGdubf+/1SEDpwh2kEwUTe5kLQQBl1dyYWp/3h+9y8U9iv4Mm0S+0wlBEEDdog6
         uqgw==
MIME-Version: 1.0
X-Received: by 10.180.212.77 with SMTP id ni13mr28939536wic.5.1403558408378;
 Mon, 23 Jun 2014 14:20:08 -0700 (PDT)
Received: by 10.194.169.234 with HTTP; Mon, 23 Jun 2014 14:20:08 -0700 (PDT)
In-Reply-To: <1403487432188-7068.post@n3.nabble.com>
References: <1403475286251-7066.post@n3.nabble.com>
	<CAJgQjQ_LHXictis2ynDsU24mh3D0GhfuMyGB2o4_Ni4_pH6+XA@mail.gmail.com>
	<1403487432188-7068.post@n3.nabble.com>
Date: Mon, 23 Jun 2014 14:20:08 -0700
Message-ID: <CAJgQjQ-gs4epo=WvpyiE_1AkwH9iw39J742NYMsDpBw6gSHvZg@mail.gmail.com>
Subject: Re: Checkpointed RDD still causing StackOverflow
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Calling checkpoint() alone doesn't cut the lineage. It only marks the
RDD as to be checkpointed. The lineage is cut after the first time
this RDD is materialized. You see StackOverflow becaure the lineage is
still there. -Xiangrui

On Sun, Jun 22, 2014 at 6:37 PM, dash <bshi@nd.edu> wrote:
> Hi Xiangrui,
>
> According to my knowledge, calling count is for materialize the RDD, does
> collect do the same thing since it also an action? I can not call count
> because for a Graph object, count does not materialize the RDD. I already
> send an issue on that.
>
> My question is, why there still have stack overflow even if `isCheckpointed`
> is true?
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Checkpointed-RDD-still-causing-StackOverflow-tp7066p7068.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8093-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 23 21:20:36 2014
Return-Path: <dev-return-8093-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 05943119BC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Jun 2014 21:20:36 +0000 (UTC)
Received: (qmail 39909 invoked by uid 500); 23 Jun 2014 21:20:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39852 invoked by uid 500); 23 Jun 2014 21:20:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39028 invoked by uid 99); 23 Jun 2014 21:20:34 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Jun 2014 21:20:34 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.212.180 as permitted sender)
Received: from [209.85.212.180] (HELO mail-wi0-f180.google.com) (209.85.212.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Jun 2014 21:20:32 +0000
Received: by mail-wi0-f180.google.com with SMTP id hi2so4855971wib.1
        for <dev@spark.incubator.apache.org>; Mon, 23 Jun 2014 14:20:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=1K6NzI4gEzl7hQpv88lTyg2pa4318VnIs42iHOkKC6U=;
        b=C85uJZe7YUnQuLy0DBvv8vyO/+hVMNzI18IMhBoOvTN44tIfKrxYETdM6LNXSi+gdp
         yvheKSmEr95RuHXFezd1bUhS0tWg7oeMTowHOWuei68uEak3FD17d2ej+/F2JHmr6FpG
         4YbcLDS8qACAATl51GEk6UoVSpl0EkD+IvC950JCdRh/HTd5BPmTYo13Bb39/nAH3v5/
         YRHA6X7C8rMFZ1RagyT07uT5dYVtYpNbLWeeNbuHzwZvYBFjzCCB+hxMvuAgZPMMw7wA
         qZsrPFGdubf+/1SEDpwh2kEwUTe5kLQQBl1dyYWp/3h+9y8U9iv4Mm0S+0wlBEEDdog6
         uqgw==
MIME-Version: 1.0
X-Received: by 10.180.212.77 with SMTP id ni13mr28939536wic.5.1403558408378;
 Mon, 23 Jun 2014 14:20:08 -0700 (PDT)
Received: by 10.194.169.234 with HTTP; Mon, 23 Jun 2014 14:20:08 -0700 (PDT)
In-Reply-To: <1403487432188-7068.post@n3.nabble.com>
References: <1403475286251-7066.post@n3.nabble.com>
	<CAJgQjQ_LHXictis2ynDsU24mh3D0GhfuMyGB2o4_Ni4_pH6+XA@mail.gmail.com>
	<1403487432188-7068.post@n3.nabble.com>
Date: Mon, 23 Jun 2014 14:20:08 -0700
Message-ID: <CAJgQjQ-gs4epo=WvpyiE_1AkwH9iw39J742NYMsDpBw6gSHvZg@mail.gmail.com>
Subject: Re: Checkpointed RDD still causing StackOverflow
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Calling checkpoint() alone doesn't cut the lineage. It only marks the
RDD as to be checkpointed. The lineage is cut after the first time
this RDD is materialized. You see StackOverflow becaure the lineage is
still there. -Xiangrui

On Sun, Jun 22, 2014 at 6:37 PM, dash <bshi@nd.edu> wrote:
> Hi Xiangrui,
>
> According to my knowledge, calling count is for materialize the RDD, does
> collect do the same thing since it also an action? I can not call count
> because for a Graph object, count does not materialize the RDD. I already
> send an issue on that.
>
> My question is, why there still have stack overflow even if `isCheckpointed`
> is true?
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Checkpointed-RDD-still-causing-StackOverflow-tp7066p7068.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8094-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 23 21:27:49 2014
Return-Path: <dev-return-8094-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0E11411A02
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Jun 2014 21:27:49 +0000 (UTC)
Received: (qmail 58410 invoked by uid 500); 23 Jun 2014 21:27:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58357 invoked by uid 500); 23 Jun 2014 21:27:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58346 invoked by uid 99); 23 Jun 2014 21:27:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Jun 2014 21:27:48 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.192.179] (HELO mail-pd0-f179.google.com) (209.85.192.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Jun 2014 21:27:43 +0000
Received: by mail-pd0-f179.google.com with SMTP id w10so6137157pde.38
        for <dev@spark.apache.org>; Mon, 23 Jun 2014 14:27:22 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:sender:in-reply-to:references:date
         :message-id:subject:from:to:content-type:content-transfer-encoding;
        bh=ldKYWM7BS44PhrbGj0CuB8+oc/+afGNkQ9QQgDJUy3U=;
        b=ARowG74G0UjAKmodjl6eHYvXjab/gTOBPA2RwHPmErfHp5FAoncrjyw9f1DMAfb0RN
         DERzYFrY+Bo6306YV2Yx8MzKQlKbqEN+AQX8UoQpfPS3gYXVp0+n+LvPA5/sy0A9C4Xb
         cm/DrYix3Urk7vVt76m6NPOpjbuwcMaRZseLnQJpWM+7cE+sXwP0WmpD9iXgumQY2DXQ
         K6kb3YsBgQTRNfMx3NFiM6kxL+VCPnEy084NOnrjH/3wKx3iiuEg0qg0SUW23TSKPGz8
         6cKrYcCOJV1jm5rRtqCfnfu/oInLhoTakqA4N4Fi24n2NyJelHAVUJ8dZ1C6E+LDgaMx
         pjdw==
X-Gm-Message-State: ALoCoQkWOrLBOGwEaDDWzeeVQT2H2blRjUwWbu/4qPuc1C8I9S1ttygDfJZFGJrctuxpWN344Lhc
MIME-Version: 1.0
X-Received: by 10.68.211.233 with SMTP id nf9mr32350993pbc.29.1403558842568;
 Mon, 23 Jun 2014 14:27:22 -0700 (PDT)
Sender: mark@coactus.com
Received: by 10.70.22.134 with HTTP; Mon, 23 Jun 2014 14:27:22 -0700 (PDT)
X-Originating-IP: [192.0.216.13]
In-Reply-To: <etPan.53a34084.6b8b4567.100@dhcp-47-206.eecs.berkeley.edu>
References: <CALcoZiqBFbcU=tdTLosT+x5DfzSxyeUvjFYDmGMXxTigvKNhhg@mail.gmail.com>
	<etPan.53a34084.6b8b4567.100@dhcp-47-206.eecs.berkeley.edu>
Date: Mon, 23 Jun 2014 17:27:22 -0400
X-Google-Sender-Auth: 082TqvhwuoqzxCRqyr0jZm-MjCY
Message-ID: <CALcoZirzViqoEDVjyM_GTN=PV=knPQYsx+OHTghRkMb7OTtRww@mail.gmail.com>
Subject: Re: Problems with Pyspark + Dill tests
From: Mark Baker <distobj@acm.org>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

On Thu, Jun 19, 2014 at 3:56 PM, Josh Rosen <rosenville@gmail.com> wrote:
> Thanks for helping with the Dill integration; I had some early first atte=
mpts, but had to set them aside when I got busy with some other work.
>
> Just to bring everyone up to speed regarding context:
> There are some objects that PySpark=E2=80=99s `cloudpickle` library doesn=
=E2=80=99t serialize properly, such as operator.getattr (https://issues.apa=
che.org/jira/browse/SPARK-791) or NamedTuples (https://issues.apache.org/ji=
ra/browse/SPARK-1687).
> My early attempt at replacing CloudPickle with Dill ran into problems bec=
ause of slight differences in how Dill pickles functions defined in doctest=
s versus functions defined elsewhere.  I opened a bug report for this with =
the Dill developers (https://github.com/uqfoundation/dill/issues/18), who s=
ubsequently fixed the bug (https://github.com/uqfoundation/dill/pull/29).
> It looks like there=E2=80=99s already a couple of Dill issues with exampl=
es of the =E2=80=9CCan=E2=80=99t pickle _ it=E2=80=99s not found as _=E2=80=
=9D bug (https://github.com/uqfoundation/dill/search?q=3D%22not+found+as%22=
&type=3DIssues).  If you can find a small test case that reproduces this bu=
g, I=E2=80=99d consider opening a new Dill issue.

Thanks for the context, Josh.

I've gone ahead and created a new test case and just opened a new issue;

https://github.com/uqfoundation/dill/issues/49

From dev-return-8095-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 23 22:58:15 2014
Return-Path: <dev-return-8095-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B6E9411E49
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Jun 2014 22:58:15 +0000 (UTC)
Received: (qmail 68824 invoked by uid 500); 23 Jun 2014 22:58:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68782 invoked by uid 500); 23 Jun 2014 22:58:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68690 invoked by uid 99); 23 Jun 2014 22:58:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Jun 2014 22:58:06 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vanzin@cloudera.com designates 209.85.216.49 as permitted sender)
Received: from [209.85.216.49] (HELO mail-qa0-f49.google.com) (209.85.216.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Jun 2014 22:58:04 +0000
Received: by mail-qa0-f49.google.com with SMTP id w8so6187978qac.22
        for <dev@spark.apache.org>; Mon, 23 Jun 2014 15:57:40 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=eiV+9ZGpMd5nCC/1K0w2QzHlCASK+Gsc9v1mbtbE3HI=;
        b=eT3SlzrIExhR6zKEntcpmqhZ7ZJh8UBsoRGecEOTHAoyJh3tQ7XL0IWYUAExwtJ/vG
         +Vj0CbD1fMMkXkmjRGUZPK13Mb9HbLml3YeG/3FNsbXuXW2aZEOiqj70nAEMu6r68rDt
         6aMCgdwLbBJHehDGlqyn8MreR+bVovT0v3516Fh0UwFhsmdpdbc5oTKVXIHQZSbmA/oD
         lkT/22AwMFp3SLNwekNEAxqpsQWRBKWpXhiDYmHM4i6u+pWtPKF+Ngn9PHq99DcdEDAY
         xhW5Z7qaa9/7dLvpbfTlY74a7YnPWfN92LsQ9CGHnUIpIaBo6YLriArEErecRsVlQIWH
         3otg==
X-Gm-Message-State: ALoCoQnk7EnnYSgDtnr1hZzy53Y936HUUZN3lUra09T3tLym6J9USdWoLNmY6l7l4FZ2AjfRUJu2
MIME-Version: 1.0
X-Received: by 10.140.36.17 with SMTP id o17mr34985783qgo.25.1403564260055;
 Mon, 23 Jun 2014 15:57:40 -0700 (PDT)
Received: by 10.229.199.194 with HTTP; Mon, 23 Jun 2014 15:57:40 -0700 (PDT)
Date: Mon, 23 Jun 2014 15:57:40 -0700
Message-ID: <CAAOnQ7uvaLEA+3wvL+B6=VCtgY5XkCdP8h5OvgiLXRjFkVE48g@mail.gmail.com>
Subject: RFC: [SPARK-529] Create constants for known config variables.
From: Marcelo Vanzin <vanzin@cloudera.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I started with some code to implement an idea I had for SPARK-529, and
before going much further (since it's a large and kinda boring change)
I'd like to get some feedback from people.

Current code it at:
https://github.com/vanzin/spark/tree/SPARK-529

There are still some parts I haven't fully fleshed out yet (see TODO
list in the commit message), but that's the basic idea. Let me know if
you have any feedback or different ideas.

Thanks!


-- 
Marcelo

From dev-return-8096-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 24 02:59:29 2014
Return-Path: <dev-return-8096-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2E61F1142A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Jun 2014 02:59:29 +0000 (UTC)
Received: (qmail 1627 invoked by uid 500); 24 Jun 2014 02:59:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1557 invoked by uid 500); 24 Jun 2014 02:59:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1539 invoked by uid 99); 24 Jun 2014 02:59:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Jun 2014 02:59:28 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.170 as permitted sender)
Received: from [209.85.192.170] (HELO mail-pd0-f170.google.com) (209.85.192.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Jun 2014 02:59:23 +0000
Received: by mail-pd0-f170.google.com with SMTP id z10so6385011pdj.15
        for <dev@spark.apache.org>; Mon, 23 Jun 2014 19:59:03 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=SIvpND0iV7FNrEEQ7dL7h8KRgqgazlXM1j2rWGxwSl8=;
        b=x2FJeig+uDZy73Ncc87/XuD/vP8puCWNj2uwU7IINpX7LvERlK0Zl7znKZBvu5o28q
         vrHNw5i3BBw3KZtxuSYs0jyBvqRJ724+qoyYSF8JLK8cNR5i9iFpxA1RHRiMIcoui6kw
         V7zYMQIyhR7Z4sAEtrLEgB80pTB8Uy2VtcR4+d5CUS8lIuvzUkCLQfcvwxqxJSEn7Sri
         pQtiYHlWGWv93RaK9XxerzCKBY29HqFMqCR724dKlT13XmPInSgw+UVbVF08vQWqAD1L
         hyzMI3QifMwaKoZg0KjU5vfLiv2/n4ZgAZomWxogAWY/f6nItQ+cfBwkoVc2XGV8Mr1Y
         OmuQ==
X-Received: by 10.67.30.97 with SMTP id kd1mr35046068pad.15.1403578743142;
        Mon, 23 Jun 2014 19:59:03 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id jt7sm29201240pbc.46.2014.06.23.19.58.59
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 23 Jun 2014 19:59:00 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
Subject: Re: RFC: [SPARK-529] Create constants for known config variables.
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAAOnQ7uvaLEA+3wvL+B6=VCtgY5XkCdP8h5OvgiLXRjFkVE48g@mail.gmail.com>
Date: Mon, 23 Jun 2014 19:58:59 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <29F28F76-AC67-48EE-90B4-F62505E12BAB@gmail.com>
References: <CAAOnQ7uvaLEA+3wvL+B6=VCtgY5XkCdP8h5OvgiLXRjFkVE48g@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.2)
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Marcelo,

When we did the configuration pull request, we actually avoided having a =
big list of defaults in one class file, because this creates a file that =
all the components in the project depend on. For example, since we have =
some settings specific to streaming and the REPL, do we want those =
settings to appear in a file that=92s in =93core=94? It might be better =
to just make sure we use constants for defaults in the code, or maybe =
have a separate class per project.

The other problem with this kind of change is that it=92s disruptive to =
all the other ongoing patches, so I wouldn=92t consider it high-priority =
right now. We haven=92t had a ton of problems with settings being =
mistyped.

If you do want to do something like this though, apart from the comment =
above about modules, please make sure this is not a public API. As soon =
as we add it to the API, it means we can=92t change or remove those =
config settings. I=92d also suggest giving each config setting a single =
name instead of having =93ui=94, =93shuffle=94, etc objects, since the =
chained calls to conf.ui.port.value look somewhat confusing.

Matei

On Jun 23, 2014, at 3:57 PM, Marcelo Vanzin <vanzin@cloudera.com> wrote:

> I started with some code to implement an idea I had for SPARK-529, and
> before going much further (since it's a large and kinda boring change)
> I'd like to get some feedback from people.
>=20
> Current code it at:
> https://github.com/vanzin/spark/tree/SPARK-529
>=20
> There are still some parts I haven't fully fleshed out yet (see TODO
> list in the commit message), but that's the basic idea. Let me know if
> you have any feedback or different ideas.
>=20
> Thanks!
>=20
>=20
> --=20
> Marcelo


From dev-return-8097-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 24 04:52:30 2014
Return-Path: <dev-return-8097-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 77A6711648
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Jun 2014 04:52:30 +0000 (UTC)
Received: (qmail 61486 invoked by uid 500); 24 Jun 2014 04:52:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61430 invoked by uid 500); 24 Jun 2014 04:52:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61414 invoked by uid 99); 24 Jun 2014 04:52:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Jun 2014 04:52:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.216.50] (HELO mail-qa0-f50.google.com) (209.85.216.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Jun 2014 04:52:24 +0000
Received: by mail-qa0-f50.google.com with SMTP id m5so6465216qaj.23
        for <dev@spark.apache.org>; Mon, 23 Jun 2014 21:52:03 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=dVIUt6rzh2ZjDkeRdx5ozqK75FbwRi2u49B6DnaU1Lc=;
        b=iZCR5qVimhRoX7Mu67AOkqsqCQWuOO5onHatMYKk2U7yXH7ntgPl4of8hkLXLtfUeK
         VunH6PhftMdBii26KLjtX0RKGRn1ur0YrRcpFEdmgKonqDiuYQ7fb6G27kbZzHhrZXtT
         lyDzXzO9z5QO0KIpiPi4MfVAIquO06WxZ5ysFlA47Pn6fg1ALA1EfX7nADPak3bkFPtG
         z7M90UeJVRtgDoU5+fPFhgbx/dsseCGh4JrD8QPk+6VXGK8zS/eb9HcgnEshbYY5peFQ
         8JXHTulmaG5S9pOpZ1b8rfbirouKKRR42+e6qiuTxELDqecIxxkYZ8e81pgYuSN8sQes
         6O8A==
X-Gm-Message-State: ALoCoQlQTAWLrDGCdeTX6gxLpTsaj67iiqujcAq0OtcWJj0rL2+anwMjCF/m7UaUVZ9/c928rq9r
X-Received: by 10.224.161.129 with SMTP id r1mr2034520qax.86.1403585523466;
 Mon, 23 Jun 2014 21:52:03 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Mon, 23 Jun 2014 21:51:43 -0700 (PDT)
In-Reply-To: <CAJiQeYLnEL+6dWiGfoim-V=vPydJ_TTFZ6VQ_jUgmuyVT8w=dg@mail.gmail.com>
References: <JIRA.12714951.1400320946136@arcas> <JIRA.12714951.1400320946136.374466.1400320994592@arcas>
 <CAJiQeYLnEL+6dWiGfoim-V=vPydJ_TTFZ6VQ_jUgmuyVT8w=dg@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 23 Jun 2014 21:51:43 -0700
Message-ID: <CAPh_B=bSZM6+D55P-u=G=4sYf5U8oFSFF2vKBC+iF8sem2a+Hw@mail.gmail.com>
Subject: Re: [jira] [Created] (SPARK-1867) Spark Documentation Error causes
 java.lang.IllegalStateException: unread block data
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: Xuefu Zhang <xzhang@cloudera.com>
Content-Type: multipart/alternative; boundary=089e0153869489e12304fc8db8c8
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0153869489e12304fc8db8c8
Content-Type: text/plain; charset=UTF-8

Mridul,

Can you comment a little bit more on this issue? We are running into the
same stack trace but not sure whether it is just different Spark versions
on each cluster (doesn't seem likely) or a bug in Spark.

Thanks.



On Sat, May 17, 2014 at 4:41 AM, Mridul Muralidharan <mridul@gmail.com>
wrote:

> I suspect this is an issue we have fixed internally here as part of a
> larger change - the issue we fixed was not a config issue but bugs in
> spark.
>
> Unfortunately we plan to contribute this as part of 1.1
>
> Regards,
> Mridul
> On 17-May-2014 4:09 pm, "sam (JIRA)" <jira@apache.org> wrote:
>
> > sam created SPARK-1867:
> > --------------------------
> >
> >              Summary: Spark Documentation Error causes
> > java.lang.IllegalStateException: unread block data
> >                  Key: SPARK-1867
> >                  URL: https://issues.apache.org/jira/browse/SPARK-1867
> >              Project: Spark
> >           Issue Type: Bug
> >             Reporter: sam
> >
> >
> > I've employed two System Administrators on a contract basis (for quite a
> > bit of money), and both contractors have independently hit the following
> > exception.  What we are doing is:
> >
> > 1. Installing Spark 0.9.1 according to the documentation on the website,
> > along with CDH4 (and another cluster with CDH5) distros of hadoop/hdfs.
> > 2. Building a fat jar with a Spark app with sbt then trying to run it on
> > the cluster
> >
> > I've also included code snippets, and sbt deps at the bottom.
> >
> > When I've Googled this, there seems to be two somewhat vague responses:
> > a) Mismatching spark versions on nodes/user code
> > b) Need to add more jars to the SparkConf
> >
> > Now I know that (b) is not the problem having successfully run the same
> > code on other clusters while only including one jar (it's a fat jar).
> >
> > But I have no idea how to check for (a) - it appears Spark doesn't have
> > any version checks or anything - it would be nice if it checked versions
> > and threw a "mismatching version exception: you have user code using
> > version X and node Y has version Z".
> >
> > I would be very grateful for advice on this.
> >
> > The exception:
> >
> > Exception in thread "main" org.apache.spark.SparkException: Job aborted:
> > Task 0.0:1 failed 32 times (most recent failure: Exception failure:
> > java.lang.IllegalStateException: unread block data)
> >         at
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1020)
> >         at
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1018)
> >         at
> >
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
> >         at
> > scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> >         at org.apache.spark.scheduler.DAGScheduler.org
> > $apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1018)
> >         at
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
> >         at
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
> >         at scala.Option.foreach(Option.scala:236)
> >         at
> >
> org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:604)
> >         at
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:190)
> >         at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
> >         at akka.actor.ActorCell.invoke(ActorCell.scala:456)
> >         at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
> >         at akka.dispatch.Mailbox.run(Mailbox.scala:219)
> >         at
> >
> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
> >         at
> > scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
> >         at
> >
> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
> >         at
> > scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
> >         at
> >
> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
> > 14/05/16 18:05:31 INFO scheduler.TaskSetManager: Loss was due to
> > java.lang.IllegalStateException: unread block data [duplicate 59]
> >
> > My code snippet:
> >
> > val conf = new SparkConf()
> >                .setMaster(clusterMaster)
> >                .setAppName(appName)
> >                .setSparkHome(sparkHome)
> >                .setJars(SparkContext.jarOfClass(this.getClass))
> >
> > println("count = " + new
> SparkContext(conf).textFile(someHdfsPath).count())
> >
> > My SBT dependencies:
> >
> > // relevant
> > "org.apache.spark" % "spark-core_2.10" % "0.9.1",
> > "org.apache.hadoop" % "hadoop-client" % "2.3.0-mr1-cdh5.0.0",
> >
> > // standard, probably unrelated
> > "com.github.seratch" %% "awscala" % "[0.2,)",
> > "org.scalacheck" %% "scalacheck" % "1.10.1" % "test",
> > "org.specs2" %% "specs2" % "1.14" % "test",
> > "org.scala-lang" % "scala-reflect" % "2.10.3",
> > "org.scalaz" %% "scalaz-core" % "7.0.5",
> > "net.minidev" % "json-smart" % "1.2"
> >
> >
> >
> > --
> > This message was sent by Atlassian JIRA
> > (v6.2#6252)
> >
>

--089e0153869489e12304fc8db8c8--

From dev-return-8098-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 24 05:28:16 2014
Return-Path: <dev-return-8098-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2861E11733
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Jun 2014 05:28:16 +0000 (UTC)
Received: (qmail 29323 invoked by uid 500); 24 Jun 2014 05:28:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 29264 invoked by uid 500); 24 Jun 2014 05:28:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 29248 invoked by uid 99); 24 Jun 2014 05:28:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Jun 2014 05:28:15 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.216.52 as permitted sender)
Received: from [209.85.216.52] (HELO mail-qa0-f52.google.com) (209.85.216.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Jun 2014 05:28:10 +0000
Received: by mail-qa0-f52.google.com with SMTP id w8so6422958qac.25
        for <dev@spark.apache.org>; Mon, 23 Jun 2014 22:27:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=DfwDSrKncXqFCH58AKkhXCLCqrltUHKYriWxO5CSx5o=;
        b=gtp7LQxQW7JLzx36gzjr5tpyCMQ39x4M5VCWkvRvJ1t0ZcL9dlTdoYLiJIs6jCSdiv
         Sm+NlAAxBMEfeoh0NTT/ZWdCekZfKHiI2VXtHYq8HxAFGs4iMf5cui+wHWh/D9MLHQUv
         +AuaizZUAU/pjdvv7RVUm3clDxlStlWmTY3cHLtSZWel7gXy4dlEC46Zimo4bPNGlVjI
         e2o5DepTU6F4KpDNL2ZJ9V32ezGyl+ycj9+ZSB6XwXOcOboMQcwF7gsQ0I2Jz4h7a9kM
         eUDMI27dqJuvuR1oGBn3fiUSOowgun5Lk71H9CVa/DLOu9f5V32xluLarfr4c75SGfHh
         tZGA==
MIME-Version: 1.0
X-Received: by 10.140.81.74 with SMTP id e68mr36566094qgd.77.1403587670275;
 Mon, 23 Jun 2014 22:27:50 -0700 (PDT)
Received: by 10.140.38.149 with HTTP; Mon, 23 Jun 2014 22:27:50 -0700 (PDT)
In-Reply-To: <CAPh_B=bSZM6+D55P-u=G=4sYf5U8oFSFF2vKBC+iF8sem2a+Hw@mail.gmail.com>
References: <JIRA.12714951.1400320946136@arcas>
	<JIRA.12714951.1400320946136.374466.1400320994592@arcas>
	<CAJiQeYLnEL+6dWiGfoim-V=vPydJ_TTFZ6VQ_jUgmuyVT8w=dg@mail.gmail.com>
	<CAPh_B=bSZM6+D55P-u=G=4sYf5U8oFSFF2vKBC+iF8sem2a+Hw@mail.gmail.com>
Date: Tue, 24 Jun 2014 10:57:50 +0530
Message-ID: <CAJiQeY+TaMvDC_N1QGQ8mtAKLzPQjZV7E4UPv6bE=ZXEuPjmqA@mail.gmail.com>
Subject: Re: [jira] [Created] (SPARK-1867) Spark Documentation Error causes
 java.lang.IllegalStateException: unread block data
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Cc: Xuefu Zhang <xzhang@cloudera.com>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

There are a few interacting issues here - and unfortunately I dont
recall all of it (since this was fixed a few months back).
>From memory though :

a) With shuffle consolidation, data sent to remote node incorrectly
includes data from partially constructed blocks - not just the request
blocks.
Actually, with shuffle consolidation (with and without failures) quite
a few things broke.

b) There might have been a few other bugs in DiskBlockObjectWriter too.

c) We also suspected buffers overlapping when using cached kryo
serializer (though never proved this, just disabled caching across
board for now : and always create new instance).

The way we debug'ed it is by introducing an Input/Output stream which
introduced checksum into the data stream and validating that at each
side for compression, serialization, etc.

Apologies for being non specific ... I really dont have the details
right now, and our internal branch is in flux due to merge effort to
port our local changes to master.
Hopefully we will be able to submit PR's as soon as this is done and
testcases are added to validate.


Regards,
Mridul




On Tue, Jun 24, 2014 at 10:21 AM, Reynold Xin <rxin@databricks.com> wrote:
> Mridul,
>
> Can you comment a little bit more on this issue? We are running into the
> same stack trace but not sure whether it is just different Spark versions
> on each cluster (doesn't seem likely) or a bug in Spark.
>
> Thanks.
>
>
>
> On Sat, May 17, 2014 at 4:41 AM, Mridul Muralidharan <mridul@gmail.com>
> wrote:
>
>> I suspect this is an issue we have fixed internally here as part of a
>> larger change - the issue we fixed was not a config issue but bugs in
>> spark.
>>
>> Unfortunately we plan to contribute this as part of 1.1
>>
>> Regards,
>> Mridul
>> On 17-May-2014 4:09 pm, "sam (JIRA)" <jira@apache.org> wrote:
>>
>> > sam created SPARK-1867:
>> > --------------------------
>> >
>> >              Summary: Spark Documentation Error causes
>> > java.lang.IllegalStateException: unread block data
>> >                  Key: SPARK-1867
>> >                  URL: https://issues.apache.org/jira/browse/SPARK-1867
>> >              Project: Spark
>> >           Issue Type: Bug
>> >             Reporter: sam
>> >
>> >
>> > I've employed two System Administrators on a contract basis (for quite a
>> > bit of money), and both contractors have independently hit the following
>> > exception.  What we are doing is:
>> >
>> > 1. Installing Spark 0.9.1 according to the documentation on the website,
>> > along with CDH4 (and another cluster with CDH5) distros of hadoop/hdfs.
>> > 2. Building a fat jar with a Spark app with sbt then trying to run it on
>> > the cluster
>> >
>> > I've also included code snippets, and sbt deps at the bottom.
>> >
>> > When I've Googled this, there seems to be two somewhat vague responses:
>> > a) Mismatching spark versions on nodes/user code
>> > b) Need to add more jars to the SparkConf
>> >
>> > Now I know that (b) is not the problem having successfully run the same
>> > code on other clusters while only including one jar (it's a fat jar).
>> >
>> > But I have no idea how to check for (a) - it appears Spark doesn't have
>> > any version checks or anything - it would be nice if it checked versions
>> > and threw a "mismatching version exception: you have user code using
>> > version X and node Y has version Z".
>> >
>> > I would be very grateful for advice on this.
>> >
>> > The exception:
>> >
>> > Exception in thread "main" org.apache.spark.SparkException: Job aborted:
>> > Task 0.0:1 failed 32 times (most recent failure: Exception failure:
>> > java.lang.IllegalStateException: unread block data)
>> >         at
>> >
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1020)
>> >         at
>> >
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$org$apache$spark$scheduler$DAGScheduler$$abortStage$1.apply(DAGScheduler.scala:1018)
>> >         at
>> >
>> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
>> >         at
>> > scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
>> >         at org.apache.spark.scheduler.DAGScheduler.org
>> > $apache$spark$scheduler$DAGScheduler$$abortStage(DAGScheduler.scala:1018)
>> >         at
>> >
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
>> >         at
>> >
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$processEvent$10.apply(DAGScheduler.scala:604)
>> >         at scala.Option.foreach(Option.scala:236)
>> >         at
>> >
>> org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:604)
>> >         at
>> >
>> org.apache.spark.scheduler.DAGScheduler$$anonfun$start$1$$anon$2$$anonfun$receive$1.applyOrElse(DAGScheduler.scala:190)
>> >         at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
>> >         at akka.actor.ActorCell.invoke(ActorCell.scala:456)
>> >         at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
>> >         at akka.dispatch.Mailbox.run(Mailbox.scala:219)
>> >         at
>> >
>> akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
>> >         at
>> > scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
>> >         at
>> >
>> scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
>> >         at
>> > scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
>> >         at
>> >
>> scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
>> > 14/05/16 18:05:31 INFO scheduler.TaskSetManager: Loss was due to
>> > java.lang.IllegalStateException: unread block data [duplicate 59]
>> >
>> > My code snippet:
>> >
>> > val conf = new SparkConf()
>> >                .setMaster(clusterMaster)
>> >                .setAppName(appName)
>> >                .setSparkHome(sparkHome)
>> >                .setJars(SparkContext.jarOfClass(this.getClass))
>> >
>> > println("count = " + new
>> SparkContext(conf).textFile(someHdfsPath).count())
>> >
>> > My SBT dependencies:
>> >
>> > // relevant
>> > "org.apache.spark" % "spark-core_2.10" % "0.9.1",
>> > "org.apache.hadoop" % "hadoop-client" % "2.3.0-mr1-cdh5.0.0",
>> >
>> > // standard, probably unrelated
>> > "com.github.seratch" %% "awscala" % "[0.2,)",
>> > "org.scalacheck" %% "scalacheck" % "1.10.1" % "test",
>> > "org.specs2" %% "specs2" % "1.14" % "test",
>> > "org.scala-lang" % "scala-reflect" % "2.10.3",
>> > "org.scalaz" %% "scalaz-core" % "7.0.5",
>> > "net.minidev" % "json-smart" % "1.2"
>> >
>> >
>> >
>> > --
>> > This message was sent by Atlassian JIRA
>> > (v6.2#6252)
>> >
>>

From dev-return-8099-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 24 17:17:47 2014
Return-Path: <dev-return-8099-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9B6CD11B92
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Jun 2014 17:17:47 +0000 (UTC)
Received: (qmail 98170 invoked by uid 500); 24 Jun 2014 17:17:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98129 invoked by uid 500); 24 Jun 2014 17:17:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98118 invoked by uid 99); 24 Jun 2014 17:17:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Jun 2014 17:17:45 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of vanzin@cloudera.com designates 209.85.216.50 as permitted sender)
Received: from [209.85.216.50] (HELO mail-qa0-f50.google.com) (209.85.216.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Jun 2014 17:17:40 +0000
Received: by mail-qa0-f50.google.com with SMTP id m5so518843qaj.9
        for <dev@spark.apache.org>; Tue, 24 Jun 2014 10:17:19 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type:content-transfer-encoding;
        bh=lS3cvFndeLeFshWkoQZOKAYJVmTBw20wRx6C2A+WvjM=;
        b=d73nz4CB01VlzPUtpwZUKVwVgOwqSPfHOli/jz6yXUN/NPaU5P6WpnoljQiO+Sks4T
         vcnr3xdZXm0mVhJGRXEoTZfFrIZce2CrSwUcHh/k9S+8QclS4s4hK72mW261rKMl9i0D
         sKtU6m5tu5HzIY7teyePKZXRwJpuXlnH7oGOk8bv77D9eYlZHNbxbopk4yKoY+D8OEDY
         E2vZMQoTHA2iI1VrM6OmDv9uILcPL4uERPNLz2r1ofJ8MRVU4AM1pDFG2hGk9GVIYVOt
         YC4weQpgfxbtc2NPBAeOFvg8em7rQ7V6Rj6HDV7hF8YFPRHJovNpLrX63BEwpnB9JtAi
         VGfg==
X-Gm-Message-State: ALoCoQmF8qNc5IMMlBt0yFvYMUHsKOhlEZih+cE/pd3DeBNijQKPmLf1WopenAjKrk9e83EOMq0v
MIME-Version: 1.0
X-Received: by 10.224.34.73 with SMTP id k9mr3976615qad.11.1403630239526; Tue,
 24 Jun 2014 10:17:19 -0700 (PDT)
Received: by 10.229.199.194 with HTTP; Tue, 24 Jun 2014 10:17:19 -0700 (PDT)
In-Reply-To: <29F28F76-AC67-48EE-90B4-F62505E12BAB@gmail.com>
References: <CAAOnQ7uvaLEA+3wvL+B6=VCtgY5XkCdP8h5OvgiLXRjFkVE48g@mail.gmail.com>
	<29F28F76-AC67-48EE-90B4-F62505E12BAB@gmail.com>
Date: Tue, 24 Jun 2014 10:17:19 -0700
Message-ID: <CAAOnQ7u9WeEJ=+JGPWe7qgurq-KakDvOL1QcFD720i=scTY8eA@mail.gmail.com>
Subject: Re: RFC: [SPARK-529] Create constants for known config variables.
From: Marcelo Vanzin <vanzin@cloudera.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Matei, thanks for the comments.

On Mon, Jun 23, 2014 at 7:58 PM, Matei Zaharia <matei.zaharia@gmail.com> wr=
ote:
> When we did the configuration pull request, we actually avoided having a =
big list of defaults in one class file, because this creates a file that al=
l the components in the project depend on. For example, since we have some =
settings specific to streaming and the REPL, do we want those settings to a=
ppear in a file that=E2=80=99s in =E2=80=9Ccore=E2=80=9D? It might be bette=
r to just make sure we use constants for defaults in the code, or maybe hav=
e a separate class per project.

I'm actually following the style used in Hadoop here, but expanding it
a little bit to add type-safety too. (e.g., see DFSConfigKeys.java in
HDFS.) I sort of see your point of "everybody needs to depend on this
file", but then all the affected code already depends on SparkConf.

> The other problem with this kind of change is that it=E2=80=99s disruptiv=
e to all the other ongoing patches, so I wouldn=E2=80=99t consider it high-=
priority right now. We haven=E2=80=99t had a ton of problems with settings =
being mistyped.

Yes, large changes like this are always disruptive. But since it
mostly touches setup code, not the innards of algorithms and the like,
it shouldn't be hard to merge things.

> If you do want to do something like this though, apart from the comment a=
bove about modules, please make sure this is not a public API. As soon as w=
e add it to the API, it means we can=E2=80=99t change or remove those confi=
g settings. I=E2=80=99d also suggest giving each config setting a single na=
me instead of having =E2=80=9Cui=E2=80=9D, =E2=80=9Cshuffle=E2=80=9D, etc o=
bjects, since the chained calls to conf.ui.port.value look somewhat confusi=
ng.

Making it not public is easy; but then how do we want to expose this
to users? It's normal right now, as you say, to hardcode the settings
in the code (or config files - which this change would not help with).
In the process of writing the p.o.c., I found cases of the same
setting with different casing in different source files, and the same
setting with different defaults. So maybe it has not affected any
users yet, but it's easy to miss things like that with the current
approach.

I don't have a good suggestion for separating the module-specific
configs; I kinda liked the "everything under SparkConf" approach and
tried to group settings in a way that made sense. Creating objects in
each module to hold the configs for those is possible, but would
probably look a bit uglier.

But in any case, this is not urgent, more of a nice thing to have in my opi=
nion.

--=20
Marcelo

From dev-return-8100-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 24 22:19:22 2014
Return-Path: <dev-return-8100-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 726A91176F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Jun 2014 22:19:22 +0000 (UTC)
Received: (qmail 10785 invoked by uid 500); 24 Jun 2014 22:19:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10725 invoked by uid 500); 24 Jun 2014 22:19:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10705 invoked by uid 99); 24 Jun 2014 22:19:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Jun 2014 22:19:21 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mayur.rustagi@gmail.com designates 209.85.212.179 as permitted sender)
Received: from [209.85.212.179] (HELO mail-wi0-f179.google.com) (209.85.212.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Jun 2014 22:19:17 +0000
Received: by mail-wi0-f179.google.com with SMTP id cc10so1499798wib.12
        for <dev@spark.incubator.apache.org>; Tue, 24 Jun 2014 15:18:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=ZSvUYp2N9gAVGhL3UFNGUUwX/PnbgNrBbZY849wtR4I=;
        b=pl1rcG5B8pggGRNclMFWjzCxE9I/H/wV+Tll5O1FeF9V+VoRZXEgBoofrSIp0dtYOq
         0njHaQvNsi4xqm+lY2ArP4WWeggsIR0QUY4/d7hxo3t9VZk0FhY8MQtI9H5rlb+mlkII
         0WuiVPrLM/IAqpLMSKPL2Rxl/tRNB3bmA/nXxdH/abksyIK8oFG5XFYFUzdVbD0pi0P8
         Mjix5qcotx8CMuYfai6WeSQUHwpe3J8TFXtPBJ3N036+XKmAjPl6TkV/MR42sU8JIStH
         CfJe70CSs+h9EL3FeIfKJN1+HmZyyZkRZ0HTO5hUQ0jYHrGpiBjH0bFXpUkWr8s74itG
         jCAA==
X-Received: by 10.194.238.6 with SMTP id vg6mr2883657wjc.24.1403648336062;
 Tue, 24 Jun 2014 15:18:56 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.202.137 with HTTP; Tue, 24 Jun 2014 15:18:36 -0700 (PDT)
In-Reply-To: <CAJgQjQ-gs4epo=WvpyiE_1AkwH9iw39J742NYMsDpBw6gSHvZg@mail.gmail.com>
References: <1403475286251-7066.post@n3.nabble.com> <CAJgQjQ_LHXictis2ynDsU24mh3D0GhfuMyGB2o4_Ni4_pH6+XA@mail.gmail.com>
 <1403487432188-7068.post@n3.nabble.com> <CAJgQjQ-gs4epo=WvpyiE_1AkwH9iw39J742NYMsDpBw6gSHvZg@mail.gmail.com>
From: Mayur Rustagi <mayur.rustagi@gmail.com>
Date: Wed, 25 Jun 2014 03:48:36 +0530
Message-ID: <CAAqHKj50-nh4YEm+BJgbmkwA+gA3COM4+kJcMrG1qwTXVSWhrQ@mail.gmail.com>
Subject: Re: Checkpointed RDD still causing StackOverflow
To: dev <dev@spark.apache.org>
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e013d0b5475f28f04fc9c58ff
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013d0b5475f28f04fc9c58ff
Content-Type: text/plain; charset=UTF-8

Do not call collect as that will perform materialization as well as
transfer of data to driver (might actually cause driver to fail if the data
is huge). You have to materialize the RDD in some way(call save, count,
collect).

Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>



On Tue, Jun 24, 2014 at 2:50 AM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Calling checkpoint() alone doesn't cut the lineage. It only marks the
> RDD as to be checkpointed. The lineage is cut after the first time
> this RDD is materialized. You see StackOverflow becaure the lineage is
> still there. -Xiangrui
>
> On Sun, Jun 22, 2014 at 6:37 PM, dash <bshi@nd.edu> wrote:
> > Hi Xiangrui,
> >
> > According to my knowledge, calling count is for materialize the RDD, does
> > collect do the same thing since it also an action? I can not call count
> > because for a Graph object, count does not materialize the RDD. I already
> > send an issue on that.
> >
> > My question is, why there still have stack overflow even if
> `isCheckpointed`
> > is true?
> >
> >
> >
> > --
> > View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Checkpointed-RDD-still-causing-StackOverflow-tp7066p7068.html
> > Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--089e013d0b5475f28f04fc9c58ff--

From dev-return-8101-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 24 22:19:23 2014
Return-Path: <dev-return-8101-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1AB0811770
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Jun 2014 22:19:23 +0000 (UTC)
Received: (qmail 11628 invoked by uid 500); 24 Jun 2014 22:19:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11580 invoked by uid 500); 24 Jun 2014 22:19:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10718 invoked by uid 99); 24 Jun 2014 22:19:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Jun 2014 22:19:22 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mayur.rustagi@gmail.com designates 74.125.82.171 as permitted sender)
Received: from [74.125.82.171] (HELO mail-we0-f171.google.com) (74.125.82.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Jun 2014 22:19:19 +0000
Received: by mail-we0-f171.google.com with SMTP id q58so1107213wes.30
        for <dev@spark.apache.org>; Tue, 24 Jun 2014 15:18:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=ZSvUYp2N9gAVGhL3UFNGUUwX/PnbgNrBbZY849wtR4I=;
        b=pl1rcG5B8pggGRNclMFWjzCxE9I/H/wV+Tll5O1FeF9V+VoRZXEgBoofrSIp0dtYOq
         0njHaQvNsi4xqm+lY2ArP4WWeggsIR0QUY4/d7hxo3t9VZk0FhY8MQtI9H5rlb+mlkII
         0WuiVPrLM/IAqpLMSKPL2Rxl/tRNB3bmA/nXxdH/abksyIK8oFG5XFYFUzdVbD0pi0P8
         Mjix5qcotx8CMuYfai6WeSQUHwpe3J8TFXtPBJ3N036+XKmAjPl6TkV/MR42sU8JIStH
         CfJe70CSs+h9EL3FeIfKJN1+HmZyyZkRZ0HTO5hUQ0jYHrGpiBjH0bFXpUkWr8s74itG
         jCAA==
X-Received: by 10.194.238.6 with SMTP id vg6mr2883657wjc.24.1403648336062;
 Tue, 24 Jun 2014 15:18:56 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.202.137 with HTTP; Tue, 24 Jun 2014 15:18:36 -0700 (PDT)
In-Reply-To: <CAJgQjQ-gs4epo=WvpyiE_1AkwH9iw39J742NYMsDpBw6gSHvZg@mail.gmail.com>
References: <1403475286251-7066.post@n3.nabble.com> <CAJgQjQ_LHXictis2ynDsU24mh3D0GhfuMyGB2o4_Ni4_pH6+XA@mail.gmail.com>
 <1403487432188-7068.post@n3.nabble.com> <CAJgQjQ-gs4epo=WvpyiE_1AkwH9iw39J742NYMsDpBw6gSHvZg@mail.gmail.com>
From: Mayur Rustagi <mayur.rustagi@gmail.com>
Date: Wed, 25 Jun 2014 03:48:36 +0530
Message-ID: <CAAqHKj50-nh4YEm+BJgbmkwA+gA3COM4+kJcMrG1qwTXVSWhrQ@mail.gmail.com>
Subject: Re: Checkpointed RDD still causing StackOverflow
To: dev <dev@spark.apache.org>
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e013d0b5475f28f04fc9c58ff
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013d0b5475f28f04fc9c58ff
Content-Type: text/plain; charset=UTF-8

Do not call collect as that will perform materialization as well as
transfer of data to driver (might actually cause driver to fail if the data
is huge). You have to materialize the RDD in some way(call save, count,
collect).

Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>



On Tue, Jun 24, 2014 at 2:50 AM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Calling checkpoint() alone doesn't cut the lineage. It only marks the
> RDD as to be checkpointed. The lineage is cut after the first time
> this RDD is materialized. You see StackOverflow becaure the lineage is
> still there. -Xiangrui
>
> On Sun, Jun 22, 2014 at 6:37 PM, dash <bshi@nd.edu> wrote:
> > Hi Xiangrui,
> >
> > According to my knowledge, calling count is for materialize the RDD, does
> > collect do the same thing since it also an action? I can not call count
> > because for a Graph object, count does not materialize the RDD. I already
> > send an issue on that.
> >
> > My question is, why there still have stack overflow even if
> `isCheckpointed`
> > is true?
> >
> >
> >
> > --
> > View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Checkpointed-RDD-still-causing-StackOverflow-tp7066p7068.html
> > Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--089e013d0b5475f28f04fc9c58ff--

From dev-return-8102-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jun 24 22:41:59 2014
Return-Path: <dev-return-8102-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6E4C0117F3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Jun 2014 22:41:59 +0000 (UTC)
Received: (qmail 58261 invoked by uid 500); 24 Jun 2014 22:41:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58172 invoked by uid 500); 24 Jun 2014 22:41:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57130 invoked by uid 99); 24 Jun 2014 22:41:57 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Jun 2014 22:41:57 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mayur.rustagi@gmail.com designates 74.125.82.46 as permitted sender)
Received: from [74.125.82.46] (HELO mail-wg0-f46.google.com) (74.125.82.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Jun 2014 22:41:53 +0000
Received: by mail-wg0-f46.google.com with SMTP id y10so1115418wgg.29
        for <multiple recipients>; Tue, 24 Jun 2014 15:41:32 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=KJ+rP8ASI0V+KNZEQr412OOes2dhR6XZA7jReH5JDwE=;
        b=jNw9JhqB2rmI087p2GI07NisQULU2iq4YzAZ5pbxcwMtWI1VnNAZFoPY2Pb6nbEuAN
         FotwdnQUiQYSZqAkdR1tHYu+eBhV1qE7HO+ftIqrTxFUWca9/2DGVic3FBnPCPmzBF8J
         4zJ7ZdGV4p1wAKhcA45HbFxCkX+iW7cs6ZIpHKL668at5Pz0mC6n3MADtrpqHBVTOI5F
         uSGOPs8wOFQ0fqEnNPqLx3UyAaw7muOb1U/IAw5bSvZNlubZw3RbBuyDQuF4FypY8I/e
         ZJavDSkB3cGq9E4BYIS5XnROvXV0ZLRr7oaXQIoMuaoehvCP8OqQGwiggbTkXN5V1tcY
         0D0A==
X-Received: by 10.194.48.8 with SMTP id h8mr4812175wjn.106.1403649691669; Tue,
 24 Jun 2014 15:41:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.202.137 with HTTP; Tue, 24 Jun 2014 15:41:11 -0700 (PDT)
In-Reply-To: <B99A8786-9BD6-43E7-8727-FE8E07147106@webtrends.com>
References: <B99A8786-9BD6-43E7-8727-FE8E07147106@webtrends.com>
From: Mayur Rustagi <mayur.rustagi@gmail.com>
Date: Wed, 25 Jun 2014 04:11:11 +0530
Message-ID: <CAAqHKj5bEvkLLzgVCz4bs4jcGMCByvfRdpGsstUZa2qfzcDqwQ@mail.gmail.com>
Subject: Re: balancing RDDs
To: user@spark.apache.org, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7ba9755442dd7604fc9ca982
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7ba9755442dd7604fc9ca982
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

This would be really useful. Especially for Shark where shift of
partitioning effects all subsequent queries unless task scheduling time
beats spark.locality.wait. Can cause overall low performance for all
subsequent tasks.

Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>



On Tue, Jun 24, 2014 at 4:10 AM, Sean McNamara <Sean.McNamara@webtrends.com=
>
wrote:

> We have a use case where we=E2=80=99d like something to execute once on e=
ach node
> and I thought it would be good to ask here.
>
> Currently we achieve this by setting the parallelism to the number of
> nodes and use a mod partitioner:
>
> val balancedRdd =3D sc.parallelize(
>         (0 until Settings.parallelism)
>         .map(id =3D> id -> Settings.settings)
>       ).partitionBy(new ModPartitioner(Settings.parallelism))
>       .cache()
>
>
> This works great except in two instances where it can become unbalanced:
>
> 1. if a worker is restarted or dies, the partition will move to a
> different node (one of the nodes will run two tasks).  When the worker
> rejoins, is there a way to have a partition move back over to the newly
> restarted worker so that it=E2=80=99s balanced again?
>
> 2. drivers need to be started in a staggered fashion, otherwise one drive=
r
> can launch two tasks on one set of workers, and the other driver will do
> the same with the other set.  Are there any scheduler/config semantics so
> that each driver will take one (and only one) core from *each* node?
>
>
> Thanks
>
> Sean
>
>
>
>
>
>
>

--047d7ba9755442dd7604fc9ca982--

From dev-return-8103-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 25 04:58:05 2014
Return-Path: <dev-return-8103-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D45E011221
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Jun 2014 04:58:05 +0000 (UTC)
Received: (qmail 58738 invoked by uid 500); 25 Jun 2014 04:58:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58680 invoked by uid 500); 25 Jun 2014 04:58:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58648 invoked by uid 99); 25 Jun 2014 04:58:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Jun 2014 04:58:04 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: unknown (nike.apache.org: error in processing during lookup of bshi@nd.edu)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Jun 2014 04:58:02 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <bshi@nd.edu>)
	id 1WzfH5-0005RF-3r
	for dev@spark.incubator.apache.org; Tue, 24 Jun 2014 21:57:35 -0700
Date: Tue, 24 Jun 2014 21:57:35 -0700 (PDT)
From: dash <bshi@nd.edu>
To: dev@spark.incubator.apache.org
Message-ID: <1403672255109-7079.post@n3.nabble.com>
In-Reply-To: <CAAqHKj50-nh4YEm+BJgbmkwA+gA3COM4+kJcMrG1qwTXVSWhrQ@mail.gmail.com>
References: <1403475286251-7066.post@n3.nabble.com> <CAJgQjQ_LHXictis2ynDsU24mh3D0GhfuMyGB2o4_Ni4_pH6+XA@mail.gmail.com> <1403487432188-7068.post@n3.nabble.com> <CAJgQjQ-gs4epo=WvpyiE_1AkwH9iw39J742NYMsDpBw6gSHvZg@mail.gmail.com> <CAAqHKj50-nh4YEm+BJgbmkwA+gA3COM4+kJcMrG1qwTXVSWhrQ@mail.gmail.com>
Subject: Re: Checkpointed RDD still causing StackOverflow
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Due to SPARK-2245, you can not use count to materialize VertexRDD. That
actually materialize PartitionRDD, so checkpoint for VertexRDD won't work.
I'll trying to fix that right now.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Checkpointed-RDD-still-causing-StackOverflow-tp7066p7079.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8104-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 25 05:06:33 2014
Return-Path: <dev-return-8104-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A358411241
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Jun 2014 05:06:33 +0000 (UTC)
Received: (qmail 68636 invoked by uid 500); 25 Jun 2014 05:06:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68577 invoked by uid 500); 25 Jun 2014 05:06:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68565 invoked by uid 99); 25 Jun 2014 05:06:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Jun 2014 05:06:32 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of andykonwinski@gmail.com designates 209.85.217.179 as permitted sender)
Received: from [209.85.217.179] (HELO mail-lb0-f179.google.com) (209.85.217.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Jun 2014 05:06:27 +0000
Received: by mail-lb0-f179.google.com with SMTP id z11so1465213lbi.10
        for <dev@spark.apache.org>; Tue, 24 Jun 2014 22:06:06 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=RasPDI3xFEaENBOuCCGkEWYnZYH2jUK484uC4wUuV2s=;
        b=rP8LoNafI9FUtVVJrg4EDNFTlJr2r3LZbHktE+//HouYkJ5fNE2RHsV1ze39AKlcuP
         wRqTHF/PIYE0t5EXLCUtrP+Ka/1KMr35wgIzzhu+4gfqiXiKRhEZo3dM/5pKWl6+Kl6k
         qRopE8mxMroFgYL4MDRCZos5Gkb6zd0VhAJGSHGSsdNXlVt0l1W+rfb09hkmLIjCpmgF
         FHVvdhQfxXpN+c9RtW156fvDhH98aRHPzEgrySSVf1qsKTEEKfCTjwQRVAyBT1fyH2Ag
         8hFbZVaZwfgDw4eyNdryLjW9SI2poehPp/BKLPA1me9dG4FMR1CXrLSLKOYRqiJcRLup
         mu4Q==
MIME-Version: 1.0
X-Received: by 10.112.12.103 with SMTP id x7mr3693082lbb.36.1403672765880;
 Tue, 24 Jun 2014 22:06:05 -0700 (PDT)
Received: by 10.112.91.44 with HTTP; Tue, 24 Jun 2014 22:06:05 -0700 (PDT)
Received: by 10.112.91.44 with HTTP; Tue, 24 Jun 2014 22:06:05 -0700 (PDT)
In-Reply-To: <CAOy0srvbBbDoKm4oyMhs17iODG8Dnr12-hKCgRoTypC8oQhTVA@mail.gmail.com>
References: <CAOy0srvbBbDoKm4oyMhs17iODG8Dnr12-hKCgRoTypC8oQhTVA@mail.gmail.com>
Date: Tue, 24 Jun 2014 22:06:05 -0700
Message-ID: <CALEZFQwWziR0KiRDKs24pdzzG0XB65QLn0q4ztuPT2NMtzaTMQ@mail.gmail.com>
Subject: Fwd: 2014 Mesos community survey results
From: Andy Konwinski <andykonwinski@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c3eaaa97617404fca208e2
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3eaaa97617404fca208e2
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I think it's cool that the Mesos team did a survey of usage and published
the aggregate results. It would be cool to do a survey for the Spark
project and publish the results on the Spark website like the Mesos team
did.
---------- Forwarded message ----------
From: "Dave Lester" <davelester@gmail.com>
Date: Jun 24, 2014 2:07 PM
Subject: 2014 Mesos community survey results
To: "user@mesos.apache.org" <user@mesos.apache.org>
Cc:

This afternoon I blogged the results of a community survey I led in May to
get the pulse of the Apache Mesos community and improve our understanding
of how others are using the software. We received a total of 55 survey
responses. Thanks all who took the time to provide this critical feedback
for the community!

http://mesos.apache.org/blog/mesos-community-survey-2014-results/

The blog post incl highlights, and a summary of the responses. As I mention
in the post, this is the first time we=E2=80=99ve run a community survey li=
ke this;
feedback is welcome on how we may improve it when we run it again! Feel
free to respond to this thread.

Dave

--001a11c3eaaa97617404fca208e2--

From dev-return-8105-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 25 16:49:27 2014
Return-Path: <dev-return-8105-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DBE3810B26
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Jun 2014 16:49:26 +0000 (UTC)
Received: (qmail 33886 invoked by uid 500); 25 Jun 2014 16:49:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33787 invoked by uid 500); 25 Jun 2014 16:49:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33038 invoked by uid 99); 25 Jun 2014 16:49:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Jun 2014 16:49:24 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=5.0
	tests=SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of Sean.McNamara@webtrends.com designates 216.64.169.23 as permitted sender)
Received: from [216.64.169.23] (HELO pdxmta02.webtrends.com) (216.64.169.23)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Jun 2014 16:49:20 +0000
Received: from pdxex2.webtrends.corp (Not Verified[10.61.2.221]) by pdxmta02.webtrends.com with MailMarshal (v7,2,2,6606) (using TLS: SSLv23)
	id <B53aafd630000>; Wed, 25 Jun 2014 16:48:35 +0000
Received: from PDXEX1.WebTrends.corp ([172.27.5.220]) by pdxex2.webtrends.corp
 ([172.27.3.221]) with mapi id 14.03.0181.006; Wed, 25 Jun 2014 16:48:59 +0000
From: Sean McNamara <Sean.McNamara@Webtrends.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
CC: "user@spark.apache.org" <user@spark.apache.org>
Subject: Re: balancing RDDs
Thread-Topic: balancing RDDs
Thread-Index: AQHPjzQw63qWoE6x1EayLtbqjDKoK5uA3GKAgAEv9gA=
Date: Wed, 25 Jun 2014 16:48:58 +0000
Message-ID: <10C3A8B6-17B2-43BD-BC25-8534D7FB9EA8@webtrends.com>
References: <B99A8786-9BD6-43E7-8727-FE8E07147106@webtrends.com>
 <CAAqHKj5bEvkLLzgVCz4bs4jcGMCByvfRdpGsstUZa2qfzcDqwQ@mail.gmail.com>
In-Reply-To: <CAAqHKj5bEvkLLzgVCz4bs4jcGMCByvfRdpGsstUZa2qfzcDqwQ@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.61.2.4]
Content-Type: text/plain; charset="Windows-1252"
Content-ID: <20C8749EF701E84D8D21F3453BF7C87C@WebTrends.com>
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Yep exactly!  I=92m not sure how complicated it would be to pull off.  If s=
omeone wouldn=92t mind helping to get me pointed in the right direction I w=
ould be happy to look into and contribute this functionality.  I imagine th=
is would be implemented in the scheduler codebase and there would be some s=
ort of rebalance configuration property to enable it possibly?

Does anyone else have any thoughts on this?

Cheers,

Sean


On Jun 24, 2014, at 4:41 PM, Mayur Rustagi <mayur.rustagi@gmail.com> wrote:

> This would be really useful. Especially for Shark where shift of
> partitioning effects all subsequent queries unless task scheduling time
> beats spark.locality.wait. Can cause overall low performance for all
> subsequent tasks.
>=20
> Mayur Rustagi
> Ph: +1 (760) 203 3257
> http://www.sigmoidanalytics.com
> @mayur_rustagi <https://twitter.com/mayur_rustagi>
>=20
>=20
>=20
> On Tue, Jun 24, 2014 at 4:10 AM, Sean McNamara <Sean.McNamara@webtrends.c=
om>
> wrote:
>=20
>> We have a use case where we=92d like something to execute once on each n=
ode
>> and I thought it would be good to ask here.
>>=20
>> Currently we achieve this by setting the parallelism to the number of
>> nodes and use a mod partitioner:
>>=20
>> val balancedRdd =3D sc.parallelize(
>>        (0 until Settings.parallelism)
>>        .map(id =3D> id -> Settings.settings)
>>      ).partitionBy(new ModPartitioner(Settings.parallelism))
>>      .cache()
>>=20
>>=20
>> This works great except in two instances where it can become unbalanced:
>>=20
>> 1. if a worker is restarted or dies, the partition will move to a
>> different node (one of the nodes will run two tasks).  When the worker
>> rejoins, is there a way to have a partition move back over to the newly
>> restarted worker so that it=92s balanced again?
>>=20
>> 2. drivers need to be started in a staggered fashion, otherwise one driv=
er
>> can launch two tasks on one set of workers, and the other driver will do
>> the same with the other set.  Are there any scheduler/config semantics s=
o
>> that each driver will take one (and only one) core from *each* node?
>>=20
>>=20
>> Thanks
>>=20
>> Sean
>>=20
>>=20
>>=20
>>=20
>>=20
>>=20
>>=20


From dev-return-8106-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 25 19:56:10 2014
Return-Path: <dev-return-8106-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A7D7C11537
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Jun 2014 19:56:10 +0000 (UTC)
Received: (qmail 36092 invoked by uid 500); 25 Jun 2014 19:56:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36033 invoked by uid 500); 25 Jun 2014 19:56:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36020 invoked by uid 99); 25 Jun 2014 19:56:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Jun 2014 19:56:09 +0000
X-ASF-Spam-Status: No, hits=0.7 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.192.174] (HELO mail-pd0-f174.google.com) (209.85.192.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Jun 2014 19:56:02 +0000
Received: by mail-pd0-f174.google.com with SMTP id y10so2045428pdj.19
        for <dev@spark.apache.org>; Wed, 25 Jun 2014 12:55:37 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:sender:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=q1LtwIXVp2E5uOS2EEvLq1X8e8raGb0bI0k+KeMstMc=;
        b=K2f0x6mxARlB2xtFzGnsWHSRcGEdwrCUuBer7woixCJgiWhp92s5uxiHijrKrN0jne
         TjHxWrxPiQ1qt+iU39FkyrajB4VqMx4LLqzDwEwVy1vEKUqF7csrOiyiOc7OiKzwChsB
         rJTF/Gsc2Ho9CIaCLH2yH2hVoIFYEYTBq2aCiFYnO0mHKLQ1QXBlfZ5J2cjXxWcnYV1t
         r+oQUVHZW2XyEIWxtXWe4syoh4hk9BZEEkjPpjOZnoiZojzKf1d+ExIUFaI8us5X/jZt
         zmuko4QYUkKJr7ThZLiSmF+HDg6v1IE70iAmFXnlPMCV5H5u4c4hV/w2k/dErTle6FdJ
         2QEw==
X-Gm-Message-State: ALoCoQkSPiNJMqXREnLUv5pqGsNA1E+cJPPuiDX+TE8QvsfSiLMqEFZroeTWRxX9WoFrhfZJBt0z
MIME-Version: 1.0
X-Received: by 10.66.119.172 with SMTP id kv12mr15011568pab.34.1403726137713;
 Wed, 25 Jun 2014 12:55:37 -0700 (PDT)
Sender: mark@coactus.com
Received: by 10.70.22.134 with HTTP; Wed, 25 Jun 2014 12:55:37 -0700 (PDT)
X-Originating-IP: [192.0.216.13]
In-Reply-To: <CALcoZirzViqoEDVjyM_GTN=PV=knPQYsx+OHTghRkMb7OTtRww@mail.gmail.com>
References: <CALcoZiqBFbcU=tdTLosT+x5DfzSxyeUvjFYDmGMXxTigvKNhhg@mail.gmail.com>
	<etPan.53a34084.6b8b4567.100@dhcp-47-206.eecs.berkeley.edu>
	<CALcoZirzViqoEDVjyM_GTN=PV=knPQYsx+OHTghRkMb7OTtRww@mail.gmail.com>
Date: Wed, 25 Jun 2014 15:55:37 -0400
X-Google-Sender-Auth: wmkk5HdxjXyaIQq7AVHa7nQ4w48
Message-ID: <CALcoZirrUsCmRYYOeMn+Kh9fKrSzCEbfZ3EWXUmKznGo+QyDOQ@mail.gmail.com>
Subject: Re: Problems with Pyspark + Dill tests
From: Mark Baker <distobj@acm.org>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hey,

On Mon, Jun 23, 2014 at 5:27 PM, Mark Baker <distobj@acm.org> wrote:
> Thanks for the context, Josh.
>
> I've gone ahead and created a new test case and just opened a new issue;
>
> https://github.com/uqfoundation/dill/issues/49

So that one's dealt with; it was a sys.prefix issue with me using a
virtualenv and was fixed in a soon-to-be pull request a couple weeks
ago.

With that patch applied though, I'm now running into other doctest
issues, these involving serializing Py4J objects, and again, only
occurring inside doctests, not from the shell. I've been unable to
distill this one down to a compact test case, nor gain any insight to
the cause, and could really use a nudge in the right direction.
Thanks!

Top and bottom of sample trace (excluded middle is the usual recursive
pickling calls);

File "pyspark/rdd.py", line 1487, in __main__.PipelinedRDD
Failed example:
    rdd.map(lambda x: 2 * x).cache().map(lambda x: 2 * x).collect()
Exception raised:
    Traceback (most recent call last):
      File "/usr/lib/python2.7/doctest.py", line 1289, in __run
        compileflags, 1) in test.globs
      File "<doctest __main__.PipelinedRDD[1]>", line 1, in <module>
        rdd.map(lambda x: 2 * x).cache().map(lambda x: 2 * x).collect()
      File "/home/mbaker/venvs/bibframe/src/spark-python3/python/pyspark/rdd.py",
line 201, in cache
        self._jrdd.cache()
      File "/home/mbaker/venvs/bibframe/src/spark-python3/python/pyspark/rdd.py",
line 1531, in _jrdd
        pickled_command = DillSerializer().dumps(command)
      File "/home/mbaker/venvs/bibframe/src/spark-python3/python/pyspark/serializers.py",
line 284, in dumps
        def dumps(self, obj): return dill.dumps(obj, 2)
      File "/home/mbaker/venvs/bibframe/local/lib/python2.7/site-packages/dill-0.2.2.dev-py2.7.egg/dill/dill.py",
line 169, in dumps
        dump(obj, file, protocol, byref)
      File "/home/mbaker/venvs/bibframe/local/lib/python2.7/site-packages/dill-0.2.2.dev-py2.7.egg/dill/dill.py",
line 162, in dump
        pik.dump(obj)
      File "/usr/lib/python2.7/pickle.py", line 224, in dump
        self.save(obj)
...
      File "/home/mbaker/venvs/bibframe/local/lib/python2.7/site-packages/dill-0.2.2.dev-py2.7.egg/dill/dill.py",
line 543, in save_module_dict
        StockPickler.save_dict(pickler, obj)
      File "/usr/lib/python2.7/pickle.py", line 650, in save_dict
        self._batch_setitems(obj.iteritems())
      File "/usr/lib/python2.7/pickle.py", line 682, in _batch_setitems
        save(v)
      File "/usr/lib/python2.7/pickle.py", line 307, in save
        rv = reduce(self.proto)
      File "/home/mbaker/venvs/bibframe/src/spark-python3/python/lib/py4j-0.8.1-src.zip/py4j/java_gateway.py",
line 537, in __call__
        self.target_id, self.name)
      File "/home/mbaker/venvs/bibframe/src/spark-python3/python/lib/py4j-0.8.1-src.zip/py4j/protocol.py",
line 304, in get_return_value
        format(target_id, '.', name, value))
    Py4JError: An error occurred while calling o15.__getnewargs__. Trace:
    py4j.Py4JException: Method __getnewargs__([]) does not exist
        at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:333)
        at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:342)
        at py4j.Gateway.invoke(Gateway.java:251)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:207)
        at java.lang.Thread.run(Thread.java:745)

From dev-return-8107-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jun 25 20:38:03 2014
Return-Path: <dev-return-8107-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6FEAF1175B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Jun 2014 20:38:03 +0000 (UTC)
Received: (qmail 45318 invoked by uid 500); 25 Jun 2014 20:38:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45258 invoked by uid 500); 25 Jun 2014 20:38:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45241 invoked by uid 99); 25 Jun 2014 20:38:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Jun 2014 20:38:02 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of codedeft@gmail.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Jun 2014 20:37:59 +0000
Received: by mail-wi0-f175.google.com with SMTP id r20so8632595wiv.2
        for <dev@spark.apache.org>; Wed, 25 Jun 2014 13:37:35 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:from:date:message-id
         :subject:to:cc:content-type;
        bh=8ObioMk6Ej+HcR4SmPswbmdUkbfFB5OAkUlRBE8v7gY=;
        b=jNCkeFWJlNFLqIqjCRxs2igBViOB7bbzgElsAS117EWQQOu+EsaybYAMoBTMljE6sw
         kmSK5Lfmlxw5E4CkeY9nCKhORljs36ywyvcamGUfTNZtb4o/qrlPmZhDlVQUd4QptIty
         TdRSBpmdwtGjBf+Wae5Dm7h9cMYBUY4d6UmsXmSjnApqylTKQ+wCgVA4hmcd0M0jFvMO
         ZQrfPhC74zl6Dn8go3LDN1w0fghJ5/pyk+EHQANX88qWZxZ6bP6ftSLUmeJxbAs8hebn
         kjwqvX83oxPPJd5b4zKLAQZkVNvFtTC0IYLoJ/yf5LoGy0r44I1bYSzMzdyhqLB+YOIn
         m5Pg==
X-Received: by 10.180.100.41 with SMTP id ev9mr44374619wib.22.1403728655238;
 Wed, 25 Jun 2014 13:37:35 -0700 (PDT)
MIME-Version: 1.0
Sender: codedeft@gmail.com
Received: by 10.216.40.68 with HTTP; Wed, 25 Jun 2014 13:36:55 -0700 (PDT)
In-Reply-To: <CFC6248F.577%xwei@palantir.com>
References: <CFC6248F.577%xwei@palantir.com>
From: Sung Hwan Chung <codedeft@cs.stanford.edu>
Date: Wed, 25 Jun 2014 13:36:55 -0700
X-Google-Sender-Auth: 5o7NOjJS6tQU-7ounei3St2bzlU
Message-ID: <CAP7bEL1W+B=qdj8bWhDMRApJb5QVqxE1rfxt23wTKSYJbt27Fg@mail.gmail.com>
Subject: Re: Contributing to MLlib on GLM
To: dev@spark.apache.org
Cc: Jason Zhao <jasonz@palantir.com>
Content-Type: multipart/alternative; boundary=f46d044402f4db4dbe04fcaf0b37
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d044402f4db4dbe04fcaf0b37
Content-Type: text/plain; charset=UTF-8

Well, as you said, MLLib already supports GLM in a sense. Except they only
support two link functions - identity (linear regression) and logit
(logistic regression). It should not be too hard to add other link
functions, as all you have to do is add a different gradient function for
Poisson/Gamma, etc - look at Gradient.scala in mllib.


On Tue, Jun 17, 2014 at 5:00 PM, Xiaokai Wei <xwei@palantir.com> wrote:

> Hi,
>
> I am an intern at PalantirTech and we are building some stuff on top of
> MLlib. In Particular, GLM is of great interest to us.  Though
> GeneralizedLinearModel in MLlib 1.0.0 has some important GLMs such as
> Logistic Regression, Linear Regression, some other important GLMs like
> Poisson Regression are still missing.
>
> I am curious that if anyone is already working on other GLMs (e.g.
> Poisson, Gamma). If not, we would like to contribute to MLlib on GLM. Is
> adding more GLMs on the roadmap of MLlib?
>
>
> Sincerely,
>
> Xiaokai
>

--f46d044402f4db4dbe04fcaf0b37--

From dev-return-8108-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun 26 06:21:21 2014
Return-Path: <dev-return-8108-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0D36D11638
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Jun 2014 06:21:21 +0000 (UTC)
Received: (qmail 25701 invoked by uid 500); 26 Jun 2014 06:21:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25646 invoked by uid 500); 26 Jun 2014 06:21:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25621 invoked by uid 99); 26 Jun 2014 06:21:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Jun 2014 06:21:20 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rosenville@gmail.com designates 209.85.160.42 as permitted sender)
Received: from [209.85.160.42] (HELO mail-pb0-f42.google.com) (209.85.160.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Jun 2014 06:21:17 +0000
Received: by mail-pb0-f42.google.com with SMTP id ma3so2731497pbc.1
        for <dev@spark.apache.org>; Wed, 25 Jun 2014 23:20:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=BlK4sA+MSoJcFA7RWo03AzBJrf4S5VZ17IUMNRmGe+M=;
        b=Jv3lyvhkDuB+H46CnCVQlYIus6LPyT52JUPA4y2BXTcd0prYPpCvWk0+UksiqnwmkQ
         FcU26t6Lt1gEvyIBDzHE5Dd1I7M3Zzk3LeQQJc81x2QwDyTdh6nfw/JrNZFt+s09GQio
         5tmOHUyT9LIs6RfqtE5OoFoI3vFUsHoRcjVen9eQBycPngf/C/GJiMQB2UzN7g6QWj0b
         daHXxAUi/xOrgfxMKTTwCQ3pYuqjqNu0pBgDGKS0oSo3v9VMf3NV29TegPqfNmUXjIMH
         Wc+XHNBTVf8HOZgu/YuGhYSURP2Dq3D8EFDVTPjWwTW01wokBSdjeRAFvHlsDyssQvhU
         /aYw==
X-Received: by 10.68.164.4 with SMTP id ym4mr18402916pbb.53.1403763652803;
        Wed, 25 Jun 2014 23:20:52 -0700 (PDT)
Received: from dhcp-47-206.eecs.berkeley.edu (dhcp-47-206.EECS.Berkeley.EDU. [128.32.47.206])
        by mx.google.com with ESMTPSA id gd7sm29566551pac.34.2014.06.25.23.20.52
        for <dev@spark.apache.org>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 25 Jun 2014 23:20:52 -0700 (PDT)
Date: Wed, 25 Jun 2014 23:20:53 -0700
From: Josh Rosen <rosenville@gmail.com>
To: dev@spark.apache.org
Message-ID: <etPan.53abbbc5.7cd1aac5.da@dhcp-47-206.eecs.berkeley.edu>
In-Reply-To: <CALcoZirrUsCmRYYOeMn+Kh9fKrSzCEbfZ3EWXUmKznGo+QyDOQ@mail.gmail.com>
References: <CALcoZiqBFbcU=tdTLosT+x5DfzSxyeUvjFYDmGMXxTigvKNhhg@mail.gmail.com>
 <etPan.53a34084.6b8b4567.100@dhcp-47-206.eecs.berkeley.edu>
 <CALcoZirzViqoEDVjyM_GTN=PV=knPQYsx+OHTghRkMb7OTtRww@mail.gmail.com>
 <CALcoZirrUsCmRYYOeMn+Kh9fKrSzCEbfZ3EWXUmKznGo+QyDOQ@mail.gmail.com>
Subject: Re: Problems with Pyspark + Dill tests
X-Mailer: Airmail Beta (242)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53abbbc5_738eb36f_da"
X-Virus-Checked: Checked by ClamAV on apache.org

--53abbbc5_738eb36f_da
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

The problem seems to be that unpicklable RDD objects are being pulled int=
o function closures. =C2=A0In your failing dockets, it looks like the=C2=A0=
rdd created through sc.parallelize is being pulled into the map lambda=E2=
=80=99s function closure.

I opened a new Dill bug with a small test case that reproduces this issue=
:=C2=A0https://github.com/uqfoundation/dill/issues/50.

I tried manually modifying dill to drop =60rdd=60 and =60sc=60 variables =
from function globals dicts, which seemed to solve the problem, but that=E2=
=80=99s not a general-purpose fix.
On June 25, 2014 at 12:56:10 PM, Mark Baker (distobj=40acm.org) wrote:

Hey, =20

On Mon, Jun 23, 2014 at 5:27 PM, Mark Baker <distobj=40acm.org> wrote: =20
> Thanks for the context, Josh. =20
> =20
> I've gone ahead and created a new test case and just opened a new issue=
; =20
> =20
> https://github.com/uqfoundation/dill/issues/49 =20

So that one's dealt with; it was a sys.prefix issue with me using a =20
virtualenv and was fixed in a soon-to-be pull request a couple weeks =20
ago. =20

With that patch applied though, I'm now running into other doctest =20
issues, these involving serializing Py4J objects, and again, only =20
occurring inside doctests, not from the shell. I've been unable to =20
distill this one down to a compact test case, nor gain any insight to =20
the cause, and could really use a nudge in the right direction. =20
Thanks=21 =20

Top and bottom of sample trace (excluded middle is the usual recursive =20
pickling calls); =20

=46ile =22pyspark/rdd.py=22, line 1487, in =5F=5Fmain=5F=5F.PipelinedRDD =
=20
=46ailed example: =20
rdd.map(lambda x: 2 * x).cache().map(lambda x: 2 * x).collect() =20
Exception raised: =20
Traceback (most recent call last): =20
=46ile =22/usr/lib/python2.7/doctest.py=22, line 1289, in =5F=5Frun =20
compileflags, 1) in test.globs =20
=46ile =22<doctest =5F=5Fmain=5F=5F.PipelinedRDD=5B1=5D>=22, line 1, in <=
module> =20
rdd.map(lambda x: 2 * x).cache().map(lambda x: 2 * x).collect() =20
=46ile =22/home/mbaker/venvs/bibframe/src/spark-python3/python/pyspark/rd=
d.py=22, =20
line 201, in cache =20
self.=5Fjrdd.cache() =20
=46ile =22/home/mbaker/venvs/bibframe/src/spark-python3/python/pyspark/rd=
d.py=22, =20
line 1531, in =5Fjrdd =20
pickled=5Fcommand =3D DillSerializer().dumps(command) =20
=46ile =22/home/mbaker/venvs/bibframe/src/spark-python3/python/pyspark/se=
rializers.py=22, =20
line 284, in dumps =20
def dumps(self, obj): return dill.dumps(obj, 2) =20
=46ile =22/home/mbaker/venvs/bibframe/local/lib/python2.7/site-packages/d=
ill-0.2.2.dev-py2.7.egg/dill/dill.py=22, =20
line 169, in dumps =20
dump(obj, file, protocol, byref) =20
=46ile =22/home/mbaker/venvs/bibframe/local/lib/python2.7/site-packages/d=
ill-0.2.2.dev-py2.7.egg/dill/dill.py=22, =20
line 162, in dump =20
pik.dump(obj) =20
=46ile =22/usr/lib/python2.7/pickle.py=22, line 224, in dump =20
self.save(obj) =20
... =20
=46ile =22/home/mbaker/venvs/bibframe/local/lib/python2.7/site-packages/d=
ill-0.2.2.dev-py2.7.egg/dill/dill.py=22, =20
line 543, in save=5Fmodule=5Fdict =20
StockPickler.save=5Fdict(pickler, obj) =20
=46ile =22/usr/lib/python2.7/pickle.py=22, line 650, in save=5Fdict =20
self.=5Fbatch=5Fsetitems(obj.iteritems()) =20
=46ile =22/usr/lib/python2.7/pickle.py=22, line 682, in =5Fbatch=5Fsetite=
ms =20
save(v) =20
=46ile =22/usr/lib/python2.7/pickle.py=22, line 307, in save =20
rv =3D reduce(self.proto) =20
=46ile =22/home/mbaker/venvs/bibframe/src/spark-python3/python/lib/py4j-0=
.8.1-src.zip/py4j/java=5Fgateway.py=22, =20
line 537, in =5F=5Fcall=5F=5F =20
self.target=5Fid, self.name) =20
=46ile =22/home/mbaker/venvs/bibframe/src/spark-python3/python/lib/py4j-0=
.8.1-src.zip/py4j/protocol.py=22, =20
line 304, in get=5Freturn=5Fvalue =20
format(target=5Fid, '.', name, value)) =20
Py4JError: An error occurred while calling o15.=5F=5Fgetnewargs=5F=5F. Tr=
ace: =20
py4j.Py4JException: Method =5F=5Fgetnewargs=5F=5F(=5B=5D) does not exist =
=20
at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:333) =
=20
at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:342) =
=20
at py4j.Gateway.invoke(Gateway.java:251) =20
at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) =20
at py4j.commands.CallCommand.execute(CallCommand.java:79) =20
at py4j.GatewayConnection.run(GatewayConnection.java:207) =20
at java.lang.Thread.run(Thread.java:745) =20

--53abbbc5_738eb36f_da--


From dev-return-8109-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jun 26 23:43:21 2014
Return-Path: <dev-return-8109-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 08BD81188E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Jun 2014 23:43:21 +0000 (UTC)
Received: (qmail 65984 invoked by uid 500); 26 Jun 2014 23:43:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65941 invoked by uid 500); 26 Jun 2014 23:43:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64837 invoked by uid 99); 26 Jun 2014 23:43:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Jun 2014 23:43:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mayur.rustagi@gmail.com designates 74.125.82.179 as permitted sender)
Received: from [74.125.82.179] (HELO mail-we0-f179.google.com) (74.125.82.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Jun 2014 23:43:17 +0000
Received: by mail-we0-f179.google.com with SMTP id w62so4503053wes.38
        for <multiple recipients>; Thu, 26 Jun 2014 16:42:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=+XuAEWvWi04v7KWBqaC6/ARP2OFC8SQ4w877wEQ9olQ=;
        b=mH4rhO0Pn24MlFFCUz0ZlQmwyVBjoQsGWyuPaa7p1ctnXufEGApfWrnJcSr2t2BGfy
         W/o+KKfAvt647yJDc6R1FLZwSl2oFoauFNU+ioWevxUDwbX8qWjBxaTD6h5d/FO41hgv
         hJnOcI8id1P5MZgBjUX9K3wbF/G9+Ua3wo+5tkI6z/BKy4XgeGTiTuT8+eWkBrvQGyK3
         4lCBdYu0kf1ZuJ3hHyCkuWbF+aYzvo/r7OGsRkjwplKYJl8qBlXldzXb9t5an7tqijBh
         OUtGfHGLYy5W1yxTWa/bjLrelGX2/gb3ROUNkkcZIKhUbsgKdO7KFnYXyWsLJIFEAdC1
         Hdow==
X-Received: by 10.180.184.198 with SMTP id ew6mr7770621wic.13.1403826173384;
 Thu, 26 Jun 2014 16:42:53 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.202.137 with HTTP; Thu, 26 Jun 2014 16:42:33 -0700 (PDT)
From: Mayur Rustagi <mayur.rustagi@gmail.com>
Date: Fri, 27 Jun 2014 05:12:33 +0530
Message-ID: <CAAqHKj4VuwO1c0z4GX6Ats=nvznC-PkKgmXu9W2tS74Y1cm-xg@mail.gmail.com>
Subject: Google Cloud Engine adds out of the box Spark/Shark support
To: user@spark.apache.org, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c3348c641d7404fcc5c03c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3348c641d7404fcc5c03c
Content-Type: text/plain; charset=UTF-8

https://groups.google.com/forum/#!topic/gcp-hadoop-announce/EfQms8tK5cE

I suspect they are using thr own builds.. has anybody had a chance to look
at it?

Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>

--001a11c3348c641d7404fcc5c03c--

From dev-return-8110-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 00:16:59 2014
Return-Path: <dev-return-8110-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D439411999
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 00:16:59 +0000 (UTC)
Received: (qmail 32342 invoked by uid 500); 27 Jun 2014 00:16:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32286 invoked by uid 500); 27 Jun 2014 00:16:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32269 invoked by uid 99); 27 Jun 2014 00:16:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 00:16:59 +0000
X-ASF-Spam-Status: No, hits=2.7 required=10.0
	tests=HTML_MESSAGE,MISSING_HEADERS,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mayur.rustagi@gmail.com designates 209.85.212.171 as permitted sender)
Received: from [209.85.212.171] (HELO mail-wi0-f171.google.com) (209.85.212.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 00:16:55 +0000
Received: by mail-wi0-f171.google.com with SMTP id n15so1989215wiw.10
        for <dev@spark.apache.org>; Thu, 26 Jun 2014 17:16:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:cc
         :content-type;
        bh=9Qq/dTEIP6eOfe03FVyUvMQq3d3xc56eMB38pRQvpKk=;
        b=VtvLdyuu39hbqkWcpRRRsVD1OSp2cYT2qEYgXfmGNqQxaiSOzgQF4s/782qbQW4FaY
         t7BzFsP2FVIbQWxl5eJMceCeDzFMppbJgzPQnZ99bUrpUMYr3cOV7FIag+HX5jbIzntW
         Unt4F6Rh/6FjKySSlQ0jaBpqx4IBMvtGThrQERKZKYj18i3njDXNTUJXHg8vulrqQ6JZ
         cNUhDR43sWQrJ/FxQeeGo7bbp1jCTXUwmhjH7sptjMahXpC7J7GE1nv92g6AEpmaSWkt
         qhpHplcSpZ8REot1FrSW5a/pLlCkHFDZGZg5Xqfy3F0P7nhSqywut96AI82HlrL+efEO
         FVJA==
X-Received: by 10.194.77.238 with SMTP id v14mr21406221wjw.103.1403828193859;
 Thu, 26 Jun 2014 17:16:33 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.202.137 with HTTP; Thu, 26 Jun 2014 17:16:13 -0700 (PDT)
In-Reply-To: <10C3A8B6-17B2-43BD-BC25-8534D7FB9EA8@webtrends.com>
References: <B99A8786-9BD6-43E7-8727-FE8E07147106@webtrends.com>
 <CAAqHKj5bEvkLLzgVCz4bs4jcGMCByvfRdpGsstUZa2qfzcDqwQ@mail.gmail.com> <10C3A8B6-17B2-43BD-BC25-8534D7FB9EA8@webtrends.com>
From: Mayur Rustagi <mayur.rustagi@gmail.com>
Date: Fri, 27 Jun 2014 05:46:13 +0530
Message-ID: <CAAqHKj6SN7wLOZjR_gViMqmR1LZm7EDB_Sf7mRD-JMvnbYqNPw@mail.gmail.com>
Subject: Re: balancing RDDs
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bfd021cd23f9904fcc63856
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfd021cd23f9904fcc63856
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I would imagine this would be an extension of SchemaRDD (for Sparksql)  or
a new RDD altogether.
The RDD location is determined based on where task generating the RDD is
scheduled, the scheduler schedules basis of input RDD/sourcedata location.
So ideally RDD codebase needs to check location of input partition across
nodes & scheduling  preference of task related to unbalanced partition to
different nodes.. I am not sure if RDD can influence location of tasks
/partition location.


Mayur Rustagi
Ph: +1 (760) 203 3257
http://www.sigmoidanalytics.com
@mayur_rustagi <https://twitter.com/mayur_rustagi>



On Wed, Jun 25, 2014 at 10:18 PM, Sean McNamara <Sean.McNamara@webtrends.co=
m
> wrote:

> Yep exactly!  I=E2=80=99m not sure how complicated it would be to pull of=
f.  If
> someone wouldn=E2=80=99t mind helping to get me pointed in the right dire=
ction I
> would be happy to look into and contribute this functionality.  I imagine
> this would be implemented in the scheduler codebase and there would be so=
me
> sort of rebalance configuration property to enable it possibly?
>
> Does anyone else have any thoughts on this?
>
> Cheers,
>
> Sean
>
>
> On Jun 24, 2014, at 4:41 PM, Mayur Rustagi <mayur.rustagi@gmail.com>
> wrote:
>
> > This would be really useful. Especially for Shark where shift of
> > partitioning effects all subsequent queries unless task scheduling time
> > beats spark.locality.wait. Can cause overall low performance for all
> > subsequent tasks.
> >
> > Mayur Rustagi
> > Ph: +1 (760) 203 3257
> > http://www.sigmoidanalytics.com
> > @mayur_rustagi <https://twitter.com/mayur_rustagi>
> >
> >
> >
> > On Tue, Jun 24, 2014 at 4:10 AM, Sean McNamara <
> Sean.McNamara@webtrends.com>
> > wrote:
> >
> >> We have a use case where we=E2=80=99d like something to execute once o=
n each
> node
> >> and I thought it would be good to ask here.
> >>
> >> Currently we achieve this by setting the parallelism to the number of
> >> nodes and use a mod partitioner:
> >>
> >> val balancedRdd =3D sc.parallelize(
> >>        (0 until Settings.parallelism)
> >>        .map(id =3D> id -> Settings.settings)
> >>      ).partitionBy(new ModPartitioner(Settings.parallelism))
> >>      .cache()
> >>
> >>
> >> This works great except in two instances where it can become unbalance=
d:
> >>
> >> 1. if a worker is restarted or dies, the partition will move to a
> >> different node (one of the nodes will run two tasks).  When the worker
> >> rejoins, is there a way to have a partition move back over to the newl=
y
> >> restarted worker so that it=E2=80=99s balanced again?
> >>
> >> 2. drivers need to be started in a staggered fashion, otherwise one
> driver
> >> can launch two tasks on one set of workers, and the other driver will =
do
> >> the same with the other set.  Are there any scheduler/config semantics
> so
> >> that each driver will take one (and only one) core from *each* node?
> >>
> >>
> >> Thanks
> >>
> >> Sean
> >>
> >>
> >>
> >>
> >>
> >>
> >>
>
>

--047d7bfd021cd23f9904fcc63856--

From dev-return-8111-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 00:47:00 2014
Return-Path: <dev-return-8111-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E8B7311A42
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 00:47:00 +0000 (UTC)
Received: (qmail 94865 invoked by uid 500); 27 Jun 2014 00:47:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94806 invoked by uid 500); 27 Jun 2014 00:47:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94794 invoked by uid 99); 27 Jun 2014 00:47:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 00:47:00 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of Bert.Greevenbosch@huawei.com designates 119.145.14.64 as permitted sender)
Received: from [119.145.14.64] (HELO szxga01-in.huawei.com) (119.145.14.64)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 00:46:54 +0000
Received: from 172.24.2.119 (EHLO SZXEMA405-HUB.china.huawei.com) ([172.24.2.119])
	by szxrg01-dlp.huawei.com (MOS 4.3.7-GA FastPath queued)
	with ESMTP id BXU99120;
	Fri, 27 Jun 2014 08:46:32 +0800 (CST)
Received: from SZXEMA510-MBX.china.huawei.com ([169.254.3.190]) by
 SZXEMA405-HUB.china.huawei.com ([10.82.72.37]) with mapi id 14.03.0158.001;
 Fri, 27 Jun 2014 08:46:30 +0800
From: Bert Greevenbosch <Bert.Greevenbosch@huawei.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Artificial Neural Network in Spark?
Thread-Topic: Artificial Neural Network in Spark?
Thread-Index: Ac+RoTqSDV42paQFQ+OUNd1O9igqiA==
Date: Fri, 27 Jun 2014 00:46:29 +0000
Message-ID: <46A1DF3F04371240B504290A071B4DB63E632EDC@SZXEMA510-MBX.china.huawei.com>
Accept-Language: en-GB, zh-CN, en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [10.66.162.63]
Content-Type: multipart/alternative;
	boundary="_000_46A1DF3F04371240B504290A071B4DB63E632EDCSZXEMA510MBXchi_"
MIME-Version: 1.0
X-CFilter-Loop: Reflected
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_46A1DF3F04371240B504290A071B4DB63E632EDCSZXEMA510MBXchi_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Hello all,

I was wondering whether Spark/mllib supports Artificial Neural Networks (AN=
Ns)?

If not, I am currently working on an implementation of it. I re-use the cod=
e for linear regression and gradient descent as much as possible.

Would the community be interested in such implementation? Or maybe somebody=
 is already working on it?

Best regards,
Bert

--_000_46A1DF3F04371240B504290A071B4DB63E632EDCSZXEMA510MBXchi_--

From dev-return-8112-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 01:04:26 2014
Return-Path: <dev-return-8112-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 78F9311AA9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 01:04:26 +0000 (UTC)
Received: (qmail 20249 invoked by uid 500); 27 Jun 2014 01:04:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20191 invoked by uid 500); 27 Jun 2014 01:04:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20166 invoked by uid 99); 27 Jun 2014 01:04:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 01:04:25 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of weixiaokai@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 01:04:21 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <weixiaokai@gmail.com>)
	id 1X0Ka8-0007ql-GG
	for dev@spark.incubator.apache.org; Thu, 26 Jun 2014 18:04:00 -0700
Date: Thu, 26 Jun 2014 18:04:00 -0700 (PDT)
From: xwei <weixiaokai@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1403831040483-7088.post@n3.nabble.com>
In-Reply-To: <CAP7bEL1W+B=qdj8bWhDMRApJb5QVqxE1rfxt23wTKSYJbt27Fg@mail.gmail.com>
References: <CFC6248F.577%xwei@palantir.com> <CAP7bEL1W+B=qdj8bWhDMRApJb5QVqxE1rfxt23wTKSYJbt27Fg@mail.gmail.com>
Subject: Re: Contributing to MLlib on GLM
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Yes, that's what we did: adding two gradient functions to Gradient.scala and
create PoissonRegression and GammaRegression using these gradients. We made
a PR on this.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-on-GLM-tp7033p7088.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8113-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 02:07:13 2014
Return-Path: <dev-return-8113-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9BFB811BE4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 02:07:13 +0000 (UTC)
Received: (qmail 19509 invoked by uid 500); 27 Jun 2014 02:07:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19446 invoked by uid 500); 27 Jun 2014 02:07:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19425 invoked by uid 99); 27 Jun 2014 02:07:12 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 02:07:12 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.178 as permitted sender)
Received: from [209.85.214.178] (HELO mail-ob0-f178.google.com) (209.85.214.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 02:07:07 +0000
Received: by mail-ob0-f178.google.com with SMTP id wn1so4872060obc.23
        for <dev@spark.apache.org>; Thu, 26 Jun 2014 19:06:46 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=JBXbgNtIhnJkCvglGP93To/IpJRQAUlSmJyKDB/r1qI=;
        b=iPxhyjM4m2ctEhj8xk5ArK/g6yJEXk20VQ84ENGV+UaNOrL5m9921Fy+/qmectTn3h
         4w8O0HGedXQzw6Z1S1L7hgupeThrFz5GHqg5cC1xtkMYCxE55sxuD6Tqdp3mroCJmdu8
         +HJrRw+6ScT96NTeQtOu1kj1n202upG4J9L37l6rRPhyapK/unOpiDzZcmz6GSg/mIYY
         IYnNScr3Nz3YkZwmhADtVoPmaLu0QDLStnCSTtKWhRcLr8RabZFoVOc8JXq/O5PslLa/
         +PsYV0KQiHYBnhA/SbmA8DM8UuMDfead8mLwYfeSbCr/J4yH2uSE9FAo8hekUAVRV93U
         RBlw==
MIME-Version: 1.0
X-Received: by 10.182.199.5 with SMTP id jg5mr7544556obc.75.1403834806654;
 Thu, 26 Jun 2014 19:06:46 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Thu, 26 Jun 2014 19:06:46 -0700 (PDT)
Date: Thu, 26 Jun 2014 19:06:46 -0700
Message-ID: <CABPQxsvAioKpE29GzUPts6tpk=6dSnXpfXMCYwKKX2fKX09Udw@mail.gmail.com>
Subject: [VOTE] Release Apache Spark 1.0.1 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Please vote on releasing the following candidate as Apache Spark version 1.0.1!

The tag to be voted on is v1.0.1-rc1 (commit 7feeda3):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7feeda3d729f9397aa15ee8750c01ef5aa601962

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.0.1-rc1/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1020/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.0.1-rc1-docs/

Please vote on releasing this package as Apache Spark 1.0.1!

The vote is open until Monday, June 30, at 03:00 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.0.1
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

=== About this release ===
This release fixes a few high-priority bugs in 1.0 and has a variety
of smaller fixes. The full list is here: http://s.apache.org/b45. Some
of the more visible patches are:

SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame size.
SPARK-1790: Support r3 instance types on EC2.

This is the first maintenance release on the 1.0 line. We plan to make
additional maintenance releases as new fixes come in.

- Patrick

From dev-return-8114-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 02:43:56 2014
Return-Path: <dev-return-8114-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 61E0311CD5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 02:43:56 +0000 (UTC)
Received: (qmail 73459 invoked by uid 500); 27 Jun 2014 02:43:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73392 invoked by uid 500); 27 Jun 2014 02:43:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73374 invoked by uid 99); 27 Jun 2014 02:43:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 02:43:55 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ron.hu@huawei.com designates 119.145.14.65 as permitted sender)
Received: from [119.145.14.65] (HELO szxga02-in.huawei.com) (119.145.14.65)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 02:43:50 +0000
Received: from 172.24.2.119 (EHLO SZXEML454-HUB.china.huawei.com) ([172.24.2.119])
	by szxrg02-dlp.huawei.com (MOS 4.3.7-GA FastPath queued)
	with ESMTP id BVX12658;
	Fri, 27 Jun 2014 10:43:27 +0800 (CST)
Received: from szxeml558-mbs.china.huawei.com ([169.254.8.208]) by
 SZXEML454-HUB.china.huawei.com ([10.82.67.197]) with mapi id 14.03.0158.001;
 Fri, 27 Jun 2014 10:43:24 +0800
From: "Ron Chung Hu (Ron Hu, ARC)" <ron.hu@huawei.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: IntelliJ IDEA cannot compile TreeNode.scala
Thread-Topic: IntelliJ IDEA cannot compile TreeNode.scala
Thread-Index: Ac+RsY+GtRXlA17OTrubDGHwGKyWYQ==
Date: Fri, 27 Jun 2014 02:43:23 +0000
Message-ID: <54541F80AA2A6F47BDC10C496192CA75D899B9@szxeml558-mbs.china.huawei.com>
Accept-Language: en-US, zh-CN
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [10.193.35.52]
Content-Type: multipart/alternative;
	boundary="_000_54541F80AA2A6F47BDC10C496192CA75D899B9szxeml558mbschina_"
MIME-Version: 1.0
X-CFilter-Loop: Reflected
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_54541F80AA2A6F47BDC10C496192CA75D899B9szxeml558mbschina_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Hi,

I am a Spark newbie.  I just downloaded Spark1.0.0 and latest IntelliJ vers=
ion 13.1 with Scala plug-in.  At spark-1.0.0 top level, I executed the foll=
owing SBT commands and they ran successfully.


-          ./sbt/sbt assembly

-          ./sbt/sbt update gen-idea

After opening IntelliJ IDEA, I tried to compile ...../sql/catalyst/trees/Tr=
eeNode.scala inside IntelliJ.  I got many compile errors such as "cannot re=
solve symbol children", "cannot resolve symbol id".  Actually both symbols =
are defined in the same file.   As Spark was built successfully with "sbt/s=
bt assembly" command, I wondered what went wrong in compiling TreeNode.scal=
a.  Any pointer will be appreciated.

Thanks.

Best,
Ron Hu


--_000_54541F80AA2A6F47BDC10C496192CA75D899B9szxeml558mbschina_--

From dev-return-8115-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 03:58:02 2014
Return-Path: <dev-return-8115-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 085C211E24
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 03:58:02 +0000 (UTC)
Received: (qmail 47259 invoked by uid 500); 27 Jun 2014 03:58:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47180 invoked by uid 500); 27 Jun 2014 03:58:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47168 invoked by uid 99); 27 Jun 2014 03:58:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 03:58:00 +0000
X-ASF-Spam-Status: No, hits=2.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.192.54] (HELO mail-qg0-f54.google.com) (209.85.192.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 03:57:57 +0000
Received: by mail-qg0-f54.google.com with SMTP id q107so3935769qgd.13
        for <dev@spark.apache.org>; Thu, 26 Jun 2014 20:57:32 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=VJeMiRK5F5lGInjRR9xTCR0kGpQ7fmGCb3wh4KrC4kM=;
        b=DvOmzuc7me1OYaNbxdUVtKUqTsGBw1IU+l+5aVr35+RXuGlGZqJAiVBZ5a4vH9cjL9
         krL9gzf+7qISNx3DdgsrKUxuRU1kkc8j63UzxMy0Is9SxMRr1p/xiTxdzD5poKo+u6vG
         DdlCpTKedMO80j+qE7+wxmURSYy3b27m/9hM3FUESyJ6F+aD3/zjy5R9p2wueb8jAxvG
         c/LPEEiuNUHBCkrDgdk0w9yMb9vcSNmXwrpz/y3iUzIMB/GUUiPx4wRWckboyHZIgA86
         m51LTM9dGeUczyiMHqacmNHv5qVYO1MCXIRQ0jlOHL7bWsaZkGSp8MemTAp22yqCrBKI
         Ud1A==
X-Gm-Message-State: ALoCoQm9EFcxx36tHO4VHF+fepy8P/euTCObe6La3GoP7+zFO9HgRRU2YoC4c94z3UsCZPs4tOID
X-Received: by 10.140.26.201 with SMTP id 67mr27387167qgv.51.1403841452241;
 Thu, 26 Jun 2014 20:57:32 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Thu, 26 Jun 2014 20:57:12 -0700 (PDT)
In-Reply-To: <54541F80AA2A6F47BDC10C496192CA75D899B9@szxeml558-mbs.china.huawei.com>
References: <54541F80AA2A6F47BDC10C496192CA75D899B9@szxeml558-mbs.china.huawei.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 26 Jun 2014 20:57:12 -0700
Message-ID: <CAPh_B=bQNio8Cp57te7j4on7ek0OJzP7E6K+PFAmyxnPcMRRaQ@mail.gmail.com>
Subject: Re: IntelliJ IDEA cannot compile TreeNode.scala
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c152fa150ac104fcc94fdf
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c152fa150ac104fcc94fdf
Content-Type: text/plain; charset=UTF-8

IntelliJ parser/analyzer/compiler behaves differently from Scala compiler,
and sometimes lead to inconsistent behavior. This is one of the case.

In general while we use IntelliJ, we don't use it to build stuff. I
personally always build in command line with sbt or Maven.



On Thu, Jun 26, 2014 at 7:43 PM, Ron Chung Hu (Ron Hu, ARC) <
ron.hu@huawei.com> wrote:

> Hi,
>
> I am a Spark newbie.  I just downloaded Spark1.0.0 and latest IntelliJ
> version 13.1 with Scala plug-in.  At spark-1.0.0 top level, I executed the
> following SBT commands and they ran successfully.
>
>
> -          ./sbt/sbt assembly
>
> -          ./sbt/sbt update gen-idea
>
> After opening IntelliJ IDEA, I tried to compile
> ...../sql/catalyst/trees/TreeNode.scala inside IntelliJ.  I got many
> compile errors such as "cannot resolve symbol children", "cannot resolve
> symbol id".  Actually both symbols are defined in the same file.   As Spark
> was built successfully with "sbt/sbt assembly" command, I wondered what
> went wrong in compiling TreeNode.scala.  Any pointer will be appreciated.
>
> Thanks.
>
> Best,
> Ron Hu
>
>

--001a11c152fa150ac104fcc94fdf--

From dev-return-8116-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 04:18:23 2014
Return-Path: <dev-return-8116-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8809D11E84
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 04:18:23 +0000 (UTC)
Received: (qmail 69573 invoked by uid 500); 27 Jun 2014 04:18:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69502 invoked by uid 500); 27 Jun 2014 04:18:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69490 invoked by uid 99); 27 Jun 2014 04:18:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 04:18:22 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of reachbach@gmail.com designates 209.85.215.53 as permitted sender)
Received: from [209.85.215.53] (HELO mail-la0-f53.google.com) (209.85.215.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 04:18:18 +0000
Received: by mail-la0-f53.google.com with SMTP id b8so2490520lan.26
        for <dev@spark.apache.org>; Thu, 26 Jun 2014 21:17:54 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=zoFKpJ6bavBNXkqPRlevNjkeICbavqp7G6SQbOJAwZw=;
        b=ya8Set/LrM1FczSXISG0+hO3CoQJOldkB5ft+bRY85cHxLb61yc4Dac61aTE81Y0Yz
         FAqhSmbrWSgIMHA4uGG5YTydHEjWa2JuwW3Ok792om33u6KmF95Qa7I520qHwDZd0ezN
         yOlS+w8pFnP42lwPlH+L+gF+qcWwQkyqk8ZID0gfpEU6KbW/SZWiclg9pKDhpEdjicDm
         HwpPdRP7vmOVo8FKAFpIY8gxKRe4XdImiOUitiVwk55w4XQr1Fr3fohbcf6ytK33BS9J
         H4JMCkdsnH8r2W1HL3VkiRZ9lWAPM+mKLXpm6/8iNNY4xe3rH3BoWBLl2yC8wIR9VvT3
         PQLQ==
MIME-Version: 1.0
X-Received: by 10.112.162.70 with SMTP id xy6mr14042820lbb.40.1403842674257;
 Thu, 26 Jun 2014 21:17:54 -0700 (PDT)
Received: by 10.114.5.72 with HTTP; Thu, 26 Jun 2014 21:17:54 -0700 (PDT)
Date: Fri, 27 Jun 2014 09:47:54 +0530
Message-ID: <CALxMP-DKDZU1bOTA0YFnFOe5cS3Xzgnbp7uxfSm9N78YN1_Gkg@mail.gmail.com>
Subject: NPE calling reduceByKey on JavaPairRDD
From: Bharath Ravi Kumar <reachbach@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0112d15eeb898304fcc9974c
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0112d15eeb898304fcc9974c
Content-Type: text/plain; charset=UTF-8

Hi,

I've been encountering a NPE invoking reduceByKey on JavaPairRDD since
upgrading to 1.0.0 . The issue is straightforward to reproduce with 1.0.0
and doesn't occur with 0.9.0.  The stack trace is as follows:

14/06/26 21:05:35 WARN scheduler.TaskSetManager: Loss was due to
java.lang.NullPointerException
java.lang.NullPointerException
at
org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:750)
at
org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:750)
at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:59)
at
org.apache.spark.rdd.PairRDDFunctions$$anonfun$1.apply(PairRDDFunctions.scala:96)
at
org.apache.spark.rdd.PairRDDFunctions$$anonfun$1.apply(PairRDDFunctions.scala:95)
at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:582)
at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:582)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
at org.apache.spark.scheduler.Task.run(Task.scala:51)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
at java.lang.Thread.run(Thread.java:722)


I've raised a bug to track this issue :
https://issues.apache.org/jira/browse/SPARK-2292

Thanks,
Bharath

--089e0112d15eeb898304fcc9974c--

From dev-return-8117-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 04:34:41 2014
Return-Path: <dev-return-8117-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6D46911EC6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 04:34:41 +0000 (UTC)
Received: (qmail 86398 invoked by uid 500); 27 Jun 2014 04:34:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86340 invoked by uid 500); 27 Jun 2014 04:34:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86329 invoked by uid 99); 27 Jun 2014 04:34:40 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 04:34:40 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.46] (HELO mail-qa0-f46.google.com) (209.85.216.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 04:34:37 +0000
Received: by mail-qa0-f46.google.com with SMTP id i13so3557722qae.19
        for <dev@spark.apache.org>; Thu, 26 Jun 2014 21:34:12 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=sLWcF6QvwFKRNUb5QGBqP3sRrsXJuhXe55SUrhTUAks=;
        b=MSvj1kzWVX5V+snP0lOgLy/Lyf2ziXZIzedwh166pDd0aIdLc7mBt9vMKoI008GDt4
         TTiLq+k1zLHKzX7DinVveRvT7IKQAWMcKsMX39YUNZSljoCR8JvSN3+TZwNTGCdKnZqb
         7s7u/MFeto7c8wZH+9X2tn6HgDr2KOxazY5LNtC5AqFmYKRTOE8Ikuq+p8r0w5m68jhq
         iLdgTTywsJsUwNBUb846PL2ov8GMm4Y8+RARTHC9gegD/hn38hm65zRpmOIk0il7fXoO
         i25yUbkJXItNwxsIi6TXY4+hK/lt1Ds97vN/Bbbv1e8eUJSEp6vtGMMIWX47xN2s0pMi
         zF5w==
X-Gm-Message-State: ALoCoQkAgeQevGi1o3CA0ojYTsbE7MGXm9tj6Gf1cdIrbFmAQyE8I6fmG7/i+PqTY5T6bZ4k2OKg
X-Received: by 10.140.81.240 with SMTP id f103mr28335970qgd.47.1403843652124;
 Thu, 26 Jun 2014 21:34:12 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Thu, 26 Jun 2014 21:33:51 -0700 (PDT)
In-Reply-To: <CALxMP-DKDZU1bOTA0YFnFOe5cS3Xzgnbp7uxfSm9N78YN1_Gkg@mail.gmail.com>
References: <CALxMP-DKDZU1bOTA0YFnFOe5cS3Xzgnbp7uxfSm9N78YN1_Gkg@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 26 Jun 2014 21:33:51 -0700
Message-ID: <CAPh_B=bBfPtjObnEkCizh0XRABDbbsapEc4X=0hSPF4e8UnLMg@mail.gmail.com>
Subject: Re: NPE calling reduceByKey on JavaPairRDD
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c13b523496ee04fcc9d2a1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c13b523496ee04fcc9d2a1
Content-Type: text/plain; charset=UTF-8

Responded on the jira...




On Thu, Jun 26, 2014 at 9:17 PM, Bharath Ravi Kumar <reachbach@gmail.com>
wrote:

> Hi,
>
> I've been encountering a NPE invoking reduceByKey on JavaPairRDD since
> upgrading to 1.0.0 . The issue is straightforward to reproduce with 1.0.0
> and doesn't occur with 0.9.0.  The stack trace is as follows:
>
> 14/06/26 21:05:35 WARN scheduler.TaskSetManager: Loss was due to
> java.lang.NullPointerException
> java.lang.NullPointerException
> at
>
> org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:750)
> at
>
> org.apache.spark.api.java.JavaPairRDD$$anonfun$pairFunToScalaFun$1.apply(JavaPairRDD.scala:750)
> at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
> at org.apache.spark.Aggregator.combineValuesByKey(Aggregator.scala:59)
> at
>
> org.apache.spark.rdd.PairRDDFunctions$$anonfun$1.apply(PairRDDFunctions.scala:96)
> at
>
> org.apache.spark.rdd.PairRDDFunctions$$anonfun$1.apply(PairRDDFunctions.scala:95)
> at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:582)
> at org.apache.spark.rdd.RDD$$anonfun$14.apply(RDD.scala:582)
> at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
> at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> at
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
> at
> org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
> at org.apache.spark.scheduler.Task.run(Task.scala:51)
> at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
> at
>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
> at
>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
> at java.lang.Thread.run(Thread.java:722)
>
>
> I've raised a bug to track this issue :
> https://issues.apache.org/jira/browse/SPARK-2292
>
> Thanks,
> Bharath
>

--001a11c13b523496ee04fcc9d2a1--

From dev-return-8118-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 05:51:46 2014
Return-Path: <dev-return-8118-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C315211021
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 05:51:46 +0000 (UTC)
Received: (qmail 62388 invoked by uid 500); 27 Jun 2014 05:51:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62328 invoked by uid 500); 27 Jun 2014 05:51:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62313 invoked by uid 99); 27 Jun 2014 05:51:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 05:51:45 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [15.201.208.55] (HELO g4t3427.houston.hp.com) (15.201.208.55)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 05:51:40 +0000
Received: from G9W0364.americas.hpqcorp.net (g9w0364.houston.hp.com [16.216.193.45])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3427.houston.hp.com (Postfix) with ESMTPS id 8349230F
	for <dev@spark.apache.org>; Fri, 27 Jun 2014 05:51:19 +0000 (UTC)
Received: from G4W6306.americas.hpqcorp.net (16.210.26.231) by
 G9W0364.americas.hpqcorp.net (16.216.193.45) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Fri, 27 Jun 2014 05:48:34 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.199]) by
 G4W6306.americas.hpqcorp.net ([16.210.26.231]) with mapi id 14.03.0169.001;
 Fri, 27 Jun 2014 05:48:34 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: "<dev@spark.apache.org>" <dev@spark.apache.org>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: IntelliJ IDEA cannot compile TreeNode.scala
Thread-Topic: IntelliJ IDEA cannot compile TreeNode.scala
Thread-Index: Ac+RsY+GtRXlA17OTrubDGHwGKyWYQAGd9La
Date: Fri, 27 Jun 2014 05:48:34 +0000
Message-ID: <E73D6DE3-55DB-4B8F-8816-89CE263FC96E@hp.com>
References: <54541F80AA2A6F47BDC10C496192CA75D899B9@szxeml558-mbs.china.huawei.com>
In-Reply-To: <54541F80AA2A6F47BDC10C496192CA75D899B9@szxeml558-mbs.china.huawei.com>
Accept-Language: en-US
Content-Language: ru-RU
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
Content-Type: text/plain; charset="koi8-r"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Ron Hu,

The Idea project generated with "update gen-idea" didn't work properly for =
me as well. My workaround is to open corresponding Maven project in Idea (F=
ile->Open look for .bom file). To compile the opened project I use Maven wi=
ndow in Idea (View->show Maven ). However, tests fail to compile due to som=
e mess with paths (it tries to create something like c:\project\blabla\c:\p=
roject\tests). I am also able to compile separate files by right-clicking o=
n a file in Idea project tree and choosing "compile". This solves the test =
problem, because I am able to run test files with this approach.

Best regards, Alexander

27.06.2014, =D7 6:43, "Ron Chung Hu (Ron Hu, ARC)" <ron.hu@huawei.com> =CE=
=C1=D0=C9=D3=C1=CC(=C1):

> Hi,
>=20
> I am a Spark newbie.  I just downloaded Spark1.0.0 and latest IntelliJ ve=
rsion 13.1 with Scala plug-in.  At spark-1.0.0 top level, I executed the fo=
llowing SBT commands and they ran successfully.
>=20
>=20
> -          ./sbt/sbt assembly
>=20
> -          ./sbt/sbt update gen-idea
>=20
> After opening IntelliJ IDEA, I tried to compile ...../sql/catalyst/trees/=
TreeNode.scala inside IntelliJ.  I got many compile errors such as "cannot =
resolve symbol children", "cannot resolve symbol id".  Actually both symbol=
s are defined in the same file.   As Spark was built successfully with "sbt=
/sbt assembly" command, I wondered what went wrong in compiling TreeNode.sc=
ala.  Any pointer will be appreciated.
>=20
> Thanks.
>=20
> Best,
> Ron Hu
>=20

From dev-return-8119-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 05:54:36 2014
Return-Path: <dev-return-8119-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 53D511102F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 05:54:36 +0000 (UTC)
Received: (qmail 68845 invoked by uid 500); 27 Jun 2014 05:54:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68789 invoked by uid 500); 27 Jun 2014 05:54:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68778 invoked by uid 99); 27 Jun 2014 05:54:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 05:54:35 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [15.201.208.53] (HELO g4t3425.houston.hp.com) (15.201.208.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 05:54:32 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3425.houston.hp.com (Postfix) with ESMTPS id 368113B2
	for <dev@spark.apache.org>; Fri, 27 Jun 2014 05:54:07 +0000 (UTC)
Received: from G4W6305.americas.hpqcorp.net (16.210.26.230) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Fri, 27 Jun 2014 05:51:44 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.199]) by
 G4W6305.americas.hpqcorp.net ([16.210.26.230]) with mapi id 14.03.0169.001;
 Fri, 27 Jun 2014 05:51:44 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: "<dev@spark.apache.org>" <dev@spark.apache.org>
Subject: Re: Artificial Neural Network in Spark?
Thread-Topic: Artificial Neural Network in Spark?
Thread-Index: Ac+RoTqSDV42paQFQ+OUNd1O9igqiAAKqVEX
Date: Fri, 27 Jun 2014 05:51:43 +0000
Message-ID: <092662BE-0168-4260-B6D4-2EF5E9E3F42D@hp.com>
References: <46A1DF3F04371240B504290A071B4DB63E632EDC@SZXEMA510-MBX.china.huawei.com>
In-Reply-To: <46A1DF3F04371240B504290A071B4DB63E632EDC@SZXEMA510-MBX.china.huawei.com>
Accept-Language: en-US
Content-Language: ru-RU
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
Content-Type: text/plain; charset="koi8-r"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Bert,

It would be extremely interesting. Do you plan to implement autoencoder as =
well? It would be great to have deep learning in Spark.

Best regards, Alexander

27.06.2014, =D7 4:47, "Bert Greevenbosch" <Bert.Greevenbosch@huawei.com> =
=CE=C1=D0=C9=D3=C1=CC(=C1):

> Hello all,
>=20
> I was wondering whether Spark/mllib supports Artificial Neural Networks (=
ANNs)?
>=20
> If not, I am currently working on an implementation of it. I re-use the c=
ode for linear regression and gradient descent as much as possible.
>=20
> Would the community be interested in such implementation? Or maybe somebo=
dy is already working on it?
>=20
> Best regards,
> Bert

From dev-return-8120-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 06:02:56 2014
Return-Path: <dev-return-8120-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 761BA1105E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 06:02:56 +0000 (UTC)
Received: (qmail 85113 invoked by uid 500); 27 Jun 2014 06:02:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85058 invoked by uid 500); 27 Jun 2014 06:02:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85042 invoked by uid 99); 27 Jun 2014 06:02:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 06:02:55 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.179 as permitted sender)
Received: from [209.85.216.179] (HELO mail-qc0-f179.google.com) (209.85.216.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 06:02:53 +0000
Received: by mail-qc0-f179.google.com with SMTP id x3so4052500qcv.24
        for <dev@spark.apache.org>; Thu, 26 Jun 2014 23:02:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=VKDKksw3uqCywUwNWLWXanbIUc7KGtRw5rimFuD2WKs=;
        b=uoZY9OmkfJb+TmdXnM6R9QsDGmrgXILrdDqOndr/arAhXzd/aseS1NyOP+bABw7KkC
         2cTwG/wW+X4236x1bFJ+DYRDritMGHTurzRAV96pQbPUf2ve2YeQYuZ9kjCWzGxYsI1R
         bn1uiaLVCgjT5PQPLq7EElFigOhTygHXBifnkW76WdeHt9aX+l9w46GM3GwGx2kFNaBX
         THq/nz9vaRqLV77u9gNusuQDEJGtO4+G6QRw09a4mfJIU2+Nun2LwbmltDeNGGQQIWt3
         K9RmgfxziU+rW6C5i/mcx9FgfArwVdBIdYpqmZJbJBfXR5FRM5UYLyTE2qZ0Y/vDKGqT
         q9mA==
MIME-Version: 1.0
X-Received: by 10.224.136.65 with SMTP id q1mr30460373qat.93.1403848948951;
 Thu, 26 Jun 2014 23:02:28 -0700 (PDT)
Received: by 10.140.85.149 with HTTP; Thu, 26 Jun 2014 23:02:28 -0700 (PDT)
In-Reply-To: <092662BE-0168-4260-B6D4-2EF5E9E3F42D@hp.com>
References: <46A1DF3F04371240B504290A071B4DB63E632EDC@SZXEMA510-MBX.china.huawei.com>
	<092662BE-0168-4260-B6D4-2EF5E9E3F42D@hp.com>
Date: Thu, 26 Jun 2014 23:02:28 -0700
Message-ID: <CA+B-+fy7fw7RVvvvWUOeQTFtZzvSiW2HXPz7TNxGWUFM5UOfug@mail.gmail.com>
Subject: Re: Artificial Neural Network in Spark?
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2b810ebb22a04fccb0d42
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2b810ebb22a04fccb0d42
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Look into Powered by Spark page...I found a project there which used
autoencoder functions...It's not updated for a long time now !

On Thu, Jun 26, 2014 at 10:51 PM, Ulanov, Alexander <alexander.ulanov@hp.co=
m
> wrote:

> Hi Bert,
>
> It would be extremely interesting. Do you plan to implement autoencoder a=
s
> well? It would be great to have deep learning in Spark.
>
> Best regards, Alexander
>
> 27.06.2014, =D0=B2 4:47, "Bert Greevenbosch" <Bert.Greevenbosch@huawei.co=
m>
> =D0=BD=D0=B0=D0=BF=D0=B8=D1=81=D0=B0=D0=BB(=D0=B0):
>
> > Hello all,
> >
> > I was wondering whether Spark/mllib supports Artificial Neural Networks
> (ANNs)?
> >
> > If not, I am currently working on an implementation of it. I re-use the
> code for linear regression and gradient descent as much as possible.
> >
> > Would the community be interested in such implementation? Or maybe
> somebody is already working on it?
> >
> > Best regards,
> > Bert
>

--001a11c2b810ebb22a04fccb0d42--

From dev-return-8121-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 10:41:25 2014
Return-Path: <dev-return-8121-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A5C09116AA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 10:41:25 +0000 (UTC)
Received: (qmail 87044 invoked by uid 500); 27 Jun 2014 10:41:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86917 invoked by uid 500); 27 Jun 2014 10:41:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86663 invoked by uid 99); 27 Jun 2014 10:41:24 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 10:41:24 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of shankark+sys@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 10:41:21 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <shankark+sys@gmail.com>)
	id 1X0TaS-0008S1-Py
	for dev@spark.incubator.apache.org; Fri, 27 Jun 2014 03:40:56 -0700
Date: Fri, 27 Jun 2014 03:40:56 -0700 (PDT)
From: Krakna H <shankark+sys@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1403865656657-7097.post@n3.nabble.com>
In-Reply-To: <CAH3_EVOj6RtYgkk+G+5FRGqrWfO3y==4BVyK0RNRQceHmLFqzg@mail.gmail.com>
References: <CA+B-+fypoW8=frGnV=G_C12=HwHMWLKiz4OAjL-fSJgr82cgtg@mail.gmail.com> <CAH3_EVP+=m6BGoEzoWGX1mqVEV3B7nw1rZLmonOZBu_xSABEXw@mail.gmail.com> <CA+B-+fymrGhk8jn1a1_V==ZPk3Grf++ed601tkBcfDk7Y-=nMQ@mail.gmail.com> <CAH3_EVOj6RtYgkk+G+5FRGqrWfO3y==4BVyK0RNRQceHmLFqzg@mail.gmail.com>
Subject: Re: Spark Matrix Factorization
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all,

Just found this thread -- is there an update on including DSGD in Spark? We
have a project that entails topic modeling on a document-term matrix using
matrix factorization, and were wondering if we should use ALS or attempt
writing our own matrix factorization implementation on top of Spark.

Thanks.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-Matrix-Factorization-tp55p7097.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8122-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 16:47:22 2014
Return-Path: <dev-return-8122-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2F1D21152F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 16:47:22 +0000 (UTC)
Received: (qmail 54323 invoked by uid 500); 27 Jun 2014 16:47:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54265 invoked by uid 500); 27 Jun 2014 16:47:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54247 invoked by uid 99); 27 Jun 2014 16:47:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 16:47:21 +0000
X-ASF-Spam-Status: No, hits=3.1 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.43 as permitted sender)
Received: from [209.85.216.43] (HELO mail-qa0-f43.google.com) (209.85.216.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 16:47:16 +0000
Received: by mail-qa0-f43.google.com with SMTP id k15so4227408qaq.16
        for <dev@spark.incubator.apache.org>; Fri, 27 Jun 2014 09:46:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=WraauNCYHPtmF8C4dn/EZFqNcEqGJmhxRjsbRGnEzzc=;
        b=zLvFJqWBLbummK2wHQtTinHuUWVqXSWTc7tCDoT1jH1N56GvNtwRfS3B2v4fO/TpQ7
         Yd66COosV4pnJdZa6bjg2dnc64c8J22jfvw7FnbE9ycao/D66fnTCJB8pO2pWICfhChK
         3/8fxeQqD8BkWTwokxdU7lyIJvPASE7KYPecdJk94LrA3ogz9V6TzX9z4qhXOI4sweWP
         DfE2QHqU76Fq1h82mGC0r7psFFsIUEHxdBFIFd639qInVy04RfrDZ8f2JsKfuQxpN3wd
         U9bZi7KlCpp4jxEEz6hBXS3lxjMSZc2EWFE0fAb4xoTB74zziNfU25m5StNLdN4J3aTU
         xs/A==
MIME-Version: 1.0
X-Received: by 10.224.40.194 with SMTP id l2mr36172388qae.81.1403887615987;
 Fri, 27 Jun 2014 09:46:55 -0700 (PDT)
Received: by 10.140.85.149 with HTTP; Fri, 27 Jun 2014 09:46:55 -0700 (PDT)
In-Reply-To: <1403865656657-7097.post@n3.nabble.com>
References: <CA+B-+fypoW8=frGnV=G_C12=HwHMWLKiz4OAjL-fSJgr82cgtg@mail.gmail.com>
	<CAH3_EVP+=m6BGoEzoWGX1mqVEV3B7nw1rZLmonOZBu_xSABEXw@mail.gmail.com>
	<CA+B-+fymrGhk8jn1a1_V==ZPk3Grf++ed601tkBcfDk7Y-=nMQ@mail.gmail.com>
	<CAH3_EVOj6RtYgkk+G+5FRGqrWfO3y==4BVyK0RNRQceHmLFqzg@mail.gmail.com>
	<1403865656657-7097.post@n3.nabble.com>
Date: Fri, 27 Jun 2014 09:46:55 -0700
Message-ID: <CA+B-+fw35hCRMWuLVUxreYATFWoBLCHhBZs04gdwv+mPmVXZ6Q@mail.gmail.com>
Subject: Re: Spark Matrix Factorization
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2ca4aa7df6704fcd40eca
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ca4aa7df6704fcd40eca
Content-Type: text/plain; charset=UTF-8

Hi,

In my experiments with Jellyfish I did not see any substantial RMSE loss
over DSGD for Netflix dataset...

So we decided to stick with ALS and implemented a family of Quadratic
Minimization solvers that stays in the ALS realm but can solve interesting
constraints(positivity, bounds, L1, equality constrained bounds etc)...We
are going to show it at the Spark Summit...Also ALS structure is favorable
to matrix factorization use-cases where missing entries means zero and you
want to compute a global gram matrix using broadcast and use that for each
Quadratic Minimization for all users/products...

Implementing DSGD in the data partitioning that Spark ALS uses will be
straightforward but I would be more keen to see a dataset where DSGD is
showing you better RMSEs than ALS....

If you have a dataset where DSGD produces much better result could you
please point it to us ?

Also you can use Jellyfish to run DSGD benchmarks to compare against
ALS...It is multithreaded and if you have good RAM, you should be able to
run fairly large datasets...

Be careful about the default Jellyfish...it has been tuned for netflix
dataset (regularization, rating normalization etc)...So before you compare
RMSE make sure ALS and Jellyfish is running same algorithm (L2 regularized
Quadratic Loss)....

Thanks.
Deb


On Fri, Jun 27, 2014 at 3:40 AM, Krakna H <shankark+sys@gmail.com> wrote:

> Hi all,
>
> Just found this thread -- is there an update on including DSGD in Spark? We
> have a project that entails topic modeling on a document-term matrix using
> matrix factorization, and were wondering if we should use ALS or attempt
> writing our own matrix factorization implementation on top of Spark.
>
> Thanks.
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-Matrix-Factorization-tp55p7097.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c2ca4aa7df6704fcd40eca--

From dev-return-8123-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 16:47:23 2014
Return-Path: <dev-return-8123-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D6CAF11530
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 16:47:23 +0000 (UTC)
Received: (qmail 55196 invoked by uid 500); 27 Jun 2014 16:47:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55142 invoked by uid 500); 27 Jun 2014 16:47:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55129 invoked by uid 99); 27 Jun 2014 16:47:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 16:47:22 +0000
X-ASF-Spam-Status: No, hits=3.1 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.43 as permitted sender)
Received: from [209.85.216.43] (HELO mail-qa0-f43.google.com) (209.85.216.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 16:47:20 +0000
Received: by mail-qa0-f43.google.com with SMTP id k15so4213077qaq.2
        for <dev@spark.apache.org>; Fri, 27 Jun 2014 09:46:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=WraauNCYHPtmF8C4dn/EZFqNcEqGJmhxRjsbRGnEzzc=;
        b=zLvFJqWBLbummK2wHQtTinHuUWVqXSWTc7tCDoT1jH1N56GvNtwRfS3B2v4fO/TpQ7
         Yd66COosV4pnJdZa6bjg2dnc64c8J22jfvw7FnbE9ycao/D66fnTCJB8pO2pWICfhChK
         3/8fxeQqD8BkWTwokxdU7lyIJvPASE7KYPecdJk94LrA3ogz9V6TzX9z4qhXOI4sweWP
         DfE2QHqU76Fq1h82mGC0r7psFFsIUEHxdBFIFd639qInVy04RfrDZ8f2JsKfuQxpN3wd
         U9bZi7KlCpp4jxEEz6hBXS3lxjMSZc2EWFE0fAb4xoTB74zziNfU25m5StNLdN4J3aTU
         xs/A==
MIME-Version: 1.0
X-Received: by 10.224.40.194 with SMTP id l2mr36172388qae.81.1403887615987;
 Fri, 27 Jun 2014 09:46:55 -0700 (PDT)
Received: by 10.140.85.149 with HTTP; Fri, 27 Jun 2014 09:46:55 -0700 (PDT)
In-Reply-To: <1403865656657-7097.post@n3.nabble.com>
References: <CA+B-+fypoW8=frGnV=G_C12=HwHMWLKiz4OAjL-fSJgr82cgtg@mail.gmail.com>
	<CAH3_EVP+=m6BGoEzoWGX1mqVEV3B7nw1rZLmonOZBu_xSABEXw@mail.gmail.com>
	<CA+B-+fymrGhk8jn1a1_V==ZPk3Grf++ed601tkBcfDk7Y-=nMQ@mail.gmail.com>
	<CAH3_EVOj6RtYgkk+G+5FRGqrWfO3y==4BVyK0RNRQceHmLFqzg@mail.gmail.com>
	<1403865656657-7097.post@n3.nabble.com>
Date: Fri, 27 Jun 2014 09:46:55 -0700
Message-ID: <CA+B-+fw35hCRMWuLVUxreYATFWoBLCHhBZs04gdwv+mPmVXZ6Q@mail.gmail.com>
Subject: Re: Spark Matrix Factorization
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Cc: dev <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2ca4aa7df6704fcd40eca
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ca4aa7df6704fcd40eca
Content-Type: text/plain; charset=UTF-8

Hi,

In my experiments with Jellyfish I did not see any substantial RMSE loss
over DSGD for Netflix dataset...

So we decided to stick with ALS and implemented a family of Quadratic
Minimization solvers that stays in the ALS realm but can solve interesting
constraints(positivity, bounds, L1, equality constrained bounds etc)...We
are going to show it at the Spark Summit...Also ALS structure is favorable
to matrix factorization use-cases where missing entries means zero and you
want to compute a global gram matrix using broadcast and use that for each
Quadratic Minimization for all users/products...

Implementing DSGD in the data partitioning that Spark ALS uses will be
straightforward but I would be more keen to see a dataset where DSGD is
showing you better RMSEs than ALS....

If you have a dataset where DSGD produces much better result could you
please point it to us ?

Also you can use Jellyfish to run DSGD benchmarks to compare against
ALS...It is multithreaded and if you have good RAM, you should be able to
run fairly large datasets...

Be careful about the default Jellyfish...it has been tuned for netflix
dataset (regularization, rating normalization etc)...So before you compare
RMSE make sure ALS and Jellyfish is running same algorithm (L2 regularized
Quadratic Loss)....

Thanks.
Deb


On Fri, Jun 27, 2014 at 3:40 AM, Krakna H <shankark+sys@gmail.com> wrote:

> Hi all,
>
> Just found this thread -- is there an update on including DSGD in Spark? We
> have a project that entails topic modeling on a document-term matrix using
> matrix factorization, and were wondering if we should use ALS or attempt
> writing our own matrix factorization implementation on top of Spark.
>
> Thanks.
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-Matrix-Factorization-tp55p7097.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c2ca4aa7df6704fcd40eca--

From dev-return-8124-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 17:02:41 2014
Return-Path: <dev-return-8124-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E984B115E0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 17:02:40 +0000 (UTC)
Received: (qmail 89952 invoked by uid 500); 27 Jun 2014 17:02:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89889 invoked by uid 500); 27 Jun 2014 17:02:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89871 invoked by uid 99); 27 Jun 2014 17:02:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 17:02:40 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ron.hu@huawei.com designates 119.145.14.64 as permitted sender)
Received: from [119.145.14.64] (HELO szxga01-in.huawei.com) (119.145.14.64)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 17:02:35 +0000
Received: from 172.24.2.119 (EHLO szxeml413-hub.china.huawei.com) ([172.24.2.119])
	by szxrg01-dlp.huawei.com (MOS 4.3.7-GA FastPath queued)
	with ESMTP id BXV93644;
	Sat, 28 Jun 2014 01:02:13 +0800 (CST)
Received: from szxeml558-mbs.china.huawei.com ([169.254.8.208]) by
 szxeml413-hub.china.huawei.com ([10.82.67.152]) with mapi id 14.03.0158.001;
 Sat, 28 Jun 2014 01:02:12 +0800
From: "Ron Chung Hu (Ron Hu, ARC)" <ron.hu@huawei.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: IntelliJ IDEA cannot compile TreeNode.scala
Thread-Topic: IntelliJ IDEA cannot compile TreeNode.scala
Thread-Index: Ac+RsY+GtRXlA17OTrubDGHwGKyWYf//joQA//6eslA=
Date: Fri, 27 Jun 2014 17:02:10 +0000
Message-ID: <54541F80AA2A6F47BDC10C496192CA75D89A0A@szxeml558-mbs.china.huawei.com>
References: <54541F80AA2A6F47BDC10C496192CA75D899B9@szxeml558-mbs.china.huawei.com>
 <CAPh_B=bQNio8Cp57te7j4on7ek0OJzP7E6K+PFAmyxnPcMRRaQ@mail.gmail.com>
In-Reply-To: <CAPh_B=bQNio8Cp57te7j4on7ek0OJzP7E6K+PFAmyxnPcMRRaQ@mail.gmail.com>
Accept-Language: en-US, zh-CN
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [10.193.35.52]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-CFilter-Loop: Reflected
X-Virus-Checked: Checked by ClamAV on apache.org

VGhhbmtzIFJleW5vbGQgZm9yIGFkdmljZS4NCg0KUm9uDQoNCi0tLS0tT3JpZ2luYWwgTWVzc2Fn
ZS0tLS0tDQpGcm9tOiBSZXlub2xkIFhpbiBbbWFpbHRvOnJ4aW5AZGF0YWJyaWNrcy5jb21dIA0K
U2VudDogVGh1cnNkYXksIEp1bmUgMjYsIDIwMTQgODo1NyBQTQ0KVG86IGRldkBzcGFyay5hcGFj
aGUub3JnDQpTdWJqZWN0OiBSZTogSW50ZWxsaUogSURFQSBjYW5ub3QgY29tcGlsZSBUcmVlTm9k
ZS5zY2FsYQ0KDQpJbnRlbGxpSiBwYXJzZXIvYW5hbHl6ZXIvY29tcGlsZXIgYmVoYXZlcyBkaWZm
ZXJlbnRseSBmcm9tIFNjYWxhIGNvbXBpbGVyLA0KYW5kIHNvbWV0aW1lcyBsZWFkIHRvIGluY29u
c2lzdGVudCBiZWhhdmlvci4gVGhpcyBpcyBvbmUgb2YgdGhlIGNhc2UuDQoNCkluIGdlbmVyYWwg
d2hpbGUgd2UgdXNlIEludGVsbGlKLCB3ZSBkb24ndCB1c2UgaXQgdG8gYnVpbGQgc3R1ZmYuIEkN
CnBlcnNvbmFsbHkgYWx3YXlzIGJ1aWxkIGluIGNvbW1hbmQgbGluZSB3aXRoIHNidCBvciBNYXZl
bi4NCg0KDQoNCk9uIFRodSwgSnVuIDI2LCAyMDE0IGF0IDc6NDMgUE0sIFJvbiBDaHVuZyBIdSAo
Um9uIEh1LCBBUkMpIDwNCnJvbi5odUBodWF3ZWkuY29tPiB3cm90ZToNCg0KPiBIaSwNCj4NCj4g
SSBhbSBhIFNwYXJrIG5ld2JpZS4gIEkganVzdCBkb3dubG9hZGVkIFNwYXJrMS4wLjAgYW5kIGxh
dGVzdCBJbnRlbGxpSg0KPiB2ZXJzaW9uIDEzLjEgd2l0aCBTY2FsYSBwbHVnLWluLiAgQXQgc3Bh
cmstMS4wLjAgdG9wIGxldmVsLCBJIGV4ZWN1dGVkIHRoZQ0KPiBmb2xsb3dpbmcgU0JUIGNvbW1h
bmRzIGFuZCB0aGV5IHJhbiBzdWNjZXNzZnVsbHkuDQo+DQo+DQo+IC0gICAgICAgICAgLi9zYnQv
c2J0IGFzc2VtYmx5DQo+DQo+IC0gICAgICAgICAgLi9zYnQvc2J0IHVwZGF0ZSBnZW4taWRlYQ0K
Pg0KPiBBZnRlciBvcGVuaW5nIEludGVsbGlKIElERUEsIEkgdHJpZWQgdG8gY29tcGlsZQ0KPiAu
Li4uLi9zcWwvY2F0YWx5c3QvdHJlZXMvVHJlZU5vZGUuc2NhbGEgaW5zaWRlIEludGVsbGlKLiAg
SSBnb3QgbWFueQ0KPiBjb21waWxlIGVycm9ycyBzdWNoIGFzICJjYW5ub3QgcmVzb2x2ZSBzeW1i
b2wgY2hpbGRyZW4iLCAiY2Fubm90IHJlc29sdmUNCj4gc3ltYm9sIGlkIi4gIEFjdHVhbGx5IGJv
dGggc3ltYm9scyBhcmUgZGVmaW5lZCBpbiB0aGUgc2FtZSBmaWxlLiAgIEFzIFNwYXJrDQo+IHdh
cyBidWlsdCBzdWNjZXNzZnVsbHkgd2l0aCAic2J0L3NidCBhc3NlbWJseSIgY29tbWFuZCwgSSB3
b25kZXJlZCB3aGF0DQo+IHdlbnQgd3JvbmcgaW4gY29tcGlsaW5nIFRyZWVOb2RlLnNjYWxhLiAg
QW55IHBvaW50ZXIgd2lsbCBiZSBhcHByZWNpYXRlZC4NCj4NCj4gVGhhbmtzLg0KPg0KPiBCZXN0
LA0KPiBSb24gSHUNCj4NCj4NCg==

From dev-return-8125-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 18:18:53 2014
Return-Path: <dev-return-8125-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CAAC7118A5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 18:18:53 +0000 (UTC)
Received: (qmail 77926 invoked by uid 500); 27 Jun 2014 18:18:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77872 invoked by uid 500); 27 Jun 2014 18:18:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77859 invoked by uid 99); 27 Jun 2014 18:18:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 18:18:52 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.49 as permitted sender)
Received: from [209.85.160.49] (HELO mail-pb0-f49.google.com) (209.85.160.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 18:18:47 +0000
Received: by mail-pb0-f49.google.com with SMTP id rr13so4807217pbb.8
        for <dev@spark.apache.org>; Fri, 27 Jun 2014 11:18:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=w5IQbSn1SaNL3MixPM+Fk9R5So9pstGLlG9tAkodAQg=;
        b=TjF0MRynDvncbZIJYmvc7wp0rybEtAc0TrzRXt7ldw4RkpeMrAz5KSqB/wM5hmjgfG
         ATVhmWUnaBEYO39xOfpIDA+jP0fG3VgGlPJ/lof6ZACNZgnMKT1FCRONmc9EinvKAVRm
         9W0rODRmNnzMG6mYVgYI+Pew26ZNnVJs+B+Az2y9urogsXea1IDd1nERY5krlZlcHmmK
         xwUT3HYU67OLsfjc/Jyti0tLXclWQB1YlS1XoUgnGsA4QleKNusdGnkRKAuoyeoryY2f
         cSgq06389ZB7YomHKm/KUslwr3tLKMWspMFGTXhJnoUhSC3ILXnw8TZFvXRfcYjo7VN2
         qWrw==
X-Received: by 10.68.164.4 with SMTP id ym4mr32731646pbb.53.1403893102583;
        Fri, 27 Jun 2014 11:18:22 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id gg4sm15841528pbb.12.2014.06.27.11.18.18
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 27 Jun 2014 11:18:18 -0700 (PDT)
Content-Type: text/plain; charset=iso-8859-1
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC1)
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CABPQxsvAioKpE29GzUPts6tpk=6dSnXpfXMCYwKKX2fKX09Udw@mail.gmail.com>
Date: Fri, 27 Jun 2014 11:18:17 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <3D27857F-0798-48EB-BC07-622FEB465AA2@gmail.com>
References: <CABPQxsvAioKpE29GzUPts6tpk=6dSnXpfXMCYwKKX2fKX09Udw@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.2)
X-Virus-Checked: Checked by ClamAV on apache.org

+1

Tested it out on Mac OS X and Windows, looked through docs.

Matei

On Jun 26, 2014, at 7:06 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Please vote on releasing the following candidate as Apache Spark =
version 1.0.1!
>=20
> The tag to be voted on is v1.0.1-rc1 (commit 7feeda3):
> =
https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D7feed=
a3d729f9397aa15ee8750c01ef5aa601962
>=20
> The release files, including signatures, digests, etc. can be found =
at:
> http://people.apache.org/~pwendell/spark-1.0.1-rc1/
>=20
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>=20
> The staging repository for this release can be found at:
> =
https://repository.apache.org/content/repositories/orgapachespark-1020/
>=20
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.0.1-rc1-docs/
>=20
> Please vote on releasing this package as Apache Spark 1.0.1!
>=20
> The vote is open until Monday, June 30, at 03:00 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>=20
> [ ] +1 Release this package as Apache Spark 1.0.1
> [ ] -1 Do not release this package because ...
>=20
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>=20
> =3D=3D=3D About this release =3D=3D=3D
> This release fixes a few high-priority bugs in 1.0 and has a variety
> of smaller fixes. The full list is here: http://s.apache.org/b45. Some
> of the more visible patches are:
>=20
> SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
> SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame =
size.
> SPARK-1790: Support r3 instance types on EC2.
>=20
> This is the first maintenance release on the 1.0 line. We plan to make
> additional maintenance releases as new fixes come in.
>=20
> - Patrick


From dev-return-8126-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 21:54:56 2014
Return-Path: <dev-return-8126-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BC9EA11251
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 21:54:56 +0000 (UTC)
Received: (qmail 636 invoked by uid 500); 27 Jun 2014 21:54:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 581 invoked by uid 500); 27 Jun 2014 21:54:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 569 invoked by uid 99); 27 Jun 2014 21:54:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 21:54:55 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.54 as permitted sender)
Received: from [209.85.216.54] (HELO mail-qa0-f54.google.com) (209.85.216.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 21:54:53 +0000
Received: by mail-qa0-f54.google.com with SMTP id v10so4471245qac.41
        for <dev@spark.apache.org>; Fri, 27 Jun 2014 14:54:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=kBhToWv2Z2CaaGQ5GX8nn8CuKzJ7Xd6211OYNjX4GBk=;
        b=PtjxCHZwsbMNm9drLB0pFQQzaEIWL2GSr533zkhcMQg+u1wIxbCRgNzkePfMV1AzfM
         ZO9pGL34NSAOxjowKLAKWdJShze8gUfTxgcBVIqR+R5GVKdyyyMhEQKE1/q1v1i3tDs9
         VVolSciy5woPGykMArVfpTZmQZ3+8+kPKdsO957VHmiCLRMvHpMcGBdbIeCP7yGBfa3d
         YtYwJc0tZ6k0eoUrUFka8ld7roiYFGuQMyy0XWIdGwtp1gzB/mKIoEKP2MpGeTU5OiEy
         2KOHq+QFGM8uOXOUOp82hMq0nF7uFgAl2PCLrOzgE5l6YXiCtfDcgo1avrgObp8Ywm0Y
         McNQ==
MIME-Version: 1.0
X-Received: by 10.140.28.37 with SMTP id 34mr36329366qgy.28.1403906069463;
 Fri, 27 Jun 2014 14:54:29 -0700 (PDT)
Received: by 10.140.85.149 with HTTP; Fri, 27 Jun 2014 14:54:29 -0700 (PDT)
Date: Fri, 27 Jun 2014 14:54:29 -0700
Message-ID: <CA+B-+fwtVvTEGghCywJFEjcXtrH7E3i61z8WfxisiihNZxy-fA@mail.gmail.com>
Subject: Linear CG solver
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11393d42918c5104fcd85a31
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11393d42918c5104fcd85a31
Content-Type: text/plain; charset=UTF-8

Hi,

I am looking for an efficient linear CG to be put inside the Quadratic
Minimization algorithms we added for Spark mllib.

With a good linear CG, we should be able to solve kernel SVMs with this
solver in mllib...

I use direct solves right now using cholesky decomposition which has higher
complexity as matrix sizes become large...

I found out some jblas example code:

https://github.com/mikiobraun/jblas-examples/blob/master/src/CG.java

I was wondering if mllib developers have any experience using this solver
and if this is better than apache commons linear CG ?

Thanks.
Deb

--001a11393d42918c5104fcd85a31--

From dev-return-8127-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 22:13:11 2014
Return-Path: <dev-return-8127-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B8F2A11398
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 22:13:11 +0000 (UTC)
Received: (qmail 48830 invoked by uid 500); 27 Jun 2014 22:13:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48769 invoked by uid 500); 27 Jun 2014 22:13:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48757 invoked by uid 99); 27 Jun 2014 22:13:10 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 22:13:10 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of david.lw.hall@gmail.com designates 209.85.192.51 as permitted sender)
Received: from [209.85.192.51] (HELO mail-qg0-f51.google.com) (209.85.192.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 22:13:08 +0000
Received: by mail-qg0-f51.google.com with SMTP id z60so52958qgd.38
        for <dev@spark.apache.org>; Fri, 27 Jun 2014 15:12:43 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=CpzTHiaknW0UXJ0xRtngrZoQe2dZcU7gTeJgBaErKmk=;
        b=lfS573geEplAw/+WHtr9hKgefr07RDxyJFUDWw6QuXYkF9hhzB026kurnJQAw6JiRj
         WRyPFZj8gBhlqu6yhc+vAsq0OiRL8APy5yurmDgfVT4F16kodur3XzLZzUMU9l2M0aJz
         eI0wLbccHBAipFW33feh1hvqUyZVT6kklVf+OeWxobDyItmzOGvG5d6EbI8kCYnBU3ru
         ebcXqvH8btdmjcQBvHFVXNKFc9vtwpeZa//ek9YHX8koGLH7/m4WXTCeF5dQRZbLd94n
         Jc42wWX76wgpOw67WbpHRo3a2C2BRp6a5V9mUMy8iNLTiyQ2PWsSCG+h4gVVNQST7pfx
         Stew==
MIME-Version: 1.0
X-Received: by 10.140.42.19 with SMTP id b19mr35168759qga.109.1403907163364;
 Fri, 27 Jun 2014 15:12:43 -0700 (PDT)
Sender: david.lw.hall@gmail.com
Received: by 10.140.44.97 with HTTP; Fri, 27 Jun 2014 15:12:43 -0700 (PDT)
In-Reply-To: <CA+B-+fwtVvTEGghCywJFEjcXtrH7E3i61z8WfxisiihNZxy-fA@mail.gmail.com>
References: <CA+B-+fwtVvTEGghCywJFEjcXtrH7E3i61z8WfxisiihNZxy-fA@mail.gmail.com>
Date: Fri, 27 Jun 2014 18:12:43 -0400
X-Google-Sender-Auth: KGA6_5dCl0h9HVeaZ6fUw5EGGeU
Message-ID: <CALW2ey3QdNmASQFbuGyN6pbzmUVXuq-i=i6gRktrabk6YyFYpQ@mail.gmail.com>
Subject: Re: Linear CG solver
From: David Hall <dlwh@cs.berkeley.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c124dcc640e504fcd89b50
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c124dcc640e504fcd89b50
Content-Type: text/plain; charset=UTF-8

I have no ideas on benchmarks, but breeze has a CG solver:
https://github.com/scalanlp/breeze/tree/master/math/src/main/scala/breeze/optimize/linear/ConjugateGradient.scala

https://github.com/scalanlp/breeze/blob/e2adad3b885736baf890b306806a56abc77a3ed3/math/src/test/scala/breeze/optimize/linear/ConjugateGradientTest.scala

It's based on the code from TRON, and so I think it's more targeted for
norm-constrained solutions of the CG problem.








On Fri, Jun 27, 2014 at 5:54 PM, Debasish Das <debasish.das83@gmail.com>
wrote:

> Hi,
>
> I am looking for an efficient linear CG to be put inside the Quadratic
> Minimization algorithms we added for Spark mllib.
>
> With a good linear CG, we should be able to solve kernel SVMs with this
> solver in mllib...
>
> I use direct solves right now using cholesky decomposition which has higher
> complexity as matrix sizes become large...
>
> I found out some jblas example code:
>
> https://github.com/mikiobraun/jblas-examples/blob/master/src/CG.java
>
> I was wondering if mllib developers have any experience using this solver
> and if this is better than apache commons linear CG ?
>
> Thanks.
> Deb
>

--001a11c124dcc640e504fcd89b50--

From dev-return-8128-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 22:42:37 2014
Return-Path: <dev-return-8128-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 18AF71149C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 22:42:37 +0000 (UTC)
Received: (qmail 12767 invoked by uid 500); 27 Jun 2014 22:42:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12706 invoked by uid 500); 27 Jun 2014 22:42:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12693 invoked by uid 99); 27 Jun 2014 22:42:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 22:42:36 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.220.54] (HELO mail-pa0-f54.google.com) (209.85.220.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 22:42:32 +0000
Received: by mail-pa0-f54.google.com with SMTP id et14so5229764pad.27
        for <dev@spark.apache.org>; Fri, 27 Jun 2014 15:42:11 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=5CDx8f34v2P+18JUIl5Nbsnh96CgSWuka4Maxl7Srl8=;
        b=RY77NtVb6UJT1w7n5GBRsyaNan1O1vRA9mN/T1AIlN4Uy1/zzrRore5fQxgSHcxipn
         S6wbspehMx9+D7fKAWH1q/u5813YuFPJYssbdQ9Oooil1/b/57YxAcedeRo3gaSVW8TO
         8Sl57R00jhc27LVb/VNQP3w58/gdG8FQDHMgtKpYvH1PEWizEd9deLx5DZB0EPBA7iYI
         UC3oFN4erU/eMp4AgwyUImfPnkEIEaAUB1D6ofwOke7VuDaTLK0iqtEu8b/e3yAEvumh
         fC9uOn4IzPGZvCGzcLo4FyZxgimcJdWEferRnI2q4qERV4ZkU6VTE9/uENr3AXMJ9IOJ
         DF1Q==
X-Gm-Message-State: ALoCoQklTcQ96q1LURFDiijiuD/TROmwjYtnfdo5loKVGR8m3ocGf/Yyyq08jxg9uMudd2Uaz/Zp
MIME-Version: 1.0
X-Received: by 10.66.147.99 with SMTP id tj3mr34563834pab.47.1403908931102;
 Fri, 27 Jun 2014 15:42:11 -0700 (PDT)
Received: by 10.70.53.8 with HTTP; Fri, 27 Jun 2014 15:42:11 -0700 (PDT)
In-Reply-To: <3D27857F-0798-48EB-BC07-622FEB465AA2@gmail.com>
References: <CABPQxsvAioKpE29GzUPts6tpk=6dSnXpfXMCYwKKX2fKX09Udw@mail.gmail.com>
	<3D27857F-0798-48EB-BC07-622FEB465AA2@gmail.com>
Date: Fri, 27 Jun 2014 15:42:11 -0700
Message-ID: <CAMJOb8kQf6uZE8Ym7PkQB-k1jNH-cbMgZSsWQfqOhUJpp_qU9g@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC1)
From: Andrew Or <andrew@databricks.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b6dce6a22bcf004fcd90532
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6dce6a22bcf004fcd90532
Content-Type: text/plain; charset=UTF-8

There is an issue with the SparkUI: the storage page continues to display
RDDs that are dropped from memory. This is fixed in
https://github.com/apache/spark/commit/21e0f77b6321590ed86223a60cdb8ae08ea4057f
but is not part of this RC.


2014-06-27 11:18 GMT-07:00 Matei Zaharia <matei.zaharia@gmail.com>:

> +1
>
> Tested it out on Mac OS X and Windows, looked through docs.
>
> Matei
>
> On Jun 26, 2014, at 7:06 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>
> > Please vote on releasing the following candidate as Apache Spark version
> 1.0.1!
> >
> > The tag to be voted on is v1.0.1-rc1 (commit 7feeda3):
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7feeda3d729f9397aa15ee8750c01ef5aa601962
> >
> > The release files, including signatures, digests, etc. can be found at:
> > http://people.apache.org/~pwendell/spark-1.0.1-rc1/
> >
> > Release artifacts are signed with the following key:
> > https://people.apache.org/keys/committer/pwendell.asc
> >
> > The staging repository for this release can be found at:
> > https://repository.apache.org/content/repositories/orgapachespark-1020/
> >
> > The documentation corresponding to this release can be found at:
> > http://people.apache.org/~pwendell/spark-1.0.1-rc1-docs/
> >
> > Please vote on releasing this package as Apache Spark 1.0.1!
> >
> > The vote is open until Monday, June 30, at 03:00 UTC and passes if
> > a majority of at least 3 +1 PMC votes are cast.
> >
> > [ ] +1 Release this package as Apache Spark 1.0.1
> > [ ] -1 Do not release this package because ...
> >
> > To learn more about Apache Spark, please see
> > http://spark.apache.org/
> >
> > === About this release ===
> > This release fixes a few high-priority bugs in 1.0 and has a variety
> > of smaller fixes. The full list is here: http://s.apache.org/b45. Some
> > of the more visible patches are:
> >
> > SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
> > SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame
> size.
> > SPARK-1790: Support r3 instance types on EC2.
> >
> > This is the first maintenance release on the 1.0 line. We plan to make
> > additional maintenance releases as new fixes come in.
> >
> > - Patrick
>
>

--047d7b6dce6a22bcf004fcd90532--

From dev-return-8129-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 22:43:04 2014
Return-Path: <dev-return-8129-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7A3581149D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 22:43:04 +0000 (UTC)
Received: (qmail 13845 invoked by uid 500); 27 Jun 2014 22:43:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13786 invoked by uid 500); 27 Jun 2014 22:43:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13765 invoked by uid 99); 27 Jun 2014 22:43:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 22:43:03 +0000
X-ASF-Spam-Status: No, hits=2.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.43 as permitted sender)
Received: from [209.85.192.43] (HELO mail-qg0-f43.google.com) (209.85.192.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 22:42:59 +0000
Received: by mail-qg0-f43.google.com with SMTP id z60so80046qgd.2
        for <dev@spark.apache.org>; Fri, 27 Jun 2014 15:42:38 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=YTbJpYARFXwGL+HR7AvHt2amPqE8IqgrFtO4fIIaUHk=;
        b=KzukleM06RdrOCjz5tRgjoFkMvYC+1jg5TVyC7ZZUrc7EkGGJIaIu/nrZEozqhlqR5
         Nvpa+VjSxisuLSatNAN9oLoD448JIliVerZuEbyB98e+PEFZw+oMWVqGGgHA8crabWAk
         ShJJDeQqGXKvrcGXwNt6jCk9JmiyWWnn6RwIw9B6oOiMuFs7eZXJmj5A6KMuj8Lfb476
         kSTJ0N0J+WIY9yTLlA+IK5WCGlxfqYfChlrUSOM2Rk2ZXU5tOKS3i/gN+TTs8wEI9VVl
         djoOsN2xCjgXI6tK7eTyfn1Kqwz7fyCO8puWIx4d9KDCcxUw35dinxcL6DKbfa1HxvlN
         r8iw==
MIME-Version: 1.0
X-Received: by 10.140.19.21 with SMTP id 21mr36555996qgg.76.1403908958513;
 Fri, 27 Jun 2014 15:42:38 -0700 (PDT)
Received: by 10.140.85.149 with HTTP; Fri, 27 Jun 2014 15:42:38 -0700 (PDT)
In-Reply-To: <CALW2ey3QdNmASQFbuGyN6pbzmUVXuq-i=i6gRktrabk6YyFYpQ@mail.gmail.com>
References: <CA+B-+fwtVvTEGghCywJFEjcXtrH7E3i61z8WfxisiihNZxy-fA@mail.gmail.com>
	<CALW2ey3QdNmASQFbuGyN6pbzmUVXuq-i=i6gRktrabk6YyFYpQ@mail.gmail.com>
Date: Fri, 27 Jun 2014 15:42:38 -0700
Message-ID: <CA+B-+fx7G-1Fv-2rbuPiMUoGd7eY5gsLupLsaniUtqjpXvEkhQ@mail.gmail.com>
Subject: Re: Linear CG solver
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113550d6c4f34f04fcd90608
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113550d6c4f34f04fcd90608
Content-Type: text/plain; charset=UTF-8

Thanks David...Let me try it...I am keen to see the results first and later
will look into runtime optimizations...

Deb





On Fri, Jun 27, 2014 at 3:12 PM, David Hall <dlwh@cs.berkeley.edu> wrote:

> I have no ideas on benchmarks, but breeze has a CG solver:
>
> https://github.com/scalanlp/breeze/tree/master/math/src/main/scala/breeze/optimize/linear/ConjugateGradient.scala
>
>
> https://github.com/scalanlp/breeze/blob/e2adad3b885736baf890b306806a56abc77a3ed3/math/src/test/scala/breeze/optimize/linear/ConjugateGradientTest.scala
>
> It's based on the code from TRON, and so I think it's more targeted for
> norm-constrained solutions of the CG problem.
>
>
>
>
>
>
>
>
> On Fri, Jun 27, 2014 at 5:54 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>
> > Hi,
> >
> > I am looking for an efficient linear CG to be put inside the Quadratic
> > Minimization algorithms we added for Spark mllib.
> >
> > With a good linear CG, we should be able to solve kernel SVMs with this
> > solver in mllib...
> >
> > I use direct solves right now using cholesky decomposition which has
> higher
> > complexity as matrix sizes become large...
> >
> > I found out some jblas example code:
> >
> > https://github.com/mikiobraun/jblas-examples/blob/master/src/CG.java
> >
> > I was wondering if mllib developers have any experience using this solver
> > and if this is better than apache commons linear CG ?
> >
> > Thanks.
> > Deb
> >
>

--001a113550d6c4f34f04fcd90608--

From dev-return-8130-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 23:14:38 2014
Return-Path: <dev-return-8130-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8F7B311555
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 23:14:38 +0000 (UTC)
Received: (qmail 66830 invoked by uid 500); 27 Jun 2014 23:14:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66776 invoked by uid 500); 27 Jun 2014 23:14:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66765 invoked by uid 99); 27 Jun 2014 23:14:37 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 23:14:37 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of Yan.Zhou.sc@huawei.com designates 206.16.17.72 as permitted sender)
Received: from [206.16.17.72] (HELO dfwrgout.huawei.com) (206.16.17.72)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 23:14:35 +0000
Received: from 172.18.9.243 (EHLO dfweml701-chm.china.huawei.com) ([172.18.9.243])
	by dfwrg01-dlp.huawei.com (MOS 4.3.7-GA FastPath queued)
	with ESMTP id CDY99803;
	Fri, 27 Jun 2014 18:14:10 -0500 (CDT)
Received: from SJCEML703-CHM.china.huawei.com (10.212.94.49) by
 dfweml701-chm.china.huawei.com (10.193.5.50) with Microsoft SMTP Server (TLS)
 id 14.3.158.1; Fri, 27 Jun 2014 16:14:10 -0700
Received: from SJCEML701-CHM.china.huawei.com ([169.254.3.37]) by
 SJCEML703-CHM.china.huawei.com ([169.254.5.229]) with mapi id 14.03.0158.001;
 Fri, 27 Jun 2014 16:14:04 -0700
From: "Yan Zhou.sc" <Yan.Zhou.sc@huawei.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: IntelliJ IDEA cannot compile TreeNode.scala
Thread-Topic: IntelliJ IDEA cannot compile TreeNode.scala
Thread-Index: Ac+RsY+GtRXlA17OTrubDGHwGKyWYQARPzYAABhed9A=
Date: Fri, 27 Jun 2014 23:14:03 +0000
Message-ID: <C434A3773D08A842B26FED6A1BA2E6546D13BBA1@SJCEML701-CHM.china.huawei.com>
References: <54541F80AA2A6F47BDC10C496192CA75D899B9@szxeml558-mbs.china.huawei.com>
 <CAPh_B=bQNio8Cp57te7j4on7ek0OJzP7E6K+PFAmyxnPcMRRaQ@mail.gmail.com>
In-Reply-To: <CAPh_B=bQNio8Cp57te7j4on7ek0OJzP7E6K+PFAmyxnPcMRRaQ@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [10.193.36.52]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-CFilter-Loop: Reflected
X-Virus-Checked: Checked by ClamAV on apache.org
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From dev-return-8131-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jun 27 23:28:14 2014
Return-Path: <dev-return-8131-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 95803115CA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Jun 2014 23:28:14 +0000 (UTC)
Received: (qmail 94012 invoked by uid 500); 27 Jun 2014 23:28:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93948 invoked by uid 500); 27 Jun 2014 23:28:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93935 invoked by uid 99); 27 Jun 2014 23:28:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 23:28:13 +0000
X-ASF-Spam-Status: No, hits=2.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.192.169] (HELO mail-pd0-f169.google.com) (209.85.192.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Jun 2014 23:28:11 +0000
Received: by mail-pd0-f169.google.com with SMTP id g10so5058904pdj.0
        for <dev@spark.apache.org>; Fri, 27 Jun 2014 16:27:46 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=aom+iqBIBROJs8MAWfd94U7SOenFdeAgadaCDinOPIg=;
        b=gXHuno+nna1kV6IMHUwMiTNOlf1dQpnuxWJyJecA2QuesE+1nfvsVSKUf5unwdDoph
         EZFKVQurt3QlyInLQt96wty1+fkWtmATREZW5XFJ5oY2pICEZarnGVeuaUPBOC8pXMtk
         DNn5uexrmFDMB8fd1ITd9It8W3650FRbB9nFAUueQwUvN8frrQ84blZerlXrhtnlYv6s
         K7cyM3H24lVDTO7C7mF7+ENpYAvxrYhzPpFy1O2iZd30NJhqSPHctfgAmxSgtTJAJ51X
         sXNuGYd1qfd/bm2KTQbmufX4f+wDeyHR3R87M/rn+lPX5Cxhe6I3HvNScIWYeh9rxbM9
         u6RA==
X-Gm-Message-State: ALoCoQnLTMqDsza/kWoKXyO8t01X7h7gVr/Q+xxMxaRf3iNw5EaboyO1fbgjNp8HrtUYt2TXlXv+
MIME-Version: 1.0
X-Received: by 10.66.146.170 with SMTP id td10mr35243835pab.105.1403911666170;
 Fri, 27 Jun 2014 16:27:46 -0700 (PDT)
Received: by 10.70.53.8 with HTTP; Fri, 27 Jun 2014 16:27:46 -0700 (PDT)
In-Reply-To: <CAMJOb8kQf6uZE8Ym7PkQB-k1jNH-cbMgZSsWQfqOhUJpp_qU9g@mail.gmail.com>
References: <CABPQxsvAioKpE29GzUPts6tpk=6dSnXpfXMCYwKKX2fKX09Udw@mail.gmail.com>
	<3D27857F-0798-48EB-BC07-622FEB465AA2@gmail.com>
	<CAMJOb8kQf6uZE8Ym7PkQB-k1jNH-cbMgZSsWQfqOhUJpp_qU9g@mail.gmail.com>
Date: Fri, 27 Jun 2014 16:27:46 -0700
Message-ID: <CAMJOb8=EMz5z2na3OgqaTUaCYahEW80LL1eUgiTvEUM83w+hUw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC1)
From: Andrew Or <andrew@databricks.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b6788802894ff04fcd9a81d
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6788802894ff04fcd9a81d
Content-Type: text/plain; charset=UTF-8

(Forgot to mention, that UI bug is not in Spark 1.0.0, so it is technically
a regression)


2014-06-27 15:42 GMT-07:00 Andrew Or <andrew@databricks.com>:

> There is an issue with the SparkUI: the storage page continues to display
> RDDs that are dropped from memory. This is fixed in
> https://github.com/apache/spark/commit/21e0f77b6321590ed86223a60cdb8ae08ea4057f
> but is not part of this RC.
>
>
> 2014-06-27 11:18 GMT-07:00 Matei Zaharia <matei.zaharia@gmail.com>:
>
> +1
>>
>> Tested it out on Mac OS X and Windows, looked through docs.
>>
>> Matei
>>
>> On Jun 26, 2014, at 7:06 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>
>> > Please vote on releasing the following candidate as Apache Spark
>> version 1.0.1!
>> >
>> > The tag to be voted on is v1.0.1-rc1 (commit 7feeda3):
>> >
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7feeda3d729f9397aa15ee8750c01ef5aa601962
>> >
>> > The release files, including signatures, digests, etc. can be found at:
>> > http://people.apache.org/~pwendell/spark-1.0.1-rc1/
>> >
>> > Release artifacts are signed with the following key:
>> > https://people.apache.org/keys/committer/pwendell.asc
>> >
>> > The staging repository for this release can be found at:
>> > https://repository.apache.org/content/repositories/orgapachespark-1020/
>> >
>> > The documentation corresponding to this release can be found at:
>> > http://people.apache.org/~pwendell/spark-1.0.1-rc1-docs/
>> >
>> > Please vote on releasing this package as Apache Spark 1.0.1!
>> >
>> > The vote is open until Monday, June 30, at 03:00 UTC and passes if
>> > a majority of at least 3 +1 PMC votes are cast.
>> >
>> > [ ] +1 Release this package as Apache Spark 1.0.1
>> > [ ] -1 Do not release this package because ...
>> >
>> > To learn more about Apache Spark, please see
>> > http://spark.apache.org/
>> >
>> > === About this release ===
>> > This release fixes a few high-priority bugs in 1.0 and has a variety
>> > of smaller fixes. The full list is here: http://s.apache.org/b45. Some
>> > of the more visible patches are:
>> >
>> > SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
>> > SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame
>> size.
>> > SPARK-1790: Support r3 instance types on EC2.
>> >
>> > This is the first maintenance release on the 1.0 line. We plan to make
>> > additional maintenance releases as new fixes come in.
>> >
>> > - Patrick
>>
>>
>

--047d7b6788802894ff04fcd9a81d--

From dev-return-8132-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jun 28 01:45:17 2014
Return-Path: <dev-return-8132-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3971911902
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Jun 2014 01:45:17 +0000 (UTC)
Received: (qmail 52872 invoked by uid 500); 28 Jun 2014 01:45:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52811 invoked by uid 500); 28 Jun 2014 01:45:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52800 invoked by uid 99); 28 Jun 2014 01:45:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Jun 2014 01:45:16 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [219.142.118.228] (HELO SINA-HUB03.staff.sina.com.cn) (219.142.118.228)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Jun 2014 01:45:12 +0000
Received: from SINA-MBX01.staff.sina.com.cn ([fe80::2d6a:8223:49be:c0c]) by
 SINA-HUB03.staff.sina.com.cn ([fe80::f5c8:67f3:65aa:8d94%14]) with mapi id
 14.02.0247.003; Sat, 28 Jun 2014 09:44:47 +0800
From: =?gb2312?B?sNe41Q==?= <baigang@staff.sina.com.cn>
To: "<dev@spark.apache.org>" <dev@spark.apache.org>
Subject: Re: Contributing to MLlib on GLM
Thread-Topic: Contributing to MLlib on GLM
Thread-Index: AQHPiog/r6mk1iBN0UqEXXCsX5jdr5uBzzqAgAHc9ACAAiPXgQ==
Date: Sat, 28 Jun 2014 01:44:47 +0000
Message-ID: <D350552E-A930-43C0-AB12-FC7E1957EB78@staff.sina.com.cn>
References: <CFC6248F.577%xwei@palantir.com>
 <CAP7bEL1W+B=qdj8bWhDMRApJb5QVqxE1rfxt23wTKSYJbt27Fg@mail.gmail.com>,<1403831040483-7088.post@n3.nabble.com>
In-Reply-To: <1403831040483-7088.post@n3.nabble.com>
Accept-Language: zh-CN, en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
Content-Type: text/plain; charset="gb2312"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

SGkgWGlhb2thaSwNCg0KTXkgYmFkLiBJIGRpZG4ndCBub3RpY2UgdGhpcyBiZWZvcmUgSSBjcmVh
dGVkIGFub3RoZXIgUFIgZm9yIFBvaXNzb24gcmVncmVzc2lvbi4gVGhlIG1haWxzIHdlcmUgYnVy
aWVkIGluIGp1bmsgYnkgdGhlIGNvcnAgbWFpbCBtYXN0ZXIuIEFsc28sIHRoYW5rcyBmb3IgY29u
c2lkZXJpbmcgbXkgY29tbWVudHMgYW5kIGFkdmljZSBpbiB5b3VyIFBSLg0KDQpBZGRpbmcgbXkg
dHdvIGNlbnRzIGhlcmU6DQoNCiogUG9pc3NvblJlZ3Jlc3Npb25Nb2RlbCBhbmQgR2FtbWFSZWdy
ZXNzaW9uTW9kZWwgaGF2ZSB0aGUgc2FtZSBmaWVsZHMgYW5kIHByZWRpY3Rpb24gbWV0aG9kLiBT
aGFsbCB3ZSB1c2Ugb25lIGluc3RlYWQgb2YgdHdvIHJlZHVuZGFudCBjbGFzc2VzPyBTYXksIGEg
TG9nTGluZWFyTW9kZWwuDQoqIFRoZSBMQkZHUyBvcHRpbWl6ZXIgdGFrZXMgZmV3ZXIgaXRlcmF0
aW9ucyBhbmQgcmVzdWx0cyBpbiBiZXR0ZXIgY29udmVyZ2VuY2UgdGhhbiBTR0QuIEkgaW1wbGVt
ZW50ZWQgdHdvIEdlbmVyYWxpemVkTGluZWFyQWxnb3JpdGhtIGNsYXNzZXMgdXNpbmcgTEJGR1Mg
YW5kIFNHRCByZXNwZWN0aXZlbHkuIFlvdSBtYXkgdGFrZSBhIGxvb2sgaW50byBpdC4gSWYgaXQn
cyBPSyB0byB5b3UsIEknZCBiZSBoYXBweSB0byBzZW5kIGEgUFIgdG8geW91ciBicmFuY2guDQoq
IEluIGFkZGl0aW9uIHRvIHRoZSBnZW5lcmF0ZWQgdGVzdCBkYXRhLCBXZSBtYXkgdXNlIHNvbWUg
cmVhbC13b3JsZCBkYXRhIGZvciB0ZXN0aW5nLiBJbiBteSBpbXBsZW1lbnRhdGlvbiwgSSBhZGRl
ZCB0aGUgdGVzdCBkYXRhIGZyb20gaHR0cHM6Ly9vbmxpbmVjb3Vyc2VzLnNjaWVuY2UucHN1LmVk
dS9zdGF0NTA0L25vZGUvMjIzLiBQbGVhc2UgY2hlY2sgbXkgdGVzdCBzdWl0ZS4NCg0KLUdhbmcN
ClNlbnQgZnJvbSBteSBpUGFkDQoNCj4gT24gMjAxNMTqNtTCMjfI1SwgYXQgz8LO5zY6MDMsICJ4
d2VpIiA8d2VpeGlhb2thaUBnbWFpbC5jb20+IHdyb3RlOg0KPiANCj4gDQo+IFllcywgdGhhdCdz
IHdoYXQgd2UgZGlkOiBhZGRpbmcgdHdvIGdyYWRpZW50IGZ1bmN0aW9ucyB0byBHcmFkaWVudC5z
Y2FsYSBhbmQNCj4gY3JlYXRlIFBvaXNzb25SZWdyZXNzaW9uIGFuZCBHYW1tYVJlZ3Jlc3Npb24g
dXNpbmcgdGhlc2UgZ3JhZGllbnRzLiBXZSBtYWRlDQo+IGEgUFIgb24gdGhpcy4NCj4gDQo+IA0K
PiANCj4gLS0NCj4gVmlldyB0aGlzIG1lc3NhZ2UgaW4gY29udGV4dDogaHR0cDovL2FwYWNoZS1z
cGFyay1kZXZlbG9wZXJzLWxpc3QuMTAwMTU1MS5uMy5uYWJibGUuY29tL0NvbnRyaWJ1dGluZy10
by1NTGxpYi1vbi1HTE0tdHA3MDMzcDcwODguaHRtbA0KPiBTZW50IGZyb20gdGhlIEFwYWNoZSBT
cGFyayBEZXZlbG9wZXJzIExpc3QgbWFpbGluZyBsaXN0IGFyY2hpdmUgYXQgTmFiYmxlLmNvbS4N
Cg==

From dev-return-8133-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jun 28 03:39:57 2014
Return-Path: <dev-return-8133-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 87A9D11D98
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Jun 2014 03:39:57 +0000 (UTC)
Received: (qmail 49176 invoked by uid 500); 28 Jun 2014 03:39:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49120 invoked by uid 500); 28 Jun 2014 03:39:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49108 invoked by uid 99); 28 Jun 2014 03:39:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Jun 2014 03:39:56 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ksankar42@gmail.com designates 209.85.220.53 as permitted sender)
Received: from [209.85.220.53] (HELO mail-pa0-f53.google.com) (209.85.220.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Jun 2014 03:39:51 +0000
Received: by mail-pa0-f53.google.com with SMTP id ey11so5482084pad.12
        for <dev@spark.apache.org>; Fri, 27 Jun 2014 20:39:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=6dEFo3Q+y8I62mdMRHjlYx9E+pCus22UfWCR+DTxXiM=;
        b=b065z7LkSMCa4PoBl/P74jr/J1w5QXMtZ0x4Xxlxaq0EOsFNf7Bo/rbpYxSGpBYXWX
         c0hc+p4y9cLB4ize4cVhS7wm62e34FBxQCeNrRh2grTjqmDTl/Ui0sA5CsuBQq/mapjY
         vcXyDzoJH7A7zYBWkJTUDod+qJtdBroEGN6mNd2yHIQPb72mMbXKOqn+vMtSbfblyBTq
         9hagwi15CDVcHkCdBebZOBDJ3ZT+kODJcBZYktCNuamJTbCVRxTOTWZ5qw0Alon/WYW0
         aSCj0wXnTuRIkldEpvfegyxx1+LouHpgaqLC8x6HP+D6lu8iqyfsa6h9DGC24HtBL6k1
         BIRA==
MIME-Version: 1.0
X-Received: by 10.68.226.105 with SMTP id rr9mr35253114pbc.161.1403926771033;
 Fri, 27 Jun 2014 20:39:31 -0700 (PDT)
Received: by 10.70.12.2 with HTTP; Fri, 27 Jun 2014 20:39:30 -0700 (PDT)
In-Reply-To: <CABPQxsvAioKpE29GzUPts6tpk=6dSnXpfXMCYwKKX2fKX09Udw@mail.gmail.com>
References: <CABPQxsvAioKpE29GzUPts6tpk=6dSnXpfXMCYwKKX2fKX09Udw@mail.gmail.com>
Date: Fri, 27 Jun 2014 20:39:30 -0700
Message-ID: <CAOTBr2npGLc852R8s7j8PtaQKr5jqzqtnDnJ8vDRRi+p06ycxw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC1)
From: Krishna Sankar <ksankar42@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=e89a8ff1ce787a6be304fcdd2c4f
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8ff1ce787a6be304fcdd2c4f
Content-Type: text/plain; charset=UTF-8

+1
Compiled for CentOS 6.5, deployed in our 4 node cluster (Hadoop 2.2, YARN)
Smoke Tests (sparkPi,spark-shell, web UI) successful

Cheers
<k/>


On Thu, Jun 26, 2014 at 7:06 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Please vote on releasing the following candidate as Apache Spark version
> 1.0.1!
>
> The tag to be voted on is v1.0.1-rc1 (commit 7feeda3):
>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7feeda3d729f9397aa15ee8750c01ef5aa601962
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.0.1-rc1/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1020/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.0.1-rc1-docs/
>
> Please vote on releasing this package as Apache Spark 1.0.1!
>
> The vote is open until Monday, June 30, at 03:00 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.0.1
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> === About this release ===
> This release fixes a few high-priority bugs in 1.0 and has a variety
> of smaller fixes. The full list is here: http://s.apache.org/b45. Some
> of the more visible patches are:
>
> SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
> SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame size.
> SPARK-1790: Support r3 instance types on EC2.
>
> This is the first maintenance release on the 1.0 line. We plan to make
> additional maintenance releases as new fixes come in.
>
> - Patrick
>

--e89a8ff1ce787a6be304fcdd2c4f--

From dev-return-8134-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jun 28 09:46:46 2014
Return-Path: <dev-return-8134-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E693D11239
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Jun 2014 09:46:46 +0000 (UTC)
Received: (qmail 7105 invoked by uid 500); 28 Jun 2014 09:46:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7041 invoked by uid 500); 28 Jun 2014 09:46:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7030 invoked by uid 99); 28 Jun 2014 09:46:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Jun 2014 09:46:45 +0000
X-ASF-Spam-Status: No, hits=4.5 required=10.0
	tests=HTML_MESSAGE,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of shankark+sys@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Jun 2014 09:46:41 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <shankark+sys@gmail.com>)
	id 1X0pDA-0007MP-MT
	for dev@spark.incubator.apache.org; Sat, 28 Jun 2014 02:46:20 -0700
Date: Sat, 28 Jun 2014 02:46:20 -0700 (PDT)
From: Krakna H <shankark+sys@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <CAA+rBFF58YtBceUfE6U76bzbNB9KuydQO+uT6-ZgwTc6XN6S5A@mail.gmail.com>
In-Reply-To: <CA+B-+fw35hCRMWuLVUxreYATFWoBLCHhBZs04gdwv+mPmVXZ6Q@mail.gmail.com>
References: <CA+B-+fypoW8=frGnV=G_C12=HwHMWLKiz4OAjL-fSJgr82cgtg@mail.gmail.com> <CAH3_EVP+=m6BGoEzoWGX1mqVEV3B7nw1rZLmonOZBu_xSABEXw@mail.gmail.com> <CA+B-+fymrGhk8jn1a1_V==ZPk3Grf++ed601tkBcfDk7Y-=nMQ@mail.gmail.com> <CAH3_EVOj6RtYgkk+G+5FRGqrWfO3y==4BVyK0RNRQceHmLFqzg@mail.gmail.com> <1403865656657-7097.post@n3.nabble.com> <CA+B-+fw35hCRMWuLVUxreYATFWoBLCHhBZs04gdwv+mPmVXZ6Q@mail.gmail.com>
Subject: Re: Spark Matrix Factorization
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_53611_16824117.1403948780688"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_53611_16824117.1403948780688
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit

Hi Deb,

Thanks so much for your response! At this point, we haven't determined
which of DSGD/ALS to go with and were waiting on guidance like yours to
tell us what the right option would be. It looks like ALS seems to be good
enough for our purposes.

Regards.


On Fri, Jun 27, 2014 at 12:47 PM, Debasish Das [via Apache Spark Developers
List] <ml-node+s1001551n7098h95@n3.nabble.com> wrote:

> Hi,
>
> In my experiments with Jellyfish I did not see any substantial RMSE loss
> over DSGD for Netflix dataset...
>
> So we decided to stick with ALS and implemented a family of Quadratic
> Minimization solvers that stays in the ALS realm but can solve interesting
> constraints(positivity, bounds, L1, equality constrained bounds etc)...We
> are going to show it at the Spark Summit...Also ALS structure is favorable
> to matrix factorization use-cases where missing entries means zero and you
> want to compute a global gram matrix using broadcast and use that for each
> Quadratic Minimization for all users/products...
>
> Implementing DSGD in the data partitioning that Spark ALS uses will be
> straightforward but I would be more keen to see a dataset where DSGD is
> showing you better RMSEs than ALS....
>
> If you have a dataset where DSGD produces much better result could you
> please point it to us ?
>
> Also you can use Jellyfish to run DSGD benchmarks to compare against
> ALS...It is multithreaded and if you have good RAM, you should be able to
> run fairly large datasets...
>
> Be careful about the default Jellyfish...it has been tuned for netflix
> dataset (regularization, rating normalization etc)...So before you compare
> RMSE make sure ALS and Jellyfish is running same algorithm (L2 regularized
> Quadratic Loss)....
>
> Thanks.
> Deb
>
>
> On Fri, Jun 27, 2014 at 3:40 AM, Krakna H <[hidden email]
> <http://user/SendEmail.jtp?type=node&node=7098&i=0>> wrote:
>
> > Hi all,
> >
> > Just found this thread -- is there an update on including DSGD in Spark?
> We
> > have a project that entails topic modeling on a document-term matrix
> using
> > matrix factorization, and were wondering if we should use ALS or attempt
> > writing our own matrix factorization implementation on top of Spark.
> >
> > Thanks.
> >
> >
> >
> > --
> > View this message in context:
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-Matrix-Factorization-tp55p7097.html
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> >
>
>
> ------------------------------
>  If you reply to this email, your message will be added to the discussion
> below:
>
> http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-Matrix-Factorization-tp55p7098.html
>  To start a new topic under Apache Spark Developers List, email
> ml-node+s1001551n1h88@n3.nabble.com
> To unsubscribe from Apache Spark Developers List, click here
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=c2hhbmthcmsrc3lzQGdtYWlsLmNvbXwxfDk3NjU5Mzg0>
> .
> NAML
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-Matrix-Factorization-tp55p7109.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
------=_Part_53611_16824117.1403948780688--

From dev-return-8135-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jun 28 14:56:59 2014
Return-Path: <dev-return-8135-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3F0E511821
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Jun 2014 14:56:59 +0000 (UTC)
Received: (qmail 77878 invoked by uid 500); 28 Jun 2014 14:56:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77812 invoked by uid 500); 28 Jun 2014 14:56:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77800 invoked by uid 99); 28 Jun 2014 14:56:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Jun 2014 14:56:58 +0000
X-ASF-Spam-Status: No, hits=3.1 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.178 as permitted sender)
Received: from [209.85.216.178] (HELO mail-qc0-f178.google.com) (209.85.216.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Jun 2014 14:56:55 +0000
Received: by mail-qc0-f178.google.com with SMTP id c9so5485704qcz.23
        for <dev@spark.apache.org>; Sat, 28 Jun 2014 07:56:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=J3v+D17NtLf+RntmDYudF4Tw2Y7IwuwVHhIMKvTDTtI=;
        b=YPuyreuOQHnL5n1RuRcw3KpEO+L+s5K+9gIOMsSsV4XhkcujWIxGvEHWR7PSBf++IM
         ZCP8EgeQxphpURXtjzdiyPrtFDsv7KavENLavZ8ZqVt4vlslnQYGJx6o1oYiNHuBI904
         Crmw2UMXu3kJ+8PyfxrY2h2LVv0KhK50UWizFditQcPq6pIo8YWdddQYzW+xTOE7P5PH
         Jfg6nA+nRZjeTTdnk06Edt5pd78mcuRt8pYgeMlJxFvKv7ZsITWbGEy7nJt4RDp3jWW/
         D9d32GRH6NslvxJA5IHLPnqVOw00K+y2dvbkjbHXMZunlrIaG4j06EVk8UWnjxlqrxVA
         rc+w==
MIME-Version: 1.0
X-Received: by 10.140.28.37 with SMTP id 34mr42143097qgy.28.1403967390472;
 Sat, 28 Jun 2014 07:56:30 -0700 (PDT)
Received: by 10.140.85.149 with HTTP; Sat, 28 Jun 2014 07:56:30 -0700 (PDT)
In-Reply-To: <CAA+rBFF58YtBceUfE6U76bzbNB9KuydQO+uT6-ZgwTc6XN6S5A@mail.gmail.com>
References: <CA+B-+fypoW8=frGnV=G_C12=HwHMWLKiz4OAjL-fSJgr82cgtg@mail.gmail.com>
	<CAH3_EVP+=m6BGoEzoWGX1mqVEV3B7nw1rZLmonOZBu_xSABEXw@mail.gmail.com>
	<CA+B-+fymrGhk8jn1a1_V==ZPk3Grf++ed601tkBcfDk7Y-=nMQ@mail.gmail.com>
	<CAH3_EVOj6RtYgkk+G+5FRGqrWfO3y==4BVyK0RNRQceHmLFqzg@mail.gmail.com>
	<1403865656657-7097.post@n3.nabble.com>
	<CA+B-+fw35hCRMWuLVUxreYATFWoBLCHhBZs04gdwv+mPmVXZ6Q@mail.gmail.com>
	<CAA+rBFF58YtBceUfE6U76bzbNB9KuydQO+uT6-ZgwTc6XN6S5A@mail.gmail.com>
Date: Sat, 28 Jun 2014 07:56:30 -0700
Message-ID: <CA+B-+fxCyMQvP1F6aM86B3qn_F1jhzJDrcootvx7Pj5P7gCKqg@mail.gmail.com>
Subject: Re: Spark Matrix Factorization
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11393d4295e11904fce6a15d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11393d4295e11904fce6a15d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Factorization problems are non-convex and so both ALS and DSGD will
converge to local minima and it is not clear which minima will be better
than the other until we run both the algorithms and see...

So I will still say get a DSGD version running in the test setup while you
experiment with the Spark ALS...so that you can see if on your particular
dataset DSGD is converging to a better minima...

If you want I can put the DSGD code base that I used for experimentation on
github...I am not sure if Professor Re already put it on github...


On Sat, Jun 28, 2014 at 2:46 AM, Krakna H <shankark+sys@gmail.com> wrote:

> Hi Deb,
>
> Thanks so much for your response! At this point, we haven't determined
> which of DSGD/ALS to go with and were waiting on guidance like yours to
> tell us what the right option would be. It looks like ALS seems to be goo=
d
> enough for our purposes.
>
> Regards.
>
>
> On Fri, Jun 27, 2014 at 12:47 PM, Debasish Das [via Apache Spark Develope=
rs
> List] <ml-node+s1001551n7098h95@n3.nabble.com> wrote:
>
> > Hi,
> >
> > In my experiments with Jellyfish I did not see any substantial RMSE los=
s
> > over DSGD for Netflix dataset...
> >
> > So we decided to stick with ALS and implemented a family of Quadratic
> > Minimization solvers that stays in the ALS realm but can solve
> interesting
> > constraints(positivity, bounds, L1, equality constrained bounds etc)...=
We
> > are going to show it at the Spark Summit...Also ALS structure is
> favorable
> > to matrix factorization use-cases where missing entries means zero and
> you
> > want to compute a global gram matrix using broadcast and use that for
> each
> > Quadratic Minimization for all users/products...
> >
> > Implementing DSGD in the data partitioning that Spark ALS uses will be
> > straightforward but I would be more keen to see a dataset where DSGD is
> > showing you better RMSEs than ALS....
> >
> > If you have a dataset where DSGD produces much better result could you
> > please point it to us ?
> >
> > Also you can use Jellyfish to run DSGD benchmarks to compare against
> > ALS...It is multithreaded and if you have good RAM, you should be able =
to
> > run fairly large datasets...
> >
> > Be careful about the default Jellyfish...it has been tuned for netflix
> > dataset (regularization, rating normalization etc)...So before you
> compare
> > RMSE make sure ALS and Jellyfish is running same algorithm (L2
> regularized
> > Quadratic Loss)....
> >
> > Thanks.
> > Deb
> >
> >
> > On Fri, Jun 27, 2014 at 3:40 AM, Krakna H <[hidden email]
> > <http://user/SendEmail.jtp?type=3Dnode&node=3D7098&i=3D0>> wrote:
> >
> > > Hi all,
> > >
> > > Just found this thread -- is there an update on including DSGD in
> Spark?
> > We
> > > have a project that entails topic modeling on a document-term matrix
> > using
> > > matrix factorization, and were wondering if we should use ALS or
> attempt
> > > writing our own matrix factorization implementation on top of Spark.
> > >
> > > Thanks.
> > >
> > >
> > >
> > > --
> > > View this message in context:
> > >
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-Matrix-Fa=
ctorization-tp55p7097.html
> > > Sent from the Apache Spark Developers List mailing list archive at
> > > Nabble.com.
> > >
> >
> >
> > ------------------------------
> >  If you reply to this email, your message will be added to the discussi=
on
> > below:
> >
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-Matrix-Fa=
ctorization-tp55p7098.html
> >  To start a new topic under Apache Spark Developers List, email
> > ml-node+s1001551n1h88@n3.nabble.com
> > To unsubscribe from Apache Spark Developers List, click here
> > <
> http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlSe=
rvlet.jtp?macro=3Dunsubscribe_by_code&node=3D1&code=3Dc2hhbmthcmsrc3lzQGdtY=
WlsLmNvbXwxfDk3NjU5Mzg0
> >
> > .
> > NAML
> > <
> http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlSe=
rvlet.jtp?macro=3Dmacro_viewer&id=3Dinstant_html%21nabble%3Aemail.naml&base=
=3Dnabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNam=
espace-nabble.view.web.template.NodeNamespace&breadcrumbs=3Dnotify_subscrib=
ers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant=
_email%21nabble%3Aemail.naml
> >
> >
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-Matrix-Fa=
ctorization-tp55p7109.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11393d4295e11904fce6a15d--

From dev-return-8136-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jun 28 15:56:05 2014
Return-Path: <dev-return-8136-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 689F4118F0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Jun 2014 15:56:05 +0000 (UTC)
Received: (qmail 51662 invoked by uid 500); 28 Jun 2014 15:56:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51593 invoked by uid 500); 28 Jun 2014 15:56:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51570 invoked by uid 99); 28 Jun 2014 15:56:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Jun 2014 15:56:04 +0000
X-ASF-Spam-Status: No, hits=4.5 required=10.0
	tests=HTML_MESSAGE,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of shankark+sys@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Jun 2014 15:55:59 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <shankark+sys@gmail.com>)
	id 1X0uyY-00077l-Tq
	for dev@spark.incubator.apache.org; Sat, 28 Jun 2014 08:55:38 -0700
Date: Sat, 28 Jun 2014 08:55:38 -0700 (PDT)
From: Krakna H <shankark+sys@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <CAA+rBFEny6+WUm7BJF8FC12oWsRjT7UpUFce9_qv0E3so0G8Qg@mail.gmail.com>
In-Reply-To: <CA+B-+fxCyMQvP1F6aM86B3qn_F1jhzJDrcootvx7Pj5P7gCKqg@mail.gmail.com>
References: <CA+B-+fypoW8=frGnV=G_C12=HwHMWLKiz4OAjL-fSJgr82cgtg@mail.gmail.com> <CAH3_EVP+=m6BGoEzoWGX1mqVEV3B7nw1rZLmonOZBu_xSABEXw@mail.gmail.com> <CA+B-+fymrGhk8jn1a1_V==ZPk3Grf++ed601tkBcfDk7Y-=nMQ@mail.gmail.com> <CAH3_EVOj6RtYgkk+G+5FRGqrWfO3y==4BVyK0RNRQceHmLFqzg@mail.gmail.com> <1403865656657-7097.post@n3.nabble.com> <CA+B-+fw35hCRMWuLVUxreYATFWoBLCHhBZs04gdwv+mPmVXZ6Q@mail.gmail.com> <CAA+rBFF58YtBceUfE6U76bzbNB9KuydQO+uT6-ZgwTc6XN6S5A@mail.gmail.com> <CA+B-+fxCyMQvP1F6aM86B3qn_F1jhzJDrcootvx7Pj5P7gCKqg@mail.gmail.com>
Subject: Re: Spark Matrix Factorization
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_58058_25871837.1403970938917"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_58058_25871837.1403970938917
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit

Hi Deb,

Putting your code on github will be much appreciated -- it will give us a
good starting point to adapt for our purposes.

Regards.


On Sat, Jun 28, 2014 at 10:57 AM, Debasish Das [via Apache Spark Developers
List] <ml-node+s1001551n7110h67@n3.nabble.com> wrote:

> Factorization problems are non-convex and so both ALS and DSGD will
> converge to local minima and it is not clear which minima will be better
> than the other until we run both the algorithms and see...
>
> So I will still say get a DSGD version running in the test setup while you
> experiment with the Spark ALS...so that you can see if on your particular
> dataset DSGD is converging to a better minima...
>
> If you want I can put the DSGD code base that I used for experimentation
> on
> github...I am not sure if Professor Re already put it on github...
>
>
> On Sat, Jun 28, 2014 at 2:46 AM, Krakna H <[hidden email]
> <http://user/SendEmail.jtp?type=node&node=7110&i=0>> wrote:
>
> > Hi Deb,
> >
> > Thanks so much for your response! At this point, we haven't determined
> > which of DSGD/ALS to go with and were waiting on guidance like yours to
> > tell us what the right option would be. It looks like ALS seems to be
> good
> > enough for our purposes.
> >
> > Regards.
> >
> >
> > On Fri, Jun 27, 2014 at 12:47 PM, Debasish Das [via Apache Spark
> Developers
> > List] <[hidden email]
> <http://user/SendEmail.jtp?type=node&node=7110&i=1>> wrote:
> >
> > > Hi,
> > >
> > > In my experiments with Jellyfish I did not see any substantial RMSE
> loss
> > > over DSGD for Netflix dataset...
> > >
> > > So we decided to stick with ALS and implemented a family of Quadratic
> > > Minimization solvers that stays in the ALS realm but can solve
> > interesting
> > > constraints(positivity, bounds, L1, equality constrained bounds
> etc)...We
> > > are going to show it at the Spark Summit...Also ALS structure is
> > favorable
> > > to matrix factorization use-cases where missing entries means zero and
> > you
> > > want to compute a global gram matrix using broadcast and use that for
> > each
> > > Quadratic Minimization for all users/products...
> > >
> > > Implementing DSGD in the data partitioning that Spark ALS uses will be
> > > straightforward but I would be more keen to see a dataset where DSGD
> is
> > > showing you better RMSEs than ALS....
> > >
> > > If you have a dataset where DSGD produces much better result could you
> > > please point it to us ?
> > >
> > > Also you can use Jellyfish to run DSGD benchmarks to compare against
> > > ALS...It is multithreaded and if you have good RAM, you should be able
> to
> > > run fairly large datasets...
> > >
> > > Be careful about the default Jellyfish...it has been tuned for netflix
> > > dataset (regularization, rating normalization etc)...So before you
> > compare
> > > RMSE make sure ALS and Jellyfish is running same algorithm (L2
> > regularized
> > > Quadratic Loss)....
> > >
> > > Thanks.
> > > Deb
> > >
> > >
> > > On Fri, Jun 27, 2014 at 3:40 AM, Krakna H <[hidden email]
> > > <http://user/SendEmail.jtp?type=node&node=7098&i=0>> wrote:
> > >
> > > > Hi all,
> > > >
> > > > Just found this thread -- is there an update on including DSGD in
> > Spark?
> > > We
> > > > have a project that entails topic modeling on a document-term matrix
> > > using
> > > > matrix factorization, and were wondering if we should use ALS or
> > attempt
> > > > writing our own matrix factorization implementation on top of Spark.
> > > >
> > > > Thanks.
> > > >
> > > >
> > > >
> > > > --
> > > > View this message in context:
> > > >
> > >
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-Matrix-Factorization-tp55p7097.html
> > > > Sent from the Apache Spark Developers List mailing list archive at
> > > > Nabble.com.
> > > >
> > >
> > >
> > > ------------------------------
> > >  If you reply to this email, your message will be added to the
> discussion
> > > below:
> > >
> > >
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-Matrix-Factorization-tp55p7098.html
> > >  To start a new topic under Apache Spark Developers List, email
> > > [hidden email] <http://user/SendEmail.jtp?type=node&node=7110&i=2>
> > > To unsubscribe from Apache Spark Developers List, click here
> > > <
> >
> >
> > > .
> > > NAML
> > > <
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
>
> > >
> > >
> >
> >
> >
> >
> > --
> > View this message in context:
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-Matrix-Factorization-tp55p7109.html
>
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> >
>
>
> ------------------------------
>  If you reply to this email, your message will be added to the discussion
> below:
>
> http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-Matrix-Factorization-tp55p7110.html
>  To start a new topic under Apache Spark Developers List, email
> ml-node+s1001551n1h88@n3.nabble.com
> To unsubscribe from Apache Spark Developers List, click here
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=c2hhbmthcmsrc3lzQGdtYWlsLmNvbXwxfDk3NjU5Mzg0>
> .
> NAML
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spark-Matrix-Factorization-tp55p7111.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
------=_Part_58058_25871837.1403970938917--

From dev-return-8137-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jun 28 18:27:15 2014
Return-Path: <dev-return-8137-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DB10411B19
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Jun 2014 18:27:14 +0000 (UTC)
Received: (qmail 79073 invoked by uid 500); 28 Jun 2014 18:27:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79010 invoked by uid 500); 28 Jun 2014 18:27:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78996 invoked by uid 99); 28 Jun 2014 18:27:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Jun 2014 18:27:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of minnesota.cs@gmail.com designates 209.85.219.54 as permitted sender)
Received: from [209.85.219.54] (HELO mail-oa0-f54.google.com) (209.85.219.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Jun 2014 18:27:12 +0000
Received: by mail-oa0-f54.google.com with SMTP id eb12so7144742oac.27
        for <dev@spark.apache.org>; Sat, 28 Jun 2014 11:26:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=uOAtk8qtLYMdlmf145xrs5wIEEmr4dQg3QgmEGONZ9s=;
        b=dLGSMzAG0rqSwCQZgosxDGfDmk/L3W68YhAXIdB/wJjHO3y/HdnQxsvN6cysvw/fjJ
         waQZGce2ZBkz8BAJxPkS17Ua0isd8qKzwwbWe+NajwrzBry91mjwYAZ0pHNXOoBRA9jm
         GgfrXHqlnlW9KWNVd9W48XPn7IcAWUBm/nW+wTxm0LR26MMyu6tFPKGWFnE/f7XcbG/X
         6319+QsWTbNp1hMoSW/i2lSOF/HLhVsAwNYoDUCFELbpAuzUCLOk3ONHEDP9ODgMf5cK
         X3Hpf32jQN7qnvHcIrWT5LQD8B7pIuo/Y55JLY7RXVPuoBDM+lzw1nPkuVhGgerGoqqF
         fFog==
MIME-Version: 1.0
X-Received: by 10.60.56.98 with SMTP id z2mr31060630oep.62.1403980007521; Sat,
 28 Jun 2014 11:26:47 -0700 (PDT)
Received: by 10.182.168.68 with HTTP; Sat, 28 Jun 2014 11:26:47 -0700 (PDT)
In-Reply-To: <CA+B-+fx7G-1Fv-2rbuPiMUoGd7eY5gsLupLsaniUtqjpXvEkhQ@mail.gmail.com>
References: <CA+B-+fwtVvTEGghCywJFEjcXtrH7E3i61z8WfxisiihNZxy-fA@mail.gmail.com>
	<CALW2ey3QdNmASQFbuGyN6pbzmUVXuq-i=i6gRktrabk6YyFYpQ@mail.gmail.com>
	<CA+B-+fx7G-1Fv-2rbuPiMUoGd7eY5gsLupLsaniUtqjpXvEkhQ@mail.gmail.com>
Date: Sat, 28 Jun 2014 13:26:47 -0500
Message-ID: <CANN3bXYZsq7iuyAKH6DZz0kGZ+2xcEcoFhu0mNCQNO99tKjH1A@mail.gmail.com>
Subject: Re: Linear CG solver
From: Tom Vacek <minnesota.cs@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c221729ec1be04fce99189
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c221729ec1be04fce99189
Content-Type: text/plain; charset=UTF-8

What flavor of SVM are you trying to support? LSSVM doesn't need a bound
constraint, but most other formulations do.  There have been ideas for
bound-constrained CG, though bounded LBFGS is more common.  I think code
for Nystrom approximations or kernel mappings would be more useful.


On Fri, Jun 27, 2014 at 5:42 PM, Debasish Das <debasish.das83@gmail.com>
wrote:

> Thanks David...Let me try it...I am keen to see the results first and later
> will look into runtime optimizations...
>
> Deb
>
>
>
>
>
> On Fri, Jun 27, 2014 at 3:12 PM, David Hall <dlwh@cs.berkeley.edu> wrote:
>
> > I have no ideas on benchmarks, but breeze has a CG solver:
> >
> >
> https://github.com/scalanlp/breeze/tree/master/math/src/main/scala/breeze/optimize/linear/ConjugateGradient.scala
> >
> >
> >
> https://github.com/scalanlp/breeze/blob/e2adad3b885736baf890b306806a56abc77a3ed3/math/src/test/scala/breeze/optimize/linear/ConjugateGradientTest.scala
> >
> > It's based on the code from TRON, and so I think it's more targeted for
> > norm-constrained solutions of the CG problem.
> >
> >
> >
> >
> >
> >
> >
> >
> > On Fri, Jun 27, 2014 at 5:54 PM, Debasish Das <debasish.das83@gmail.com>
> > wrote:
> >
> > > Hi,
> > >
> > > I am looking for an efficient linear CG to be put inside the Quadratic
> > > Minimization algorithms we added for Spark mllib.
> > >
> > > With a good linear CG, we should be able to solve kernel SVMs with this
> > > solver in mllib...
> > >
> > > I use direct solves right now using cholesky decomposition which has
> > higher
> > > complexity as matrix sizes become large...
> > >
> > > I found out some jblas example code:
> > >
> > > https://github.com/mikiobraun/jblas-examples/blob/master/src/CG.java
> > >
> > > I was wondering if mllib developers have any experience using this
> solver
> > > and if this is better than apache commons linear CG ?
> > >
> > > Thanks.
> > > Deb
> > >
> >
>

--001a11c221729ec1be04fce99189--

From dev-return-8138-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jun 28 18:47:42 2014
Return-Path: <dev-return-8138-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6E49011B3C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Jun 2014 18:47:42 +0000 (UTC)
Received: (qmail 97708 invoked by uid 500); 28 Jun 2014 18:47:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97642 invoked by uid 500); 28 Jun 2014 18:47:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 97629 invoked by uid 99); 28 Jun 2014 18:47:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Jun 2014 18:47:41 +0000
X-ASF-Spam-Status: No, hits=2.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.44 as permitted sender)
Received: from [209.85.192.44] (HELO mail-qg0-f44.google.com) (209.85.192.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Jun 2014 18:47:37 +0000
Received: by mail-qg0-f44.google.com with SMTP id j107so657702qga.3
        for <dev@spark.apache.org>; Sat, 28 Jun 2014 11:47:16 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=nDFtNOOxQSnHCkkyX65WXjARuxGbZHl2dyGb+mUU+RA=;
        b=RRAYATDxBcnjZ5GFC7bvgrmSyNXoBZ8GXVBJa6Vtp5TFW3b0fwnnIEVPQT432x1kVO
         K0EUpjse3U8uJapzL7wmBIYoqDqLbzY0eisv5UUQXBMLVPOmXs8IPOYScUnnouJ4AGIR
         QSu1wHIfXBVzlHj0FamMX5M6pIOMC1K3bhRYuUcKlVMVdFJaBrr2YfPWHCwVzFs2Lbey
         CKXH9RBBhU2Y86UTxfLDuWe9CSA4FPZC4XhK2oLMosKjc97MUZeCpLNArH2G7EqBHc9y
         slkHLJqGkWASRox05dgrDeOEyR4mC08KD5bqU0Pm/A/SG2IpmC76I/StgsAbtj/vW239
         Eejg==
MIME-Version: 1.0
X-Received: by 10.224.7.202 with SMTP id e10mr10765813qae.15.1403981236738;
 Sat, 28 Jun 2014 11:47:16 -0700 (PDT)
Received: by 10.140.85.149 with HTTP; Sat, 28 Jun 2014 11:47:16 -0700 (PDT)
In-Reply-To: <CANN3bXYZsq7iuyAKH6DZz0kGZ+2xcEcoFhu0mNCQNO99tKjH1A@mail.gmail.com>
References: <CA+B-+fwtVvTEGghCywJFEjcXtrH7E3i61z8WfxisiihNZxy-fA@mail.gmail.com>
	<CALW2ey3QdNmASQFbuGyN6pbzmUVXuq-i=i6gRktrabk6YyFYpQ@mail.gmail.com>
	<CA+B-+fx7G-1Fv-2rbuPiMUoGd7eY5gsLupLsaniUtqjpXvEkhQ@mail.gmail.com>
	<CANN3bXYZsq7iuyAKH6DZz0kGZ+2xcEcoFhu0mNCQNO99tKjH1A@mail.gmail.com>
Date: Sat, 28 Jun 2014 11:47:16 -0700
Message-ID: <CA+B-+fw0kjrk0iunviAhcOYFvgaFe5jz-X7cO3BhbxtM3zLZsQ@mail.gmail.com>
Subject: Re: Linear CG solver
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c229e8e3260704fce9da56
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c229e8e3260704fce9da56
Content-Type: text/plain; charset=UTF-8

Hi,

I am coming up with an iterative solver for Equality and bound constrained
quadratic minimization...

I have the cholesky versions running but cholesky does not scale for large
dimensions but works fine for matrix factorization use-cases where ranks
are low..

Minimize 0.5x'Px + q'x
s.t Aeq x = beq
lb <= x <= ub

Based on your decomposition you will end up using linear CG  in x-update or
NLCG/BFGS with bounds...I am not sure which one is better unless I see both
of them running on datasets....

I am hoping we can re-use the solver for SVM variants...

Could you please point to some implementation references for Nystrom
approximations or kernel mappings ?

Thanks.
Deb


On Sat, Jun 28, 2014 at 11:26 AM, Tom Vacek <minnesota.cs@gmail.com> wrote:

> What flavor of SVM are you trying to support? LSSVM doesn't need a bound
> constraint, but most other formulations do.  There have been ideas for
> bound-constrained CG, though bounded LBFGS is more common.  I think code
> for Nystrom approximations or kernel mappings would be more useful.
>
>
> On Fri, Jun 27, 2014 at 5:42 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>
> > Thanks David...Let me try it...I am keen to see the results first and
> later
> > will look into runtime optimizations...
> >
> > Deb
> >
> >
> >
> >
> >
> > On Fri, Jun 27, 2014 at 3:12 PM, David Hall <dlwh@cs.berkeley.edu>
> wrote:
> >
> > > I have no ideas on benchmarks, but breeze has a CG solver:
> > >
> > >
> >
> https://github.com/scalanlp/breeze/tree/master/math/src/main/scala/breeze/optimize/linear/ConjugateGradient.scala
> > >
> > >
> > >
> >
> https://github.com/scalanlp/breeze/blob/e2adad3b885736baf890b306806a56abc77a3ed3/math/src/test/scala/breeze/optimize/linear/ConjugateGradientTest.scala
> > >
> > > It's based on the code from TRON, and so I think it's more targeted for
> > > norm-constrained solutions of the CG problem.
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > >
> > > On Fri, Jun 27, 2014 at 5:54 PM, Debasish Das <
> debasish.das83@gmail.com>
> > > wrote:
> > >
> > > > Hi,
> > > >
> > > > I am looking for an efficient linear CG to be put inside the
> Quadratic
> > > > Minimization algorithms we added for Spark mllib.
> > > >
> > > > With a good linear CG, we should be able to solve kernel SVMs with
> this
> > > > solver in mllib...
> > > >
> > > > I use direct solves right now using cholesky decomposition which has
> > > higher
> > > > complexity as matrix sizes become large...
> > > >
> > > > I found out some jblas example code:
> > > >
> > > > https://github.com/mikiobraun/jblas-examples/blob/master/src/CG.java
> > > >
> > > > I was wondering if mllib developers have any experience using this
> > solver
> > > > and if this is better than apache commons linear CG ?
> > > >
> > > > Thanks.
> > > > Deb
> > > >
> > >
> >
>

--001a11c229e8e3260704fce9da56--

From dev-return-8139-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jun 28 19:13:15 2014
Return-Path: <dev-return-8139-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DF53111B92
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Jun 2014 19:13:14 +0000 (UTC)
Received: (qmail 19428 invoked by uid 500); 28 Jun 2014 19:13:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19377 invoked by uid 500); 28 Jun 2014 19:13:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19361 invoked by uid 99); 28 Jun 2014 19:13:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Jun 2014 19:13:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of minnesota.cs@gmail.com designates 209.85.219.47 as permitted sender)
Received: from [209.85.219.47] (HELO mail-oa0-f47.google.com) (209.85.219.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Jun 2014 19:13:10 +0000
Received: by mail-oa0-f47.google.com with SMTP id n16so7049618oag.20
        for <dev@spark.apache.org>; Sat, 28 Jun 2014 12:12:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=t2/kpSxtadvA5RHR88beDjC4QK498aAewi8KSNm3Vxo=;
        b=gDryfhCMc4eWjOMJJnrV42eqJTzf2WkJ5Br+v08VpkBXlvY9qjSbZ8SNTADUiCBilo
         S0a7vqDru50IFaP8Y/44qTl8oOwAz5nlEb9+WRVuQbaQ3j7OkkrctKvGxz32EA1cQnnB
         0cz3Mp3UOU4YqM8OKXFmZwjGEi6ToRgHFLGhqFBusB2yVPeIeTeDn7qwgjnB9/eTJGm1
         lLpmpWghL8XQUf77rf4LzcMq5XW/2sHU7IfEYx/oguuNUKoV+CY5gnbqMQDfBjzVlLgC
         1CElChbE7fnOJNidkFDO2ID/WIQ6EDnvV62MG4ahgudJWxecspYiNKo1wVp6Jlw5KeM0
         SauA==
MIME-Version: 1.0
X-Received: by 10.182.66.79 with SMTP id d15mr31101802obt.49.1403982769655;
 Sat, 28 Jun 2014 12:12:49 -0700 (PDT)
Received: by 10.182.168.68 with HTTP; Sat, 28 Jun 2014 12:12:49 -0700 (PDT)
In-Reply-To: <CA+B-+fw0kjrk0iunviAhcOYFvgaFe5jz-X7cO3BhbxtM3zLZsQ@mail.gmail.com>
References: <CA+B-+fwtVvTEGghCywJFEjcXtrH7E3i61z8WfxisiihNZxy-fA@mail.gmail.com>
	<CALW2ey3QdNmASQFbuGyN6pbzmUVXuq-i=i6gRktrabk6YyFYpQ@mail.gmail.com>
	<CA+B-+fx7G-1Fv-2rbuPiMUoGd7eY5gsLupLsaniUtqjpXvEkhQ@mail.gmail.com>
	<CANN3bXYZsq7iuyAKH6DZz0kGZ+2xcEcoFhu0mNCQNO99tKjH1A@mail.gmail.com>
	<CA+B-+fw0kjrk0iunviAhcOYFvgaFe5jz-X7cO3BhbxtM3zLZsQ@mail.gmail.com>
Date: Sat, 28 Jun 2014 14:12:49 -0500
Message-ID: <CANN3bXbjOt89TZm3i+tnKVhaxyWQhGShkp6qA6P9nsniMNgDXw@mail.gmail.com>
Subject: Re: Linear CG solver
From: Tom Vacek <minnesota.cs@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=e89a8fb1fca441970804fcea36b0
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8fb1fca441970804fcea36b0
Content-Type: text/plain; charset=UTF-8

What is your general solver?  IPM or simplex or something else?  I have
seen a lot of attempts to apply iterative solvers for the subproblems on
those without much luck because the conditioning of the linear systems gets
worse and worse near the optimum.  IPOPT (interior point method) has an
LBFGS subproblem solver built in, so maybe it's worth a try to see if the
approach would meet your needs.  Methods more amenable to iterative
subproblem solvers might be augmented Lagrangian, nonlinear rescaling, or
Murty's (
http://www.researchgate.net/publication/250180549_A_New_Practically_Efficient_Interior_Point_Method_for_Quadratic_Programming)
methods.

This blog post gives a decent introduction to the kernel approximation
topic:
http://peekaboo-vision.blogspot.com/2012/12/kernel-approximations-for-efficient.html.
Missing is mention of the research into how to choose the best set of
prototype vectors.  (I believe Kmeans on the data is practically best.)  In
the approximation domain, "Fastfood" by Smola, et al. is a neat idea.
That's something I've thought about for MLLib, but I think the numeric
support is lacking in Java land.


On Sat, Jun 28, 2014 at 1:47 PM, Debasish Das <debasish.das83@gmail.com>
wrote:

> Hi,
>
> I am coming up with an iterative solver for Equality and bound constrained
> quadratic minimization...
>
> I have the cholesky versions running but cholesky does not scale for large
> dimensions but works fine for matrix factorization use-cases where ranks
> are low..
>
> Minimize 0.5x'Px + q'x
> s.t Aeq x = beq
> lb <= x <= ub
>
> Based on your decomposition you will end up using linear CG  in x-update or
> NLCG/BFGS with bounds...I am not sure which one is better unless I see both
> of them running on datasets....
>
> I am hoping we can re-use the solver for SVM variants...
>
> Could you please point to some implementation references for Nystrom
> approximations or kernel mappings ?
>
> Thanks.
> Deb
>
>
> On Sat, Jun 28, 2014 at 11:26 AM, Tom Vacek <minnesota.cs@gmail.com>
> wrote:
>
> > What flavor of SVM are you trying to support? LSSVM doesn't need a bound
> > constraint, but most other formulations do.  There have been ideas for
> > bound-constrained CG, though bounded LBFGS is more common.  I think code
> > for Nystrom approximations or kernel mappings would be more useful.
> >
> >
> > On Fri, Jun 27, 2014 at 5:42 PM, Debasish Das <debasish.das83@gmail.com>
> > wrote:
> >
> > > Thanks David...Let me try it...I am keen to see the results first and
> > later
> > > will look into runtime optimizations...
> > >
> > > Deb
> > >
> > >
> > >
> > >
> > >
> > > On Fri, Jun 27, 2014 at 3:12 PM, David Hall <dlwh@cs.berkeley.edu>
> > wrote:
> > >
> > > > I have no ideas on benchmarks, but breeze has a CG solver:
> > > >
> > > >
> > >
> >
> https://github.com/scalanlp/breeze/tree/master/math/src/main/scala/breeze/optimize/linear/ConjugateGradient.scala
> > > >
> > > >
> > > >
> > >
> >
> https://github.com/scalanlp/breeze/blob/e2adad3b885736baf890b306806a56abc77a3ed3/math/src/test/scala/breeze/optimize/linear/ConjugateGradientTest.scala
> > > >
> > > > It's based on the code from TRON, and so I think it's more targeted
> for
> > > > norm-constrained solutions of the CG problem.
> > > >
> > > >
> > > >
> > > >
> > > >
> > > >
> > > >
> > > >
> > > > On Fri, Jun 27, 2014 at 5:54 PM, Debasish Das <
> > debasish.das83@gmail.com>
> > > > wrote:
> > > >
> > > > > Hi,
> > > > >
> > > > > I am looking for an efficient linear CG to be put inside the
> > Quadratic
> > > > > Minimization algorithms we added for Spark mllib.
> > > > >
> > > > > With a good linear CG, we should be able to solve kernel SVMs with
> > this
> > > > > solver in mllib...
> > > > >
> > > > > I use direct solves right now using cholesky decomposition which
> has
> > > > higher
> > > > > complexity as matrix sizes become large...
> > > > >
> > > > > I found out some jblas example code:
> > > > >
> > > > >
> https://github.com/mikiobraun/jblas-examples/blob/master/src/CG.java
> > > > >
> > > > > I was wondering if mllib developers have any experience using this
> > > solver
> > > > > and if this is better than apache commons linear CG ?
> > > > >
> > > > > Thanks.
> > > > > Deb
> > > > >
> > > >
> > >
> >
>

--e89a8fb1fca441970804fcea36b0--

From dev-return-8140-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jun 28 19:25:04 2014
Return-Path: <dev-return-8140-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AE4A911BB5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Jun 2014 19:25:04 +0000 (UTC)
Received: (qmail 31878 invoked by uid 500); 28 Jun 2014 19:25:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31836 invoked by uid 500); 28 Jun 2014 19:25:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31824 invoked by uid 99); 28 Jun 2014 19:25:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Jun 2014 19:25:03 +0000
X-ASF-Spam-Status: No, hits=2.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.45 as permitted sender)
Received: from [209.85.192.45] (HELO mail-qg0-f45.google.com) (209.85.192.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Jun 2014 19:25:00 +0000
Received: by mail-qg0-f45.google.com with SMTP id a108so615064qge.18
        for <dev@spark.apache.org>; Sat, 28 Jun 2014 12:24:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=ezmDLjmbTVox56c/XWsmyrBUM5RjDDpPJ0ErSjo1gIg=;
        b=EdOC+Ul3K8WcG4mhSY84oLwNcJTKlFNFCOvbkPMb5pU59OZkRlGlQdH7sHiA6Dxn+u
         S40U1/iZva2WcajknLhNwwGeqNF0y2Iho0yp4p8QNcgpCi+7g2PsIipwp8YOeMR1jJC7
         K9Ekv8DEBVM/ek3XL14PNa+XCe0h1PX7zTy83jee381n+dFopcbsQy02ZtBWzOuwoPSK
         HK1UsiJwU+zf1ODVBUt7Nmv/qrwnw8/hVUYBkHv03HvnlawpE0ceqam1IkRgsR/YeBZ9
         shg7JIV1lZv04HJOy17Ap+C7sJV6j9UXxVohBcW9cNWE4hF8Fw/gbAc1yYYp+CY+KPDG
         DqqQ==
MIME-Version: 1.0
X-Received: by 10.140.28.37 with SMTP id 34mr43964532qgy.28.1403983479136;
 Sat, 28 Jun 2014 12:24:39 -0700 (PDT)
Received: by 10.140.85.149 with HTTP; Sat, 28 Jun 2014 12:24:39 -0700 (PDT)
In-Reply-To: <CANN3bXbjOt89TZm3i+tnKVhaxyWQhGShkp6qA6P9nsniMNgDXw@mail.gmail.com>
References: <CA+B-+fwtVvTEGghCywJFEjcXtrH7E3i61z8WfxisiihNZxy-fA@mail.gmail.com>
	<CALW2ey3QdNmASQFbuGyN6pbzmUVXuq-i=i6gRktrabk6YyFYpQ@mail.gmail.com>
	<CA+B-+fx7G-1Fv-2rbuPiMUoGd7eY5gsLupLsaniUtqjpXvEkhQ@mail.gmail.com>
	<CANN3bXYZsq7iuyAKH6DZz0kGZ+2xcEcoFhu0mNCQNO99tKjH1A@mail.gmail.com>
	<CA+B-+fw0kjrk0iunviAhcOYFvgaFe5jz-X7cO3BhbxtM3zLZsQ@mail.gmail.com>
	<CANN3bXbjOt89TZm3i+tnKVhaxyWQhGShkp6qA6P9nsniMNgDXw@mail.gmail.com>
Date: Sat, 28 Jun 2014 12:24:39 -0700
Message-ID: <CA+B-+fxFNeTaRpd7h91m-PK1RDuVutfTLhxYDbMiDYVrc74mnw@mail.gmail.com>
Subject: Re: Linear CG solver
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11393d428b676904fcea6022
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11393d428b676904fcea6022
Content-Type: text/plain; charset=UTF-8

Thanks Tom for the pointers...

I have a IPM running on the JVM which uses SOCP formulation for the
quadratic program I wrote above

We are going to show the details of it at the Summit....IPM runtimes and
accuracy give a baseline for the problem that we are solving...

Now we are trying to see how to come up with efficient versions for cases
where the constraints are not that many or not very complicated...which is
what most ML problems have...The idea is to use ADMM decomposition for
them...

If the constraints are LP style complex, it is better to use the IPM with
the SOCP directly...

Let me look into the blog and update you more...with jblas and
breeze-netlib-java I doubt there is any numerics that we cannot do on JVM
!...With these packages we call BLAS libraries from fortran...Missing is
sparse linear algebra from Tim Davis which will be exposed to the JVM in
the IPM package that I built...



On Sat, Jun 28, 2014 at 12:12 PM, Tom Vacek <minnesota.cs@gmail.com> wrote:

> What is your general solver?  IPM or simplex or something else?  I have
> seen a lot of attempts to apply iterative solvers for the subproblems on
> those without much luck because the conditioning of the linear systems gets
> worse and worse near the optimum.  IPOPT (interior point method) has an
> LBFGS subproblem solver built in, so maybe it's worth a try to see if the
> approach would meet your needs.  Methods more amenable to iterative
> subproblem solvers might be augmented Lagrangian, nonlinear rescaling, or
> Murty's (
>
> http://www.researchgate.net/publication/250180549_A_New_Practically_Efficient_Interior_Point_Method_for_Quadratic_Programming
> )
> methods.
>
> This blog post gives a decent introduction to the kernel approximation
> topic:
>
> http://peekaboo-vision.blogspot.com/2012/12/kernel-approximations-for-efficient.html
> .
> Missing is mention of the research into how to choose the best set of
> prototype vectors.  (I believe Kmeans on the data is practically best.)  In
> the approximation domain, "Fastfood" by Smola, et al. is a neat idea.
> That's something I've thought about for MLLib, but I think the numeric
> support is lacking in Java land.
>
>
> On Sat, Jun 28, 2014 at 1:47 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>
> > Hi,
> >
> > I am coming up with an iterative solver for Equality and bound
> constrained
> > quadratic minimization...
> >
> > I have the cholesky versions running but cholesky does not scale for
> large
> > dimensions but works fine for matrix factorization use-cases where ranks
> > are low..
> >
> > Minimize 0.5x'Px + q'x
> > s.t Aeq x = beq
> > lb <= x <= ub
> >
> > Based on your decomposition you will end up using linear CG  in x-update
> or
> > NLCG/BFGS with bounds...I am not sure which one is better unless I see
> both
> > of them running on datasets....
> >
> > I am hoping we can re-use the solver for SVM variants...
> >
> > Could you please point to some implementation references for Nystrom
> > approximations or kernel mappings ?
> >
> > Thanks.
> > Deb
> >
> >
> > On Sat, Jun 28, 2014 at 11:26 AM, Tom Vacek <minnesota.cs@gmail.com>
> > wrote:
> >
> > > What flavor of SVM are you trying to support? LSSVM doesn't need a
> bound
> > > constraint, but most other formulations do.  There have been ideas for
> > > bound-constrained CG, though bounded LBFGS is more common.  I think
> code
> > > for Nystrom approximations or kernel mappings would be more useful.
> > >
> > >
> > > On Fri, Jun 27, 2014 at 5:42 PM, Debasish Das <
> debasish.das83@gmail.com>
> > > wrote:
> > >
> > > > Thanks David...Let me try it...I am keen to see the results first and
> > > later
> > > > will look into runtime optimizations...
> > > >
> > > > Deb
> > > >
> > > >
> > > >
> > > >
> > > >
> > > > On Fri, Jun 27, 2014 at 3:12 PM, David Hall <dlwh@cs.berkeley.edu>
> > > wrote:
> > > >
> > > > > I have no ideas on benchmarks, but breeze has a CG solver:
> > > > >
> > > > >
> > > >
> > >
> >
> https://github.com/scalanlp/breeze/tree/master/math/src/main/scala/breeze/optimize/linear/ConjugateGradient.scala
> > > > >
> > > > >
> > > > >
> > > >
> > >
> >
> https://github.com/scalanlp/breeze/blob/e2adad3b885736baf890b306806a56abc77a3ed3/math/src/test/scala/breeze/optimize/linear/ConjugateGradientTest.scala
> > > > >
> > > > > It's based on the code from TRON, and so I think it's more targeted
> > for
> > > > > norm-constrained solutions of the CG problem.
> > > > >
> > > > >
> > > > >
> > > > >
> > > > >
> > > > >
> > > > >
> > > > >
> > > > > On Fri, Jun 27, 2014 at 5:54 PM, Debasish Das <
> > > debasish.das83@gmail.com>
> > > > > wrote:
> > > > >
> > > > > > Hi,
> > > > > >
> > > > > > I am looking for an efficient linear CG to be put inside the
> > > Quadratic
> > > > > > Minimization algorithms we added for Spark mllib.
> > > > > >
> > > > > > With a good linear CG, we should be able to solve kernel SVMs
> with
> > > this
> > > > > > solver in mllib...
> > > > > >
> > > > > > I use direct solves right now using cholesky decomposition which
> > has
> > > > > higher
> > > > > > complexity as matrix sizes become large...
> > > > > >
> > > > > > I found out some jblas example code:
> > > > > >
> > > > > >
> > https://github.com/mikiobraun/jblas-examples/blob/master/src/CG.java
> > > > > >
> > > > > > I was wondering if mllib developers have any experience using
> this
> > > > solver
> > > > > > and if this is better than apache commons linear CG ?
> > > > > >
> > > > > > Thanks.
> > > > > > Deb
> > > > > >
> > > > >
> > > >
> > >
> >
>

--001a11393d428b676904fcea6022--

From dev-return-8141-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun 29 00:12:02 2014
Return-Path: <dev-return-8141-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9813111F68
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 29 Jun 2014 00:12:02 +0000 (UTC)
Received: (qmail 24553 invoked by uid 500); 29 Jun 2014 00:12:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24484 invoked by uid 500); 29 Jun 2014 00:12:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24472 invoked by uid 99); 29 Jun 2014 00:12:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 29 Jun 2014 00:12:01 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of evan.sparks@gmail.com designates 209.85.192.176 as permitted sender)
Received: from [209.85.192.176] (HELO mail-pd0-f176.google.com) (209.85.192.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 29 Jun 2014 00:11:57 +0000
Received: by mail-pd0-f176.google.com with SMTP id ft15so6240666pdb.21
        for <dev@spark.apache.org>; Sat, 28 Jun 2014 17:11:37 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=from:content-type:content-transfer-encoding:mime-version:subject
         :message-id:date:references:in-reply-to:to;
        bh=kZFNMOWUscBYvXghl9JIF54XgTJB0EzBRTRUXFLxfx8=;
        b=0MMrO9IDkfYjHnpMEAaQSyjAnbdjsYw5VFCMvX10YdRuX+GyE9vvneeAMysBBwa/6T
         SnjcjVQhraZls0gsyC1eSpIMA1DbDHu8gw7CgEXuDv/N/TWU1P2+0FK3lQSr6LleD/nc
         Yg4xhOOFzKsUWWkwDf0wlBV3KaxawQjbAG2vTTgxF0lNssaJQHkDWMZIgAnwMw+GpfJQ
         dPbf+Tg9ZXcFVQmfCXfBRYLeM2fmN/IFJjDLq0HFL7u8yUlisrOqMd0ZU51NjRAyjfMy
         eWr7eqo5qVH68LTtIX3Mq2+RDZJ3jApxi2z5ztrwKX15pZj1EYTQuRyJqELquMJ5Bo9K
         nufQ==
X-Received: by 10.67.12.171 with SMTP id er11mr41434391pad.132.1404000697054;
        Sat, 28 Jun 2014 17:11:37 -0700 (PDT)
Received: from [10.0.1.116] (c-50-184-126-67.hsd1.ca.comcast.net. [50.184.126.67])
        by mx.google.com with ESMTPSA id nx12sm74658240pab.6.2014.06.28.17.11.35
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sat, 28 Jun 2014 17:11:36 -0700 (PDT)
From: Evan Sparks <evan.sparks@gmail.com>
Content-Type: text/plain;
	charset=us-ascii
Content-Transfer-Encoding: quoted-printable
Mime-Version: 1.0 (1.0)
Subject: Re: Linear CG solver
Message-Id: <961730A5-7755-4A4C-BFED-2FE8C84BC426@gmail.com>
Date: Sat, 28 Jun 2014 17:11:35 -0700
References: <CA+B-+fwtVvTEGghCywJFEjcXtrH7E3i61z8WfxisiihNZxy-fA@mail.gmail.com> <CALW2ey3QdNmASQFbuGyN6pbzmUVXuq-i=i6gRktrabk6YyFYpQ@mail.gmail.com> <CA+B-+fx7G-1Fv-2rbuPiMUoGd7eY5gsLupLsaniUtqjpXvEkhQ@mail.gmail.com> <CANN3bXYZsq7iuyAKH6DZz0kGZ+2xcEcoFhu0mNCQNO99tKjH1A@mail.gmail.com> <CA+B-+fw0kjrk0iunviAhcOYFvgaFe5jz-X7cO3BhbxtM3zLZsQ@mail.gmail.com> <CANN3bXbjOt89TZm3i+tnKVhaxyWQhGShkp6qA6P9nsniMNgDXw@mail.gmail.com>
In-Reply-To: <CANN3bXbjOt89TZm3i+tnKVhaxyWQhGShkp6qA6P9nsniMNgDXw@mail.gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
X-Mailer: iPhone Mail (11D201)
X-Virus-Checked: Checked by ClamAV on apache.org

Hey,

We're actually working on similar ideas in the AMPlab with spark - for examp=
le we've got some image classification pipelines built on this idea - http:/=
/www.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf

Approximating kernel methods via random projections hit with nonlinearity.=20=


Additionally, block coordinate descent seems to work pretty well for these s=
cenarios where you end up with lots of features. An advantage of this approa=
ch in spark is that you can avoid materializing the whole data matrix if you=
're working on a subset of columns at a time.=20

We're hoping to have some code out for public consumption later in the summe=
r.

- Evan

> On Jun 28, 2014, at 12:12 PM, Tom Vacek <minnesota.cs@gmail.com> wrote:
>=20
> What is your general solver?  IPM or simplex or something else?  I have
> seen a lot of attempts to apply iterative solvers for the subproblems on
> those without much luck because the conditioning of the linear systems get=
s
> worse and worse near the optimum.  IPOPT (interior point method) has an
> LBFGS subproblem solver built in, so maybe it's worth a try to see if the
> approach would meet your needs.  Methods more amenable to iterative
> subproblem solvers might be augmented Lagrangian, nonlinear rescaling, or
> Murty's (
> http://www.researchgate.net/publication/250180549_A_New_Practically_Effici=
ent_Interior_Point_Method_for_Quadratic_Programming)
> methods.
>=20
> This blog post gives a decent introduction to the kernel approximation
> topic:
> http://peekaboo-vision.blogspot.com/2012/12/kernel-approximations-for-effi=
cient.html.
> Missing is mention of the research into how to choose the best set of
> prototype vectors.  (I believe Kmeans on the data is practically best.)  I=
n
> the approximation domain, "Fastfood" by Smola, et al. is a neat idea.
> That's something I've thought about for MLLib, but I think the numeric
> support is lacking in Java land.
>=20
>=20
> On Sat, Jun 28, 2014 at 1:47 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>=20
>> Hi,
>>=20
>> I am coming up with an iterative solver for Equality and bound constraine=
d
>> quadratic minimization...
>>=20
>> I have the cholesky versions running but cholesky does not scale for larg=
e
>> dimensions but works fine for matrix factorization use-cases where ranks
>> are low..
>>=20
>> Minimize 0.5x'Px + q'x
>> s.t Aeq x =3D beq
>> lb <=3D x <=3D ub
>>=20
>> Based on your decomposition you will end up using linear CG  in x-update o=
r
>> NLCG/BFGS with bounds...I am not sure which one is better unless I see bo=
th
>> of them running on datasets....
>>=20
>> I am hoping we can re-use the solver for SVM variants...
>>=20
>> Could you please point to some implementation references for Nystrom
>> approximations or kernel mappings ?
>>=20
>> Thanks.
>> Deb
>>=20
>>=20
>> On Sat, Jun 28, 2014 at 11:26 AM, Tom Vacek <minnesota.cs@gmail.com>
>> wrote:
>>=20
>>> What flavor of SVM are you trying to support? LSSVM doesn't need a bound=

>>> constraint, but most other formulations do.  There have been ideas for
>>> bound-constrained CG, though bounded LBFGS is more common.  I think code=

>>> for Nystrom approximations or kernel mappings would be more useful.
>>>=20
>>>=20
>>> On Fri, Jun 27, 2014 at 5:42 PM, Debasish Das <debasish.das83@gmail.com>=

>>> wrote:
>>>=20
>>>> Thanks David...Let me try it...I am keen to see the results first and
>>> later
>>>> will look into runtime optimizations...
>>>>=20
>>>> Deb
>>>>=20
>>>>=20
>>>>=20
>>>>=20
>>>>=20
>>>>> On Fri, Jun 27, 2014 at 3:12 PM, David Hall <dlwh@cs.berkeley.edu>
>>>> wrote:
>>>>=20
>>>>> I have no ideas on benchmarks, but breeze has a CG solver:
>> https://github.com/scalanlp/breeze/tree/master/math/src/main/scala/breeze=
/optimize/linear/ConjugateGradient.scala
>> https://github.com/scalanlp/breeze/blob/e2adad3b885736baf890b306806a56abc=
77a3ed3/math/src/test/scala/breeze/optimize/linear/ConjugateGradientTest.sca=
la
>>>>>=20
>>>>> It's based on the code from TRON, and so I think it's more targeted
>> for
>>>>> norm-constrained solutions of the CG problem.
>>>>>=20
>>>>>=20
>>>>>=20
>>>>>=20
>>>>>=20
>>>>>=20
>>>>>=20
>>>>>=20
>>>>> On Fri, Jun 27, 2014 at 5:54 PM, Debasish Das <
>>> debasish.das83@gmail.com>
>>>>> wrote:
>>>>>=20
>>>>>> Hi,
>>>>>>=20
>>>>>> I am looking for an efficient linear CG to be put inside the
>>> Quadratic
>>>>>> Minimization algorithms we added for Spark mllib.
>>>>>>=20
>>>>>> With a good linear CG, we should be able to solve kernel SVMs with
>>> this
>>>>>> solver in mllib...
>>>>>>=20
>>>>>> I use direct solves right now using cholesky decomposition which
>> has
>>>>> higher
>>>>>> complexity as matrix sizes become large...
>>>>>>=20
>>>>>> I found out some jblas example code:
>> https://github.com/mikiobraun/jblas-examples/blob/master/src/CG.java
>>>>>>=20
>>>>>> I was wondering if mllib developers have any experience using this
>>>> solver
>>>>>> and if this is better than apache commons linear CG ?
>>>>>>=20
>>>>>> Thanks.
>>>>>> Deb
>>=20

From dev-return-8142-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun 29 00:17:31 2014
Return-Path: <dev-return-8142-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0B0EF11F7C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 29 Jun 2014 00:17:31 +0000 (UTC)
Received: (qmail 27982 invoked by uid 500); 29 Jun 2014 00:17:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27918 invoked by uid 500); 29 Jun 2014 00:17:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27907 invoked by uid 99); 29 Jun 2014 00:17:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 29 Jun 2014 00:17:30 +0000
X-ASF-Spam-Status: No, hits=4.5 required=10.0
	tests=HTML_MESSAGE,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of weixiaokai@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 29 Jun 2014 00:17:26 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <weixiaokai@gmail.com>)
	id 1X12np-0005MZ-Cb
	for dev@spark.incubator.apache.org; Sat, 28 Jun 2014 17:17:05 -0700
Date: Sat, 28 Jun 2014 17:17:05 -0700 (PDT)
From: xwei <weixiaokai@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <B3E02C88-8C42-497B-8D78-3B58186A26BB@gmail.com>
In-Reply-To: <D350552E-A930-43C0-AB12-FC7E1957EB78@staff.sina.com.cn>
References: <CFC6248F.577%xwei@palantir.com> <CAP7bEL1W+B=qdj8bWhDMRApJb5QVqxE1rfxt23wTKSYJbt27Fg@mail.gmail.com> <1403831040483-7088.post@n3.nabble.com> <D350552E-A930-43C0-AB12-FC7E1957EB78@staff.sina.com.cn>
Subject: Re: Contributing to MLlib on GLM
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_64931_11722262.1404001025381"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_64931_11722262.1404001025381
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Gang,

No worries!=20

I agree LBFGS would converge faster and your test suite is more comprehensi=
ve. I'd like to merge my branch with yours.

I also agree with your viewpoint on the redundancy issue. For different GLM=
s, usually they only differ in gradient calculation but the ****regression.=
scala files are quite similar. For example, linearRegressionSGD, logisticRe=
gressionSGD, RidgeRegressionSGD, poissonRegressionSGD all share quite a bit=
 of common code in their class implementations. Since such redundancy is al=
ready there in the legacy code, simply merging Poisson and Gamma does not s=
eem to help much. So I suggest we just leave them as separate classes for t=
he time being.=20


Best regards,

Xiaokai

On Jun 27, 2014, at 6:45 PM, Gang Bai [via Apache Spark Developers List] wr=
ote:

> Hi Xiaokai,=20
>=20
> My bad. I didn't notice this before I created another PR for Poisson regr=
ession. The mails were buried in junk by the corp mail master. Also, thanks=
 for considering my comments and advice in your PR.=20
>=20
> Adding my two cents here:=20
>=20
> * PoissonRegressionModel and GammaRegressionModel have the same fields an=
d prediction method. Shall we use one instead of two redundant classes? Say=
, a LogLinearModel.=20
> * The LBFGS optimizer takes fewer iterations and results in better conver=
gence than SGD. I implemented two GeneralizedLinearAlgorithm classes using =
LBFGS and SGD respectively. You may take a look into it. If it's OK to you,=
 I'd be happy to send a PR to your branch.=20
> * In addition to the generated test data, We may use some real-world data=
 for testing. In my implementation, I added the test data from https://onli=
necourses.science.psu.edu/stat504/node/223. Please check my test suite.=20
>=20
> -Gang=20
> Sent from my iPad=20
>=20
> > On 2014=E5=B9=B46=E6=9C=8827=E6=97=A5, at =E4=B8=8B=E5=8D=886:03, "xwei=
" <[hidden email]> wrote:=20
> >=20
> >=20
> > Yes, that's what we did: adding two gradient functions to Gradient.scal=
a and=20
> > create PoissonRegression and GammaRegression using these gradients. We =
made=20
> > a PR on this.=20
> >=20
> >=20
> >=20
> > --=20
> > View this message in context: http://apache-spark-developers-list.10015=
51.n3.nabble.com/Contributing-to-MLlib-on-GLM-tp7033p7088.html
> > Sent from the Apache Spark Developers List mailing list archive at Nabb=
le.com.=20
>=20
>=20
> If you reply to this email, your message will be added to the discussion =
below:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to=
-MLlib-on-GLM-tp7033p7107.html
> To unsubscribe from Contributing to MLlib on GLM, click here.
> NAML





--
View this message in context: http://apache-spark-developers-list.1001551.n=
3.nabble.com/Contributing-to-MLlib-on-GLM-tp7033p7117.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.c=
om.
------=_Part_64931_11722262.1404001025381--

From dev-return-8143-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jun 29 23:57:07 2014
Return-Path: <dev-return-8143-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 75D07116A0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 29 Jun 2014 23:57:07 +0000 (UTC)
Received: (qmail 98670 invoked by uid 500); 29 Jun 2014 23:57:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98620 invoked by uid 500); 29 Jun 2014 23:57:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98609 invoked by uid 99); 29 Jun 2014 23:57:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 29 Jun 2014 23:57:06 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.170] (HELO mail-qc0-f170.google.com) (209.85.216.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 29 Jun 2014 23:57:03 +0000
Received: by mail-qc0-f170.google.com with SMTP id l6so6498727qcy.1
        for <dev@spark.apache.org>; Sun, 29 Jun 2014 16:56:38 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=ERun3ODw5tEPiaPhoJryny8v3rDOkeqJH3gJ/zOCzPA=;
        b=WjVI3HSJqccWc2cf0lcPPbqRBgB4yRRlEpFJPUDBX4K/yIfvLMLJSP+qvC9DJwBl0U
         yMkMsSITOsFTspdZ8Nhq04AYfQks8os/VExpKWko1CN6FwlRT3NxtxARdU0lxsuxzFQT
         kCt8sDUxFH4I7kS0zLceE66VzEMmzIHZweghazQEXzoMUeTERw28NIFW7ZNUbNeHxMB5
         R0qWC3rW06lB5WQ6I94tAng/Ip+FdMZvb2rMVbk+va9GXdoR+FbHzuv/PRIC3IKFgTyU
         P56owtFOFKHFZ+VU3zwvPBCfFvw3uaOtkT2e7ZIHjKZks/pUR6cd6b2hZXsQ2JDBvqvY
         GHow==
X-Gm-Message-State: ALoCoQkGVn7K9ZifTCH1gnHjXRJcEw9pL0zeelRIpJZ8kwY9llCvYJxT9qEIHnr/zLxc1GiBwttx
X-Received: by 10.224.22.12 with SMTP id l12mr56683167qab.88.1404086198222;
 Sun, 29 Jun 2014 16:56:38 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Sun, 29 Jun 2014 16:56:18 -0700 (PDT)
In-Reply-To: <CAOTBr2npGLc852R8s7j8PtaQKr5jqzqtnDnJ8vDRRi+p06ycxw@mail.gmail.com>
References: <CABPQxsvAioKpE29GzUPts6tpk=6dSnXpfXMCYwKKX2fKX09Udw@mail.gmail.com>
 <CAOTBr2npGLc852R8s7j8PtaQKr5jqzqtnDnJ8vDRRi+p06ycxw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Sun, 29 Jun 2014 16:56:18 -0700
Message-ID: <CAPh_B=YznF7T3=n5dHbZmH1PBy4DjEqMc7JzawHbV7WeO0zUwg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC1)
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bf0e59c1444fe04fd024b81
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bf0e59c1444fe04fd024b81
Content-Type: text/plain; charset=UTF-8

We should make sure we include the following two patches:

https://github.com/apache/spark/pull/1264

https://github.com/apache/spark/pull/1263




On Fri, Jun 27, 2014 at 8:39 PM, Krishna Sankar <ksankar42@gmail.com> wrote:

> +1
> Compiled for CentOS 6.5, deployed in our 4 node cluster (Hadoop 2.2, YARN)
> Smoke Tests (sparkPi,spark-shell, web UI) successful
>
> Cheers
> <k/>
>
>
> On Thu, Jun 26, 2014 at 7:06 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> > Please vote on releasing the following candidate as Apache Spark version
> > 1.0.1!
> >
> > The tag to be voted on is v1.0.1-rc1 (commit 7feeda3):
> >
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7feeda3d729f9397aa15ee8750c01ef5aa601962
> >
> > The release files, including signatures, digests, etc. can be found at:
> > http://people.apache.org/~pwendell/spark-1.0.1-rc1/
> >
> > Release artifacts are signed with the following key:
> > https://people.apache.org/keys/committer/pwendell.asc
> >
> > The staging repository for this release can be found at:
> > https://repository.apache.org/content/repositories/orgapachespark-1020/
> >
> > The documentation corresponding to this release can be found at:
> > http://people.apache.org/~pwendell/spark-1.0.1-rc1-docs/
> >
> > Please vote on releasing this package as Apache Spark 1.0.1!
> >
> > The vote is open until Monday, June 30, at 03:00 UTC and passes if
> > a majority of at least 3 +1 PMC votes are cast.
> >
> > [ ] +1 Release this package as Apache Spark 1.0.1
> > [ ] -1 Do not release this package because ...
> >
> > To learn more about Apache Spark, please see
> > http://spark.apache.org/
> >
> > === About this release ===
> > This release fixes a few high-priority bugs in 1.0 and has a variety
> > of smaller fixes. The full list is here: http://s.apache.org/b45. Some
> > of the more visible patches are:
> >
> > SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
> > SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame
> size.
> > SPARK-1790: Support r3 instance types on EC2.
> >
> > This is the first maintenance release on the 1.0 line. We plan to make
> > additional maintenance releases as new fixes come in.
> >
> > - Patrick
> >
>

--047d7bf0e59c1444fe04fd024b81--

From dev-return-8144-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 30 02:57:32 2014
Return-Path: <dev-return-8144-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 00AFA118F9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 30 Jun 2014 02:57:32 +0000 (UTC)
Received: (qmail 60958 invoked by uid 500); 30 Jun 2014 02:57:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60900 invoked by uid 500); 30 Jun 2014 02:57:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60881 invoked by uid 99); 30 Jun 2014 02:57:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Jun 2014 02:57:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of chester@alpinenow.com designates 74.125.82.171 as permitted sender)
Received: from [74.125.82.171] (HELO mail-we0-f171.google.com) (74.125.82.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Jun 2014 02:57:25 +0000
Received: by mail-we0-f171.google.com with SMTP id q58so7519875wes.2
        for <dev@spark.apache.org>; Sun, 29 Jun 2014 19:57:04 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=xJ8WormtNAlDQ6cossBTLMZRGoob8qu7M3YcGROjxg8=;
        b=IXBtuh4kFdA7T8BIG9oik6cEAkCjikUQuq6yepm35iT9RepsYgZW4CMUYm0kzKzOH7
         BLuKycoaWZyer9mcYmPjiRqkvCoSh5ZiNDhwz0Sb95/V3BS0aMgpDmZOzdWvLN0LO8mw
         ELxAAwkNe4SunMA7J+a+kZa5SuULCpHUtoi88X3HPf+S8nJDeWNNKV9ZFk2d7wfI/VhD
         i6AIukvXS43z6y4NtAwar/7xU+vxNMvQpm65tkVNCmGOFZcCOXrEms/PeF3Y0MTxdhTu
         z5TjW6bFYGhcto94J+hecolDjRbLGJBWGX/0fnWZGz+uoqP59a0B/zWmP2UeBlY0D9uT
         BIyQ==
X-Gm-Message-State: ALoCoQmA0aRg0eCdUnIZJVDHRY+BresCNLMWquAmoy2gptCJppWMS99izT7NVqIFakE5RvPaQo2d
MIME-Version: 1.0
X-Received: by 10.194.20.230 with SMTP id q6mr41363270wje.43.1404097024489;
 Sun, 29 Jun 2014 19:57:04 -0700 (PDT)
Received: by 10.194.14.34 with HTTP; Sun, 29 Jun 2014 19:57:04 -0700 (PDT)
Date: Sun, 29 Jun 2014 19:57:04 -0700
Message-ID: <CAPYnQ0WQnjdOw-_M_9M7aJ5csqprB2n9RnG26_SOg_jWdOgnQw@mail.gmail.com>
Subject: Application level progress monitoring and communication
From: Chester Chen <chester@alpinenow.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b5d971b6011db04fd04d009
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b5d971b6011db04fd04d009
Content-Type: text/plain; charset=UTF-8

Hi Spark dev community:

I have several questions regarding Application and Spark communication

1) Application Level Progress Monitoring

Currently, our application using in YARN_CLUSTER model running Spark Jobs.
This works well so far, but we would like to monitoring the application
level progress ( not spark system level progress).

For example,
If we are doing Machine Learning Training, I would like to send some
message back the our application, current status of the training, number of
iterations etc via API.

We can't use YARN_CLIENT mode for this purpose as we are running the spark
application in servlet container (tomcat/Jetty). If we run the yarn_client
mode, we will be limited to one SparkContext per JVM.

So we are considering to leverage Akka messaging, essentially create
another Actor to send message back to the client application.
Notice that Spark already has an Akka ActorSystem defined for each
Executor. All we need to find Actor address (host, port) for the spark
driver executor.

The trouble is that driver's host and port are not known until later when
Resource Manager give to the executor node. How to communicate the host,
port info back to the client application ?

May be there is an Yarn API to obtain this information from Yarn Client.


2) Application and Spark Job communication In YARN Cluster mode.

    There are several use cases we are thinking may require communication
between the client side application and Spark Running Job.

     One example,
       * Try to stop a running job -- while job is running, abort the long
running job in Yarn

      Again, we are think to use Akka Actor to send a STOP job message.



So here some of  questions:

* Is there any work regarding this area in the community ?

* what do you think the Akka approach ? Alternatives ?

* Is there a way to get Spark's Akka host and port from Yarn Resource
Manager to Yarn Client ?

Any suggestions welcome

Thanks
Chester

--047d7b5d971b6011db04fd04d009--

From dev-return-8145-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 30 05:54:40 2014
Return-Path: <dev-return-8145-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2BB5011D80
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 30 Jun 2014 05:54:40 +0000 (UTC)
Received: (qmail 44728 invoked by uid 500); 30 Jun 2014 05:54:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44667 invoked by uid 500); 30 Jun 2014 05:54:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44655 invoked by uid 99); 30 Jun 2014 05:54:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Jun 2014 05:54:39 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.43 as permitted sender)
Received: from [209.85.219.43] (HELO mail-oa0-f43.google.com) (209.85.219.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Jun 2014 05:54:35 +0000
Received: by mail-oa0-f43.google.com with SMTP id o6so8318888oag.30
        for <dev@spark.apache.org>; Sun, 29 Jun 2014 22:54:14 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=2XDl8/i4dFeuOe385Is4Uy2/qVvVgcBIwRVJta74h9s=;
        b=TC1lIfCjcPzTmpFD/Tt7fxaK6Lo5La6FRq/LLsUXIQa+zEmPbaywE1f4ftOSGKaEYz
         Ayr7Qj72taw7KU8sl/WeNPwCOY3+hKbN3PGWGflGT5n0sqlF4Fgo0RnmNn7sy7qfEB0e
         bnf1nxBH3QTL6lSiGr4pZEeFn+YxW4sCZWLxc/r5nibvZ9aRjMNpdCpZcd4aykswgtc8
         Rg17I+oi+1tz7BNAtO06P6ahYUDjJk5srTUtlixdh6umflHo0ILR+yp2z1Oi42CmGQM8
         LPnCzsEW7zJmztnzvW5iVSNM7y28z6Tnj4HhtzwXhDlV1L39CFQjCOXrbqwYaHj8oZcb
         MZRg==
MIME-Version: 1.0
X-Received: by 10.60.179.80 with SMTP id de16mr5901399oec.69.1404107654582;
 Sun, 29 Jun 2014 22:54:14 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Sun, 29 Jun 2014 22:54:14 -0700 (PDT)
In-Reply-To: <CAPh_B=YznF7T3=n5dHbZmH1PBy4DjEqMc7JzawHbV7WeO0zUwg@mail.gmail.com>
References: <CABPQxsvAioKpE29GzUPts6tpk=6dSnXpfXMCYwKKX2fKX09Udw@mail.gmail.com>
	<CAOTBr2npGLc852R8s7j8PtaQKr5jqzqtnDnJ8vDRRi+p06ycxw@mail.gmail.com>
	<CAPh_B=YznF7T3=n5dHbZmH1PBy4DjEqMc7JzawHbV7WeO0zUwg@mail.gmail.com>
Date: Sun, 29 Jun 2014 22:54:14 -0700
Message-ID: <CABPQxsv8JMc0_rO77WB9Q_h83RRktWSYMuf2co38WEQ=qFeJ=w@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey All,

We're going to move onto another rc because of this vote.
Unfortunately with the summit activities I haven't been able to usher
in the necessary patches and cut the RC. I will do so as soon as
possible and we can commence official voting.

- Patrick

On Sun, Jun 29, 2014 at 4:56 PM, Reynold Xin <rxin@databricks.com> wrote:
> We should make sure we include the following two patches:
>
> https://github.com/apache/spark/pull/1264
>
> https://github.com/apache/spark/pull/1263
>
>
>
>
> On Fri, Jun 27, 2014 at 8:39 PM, Krishna Sankar <ksankar42@gmail.com> wrote:
>
>> +1
>> Compiled for CentOS 6.5, deployed in our 4 node cluster (Hadoop 2.2, YARN)
>> Smoke Tests (sparkPi,spark-shell, web UI) successful
>>
>> Cheers
>> <k/>
>>
>>
>> On Thu, Jun 26, 2014 at 7:06 PM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>> > Please vote on releasing the following candidate as Apache Spark version
>> > 1.0.1!
>> >
>> > The tag to be voted on is v1.0.1-rc1 (commit 7feeda3):
>> >
>> >
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7feeda3d729f9397aa15ee8750c01ef5aa601962
>> >
>> > The release files, including signatures, digests, etc. can be found at:
>> > http://people.apache.org/~pwendell/spark-1.0.1-rc1/
>> >
>> > Release artifacts are signed with the following key:
>> > https://people.apache.org/keys/committer/pwendell.asc
>> >
>> > The staging repository for this release can be found at:
>> > https://repository.apache.org/content/repositories/orgapachespark-1020/
>> >
>> > The documentation corresponding to this release can be found at:
>> > http://people.apache.org/~pwendell/spark-1.0.1-rc1-docs/
>> >
>> > Please vote on releasing this package as Apache Spark 1.0.1!
>> >
>> > The vote is open until Monday, June 30, at 03:00 UTC and passes if
>> > a majority of at least 3 +1 PMC votes are cast.
>> >
>> > [ ] +1 Release this package as Apache Spark 1.0.1
>> > [ ] -1 Do not release this package because ...
>> >
>> > To learn more about Apache Spark, please see
>> > http://spark.apache.org/
>> >
>> > === About this release ===
>> > This release fixes a few high-priority bugs in 1.0 and has a variety
>> > of smaller fixes. The full list is here: http://s.apache.org/b45. Some
>> > of the more visible patches are:
>> >
>> > SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
>> > SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame
>> size.
>> > SPARK-1790: Support r3 instance types on EC2.
>> >
>> > This is the first maintenance release on the 1.0 line. We plan to make
>> > additional maintenance releases as new fixes come in.
>> >
>> > - Patrick
>> >
>>

From dev-return-8146-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 30 06:07:51 2014
Return-Path: <dev-return-8146-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CE0D711DC3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 30 Jun 2014 06:07:51 +0000 (UTC)
Received: (qmail 63687 invoked by uid 500); 30 Jun 2014 06:07:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63633 invoked by uid 500); 30 Jun 2014 06:07:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63622 invoked by uid 99); 30 Jun 2014 06:07:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Jun 2014 06:07:50 +0000
X-ASF-Spam-Status: No, hits=2.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Jun 2014 06:07:48 +0000
Received: by mail-qg0-f50.google.com with SMTP id j5so1593072qga.9
        for <dev@spark.apache.org>; Sun, 29 Jun 2014 23:07:23 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=QFDfkybcS/F3jV8pi/0Zjpo7ngvBb2u6aWX/0mzU5FY=;
        b=es/Y4FkYy4c5LDf/0zGiwQ2wJDwYfwjqe2yQvfL4Nbu7PKGxiDsgKGxqioPcgdq1K/
         VniEdTWqdM7Bv+/E0qc2jo7VKoUk8j+rgbrmO8UpXcbnkjv6odhvZHr+6Zs3C4Z/fh1M
         euoJfOShwZGrSafGVVP5eI3uolFBGsl0PsI2S3fb2JJ/LSYC4rIIXSKmtuMYGysBHNdT
         Dtw1i6AeM02TTFu54Egkh49Aa5AuarNkecd411KTZ8op+b3y8svy2ORz6ZstfC+7LO7N
         fh484/UhnjFATAQ+JvCdaPhNC8M0r0e1oS5g+Pa6zYumZtZMwq4kn2Jbp0Keqt7UPgOa
         FEPA==
X-Gm-Message-State: ALoCoQnniFY1nCMWTpGQTaS+F84OPOTKtAM5v2vb6LkU/pBc9R7xscejfPiDLsFxWtUDCE5pkCBW
X-Received: by 10.224.104.10 with SMTP id m10mr57621811qao.27.1404108443310;
 Sun, 29 Jun 2014 23:07:23 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Sun, 29 Jun 2014 23:07:03 -0700 (PDT)
In-Reply-To: <CAPYnQ0WQnjdOw-_M_9M7aJ5csqprB2n9RnG26_SOg_jWdOgnQw@mail.gmail.com>
References: <CAPYnQ0WQnjdOw-_M_9M7aJ5csqprB2n9RnG26_SOg_jWdOgnQw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Sun, 29 Jun 2014 23:07:03 -0700
Message-ID: <CAPh_B=bbjzRPz8E+Z5G+Ywj-X5awRdKyUfyg79D4tYzAAtFKxg@mail.gmail.com>
Subject: Re: Application level progress monitoring and communication
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1ca16fd749e04fd0778f9
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1ca16fd749e04fd0778f9
Content-Type: text/plain; charset=UTF-8

This isn't exactly about Spark itself, more about how an application on
YARN/Mesos can communicate with another one.

How about your application launch program just takes in a parameter (or env
variable or command line argument) for the IP address of your client
application, and just send updates? You basically just want to send
messages to report progress. You can do it with a lot of different ways,
such as Akka, custom REST API, Thrift ... I think any of them will do.




On Sun, Jun 29, 2014 at 7:57 PM, Chester Chen <chester@alpinenow.com> wrote:

> Hi Spark dev community:
>
> I have several questions regarding Application and Spark communication
>
> 1) Application Level Progress Monitoring
>
> Currently, our application using in YARN_CLUSTER model running Spark Jobs.
> This works well so far, but we would like to monitoring the application
> level progress ( not spark system level progress).
>
> For example,
> If we are doing Machine Learning Training, I would like to send some
> message back the our application, current status of the training, number of
> iterations etc via API.
>
> We can't use YARN_CLIENT mode for this purpose as we are running the spark
> application in servlet container (tomcat/Jetty). If we run the yarn_client
> mode, we will be limited to one SparkContext per JVM.
>
> So we are considering to leverage Akka messaging, essentially create
> another Actor to send message back to the client application.
> Notice that Spark already has an Akka ActorSystem defined for each
> Executor. All we need to find Actor address (host, port) for the spark
> driver executor.
>
> The trouble is that driver's host and port are not known until later when
> Resource Manager give to the executor node. How to communicate the host,
> port info back to the client application ?
>
> May be there is an Yarn API to obtain this information from Yarn Client.
>
>
> 2) Application and Spark Job communication In YARN Cluster mode.
>
>     There are several use cases we are thinking may require communication
> between the client side application and Spark Running Job.
>
>      One example,
>        * Try to stop a running job -- while job is running, abort the long
> running job in Yarn
>
>       Again, we are think to use Akka Actor to send a STOP job message.
>
>
>
> So here some of  questions:
>
> * Is there any work regarding this area in the community ?
>
> * what do you think the Akka approach ? Alternatives ?
>
> * Is there a way to get Spark's Akka host and port from Yarn Resource
> Manager to Yarn Client ?
>
> Any suggestions welcome
>
> Thanks
> Chester
>

--001a11c1ca16fd749e04fd0778f9--

From dev-return-8147-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 30 06:10:02 2014
Return-Path: <dev-return-8147-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 59F3911DEE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 30 Jun 2014 06:10:02 +0000 (UTC)
Received: (qmail 68867 invoked by uid 500); 30 Jun 2014 06:10:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68799 invoked by uid 500); 30 Jun 2014 06:10:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68788 invoked by uid 99); 30 Jun 2014 06:10:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Jun 2014 06:10:01 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.220.182] (HELO mail-vc0-f182.google.com) (209.85.220.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Jun 2014 06:09:56 +0000
Received: by mail-vc0-f182.google.com with SMTP id il7so7098238vcb.13
        for <dev@spark.apache.org>; Sun, 29 Jun 2014 23:09:35 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=MTWu/Y0EZZHP0kGftLVnqdkxJnIKL1laOyuBoQK6spA=;
        b=NRQj40ti2iIaPIWTRPIPhH9hbWd5oyLJnCx66RK8/jFKuxcb/z3mJdDTkAIMphagdT
         nd7RBwPNCyXdUS3Dyd9qhLvaeEQpv/Yz28RMACSX6Esid7nrFcXd6BEmYqnswQCIxh5b
         tCaHUz4oYYdMJ9nq6T3HgE9g11FouZqM/C3UCoY1ns/G9usig1uCjYtsH34qGyp5s2dJ
         81fH+JauuW4Ib2s/W6KzRjrnE50dg0P9UuReerCXp1bsmqfv0G+uUDu1XL3OE0n2lqIe
         hBSt5owgsVctjVo9jph9g1V6aWa0Vu+m1g8k8DnXhd66o+bVNYACDtSIRDPbDQkQ5gZD
         5GRw==
X-Gm-Message-State: ALoCoQnT5Ndz21Zqk9R1V4HWbUsdMQAjMbhmldqBY7ZtfgFnbMjVRau0dNRc9bXTN1XQDOkNQUtt
X-Received: by 10.53.8.162 with SMTP id dl2mr30536987vdd.24.1404108575509;
        Sun, 29 Jun 2014 23:09:35 -0700 (PDT)
Received: from mail-vc0-f181.google.com (mail-vc0-f181.google.com [209.85.220.181])
        by mx.google.com with ESMTPSA id pr4sm35311477vdb.15.2014.06.29.23.09.34
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 29 Jun 2014 23:09:34 -0700 (PDT)
Received: by mail-vc0-f181.google.com with SMTP id il7so7018564vcb.40
        for <dev@spark.apache.org>; Sun, 29 Jun 2014 23:09:33 -0700 (PDT)
X-Received: by 10.220.185.202 with SMTP id cp10mr36251295vcb.15.1404108573843;
 Sun, 29 Jun 2014 23:09:33 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.222.17 with HTTP; Sun, 29 Jun 2014 23:09:12 -0700 (PDT)
In-Reply-To: <CABPQxsv8JMc0_rO77WB9Q_h83RRktWSYMuf2co38WEQ=qFeJ=w@mail.gmail.com>
References: <CABPQxsvAioKpE29GzUPts6tpk=6dSnXpfXMCYwKKX2fKX09Udw@mail.gmail.com>
 <CAOTBr2npGLc852R8s7j8PtaQKr5jqzqtnDnJ8vDRRi+p06ycxw@mail.gmail.com>
 <CAPh_B=YznF7T3=n5dHbZmH1PBy4DjEqMc7JzawHbV7WeO0zUwg@mail.gmail.com> <CABPQxsv8JMc0_rO77WB9Q_h83RRktWSYMuf2co38WEQ=qFeJ=w@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Sun, 29 Jun 2014 23:09:12 -0700
Message-ID: <CA+-p3AHFE2_TkvO2wN5J_0EJE5eF58S84r=cM_Xh13nRE1YFyA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC1)
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c1bd8cc5264504fd0780e5
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1bd8cc5264504fd0780e5
Content-Type: text/plain; charset=UTF-8

Thanks for helping shepherd the voting on 1.0.1 Patrick.

I'd like to call attention to
https://issues.apache.org/jira/browse/SPARK-2157 and
https://github.com/apache/spark/pull/1107 -- "Ability to write tight
firewall rules for Spark"

I'm currently unable to run Spark on some projects because our cloud ops
team is uncomfortable with the firewall situation around Spark at the
moment.  Currently Spark starts listening on random ephemeral ports and
does server to server communication on them.  This keeps the team from
writing tight firewall rules between the services -- they get real queasy
when asked to open inbound connections to the entire ephemeral port range
of a cluster.  We can tighten the size of the ephemeral range using kernel
settings to mitigate the issue, but it doesn't actually solve the problem.

The PR above aims to make every listening port on JVMs in a Spark
standalone cluster configurable with an option.  If not set, the current
behavior stands (start listening on an ephemeral port).  Is this something
the Spark team would consider merging into 1.0.1?

Thanks!
Andrew



On Sun, Jun 29, 2014 at 10:54 PM, Patrick Wendell <pwendell@gmail.com>
wrote:

> Hey All,
>
> We're going to move onto another rc because of this vote.
> Unfortunately with the summit activities I haven't been able to usher
> in the necessary patches and cut the RC. I will do so as soon as
> possible and we can commence official voting.
>
> - Patrick
>
> On Sun, Jun 29, 2014 at 4:56 PM, Reynold Xin <rxin@databricks.com> wrote:
> > We should make sure we include the following two patches:
> >
> > https://github.com/apache/spark/pull/1264
> >
> > https://github.com/apache/spark/pull/1263
> >
> >
> >
> >
> > On Fri, Jun 27, 2014 at 8:39 PM, Krishna Sankar <ksankar42@gmail.com>
> wrote:
> >
> >> +1
> >> Compiled for CentOS 6.5, deployed in our 4 node cluster (Hadoop 2.2,
> YARN)
> >> Smoke Tests (sparkPi,spark-shell, web UI) successful
> >>
> >> Cheers
> >> <k/>
> >>
> >>
> >> On Thu, Jun 26, 2014 at 7:06 PM, Patrick Wendell <pwendell@gmail.com>
> >> wrote:
> >>
> >> > Please vote on releasing the following candidate as Apache Spark
> version
> >> > 1.0.1!
> >> >
> >> > The tag to be voted on is v1.0.1-rc1 (commit 7feeda3):
> >> >
> >> >
> >>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7feeda3d729f9397aa15ee8750c01ef5aa601962
> >> >
> >> > The release files, including signatures, digests, etc. can be found
> at:
> >> > http://people.apache.org/~pwendell/spark-1.0.1-rc1/
> >> >
> >> > Release artifacts are signed with the following key:
> >> > https://people.apache.org/keys/committer/pwendell.asc
> >> >
> >> > The staging repository for this release can be found at:
> >> >
> https://repository.apache.org/content/repositories/orgapachespark-1020/
> >> >
> >> > The documentation corresponding to this release can be found at:
> >> > http://people.apache.org/~pwendell/spark-1.0.1-rc1-docs/
> >> >
> >> > Please vote on releasing this package as Apache Spark 1.0.1!
> >> >
> >> > The vote is open until Monday, June 30, at 03:00 UTC and passes if
> >> > a majority of at least 3 +1 PMC votes are cast.
> >> >
> >> > [ ] +1 Release this package as Apache Spark 1.0.1
> >> > [ ] -1 Do not release this package because ...
> >> >
> >> > To learn more about Apache Spark, please see
> >> > http://spark.apache.org/
> >> >
> >> > === About this release ===
> >> > This release fixes a few high-priority bugs in 1.0 and has a variety
> >> > of smaller fixes. The full list is here: http://s.apache.org/b45.
> Some
> >> > of the more visible patches are:
> >> >
> >> > SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
> >> > SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame
> >> size.
> >> > SPARK-1790: Support r3 instance types on EC2.
> >> >
> >> > This is the first maintenance release on the 1.0 line. We plan to make
> >> > additional maintenance releases as new fixes come in.
> >> >
> >> > - Patrick
> >> >
> >>
>

--001a11c1bd8cc5264504fd0780e5--

From dev-return-8148-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 30 06:15:28 2014
Return-Path: <dev-return-8148-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8FCAE11E0F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 30 Jun 2014 06:15:28 +0000 (UTC)
Received: (qmail 75983 invoked by uid 500); 30 Jun 2014 06:15:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75941 invoked by uid 500); 30 Jun 2014 06:15:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75927 invoked by uid 99); 30 Jun 2014 06:15:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Jun 2014 06:15:27 +0000
X-ASF-Spam-Status: No, hits=2.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Jun 2014 06:15:23 +0000
Received: by mail-qg0-f50.google.com with SMTP id j5so1539365qga.23
        for <dev@spark.apache.org>; Sun, 29 Jun 2014 23:15:02 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=rkn5kZSrWGkhj0jhPn078RXZf8Yzbsa4z9si/XEc9kg=;
        b=Z87eivxshnEKEyJhdFAJ0iY0e82tahgInnm2GTJam/o1E1u5z6G2oeyFoK2gEsOxtO
         lb2xWbewQ1brPC71dVzxqi2JwwK7xKhzOj5oPBfcjA/BP1a4CLBMYxGpJd8e9Y5FBUDY
         ckCifMqFFmXXnB8zTha/a3PajTw7jPXJwRvHTcJMn/owLwobqC0IBKA8EiyJGGoalm8z
         N7xL2YZz9sVCq+sOx4/KnLaTh6PSLziotQrvu22SwqVcmg+y10AL/r61KUaLEh+9pP9I
         o1zPs2KcIbczuJAeB591panuyhquGOwPwxjQLzqvRXyelv9M5Q5Rg8Mqiy0tmKIrLn5Q
         sefw==
X-Gm-Message-State: ALoCoQmjWUjHxkDiyNpvJ1jDck1ntKPM73RQXxe6nJtKDLWy6dboH6toFsAitOg/vA1nlsw6jI5a
X-Received: by 10.140.26.201 with SMTP id 67mr55041919qgv.51.1404108902739;
 Sun, 29 Jun 2014 23:15:02 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Sun, 29 Jun 2014 23:14:42 -0700 (PDT)
In-Reply-To: <CA+-p3AHFE2_TkvO2wN5J_0EJE5eF58S84r=cM_Xh13nRE1YFyA@mail.gmail.com>
References: <CABPQxsvAioKpE29GzUPts6tpk=6dSnXpfXMCYwKKX2fKX09Udw@mail.gmail.com>
 <CAOTBr2npGLc852R8s7j8PtaQKr5jqzqtnDnJ8vDRRi+p06ycxw@mail.gmail.com>
 <CAPh_B=YznF7T3=n5dHbZmH1PBy4DjEqMc7JzawHbV7WeO0zUwg@mail.gmail.com>
 <CABPQxsv8JMc0_rO77WB9Q_h83RRktWSYMuf2co38WEQ=qFeJ=w@mail.gmail.com> <CA+-p3AHFE2_TkvO2wN5J_0EJE5eF58S84r=cM_Xh13nRE1YFyA@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Sun, 29 Jun 2014 23:14:42 -0700
Message-ID: <CAPh_B=Yfxq024Wfeop+ka1PN9m-XtAqej+gTGNBkfTjoK0ghEA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC1)
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c152fa5fcf9904fd0794e7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c152fa5fcf9904fd0794e7
Content-Type: text/plain; charset=UTF-8

Hi Andrew,

The port stuff is great to have, but they are pretty big changes to the
core that are introducing new features and are not exactly fixing important
bugs. For this reason, it probably can't block a release (I'm not even sure
if it should go into a maintenance release where we fix critical bugs for
Spark core).

We should definitely include them for 1.1.0 though (~Aug).




On Sun, Jun 29, 2014 at 11:09 PM, Andrew Ash <andrew@andrewash.com> wrote:

> Thanks for helping shepherd the voting on 1.0.1 Patrick.
>
> I'd like to call attention to
> https://issues.apache.org/jira/browse/SPARK-2157 and
> https://github.com/apache/spark/pull/1107 -- "Ability to write tight
> firewall rules for Spark"
>
> I'm currently unable to run Spark on some projects because our cloud ops
> team is uncomfortable with the firewall situation around Spark at the
> moment.  Currently Spark starts listening on random ephemeral ports and
> does server to server communication on them.  This keeps the team from
> writing tight firewall rules between the services -- they get real queasy
> when asked to open inbound connections to the entire ephemeral port range
> of a cluster.  We can tighten the size of the ephemeral range using kernel
> settings to mitigate the issue, but it doesn't actually solve the problem.
>
> The PR above aims to make every listening port on JVMs in a Spark
> standalone cluster configurable with an option.  If not set, the current
> behavior stands (start listening on an ephemeral port).  Is this something
> the Spark team would consider merging into 1.0.1?
>
> Thanks!
> Andrew
>
>
>
> On Sun, Jun 29, 2014 at 10:54 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> > Hey All,
> >
> > We're going to move onto another rc because of this vote.
> > Unfortunately with the summit activities I haven't been able to usher
> > in the necessary patches and cut the RC. I will do so as soon as
> > possible and we can commence official voting.
> >
> > - Patrick
> >
> > On Sun, Jun 29, 2014 at 4:56 PM, Reynold Xin <rxin@databricks.com>
> wrote:
> > > We should make sure we include the following two patches:
> > >
> > > https://github.com/apache/spark/pull/1264
> > >
> > > https://github.com/apache/spark/pull/1263
> > >
> > >
> > >
> > >
> > > On Fri, Jun 27, 2014 at 8:39 PM, Krishna Sankar <ksankar42@gmail.com>
> > wrote:
> > >
> > >> +1
> > >> Compiled for CentOS 6.5, deployed in our 4 node cluster (Hadoop 2.2,
> > YARN)
> > >> Smoke Tests (sparkPi,spark-shell, web UI) successful
> > >>
> > >> Cheers
> > >> <k/>
> > >>
> > >>
> > >> On Thu, Jun 26, 2014 at 7:06 PM, Patrick Wendell <pwendell@gmail.com>
> > >> wrote:
> > >>
> > >> > Please vote on releasing the following candidate as Apache Spark
> > version
> > >> > 1.0.1!
> > >> >
> > >> > The tag to be voted on is v1.0.1-rc1 (commit 7feeda3):
> > >> >
> > >> >
> > >>
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7feeda3d729f9397aa15ee8750c01ef5aa601962
> > >> >
> > >> > The release files, including signatures, digests, etc. can be found
> > at:
> > >> > http://people.apache.org/~pwendell/spark-1.0.1-rc1/
> > >> >
> > >> > Release artifacts are signed with the following key:
> > >> > https://people.apache.org/keys/committer/pwendell.asc
> > >> >
> > >> > The staging repository for this release can be found at:
> > >> >
> > https://repository.apache.org/content/repositories/orgapachespark-1020/
> > >> >
> > >> > The documentation corresponding to this release can be found at:
> > >> > http://people.apache.org/~pwendell/spark-1.0.1-rc1-docs/
> > >> >
> > >> > Please vote on releasing this package as Apache Spark 1.0.1!
> > >> >
> > >> > The vote is open until Monday, June 30, at 03:00 UTC and passes if
> > >> > a majority of at least 3 +1 PMC votes are cast.
> > >> >
> > >> > [ ] +1 Release this package as Apache Spark 1.0.1
> > >> > [ ] -1 Do not release this package because ...
> > >> >
> > >> > To learn more about Apache Spark, please see
> > >> > http://spark.apache.org/
> > >> >
> > >> > === About this release ===
> > >> > This release fixes a few high-priority bugs in 1.0 and has a variety
> > >> > of smaller fixes. The full list is here: http://s.apache.org/b45.
> > Some
> > >> > of the more visible patches are:
> > >> >
> > >> > SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
> > >> > SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka
> frame
> > >> size.
> > >> > SPARK-1790: Support r3 instance types on EC2.
> > >> >
> > >> > This is the first maintenance release on the 1.0 line. We plan to
> make
> > >> > additional maintenance releases as new fixes come in.
> > >> >
> > >> > - Patrick
> > >> >
> > >>
> >
>

--001a11c152fa5fcf9904fd0794e7--

From dev-return-8149-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 30 06:24:09 2014
Return-Path: <dev-return-8149-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4515D11E34
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 30 Jun 2014 06:24:09 +0000 (UTC)
Received: (qmail 83098 invoked by uid 500); 30 Jun 2014 06:24:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83035 invoked by uid 500); 30 Jun 2014 06:24:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83023 invoked by uid 99); 30 Jun 2014 06:24:08 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Jun 2014 06:24:08 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.220.181] (HELO mail-vc0-f181.google.com) (209.85.220.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Jun 2014 06:24:05 +0000
Received: by mail-vc0-f181.google.com with SMTP id il7so7126107vcb.12
        for <dev@spark.apache.org>; Sun, 29 Jun 2014 23:23:40 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=D3A3CyUofgClvgoj8pn+2M6xCgrLNnGS3C5scDyXiN4=;
        b=TrMdPY83bQOfVCnBF00HUp7oW0fj+obddUkriDZ+x1I0w8ck0xnovCAqj3BQjHTLRn
         6DuvbvhYV7KkSAFaT0iSYxuBDx76YnpLYK8VuMwyPU/Pb2BugdLS90bMSSVI3MzlDMpT
         eGr6y5st6/Tu/8znHuVBdL/SVVvucU16XwXNht0py+OaSDMWRpm4cx9LTsJdttliqp1/
         Mb7dFfdT6VCjd6gOyvKmOnGIVQF3oU119uVhL1Hwx2iOtgsnKHBP6EKoACyp34mjJXG/
         fDr3z8Liv/52OLBG7VRwwiH23LgP/b35WtEa9YiF02xf5IKjjo8C1JLGzEy8ul3zXnN7
         KgBA==
X-Gm-Message-State: ALoCoQkz45ZzgrtI7GIH4SAig1BFT1/8CoAZteR2gfxhPFMoNtB9LKmnyjJGbgsF09x8fRPNF5+Y
X-Received: by 10.52.96.8 with SMTP id do8mr30314801vdb.4.1404109419821;
        Sun, 29 Jun 2014 23:23:39 -0700 (PDT)
Received: from mail-vc0-f176.google.com (mail-vc0-f176.google.com [209.85.220.176])
        by mx.google.com with ESMTPSA id r8sm19093234vet.7.2014.06.29.23.23.38
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 29 Jun 2014 23:23:38 -0700 (PDT)
Received: by mail-vc0-f176.google.com with SMTP id ik5so6979634vcb.7
        for <dev@spark.apache.org>; Sun, 29 Jun 2014 23:23:38 -0700 (PDT)
X-Received: by 10.220.53.72 with SMTP id l8mr36605970vcg.16.1404109418236;
 Sun, 29 Jun 2014 23:23:38 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.222.17 with HTTP; Sun, 29 Jun 2014 23:23:18 -0700 (PDT)
In-Reply-To: <CAPh_B=Yfxq024Wfeop+ka1PN9m-XtAqej+gTGNBkfTjoK0ghEA@mail.gmail.com>
References: <CABPQxsvAioKpE29GzUPts6tpk=6dSnXpfXMCYwKKX2fKX09Udw@mail.gmail.com>
 <CAOTBr2npGLc852R8s7j8PtaQKr5jqzqtnDnJ8vDRRi+p06ycxw@mail.gmail.com>
 <CAPh_B=YznF7T3=n5dHbZmH1PBy4DjEqMc7JzawHbV7WeO0zUwg@mail.gmail.com>
 <CABPQxsv8JMc0_rO77WB9Q_h83RRktWSYMuf2co38WEQ=qFeJ=w@mail.gmail.com>
 <CA+-p3AHFE2_TkvO2wN5J_0EJE5eF58S84r=cM_Xh13nRE1YFyA@mail.gmail.com> <CAPh_B=Yfxq024Wfeop+ka1PN9m-XtAqej+gTGNBkfTjoK0ghEA@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Sun, 29 Jun 2014 23:23:18 -0700
Message-ID: <CA+-p3AHakafzEs5HAERT5w=DAcOUCr0mtv5TV7MR1LKnMy6Buw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC1)
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2cb121993ae04fd07b379
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2cb121993ae04fd07b379
Content-Type: text/plain; charset=UTF-8

Ok that's reasonable -- it's certainly more of an enhancement than a
critical bug-fix.  I would like to get this in for 1.1.0 though, so let's
talk through the right way to do that on the PR.

In the meantime the best alternative is running with lax firewall settings,
which can be somewhat mitigated by modifying the ephemeral port range.

Thanks!
Andrew


On Sun, Jun 29, 2014 at 11:14 PM, Reynold Xin <rxin@databricks.com> wrote:

> Hi Andrew,
>
> The port stuff is great to have, but they are pretty big changes to the
> core that are introducing new features and are not exactly fixing important
> bugs. For this reason, it probably can't block a release (I'm not even sure
> if it should go into a maintenance release where we fix critical bugs for
> Spark core).
>
> We should definitely include them for 1.1.0 though (~Aug).
>
>
>
>
> On Sun, Jun 29, 2014 at 11:09 PM, Andrew Ash <andrew@andrewash.com> wrote:
>
> > Thanks for helping shepherd the voting on 1.0.1 Patrick.
> >
> > I'd like to call attention to
> > https://issues.apache.org/jira/browse/SPARK-2157 and
> > https://github.com/apache/spark/pull/1107 -- "Ability to write tight
> > firewall rules for Spark"
> >
> > I'm currently unable to run Spark on some projects because our cloud ops
> > team is uncomfortable with the firewall situation around Spark at the
> > moment.  Currently Spark starts listening on random ephemeral ports and
> > does server to server communication on them.  This keeps the team from
> > writing tight firewall rules between the services -- they get real queasy
> > when asked to open inbound connections to the entire ephemeral port range
> > of a cluster.  We can tighten the size of the ephemeral range using
> kernel
> > settings to mitigate the issue, but it doesn't actually solve the
> problem.
> >
> > The PR above aims to make every listening port on JVMs in a Spark
> > standalone cluster configurable with an option.  If not set, the current
> > behavior stands (start listening on an ephemeral port).  Is this
> something
> > the Spark team would consider merging into 1.0.1?
> >
> > Thanks!
> > Andrew
> >
> >
> >
> > On Sun, Jun 29, 2014 at 10:54 PM, Patrick Wendell <pwendell@gmail.com>
> > wrote:
> >
> > > Hey All,
> > >
> > > We're going to move onto another rc because of this vote.
> > > Unfortunately with the summit activities I haven't been able to usher
> > > in the necessary patches and cut the RC. I will do so as soon as
> > > possible and we can commence official voting.
> > >
> > > - Patrick
> > >
> > > On Sun, Jun 29, 2014 at 4:56 PM, Reynold Xin <rxin@databricks.com>
> > wrote:
> > > > We should make sure we include the following two patches:
> > > >
> > > > https://github.com/apache/spark/pull/1264
> > > >
> > > > https://github.com/apache/spark/pull/1263
> > > >
> > > >
> > > >
> > > >
> > > > On Fri, Jun 27, 2014 at 8:39 PM, Krishna Sankar <ksankar42@gmail.com
> >
> > > wrote:
> > > >
> > > >> +1
> > > >> Compiled for CentOS 6.5, deployed in our 4 node cluster (Hadoop 2.2,
> > > YARN)
> > > >> Smoke Tests (sparkPi,spark-shell, web UI) successful
> > > >>
> > > >> Cheers
> > > >> <k/>
> > > >>
> > > >>
> > > >> On Thu, Jun 26, 2014 at 7:06 PM, Patrick Wendell <
> pwendell@gmail.com>
> > > >> wrote:
> > > >>
> > > >> > Please vote on releasing the following candidate as Apache Spark
> > > version
> > > >> > 1.0.1!
> > > >> >
> > > >> > The tag to be voted on is v1.0.1-rc1 (commit 7feeda3):
> > > >> >
> > > >> >
> > > >>
> > >
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7feeda3d729f9397aa15ee8750c01ef5aa601962
> > > >> >
> > > >> > The release files, including signatures, digests, etc. can be
> found
> > > at:
> > > >> > http://people.apache.org/~pwendell/spark-1.0.1-rc1/
> > > >> >
> > > >> > Release artifacts are signed with the following key:
> > > >> > https://people.apache.org/keys/committer/pwendell.asc
> > > >> >
> > > >> > The staging repository for this release can be found at:
> > > >> >
> > >
> https://repository.apache.org/content/repositories/orgapachespark-1020/
> > > >> >
> > > >> > The documentation corresponding to this release can be found at:
> > > >> > http://people.apache.org/~pwendell/spark-1.0.1-rc1-docs/
> > > >> >
> > > >> > Please vote on releasing this package as Apache Spark 1.0.1!
> > > >> >
> > > >> > The vote is open until Monday, June 30, at 03:00 UTC and passes if
> > > >> > a majority of at least 3 +1 PMC votes are cast.
> > > >> >
> > > >> > [ ] +1 Release this package as Apache Spark 1.0.1
> > > >> > [ ] -1 Do not release this package because ...
> > > >> >
> > > >> > To learn more about Apache Spark, please see
> > > >> > http://spark.apache.org/
> > > >> >
> > > >> > === About this release ===
> > > >> > This release fixes a few high-priority bugs in 1.0 and has a
> variety
> > > >> > of smaller fixes. The full list is here: http://s.apache.org/b45.
> > > Some
> > > >> > of the more visible patches are:
> > > >> >
> > > >> > SPARK-2043: ExternalAppendOnlyMap doesn't always find matching
> keys
> > > >> > SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka
> > frame
> > > >> size.
> > > >> > SPARK-1790: Support r3 instance types on EC2.
> > > >> >
> > > >> > This is the first maintenance release on the 1.0 line. We plan to
> > make
> > > >> > additional maintenance releases as new fixes come in.
> > > >> >
> > > >> > - Patrick
> > > >> >
> > > >>
> > >
> >
>

--001a11c2cb121993ae04fd07b379--

From dev-return-8150-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 30 15:55:24 2014
Return-Path: <dev-return-8150-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6847311353
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 30 Jun 2014 15:55:24 +0000 (UTC)
Received: (qmail 1217 invoked by uid 500); 30 Jun 2014 15:55:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1177 invoked by uid 500); 30 Jun 2014 15:55:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1165 invoked by uid 99); 30 Jun 2014 15:55:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Jun 2014 15:55:23 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of salexln@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Jun 2014 15:55:19 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <salexln@gmail.com>)
	id 1X1dv0-0002jg-MD
	for dev@spark.incubator.apache.org; Mon, 30 Jun 2014 08:54:58 -0700
Date: Mon, 30 Jun 2014 08:54:58 -0700 (PDT)
From: salexln <salexln@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1404143698646-7125.post@n3.nabble.com>
Subject: Contributing to MLlib
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi guys,

I'm new to Spark & MLlib and this may be a dumb question, but still....

As part of my M.Sc project, i'm working on implementation of Fuzzy C-means
(FCM) algorithm in MLlib.
FCM has many things in common with K - Means algorithm, which is already
implemented,  and I wanted to know whether should I create some inheritance
between them (some base class that would hold all the common stuff).

I could not find an answer to that in the "Spark Coding Guide"
(https://cwiki.apache.org/confluence/display/SPARK/Spark+Code+Style+Guide)


Appreciate your help

thanks,
Alex 




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-tp7125.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8151-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 30 16:17:33 2014
Return-Path: <dev-return-8151-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C9E061144A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 30 Jun 2014 16:17:33 +0000 (UTC)
Received: (qmail 59599 invoked by uid 500); 30 Jun 2014 16:17:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59545 invoked by uid 500); 30 Jun 2014 16:17:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59534 invoked by uid 99); 30 Jun 2014 16:17:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Jun 2014 16:17:32 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of chester@alpinenow.com designates 209.85.212.174 as permitted sender)
Received: from [209.85.212.174] (HELO mail-wi0-f174.google.com) (209.85.212.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Jun 2014 16:17:28 +0000
Received: by mail-wi0-f174.google.com with SMTP id bs8so6349031wib.7
        for <dev@spark.apache.org>; Mon, 30 Jun 2014 09:17:07 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=dqmt6xQbVYEe3VUpTOwQlyCugEP53tHTLPP/Icl2hqk=;
        b=CdKRMLRufClAzdRpQqSR0CTM7YYCceP53RIJDJWyd57sFySILVrwmzd8V2wett3VnL
         gQo7o7d2xAkrUJMPQVJL4UDYqddvx0eg/RZ8D8v1sURqQPhbJrFfz8BLp5Rp/uUQcnsn
         vW4czyH3hzOIBIbwj5FNXEmhz3dquGiJbYZdzap7o2ilbvauLWTds7jM32MOCy1cR10E
         CW9doRstu8FxRpQJaTQlTB2pdngvE8yxl6F51CQOcs2FWPw41/ftJw6E41RKd1d6atKU
         GmseaohkL7bfE9SMVTi9gpdOpQkbobTyHux56W2qZmlV8OA/XdN0Nydsv4IPud2VqrLE
         jzYQ==
X-Gm-Message-State: ALoCoQnmgrHzM9YOqXk96PHPkOt/w6BarhdMmgfIzEpkSGaj8HRHF8zGKZl+TRquV/KtSDLA+zas
MIME-Version: 1.0
X-Received: by 10.194.78.68 with SMTP id z4mr3933224wjw.126.1404145027297;
 Mon, 30 Jun 2014 09:17:07 -0700 (PDT)
Received: by 10.194.14.34 with HTTP; Mon, 30 Jun 2014 09:17:07 -0700 (PDT)
In-Reply-To: <CAPh_B=bbjzRPz8E+Z5G+Ywj-X5awRdKyUfyg79D4tYzAAtFKxg@mail.gmail.com>
References: <CAPYnQ0WQnjdOw-_M_9M7aJ5csqprB2n9RnG26_SOg_jWdOgnQw@mail.gmail.com>
	<CAPh_B=bbjzRPz8E+Z5G+Ywj-X5awRdKyUfyg79D4tYzAAtFKxg@mail.gmail.com>
Date: Mon, 30 Jun 2014 09:17:07 -0700
Message-ID: <CAPYnQ0V2q4onMjT1MLD=YkPUjU3udzcOdVRhbccrSaCPDFnRtg@mail.gmail.com>
Subject: Re: Application level progress monitoring and communication
From: Chester Chen <chester@alpinenow.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bf0d53290c5b304fd0ffd02
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bf0d53290c5b304fd0ffd02
Content-Type: text/plain; charset=UTF-8

Reynold
    thanks for the reply. It's true, this is more to Yarn communication
than Spark.
But this is a general enough problem for all the YARN_CLUSTER mode
application. I thought
just to reach out to the community.

  If we choose to using Akka solution, then this is related to Spark, as
there is only one Akka actor system per JVM.

  Thanks for the suggestion regarding pass the client IP address. I was
only thinking  how to find out the IP address
of the spark drive node initially.

  Reporting Progress is just one of the use case, stopping spark job, We
are also considering interactive query jobs.

This gives me some thing to start with. I will try to with Akka first. Will
let community know once we got somewhere.

thanks
Chester


On Sun, Jun 29, 2014 at 11:07 PM, Reynold Xin <rxin@databricks.com> wrote:

> This isn't exactly about Spark itself, more about how an application on
> YARN/Mesos can communicate with another one.
>
> How about your application launch program just takes in a parameter (or env
> variable or command line argument) for the IP address of your client
> application, and just send updates? You basically just want to send
> messages to report progress. You can do it with a lot of different ways,
> such as Akka, custom REST API, Thrift ... I think any of them will do.
>
>
>
>
> On Sun, Jun 29, 2014 at 7:57 PM, Chester Chen <chester@alpinenow.com>
> wrote:
>
> > Hi Spark dev community:
> >
> > I have several questions regarding Application and Spark communication
> >
> > 1) Application Level Progress Monitoring
> >
> > Currently, our application using in YARN_CLUSTER model running Spark
> Jobs.
> > This works well so far, but we would like to monitoring the application
> > level progress ( not spark system level progress).
> >
> > For example,
> > If we are doing Machine Learning Training, I would like to send some
> > message back the our application, current status of the training, number
> of
> > iterations etc via API.
> >
> > We can't use YARN_CLIENT mode for this purpose as we are running the
> spark
> > application in servlet container (tomcat/Jetty). If we run the
> yarn_client
> > mode, we will be limited to one SparkContext per JVM.
> >
> > So we are considering to leverage Akka messaging, essentially create
> > another Actor to send message back to the client application.
> > Notice that Spark already has an Akka ActorSystem defined for each
> > Executor. All we need to find Actor address (host, port) for the spark
> > driver executor.
> >
> > The trouble is that driver's host and port are not known until later when
> > Resource Manager give to the executor node. How to communicate the host,
> > port info back to the client application ?
> >
> > May be there is an Yarn API to obtain this information from Yarn Client.
> >
> >
> > 2) Application and Spark Job communication In YARN Cluster mode.
> >
> >     There are several use cases we are thinking may require communication
> > between the client side application and Spark Running Job.
> >
> >      One example,
> >        * Try to stop a running job -- while job is running, abort the
> long
> > running job in Yarn
> >
> >       Again, we are think to use Akka Actor to send a STOP job message.
> >
> >
> >
> > So here some of  questions:
> >
> > * Is there any work regarding this area in the community ?
> >
> > * what do you think the Akka approach ? Alternatives ?
> >
> > * Is there a way to get Spark's Akka host and port from Yarn Resource
> > Manager to Yarn Client ?
> >
> > Any suggestions welcome
> >
> > Thanks
> > Chester
> >
>

--047d7bf0d53290c5b304fd0ffd02--

From dev-return-8152-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 30 17:38:22 2014
Return-Path: <dev-return-8152-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 223F711724
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 30 Jun 2014 17:38:22 +0000 (UTC)
Received: (qmail 38454 invoked by uid 500); 30 Jun 2014 17:38:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38396 invoked by uid 500); 30 Jun 2014 17:38:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38378 invoked by uid 99); 30 Jun 2014 17:38:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Jun 2014 17:38:21 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.216.47 as permitted sender)
Received: from [209.85.216.47] (HELO mail-qa0-f47.google.com) (209.85.216.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Jun 2014 17:38:15 +0000
Received: by mail-qa0-f47.google.com with SMTP id hw13so6748837qab.34
        for <dev@spark.apache.org>; Mon, 30 Jun 2014 10:37:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=etZZuiLuNK2elIelcbm5EqfuroNSTtxvMM47ppOmp2U=;
        b=IgtD9/MiuEBk7y+560cmdTCevOArn8EKPBDRqyTCo2If53CmZxsiQ31UHj4p1QsIZE
         ugaSaW3Y2wNNokL/Np1je/e+jqWaNzT7mVLIdfRmBtW1/+01VKam5xbBSNcKd1e3O1vO
         SlToQkOtBhXAXw8cs7zUiNquv8kMXVmDgbGSs4lQilj5AA+ncIVkQ/9gpEqtoLBZr0qa
         +vBjSM6wDIX0+cWPldkdGcub9GW7/eOn2+k3bE78PPbt+W0zHA/x0oJD0H7+abbWsjDG
         t9WdmkH0fCHlX6V25O6Xp/dZC7oOsK9aXI43x8wIzFbHjq27wmDwiFVrz6SKFFFfgcZ7
         9VhA==
MIME-Version: 1.0
X-Received: by 10.224.34.73 with SMTP id k9mr64936392qad.11.1404149875144;
 Mon, 30 Jun 2014 10:37:55 -0700 (PDT)
Received: by 10.140.38.149 with HTTP; Mon, 30 Jun 2014 10:37:55 -0700 (PDT)
Date: Mon, 30 Jun 2014 23:07:55 +0530
Message-ID: <CAJiQeYJ=UvuOownStpX+fTxKrZS9Sg7=hSCeD_RJ=-SAX1snWQ@mail.gmail.com>
Subject: Eliminate copy while sending data : any Akka experts here ?
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

  While sending map output tracker result, the same serialized byte
array is sent multiple times - but the akka implementation copies it
to a private byte array within ByteString for each send.
Caching a ByteString instead of Array[Byte] did not help, since akka
does not support special casing ByteString : serializes the
ByteString, and copies the result out to an array before creating
ByteString out of it (in Array[Byte] serializing is thankfully simply
returning same array - so one copy only).


Given the need to send immutable data large number of times, is there
any way to do it in akka without copying internally in akka ?


To see how expensive it is, for 200 nodes withi large number of
mappers and reducers, the status becomes something like 30 mb for us -
and pulling this about 200 to 300 times results in OOM due to the
large number of copies sent out.


Thanks,
Mridul

From dev-return-8153-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jun 30 19:13:55 2014
Return-Path: <dev-return-8153-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4F7A111B4C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 30 Jun 2014 19:13:55 +0000 (UTC)
Received: (qmail 72112 invoked by uid 500); 30 Jun 2014 19:13:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72057 invoked by uid 500); 30 Jun 2014 19:13:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72044 invoked by uid 99); 30 Jun 2014 19:13:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Jun 2014 19:13:54 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.216.181 as permitted sender)
Received: from [209.85.216.181] (HELO mail-qc0-f181.google.com) (209.85.216.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Jun 2014 19:13:51 +0000
Received: by mail-qc0-f181.google.com with SMTP id x13so7538928qcv.12
        for <dev@spark.apache.org>; Mon, 30 Jun 2014 12:13:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=ewSRA3f0JJ47V8RAVjt+i05A/efE3wMEWY1n077pVfM=;
        b=Xt0OQzkgrNXl8jn8PrtuPIixa+43R9Z0bSJUb/i6fiJCNxuM+e128Gh3BaGGJbXtFM
         jZ2OyrGEVU7Chixj93VaxkEhrrh9mq/spXbggFHcMy/8iGQYkMqHeLdE3+HIzgQ6rXzg
         96ECDkXanKzXC7Gzw/hQKEqj/uKZRI2pRDuEcaSBT+hIziEJYA4K5FHTd1RCE8dAFQ/5
         FOnGQNPoZhJmXWPA6kUJI27nd/gooufMsmaiquhjAabiJv9s6YkoB7CgQ3zz6xoSWc0x
         f9hmV9md50sM6H/xjbb7Ai1vuqiUe6R8uxG28Vx7iPsqH3ObrsLyfdNmNF5aVZoF5vzJ
         CkQQ==
MIME-Version: 1.0
X-Received: by 10.224.34.73 with SMTP id k9mr65919440qad.11.1404155606815;
 Mon, 30 Jun 2014 12:13:26 -0700 (PDT)
Received: by 10.140.38.149 with HTTP; Mon, 30 Jun 2014 12:13:26 -0700 (PDT)
In-Reply-To: <CAJiQeYJ=UvuOownStpX+fTxKrZS9Sg7=hSCeD_RJ=-SAX1snWQ@mail.gmail.com>
References: <CAJiQeYJ=UvuOownStpX+fTxKrZS9Sg7=hSCeD_RJ=-SAX1snWQ@mail.gmail.com>
Date: Tue, 1 Jul 2014 00:43:26 +0530
Message-ID: <CAJiQeYL9xXFzHKZ-3VjO8xT2DnQR0yfY5Ls9em9JXb2HXZZenw@mail.gmail.com>
Subject: Re: Eliminate copy while sending data : any Akka experts here ?
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Our current hack is to use Broadcast variables when serialized
statuses are above some (configurable) size : and have the workers
directly pull them from master.
This is a workaround : so would be great if there was a
better/principled solution.

Please note that the responses are going to different workers
requesting for the output statuses for shuffle (after map) - so not
sure if back pressure buffers, etc would help.


Regards,
Mridul


On Mon, Jun 30, 2014 at 11:07 PM, Mridul Muralidharan <mridul@gmail.com> wrote:
> Hi,
>
>   While sending map output tracker result, the same serialized byte
> array is sent multiple times - but the akka implementation copies it
> to a private byte array within ByteString for each send.
> Caching a ByteString instead of Array[Byte] did not help, since akka
> does not support special casing ByteString : serializes the
> ByteString, and copies the result out to an array before creating
> ByteString out of it (in Array[Byte] serializing is thankfully simply
> returning same array - so one copy only).
>
>
> Given the need to send immutable data large number of times, is there
> any way to do it in akka without copying internally in akka ?
>
>
> To see how expensive it is, for 200 nodes withi large number of
> mappers and reducers, the status becomes something like 30 mb for us -
> and pulling this about 200 to 300 times results in OOM due to the
> large number of copies sent out.
>
>
> Thanks,
> Mridul

